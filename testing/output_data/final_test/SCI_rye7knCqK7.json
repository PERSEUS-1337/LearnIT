{
    "title": "rye7knCqK7",
    "content": "In this paper, the Individualized Controlled Continuous Communication Model (IC3Net) is introduced for multi-agent tasks. IC3Net improves training efficiency by controlling communication with a gating mechanism and using individualized rewards for each agent. It can be applied to semi-cooperative and competitive settings, in addition to cooperative settings. Results from tasks like StarCraft BroodWars scenarios show that IC3Net outperforms baselines in performance and convergence rates as the scale increases. Communication is crucial for intelligence, allowing agents to cooperate towards common goals in multi-agent settings. In partially observable environments, agents can share information and learnings through communication. The field of reinforcement learning has seen success in single-agent domains, but the number of multi-agent systems and applications is growing, from teams of robots in manufacturing plants to networks of self-driving cars. In multi-agent environments, scaling reinforcement learning is crucial for building intelligent systems for higher productivity. Different scenarios like cooperative, semi-cooperative, and competitive have not been extensively studied. Mixed scenarios, where agents are cooperative but not fully, resemble real-life situations where individuals work towards personal goals while cooperating. In competitive scenarios, agents compete for rewards, similar to humans who can communicate but choose when to do so, like in sports matches where teams may choose not to communicate to prevent sharing strategies or use dishonest signaling. Agents in multi-agent environments need to optimize their own reward and handicap opponents by knowing when to communicate. Teaching agents how to communicate eliminates the need for hand-coding communication protocols. Continuous communication through a vector is easier to train and has a higher information throughput than discrete symbols. This type of communication is differentiable and can be efficiently trained with back-propagation, assuming full cooperation between agents. The Individualized Controlled Continuous Communication Model (IC3Net) allows agents to be trained with individual rewards, enabling application in any scenario, whether cooperative or competitive. The model addresses issues of full-cooperation and average global rewards, improving credit assignment and scalability. Through a gating mechanism, agents can learn when to communicate effectively. The IC3Net model utilizes a gating mechanism for agents to control communication, showing superior performance in various environments like StarCraft. Compared to using independent controllers for each agent, IC3Net converges individual rewards faster and better, highlighting the limitations of naive approaches in multi-agent reinforcement learning settings. Most algorithms in multi-agent reinforcement learning are designed for cooperative settings, with limited applicability in competitive or mixed scenarios. Our work extends CommNet for collaboration among agents using continuous communication, suitable only for cooperative tasks. The model allows for backpropagation learning of the controller but is restricted to fully cooperative tasks due to complete communication of hidden states. The Multi-Agent Deep Deterministic Policy Gradient (MADDPG) model by BID19 aims to achieve similar goals as CommNet but differs in providing coordination signal through a centralized critic per agent. Another model, COMA by BID8, uses centralized critic and decentralized actors with counterfactual reward to address multiagent credit assignment challenges. VAIN by BID13 utilizes Interaction Networks to model multi-agent communication. BID13 utilizes Interaction Networks with attention mechanism for predictive modelling in supervised settings, focusing on agents' communication content in discrete settings with two agents. BID16 and BID10 demonstrated agents' coordination and grounding communication protocol to a symbols's sequence. BID14 showed how language can be made more human-like with certain restrictions. The model uses a shared LSTM for all agents to be invariant to permutations. IC3Net extends the independent controller model by allowing agents to communicate their internal state through a gating action. The policy of each agent is determined by a communication vector and a gating function, which decides whether the agent will communicate with others. Both the action policy and gating function are trained using REINFORCE algorithm. In this work, the approach moves away from a single big network controller to multiple big networks with shared parameters, each controlling a single agent separately. Each big network consists of multiple LSTM networks processing an observation of a single agent, allowing for training each agent with different rewards. This change has little effect on implementation but enables individualized agent training. The approach in this work involves using multiple big networks with shared parameters to control individual agents separately, allowing for training each agent with different rewards. This method enables individualized agent training and addresses the credit assignment issue in multi-agent algorithms. The network is tested in cooperative, mixed, and competitive scenarios to understand its effectiveness in communication and scalability using individual rewards. In multi-agent communication settings, the effectiveness of individual rewards versus global rewards is analyzed. The network is trained in three different environments with varying difficulty levels and coordination requirements. The tasks involve controlling multiple agents in scenarios where they must decipher observations for multiple opposing units. One specific task involves predators with limited vision trying to locate a stationary prey, with different cooperation settings designed for the agents. In multi-agent communication settings, three cooperation settings (competitive, mixed, and cooperative) with different reward structures are designed to test the network. The competitive setting penalizes agents if others reach the prey, while the cooperative setting rewards more agents reaching the prey. The network's performance is compared in different environments with increasing agents and grid size. In the traffic junction task, cars enter a junction with a probability p arr and can take gas or brake actions. The task has three difficulty levels with varying routes and entry points. Vision is always set to zero to make the task unsolvable without communication. The architecture is tested on StarCraft for scalability in complex scenarios. In StarCraft, the architecture is tested for scalability in complex scenarios involving combat and exploration tasks. Different settings like cooperative, mixed, and competitive show varying communication patterns among agents based on their objectives and rewards. Prey agents rarely communicate with predators, while team agents in competitive scenarios only communicate when necessary to avoid reward division near enemies. In StarCraft, agents communicate only when necessary to avoid reward division near enemies. The game has macro-actions for targeting enemy units, but we make the task harder by removing them and limiting vision. Two types of tasks are considered: Explore, where agents try to find an enemy unit, and Combat. In StarCraft, agents are tested on their combat capabilities in a complex task that involves coordination, exploration, understanding of enemy units, and strategic planning. A team of agents must find and eliminate a team of enemies in a partially observable environment. The agents have limited vision and must locate and defeat all enemy units to win. The working of gating action in IC3Net is analyzed using cooperative, competitive, and mixed settings in Predator-Prey and StarCraft explore tasks. The enemy unit shares parameters with predators and is trained alongside them. The enemy unit in the Predator-Prey task is stationary with noop actions, receiving a positive reward until captured. Results show that the prey does not communicate with predators to avoid being caught faster and achieve higher rewards. In a cooperative setting, predators communicate openly, while in a competitive setting, similar results are observed in a larger map size for the StarCraft explore task. In cooperative settings, predators openly communicate with a high level of cooperation, while prey initially communicates but learns to stay silent to avoid sharing its location. It takes longer to train the prey to be silent in this setting. In mixed settings, agents do not always communicate, and the prey easily learns not to communicate. In competitive settings, predators rarely communicate due to the competitive nature of the task. In competitive settings, predators rarely communicate due to the competitive nature of the task. Predators can initially explore faster if they communicate, leading to higher rewards. Agents only communicate when profitable, before reaching the prey, as it can impact future rewards. Experiments suggest agents can \"learn to communicate when profitable,\" allowing the same network to be used in all settings. CommNet allows communication between agents using hidden state representations as signals, similar to IC3Net but without gating actions or individual rewards. This approach addresses credit assignment issues seen in CommNet and explores behaviors of agents in experiments. In Appendix 6.2, behaviors of agents in Predator Prey task were analyzed. IC3Net outperformed baselines in finding prey faster as the number of agents and maze size increased. Scalability graph showed IC3Net converges faster than CommNet with more agents. CommNet struggled in mixed scenarios. Training plot of 20x20 grid with 10 agents displayed IC3Net's faster performance improvement. The plot demonstrates IC3Net's faster performance improvement compared to CommNet in finding prey. Gating action values show a similar pattern in Traffic Junction, where IC outperforms IRIC in the hard case. IC3Net excels in BID29 with vision > 0, showcasing superior performance over CommNet. IC3Net outperforms CommNet and IRIC in fully-cooperative tasks with communication due to individualized rewards, achieving a performance gap greater than 30%. In a reverse scenario, IC3Net controls 3 Zealots against 10 marines, reaching a 100% success rate faster than IRIC. IC3Net aims to solve multi-agent tasks by learning when to communicate, enabling efficient training through continuous communication and reinforcement learning. IC3Net demonstrates strong performance in various settings and learns to communicate effectively only when necessary. The network shows scalability in experiments and aims to explore multi-channel communication in the future. Key details include using LSTM BID12 with recurrence, RMSProp BID31 with tuned hyper-parameters, and distributed training over 16 cores. In experiments using PyTorch and Gym environments, IC3Net outperforms CommNet in explore task. Training involves 10 weight updates per epoch, with tasks completing in varying times - StarCraft tasks take over a day, while predator-prey and traffic junction tasks finish in under 12 hours. Five runs are conducted on each task to compile results, with analysis of behaviors and patterns in the outcomes. IC3Net performs better than CommNet in explore task but not in Combat task. Units staying together and focused firepower with more attacks result in good combat performance. A heuristics baseline \"attack closest\" shows success in killing zealots. Global reward for winning in Combat task is significantly higher than individual rewards. Coordination to stay together and focus fire are key for achieving success. In exploration, agents in CommNet struggle to coordinate individually due to global rewards not reflecting individual contributions. Agents tend to gather and explore together, making it easier to coordinate. Significant variance in IC3Net results for StarCraft was observed in experiments. In StarCraft experiments, significant variance is attributed to stochasticity in the environment, leading to a large number of possible states for agents. This variance is challenging to learn, especially in higher Win % models. Adding gating action in CommNet improves results but also increases difficulty in learning. CommNet performs worse than IRIC and IC in the StarCraftExplore task. In the StarCraftExplore task, CommNet performs worse than IRIC and IC. A hypothesis suggests that individualized rewards are more effective than global rewards for exploration. IRIC outperforms IC overall, indicating the benefits of individual rewards. CommNet's ease of communication leads to slower exploration as agents tend to gather before exploring, unlike IC which promotes faster exploration by making gathering difficult. In the StarCraftExplore task, individualized rewards are more effective than global rewards for exploration. In the mixed scenario, agents receive constant penalties until they find the prey, and once they do, they receive positive rewards that do not depend on the number of agents present. In cooperative and competitive settings, agents receive rewards based on the number of agents on the prey. In the Predator-Prey Cooperative task, the agent's behavior is determined by a formula involving exploration and prey rewards, with different parameters for competitive, mixed, and cooperative scenarios. Grid sizes and number of predators vary, with each predator having five movement actions. Observations are represented as one-hot binary vectors, and each agent's observation has a dimension of 32 times the sum of all vectors. Table 3 shows IC3Net outperforming baselines in cooperative predator-prey environments across three difficulty levels. The individualized rewards in IC3Net make it more scalable, even matching CommNet's performance in cooperative scenarios. In a mixed cooperation setting, individualized rewards significantly aid agents in understanding their contributions. Adding a gating action allows the model to adapt to various scenarios, with a slight decrease in performance but enabling communication similar to humans. The observation vocabulary includes one-hot vectors for locations and car class, aiding agents in decision-making. In a mixed cooperation setting, individualized rewards aid agents in understanding their contributions. Agents observe previous actions, route identifiers, and one-hot vectors for classes at their location. Collisions occur when cars are in the same location, with negative rewards set for collisions and traffic jams. Curriculum learning is utilized to ease training, with a linear increase in a parameter over epochs. Training continues with different difficulty levels having specific parameter values. The training continues for 750 epochs with a fixed learning rate of 0.003. Three difficulty variations of the game are implemented: easy, medium, and hard. The easy version has two arrival points on a 7x7 grid, the medium version has four arrival points on a 14x14 grid, and the hard version has eight arrival points on an 18x18 grid. In Table 1, it is observed that IRIC and IC perform worse in the medium level compared to the hard level due to a high final add-rate in the medium version. The medium difficulty level of the traffic junction task has a high final add-rate leading to more collisions and lower success rates compared to the hard level. The hard level has a lower final add-rate to allow for easier passage through junctions despite having more entry points. Curriculum learning is used to train models successfully on the hard level with four connected junctions and eight entry points. To complete the explore task, agents must be within a particular range of enemy unit called explore vision. The reward structure is the same as the PP task, but agents need to be within the explore vision range of the enemy unit to receive a non-negative reward. Medic units are used to simulate the explore task without combat. Agents have 9 actions to choose from, including 8 basic directions and one stay action. Observation for each agent includes absolute x, absolute y, and enemy's relative position when in explore vision range. In combat, agents observe enemy positions and health, normalized between 0 and 1. They choose from 9+m actions, including attacking visible enemies. No comparison with prior work on StarCraft due to different environment. Negative reward given when not in combat. In combat, agents receive rewards based on their health difference and enemy attacks. Terminal rewards include negative rewards for remaining health, positive rewards for winning, and negative rewards for enemies' health. The task involves enemies and agents being initialized separately, requiring communication. An example sequence of states in StarCraft cooperative explore mode is provided. Agents in SC Cooperative Explore communicate to locate and reach the enemy faster, with some agents near the prey and others arriving quickly due to communication."
}