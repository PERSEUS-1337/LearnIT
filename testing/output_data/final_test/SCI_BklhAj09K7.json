{
    "title": "BklhAj09K7",
    "content": "Unsupervised domain adaptation aims to improve deep neural network performance on a target domain using only source domain labels. This paper introduces a method to learn a representation space that is discriminative for both the labeled source and unlabeled target domains, while keeping the domains well-separated. Inspired by a theoretical analysis, the paper reformulates the disjoint classification task to a verification task, proposing a Feature Transfer Network (FTN) to separate the target feature space from the original source space. The paper introduces a Feature Transfer Network (FTN) to separate the target feature space from the original source space and boost discriminative power. It includes a non-parametric multi-class entropy minimization loss. FTNs are shown to be effective in adapting from MNIST-M to MNIST with disjoint digit classes and in cross-ethnicity face recognition tasks. Despite strong performances on facial analysis using deep neural networks, learning a model that generalizes across variations in attributes like ethnicity, gender, or age remains a challenge. Biases in commercial engines can lead to mistakes in detecting gender for images of darker-skinned females, with significant social consequences. This paper proposes a deep unsupervised domain adaptation approach to overcome biases in face verification and identification. Deep domain adaptation allows for porting a deep learning model to different domains. Deep domain adaptation techniques aim to reduce source-target domain divergence using adversarial training or discrepancy minimization, maintaining feature space discriminative power. This approach assumes a common task between domains and is typically applied to classification problems with shared label spaces. Domain adaptation techniques aim to reduce source-target domain divergence by applying entropy minimization or self-ensembling on target examples. However, challenges arise when label spaces of source and target domains do not align, as seen in cross-ethnicity face recognition. Different ethnicity domains have disjoint label spaces, requiring representations in the embedding space to be distant from each other. In this work, the focus is on learning a shared representation space between source and target domains with disjoint label spaces. The goal is to keep examples from different domains well-separated while remaining discriminative. The approach involves converting disjoint classification tasks into a unified binary verification task, termed cross-domain distance metric adaptation (CD2MA), to address the limitations of domain adversarial neural networks. In this study, the focus is on cross-domain distance metric adaptation (CD2MA) to address the challenge of verifying examples from different domains. A Feature Transfer Network (FTN) is introduced to align target and source features by learning a shared feature extractor and a feature transfer module. This alignment enhances discriminative power for cross-domain verification tasks. The study introduces a Feature Transfer Network (FTN) for cross-domain distance metric adaptation (CD2MA) to enhance discriminative power for verification tasks. It aligns target and source features through domain adversarial loss and introduces a novel entropy minimization loss called multi-class entropy minimization (MCEM). MCEM leverages unlabeled target examples using N-pair metric loss and hierarchical clustering. Experiments show the effectiveness of FTN in adapting between disjoint sets of digit classes. The study introduces a Feature Transfer Network (FTN) for cross-domain distance metric adaptation (CD2MA) to enhance discriminative power for verification tasks. It aligns target and source features through domain adversarial loss and introduces a novel entropy minimization loss called multi-class entropy minimization (MCEM). The proposed unsupervised CD2MA method significantly improves face verification and identification for cross-ethnicity face recognition tasks. Research in deep domain adaptation focuses on measuring the variational distance between domains and regularizing neural networks to minimize this distance. Different methods like maximum mean discrepancy and domain adversarial neural networks have been successful in computer vision applications. Unlike previous works, this study addresses a cross-domain distance metric adaptation problem with different label spaces between source and target domains. In this study, the problem setting involves adapting from a labeled source to an unlabeled target with disjoint label spaces, combining elements of domain adaptation and transfer learning. The difference in input distribution and lack of labels in the target domain resemble domain adaptation or transductive transfer learning, while the difference in label distribution and task definitions is similar to inductive transfer learning. The problem is formalized in a domain adaptation framework using verification as a common task, allowing for theoretical analysis on generalization bounds and applications like cross-ethnicity face recognition. In domain adaptation, various approaches like Hu et al. (2015) and Sohn et al. (2017) focus on distance metric learning but struggle to separate source and target domains. Luo et al. (2017) addresses domain adaptation with disjoint label spaces, while Long et al. (2016) uses a residual transfer network for scenarios with shared label spaces. BID0 shows that the empirical risk on the target domain is bounded by the empirical risk on the source domain and the variational distance between the two. The verification task is reformulated as a binary classification task shared across two domains, with the goal of predicting if a pair of images share the same identity. Theoretical results from BID0 can be applied to this new setup, where the empirical within target domain verification loss is bounded by the within source domain. The empirical within target domain verification loss is bounded by the within source domain verification loss and the variational distance between X S \u00d7 X S and X T \u00d7 X T. Domain adversarial training coupled with supervised source domain binary classification loss can yield target representations with high discriminative power for within-domain verification. The proposed framework addresses the discrepancy in cross-domain verification tasks by introducing a feature transfer module and domain separation objective. The goal is to verify random samples drawn from different data distributions without prior knowledge of their origin. The framework includes a feature transfer network (FTN) and training objectives to achieve desired properties, with practical considerations for implementation. The proposed framework introduces a Feature Transfer Network (FTN) for cross-domain verification tasks. It includes a feature generation module, feature transfer module, and domain discriminators. The objective is to align domains and distinguish source from target domains. The FTN addresses the discrepancy in verifying samples from different data distributions without prior knowledge of their origin. The proposed Feature Transfer Network (FTN) is designed to address cross-domain verification tasks by transferring the discriminative power of a classifier from the source domain to the target domain. This is achieved by adapting the representation spaces of the domains and utilizing the same competent classifier. FTN consists of multiple modules to handle scenarios where examples are labeled in the source domain but unlabeled in the target domain, as well as cases where examples cannot be of the same class. The Feature Transfer Network (FTN) consists of modules for feature generation, transfer, and verification. The feature generation module maps input data to distinguishable spaces, while the feature transfer module aligns the source domain features with the target domain. Verification losses are applied using classifiers during testing to compare metric distances between features. The desired capabilities include separating features from different domains and aligning features from the same class. The Feature Transfer Network (FTN) aims to align source domain features with the target domain by optimizing h f and h g through discriminative power. Verification losses are evaluated at different representation spaces to compare metric distances between features for classification. The objective is to separate features from different domains and align those from the same class. The goal of the Feature Transfer Network (FTN) is to align source domain features with the target domain using neural networks to learn generators f and g for distance metrics. Discriminators D1 and D2 are trained to distinguish between source and target distributions, pushing them apart in representation spaces. The objective is to separate features from different domains and align those from the same class. The Feature Transfer Network (FTN) aims to align source domain features with the target domain by training generators f and g. Discriminators D1 and D2 distinguish between source and target distributions to separate features from different domains and align those from the same class. The training objectives include domain adversarial and domain separation objectives with regularization to prevent mode collapse in domain adversarial learning. The Feature Transfer Network (FTN) aligns source domain features with the target domain using regularization methods to prevent mode collapse. The feature reconstruction loss is crucial for stable training and optimal performance. The Feature Transfer Network (FTN) uses N-pair loss instead of verification loss for better deep distance metric learning. Entropy minimization is also employed for unsupervised domain adaptation to minimize class prediction distribution entropy for confident decision rules. The Feature Transfer Network (FTN) utilizes N-pair loss for deep distance metric learning and entropy minimization for confident decision rules. Entropy minimization is extended for distance metric adaptation by incorporating a multi-class entropy minimization (MCEM) objective in the target domain, requiring N pair examples sampled from the unlabeled target domain. Plausible label structure discovery in the target domain is done offline using HDBSCAN. The Feature Transfer Network (FTN) utilizes N-pair loss for deep distance metric learning and entropy minimization for confident decision rules. It incorporates a multi-class entropy minimization (MCEM) objective in the target domain, discovered offline using HDBSCAN. The clusters provide pseudo-labels for sampling N pair examples to optimize f in Equation FORMULA10. Experimental proof on digit datasets and cross-ethnicity generalization in face recognition demonstrate the effectiveness of FTN. The Feature Transfer Network (FTN) uses learning objectives for DANN with fixed parameters. An experiment is conducted to adapt digits from different domains, aiming to separate digit classes within and across domains. The feature generator and transfer module are designed with discriminators to induce domain adversarial and separation losses. More architecture details are provided in the appendix. The Feature Transfer Network (FTN) in FIG0 (c) successfully separates digit classes within and across domains, achieving 10 clean clusters without visual overlap among 10 digit classes. It shows 2.1% verification error within the target domain and 0.3% domain differentiation error. The performance of face recognition engines has greatly improved due to advances in deep learning and large-scale datasets. However, most public datasets have a significant label bias towards Caucasian ethnicity, leading to imbalances in training data that can impact identification performance for minorities. The model trained on a Caucasian-dominated dataset performs poorly on other ethnicities, leading to a drop in identification performance for data-scarce minorities. Leveraging unlabeled data from non-Caucasian target ethnicities can substantially improve face verification performances. The experiment involves adapting from CAU to a mixture of AA and EA using the MS-1M dataset. The training set includes 4.04M images from 60K CAU identities, 398K images from 7K AA identities, and 308K images from 4.6K EA identities. Labeled CAU images are used for domain adaptation experiments, while labeled CAU, AA, EA images are used for supervised experiments. A 38-layer ResNet is used for feature generation, and a 4096-pair loss is used for training. The network architecture and training scheme result in competitive face recognition performance compared to other state-of-the-art methods. The study evaluates the performance of baseline and proposed models on face recognition benchmarks LFW and IJB-A, which exhibit ethnicity bias. A Cross-Ethnicity Faces (CEF) dataset is created for CAU, AA, and EA ethnicities, with evaluation metrics including verification and identification accuracy. The study evaluates baseline and proposed models on face recognition benchmarks LFW and IJB-A, with a focus on ethnicity bias. A Cross-Ethnicity Faces (CEF) dataset is created for CAU, AA, and EA ethnicities, with evaluation metrics including verification and identification accuracy. Results show DANN without feature reconstruction loss leads to unstable training and marginal improvement upon Sup C. FTN training also exhibits similar instability. When testing on AA and EA with model trained on only the labeled source CAU domain, significant performance drops are observed. Cross domain identification accuracy is much higher than within domain identification accuracy, indicating a discrepancy between feature spaces and lack of discriminative power. DANN and FTN show moderate improvement when testing on AA and EA from CEF, demonstrating the effectiveness of domain adversarial learning in transferring within domain verification. The proposed FTN outperforms DANN in cross domain identification accuracy, showing both within and cross domain discriminative power. FTN combined with multi-class entropy minimization (FTN+MCEM) further boosts verification and identification accuracy, approaching the performance upper bound. HDBSCAN-based hierarchical clustering provides high-quality pseudo-class labels for MCEM to be effective, achieving high F-scores on AA and EA. The clustering algorithm achieves high F-scores on AA and EA. Performance of face recognition models is reported in Table 3, showing improvements with the proposed distance metric adaptation. The method addresses the challenge of unsupervised domain adaptation by formulating the classification problem into a verification task. A Feature Transfer Network is proposed for simultaneous optimization of domain adversarial loss and domain separation loss. The proposed framework focuses on improving adaptation quality through various loss functions and metric optimization. It excels at within-domain and cross-domain verification tasks, including cross-ethnicity face verification. Data preprocessing involves normalization and resizing, with a feature generator module comprising convolution and pooling layers. The feature generator module consists of 6 convolution layers, 3 max-pooling layers, and 2 fully-connected layers with ReLU activation. The output dimension is 128 with an L2-norm of 2. The feature transfer module uses two fully-connected layers and a residual connection to map a 128-dimensional vector. The discriminator architecture includes fully-connected layers with an output dimension of 128. Adam optimizer with a learning rate of 0.0003 is used for training, following protocols from previous studies for data preprocessing and network architecture. Face images are preprocessed by detection, alignment, and cropping to a size of 110 \u00d7. The data preparation involves detecting and aligning face images before cropping them to 110 \u00d7 110 size. The feature generation module consists of 38 convolution layers with residual blocks and max pooling, using ReLU and maxout nonlinearities. A 7 \u00d7 7 average pooling layer is added, resulting in a 320-dimensional vector normalized to size 12. The feature transfer module maps this vector using two fully-connected layers and a residual connection. The full model description is in TAB1. The feature transfer module architecture is detailed in Figure 1 (a), with discriminators having similar network structures. Models are trained with a 4096-pair loss, using Adam stochastic optimizer with varying learning rates. The feature generation module is initialized with the Sup C model and updated with a learning rate of 0.00003. Hyperparameters are summarized in Table S3, showcasing the effectiveness of feature reconstruction loss in stabilizing the model. The effectiveness of feature reconstruction loss in stabilizing the domain adversarial training in DANN framework is demonstrated. Four different DANN models were trained with varying configurations of \u03bb 3 and \u03bb 4, showing improved performance on target ethnicities initially but dropping accuracy when certain values are set to 0. The failure of discriminative information transfer is implied by the high performance on the CAU set even in this situation. The proposed feature reconstruction loss with carefully selected values of \u03bb 3 and \u03bb 4 shows stable performance in DANN and FTN models. The clustering accuracy is analyzed through verification precision and recall measurements. The verification precision and recall are used to evaluate clustering performance, aiming for high precision and recall to ensure examples with the same class labels are assigned to the same cluster. Only clusters of size 5 or larger are considered as new target classes. Additionally, a clustering strategy is proposed, along with evaluating clustering performance by finding nearest classes or examples from the source domain for zero-shot learning or semi-supervised domain adaptation. The clustering performance is evaluated by matching nearest classes or examples from the source domain for zero-shot learning or semi-supervised domain adaptation. The idea of clustering by finding the nearest source classes assumes a cross-category similarity between disjoint classes of source and target domains. Using hierarchical clustering on target features achieves higher precision and recall, with 100% precision achieved using embedding vectors of Sup C. FTN features show lower precision but higher recall, resulting in a higher overall F-score. The FTN+MCEM model further improves the F-score and returns more target examples within clusters. The FTN+MCEM model improves the F-score and returns more target examples within clusters, suggesting a potential tool for automatic labeling of unlabeled data through iterative training and hierarchical clustering. Images from each ethnicity subset are visualized for annotation quality assurance."
}