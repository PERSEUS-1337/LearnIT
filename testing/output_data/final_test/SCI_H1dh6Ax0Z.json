{
    "title": "H1dh6Ax0Z",
    "content": "TreeQN is a differentiable, recursive, tree-structured model that replaces any value function network in deep RL with discrete actions. It dynamically constructs a tree by recursively applying a transition model in a learned abstract state space and aggregating predicted rewards and state-values using a tree backup to estimate Q-values. ATreeC is an actor-critic variant that adds a softmax layer to form a stochastic policy network. Both approaches are trained end-to-end. TreeQN and ATreeC are trained end-to-end and outperform n-step DQN and A2C on tasks like box-pushing and multiple Atari games. Ablation studies show the impact of different auxiliary losses on learning transition models. Combining model-free deep RL with on-line planning refines the value function locally without new samples, using look-ahead tree search methods. The planning involves using look-ahead tree search methods in complex environments with high-dimensional observation spaces. Learning accurate models for such domains has been challenging, as existing approaches often focus on irrelevant aspects of the environment dynamics. Our approach, TreeQN, replaces fully connected layers of DQN with a recursive network for faster learning of better policies in complex environments. ATreeC, an actor-critic variant, uses the tree as a policy network and outperforms DQN-based counterparts in various games. TreeQN, a recursive network replacing fully connected layers of DQN, outperforms VPN BID18 in most Atari games. Ablation studies show the importance of grounding the transition model in the environment for improved performance and interpretable internal plans. The challenge remains in learning strongly grounded transition models and generating reliable interpretable plans without compromising performance. The Bellman optimality equation defines the Q-function recursively using the MDP state transition function and reward function. Q-learning uses a single-sample approximation to iteratively improve Q. Deep Q-learning represents Q with a neural network and improves it by regressing to a target. N-step Q-learning with synchronous environment threads is used to gather transitions for training. The actor-critic algorithm A2C is a synchronous variant of A3C, where a policy \u03c0 and state-value function V(s) are trained using a gradient. The algorithm also includes hyperparameters for entropy regularization and learning rates. The neural network architecture in deep RL typically consists of convolutional layers followed by fully connected layers. TreeQN is a novel end-to-end approach for tree-search on-line planning, using a neural network with convolutional and fully connected layers to estimate action-values based on state encodings. The method constructs a look-ahead tree of possible future states and evaluates them using a learned value function or Monte-Carlo rollouts. This approach focuses on deterministic transition functions without value uncertainty estimates, but can be extended to other tree-search variants if components remain differentiable. TreeQN is a novel end-to-end approach for deep reinforcement learning, using a recursive tree-structured neural network to estimate state-action values. It refines its estimate via learned transition, reward, and value functions, implementing an inductive bias missing from DQN. TreeQN learns an action-dependent transition function to predict the next state representation and corresponding reward for each action. It constructs a tree of state representations and rewards for all possible action sequences up to a predefined depth. The value of each predicted state is estimated using a value function module, and TreeQN performs a tree backup using these values and predicted rewards. TreeQN constructs a tree of state representations and rewards for all possible action sequences up to a predefined depth. Using predicted rewards and value estimates, TreeQN performs a tree backup by mixing k-step returns along each path in the tree using TD(\u03bb). This encourages meaningful states in the tree and reduces the impact of outlier values. The structure decomposes the value function as a sum of action-conditional reward and next-state value, using a shared value function to evaluate each next-state representation. TreeQN constructs a tree of state representations and rewards for all possible action sequences up to a predefined depth. Using predicted rewards and value estimates, TreeQN performs a tree backup by mixing k-step returns along each path in the tree using TD(\u03bb). This ensures meaningful states in the tree and reduces outlier values. The value function is decomposed as a sum of action-conditional reward and next-state value, with a shared value function evaluating each next-state representation. The components of TreeQN, including the encoder function and transition function, are described in detail. TreeQN constructs a tree of state representations and rewards for all possible action sequences up to a predefined depth. Using predicted rewards and value estimates, TreeQN performs a tree backup by mixing k-step returns along each path in the tree using TD(\u03bb). The next-state representation is calculated independently for each action using a shared transition function. To maintain quality and diversity in latent states, a unit-length projection of state representations is introduced by dividing the vector representation by its L2 norm before each application of the transition function. TreeQN constructs a tree of state representations and rewards for all possible action sequences up to a predefined depth. To maintain quality and diversity in latent states, a unit-length projection of state representations is introduced by dividing the vector representation by its L2 norm before each application of the transition function. Additionally, the reward function predicts immediate rewards for actions in a state, while the value function estimates the value of a state representation. The tree backup function recursively calculates backups using a function that mixes k-step returns along each path in the tree using TD(\u03bb). The softmax function in TreeQN allows for updating parameters along all paths using downstream gradient information, potentially reducing the impact of outlier value predictions. The architecture is fully differentiable and can be used in place of a Q-function in deep RL algorithms with discrete actions. Differentiating through the entire tree ensures learned components are useful for online planning, and auxiliary objectives could further improve performance by grounding transition and reward functions. TreeQN architecture can benefit from grounding transition and reward functions to the environment, encouraging model-based planning. This approach spans from model-free to fully model-based methods, with the potential for intermediate grounding levels to combine flexibility with explicit model learning. Two auxiliary objectives are experimented with to explore this spectrum. In the context of the TreeQN architecture, two auxiliary objectives are explored to enhance model-based planning: reward grounding with an L2 loss regression and state grounding using a latent space grounding approach. These experiments aim to combine flexibility with explicit model learning in the spectrum of model-free to fully model-based methods. In Section 7.1, results on using reward grounding and state grounding objectives are presented for the TreeQN architecture. The design intuitions of TreeQN can be applied to policy search, where a tree planner can improve action probabilities. ATreeC is proposed as an actor-critic extension of TreeQN, with a policy network and critic sharing encoder parameters. The critic predicts a scalar state value using encoder parameters and a fully connected layer. Sharing parameters between the critic value function and actor's tree-value-function module had little effect on performance. The setup is trained with A2C and auxiliary losses, similar to TreeQN. Previous work has combined model-based and model-free RL methods, such as Dyna-Q and sparse model training. In deep RL, different approaches like value iteration networks and predictron use learned models for planning and value prediction. Value prediction networks are similar but use learned model components in a tree for planning and action selection. TreeQN is a unified architecture that constructs the tree dynamically at every timestep and differentiates through it, simplifying training and improving performance on the Atari benchmark. Unlike BID6, TreeQN does not use convolutional transition functions and allows for more flexibility in training regimes. Imagination-augmented agents learn to refine policies by aggregating rollouts predicted by a model, but rely on pretraining an observation-space model. Value gradient methods differentiate through models to train policies but do not refine them during execution. Scheduled sampling variants may be used to improve exploration. Scheduled sampling variants can improve exploration in models. TreeQN and ATreeC are evaluated in simple and complex environments to determine if they outperform DQN, A2C, and VPN. The study also explores grounding the transition function with auxiliary losses, increasing parameters/computations in DQN, and the impact of tree depth. Full experimental details are provided in the appendix. In hyperparameter search on the Atari game Seaquest, experiments explore trade-offs between model-free and model-based approaches. In the Box Pushing environment, an agent must push boxes into goals while avoiding obstacles, with 'soft' obstacles generating negative rewards. TreeQN enhances a standard DQN architecture and is compared to two baselines. TreeQN is compared to two baseline architectures, DQN-Wide and DQN-Deep, with increased computation and parameters. Experimental results for TreeQN and ATreeC.7.1 GROUNDING FIG4 show the impact of hyperparameter search on predicted rewards and latent states.\u03b7 r = 1 and \u03b7 s = 0 are used for subsequent experiments. The predicted rewards in the reward-grounding objective align with the true objective in TreeQN. State-grounding loss may aid representation learning but does not explicitly learn the desired target. Pre-training the model could potentially improve performance in joint training. TreeQN results are compared to a DQN baseline in FIG3. TreeQN outperforms DQN in learning policies for obstacle avoidance and goal alignment. Even a depth-1 tree shows significant performance improvement by disentangling reward and next-state value estimation. Deeper trees with n-step Q-learning learn faster and achieve higher plateaus. Sharing value-function parameters across branches further enhances learning in this domain. Useful transition functions are easy to learn and can refine value estimates, providing advantages for additional depth. ATreeC outperforms the A2C baseline with tree depths 1, 2, and 3, showing substantial improvements over TreeQN. ATreeC's stochastic policy helps capture uncertainty and make more decisive actions compared to TreeQN. The performance of ATreeC is not significantly affected by different tree depths, indicating a ceiling effect in this domain. The quality of the critic's value function plays a role in ATreeC's performance, with experiments using a single linear layer after state encoding. TreeQN can be used as a drop-in replacement for deep RL algorithms, showing substantial benefits in many environments compared to DQN. In most environments, TreeQN outperforms DQN on average, with significant gains in Alien, CrazyClimber, Enduro, Krull, and Seaquest. TreeQN is well suited for short horizon look-ahead planning in various games, showing significant benefits over DQN-Wide and DQN-Deep. It outperforms DQN-Deep and DQN baseline, indicating that its additional structure brings benefits beyond just adding model capacity and computation. ATreeC outperforms its baseline (A2C) in all environments, with better performance across most environments compared to TreeQN. However, it struggles in Seaquest due to rapid policy entropy collapse, leading to exploration deficiencies. Krull and Frostbite present challenges for most algorithms due to poor performance and high variance in returns. In some environments, greater depth is useful for TreeQN, resulting in the best-performing agents. Combining TreeQN and ATreeC with smart exploration mechanisms could improve training robustness. The Atari domain has more complex dynamics, requiring larger embedding sizes for transition functions. The utility of deeper trees in complex domains is hindered by optimization difficulties and challenges in learning abstract-state transition functions. Future work is needed to refine methods for abstract planning. The decomposition of Q-value into reward and next-state value is beneficial in various tasks, with learned policies sometimes aligning with intuitive reasoning. However, actions corresponding to highly scored branches in the tree are often not taken in future timesteps, showcasing the flexibility of TreeQN and ATreeC in refining action-value estimates. TreeQN and ATreeC are new architectures for deep reinforcement learning that integrate differentiable on-line tree planning into the action-value function or policy. Experiments on a box-pushing domain and Atari games show the benefits of these architectures over their counterparts and VPN. Future work aims to optimize deeper trees more efficiently, encourage interpretable plans from transition functions, and integrate smart exploration. The box-pushing environment involves generating new levels with an agent, boxes, goals, and obstacles on an 8x8 grid. In a box-pushing environment on an 8x8 grid, the agent faces penalties for stepping off the grid, pushing boxes out, or moving onto obstacles. Pushing a box into a goal earns a reward, while moving into another box incurs a penalty. Episodes end after 75 timesteps, if the agent leaves the grid, or when no boxes remain. The model receives observations as a tensor with binary encodings of agent, goals, boxes, and obstacles. The architecture includes convolutional and fully connected layers with ReLU nonlinearities. Atari experiments follow a similar architecture with a different encoder design. The encoder architecture for box-pushing experiments includes conv-8x8-4-16, conv-4x4-2-32, fc-512 layers. RMSProp with a learning rate of 1e-4 and decay of \u03b1 = 0.99 is used. The learning rate was tuned using DQN on Seaquest environment. Exploration for DQN and TreeQN decayed linearly from 1 to 0.05 over 4 million environment transitions. A2C and ATreeC use \u03b1 = 0.5 and \u03b2 = 0.01 for value-function loss and entropy regularization. Reward prediction loss scaled by \u03b7 r = 1. Batch size is 80 with n steps = 5 and n envs = 16. Discount factor \u03b3 = 0.99 and target networks updated every 40,000 environment transitions."
}