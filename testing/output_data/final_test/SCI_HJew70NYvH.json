{
    "title": "HJew70NYvH",
    "content": "Monte Carlo Tree Search (MCTS) has shown success in discrete environments like Go and Arcade games, but has not fully realized its potential in continuous domains. This work introduces TPO, a tree search based policy optimization method for continuous environments. TPO uses a hybrid approach to policy optimization, limiting tree search branching factor and defining a new loss function based on trajectories' mean and standard deviations. Surprisingly, bootstrapping tree search with a pre-trained policy led to high-quality results with fewer samples and simulations. In continuous environments, TPO improves policy optimization by reducing the need for a large branching factor and simulation count in MCTS. PPO is used as the baseline algorithm, with TPO achieving significant improvements in benchmarks like Humanoid. The evolution of model-free reinforcement learning is driven by advances in neural representation learning and algorithms capable of handling larger action spaces and longer time horizons. In continuous environments, TPO introduces Tree Search Policy Optimization for continuous action spaces, addressing the limitations of traditional tree search methods. It aims to improve policy optimization by reducing the computational complexity in building the search tree, especially in environments with unbounded action spaces. Tree Search Policy Optimization (TPO) is a method that addresses challenges in building and running simulations in environments with continuous action spaces. It involves training a policy using model-free RL methods, then using the pre-trained policy distribution to draw actions for tree construction. TPO utilizes tree search to generate high-quality trajectories for policy iteration. In Tree Search Policy Optimization (TPO), high-quality trajectories are generated using tree search as an expert. Monte Carlo Tree Search (MCTS) methods are used for tree search, but inferring the probability of selected actions for rollout is challenging in continuous domains. A new loss function is defined to update the pre-trained policy using tree search statistics. Proximal Policy Optimization (PPO) is chosen for policy optimization. Our approach in Tree Search Policy Optimization (TPO) involves training a target policy using policy gradient optimization and then iteratively building an MCTS tree to update the policy with roll-out trajectories. Despite the common need for a large number of branches and simulations in MCTS training, we found that pre-training the policy reduces the required number of simulations for generating high-quality trajectories. Additionally, exploring a greater number of branches showed diminishing returns after a small number, especially in higher dimensional action spaces. Tree Search Policy Optimization (TPO) integrates tree search into policy optimization for continuous action spaces, leading to superior performance compared to baseline techniques. TPO also introduces policy bootstrapping to improve sample efficiency and enable discretization. Performance plateaus after 32 simulations, highlighting the advantage of TPO in generating high-quality trajectories efficiently. TPO significantly improves sample efficiency by performing only 32 tree searches and narrowing down the number of tree expansions compared to other techniques. It samples 32 actions at each simulation step across all environments. Additionally, a distributed system was developed for policy optimization and data collection. Policy optimization and data collection are done on separate distributed platforms. TPO extends to challenging tasks like the Humanoid benchmark and significantly improves performance, achieving up to 2.5\u00d7 improvement. Policy iterations methods, like AlphaZero, use MCTS to train policy models. Recent work proposes theoretical extensions to AlphaZero's MCTS method. The recent work proposes theoretical extensions to AlphaZero's MCTS method for environments with continuous action spaces. It evaluates the method on the Pendulum OpenAI Gym environment but scalability to more complex environments is uncertain. TPO explores using MCTS as an expert for complex environments with continuous action spaces, falling into the category of policy iteration methods. On-policy and off-policy optimization methods differ in how the policy is optimized using samples. Reinforcement learning methods like TRPO, PPO, PGQL, and A3C are successful but sample inefficient, requiring fresh trajectories for each update. Off-policy methods like DQN and DDPG are more sample efficient but need extensive hyper-parameter tuning. Recent methods incorporate differentiable planning procedures in computation graphs, such as MCTSnets which uses simulation-based search with vector embedding for expansion, evaluation, and backup. The authors optimized three policies with a gradient-based approach, reporting results on Sokoban. SoRB combines planning with policy optimization for goal-reaching tasks. Continuous MCTS in deep reinforcement learning is unexplored. TPO uses a simple node expansion scheme without extensive tuning. In experiments, PPO was used for pre-training the policy in MCTS tree. The framework is adaptable to other RL algorithms like SAC or TD3. Policy optimization in continuous environments is modeled by MDP with states, actions, transition probabilities, and rewards. In this section, the adaptation of Monte Carlo Tree Search (MCTS) for environments with continuous action spaces and non-zero immediate rewards is highlighted. The discount factor \u03b3 is used to calculate the discounted cumulative expected reward. The neural network policy \u03c0\u03b8(a k |s t ) with trainable parameters \u03b8 and the value v\u03c6(s t ) from a neural network with parameters \u03c6 are utilized. The environment moves to a new state s t+1 and returns a bounded reward r(s t ,a k ) : S \u00d7A \u2192 [r min ,r max ] when action a k is taken at state s t. In this work, a variant of the Polynomial Upper Confident Tree (PUCT) algorithm is used in Tree-Structured Policy Optimization (TPO) for Monte Carlo Tree Search (MCTS). The Monte Carlo tree consists of nodes representing states of the environment and edges representing actions. Statistics such as the number of visits, cumulative action value, and average action value are stored for each edge. The MCTS simulation involves four steps: select, expand, evaluate, and backup. The select step utilizes a variant of the PUCT algorithm to choose actions for each node. The Tree-Structured Policy Optimization (TPO) for Monte Carlo Tree Search (MCTS) uses the PUCT algorithm to select actions. Dirichlet noise is added to promote exploration. In discrete environments, all possible actions are considered, while in continuous action spaces, samples are drawn from the policy distribution to address the challenge of innumerable actions. The proposed method involves drawing samples from the policy distribution to select actions in Monte Carlo Tree Search. After simulations, the action with the highest visit counts from the root node is chosen, with a tie-breaker based on Q-value and uncertainty. An unbiased estimate is used to optimize the policy distribution, and MCTS trajectories are stored in a replay buffer for network optimization. The neural network architecture includes fully-connected layers for both value and policy networks. The neural network architecture includes a multi-layer perceptron (MLP) with two hidden layers of 64 units. The value and policy networks have separate weights. The value network outputs a scalar value, while the policy network outputs means and variances for Normal distributions. PPO is used for on-policy optimization, with a policy loss function that penalizes large updates. The policy network uses a variant of PPO with clipping for value function updates. Dirichlet noise scaling parameter controls noise vector shape. Off-policy phase uses PPO value function for updates and a new loss function for policy updates derived from MCTS statistics. The proposed TPO algorithm combines policy gradient and MCTS. In the first phase, the policy network optimizes the PPO loss function. In the second phase, an off-policy mode uses trajectories from MCTS for updates. In the second phase of the TPO algorithm, off-policy mode is utilized with trajectories from MCTS to optimize the policy. The switching ratio \u03c1 between policy gradient and tree search is considered a hyperparameter. Starting with a lightly trained policy results in diminished performance, highlighting the importance of a stable policy for exploring trajectories with higher returns. The TPO algorithm combines training with tree search policy optimization on continuous environments from OpenAI Gym. Policy training is done on a TPU-v2 platform with eight cores, while MCTS is performed on CPU. PPO is chosen for on-policy optimization, running TPO for four million time steps. BatchEnv is used for efficient MCTS sampling, allowing up to 32 actions to be evaluated in parallel using 32 Intel CPUs. Using our parallel infrastructure, the latency of action selection is between 0.2-0.3 sec with 32 MCTS simulations per action. Results are compared with the baseline algorithm PPO, showing improved performance in some environments. Training is done for 10 epochs per policy update using Adam as the optimizer. Table 3 compares TPO and PPO performance in six environments with continuous action spaces. TPO consistently outperforms PPO, with the largest improvement seen in Ant-v2 and the smallest in Swimmer-v2. TPO leverages tree search exploration better in complex environments with large action spaces. Training progress in Figure 2 shows TPO and PPO across the environments, with TPO starting from pre-trained PPO checkpoints at around 2M time steps. At around 2M time steps, TPO resumes training from PPO checkpoints and incorporates tree search. The total return curve for both TPO and PPO align initially before a notable increase in performance with tree search. This improvement is particularly evident in complex environments like Humanoid-v2. Ablation studies are conducted to analyze the impact of key components of TPO, focusing on the switching ratio in different environments with varying action space complexities. Switching ratio impact on policy performance in tree search optimization is studied across three tasks. Prematurely switching to tree search harms policy performance, while switching at the very end of training can also be damaging. Switching at 90% of total training steps slightly improves average return compared to switching after 50% of total training steps. Switching ratio impact on policy performance in tree search optimization is studied across three tasks. Prematurely switching to tree search harms policy performance, while switching at the very end of training can also be damaging. Adjusting the point at which to switch to tree search is crucial for better policy performance. Changing the number of actions per node has a significant benefit for policy performance, with an increase from eight to 32 actions per node resulting in a \u223c1100 increase in average total return. Increasing the number of MCTS simulations generally improves performance, but increasing the branching factor (number of actions per node) has a more significant impact on policy performance. In this paper, Monte Carlo Tree Search in continuous space is studied for improving the performance of a baseline on-policy algorithm. Results show that MCTS policy optimization can enhance the quality of policy in selecting better actions during evaluation, requiring more samples during MCTS rollout. Pretraining the policy enables high performance with a low MCTS branching factor and few simulations, while without pretraining, a larger branching factor and simulation count are needed, making MCTS computationally infeasible. Future research directions include exploring techniques to improve sample efficiency and eliminate the need for a reset-able environment in Monte Carlo Tree Search (MCTS). One approach is to use a trained model of the environment, similar to model-based reinforcement learning methods, instead of directly interacting with the environment in MCTS. Recent advancements in model-based policy optimization (MBPO) have shown promising results in training accurate environment models for MCTS simulations. This can be particularly beneficial for shallow MCTS simulations like those employed in TPO, which assumes access to an environment that can be restarted from any state. In physical RL problems like Robotics, achieving sample efficiency may be challenging. Using a modeled environment in MCTS simulations can help relax this assumption."
}