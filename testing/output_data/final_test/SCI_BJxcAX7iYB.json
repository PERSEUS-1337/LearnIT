{
    "title": "BJxcAX7iYB",
    "content": "In learning to rank, the goal is to optimize the global ordering of items based on user utility. Different approaches involve scoring items individually without considering other items in the list. A new context-aware neural network model is proposed in this paper, utilizing a self-attention mechanism to determine item relevance in the context of all other items. This model shows significant performance improvements over traditional methods. The study demonstrates significant performance gains of self-attention based neural architecture in learning to rank, which is crucial in information retrieval systems. This approach outperforms Multi-Layer Perceptron baselines across various types of losses and datasets. The function evaluates IR metrics like MRR, NDCG, and MAP to determine relative preference among items in a ranking algorithm. Scoring items individually is a proxy for learning to rank. In learning to rank algorithms, inter-item dependencies are modeled at the loss level to account for the effect of interactions on evaluation metrics. Pairwise and listwise objectives are commonly used to capture these dependencies, unlike pointwise objectives. The proposed approach in this work is a learnable, context-aware, self-attention mechanism. In this work, a learnable, context-aware, self-attention based scoring function is proposed to model interitem dependencies in ranking tasks. The Transformer architecture is adapted for this purpose, showing improved performance over MLP baselines across various ranking losses on the MSLR-WEB30K dataset. The paper introduces a self-attentive context-aware ranker for learning to rank, utilizing a benchmark LTR dataset and clickthrough data from Allegro.pl. An open-source Pytorch implementation of the model is available. The paper is structured with sections on related work, problem formulation, model description, experimental results, hyperparameter ablation study, and a summary of the work. The majority of LTR methods focus on scoring functions. The majority of LTR methods focus on scoring functions that score items individually. Previous attempts at modeling inter-item dependencies include a pairwise scoring function and Groupwise Scoring Function (GSF). The GSF method concatenates feature vectors of multiple items and uses Monte-Carlo sampling to desensitize the model to the order of items. Another approach, the seq2slate model, uses an RNN and Pointer Networks to encode items in a context-aware fashion and select items one-by-one to produce the optimal list. Authors propose different approaches for re-ranking a list of items. One approach uses an RNN to encode items for re-ranking, followed by a decoding step with attention. Another approach adds delta features to capture inter-item dependencies. A variant of Conditional Variational Auto-Encoder is introduced to learn the joint distribution of items in a list and generate a ranked list based on users' feedback. The authors propose a context-aware ranking solution using the self-attention mechanism, which is a special case of the Transformer encoder. They compare their approach with previous methods like GSF and DLCM on the WEB30K dataset, outperforming both in terms of NDCG@5. In the learning to rank setting, the goal is to find a scoring function f that maximizes an IR metric like NDCG on the test set. The scoring function is trained to minimize a surrogate loss l over the training data, with controls for overfitting. Key choices in proposing a learning to rank algorithm include selecting a scoring function f and a loss function l. The learning to rank algorithm involves a scoring function f and loss function l. The context-aware scoring function f models interactions between items in a list x. The self-attention based ranking model modifies the Transformer architecture to consider all items in the list when scoring a single item. The key component is the self-attention mechanism. The self-attention mechanism, introduced in [27], computes a new representation for each item in a list by taking a weighted sum over all items based on their relevance to a query item. This is achieved using Scaled Dot-Product Attention, where query, key, and value vectors are the same. Our model utilizes Scaled Dot-Product Attention with multiple attention heads, each performing linear projections of query, key, and value matrices before concatenating and projecting the outputs. This approach is similar to the Transformer architecture designed for neural machine translation tasks. The Transformer architecture was designed for neural machine translation tasks, utilizing Scaled Dot-Product Attention with multiple attention heads. The authors proposed using positional encodings to address the order of input tokens in NMT. Experimental results in ranking and re-ranking settings showed that positional encodings may boost model performance. The model was adapted to the ranking setting by treating items on a list as tokens with item features as input token embeddings. The curr_chunk discusses the application of the Transformer architecture to ranking tasks, where items on a list are treated as tokens with input token embeddings. The model includes N encoder blocks, multi-head attention layers, skip connections, layer normalization, and a fully-connected layer to compute scores for each item. The model can be viewed as an encoder part of the Transformer with additional linear projection on the input. The Transformer architecture is applied to ranking tasks, treating list items as tokens with input embeddings. The model includes N encoder blocks, multi-head attention layers, skip connections, layer normalization, and a fully-connected layer to compute item scores. Self-attention in the encoder ensures all items are considered in score computation. Scores and ground truth labels can be used with any differentiable ranking loss for optimization. Two types of learning to rank datasets exist: multi-level relevance labels and binary labels from clickthrough logs. The model is evaluated on both types of data. The context-aware ranker is evaluated on the WEB30K dataset, containing 30,000 queries with search results. Each result has a relevance label from 0 to 4. Features are standardized before inputting into a learning algorithm. The dataset is partitioned into five folds for 5-fold cross-validation. Lists are padded or subsampled for training, while full length is used for validation and testing. The context-aware ranker is evaluated on the Allegro.pl clickthrough logs dataset, which consists of lists with one relevant item and multiple irrelevant items. Each item is represented by a 45-dimensional vector. The dataset is not cross-validated but split into train, validation, and test sets for evaluation of the context-aware ranking model. The proposed context-aware ranking model is evaluated using popular ranking losses such as RMSE, ordinal loss, NDCGLoss 2++, RankNet, LambdaRank, ListNet, and ListMLE. These losses are used to evaluate the performance of the model on the Allegro.pl clickthrough logs dataset, which is split into train, validation, and test sets. The RMSE loss is used as a baseline, where no interaction between items is considered. The self-attentive scoring function in the proposed ranking model uses sigmoid activation and rescales outputs. Each neuron predicts a relevancy level, maintaining relative order. The final loss is the mean of binary cross-entropy losses. During inference, output neurons are summed for the final score. NDCGLoss2++ is used for evaluation. The RankNet loss function is derived by removing the exponent in the l(s, y) formula, with a fixed parameter \u00b5 set to 10.0. Different weighted RankNet variants can be obtained by changing the formula in the exponent. LambdaRank formula is obtained by replacing the exponent with ListNet loss. ListMLE is given by a specific formula. Context-aware ranking models and MLP models are trained on datasets using various loss functions, including XGBoost models. In Section 5.2, various loss functions are used to train XGBoost models for e-commerce search engine. Hyperparameters are tuned on validation set, and Adam optimizer is used for neural network models. An ablation study on hyperparameters is provided in Section 6. Performance is evaluated on WEB30K using NDCG@5 metric. Results are shown in Table 1. The proposed self-attention based model shows significant performance improvement over MLP baseline across different loss functions on e-commerce search logs. Incorporating context-awareness into the model architecture has a more pronounced effect on performance than varying the underlying loss function. Ordinal loss outperforms established losses like ListNet on multi-level relevancy data, and models trained with RMSE loss also show good performance. The study compares model performance trained with RMSE loss to models optimized with RankNet and ListMLE. Results on WEB30K are provided for comparison with the current state-of-the-art. Experiments on positional encoding effects were conducted on WEB30K using XGBoost models trained on sorted datasets. The study compared model performance trained with RMSE loss to models optimized with RankNet and ListMLE on WEB30K dataset. Experiments on positional encoding effects were conducted using XGBoost models trained on sorted datasets, showing improved performance with fixed positional encoding. An ablation study was performed on a context-aware ranker with ordinal loss, varying hyperparameters such as number of encoder blocks, attention heads, dropout rate, and hidden dimension size. High dropout values were found essential to prevent overfitting. In this work, the study addressed constructing a context-aware scoring function for learning to rank. They adapted the Transformer architecture to propose a new scoring function for LTR, showing performance gains over MLP baselines. High dropout values were crucial to prevent overfitting, and increasing hidden dimensions improved performance but led to longer training times. Stacking multiple encoder blocks also enhanced performance. The proposed neural architecture showed significant performance gains over MLP baselines in ranking and re-ranking settings. Models trained with ordinal loss function outperformed those trained with LambdaLoss or LambdaMART. However, models trained with RankNet and ListMLE losses performed poorly. Future work will investigate the reasons behind these results and the relationship between ordinal loss and NDCG. In Table 1, hyperparameters for models trained on WEB30K and e-commerce search logs are detailed. Table 6 explains the columns' meanings, including dimensions, encoder blocks, attention heads, learning rate, dropout probability, and list length. Table 7 specifies hidden dimensions in MLP models."
}