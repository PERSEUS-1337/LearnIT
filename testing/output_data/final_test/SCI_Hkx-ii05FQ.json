{
    "title": "Hkx-ii05FQ",
    "content": "Combinatorial optimization is a common theme in computer science, with NP-Hard problems often having locally optimal solutions. However, defining solution neighborhoods can be challenging in some cases. To address this, a policy gradient algorithm is proposed to transform the problem to the continuous domain and optimize a new surrogate objective. This approach eliminates the need for fine-tuning hyper-parameters on a case by case basis. The method is tested on finding locally maximal cliques as an experimental benchmark. In a challenging experimental benchmark, the problem of finding locally maximal cliques is used to test clique finding algorithms. Fixing the distribution of the surrogate is crucial for consistently recovering optimal solutions. The surrogate objective leads to an algorithm that outperforms other methods in various measures. Combinatorial optimization is foundational in computer science, with locally optimal solutions often being practical. Local search methods like k-means are commonly used in clustering to minimize distances between group members. In combinatorial settings, solution neighborhoods may not always be available, limiting the connectivity within the search space. For instance, in the context of computer programs, the impact of replacing operations on program behavior is unclear. Defined neighboring solutions may only connect a small fraction of the search space, posing challenges in optimization. In combinatorial problems, solution neighborhoods may not always be available, limiting connectivity within the search space. Policy gradient methods are suitable for such scenarios, constructing parametric sampling distributions over the solution space. Policy gradient methods construct parametric sampling distributions over the search space to optimize objective functions through gradient updates. The challenge lies in tuning step sizes due to the direct dependence on sampled objective values. To extend these methods to any objective, the construction aims to only be sensitive to the order relation induced by the objective. The construction of a generic surrogate objective function aims to provide a fixed and predetermined distribution for every possible objective, allowing for the development of generic rules for setting hyper-parameters in a stochastic optimizer. The construction of a generic surrogate objective function aims to provide a fixed distribution for any objective, enabling the setting of hyper-parameters in a stochastic optimizer. By utilizing the empirical cumulative distribution function (CDF) of the original objective, various surrogate objectives can be easily constructed. The CAkEWaLK method, based on the CDF, is discussed as a way to optimize combinatorial problems using policy gradient methods. The method is related to the cross-entropy method, policy-gradient methods in reinforcement learning, and multi-arm bandit algorithms. In section 6, methods are applied to the clique problem, with experimental results reported in section 7. Additionally, Cakewalk is used in appendix section B to improve algorithms for k-medoids clustering. This demonstrates Cakewalk's effectiveness in optimizing greedy algorithms and providing empirical evidence of its versatility. In constructing a stochastic optimization algorithm for combinatorial problems, the objective is to maximize an objective function f over a set of items. The problem involves searching for an optimal solution within a discrete space, focusing on locally optimal solutions due to the NP-Hard nature of the problem. A neighborhood function is used to define locally optimal solutions, treating the objective function as a black-box. The text describes a stochastic optimization algorithm for finding locally optimal solutions in combinatorial problems. It focuses on maximizing an objective function f over a set of items, using a neighborhood function to define locally optimal solutions. The algorithm maintains random variables X and Y, distributed according to a parametric distribution P \u03b8. The algorithm iteratively samples solutions x according to P \u03b8 and updates parameters \u03b8 to reflect the objective value y = f(x). The distribution P \u03b8 becomes less uniform over time, leading to locally optimal solutions. The learning objective is to maximize J(\u03b8). The learning objective is to maximize J(\u03b8) by deriving a gradient ascent algorithm using the log-derivative trick. Monte Carlo estimation is used to calculate the gradient, and a fixed size K is sampled using P \u03b8t. The update at iteration t is determined by a learning rate parameter \u03b7 t. Positive learning rates lead to convergence to a local maximum of J using a stochastic optimization scheme. When using stochastic optimization with fixed learning rates, the goal is to converge to a local maximum of J by sampling from P \u03b8 * and obtaining optimal solutions x * \u2208 X * f. However, gradient estimates can be highly variable, requiring large samples and costly iterations. Techniques exist to reduce variance in estimates, but we propose adapting the optimization objective to rely on noisy gradient estimates involving a single example (K = 1) while still converging to a distribution allowing sampling of x * \u2208 X * f. When using stochastic optimization with fixed learning rates, the goal is to converge to a local maximum of J by sampling from P \u03b8 * and obtaining optimal solutions x * \u2208 X * f. Gradient estimates can be highly variable, requiring large samples and costly iterations. Adapting the optimization objective to rely on noisy gradient estimates involving a single example (K = 1) can reduce variance in estimates while still converging to a distribution allowing sampling of x * \u2208 X * f. The distributions of {\u03b7 t Y t } T t=1 determine the course of the optimization, with the sign and magnitude of \u03b7 t y t determining the likelihood of x t. If |\u03b7 t Y t | is unbounded from above, steps that are too large may cause divergence, while steps that are too small keep the sampling distribution too close to uniform, making finding good xs exponentially difficult. When optimizing with fixed learning rates, the distribution of {\u03b7 t Y t } T t=1 determines the optimization course. Adjusting learning rates for specific objectives may require tuning optimization case by case. Generic updates may not fit all scenarios due to unknown distributions of Y t. To achieve generic updates, a different approach is needed. To achieve generic updates, a fixed surrogate objective function preserving X * f is needed. Constructing w using the CDF F t of Y t can fix the distribution of optimal solutions. Estimating F t allows for the production of a surrogate objective with a fixed distribution. Utilizing a monotonic increasing function g for surrogate objective, estimating F t from data using order statistics, and ensuring bounded sampling distribution for estimation without large sample drawing. Utilizing a bounded sampling distribution allows for control over parameter differences between iterations, enabling small updates with small learning rates. This approach suggests using a fixed learning rate and a small value for k. The first option presented connects the algorithm to the CE method using a weight function \u0175 CE (y) = g CE F (y) with a thresholding function g CE (z) = I [z \u2265 1 \u2212 \u03c1]. This update can be seen as an online version of the CE method but has disadvantages such as manual tuning of parameter \u03c1 and only using the highest \u03c1 percentile of examples for updating P \u03b8. The algorithm suggests using weight functions to address issues with the worst examples providing valuable information. One option is to use the empirical CDFF directly, making F(Y) uniform discrete on [0, 1]. However, this can lead to a bias towards examples that have already been sampled. To counteract this bias, F is adjusted to only increase the likelihood of half of the examples. To address bias towards already sampled examples, the algorithm adjusts F to increase the likelihood of only half of the examples. This is achieved by making \u0175(Y) = 2F(Y) - 1, ensuring uniform discrete distribution on [-1, 1]. The method allows for determining the distribution of \u0175(Y) and avoids divergence risks, unlike transforming Y with z-scores. The approach is summarized in algorithm 1 with weight function \u0175 and gradient addition rule Add. Our method, Cakewalk, addresses the limitations of the CE method by introducing a different surrogate objective. Unlike CE, Cakewalk is an online algorithm that does not require drawing a large sample in each iteration. This allows us to rely on bounded gradient for optimization. Cakewalk is an online algorithm that estimates the CDF with bounded gradient updates, making it less computationally expensive than CE. It is related to policy gradient methods like REINFORCE, providing convergence guarantees. Actor critic methods use estimates of the objective to make the difference between y and \u03bc zero mean, but they are problem-specific. The natural actor-critic algorithm BID21 rescales the estimated gradient by multiplying it by the inverse of the Fisher information matrix, making it computationally expensive. It is related to multi-arm bandit algorithms like UCB and Exp3 for sequential decision problems with non-deterministic losses. The UCB algorithm Auer (2002) and other bandit algorithms like BID2 and Exp3 are used for cases with stochastic losses. These algorithms have been extended to high dimensional structured arms by McMahan & Blum. The key difference between bandit and optimization settings is that bandit settings involve non-deterministic losses, requiring a balance between estimating arm statistics and exploiting gathered information. In optimization settings, the goal is to find the best deterministic solution with the least number of steps. The Cakewalk update rule is not tied to any specific sampling distribution. A simple distribution is used as an example, factorizing into independent distributions over different dimensions. This reduces the number of parameters needed to represent the distribution linearly with M N. The softmax distribution is used to draw x j independently. The gradient of log P \u03b8 (x) is described in terms of partial derivatives, allowing for online estimation. This sets the groundwork for further investigation. In this section, we investigate whether algorithms relying solely on function evaluations can recover locally optimal solutions for NP-hard problems like finding locally maximal cliques in graphs. The focus is on recovering non-trivial optima in challenging scenarios without competing with algorithms using neighborhood functions. In this section, the focus is on designing an objective function, the soft-clique-size function, to inform algorithms that rely on function evaluations about the density of subgraphs in an undirected graph. The function favors larger subgraphs and aims to identify inclusion maximal cliques. The space X consists of strings determining membership in subgraphs, denoted as Ux. The function calculates subgraph density and aims to recover locally optimal solutions for NP-hard problems like finding locally maximal cliques in graphs. The objective function, the soft-clique-size function, introduces a parameter \u03ba to reward larger subgraphs. It adjusts the denominator to avoid division by zero and uses max(\u00b7, 1) for cases where |U x | < 2. Increasing \u03ba gives larger subgraphs a 'boost' compared to smaller ones, favoring larger cliques. The study tested various algorithms on 80 undirected graphs to find cliques, using different values of \u03ba to maximize the soft-clique-size function. Each graph had up to 4000 nodes and varying edge density. The algorithms were tested with values of \u03ba ranging from 0.0 to 1.0 to determine inclusion maximal cliques. In the study, various algorithms were tested on 80 undirected graphs to find cliques by maximizing the soft-clique-size function with values of \u03ba ranging from 0.0 to 1.0. The methods tested included CE, three versions of REINFORCE, and Exp3 bandit algorithms. The CE method used online versions with threshold values of \u03c1 = 0.1 and \u03c1 = 0.01, referred to as OCE 0.1 and OCE 0.01. The three versions of REINFORCE were vanilla, with mean subtraction from y as a baseline, and using the objective's estimated z-score y\u2212\u03bc \u03c3, referred to as REINF, REINF B, and REINF Z. The study tested 8 optimization methods on 80 graphs using 3 update steps and 11 values of \u03ba, totaling 21120 separate executions. Gradient update methods included stochastic gradient ascent, AdaGrad, and Adam updates, which are scale invariant and could help handle changes in the objective's scale. The methods used for estimating \u03bc, \u03c3, and F included REINF, REINF B, and REINF Z, which transform objective values without fixing its distribution like CE and Cakewalk. The study tested 8 optimization methods on 80 graphs using 3 update steps and 11 values of \u03ba, totaling 21120 separate executions. The experimental details are specified in the appendix. Performance measures were analyzed for each optimizer and gradient update type, with results reported in tables. Local optimality and inclusion maximality of the returned solutions were examined. The study tested 8 optimization methods on 80 graphs, analyzing inclusion maximality and sampling efficiency. Results were reported in tables, comparing solutions to known optimal values. The study compared different optimization methods on 80 graphs, analyzing inclusion maximality and sampling efficiency. Multiple hypothesis tests were performed to compare optimizers, with a significance threshold set at 10^-2. Results showed that using a fixed surrogate objective significantly improved the rate of recovering locally optimal solutions, with CW\u0175 and OCE 0.1 outperforming other methods. Uniform on [\u22121, 1] distribution is favored over others tested. CW\u0175 outperforms OCE 0.1 in terms of sample efficiency and speed in finding optimal solutions. CW\u0175 with AdaGrad shows the best performance among various gradient updates. The results show that Cakewalk is a highly effective optimization method, approaching the performance of problem-specific algorithms. It was tested on 80 undirected graphs from the DIMACS challenge BID13, a standard benchmark for clique finding algorithms. The recovered solutions were non-trivial, indicating Cakewalk's potential in other domains like continuous non-convex optimization and reinforcement learning. Exp3 was chosen over UCB for comparison in the bandits family of algorithms due to the multi-dimensionality of the problem. It was applied independently to each of the N elements, with bounded losses/gains assumption met by the soft-clique-size function. Gradient updates were done using SGA, AdaGrad, and Adam, with AdaGrad suited for indicator data and Adam effective for training neural networks. In conjunction with AdaGrad and Adam, Exp3 was experimented with despite losing its theoretical guarantees. AdaGrad was applied with \u03b4 = 10 \u22126, and Adam with \u03b2 1 = 0.9, \u03b2 2 = 0.999, = 10 \u22126, using a fixed learning rate of 0.01. The study focused on applying Cakewalk to the k-medoids problem, a combinatorial version of k-means clustering. The goal is to minimize distances to k representatives by dividing m data points into k clusters. In k-medoids, representatives are a subset of original points given in advance. The problem involves selecting k representatives from m data points and assigning each point to the closest representative based on a distance matrix. The optimization problem quickly becomes intractable due to its combinatorial nature, leading to the use of greedy algorithms. In k-medoids, representatives are selected from data points and assigned based on distance. Greedy algorithms like Voronoi iteration and PAM are commonly used for this. PAM achieves lower objective values despite being more computationally expensive. Greedy algorithms like Voronoi iteration and PAM are commonly used in k-medoids to select representatives based on distance. PAM achieves lower objective values, and the convergence of these methods is determined by their initialization. By optimizing a function g \u2022 f with Cakewalk, we can find a good initialization for greedy algorithms. The key detail is to return f(x*) instead of x* for the optimal objective value y* = g(f(x*)). This can be implemented by keeping f(x*) or applying f to the x* returned by Cakewalk. To test the effectiveness of optimizers on the k-medoids problem, 38 datasets with 500-1000 data points were collected. Numerical attributes were extracted from each dataset to create numerical vectors representing data points. Mahalanobis distance was calculated between each pair of points using diagonal covariance matrices, resulting in a distance matrix for each dataset. After calculating Mahalanobis distances with diagonal covariance matrices, various algorithms including Voronoi iteration, PAM, and vanilla Cakewalk were applied to the datasets. Cakewalk was also combined with a greedy method, specifically with the Voronoi iteration for improved efficiency. Hyper-parameters were adjusted for Cakewalk with AdaGrad BID10, with a higher learning rate of 0.02 chosen for faster convergence. These settings were used for both Cakewalk alone and the Cakewalk-Voronoi combination. The study implemented four clustering algorithms, including the Cakewalk-Voronoi combination. Convergence was determined by comparing two exponentially running averages of objective values, with a threshold of 0.01. Parameters were adjusted based on the dataset size, and the algorithms were implemented in Julia. The algorithms were benchmarked on datasets with k=10, recording the smallest objective value and number of function evaluations. The study compared four clustering algorithms based on their objective values and number of function evaluations. Cakewalk, PAM, and Cakewalk+Voronoi fully evaluated the objective in each step, while Voronoi iteration did not. The goal was to rank the algorithms based on their performance, considering both objective values and total function evaluations. The study compared four clustering algorithms based on their performance using objective values and function evaluations. The algorithms were ranked by calculating ratios and medians of scaled measurements. A one-sided sign test was used to validate the differences between algorithms, and a significance threshold was determined to control the false discovery rate. The significance threshold to control the false discovery rate was set at 10 \u22122 using the Benjamini-Hochberg method. The best to worst algorithms in terms of objective value were Cakewalk+Voronoi < PAM < Cakewalk < Voronoi, with statistically significant differences. Combining Cakewalk with a greedy algorithm outperformed commonly used algorithms for the k-medoids problem. The study found that combining Cakewalk with PAM could potentially create a better clustering method. Additionally, vanilla Cakewalk outperformed the Voronoi iteration, suggesting that it can surpass some greedy algorithms. Both PAM and Cakewalk had similar numbers of function evaluations, which were higher than the combination of Cakewalk+Voronoi. Combining Cakewalk with a greedy method produces an optimizer that outperforms its components and converges faster."
}