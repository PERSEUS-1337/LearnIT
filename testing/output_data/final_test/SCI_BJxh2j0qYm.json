{
    "title": "BJxh2j0qYm",
    "content": "In this paper, a new method called feature boosting and suppression (FBS) is proposed to reduce the computational and memory resources required for deep convolutional neural networks. FBS dynamically amplifies important convolutional channels and skips unimportant ones at run-time by introducing auxiliary connections to existing layers. Unlike channel pruning methods, FBS preserves the full network structure and accelerates convolution. The FBS-augmented networks show significant improvements on ImageNet compared to existing channel pruning and dynamic execution schemes. Feature boosting and suppression (FBS) is a new method proposed to reduce computational and memory resources for deep convolutional neural networks. FBS can provide significant savings in compute on models like VGG-16 and ResNet-18 without sacrificing accuracy. CNNs have evolved to become more efficient but still require large computational resources, making FBS a valuable approach for cost-sensitive cloud services and low-powered edge computing applications. Channel pruning is a method to reduce memory, bandwidth, and compute costs by removing unimportant channels from over-parameterized CNNs. However, this approach has drawbacks as it can permanently reduce the capabilities of CNNs, impact accuracy, and the saliency of a neuron is not static. A dynamic channel pruning strategy allows the network to prioritize certain convolutional channels based on the current input, rather than statically pruning based on importance. This approach aims to accelerate convolution by computing only a subset of channels predicted to be important at runtime. Feature boosting and suppression (FBS) is proposed as a dynamic channel pruning strategy to amplify and suppress output channels in a convolutional layer based on importance at runtime. This approach aims to save computational resources by selectively computing only important channels while preserving all neurons of the full model for minimal impact on task accuracy. Feature boosting and suppression (FBS) is a dynamic channel pruning strategy that predicts output channel saliency using previous layer features. It introduces minimal overhead to existing models compared to potential speed-ups from dynamic sparsity. Unlike other dynamic computation strategies, FBS can adapt with conventional SGD without resorting to computationally expensive reinforcement learning. Feature boosting and suppression (FBS) is a dynamic channel pruning strategy that predicts output channel saliency using previous layer features. It introduces minimal overhead to existing models compared to potential speed-ups from dynamic sparsity. FBS can produce models with validation accuracies surpassing other channel pruning and dynamic conditional execution methods. The outputs from certain channel neurons may vary drastically, with top rows greatly exciting neurons in specific channels. The distribution of maximum activations in the first 20 channels is shown. Recent literature has focused on creating more compact and efficient CNNs by pruning connections or neurons. Early methods zero out individual weight parameters, resulting in irregular sparse connections. Custom accelerator solutions have been developed to exploit sparse weights, but supporting both sparse and dense convolutions efficiently can lead to compromises. Structured sparsity, such as group Lasso on channel weights, has been introduced to allow GPUs and custom accelerators to focus on efficient dense operations. Alvarez & Salzmann (2016) and BID12 proposed methods to prune channels in CNNs by setting unimportant channels to zero and fine-tuning the network. BID25 used Lasso regularization to induce sparsity in channel weights, while BID37 learned to limit the number of active convolutional channels while minimizing reconstruction error. These methods aim to reduce the magnitude of channel weights during training and remove connections from zeroed-out channels. Several methods have been proposed to induce sparsity in CNNs, including using global thresholds, iterative algorithms, greedy algorithms, reinforcement learning, and model-agnostic masks. Structured sparsity methods result in permanently lost capabilities of pruned neurons and connections, leading to the suggestion of using dynamic networks as an alternative. Dynamic networks can choose parts of the network to evaluate during inference, especially in spatially sparse convolutional layers. Recent publications have introduced methods to accelerate convolutional neural networks by inducing spatial sparsity. For example, BID4 introduced collaborative layers to create spatial sparsity in cheap convolutions, while BID5 proposed spatially adaptive computation time for residual networks. BID0 presented dynamic capacity networks that select salient locations in input images for refinement based on entropy gradients. BID30 accelerated convolutional layers by computing non-sparse regions in input images. Additionally, there are dynamic networks that make binary decisions or multiple choices for inference paths, such as BlockDrop which skips blocks in residual networks. BlockDrop, BID24, BID28, and other dynamic networks train policy functions using reinforcement learning to make decisions such as skipping blocks, selecting modules, and pruning channels in neural networks. Some methods, like sparsely-gated mixture-of-experts and differentiable policy functions, aim to overcome limitations of non-differentiable functions in these networks. Differentiable policy functions are used to implement early exits in a DNN. BID15 learns binary policies for deciding the use of input channels in convolution, approximating gradients of non-differentiable policies with continuous ones. FBS accelerates convolutional layers with batch normalization by predicting output channel importance based on input features, allowing for skipping of inactive channels to speed up computation. The curr_chunk explains the components of a deep sequential batch-normalized model with multiple channels of features. It includes details on layer definitions, ReLU activation, trainable parameters, normalization, and convolution operations. The curr_chunk discusses optimizing convolution operations in a deep sequential batch-normalized model with multiple channels of features. It introduces a generalization of a layer with dynamic execution and explores sparsifying and accelerating the layer by adding a Lasso on certain parameters to the total loss. Despite the simplicity of the formulation, designing the parameters properly can be challenging. The curr_chunk discusses reparameterizing convolution operations in deep learning models to optimize performance. It highlights challenges in adjusting parameters to improve efficiency and points out limitations in current training algorithms and channel pruning methods. The curr_chunk introduces a method called FBS that dynamically prunes channels in deep learning models to improve efficiency. It proposes replacing layer definitions with dynamic channel pruning based on a parametric function \u03c0(x l\u22121) dependent on the previous layer's output. This approach aims to address the limitations of current channel pruning methods. The method FBS introduces dynamic channel pruning in deep learning models by using a low-overhead policy to evaluate pruning decisions for computationally demanding convolutions. It prunes the least salient output channels from the layer, allowing for a trade-off between performance and accuracy. Dynamic channel pruning in deep learning models introduces a low-overhead policy to evaluate pruning decisions for convolutions, allowing for a trade-off between performance and accuracy. Pruned channels with all-zero values enable subsequent layers to exploit input-side sparsity, resulting in a quadratic speed-up with respect to the pruning ratio. Batch normalizing the convolution output ensures efficiency in training without introducing reparameterization. Many alternative methods use nondifferentiable functions for on/off decisions, making them incompatible with SGD and resorting to reinforcement learning for training. The section explains the design of the channel saliency predictor g l (x l\u22121) by subsampling x l\u22121 to a scalar using function ss to reduce computational cost. The use of functions like wta promotes local competition between subnetworks, making networks easier and faster to train for complex tasks. The design of the channel saliency predictor g l (x l\u22121) involves subsampling x l\u22121 to a scalar using function ss for computational efficiency. Extensive experiments were conducted on CIFAR-10 and ImageNet ILSVRC2012 datasets using the custom 8-layer CNN model MCifarNet, achieving high accuracies with significantly fewer parameters compared to VGG-16. Our CNN is more challenging to accelerate due to its compactness. We compare FBS with NS under various speedup constraints by faithfully reimplementing Network Slimming (NS) BID25. We augment ResNet-18 BID11 and VGG-16 BID33 for ILSVRC2012, providing a detailed accuracy/MACs trade-off comparison against recent pruning and execution methods. Our method replaces convolutional layer computations with FORMULA7, fine-tunes the network without suppressing any channel computations, and gradually reduces the overall density to sweep the accuracy/performance trade-off. By applying NS and FBS to a CIFAR-10 classifier with increasing sparsity, a trade-off relationship between MACs and classification accuracy is shown. FBS outperforms NS in maintaining accuracy within a computational budget. Combining NS and FBS effectively regains lost accuracy, closely matching FBS's trade-off curve. The FBS, NS, and NS+FBS models achieve different speed-up ratios while maintaining accuracies above certain thresholds. The FBS model effectively learns to amplify and suppress channels based on input images, as shown in heat maps. The M-CifarNet model using FBS with d = 0.5 achieves a top-1 accuracy of 90.59%. The M-CifarNet model with FBS achieves a top-1 accuracy of 90.59% and is approximately 4\u00d7 faster than the original model. FBS naturally incorporates channel pruning strategies like NS, adjusting dynamicity across layers to prune different ratios of channels. Applying FBS to ResNet-18 consistently outperforms NS in ILSVRC2012 validation accuracy under various speed-up constraints. At d = 0.7, FBS uses 1.12 G MACs for a top-1 error rate of 31.54%, outperforming NS. FBS shows the highest speed-up and lowest error rates compared to other dynamic execution methods. FBS improves accuracy by 1.73% in top-1 and 0.46% in top-5 accuracies when applied to baseline networks. Comparisons of structured pruning and dynamic execution methods are made for VGG-16 in TAB3. Structured pruning and dynamic execution methods for VGG-16 in FBS show minimal error increases at speed-ups of 3.01\u00d7 and 5.23\u00d7. FBS reduces MACs, bandwidth, and memory requirements significantly, leading to power savings and improved cache utilization in various application scenarios. Feature Boosting and Suppression (FBS) is a method proposed to reduce compute requirements in CNNs while maintaining high accuracies. It helps achieve significant reductions in memory accesses and peak memory usage, preserving the capabilities of CNNs by predictively boosting important channels. Feature Boosting and Suppression (FBS) is a method that reduces compute requirements in CNNs while maintaining high accuracies. It achieves significant savings in computation on models like ResNet-18 and VGG-16 with minimal loss of accuracy. FBS outperforms other pruning and dynamic execution methods, can accelerate popular CNN networks, and has an open-source implementation. The architecture of M-CifarNet uses 3x3 kernels for all convolutional layers and includes a global average pooling layer. Training involved a learning rate of 0.01, a batch size of 256, and reducing the learning rate by a factor of 10 for every 100 epochs. To compare FBS against NS, models were initialized consecutively with a new MACs budget and trained for a maximum of 300 epochs. NS utilized 1-norm sparsity regularization weighted by 10^-5 on BN scaling factors, pruning at 150 epochs, and fine-tuning the network without sparsity regularization. Image augmentation procedures were also employed for preprocessing. The network structure of M-CifarNet for CIFAR-10 classification includes image augmentation procedures and comparisons of layer-wise compute costs between FBS, NS, and their composition. The models generated by the three methods maintain a classification accuracy of at least 90.5%. FIG4 shows the skipping probabilities heat maps. The skipping probabilities heat maps of the convolutional layer conv4 evolve as we fine-tune FBS-augmented M-CifarNet. The network was trained for 12 epochs, and the channel skipping probabilities become more pronounced as we train the network. ResNet-18 and VGG-16 were trained with a similar procedure, with VGG-16 being computationally intensive with over 15 G MACs. After reducing computational and memory requirements on VGG-16 with NS, a smaller network with 20% of channels pruned was achieved. The resulting network has 7.90 G MACs remaining, making it less redundant. FBS was then applied to the compressed network, and for residual networks like ResNet-18, FBS was directly applied to all convolutional layers. The feature summation in the (b + 1) th block was handled differently due to the sparse input. The (b + 1) th block processes the sum of sparse channels from two features, treating a channel as sparse when both features have sparse channels simultaneously."
}