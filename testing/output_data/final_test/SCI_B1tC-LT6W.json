{
    "title": "B1tC-LT6W",
    "content": "We propose new techniques for compressing and speeding up dense matrix multiplications in neural networks for large vocabulary continuous speech recognition. Our method involves trace norm regularization for training low rank factored versions of matrix multiplications, leading to good accuracy versus parameter trade-offs. Additionally, we have developed open sourced kernels optimized for ARM processors, resulting in 3x to 7x speed ups over existing libraries. These techniques are expected to be applicable to embedded neural networks with large fully connected or recurrent layers. In this paper, the focus is on dense matrix multiplications in neural networks for speech recognition. The main contributions are trace norm regularization technique for reducing parameters and speeding up models for embedded applications. The paper introduces a trace norm regularization technique for training models with competitive accuracy and fewer parameters. It also discusses optimizing for low batch sizes in on-device inference and introduces kernels for ARM processors that outperform publicly available ones. The insights presented are broadly applicable to other deep learning networks. The paper introduces a variational trace norm regularization technique for training models with competitive accuracy and fewer parameters. This method builds on low rank factorization of neural networks and has theoretical justification. The training technique involves variational trace norm regularization and low rank factorization of neural network weights. Other approaches for matrix compression include sparsity, hash-based parameter sharing, and other parameter-sharing schemes. Low rank factorization is effective for compressing large matrices, with models trained in two stages. Compressing convolutional models uses different techniques, which are not covered in this paper. The training technique involves variational trace norm regularization and low rank factorization of neural network weights. The truncation of weight matrices is done by retaining only as many singular values as needed to explain a specified percentage of the variance. Applying a sparsity-inducing penalty on the vector of singular values during stage 1 training can improve the approximation of the matrix. However, directly computing the trace norm and its gradients for large deep learning models is computationally infeasible. The proposed approach combines a two-stage training method with an indirect method for trace norm regularization. The proposed approach combines a two-stage training method with an indirect variational trace norm regularization technique. The trace norm is defined as the sum of singular values of a matrix, and a variational characterization of the trace norm in terms of the Frobenius norm is provided. The proposed approach involves a two-stage training method with variational trace norm regularization. The trace norm is the sum of singular values of a matrix, and a variational characterization in terms of the Frobenius norm is provided. The method involves replacing weight matrices with a product of two matrices, and using a modified loss function with a hyperparameter controlling the strength of the regularization. Empirical results show that this approach effectively reduces the trace norm of weight matrices. The proposed training scheme involves replacing weight matrices with a product of two matrices and using a modified loss function with a hyperparameter for trace norm regularization. Training is done in two stages, with the option to transition earlier to reduce training time. The experiments focused on trace norm regularization in a Deep Speech 2 model trained on the Wall Street Journal speech corpus. The model's parameters are dominated by three GRU layers and a fully connected layer, which are compressed through low-rank factorization. The experiments investigated trace norm regularization in a Deep Speech 2 model trained on the Wall Street Journal speech corpus. The model's parameters are compressed through low-rank factorization. The error metric reported is character error rate (CER) instead of word error rate (WER). Techniques are compared based on accuracy versus number of parameter trade-off curves. The study explored trace norm regularization in a Deep Speech 2 model trained on the Wall Street Journal speech corpus. Different regularization techniques were tested, including separate parameters for recurrent and non-recurrent weight matrices. The impact of \u03bb rec and \u03bb nonrec on final CER was analyzed. The study investigated the effectiveness of modified loss in reducing the trace norm in a Deep Speech 2 model. A nondimensional metric was introduced to measure the relative concentration of singular values in a matrix, with a lower coefficient indicating better approximation by a low-rank matrix. Regularization is crucial for reducing the trace norm coefficient in a weight matrix. Trace norm regularization is more effective than L2 regularization, leading to lower rank matrices. However, high L2 regularization can result in high classification error rates. Some L2-regularized models achieve low rank at high error rates for certain weights. Unregularized models perform relatively well, but some form of regularization is necessary. The study reports the results of stage 2 experiments. The study reports the results of stage 2 experiments where models were warmstarted from either trace norm or L2 regularized stage 1 models. The best models from both regularization types showed similar accuracy versus number of parameter trade-offs. Models warmstarted from unregularized stage 1 models had significantly lower accuracies, highlighting the importance of regularization. In the study, it was found that regularization is crucial for higher accuracies in models trained on the WSJ corpus. Transitioning from stage 1 to stage 2 models earlier can reduce training time without sacrificing final accuracy. By setting a fixed target of 3M parameters, the study showed it is possible to achieve this optimization. The study found that transitioning from stage 1 to stage 2 models earlier can reduce training time without sacrificing final accuracy. By setting a fixed target of 3M parameters and a training budget of 80 epochs, the results showed that lowering the transition epoch number did not significantly impact the final CER, with some cases even showing marginal improvements. The trace norm model also exhibited slightly better results compared to the other model. The study found that transitioning from stage 1 to stage 2 models earlier can reduce training time without sacrificing final accuracy. With low rank factorization techniques, large vocabulary continuous speech recognition (LVCSR) models were trained with acceptable parameters and loss of accuracy compared to a production server model. Different compressed models with lower parameters were compared, showing faster performance at the expense of some accuracy loss. The study found that transitioning to stage 2 models earlier can reduce training time without sacrificing accuracy. Low rank factorization techniques were used to train LVCSR models with acceptable parameters, reducing computational complexity. Using low-precision 8-bit integer representations for weight matrices and matrix multiplications can further optimize the system for real-time inference on mobile devices. This quantization method reduces memory and computation requirements while only slightly increasing Word Error Rate (WER). In the present study, low precision matrix multiplications were not pursued further due to inefficiency for small batch sizes in LVCSR applications on embedded devices. The study focused on transitioning to stage 2 models earlier to reduce training time without sacrificing accuracy and using low-precision 8-bit integer representations for weight matrices to optimize for real-time inference on mobile devices. In practice, batch sizes higher than around 4 resulted in high latencies, impacting user experience. Custom assembly kernels were implemented for the ARM architecture to improve GEMMs performance. Performance comparison with gemmlowp library showed faster results for batch sizes 1 to 4. The implementation details are available at https://github.com/paddlepaddle/farm. The peak single-core theoretical performance for iPhone 7, iPhone 6, and Raspberry Pi 3 are 56.16, 22.4, and 9.6 Giga Operations per Second, respectively. The gap between theoretical and achieved values is mainly due to memory bandwidth limitations. Various techniques were explored to speed up the LVCSR system, including low precision representation, customized ARM kernels, and other approaches detailed on the farm website. By combining low rank factorization, int8 quantization, and smaller language models, a range of speech recognition models tailored to different devices was created. Compression and reduction of inference latency were focused on, introducing trace norm for better model compression. To improve latency in LVCSR speech recognition models, a trace norm regularization technique was introduced for faster training on the WSJ speech corpus. Optimizing for low batch sizes and releasing optimized kernels for the ARM64 platform were shown to reduce latency at inference time. Combining these techniques demonstrated an effective path towards on-device speech recognition on embedded devices. Properties of the non-dimensional trace norm coefficient were described, including conditions for rank 1 and maximal rank matrices. The properties of the non-dimensional trace norm coefficient were discussed, including conditions for rank 1 and maximal rank matrices. The proof showed that the rank and all singular values are equal, with specific inequalities and conditions outlined. The minima occur when \u03c3 = (\u03c31, 0, ..., 0) and the maxima occur when \u03c3 = (\u03c31, ..., \u03c31), resulting in \u03bd(W) = 0 and \u03bd(W) = 1 respectively. The non-dimensional trace norm coefficient \u03bd(W) was discussed, with minima occurring when \u03c3 = (\u03c31, 0, ..., 0) and maxima when \u03c3 = (\u03c31, ..., \u03c31), resulting in \u03bd(W) = 0 and \u03bd(W) = 1 respectively. Insights on model selection for on-device streaming speech recognition were also provided, focusing on Deep Speech 2 like models with forward-only GRU layers. Shrinking recurrent layers closer to the input was found to have minimal impact on accuracy. The study focused on reducing the number of parameters in the baseline model by adopting growing GRU dimensions. The recurrent layers used the GRU architecture with specific dimensions chosen in an increasing scheme. The hope was that compression techniques would automatically optimize the layer sizes. The study explored weight sharing methods in low rank factorization of weight matrices in LSTM models. Three approaches were considered: completely joint factorization, partially joint factorization, and completely split factorization. The authors chose the LSTM analog of completely joint factorization for parameter sharing. The study compared completely joint factorization, partially joint factorization, and completely split factorization in low rank weight matrix factorization in LSTM models. They opted for partially joint factorization due to differences in behavior of U and W matrices during training, computational efficiency, and better accuracy versus number of parameters trade-offs. Switching from 161-dimensional linear spectrograms to 80-dimensional mel spectrograms led to better accuracy with reduced computational requirements. The use of Gram-CTC allowed for an increase in time stride during training. Using Gram-CTC, the time stride in the second convolution layer was doubled with minimal loss in CER, requiring a doubling of filters. This resulted in a 2x speedup for the largest GRU layers. However, Gram-CTC models could not be shrunk as much as CTC models for a given accuracy target. The technique increases model size for reduced latency, as shown in FIG5 with a trade-off between parameter reduction and CER increase. The baseline model for speech recognition is a Deep Speech 2 model with three forward-GRU layers of dimension 2560. Variations of this model with GRU dimensions scaled down to 1536 and 1024 are also considered. Low rank factorizations on weight matrices show the best trade-off between CER and parameters. Models with growing GRU dimensions and partially split low rank factorization perform well. Some models labeled as \"fast\" use Gram-CTC, mel features, and reduced filter sizes. The setup for these experiments was not perfectly controlled. Given the setup limitations, efforts to improve sparse models could make them competitive with low rank models. However, due to computational advantages, the focus will be on low rank models. Other structured forms of sparsity may still be useful in the embedded setting."
}