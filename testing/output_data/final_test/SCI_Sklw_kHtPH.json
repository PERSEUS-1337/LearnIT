{
    "title": "Sklw_kHtPH",
    "content": "Adam-typed optimizers, like AdamT, improve upon traditional methods by incorporating trend information for more efficient convergence in both convex and non-convex settings. The classic gradient descent algorithm is widely used in solving large-scale problems, with adaptive variants of SGD showing superior performance due to their rapid training time. These adaptive optimizers update model parameters using an adaptive step size based on squared gradients, with different moving average schemes for calculating the adaptive learning rate. The AdaGrad algorithm adapts the learning rate for each parameter based on historical squared gradients, leading to rapid convergence. RMSProp and ADADELTA are variations that further refine the adaptive learning rate by modifying the decay rate and parameter updates. Adam combines ideas from RMSProp and momentum methods to improve upon the adaptive learning rate algorithm. The new rule replaces the gradient in Equation (1) with first-moment estimation, showing superiority in convergence speed and memory requirement. Various variants of adaptive methods with exponential moving average gradients have gained attention in deep learning tasks. It remains uncertain if simple exponential smoothing or level information is enough to capture the cost surface landscape. Adding a trend term is suggested when clear patterns are recognized in the moving routine. In this paper, the Adam rule is modified with trend-corrected exponential smoothing schemes (AdamT) to achieve faster convergence to local minima. The research introduces trend-corrected features for gradients scaling and parameters updating, applicable to various adaptive update methods. The focus is on Adam for rule modification and performance comparison, proposing trend-corrected exponential smoothing to enhance optimizers with adaptive gradients. Our AdamT method incorporates trend information into the update rule of Adam, showing superior performance in both convex and non-convex settings compared to Adam. The method converges in convex settings with a regret bound of O( \u221a T ). The paper also discusses the fundamental idea of Adam and Holt's linear methods, update rules, experimental analysis, and recent developments of Adam-typed optimizers. There is potential to integrate our methods with non-convex optimization frameworks for future settings. In this paper, the focus is on analyzing and modifying the Adam optimizer. The method, proposed by Kingma & Ba in 2015, has become popular for its adaptive step size and exponential weighted average of previous gradients. Bias-correction is applied to moment estimates to prevent counteraction towards zero. The rules for extracting a smoothed new point from previous gradients are formally defined. The exponential weighted moving average method, introduced by Holt in 2004, extends to include trend behaviors in time series data. Holt's linear method involves updating level and trend terms using weighted averages of observations and estimations. Smoothing parameters \u03b1 and \u03b2 can be chosen between 0 and 1. The damped trend methods, popular for forecasting, incorporate both level and trend information from historical series. The damping factor \u03c6 controls the significance of the trend component and can reduce volatility. The proposed algorithm AdamT combines Adam with Holt's linear trend information for moment estimates. The proposed algorithm AdamT combines Adam with Holt's linear trend method for moment estimates, using trend-corrected exponential weighted moving averages. Hyperparameters \u03b21, \u03b31, and \u03c61 are used to estimate the level and trend information of the first and second raw moments. Equation (3.1) combines the level and trend information for the final update rule. The second raw moment in AdamT is estimated using moving averages {v_t} and {b_v_t}, initialized as zero vectors. Bias correction is applied to level and trend estimates {m_t} and {v_t} following Kingma & Ba (2015). Trend estimates {b_m_t} and {b_v_t} are corrected differently by considering damping parameters. The bias in AdamT is corrected by considering damping parameters (\u03c6 1 , \u03c6 2) for the series {m t } and {v t }. The bias-corrected trend estimates are justified in Appendix A. The update rule in AdamT uses bias-corrected first and second moment estimates. The direction of the step \u2206 t in parameter space depends on the joint effect of moment level and trend estimates. The ratio m t / |v t | is seen as a signal-to-noise ratio in the update rule. The proposed method AdamT has superior performance compared to Adam, with minimal additional computational cost. Hyperparameters are set according to Kingma & Ba (2015), with smoothing parameters for moment estimates set to 0.9. The proposed method AdamT improves upon Adam with superior performance and minimal additional computational cost. Moment estimates are set to 0.9 and smoothing parameters for the second raw moment estimates are set to 0.999. The pseudo-code for AdamT is provided in Algorithm 1, and convergence analysis results are presented. AdamT achieves a guarantee for all T \u2265 1 with bounded gradients. The average regret of AdamT converges, as shown in Corollary 3.1.1. The algorithm is evaluated on both convex and non-convex problems. The proposed algorithm AdamT is evaluated on convex and non-convex real-world optimization problems with various machine learning models, including logistic regression and different neural network models. Comparisons with Adam show that AdamT converges faster to a better minimum point, demonstrating the effectiveness of the trend information of the gradients infused in AdamT. In experiments, AdamT is compared to Adam on logistic regression with Fashion-MNIST dataset. AdamT uses default hyperparameters and is tuned for optimal results. The dataset has 60,000 training samples and 10,000 testing samples, each with 28x28 pixels. The Fashion-MNIST dataset consists of 60,000 training samples and 10,000 testing samples, each with 28x28 pixels. AdamT outperforms Adam in terms of convergence speed and performance during training, with a slight advantage in this experiment. In a digit classification experiment using feedforward neural networks on the SVHN dataset, AdamT shows a small advantage over Adam. The neural network has two fully connected hidden layers with 1,400 hidden units each, using ReLU activation function. The samples are pre-processed RGB images converted to grayscale, with softmax cross-entropy loss function used for training. In a digit classification experiment using feedforward neural networks on the SVHN dataset, AdamT outperforms Adam significantly. AdamT shows faster convergence and better performance than Adam for models with and without dropout layers. In a digit classification experiment, AdamT outperforms Adam significantly in feedforward neural networks on the SVHN dataset. AdamT demonstrates superior performance in the test phase and has better generalization ability. The loss surface becomes complex and non-convex compared to logistic regression. Training a CNN model on the CIFAR-10 dataset for multi-class classification tasks, the dataset contains 50,000 training samples and 10,000 test samples of RGB 32 \u00d7 32 images pre-processed by normalizing pixel values to [-1, 1]. The CNN model used in the experiment has 2 stages of convolution and max pooling layers with specific parameters. It includes a fully-connected layer with 600 hidden units and ReLU activation function. The model is trained with a constant learning rate and minibatch size of 128. AdamT outperforms Adam in training and test loss, as shown in Figure 3. In this experiment, a Variational Autoencoder (VAE) model is trained on the MNIST dataset with 60,000 training samples and 10,000 test samples. The VAE model architecture includes a Gaussian encoder, Bernoulli decoder, and 500 hidden units in each hidden layer. The latent space dimensionality is set to 20, and a hyperbolic tangent activation function is used. In this experiment, the VAE model is trained on the MNIST dataset with 60,000 training samples and 10,000 test samples. The latent space dimensionality is set to 20. The ELBO of the training and testing phases is examined for performance assessment using two optimizers. AdamT shows faster convergence at the early stage of training and outperforms Adam throughout the training phase, with superior performance also seen in the testing phase. Adam-type learning algorithms with exponential moving average scheme are considered. The Adam-type optimization algorithms proposed by Kingma & Ba (2015) have been extended to various variants like AdaMax, Nadam, and AdamW. However, these methods lack global convergence guarantees due to short-term memory issues. AMSGrad by Reddi et al. (2018) addresses this problem for convex settings, while Padam and AdaUSM offer solutions for non-convex optimization problems. These methods aim to improve optimization performance by introducing adaptive parameters and unified momentum concepts. In this work, a modified scheme called AdamT is introduced, which calculates the adaptive step size using trend-corrected exponential smoothing. Empirical results show that AdamT outperforms the baseline method Adam. The potential for future developments includes extending similar ideas to other adaptive gradient methods like RMSProp and AMSGrad. The AdamT method introduces adaptive step size using trend-corrected exponential smoothing, outperforming the baseline Adam method. Future developments may extend these ideas to other adaptive gradient methods like RMSProp and AMSGrad. The computational ability of AdamT on non-convex settings suggests potential for extending the theoretical framework to non-convex scenarios. The AdamT method introduces adaptive step size using trend-corrected exponential smoothing to correct bias and minimize regret. It investigates convergence with regret minimization following Zinkevich (2003). The regret is defined as the difference between total cost and the best point in hindsight, with f(xt) as the convex function and x* as the optimal parameter set. Jensen's inequality implies that the average decision converges to the optimal set. Essential theorems and lemmas on convexity and subgradient are listed. The set of subgradients of a function at x is denoted \u2202f(x) and is defined as vectors u such that f is differentiable. The function is Lipschitz continuous with an upper bound G on the norm of subgradients over K. Three core lemmas are used to prove the regret bound, including the Kingma & Ba (2015) lemma. The series {mt,i} and {vt,i} have specific summation forms for given parameters. Theorem B.5 assumes bounded gradients for the objective function ft. The proposed AdamT algorithm guarantees bounded gradients and distances between points, with specific parameters. The regret upper bound is derived for convex functions using update rules."
}