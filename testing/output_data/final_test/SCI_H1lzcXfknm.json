{
    "title": "H1lzcXfknm",
    "content": "In this paper, a novel loss function called \"tracking loss\" is designed to convert CNN-based object detectors into effective visual trackers without additional computational cost. This approach utilizes rich features learned from still images by detectors, eliminating the need for annotated video sequences for training. The tracking loss leverages the internal structure of feature maps within the detection network to simultaneously consider discrimination quality and bounding box accuracy, crucial for success. Additionally, a network compression method is proposed to accelerate tracking speed without sacrificing performance. The proposed network compression method accelerates tracking speed without performance reduction. Employing a carefully designed tracking loss ensemble enhances tracker robustness and accuracy. Evaluation results show outperformance of state-of-the-art methods on VOT 2016 Challenge. Visual tracking is essential for various applications in computer vision tasks. The success of Convolutional Neural Networks (CNNs) in computer vision tasks like image classification and object detection has led researchers to use CNNs as feature extractors in visual tracking. MDNet, a famous CNN-based tracker, achieved success on VOT 2015 but lacks generalizability due to being trained on annotated video sequences. Previous attempts to convert CNN-based classifiers for tracking relied heavily on feature engineering. One feasible idea to improve visual tracking is to convert pre-trained object detectors like RPN BID17 into trackers. These detectors provide strong features for classification and bounding box regression, making them suitable for discriminative trackers. Visual trackers typically require online training with limited ground truths, so data augmentation is used to increase training samples. Positive training samples are randomly cropped patches following a two-dimensional Gaussian distribution around the ground truth. One way to enhance visual tracking is by utilizing pre-trained object detectors like RPN BID17 as trackers. These detectors offer strong features for classification and bounding box regression, making them suitable for discriminative trackers. Positive training samples are randomly cropped patches following a two-dimensional Gaussian distribution around the ground truth. However, this sampling strategy may include noisy background pixels at the border of sampled images, affecting tracker performance over time. To address this issue, treating different feature points discriminatively is explored by analyzing top layer feature maps of RPN and designing matching strategies for improved tracking performance. The tracking loss method combines two matching strategies to effectively bridge object detection and visual tracking, without adding extra computation. A network compression method is proposed to trim the RPN for faster tracking speed. Additionally, a tracking loss ensemble with four types of loss functions is introduced, showing improved performance in evaluations. The tracking loss ensemble, including two baseline trackers, outperforms all state-of-the-art methods on VOT 2016 Challenge in terms of Expected Average Overlap (EAO) and robustness. The contributions include a novel tracking loss converting a pre-trained object detector to a visual tracker without extra computational cost, a network compression method to speed up the tracker, and a tracking loss ensemble with four types of loss functions for improved performance. The ensemble tracker, along with two baseline trackers, outperforms all state-of-the-art trackers on VOT 2016 BID12 in terms of EAO and robustness. Visual tracking algorithms can be generative or discriminative, with discriminative trackers treating tracking as a classification task. MDNet, the champion of VOT 2015 Challenge, uses binary classification to judge candidate proposals as target or background. MDNet was the champion of VOT 2015 Challenge BID11 but lacks generalizability due to being trained on labeled video sequences. Utilizing CNN features pre-trained on other tasks for visual tracking would be meaningful. BID21 and BID14 focused on cross-layer feature selection, while BID10 proposed a target-specific saliency map for sequential Bayesian filtering. RPN, first used in Faster R-CNN BID17, integrates candidate proposal generation and classification into a convolutional network. The RPN network consists of a backbone network and convolutional layers for proposal generation, classification, and bounding box regression. It uses translation invariant anchors, which are 1x1 positions in feature maps corresponding to anchor boxes in input images. Faster R-CNN assigns 9 types of anchor boxes with different aspect ratios and scales. A backbone network is needed for discriminative trackers to extract target-independent features. The RPN network utilizes a backbone network and convolutional layers for proposal generation, classification, and bounding box regression. It is suitable for constructing a discriminative tracker by utilizing the backbone network to extract target-independent features and re-initializing convolutional layers for target-specific features. Converting RPN to tracking requires re-designing the loss function to adapt to tracking tasks, a novel approach not previously attempted. The work focused on converting a detector into a tracker by adjusting the loss function. Larger deep neural networks can improve model capability but require more time and computational resources. Network compression is necessary to speed up the tracking process. A network compression pipeline involving pruning, quantization, and huffman encoding was proposed. Ensemble methods, such as combining weak classifiers or using loss ensemble, can strengthen the tracker. In this section, the text discusses the use of RPN in tracking and the importance of treating different feature points discriminately. Four matching strategies are evaluated based on discrimination in confidence scores, leading to the proposal of a tracking loss composed of two better-performing strategies. It is noted that directly employing RPN in tracking without modifications results in poor performance due to target-independent features and limited ground truths for online training in visual tracking. Data augmentation is commonly used to address this issue. Data augmentation is used in MDNet BID15 for online training due to limited ground truths. Positive training samples are cropped patches following a Gaussian distribution, leading to noise from background pixels. Discriminate treatment of pixels is suggested to improve tracking performance. In order to treat different feature points discriminately, the top layer feature maps of RPN are explored. The ZF type RPN with 14x14 top layer feature maps is selected for a trade-off between network capability and speed. Anchor boxes with aspect ratio 1:1 are reserved, reducing the number of anchor boxes and trainable weights, accelerating tracking speed in RPN. The purpose of anchor boxes in RPN is to provide fundamental coordinates in the input image. Using Intersection Over Union (IoU) to define similarity, anchor boxes are categorized as target object (positive) or background (negative) based on their similarity to the ground truth. In tracking, centric pixels are more confident in covering the target object, with corresponding central feature points being more likely to be positive. The central point of RPN is not unique due to even height and width. Anchor boxes in RPN are categorized as positive or negative based on their similarity to the ground truth. The central point of RPN is not unique due to even height and width. The confidence score of each anchor box is calculated based on IoU with the ground truth, allowing for discriminative treatment of anchors. Different matching strategies are defined based on confidence scores, with a square area with IoU 0.49 as the second matching strategy. During training, feature points in matched areas are used to calculate total loss in different matching strategies. Only matched anchor points update parameters, treating feature points discriminatively. Loss of classification (L cls) and loss of bounding box regression (L reg) are calculated using predicted probabilities and bounding boxes. Lambda (\u03bb) balances these two parts. In visual tracking, the RPN based tracker focuses on the L cls part, which includes different types of loss functions. Softmax Logistic Loss is commonly used and evaluated on VOT 2016 benchmark. Two types of experiments, unsupervised and baseline, are conducted to compare matching strategies. One Pass Experiment (OPE) without re-initialization compares trackers with different matching strategies. Trackers with the first and fourth matching strategy perform better but still fall short of expectations. Using a high IoU threshold during anchor box matching results in fewer feature points used for computing loss. This type of tracker can initially follow objects well but the bounding box gradually enlarges over time. The bounding box gradually enlarges over time with different matching strategies in tracking. Low IoU threshold improves bounding box quality but leads to quick tracker drift in challenging videos. Medium IoU thresholds result in even worse tracking performance. The first matching strategy follows the target object but provides poor bounding boxes, while the fourth strategy has accurate bounding boxes but increased classification errors. The paper discusses different matching strategies in tracking, highlighting the trade-offs between classification accuracy and bounding box quality. The proposed tracking loss combines the strengths of the first and fourth matching strategies to improve both discrimination quality and bounding box accuracy simultaneously. The new tracking loss combines the strengths of the first and fourth matching strategies to improve discrimination quality and bounding box accuracy simultaneously. It employs two matching strategies with independent RPN modules sharing features from the backbone ZF network. The tracking loss is effective in converting detection to tracking, with significant improvements in AO observed in experiments. The new tracking loss combines matching strategies to improve discrimination quality and bounding box accuracy. RPN is a large network for tracking, but additional actions are needed to accelerate its speed. Shallow features are field independent, and a robust loss function should be able to handle network variations. A new network compression method is proposed to speed up and test the robustness of tracking loss by distilling knowledge from a large network into a smaller one. Training the compressed ZF network from a pre-trained ZF network is suggested. The compressed ZF network is trained by resizing images into two resolutions, using Conv5 feature maps as guidance, and initializing weights from a pre-trained ZF network. The input image sizes are 203x203 and 107x107, with adjustments made to the network to maintain top layer feature map sizes. Pooling layers are removed to improve speed and robustness. The compressed ZF network is modified to resize images, remove pooling layers, and update weights during training. After training, the network is evaluated on the VOT 2016 benchmark, achieving similar performance to the baseline experiment. The network compression process preserves old knowledge without loss. The compressed ZF network, after being modified for network compression, shows robust tracking loss independent of feature extraction. The forward pass time decreases significantly, accelerating tracking speed four times. The network also provides high-quality features for feature extraction. Different types of loss functions are utilized for ensemble tracking. The network contains four independent branches with different loss functions for tracking. Each branch has Conv proposal layers and scoring mechanisms. The scores determine if candidate proposals belong to the target object or background. The tracking loss ensemble is composed of the four branches, with weights randomly initialized before online tracking. The tracker is implemented in MATLAB using Caffe deep learning framework, with specific values for \u03b1:\u03b2 determined before tracking. Different loss functions have varying base learning rates and momentum values. The tracker achieves a speed of 1.6 FPS with a compressed ZF network and tracking loss ensemble, outperforming MDNet by 60% in speed on VOT 2016 BID12 dataset. The benchmark focuses on content quality and annotations over quantity, with a re-initialization protocol triggered for zero overlap between predicted and ground truth bounding boxes. Tracking performance is evaluated using EAO, accuracy, and robustness metrics. Four baseline tracking losses and an ensemble are evaluated using a compressed ZF network. Comparison is made with 9 state-of-the-art algorithms on VOT 2016 Challenge BID12. The benchmark evaluates tracking performance using EAO, accuracy, and robustness metrics. The tracking loss ensemble ranks highest in terms of accuracy and robustness, outperforming state-of-the-art trackers like CCOT and TCNN. Our proposed tracking loss algorithms surpass all contrast trackers in EAO and robustness but struggle with accuracy, possibly due to the lack of bounding box regression. In this paper, data augmentation introduces noise to train trackers. Pixels in different positions are treated discriminatively, with centric pixels more likely to cover the target object. The translation invariant anchor box design of RPN allows easy mapping to top layer feature maps. Central feature points are considered ground truth, with confidence scores defined by IoU. Four central points are designated as the ground truth for matching. The text discusses the design of matching strategies and the use of RPN in tracking, along with the proposal of a novel tracking loss to treat feature points discriminatively. A network compression method is also introduced to accelerate tracking speed without performance decline. Evaluation results on VOT 2016 show that the tracking loss ensemble tracker outperforms state-of-the-art trackers in terms of EAO and robustness, indicating the effectiveness of the tracking loss in converting a popular detector RPN to a well-performed tracker. In the paper, a novel tracking loss is proposed to convert an object detector to a robust tracker without extra time or computational consuming modifications. The tracking loss fully exploits the internal structure of top layer features to treat feature points discriminatively, providing high-quality discrimination and tight bounding boxes in tracking. Network compression yields a 4 times speedup, proving the robustness of tracking loss to network variations. Evaluation results on VOT 2016 show that the tracking loss ensemble tracker outperforms all baseline trackers. The tracking loss ensemble tracker, along with two baseline tracking loss trackers, outperforms all state-of-the-art trackers in terms of EAO and robustness."
}