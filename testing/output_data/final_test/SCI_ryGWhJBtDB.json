{
    "title": "ryGWhJBtDB",
    "content": "This paper explores how hyperparameters of stochastic gradient descent impact neural network training. It identifies two regimes - noise dominated for small batch sizes and curvature dominated for large batch sizes. In the noise regime, optimal learning rate increases with batch size, while in the curvature regime, it remains constant. Training loss and test accuracy are affected differently in each regime. Experiments on various architectures validate these findings. Autoencoders are trained using a grid search over learning rates at different batch sizes. Small or moderately large batch sizes perform better than very large batches on the test set, even with the same training steps and losses. When training Wide-ResNets on CIFAR-10 with a batch size of 64, the optimal learning rate for maximizing test accuracy only decreases by a factor of 2 when the epoch budget is increased by a factor of 128. This suggests that noise in stochastic gradients can provide beneficial implicit regularization. Some scaling rules predict how changing learning rate and batch size affects network performance. Contradictory findings exist, with debates on the effectiveness of SGD with Momentum compared to SGD without Momentum. Minibatch stochastic gradient descent exhibits two regimes: noise dominated for small batch sizes and curvature dominated for larger batch sizes. In the noise dominated regime, final training loss and test accuracy are independent of batch size with an increasing optimal learning rate. In the curvature dominated regime, the optimal learning rate is constant regardless of batch size, leading to degraded training loss and test accuracy. The critical learning rate separating the two regimes varies between architectures. When performing language modelling with an LSTM, a square root scaling rule is observed due to the non-independence of consecutive gradients. SGD with Momentum and learning rate warmup may outperform vanilla SGD in the curvature dominated regime. There is a debate on the role of stochastic gradients in generalization, with some studies showing that stochastic gradients can sometimes generalize better than full batch gradient descent. No paper has demonstrated a clear generalization gap between small and large batch training under a constant step budget on a challenging benchmark while tuning the learning rate. Some authors argue that additional regularization can reduce this gap, while others suggest that a noisy quadratic model can describe neural network performance. Small or moderately large batch sizes can outperform very large batches on the test set in some cases, even under a constant step budget. The test accuracy of models trained on CIFAR-10 and Penn TreeBank datasets degrades with larger batch sizes, but no degradation in final training loss is observed. The optimal learning rate depends on the epoch budget for a fixed batch size, maximizing test accuracy for a finite epoch budget. The text discusses the impact of noise in stochastic gradients on generalization and hyper-parameter tuning strategies for optimal learning rate and epoch budget. It also explores the regimes of SGD with and without Momentum, drawing analogies to stochastic differential equations. In sections 3, 4, and 5, the study examines the relationship between optimal learning rate, batch size, and epoch budget in gradient descent. Different regimes are identified based on empirical findings, with stochastic gradients showing implicit regularization effects on test set accuracy. The impact of noise in stochastic gradients on generalization is also discussed. The full batch gradient descent update is determined by the loss function and learning rate. To minimize loss quickly, a large learning rate is set initially. The loss is approximated by a convex quadratic function, leading to parameter updates. Convergence of iterates is ensured if the learning rate is below a critical value. The curvature constraint in training is defined by crit = 2/\u03bb max, where \u03bb max is the largest Hessian eigenvalue. The optimal initial learning rate is just below crit. Learning rate decay can be introduced to minimize loss along high curvature directions later in training. Momentum methods like Heavy-Ball Momentum enable faster convergence on poorly conditioned loss landscapes by taking an exponential moving average of previous gradients. Momentum optimization allows for larger steps in low curvature directions while remaining stable in high curvature directions. It increases the critical learning rate on quadratic losses and can minimize training loss in fewer steps compared to full batch gradient descent. Noise introduced by estimating gradients over a minibatch plays a crucial role in training dynamics, especially in large batch training regimes. In large batch training regimes, the optimal initial learning rate is determined by the critical learning rate crit, with SGD Momentum outperforming SGD without Momentum. In small batch training regimes, noise in parameter updates governs the training process. Assumptions are made that gradients of individual examples are independent samples from a distribution. The noise in a gradient update is modeled by a Gaussian noise source \u03b4, with covariance inversely proportional to the batch size. The SGD update is approximated using a temperature T = /B. The dynamics of SGD are expected to follow a stochastic differential equation with the learning rate scaling linearly with the batch size in the noise dominated regime. In the noise dominated regime, the learning rate scales linearly with the batch size, as observed in empirical studies. However, this linear scaling rule may not be valid in all cases. Despite this, batch sizes in the noise dominated regime tend to achieve similar test accuracies and training losses under a constant epoch budget. Additionally, previous works have shown that SGD with and without Momentum are equivalent in the small learning rate limit when certain conditions are met. In the noise dominated regime, batch sizes tend to achieve similar test accuracies and training losses under a constant epoch budget. Previous studies have shown that SGD with and without Momentum are equivalent in the small learning rate limit. SGD with Momentum outperforms vanilla SGD in the large batch limit. Large batch training and parallel computation have been used to minimize training time for deep networks. Training deep networks has seen significant improvements in terms of time efficiency, with ResNet-50 achieving over 76% accuracy in just a few minutes. Learning rate warmup, introduced by Goyal et al. (2017), enables stable training with larger batch sizes. In the noise dominated regime, increasing batch size leads to higher learning rates, allowing for efficient training with larger minibatches. When increasing batch size, the assumption crit may be invalidated, leading to a transition to the curvature dominated regime. Experiments show a sharp transition between regimes, with optimal learning rate schedules showing both regimes. Identifying the optimal schedule within a realistic computation budget is challenging, leading practitioners to prefer simple schedules with sharp drops in learning rate. Learning rate schedules, often parameterized by an initial learning rate and sharp drops, are easy to tune and thought to generalize well. The optimal initial learning rate depends on whether training is noise or curvature dominated. There may be an optimal temperature at early times for generalization. In the noise dominated regime, the temperature T influences gradient noise dynamics. In the noisy quadratic model, the temperature T influences gradient noise dynamics. Minibatch noise can be beneficial during training, leading to parameters that perform well on the test set. Increasing batch size may affect final training loss differently under constant epoch and step budgets. Jin et al. (2019) argued about the implications of these dynamics. The optimal temperature perspective suggests that increasing batch size may lead to higher training loss under a constant epoch budget, but it does not predict the effect under a constant step budget. Beyond a certain threshold batch size, test accuracy may decrease under both constant epoch and step budgets. Performance of SGD with and without Momentum, as well as with learning rate warmup, is reported along with a grid search for optimal learning rate. The test accuracy is independent of batch size when small, but decreases when it exceeds 512. Training loss at optimal learning rate is also independent of batch size when small, but increases rapidly without acceleration techniques for large batch sizes. Optimal learning rate is proportional to batch size when small, but independent when large. Study explores performance on training and test sets, and optimal learning rate based on batch size under constant epoch budget. Model architectures and datasets are selected for analysis. In this section, model architectures and datasets are chosen for experiments. Wide-ResNets on CIFAR-10 are primarily used, with additional experiments on ResNet-50, LSTMs, and autoencoders in the appendix. The learning rate decay schedule involves holding the rate constant for the first half of the epochs, then reducing it by a factor of 2 every 20 epochs for the remainder of training. The learning rate schedule for experiments involves a single hyper-parameter, the initial learning rate. Learning rate warmup is sometimes introduced, where the rate linearly increases over the first 5 epochs. Optimal test accuracy and learning rate are evaluated for various batch sizes by training the model multiple times with different learning rates. The best runs are selected to calculate mean test accuracy and determine the optimal learning rate, ensuring results are reliable. In experiments, error bars on optimal learning rate are defined within one standard deviation. Data augmentation techniques are applied, including padding and flips. Specific coefficients and batch normalization are used. The focus is on comparing performance with different batch sizes and training procedures, not achieving state-of-the-art results. In experiments, error bars on optimal learning rate are defined within one standard deviation. Data augmentation techniques are applied, including padding and flips. Specific coefficients and batch normalization are used. The focus is on comparing performance with different batch sizes and training procedures, not achieving state-of-the-art results. We apply the same experimental protocol in each case and provide the full results of a learning rate sweep at two batch sizes. Plotting the optimal test accuracy for a range of batch sizes with a 16-4 Wide-ResNet, we observe that SGD with Momentum and learning rate warmup outperform SGD without Momentum when the batch size is large. The optimal test accuracy is independent of batch size when small, but decreases as the batch size grows. This trend is also seen in the final training loss at the optimal effective learning rate. In figure 1c, the optimal effective learning rate is shown against batch size for different SGD variations. The behavior of SGD is influenced by batch normalization. An experiment without normalization is repeated, and a modification called \"ZeroInit\" is introduced for stable training. The modification in batch normalization allows for training very deep networks and reduces the gap in optimal test accuracy between networks trained with and without batch normalization. Similar trends are observed in experiments with different model architectures, showing a transition from a small batch regime to a large batch regime where SGD with Momentum outperforms SGD. Training loss and optimal test accuracy become independent of batch size in the noise dominated regime but decrease in the curvature dominated regime. In the noise dominated regime, optimal learning rates are proportional to batch size, except for an LSTM on the Penn TreeBank dataset where it is proportional to the square root of batch size. Consecutive minibatches in a language model are correlated, violating linear scaling assumptions. SGD transitions between regimes with different behaviors in popular architectures, with results explained from various perspectives. In the noise dominated regime, optimal learning rates are proportional to batch size. The results do not clarify if minibatch noise leads to implicit regularization for better test performance. Evaluating how batch size affects optimal test accuracy under a constant step budget reveals that larger batch sizes can minimize training loss. Test accuracy of a 16-4 Wide-ResNet on CIFAR-10 is reported for batch sizes ranging from 1024 to 16384. The optimal test accuracy initially increases with increasing batch size, but then begins to fall. The optimal test accuracy at batch size 4096 is 94.7%, while at batch size 16384 it is 92.8%. The final training loss decreases continuously with batch size, despite the optimal learning rate being defined by test set accuracy. Stochastic gradient noise can enhance generalization, increasing test accuracy by nearly 2%. Previous authors have also observed this effect, but this experiment demonstrates it when training a well-respected architecture to the expected test accuracy. The experiment demonstrates that stochastic gradient noise can enhance generalization, increasing test accuracy by nearly 2%. The implicit regularization introduced by stochastic gradients should be considered complementary to the implicit bias of gradient descent. The optimal test accuracy and learning rate depend on the epoch budget, with the optimal training temperature potentially falling as the number of training epochs increases. The experiment shows that stochastic gradient noise improves generalization, increasing test accuracy by almost 2%. The optimal test accuracy and learning rate vary with the epoch budget, with the optimal training temperature potentially decreasing as training epochs increase. The effects of batch normalization on optimal learning rates are also studied. The experiment shows that stochastic gradient noise improves generalization, increasing test accuracy by almost 2%. In figure 2a, the optimal test accuracy initially increases with epoch budget, saturating with batch normalization beyond 800 epochs. Without batch normalization, it falls beyond 400 epochs, similar to early stopping. Figure 2b shows the optimal training loss decreasing as epoch budget increases for a 16-4 Wide-ResNet on CIFAR-10 using SGD with Momentum and batch size of 64. The experiment demonstrates that increasing the epoch budget initially boosts test accuracy, but it starts to decline after a certain point without batch normalization. The training loss decreases consistently with more epochs. The learning rate affecting training loss decreases rapidly with more epochs when using batch normalization, while the learning rate impacting test accuracy remains relatively stable. Without batch normalization, the learning rate for minimizing training loss decreases with more epochs, while the learning rate for maximizing test accuracy remains constant. Figures 2c and 2d reveal that the learning rate for minimizing training loss decreases rapidly with more epochs, regardless of batch normalization. When training deep networks on classification tasks, there is an optimal learning rate early in training that biases small batch SGD towards parameters performing well on the test set. Results suggest identifying the optimal learning rate for a modest epoch budget before increasing it, reducing hyper-parameter tuning costs. The study explores optimal learning rates in training deep networks, showing two regimes with different behaviors in SGD. In the noise-dominated regime, test accuracy is independent of batch size, while in the curvature-dominated regime, the optimal learning rate increases with batch size. Additional experiments on LSTM and autoencoder are provided in appendices. In the curvature dominated regime, the optimal learning rate is independent of batch size, acceleration techniques outperform vanilla SGD, and test accuracy decreases with batch size. The optimal learning rate in the noise dominated regime is proportional to batch size if certain assumptions are met. A gap in test accuracy between small or moderately large batch sizes and very large batches persists even under a constant step budget. The test accuracy of Wide-ResNets depends weakly on the epoch budget, while the training loss decreases rapidly with increasing epoch budget. The training loss decreases rapidly with increasing epoch budget, confirming that stochastic gradients introduce implicit regularization. A standardized learning rate decay schedule is illustrated in figure 3, with a linear increase in learning rate over the first 5 epochs when learning rate warmup is included. Learning rate decay occurs at points defined by the fraction of the total epoch budget completed, with a 5-epoch warmup period regardless of the epoch budget. The text discusses learning rate warmup for 5 epochs, applying the central limit theorem to approximate a single SGD step, and deriving a linear scaling rule for parameter updates. It assumes independent and uncorrelated gradients from a short-tailed distribution and considers the total change in parameters over consecutive updates. When the product of steps and learning rate is smaller than a critical rate, parameters do not move significantly. Equation 5 and 6 imply that \u03be and \u03b4 are Gaussian random variables from the same distribution. Comparing equations 3 and 7, n SGD updates at temperature T with learning rate is equivalent to a single SGD step at temperature T with learning rate n. Batch size affects noise and curvature dominance. Final test accuracy and training loss are shown in figures. SGD variants perform similarly at low learning rates but poorly at high rates. The linear scaling rule predicts that doubling both the learning rate and batch size should draw samples from the same distribution over parameters after the same number of training epochs. This assumption usually holds when the batch size is small, contradicting previous assumptions. The distribution of \u03b4 i does not matter in practice. The distribution of \u03b4 i does not matter in practice, as our dynamics are governed by noise over multiple updates. We only require that \u03be i is Gaussian, not necessarily \u03b4 i. The central limit theorem predicts that \u03be i will be Gaussian under certain conditions. Experimental results on wide residual networks trained without batch normalization using \"ZeroInit\" are provided. The scheme presented introduces three modifications to the original Wide-ResNet architecture: a scalar multiplier at the end of each residual branch, biases to each convolutional layer, and dropout on the final fully connected layer. These modifications allow for training deep residual networks without batch normalization, with dropout included for additional regularization. In this paper, the performance of different optimization techniques like SGD, SGD w/ Momentum, and SGD w/ momentum with learning rate warmup is evaluated through grid search. Test accuracy and training loss are analyzed in relation to batch size, showing that optimal learning rate varies with batch size for different optimization techniques. The optimal effective learning rate depends on batch size for SGD w/ Momentum, while it is independent for vanilla SGD. Additional results on 16-4 Wide-ResNet with batch normalization on CIFAR-10 are provided, showing similar final performance for SGD, SGD with Momentum, and warm up learning rates in the small learning rate limit. SGD performs poorly compared to SGD with Momentum and warmup learning rates when the learning rate is large. Optimal learning rates for all three methods are small with a small batch size, leading to similar performance. However, with a large batch size, SGD with Momentum and warmup learning rates outperform vanilla SGD. These results align with the two regimes of SGD and SGD with Momentum discussed earlier. The learning rate schedule is described in section 3, training for 200 epochs. Batch normalization is removed, and ZeroInit is introduced. Performance of SGD degrades with increasing batch size, while Momentum's performance remains constant for batch sizes up to 256. Above this threshold, both methods degrade rapidly. Optimal learning rates for Momentum are similar with or without warmup, increasing proportionally to batch size before saturating. SGD's optimal learning rate is curvature bound at all batch sizes. Results for ResNet-50 trained on ImageNet for 90 epochs at various batch sizes are provided in table 3. The study compares the performance of SGD with and without Momentum across different batch sizes. SGD with Momentum outperforms SGD without Momentum for large batch sizes. The optimal learning rate for Momentum increases with batch size, while the test MSE of vanilla SGD starts rising for batch sizes exceeding 16. The study compares the performance of SGD with and without Momentum across different batch sizes. The test MSE of vanilla SGD starts rising for batch sizes exceeding 16. The optimal effective learning rate is proportional to batch size for small batches, becoming independent for larger batches. SGD with Momentum has a larger optimal effective learning rate in the curvature dominated regime. Training a fully connected autoencoder on the MNIST dataset is also discussed. The architecture discussed in the curr_chunk is known for its bottleneck structure, making it challenging to optimize and often used as a benchmark. The L2 regularization parameter was set at 10^-5, with a learning rate decay schedule used for training. Performance of SGD and SGD with momentum is independent of batch size for small batches but degrades with larger batch sizes. The model's poor conditioning due to its bottleneck structure causes SGD to degrade at smaller batch sizes compared to residual networks, leading SGD with momentum to outperform at smaller batch sizes. The curr_chunk discusses training a word-level LSTM language model on the Penn TreeBank dataset, using a modified learning rate schedule and conducting a grid search to find the optimal effective learning rate. Results show that Momentum outperforms SGD when the batch size is large. The curr_chunk discusses the impact of batch sizes on the performance of SGD and SGD with Momentum in training a word-level LSTM language model on PTB. It shows that test set perplexity increases with larger batch sizes, and the optimal learning rate is related to the square root of the batch size. The assumptions behind linear scaling are violated due to the interdependence of gradients in consecutive minibatches. The LSTM model used has two layers with 650 units each, initialized uniformly in [-0.05, 0.05]. Gradient clipping at 5 and dropout with 0.5 probability on non-recurrent connections are applied during training for 40 epochs with an unroll step of 35. The learning rate decay schedule reaches the same test perplexity performance as reported in the original study. Performance of SGD and SGD with momentum is similar for small batch sizes, but degrades for larger batch sizes. The optimal learning rate increases as the square root of the batch size for small batches. The optimal learning rate increases with the square root of batch size for small batches, leveling off for larger batches due to correlations in consecutive data samples. Additional results show how test accuracy varies with batch size under a constant step budget. Training a fully connected auto-encoder on MNIST for 156,250 updates, the test set MSE rises with batch size while training set MSE decreases. The optimal effective learning rate is independent of batch size, suggesting a curvature dominated regime. Training a word-level LSTM on the Penn TreeBank dataset for 16560 updates, the test perplexity increases with batch size while training perplexity decreases. The optimal effective learning rate increases with batch size, indicating a noise dominated regime. Additional experimental results are provided in section 5. In section 5, experimental results on a word-level LSTM model on the PTB dataset are presented. Results show that the test set perplexity initially decreases then increases with the epoch budget, while the training set perplexity decreases consistently. The optimal learning rate for minimizing test set perplexity remains constant regardless of the epoch budget, as long as it is not too small. In a fully connected autoencoder trained on MNIST, the test set MSE initially decreases then increases with the epoch budget, while the training set MSE consistently decreases. The optimal learning rate for minimizing test set MSE decreases as the epoch budget rises, contrary to previous observations. For a larger learning rate, the model overfits faster on the training set. The larger learning rate of 0.004 causes overfitting on the training set, leading to a rise in test set MSE by the first learning rate drop at 400 epochs. Optimal learning rate for test MSE drops to prevent overfitting before the first decay. Early stopping is crucial in this architecture and dataset, influencing final test performance more than stochastic gradient noise. The optimal effective learning rate for minimizing test set perplexity and training set perplexity is identified in a fully connected autoencoder on MNIST using SGD with Momentum. The test set perplexity initially decreases with epoch budget but starts to rise after 56 epochs, while the training set perplexity decreases consistently. The learning rate that minimizes training set perplexity decreases with epoch budget, while the one for test set perplexity varies by a factor of 2 over two orders of magnitude. The test set MSE decreases initially with epoch budget but starts to rise slightly for large epoch budgets. The train set MSE decreases consistently with the epoch budget. The learning rate that minimizes the test set MSE decreases, while the one for the train set MSE remains constant as the epoch budget rises. This is contrary to what is observed in figures 2 and 8. The reason for this is that a larger learning rate causes the model to overfit on the training set faster, leading to an increase in test set MSE. The test set MSE initially decreases with epoch budget but starts to rise slightly for large epoch budgets. Early stopping has more influence on final test performance than stochastic gradient noise. Optimal learning rate depends on epoch budget, with evidence of an optimal temperature early in training for good generalization performance. Coupling initial and final learning rates makes it less clear whether this is beneficial. The optimal initial and final learning rates for maximizing test set accuracy and minimizing training set loss were tuned independently. Test accuracy initially increases with compute budget before saturating, while training loss decreases monotonically with epoch budget. The influence of early stopping on final test performance is greater than stochastic gradient noise. In this section, experiments were conducted on varying epoch budgets to tune initial and final learning rates independently. The learning rate schedule involved using the initial rate for the first half of epochs and decaying it by a factor every 20 epochs. The choice of decay factor influenced the optimal initial and final learning rates for maximizing test accuracy and minimizing training loss. In experiments with varying epoch budgets, the optimal initial and final learning rates were tuned independently for maximizing test accuracy and minimizing training loss. Results showed that increasing the compute budget led to an initial increase in test set accuracy, which then saturated for epoch budgets greater than 800. The optimal training loss decreased as the epoch budget grew. The optimal initial and final learning rates were plotted for maximizing test set accuracy and minimizing training set loss. The optimal initial learning rate for maximizing accuracy decays slowly with increasing epoch budget, while the final learning rate decays rapidly. Error bars on the final learning rate are larger, indicating the importance of tuning the initial learning rate. The initial learning rate for minimizing training loss decays slowly with increasing epoch budget, while the final learning rate decays much more quickly. The optimal initial learning rate for maximizing test accuracy is consistently higher than for minimizing training loss, and the optimal final learning rate for maximizing test accuracy is consistently lower than for minimizing training loss. These observations support the belief that maintaining a high learning rate at early times and decaying rapidly later on generalizes well to the test set in some architectures."
}