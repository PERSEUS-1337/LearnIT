{
    "title": "HkgsPhNYPS",
    "content": "Deep neural networks (DNNs) can over-fit a dataset with noisy labels during training. A method called self-ensemble label filtering (SELF) gradually filters out wrong labels, improving task performance by focusing on clean labels. Running averages of predictions are used to identify inconsistent predictions better than single estimates. Filtered samples are removed from supervised training but utilized in semi-supervised learning. The text discusses leveraging semi-supervised learning to improve image classification tasks in the presence of label noise. It outperforms previous works and can be applied to various network architectures. The acquisition of high-quality human annotations is a common bottleneck in deep neural networks. Crowdsourcing and web annotations are imperfect alternatives that introduce noisy labels. Zhang et al. (2017) demonstrated that DNNs can memorize data and struggle with noisy labels, leading to a decline in generalization ability. Mitigating label noise is crucial, with approaches like eliminating noisy labels or using semi-supervised learning to improve model robustness. The decision on which labels are noisy is critical for effective learning. In this paper, a self-ensemble label filtering (SELF) framework is proposed to identify and filter out noisy labels during training, allowing DNNs to focus on learning from correct samples even with high label noise. The key contribution is progressive filtering, leveraging the network's output to form a consensus and improve performance as supervision becomes less noisy. The SELF framework aims to filter out noisy labels from the data to stabilize neural representations and improve prediction accuracy. It utilizes the Mean-Teacher model for stable training and ensemble learning to provide a more reliable supervisory signal for filtering out potential noisy labels. This approach differs from traditional semi-supervised techniques by focusing on progressive filtering rather than just combining methods. Our approach, self-ensemble label filtering (SELF), utilizes model ensemble learning to filter out noisy labels progressively. It allows for supervised loss computation on cleaner subsets and leverages the entire dataset, including filtered erroneous samples, in unsupervised loss. This technique is a principled method against learning under noisy labels, as deep neural networks adapt from easy to hard samples during training. The SELF framework utilizes model ensemble learning to filter out noisy labels progressively, stabilizing the training process and improving the generalization ability of DNNs. It records network outputs from different training epochs and identifies correctly labeled samples through self-ensemble predictions, treating them as clean samples. The SELF framework improves DNN generalization by filtering noisy labels using model ensemble learning. It outperforms existing approaches on image classification tasks and remains robust across different network architectures. The framework utilizes self-ensemble predictions for stable learning signals and filtering, making it transferable to other tasks without architectural modifications. The proposed approach aims to filter out noisy labels in the training set using self-forming ensembles of models and predictions. By progressively identifying correct labels, the model counters noisy labels by maintaining a running average of model snapshots. This ensemble model provides additional learning signals for the filtering process. The ensemble model is trained on the entire dataset to provide additional learning signals for single models. The framework maintains a running average of model predictions for filtering. The model is trained until the best performance on the validation set is achieved. Correct labels are detected based on a defined strategy. The iterative training stops when no better model can be found. The focus is on detecting certainly correct labels from the initial label set. The model is trained iteratively using potentially correct labels from each label set. It stops learning when no improvement is achieved. By learning from easy samples, the network can distinguish between hard and wrongly-labeled samples. The framework focuses on detecting certainly correct labels rather than repairing all noisy labels. In an iterative training framework, potentially correct labels are used for model training, stopping when no improvement is seen. The focus is on detecting certainly correct labels rather than fixing all noisy labels. The model can reconsider clean labels removed in earlier iterations, helping to achieve high performance by filtering out wrong labels. In the SELF framework, model predictions are improved by using ensembles of models and predictions, rather than relying on single training epoch data. The ensemble model, Mean Teacher, utilizes an exponential running average of model snapshots for better filtering decisions. In the proposed framework, the Mean Teacher model and the normal model are evaluated on all data to maintain consistency. The consistency loss between student and teacher output distribution can be achieved with Mean-Square-Error loss or Kullback-Leibler-divergence. Additionally, a moving-average prediction scheme is suggested to collect sample predictions over multiple training epochs for more stable basis. Our proposed procedure for filtering predictions leads to stable filtering with minimal memory and computation overhead. Continuous training of the best model from the previous iteration reduces computation time significantly. Depending on the budget, a maximal number of filtering iterations can be set to save time. Previous works have shown that deep neural networks exhibit natural robustness to label noise without requiring modifications to the network or training procedure. Our framework SELF leverages the natural robustness of deep neural networks to kickstart a self-ensemble filtering process, extending robust behavior to challenging scenarios. Unlike previous works that use semi-supervised techniques to counteract noise, our framework performs self-supervised label corrections by learning from a dynamic set of labels determined by the network itself. This progressive filtering allows the network to focus on a label set with a lower noise ratio. The SELF framework focuses on self-detecting correct samples and does not repair wrong labels like other works. It assigns extreme weights of either zero or one to samples, making it easier to detect correct samples and achieve high performance. In the SELF framework, ensemble learning is used to counteract model fluctuations by restricting the second network to running averages of the first network. This approach gradually detects correct labels and learns from them, using not only model ensembles but also an ensemble of predictions. Other works modify the primary loss function of classification tasks to correct labels. In the SELF framework, ensemble learning is utilized to counteract model fluctuations by restricting the second network to running averages of the first network. This approach gradually detects correct labels and learns from them, using not only model ensembles but also an ensemble of predictions. Unlike other works that modify the primary loss function, SELF does not alter the main loss but focuses on tasks requiring clean labels like anomaly detection and self-supervised learning. The proposed progressive filtering procedure and self-ensemble learning are effective in countering noise in various tasks. The proposed framework SELF utilizes ensemble learning to counteract model fluctuations by restricting the second network to running averages of the first network. It performs label flips of semantically similar classes on CIFAR-10 and pair-wise label flips on CIFAR-100. The framework outperforms previous methods in semi-supervised learning strategies. The source codes for co-teaching, MentorNet, JointOpt, and DAC are available for evaluation. DAC considers a small clean validation set of 1000 and 5000 images. Additionally, experiments are conducted with a small clean set of 1000 images. Oracle experiments or methods using additional information are abandoned to maintain comparability. Reported performance from corresponding publications is adopted, and testing scenarios are kept similar for fair comparison. Testing scenarios use a noisy validation set with the same noise distribution as the training set. The model is trained with Stochastic Gradient Descent for a maximum of 300 epochs, with patience of 50 epochs. Results on CIFAR-10 and CIFAR-100 show that the approach SELF performs well with noise ratios up to 60% and outperforms previous works. However, there is a significant performance loss at 80% label noise. Despite a strong performance loss at 80% label noise, the SELF approach outperforms most previous methods. Experiment SELF* using 1000 clean validation images reveals that the performance loss mainly stems from over-reliance on the noisy validation set. Networks based on ResNext18 and Resnext50 show improved precision@1 and @5 compared to MentorNet using ResNet101. Even the weaker model ResNext18 surpasses MentorNet, showcasing significant improvements in challenging noise scenarios. Our framework SELF, based on ResNet101, demonstrates high label noise resistance in challenging scenarios on CIFAR-10 and CIFAR-100 datasets. It outperforms previous approaches in most scenarios, except for CIFAR-10 with 80% noise. The self-ensemble filtering process helps the network identify correct samples even under extreme noise ratios. SELF demonstrates high label noise resistance on CIFAR-10 and CIFAR-100 datasets, outperforming previous approaches in most scenarios. The network's performance remains consistent across different architectures, with self-supervised filtering and model ensembles improving robustness. Performance is only impaired at 80% noise, showcasing SELF's effectiveness in challenging scenarios. The analysis of different semi-supervised learning strategies shows that Co-Teaching and JointOpt sometimes perform worse than purely semi-supervised models, indicating compatibility issues with semi-supervised losses. Progressive filtering technique is compatible with various semi-supervised losses and outperforms its counterparts when combined with Entropy Learning. SELF framework outperforms all considered combinations by filtering out hard-to-learn training samples and leveraging ensemble predictions. It results in DNN models with superior generalization performance on CIFAR-10, CIFAR-100 & ImageNet, remaining robust under increasing noise ratio and network architecture changes. The Mean Teacher algorithm is applied in each iteration. The Mean Teacher algorithm is applied in each iteration of the training procedure, utilizing the assumption of natural robustness in deep networks. The algorithm kicks off the learning process by prioritizing correctly labeled samples over wrongly labeled ones, even in the presence of noise. The SELF algorithm prioritizes correctly labeled samples over wrongly labeled ones, reducing overfitting. It uses validation data for early stopping and assumes sufficient label randomness. The algorithm progressively expands correct label sets but may struggle with noise in tasks with many classes. Violating these assumptions can decrease model performance. The algorithm SELF reduces overfitting by prioritizing correctly labeled samples and expanding correct label sets. Violating assumptions can decrease model performance. The CIFAR-10 and CIFAR-100 datasets are evaluated in the pipeline with contaminated validation sets. Training uses standard configurations with specific parameters for optimization and learning rates. In the case of models without semi-supervised learning, a learning rate of 0.01 is set to prevent loss explosion. Additional hyperparameters are required for mean teacher training in CIFAR-10 and CIFAR-100 cases. The total batch size is 512, with 124 samples for labeled data and 388 for unlabeled data. Data normalization and real-time data augmentation are applied during training. The network used for evaluation were ResNet He et al. (2016) and Resnext Xie et al. (2017) for training. ResNext models have a cardinality of 32 and base width of 4 (32x4d). The initial learning rate is set to 0.1 with a single cycle of cosine annealing. Batch size is 40 with 20/20 for labeled and unlabeled samples. Consistency loss is set to mean-squared-error with a weight of 1000. The consistency loss is set to mean-squared-error with a weight of 1000. Consistency ramp up is applied for 5 epochs. Weight decay is set to 5e-5 and patience is four epochs for stopping training. Noisy samples are filtered using the topk=5 strategy instead of maximum likelihood predictions. Data augmentation includes normalization of RGB images, random rotation up to 10 degrees, resizing to 224x224, random horizontal flip, and random color jittering with specific settings. The training includes data augmentation with specific settings like brightness, contrast, saturation, and hue adjustments. Validation data is resized and cropped. The relationship between reweighting schemes and push-away-loss for wrongly labeled samples is discussed, highlighting the challenges of gradient ascent for these samples. The push-away-loss L P ush\u2212away is suggested for fair comparison with the framework, improving gradients to push the model away from potentially wrong labels. Entropy minimization encourages extreme predictions for each sample to balance predictions over all classes, expressing uncertainty about labels. The loss can be combined with a strict filtering strategy to remove potentially wrongly labeled samples. Amplifying network decisions for wrongly labeled samples with a large noise ratio could lead to even noisier models. Deleting samples from the training set leads to significant performance gaps compared to the strategy of considering removed samples as unlabeled data. Using the filtered samples continuously results in significantly better outcomes. The unsupervised-loss provides meaningful learning signals. The unsupervised-loss provides meaningful learning signals for better model training. Sample training curves of the approach SELF on CIFAR-100 with 60% and 80% noise show that the mean-teacher consistently outperforms the noisy student models, demonstrating the positive effect of temporal ensembling to counter label noise. The sample training processes of SELF under 60% and 80% noise on CIFAR-100 reveal that the mean-teacher consistently outperforms the student models, and with effective filtering strategy, both models slowly increase their performance towards 100% training accuracy. Progressive filtering helped eliminate inconsistency in the labels set."
}