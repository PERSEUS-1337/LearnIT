{
    "title": "BJk7Gf-CZ",
    "content": "We study the error landscape of deep linear and nonlinear neural networks with the squared error loss. For deep linear networks, necessary and sufficient conditions for global optimality are presented, which are efficiently checkable. Similar conditions are proven for deep nonlinear networks in a more limited function space setting. Deep neural networks have become popular since the advent of AlexNet, redefining the state-of-the-art in machine learning and artificial intelligence applications. Despite the challenges of optimizing neural networks due to nonconvexity, recent papers attempt to provide theoretical justifications for their empirical success in tasks like computer vision and natural language processing. Recent papers have provided theoretical justifications for the empirical success of neural networks in tasks like computer vision and natural language processing. One line of research relates neural networks to spherical spin-glass models, showing that larger networks have local minima close to global minima. Other results provide conditions under which a critical point of the empirical risk is a global minimum, but these are based on restrictive assumptions such as the width of hidden layers needing to be as large as the number of training examples. The recent paper BID4 provides a condition for global optimality in neural networks with parallel subnetworks and a regularizer to control the number of architectures. Deep linear neural networks simplify deep nonlinear networks by using linear activation functions. Optimization of deep linear models is nonconvex, with theoretical results emerging recently. BID0 showed in 1989 that some shallow linear neural networks have no local minima. BID0 demonstrated that certain shallow linear neural networks do not have local minima. BID8 and BID11 extended this to deep linear networks, proving that any local minimum is also a global minimum. Building on this, BID5 explored loss surfaces of deep linear residual networks, showing that every critical point is a global minimum in a near-identity region. The study focuses on deep linear and nonlinear networks, providing necessary and sufficient conditions for a critical point to be a global minimum. Theorem 2.1 and Theorem 2.2 provide conditions for global optimality in neural networks with wide hidden layers. Unlike BID8's existence result, we offer efficiently checkable conditions to distinguish between global minimum and saddle points during optimization. The curr_chunk discusses extending results on deep linear networks to deep nonlinear networks, presenting conditions for global optimality. Theorems 4.1 and 4.2 provide sufficient conditions for global optimality in deep nonlinear networks. The section describes problem formulation, notations for deep linear neural networks, and main results. The curr_chunk discusses the data matrix X, output matrix Y, hidden layers, weight matrices, and minimizing squared error loss in deep neural networks. Assumptions include dimensions of input and output matrices, full ranks of XX T and Y X T, and common conditions in supervised learning with deep neural networks. The curr_chunk discusses problems with deep neural networks, assumptions about singular values, notation for matrices, defining layers in neural networks, and presenting main results. The theorems presented in the curr_chunk provide necessary and sufficient conditions for critical points of deep linear neural networks to be globally optimal, distinguishing between global minima and saddle points. This is a significant advancement from previous works that only classified critical points as either global minima or saddle points without clear conditions. The theorems in the curr_chunk offer conditions for determining if critical points of deep linear neural networks are globally optimal, distinguishing between global minima and saddle points. This is a notable improvement from prior research that lacked clear conditions for classifying critical points. The theorems in the curr_chunk provide conditions for identifying global minima and saddle points in deep linear neural networks, building upon prior research that lacked clear classification criteria for critical points. The theorems in this section provide conditions for identifying global minima and saddle points in deep linear neural networks. The proof technique for handling degenerate critical points is simpler than previous methods. The globally optimal solution of a \"relaxation\" of L(W) is analyzed, which is useful for proving the theorems. The observation that the product W H+1 W H \u00b7 \u00b7 \u00b7 W 1 has rank at most k is crucial in the proofs. By restating this observation as an optimization problem, the solution is bounded below by a certain value. The solution of the problem in FORMULA1 is bounded below by the minimum value of a certain expression. When k is less than the minimum of d x and d y, the problem becomes non-convex, but its exact solution can still be computed easily. The optimal solution in this case involves the orthogonal projection of a certain matrix onto a space. The partial derivative of L(W) with respect to Wi is given in Lemma 3.2, which will be used in the proof of Theorems 2.1 and 2.2. Theorem 2.1 addresses the case where k = min{dx, dy}, and the unique minimum point of L0 has rank k. Any critical point in Vc1 must be a saddle point according to Kawaguchi (2016, Theorem 2.3.(iii)) and Lemma B.1. The proof outlines two cases: defining set W where any critical point is a global minimum, and proving that any critical point in Vc1 is also a critical point in W for some > 0, hence a global minimum. Proposition 3.4 states that for any > 0, set W is a global minimum point of L. Proposition 3.5 shows that for any point W in Vc1, there exists an > 0 such that W is in W. In this section, the proof of Theorem 2.2 is presented for the case where k < min{d x , d y }. The proof involves defining a new set W and showing that any critical point in Vc1 must be a saddle point. Additional notations are introduced to simplify the presentation, and it is shown that any point in V1 has rank k. Proposition 3.6 states that a tuple W \u2208 V 1 is a critical point of L if and only if A_pE = 0 and EB_p+1 = 0. This condition is necessary and sufficient for a global minimum in V 1. In this section, sufficient conditions for global optimality for deep nonlinear neural networks are presented via a function space view. The SVD decomposition is used to define matrices and critical points in V1 and V2, where global minimum solutions are found. The method described by Bartlett et al. (2017) decomposes a smooth nonlinear function mapping input to output. In this section, Bartlett et al. (2017) presented a method to decompose a smooth nonlinear function mapping input to output into multiple functions. By utilizing Fr\u00e9chet derivatives, they showed that critical points of the population risk are global minima when all functions are close to identity. This extends Theorems 2.1 and 2.2 of BID5 to deep nonlinear neural networks, providing conditions for global optimality in function space. In this section, random vectors X and Y are used to predict Y given X with a deep nonlinear neural network having H hidden layers. The goal is to minimize the population risk functional by obtaining functions h1, ..., hH+1. The minimizer of squared error risk is the conditional expectation of Y given X denoted as h*. The risk functional can be separated into two terms, with the optimal value L* being C under certain conditions. The function spaces are defined as DISPLAYFORM3 DISPLAYFORM4 where F i are defined for all i = 1, . . . , H + 1. Optimization of L(h) is done with h 1 \u2208 F 1 , . . . , h H+1 \u2208 F H+1, with assumptions on differentiability and sublinear growth. The assumption d i \u2265 min{d x , d y } for all i = 1, . . . , H + 1 is similar to k = min{d x , d y } in Theorem 2.1. Linear functional J[f ](x) maps a function \u03b7 \u2208 F i to a directional derivative. Theorems 4.1 and 4.2 provide conditions for a critical point in function space to be a global optimum. Theorems 2.1 and 2.2 offer necessary and sufficient conditions. Critical points must exist in the sets described in Theorems 4.1 and 4.2 for the claims to hold. The connection between function space and parameter space of neural network architectures is an open research direction. Understanding this relationship is crucial for determining global optimality. The Jacobian matrix being full rank is a sufficient condition for global optimality, as it allows for linear approximation of nonlinear functions. In this section, additional notation is introduced for the proofs related to the linear approximation using Jacobians in neural network architectures. Different kinds of norms are discussed, including the 2 norm for vectors and the operator norm for matrices. A \"generalized\" induced norm for nonlinear functions with sublinear growth is defined. The norm \u00b7 nl is defined for nonlinear functions. Given a linear functional G mapping a function f to a real number, define the operator norm. The proof of Theorems 4.1 and 4.2 involves checking if a particular direction is in F 1.\u03b7 is differentiable if A(X) is decomposed with SVD. If A(X) is decomposed with SVD, \u03a3 = [\u03a3 1 0] and \u03b7 \u2208 F 1. For any X \u2208 R dx, we have \u03b7(0) = 0. The function A(w) is differentiable and invertible, ensuring \u03b7 \u2208 F j. The function A(w) is differentiable and invertible, ensuring \u03b7 is differentiable. By assumption, for every x \u2208 R dx, the components of h 1 (x) are identical to h * (x), and all other components are zero. Additionally, J[h H+1:2 ](z) is all 0 except 1's in diagonal entries, making h H+1:2 (z) twice-differentiable. Lemma B.1 states that given data matrices X and Y, with X and Y having full ranks, minimizing the empirical squared error risk leads to critical points that are saddle points. The proof is divided into two cases: when the product of weight matrices is zero and when it is not. The proof shows that any critical point cannot be a local maximum, but can be a local minimum or a saddle point. By examining the Hessian of L(W) with respect to vec(W^T H+1), it is shown that the Hessian matrix is positive semidefinite. When W H \u00b7 \u00b7 \u00b7 W 1 = 0, there exists a strictly positive eigenvalue in H(W), indicating an increasing direction and ruling out W as a local maximum. The case of W H \u00b7 \u00b7 \u00b7 W 1 = 0 requires more careful treatment due to degenerate critical points. The algorithm perturbs matrices W 1 to W H+1 by sampling perturbations from Frobenius norm balls. The procedure ensures full rank matrices with probability 1, leading to a non-zero \u2206 defined by \u2206 = W H+1 \u00b7 \u00b7 \u00b7 W i * +1 \u2206 i * V i * \u22121 \u00b7 \u00b7 \u00b7 V 1. The condition (1) = (V 1, . . . , V ) must hold. The algorithm perturbs matrices W 1 to W H+1 by sampling perturbations from Frobenius norm balls to ensure full rank matrices. The condition (1) = (V 1, . . . , V ) must hold. In case of k < min{d x , d y }, the loss function can be decomposed, leading to a problem of minimizing RX \u2212 Y X T (XX T ) \u22121 X 2 F subject to the rank constraint rank(R) \u2264 k. The optimal solution is the k-rank approximation of Y X T (XX T ) \u22121 X. The unique global minimum solution of the problem is the k-rank approximation of Y X T (XX T ) \u22121 X, where \u00db is unique due to the assumption that all singular values are distinct. This solution is obtained when k < min{d x , d y }."
}