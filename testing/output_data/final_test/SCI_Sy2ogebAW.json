{
    "title": "Sy2ogebAW",
    "content": "In this work, a novel method is proposed to train a neural machine translation system in an unsupervised manner, using only monolingual corpora. The model, based on unsupervised embedding mappings, utilizes denoising and backtranslation techniques to achieve good results without the need for parallel data. The system achieves 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation tasks. The model for French-to-English and German-to-English translation achieved 21.81 and 15.24 points with 100,000 parallel sentences. Neural machine translation (NMT) has advantages over traditional statistical machine translation (SMT) due to continuous representations and larger contexts. NMT requires a large parallel corpus to be effective and can fail with insufficient training data. The lack of large parallel corpora is a practical problem for most language pairs, including low-resource languages like Basque and major language combinations like German-Russian. Authors have tried pivoting, triangulation, and semi-supervised approaches to address this issue, but these methods still need a strong cross-lingual signal. A novel method is proposed in this work to train NMT systems in an unsupervised manner, relying only on monolingual corpora and utilizing unsupervised cross-lingual embeddings. The system is trained with monolingual data to reconstruct its input by introducing noise in the form of random token swaps. Denoising and backtranslation are incorporated in the training process, alternating between sentences in two languages to optimize encoding and decoding with shared encoders and decoders. BID28 is included in the training procedure to further enhance results. The proposed system utilizes monolingual training data with noise introduction for reconstruction. BID28 is integrated to enhance results. Experimental results show significant BLEU point improvements for French \u2192 English and German \u2192 English translations. Combining this method with a small parallel corpus further boosts performance. Manual analysis confirms the system's ability to learn complex translation relations beyond word substitution. The paper is structured with an analysis of related work, description of the proposed method, and discussion of experimental settings. The paper discusses unsupervised cross-lingual embeddings and statistical decipherment for building a machine translation system. Methods for learning cross-lingual word embeddings are explored, including embedding mapping using monolingual corpora and a linear transformation based on a bilingual dictionary. BID1 proposes a self-learning extension for cross-lingual word embeddings using a bilingual dictionary, while adversarial training is also suggested for unsupervised learning. Statistical decipherment techniques are used to induce a machine translation model from monolingual data, treating the source language as ciphertext. The English generative process is modeled using n-gram language model and channel model parameters estimated using expectation maximization or Bayesian inference. Incorporating syntactic knowledge and word embeddings has shown significant improvements in statistical decipherment for machine translation. Various proposals exist to train NMT systems using resources other than direct parallel corpora, such as leveraging a third language when two languages lack direct parallel data. The use of advanced models like a teacher-student framework and multilingual extensions in NMT has shown improvements. Backtranslation of monolingual corpora in the target language and training NMT systems to directly copy target language text are effective strategies. The proposed unsupervised NMT approach involves training two agents to translate in opposite directions without using any parallel data. This method is different from previous approaches that require a parallel corpus for a warm start. The proposed unsupervised NMT approach involves training two agents to translate in opposite directions without using any parallel data. The system follows an encoder-decoder architecture with an attention mechanism, using two-layer bidirectional RNNs with GRU cells and global attention. The system differs from standard NMT in three critical aspects to enable unsupervised training. The proposed unsupervised NMT approach involves training two agents to translate in opposite directions without using parallel data. It utilizes a shared encoder with fixed cross-lingual embeddings to create language-independent representations for translation. The unsupervised NMT approach involves training agents to translate without parallel data, using fixed cross-lingual embeddings to create language-independent representations. Separate vocabularies are used for each language, allowing for different vectors for the same word in different languages. The unsupervised NMT approach involves training agents to translate without parallel data, using fixed cross-lingual embeddings to create language-independent representations. On-the-fly backtranslation is proposed to train the system in a true translation setting without violating the constraint of using only monolingual corpora. This involves using the system in inference mode with greedy decoding to translate input sentences between two languages, creating pseudo-parallel sentence pairs for training. The proposed architecture utilizes on-the-fly backtranslation during training to improve synthetic sentence pairs, alternating between denoising and backtranslation mini-batches for two languages L1 and L2. This approach, combined with access to a small parallel corpus, enhances the model in each iteration. The system can be trained in a semi-supervised fashion by predicting translations in a small parallel corpus. Experiments are conducted using French-English and German-English datasets from WMT 2014. The system is evaluated on newstest2014 using tokenized BLEU scores. Three training settings are tested: unsupervised with access to monolingual corpora, semi-supervised with a small parallel corpus, and supervised with additional in-domain parallel data. In a semi-supervised setting, the system is trained by predicting translations in a small parallel corpus. The traditional scenario in NMT involves access to a large parallel corpus for training. The experiments include using different amounts of parallel data from News Commentary and other corpora for evaluation. In a semi-supervised setting, the system is trained using a small parallel corpus for translation predictions. In separate experiments, subsets of News Commentary were used without parallel data for development or tuning. Spanish-English WMT data was used for preliminary experiments, with hyperparameters decided without rigorous exploration. Corpus preprocessing involved tokenization, truecasing, and byte pair encoding (BPE) for learning on monolingual corpora independently. The effectiveness of BPE in overcoming rare word problems in standard NMT was considered, especially in challenging unsupervised scenarios. In unsupervised scenarios, training is accelerated by limiting vocabulary to the top 50,000 tokens and discarding sentences with more than 50 elements. Pre-trained cross-lingual embeddings are used in the encoder, and word2vec BID25 is employed to independently train embeddings for each language. The embeddings are then mapped to a shared space using a recommended configuration. The proposed system utilizes numeral-based initialization to train embeddings in a shared space. The system also includes a simple baseline for word-by-word translation using nearest neighbors. Training is done with cross-entropy loss, a batch size of 50 sentences, Adam optimizer with a learning rate of \u03b1 = 0.0002, and dropout regularization with p = 0.3. Unsupervised systems explore denoising and backtranslation, without parallel data, over a fixed number of iterations. The system trains embeddings in a shared space using numeral-based initialization. Training includes cross-entropy loss, batch size of 50 sentences, Adam optimizer with learning rate \u03b1 = 0.0002, and dropout regularization with p = 0.3. Unsupervised systems explore denoising and backtranslation without parallel data over 300,000 iterations. Training each system took 4-5 days on a single Titan X GPU. Inference at test time was done using beam-search with a beam size of 12. No length or coverage penalty was used. Quantitative results are discussed in Section 5.1, and qualitative analysis in Section 5.2. BLEU scores for all variants are reported in Table 1. The proposed unsupervised system achieves strong BLEU scores in French-English and German-English, surpassing the baseline system with improvements of up to 140%. Backtranslation is crucial for the system's success, as shown by the results in Table 1. The proposed unsupervised system achieves strong BLEU scores in newstest2014, surpassing the baseline with improvements of up to 140%. Backtranslation is crucial for the system's success, as shown by the results in Table 1. Test perplexities also confirm the effectiveness of backtranslation in reducing perplexity compared to denoising alone. Both denoising and backtranslation are essential during training, with denoising capturing broad word-level equivalences and backtranslation encouraging the system to learn subtle relations. Subword translation with BPE is slightly beneficial for German, detrimental for French, and practically equivalent for English as the target language. This is surprising considering BPE's handling of rare words, but closer inspection reveals its effectiveness in certain scenarios. The use of Byte Pair Encoding (BPE) in translation systems can lead to correct translations of rare words but also introduces errors, such as combining subword units from rare words with properly translated words. BPE is less helpful in translating infrequent named entities, making it challenging for unsupervised learning procedures. Improving the handling of rare words, named entities, and numerals could enhance translation results in the future. Additionally, the semi-supervised system shows that the proposed model can benefit greatly from these improvements. The proposed model benefits greatly from a small parallel corpus in the semi-supervised system. Using 10,000 or 100,000 parallel sentences from News Crawl improves translation quality by 1-7 BLEU points. These results outperform a comparable NMT system trained on the same data, indicating the potential of this approach beyond unsupervised scenarios. The proposed model benefits from a small parallel corpus in the semi-supervised system, outperforming a comparable NMT system trained on the same data. The total number of deaths in October is the highest since April 2008. The total number of deaths in October is the highest since April 2008, with 1,073 people killed. The provinces in France, except for opera, lack cultural support. A comparable NMT model using the proposed architecture performs poorly compared to the state of the art in NMT. The comparable NMT model's poor performance suggests that additional constraints in the system, introduced for unsupervised learning, may be limiting its potential. The system could be improved by gradually relaxing these constraints during training, such as updating the weights of the encoder embeddings and decoupling fixed cross-lingual embeddings. The system could be enhanced by adjusting encoder embeddings and decoupling shared encoders during training. Further improvements can be made by exploring hyperparameters, using larger models, longer training times, and incorporating NMT techniques like ensembling and length/coverage penalty. Manual analysis shows high-quality translations with the proposed system. The proposed system for translation shows high-quality translations for complex sentences but lags behind standard supervised NMT systems in overall translation quality. It struggles to preserve concrete details from source sentences, as seen in examples where certain words are mistranslated. The proposed translation system struggles with adequacy issues, such as mistranslating words like \"octobre\" as \"May\" and numbers. Incorporating character level information may help improve translations. There are cases where fluency and adequacy problems hinder understanding the original message in the proposed translation. The text proposes a novel method for training an NMT system in an unsupervised manner, utilizing cross-lingual embeddings in an attentional encoder-decoder model. Experimental results show significant improvements in BLEU score compared to a baseline system, demonstrating the system's ability to model complex cross-lingual relations effectively. The study demonstrates the effectiveness of a novel method for training an NMT system using cross-lingual embeddings in an attentional encoder-decoder model. Results show improved translations and suggest potential for further enhancements by relaxing constraints during training and incorporating character-level information. The study suggests incorporating character-level information into the model to address adequacy issues observed in manual analysis. Additionally, exploring different neighborhood functions for denoising and analyzing their impact on typological divergences of language pairs is recommended."
}