{
    "title": "HyxOIoRqFQ",
    "content": "Each training step for a variational autoencoder (VAE) requires sampling from simple approximate posteriors to fully utilize GPU parallelism. However, these simple posteriors often lack statistical dependencies. To address this, we introduce a fast, parallel sampling method for autoregressive distributions based on fixed-point iterations, enabling efficient variational inference in discrete state-space models. To optimize the variational bound, two ways were considered to evaluate probabilities in models: inserting relaxed samples directly into the pmf for the discrete distribution or converting to continuous latent variables. Converting to continuous latent variables showed a considerable scope for mismatch between true and approximate posteriors, leading to biased inferences. The approach was tested on inferring spiking activity from noisy calcium-imaging data in neuroscience, providing accurate connectivity estimates in significantly less time. The development of variational auto-encoders (VAE) has enabled Bayesian methods to be applied to large-scale data by leveraging neural networks trained to approximate the posterior using stochastic gradient ascent. To optimize the variational bound, converting to continuous latent variables can lead to biased inferences due to mismatch between true and approximate posteriors. Normalizing flows can introduce correlations into the approximate posterior by transforming variables from a simple distribution to a complex correlated distribution. This approach allows for efficient sampling using parallel GPU computations in VAE models. The normalizing flow approach introduces correlations into the approximate posterior by transforming variables from a simple distribution to a complex correlated distribution. This is particularly challenging for problems requiring discrete latent variables and correlated posteriors, such as inferring correlated spiking activity in neuroscience. The study addressed challenges with discrete latent variables and correlated approximate posteriors by considering two approaches: normalizing flows and a fast-sampling procedure for discrete autoregressive posteriors. The normalizing flow approach introduced correlations into the approximate posterior but led to biased inferences. Instead, a fast-sampling procedure was developed to sample the true posterior without the need for slow sequential sampling. The study introduced a fast-sampling procedure for discrete autoregressive posteriors, which converges rapidly and exploits GPU parallelism. This method allows sampling from autoregressive approximate posteriors almost as quickly as factorised posteriors, and much faster than sequentially sampling from the underlying autoregressive process. This enables the benefits of correlated posteriors in large-scale settings. The reparameterisation trick is used to optimize expressions involving recognition parameters in stochastic gradient descent. It is effective for continuous latent variables but not for discrete latents, which can be relaxed to form an equivalent model with continuous random variables for gradient back-propagation. In the context of optimizing expressions with recognition parameters, the reparameterization trick is effective for continuous latent variables but not for discrete latents. Discrete latents can be relaxed to form an equivalent model with continuous random variables for gradient back-propagation. This involves using a logistic distribution to sample a relaxed version of the original discrete latent variable. The approach involves inserting relaxed variables into the original probability mass function for the discrete model to obtain a valid variational bound. However, using a different expression for evaluating the probability density of samples from the approximate posterior may not provide a valid bound. To ensure validity, the actual probability of the relaxed variable needs to be computed by evaluating Q(\u1e91) and P(\u1e91). Working directly with \u1e91 is numerically unstable, so an alternative method is used. The approach involves using logistic variables instead of working directly with \u1e91 to ensure numerical stability. The generative model is described, but the approximate posterior Q(l) is not specified yet. A Logistic distribution is used for the approximate posterior with learned mean and scale parameter. Different inverse-temperatures can be used to obtain either \u1e91 or z from the same l. The focus is on discrete dynamical systems with autoregressive generative models and approximate posteriors. In discrete dynamical systems, autoregressive generative models use Logistic variables for approximate posteriors. Sampling the autoregressive posterior involves using Logistic noise and length N vectors. Fixed-point iterations are considered for GPU parallelism. Fixed-point iterations utilizing GPU parallelism apply autoregressive updates in parallel across all time-steps. The iterations converge to the autoregressive model after K = T iterations. Despite the worst-case scenario being slow, in practice, the iterations reach steady-state rapidly. In practice, fixed-point iterations using GPU parallelism converge rapidly to the autoregressive model after K = T iterations, leading to a significant efficiency improvement. The procedure can be extended to categorical variables by converting sigmoid to softmax and logistic to Gumbel distribution. Sampling from the approximate posterior is efficient, but evaluating probabilities introduces a mismatch between the sampled distribution and the actual distribution. The text discusses the mismatch between the distribution used for sampling and the distribution for evaluating probabilities in fixed point iterations. Convergence is well-defined for discrete models but more challenging for relaxed models. Normalizing flows are used to compute the probability density of continuous variables during fixed point iterations. Normalizing flows exploit the ability to compute the probability density of a random variable by transforming a sample through a one-to-one function. By using a restricted family of transformations, the Jacobian can be simplified to 1, ensuring that fixed-point iterations lie within this family. Continuous latent variables allow for computing the approximate posterior probability even before convergence of fixed point iterations. Moving from discrete to continuous latent variables introduces a mismatch between the true and approximate posterior, leading to discrepancies in the evidence and biased inferences. This discrepancy grows as evidence for specific values increases, potentially modifying the variational inference objective function. The approximate posterior underestimates evidence for z = 0 or z = 1, especially when optimized with relaxed binary variables. Using a Gaussian likelihood, as the variance decreases, evidence that z is close to 0.5 increases. By combining relaxation with a Logistic approximate posterior, the variance can be reduced enough for the relaxed Bernoulli variable to be close to 0.5. The standard deviation of the binary latent variables is crucial for calcium spike deconvolution, where inferring spikes from noisy optical observations is necessary. The generative model includes inputs from past spikes filtered by temporal kernels, with inferred rates and spiking patterns compared between true posterior, VAE, and supervised models. The study compared factorised posteriors trained using VAE and supervised procedures, along with a new discrete flow trained using VAE. It also analyzed the true marginal probability of spike presence in simulated data, the number of inferred spikes, and the time course of the VAE objective under different models. The recognition model was adjusted to capture prior-induced statistical structure. In an experiment with a single neuron, spiking was IID (Poisson) with a firing rate of 0.25 Hz, and fluorescence data was simulated by convolving spikes. The recognition model consists of a neural network mapping data to spike inferences and a recognition temporal kernel capturing anticorrelations induced by explaining away. Different strategies for training, including factorised and autoregressive VAE's, give similar reconstructions. The factorised VAE had narrow posteriors indicating high certainty in spike timing, while supervised and autoregressive VAEs showed higher temporal uncertainty. Differences were evident in sampled spike trains and calibration. The autoregressive VAE combines timing uncertainty with reasonable spike counts, outperforming factorised methods in terms of the IWAE objective. It is crucial to compute probabilities using discrete pmf and utilize parallel fixed point iteration based flow for good performance. In the second experiment, synaptic connectivity between cells was inferred using a network of 100 cells with sparse weights drawn from a Gaussian distribution. An autoregressive, fully connected recognition model was used with realistic parameters, showing improved performance compared to factorised methods. The study compared three methods for inferring synaptic weights: factorised VAE, autoregressive posterior with fast-flow sampling, and autoregressive posterior with slow sequential sampling. The fast-flow sampling method showed better correlation and reduced bias in estimating weights, leading to a significant increase in ELBO compared to the other methods. This improvement was attributed to the faster training iterations of the fast-flow sampling approach. The approach described involves sampling from a discrete autoregressive distribution using a parallel, flow-like procedure to speed up sampling from autoregressive approximate posteriors in variational inference training. This allows for rapid learning of autoregressive posteriors in neural data analysis, benefiting single and multi-cell data in reasonable timescales. The use of fixed-point iterations introduces approximation in evaluating the probability of a sample once it has converged. Our approach involves using fixed-point iterations to evaluate the model with samples drawn from the underlying discrete, autoregressive approximate posterior. This method can be preferable for long-range temporal dependencies when exact inference is not possible. Additionally, future work could explore the effectiveness of different normalizing flows for defining approximate posteriors in continuous state-space models. Our approach involves using fixed-point iterations to evaluate the model with samples drawn from the underlying discrete, autoregressive approximate posterior. While converting a discrete latent variable model to one with continuous latents can introduce mismatch between the prior and approximate posterior, designing more closely matching approximate posteriors or relaxation schemes may lead to gains in performance."
}