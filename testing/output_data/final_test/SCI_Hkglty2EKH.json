{
    "title": "Hkglty2EKH",
    "content": "A framework for efficient Bayesian inference in probabilistic programs is introduced by embedding a sampler inside a variational posterior approximation. It offers ease of implementation and automatically tunes sampler parameters to speed up mixing time. Various strategies for approximating the evidence lower bound (ELBO) computation are presented, along with experimental evidence from tasks such as density estimation, influence diagram solving, and state-space modeling for time-series data. The framework utilizes a probabilistic program to define a distribution p(x, z) and addresses queries involving the posterior p(z|x), leveraging probabilistic programming languages (PPLs). The text introduces a framework for efficient Bayesian inference in probabilistic programs by embedding a sampler in a variational posterior approximation. It aims to automatically adapt the posterior shape and tune sampler parameters for faster mixing time, addressing the challenge of intractable distributions in probabilistic programming languages. Various strategies for approximating the evidence lower bound (ELBO) computation are discussed, with experimental evidence from tasks like density estimation and state-space modeling for time-series data. The framework enhances variational posterior approximation by embedding a sampler to automatically adapt the posterior shape and tune parameters for Bayesian inference in probabilistic programs. It offers a more flexible and unbiased variational approximation, improving the initial approximation with a stochastic process. The refined variational approximation, Q \u03b7,T (z|z 0 ), evolves the original density q 0,\u03c6 (z|x) to better approximate the exact posterior. By maximizing the refined ELBO objective, the divergence KL(q \u03c6,\u03b7 (z|x)||p(z|x)) is optimized. Different families of Q \u03b7,T (z|z 0 ) are considered for this purpose. The evolving density is evaluated using different families of sampling algorithms. For continuous latent variables, the original variational density is evolved through a stochastic diffusion process using the SGLD sampler. The sampler iterates T times, with the only parameter being the learning rate \u03b7. The noise for the SGLD is Gaussian with mean 0 and variance 2\u03b7I. The initial variational distribution is a Gaussian parameterized by a deep neural network, and T iterations of a sampler parameterized by \u03b7 are applied to obtain q \u03c6,\u03b7. The evolving density is evaluated using different families of sampling algorithms for continuous latent variables. The initial variational distribution is refined with just stochastic gradient descent (SGD) or Stein variational gradient descent (SVGD) to promote exploration of the latent space. Guidelines for ELBO optimization using the refined variational approximation are proposed. Particle approximation involves sampling a finite set of particles from the flow Q \u03b7,T (z|z 0 ) and computing the entropy for each factor. In variational inference, the variational approximation q(z|x; \u03c6) is parameterized by \u03c6 and learned using SGD or Adam. Embedding a sampler inside the variational guide allows computing gradients with respect to the sampler parameters \u03b7, enabling gradient computation for the learning process. The VIS framework incorporates a sampler to compute gradients with respect to the learning rate \u03b7 for optimal step size at each VI iteration. Code is available at https://github.com/vicgalle/vis. The framework was implemented using Pytorch, with a notebook for Jax implementation. Additional experiments and implementation details are in Appendices B and C. A preliminary experiment tests the framework on a synthetic complex target distribution defined as a bi-dimensional density. Variational approximation is done using a diagonal Gaussian. The VIS framework refines the variational approximation using SGLD for T = 1 steps, achieving a tighter bound. Testing on Mauna Loa CO2 data, a DLM model with a local linear trend and seasonality is used. Time series is standardized for preprocessing. The VIS framework refines the variational approximation using SGLD for T = 1 steps, achieving a tighter bound. Testing on Mauna Loa CO2 data, a DLM model with a local linear trend and seasonality is used. Time series is standardized for preprocessing. To guarantee the same computational budget time, the model without refining is run for 10 epochs, whereas the model with refinement is run for 4 epochs. We report mean absolute error (MAE) and predictive entropy in Table 1. The refined model achieves lower MAE and narrower predictive intervals compared to the non-refined counterpart in a Variational Autoencoder (VAE) model. The VAE introduces a variational approximation q \u03c6 (z|x) to learn parameters \u03b8 for two image distributions, MNIST and fashion-MNIST. Using the MC approximation with various values of T, we achieve similar results as the Gaussian approximation. Results are compared with Titsias and Ruiz (2019) in Table 2, where VIS-5-10 variant is trained for fewer epochs but with increased MCMC iterations T for improved performance on test. The VIS framework improves test log-likelihood by increasing MCMC iterations T, with a moderate increase in computing time. Results are compared with contrastive divergence approach. The framework allows tuning of SG-MCMC sampler parameters and adapting initial distributions and learning rate for wide classes of models. The VIS framework introduces approximations for intractable parts of variational approximations, improving MCMC mixing time. Previous work focused on preconditioning the posterior distribution, while our proposed method includes tuning sampler parameters. Normalizing flows offer a flexible framework for variational distributions. The normalizing flows framework enhances the flexibility of variational distributions in approximate Bayesian inference. It overcomes the instability issue of using generative adversarial networks (GANs) in high-dimensional spaces. The framework is compatible with various optimizers and does not rely on Langevin dynamics. Our variational approximation, similar to VAE learned using HMC, uses a compound distribution and allows for any SG-MCMC sampler. We address the amortization gap issue by evolving a set of strategies for entropy approximation. The VIS framework addresses the amortization gap issue by evolving particles in the latent space, reducing bias from initial approximation. It uses reverse KL divergence for efficient approximation and can handle general probabilistic graphical models. Influence diagrams are commonly used in decision analysis. The proposed scheme showcases flexibility in solving inference problems, using a conditional VAE model for classification tasks in a high-dimensional setting like the MNIST dataset. The cVAE extends the VAE model by conditioning it on a discrete variable y, defining a decoder distribution p \u03b8 (x|z, y) on an input space x \u2208 R D given class label y \u2208 Y and latent variable z \u2208 R d. Variational posterior is learned as an encoder q \u03c6 (z|x, y) from a prior p(z) \u223c N (0. The proposed scheme introduces a conditional VAE model for classification tasks, extending the VAE model by conditioning it on a discrete variable y. A variational posterior is learned as an encoder q \u03c6 (z|x, y) from a prior p(z) \u223c N (0, I). The generative model is used as a classifier using Bayes rule, with experiments conducted by changing T for the transition distribution Q \u03b7,T in the refined variational approximation. Test accuracy results are reported in Table 3, with different values of T used during training and testing phases. The model with T tr = 5 was trained for 10 epochs, while the other settings for 15 epochs to ensure similar training times. Results are averaged from 3 runs with different random seeds, showing the importance of using the refined variational approximation for higher accuracy. Learning a good initial distribution and inner learning rate has a positive impact on accuracy. Results without learning an initial distribution were much worse, suggesting the importance of the variational approximation. In this subsection, variational approximation is used on two state-space models for discrete and continuous data. The model equations involve Categorical and Gaussian distributions for emission and transition probabilities. Inference is performed on the parameters \u03b8 using a PPL based on Pytorch autodiff framework. Synthetic datasets are generated for each model. The full model implementations can be found in Appendix C.1. The text discusses using variational approximation with synthetic datasets on two state-space models for discrete and continuous data. Results show that VIS (T > 0) outperform regular optimization with VI (T = 0) in terms of loglikelihood evolution and optimization time. Model details are provided in Appendix C.1.1, and results are shown in Figure 3. The text discusses using variational approximation with synthetic datasets on two state-space models for discrete and continuous data. Results show that VIS (T > 0) outperform regular optimization with VI (T = 0) in terms of loglikelihood evolution and optimization time. The refined model achieves higher accuracy and confidence in predictions compared to the model without refinement. The DLM model consists of a linear trend component and a seasonal block with a period of 12. The seasonal component cycles the state vector at each timestep. The VAE model is implemented with an encoder and decoder for processing data. The VAE model is implemented with PyTorch for latent variables z. The optimizer Adam is used with a learning rate of 0.001. Training is done for 15 epochs (fMNIST) and 20 epochs (MNIST). The cVAE model is also implemented with PyTorch for latent variables z. The decoder and encoder distributions in the VAE model are parameterized by neural networks. Variational inference is performed with a refined guide to optimize a tighter ELBO. The optimizer Adam is used with a learning rate of 0.01. The VAE model uses neural networks to parameterize decoder and encoder distributions. Variational inference is optimized with a refined guide to tighten the ELBO, using the Dirac Delta approximation for q(z|z 0 ). This modified objective is equivalent to the refined ELBO introduced in previous work. The optimization framework allows for clear implementation in any PPL supporting algorithmic differentiation. In the VIS framework, optimization is done by maximizing log p(x, z + \u2206z) instead of log p(x, z). A first-order Taylor expansion is used to approximate the refined objective, resulting in a refined gradient that includes a second-order correction. This correction, adapted by the chosen sampler, can lead to faster optimization convergence. In the optimization process within the VIS framework, utilizing a second-order correction through gradients with the \u2206z term can lead to quicker convergence with lower losses. Two variants of the ELBO objective, Full AD and Fast AD, are described for tuning sampler parameters and optimizing computational efficiency. In the VIS framework, optimization involves marginalizing out latent variables for reduced variance. Linear-Gaussian models allow for exact posterior computation, while non-linear models require gradient estimators or Delta approximation for inference."
}