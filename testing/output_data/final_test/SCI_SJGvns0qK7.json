{
    "title": "SJGvns0qK7",
    "content": "Addressing uncertainty is crucial for autonomous systems to adapt robustly in the real world. The problem of model uncertainty is formulated as a continuous Bayes-Adaptive Markov Decision Process (BAMDP). An agent maintains a posterior distribution over model parameters and maximizes long-term reward based on this belief. The Bayesian Policy Optimization algorithm enhances policy optimization by learning a universal policy that balances exploration and exploitation to maximize the Bayesian value function. A new policy network architecture is proposed to handle challenges from discretizing the continuous parameter space. This method outperforms algorithms that do not consider belief distributions and is competitive with state-of-the-art approaches. The Bayes-Adaptive Markov Decision Process (BAMDP) framework BID9 elegantly captures the exploration-exploitation dilemma that the agent faces in real-world robotics, where uncertainty is a key challenge. The agent maintains a belief, a posterior distribution over unknown parameters, to maximize long-term reward through exploration and exploitation. The BAMDP framework captures the exploration-exploitation dilemma in robotics by maintaining a belief over latent parameters. It can be seen as a POMDP where planning in belief space balances explorative and exploitative actions. The focus is on BAMDPs with discrete or bounded continuous parameter spaces, represented by a categorical distribution. The challenge lies in exploring the belief space efficiently, especially when discretizing the latent space leads to exponential growth in the belief vector size. Approximating the value function over the reachable belief space can be challenging due to the large belief vector size. Point-based value approximations have been successful for discrete POMDP problems but struggle with continuous state-action spaces. Monte-Carlo Tree Search is expensive in continuous spaces. The key insight is to learn a policy directly mapping beliefs to actions using batch policy optimization algorithms, examining model uncertainty through a BAMDP lens. Bayesian Policy Optimization (BPO) is a batch policy optimization method that utilizes a black-box Bayesian filter and augmented state-belief representation to learn policies directly reasoning about model uncertainty in continuous BAMDPs. The algorithm simulates the policy on multiple latent models sampled from the source distribution and updates the posterior belief along the simulated trajectory in each sampled model. This approach offers a practical and scalable solution for continuous BAMDPs. The Bayesian policy optimization algorithm introduces a method to reason about model uncertainty in BAMDPs and continuous POMDPs. It utilizes encoder networks to balance belief and state embeddings in the policy network. Through experiments, it outperforms algorithms that do not explicitly reason about beliefs and is competitive with state-of-the-art POMDP algorithms. The Bayes-Adaptive Markov Decision Process framework was originally proposed to address uncertainty in MDP transition functions. The BAMDP framework addresses uncertainty in the transition function and reward function using latent variables. It defines a tuple with observable state space, latent space, action space, parameterized transition and reward functions, initial distribution, and discount factor. The transition function is defined as a conditional probability distribution. Bayesian Reinforcement Learning focuses on long-term expected rewards. Bayesian Reinforcement Learning (BRL) maximizes the Bayesian value function considering uncertainty over latent variables \u03c6. A belief distribution b \u2208 B is maintained recursively with a Bayes filter for posterior updates. The partially observable BAMDP is cast as a fully observable MDP in belief space for policy gradient methods. A reactive Bayesian policy in belief space is equivalent to a policy with memory in observable space. In belief space, memory complexity is handled by a Bayes filter computing a history statistic. In partially observable MDPs, states are observed through a noisy function. BAMDPs and MOMDPs have observable and latent states. Uncertainty in BAMDPs comes from the transition function. Research on belief-space reinforcement learning and robust reinforcement learning has been extensive. Relevant work is highlighted, with more detailed reviews available. Belief-Space Reinforcement Learning focuses on planning in belief space, where the state representation includes a belief distribution. Various approximate solvers address the curse of dimensionality by using value function approximation, compact belief representation, or direct belief-to-action mapping. QMDP assumes full observability after one step, while point-based solvers like SARSOP and PBVI exploit the structure of POMDP value functions. Sampling-based approaches such as BAMCP and POMCP combine Monte Carlo sampling and simple rollout policies. Model-based trajectory optimization methods like BID28 and BID40 have been successful for navigation on unmanned aerial vehicles and mobile robots. Neural network variants of POMDP algorithms, such as QMDP-Net BID14, are effective in compressing high-dimensional belief states. Deep Variational Reinforcement Learning BID12 uses variational inference and a particle filter to approximate beliefs and generate actions. Exp-GPOMDP BID1 is a model-free policy closely related to our method. Our method, BPO, leverages model knowledge from BAMDP and uses LSTM to encode observations for action generation. Unlike Peng et al., BPO explicitly utilizes belief distribution for improved data efficiency. Robust Reinforcement Learning can find a robust policy by maximizing return for worst-case scenarios. The approach of BID19 uses a min-max objective similar to H-infinity control from robust control theory. Recent works have adapted this objective to train agents against external disturbances and adversarial scenarios. Ensemble model approaches like EPOpt and Ensemble-CIO aim to improve worst-case performance by training on multiple MDPs. However, these methods may result in overly conservative behavior in extreme worst-case scenarios and perform poorly without inferring uncertainty. Our approach maintains a belief distribution internally, leading to superior policies compared to robust policies in various scenarios. Adaptive policy methods like Adaptive-EPOpt and PSRL can adapt to changing model estimates without operating in belief space. Universal Policy with Online System Identification (UP-OSI) learns to predict the maximum likelihood estimate and trains a universal policy accordingly. Bayesian Policy Optimization (BPO) is a policy gradient algorithm for Bayesian Adaptive Markov Decision Processes (BAMDPs). It trains a stochastic Bayesian policy that maps state-belief pairs to action probability distributions. BPO collects trajectories by simulating the policy on multiple MDPs sampled from a prior distribution, updating belief distributions with a Bayes filter. By observing state-belief evolution across different MDPs, BPO improves performance over various scenarios. Bayesian Policy Optimization (BPO) observes the evolution of state-belief representations in multiple trajectories. The key challenge is representing belief distributions over the latent state space with fixed-size vectors, such as categorical distributions for discrete spaces or Gaussian distributions for continuous spaces. To address the challenge of representing belief distributions over latent state spaces, a new policy network structure with separate state and belief encoders is introduced. These encoders output compact representations that are concatenated and used as input to the policy network, which is jointly trained using batch policy optimization. The belief encoder aims to achieve robustness by learning to compactly represent beliefs. The belief encoder in the new policy network structure aims to achieve robustness by learning to compactly represent arbitrarily large belief distributions. Empirical verification in Section 5 shows that this separate belief encoder enhances the algorithm's robustness to large belief representations. The Bayes filter recursively updates the posterior belief distribution given the history of states and actions, enabling the algorithm to produce robust policies that scale to high-dimensional observable states and beliefs. The algorithm utilizes a black-box Bayes filter to produce a posterior distribution over latent states. It discretizes the latent space into bins and forms an MDP from the mean bin values for each latent parameter, approximating the belief distribution with a categorical distribution over the resulting MDPs. The algorithm approximates the belief distribution with a categorical distribution over MDPs and computes the probability of observing states under discretized parameters. It is robust to approximate beliefs and can be applied to POMDPs by removing the state encoder network and constructing a Bayes filter to compute beliefs over hidden states. BPO optimizes the Bellman equation for POMDPs with large state spaces, emphasizing beliefs with conjugate distributions like Gaussians. It is evaluated on discrete and continuous POMDP benchmarks, as well as BAMDP problems with varying physical model parameters. BPO is compared to EPOpt and UP-MLE, showcasing its use of information-gathering actions. The study compares robust and adaptive policy gradient algorithms, including BPO, BPO-, and UP-MLE, to understand the effect of encoders. UP-MLE uses maximum likelihood estimate from a Bayes filter, allowing direct comparison with BPO. TRPO is used as the underlying batch policy optimization subroutine for all algorithms. The study compares robust and adaptive policy gradient algorithms, including BPO, BPO-, and UP-MLE, to understand the effect of encoders. For all algorithms, results are compared using the seed with the highest mean reward. EPOpt and UP-MLE are relevant algorithms for batch policy optimization but do not formulate problems as BAMDPs. The BPO network's components consist of fully connected layers with N h hidden units and tanh activations. Policy networks have two fully connected layers with N h hidden units and tanh activations, with different output activations for discrete and continuous action spaces. In the Tiger problem, an agent must choose between listening or opening doors to find a hidden tiger. The optimal strategy involves listening until one door is significantly more likely to have the tiger. The study compares various policy gradient algorithms using normalized performance metrics. The Tiger problem involves choosing between listening or opening doors to find a hidden tiger. The study compares different policy gradient algorithms, showing that BPO outperforms EPOpt and UP-MLE in learning to listen until confident about the tiger's position. The study compares different policy gradient algorithms, showing that BPO achieves close to optimal return in solving the Chain problem, using independent encoder networks for better state estimation. In the Chain problem, Action B transitions to state s1 with a reward of 2, but with noise. Slip probability varies in our variant, affecting performance of BPO and UP-MLE. The algorithm's robustness is shown in FIG1 across different discretization levels of the latent space. BPO outperforms BPO- at finer discretizations, but degrades at coarser ones. In the Light-Dark problem, an agent aims to reach a goal location while uncertain about its own position, receiving noisy observations at each timestep. The problem involves a light source and comparisons to other algorithms like BID29 and BID41 are made. In the Light-Dark problem, the agent must navigate towards a light source to reduce observation noise. The belief is parameterized as a Gaussian distribution, and an Extended Kalman Filter is used for posterior updates. Sample trajectories from different algorithms are compared in the LightDark environment. The agent, using algorithms like EPOpt and UP-MLE, moves confidently towards the light source in the Light-Dark problem. It then heads straight to the goal without reducing uncertainty initially. The algorithms are evaluated on MuJoCo benchmarks like HalfCheetah, Swimmer, and Ant, showcasing BPO's robustness to model uncertainty. BPO efficiently adapts to changing beliefs in the environment, as demonstrated in the Ant benchmark. The BPO algorithm quickly reduces entropy and adapts the policy to the identified model, outperforming EPOpt and UP-MLE in terms of average return in the HalfCheetah environment. Despite slightly lower performance in Swimmer, BPO still produces reasonable gaits in most environments due to the belief collapsing quickly in deterministic systems. Bayesian Policy Optimization (BPO) is a practical and scalable approach for continuous BAMDP problems, achieving performance comparable to state-of-the-art discrete POMDP solvers. BPO is agnostic to the choice of batch policy optimization subroutine and outperforms robust policy gradient algorithms in addressing model uncertainty. The network architecture of BPO scales well with latent parameter space discretization, encoding state and belief independently. BPO is agnostic to batch policy optimization choice, outperforming algorithms not reasoning about belief distributions. Bayesian approach is crucial for reducing uncertainty in environments, as seen in FIG1 and FIG2. BPO scales to fine-grained latent space discretizations, but optimal level varies per problem. Variable-resolution discretization may be preferable over extremely fine resolutions. Adapting iterative densification ideas from motion planning and optimal control to discretize latent space for a more compact belief representation. Learning to directly map observations to a lower-dimensional belief embedding could improve performance without losing information. Combining a recurrent policy for unidentified parameters with a Bayes filter for identified parameters shows promise for future research. The environment parameters are sampled from a Gaussian distribution with noise variance minimized at x=5. The reward function incurs a penalty if the agent does not reach the goal. Parameters are varied for each environment, such as leg lengths for HalfCheetah and Ant, and link lengths for Swimmer. The 2D-parameter space is discretized into a 5 \u00d7 5 grid with a uniform initial belief. Gaussian noise is assumed on the observation, with the belief quickly concentrating in a single cell of the grid."
}