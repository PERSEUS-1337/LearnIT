{
    "title": "r1drp-WCZ",
    "content": "State Space LSTM (SSL) is introduced in this paper as a way to combine the interpretability of state space models with the power of LSTM. Unlike previous work, SSL does not make factorization assumptions in the inference algorithm. An efficient sampler based on sequential Monte Carlo (SMC) method is presented for drawing from the joint posterior directly. Experimental results demonstrate the superiority and stability of this SMC inference algorithm across various domains. From a graphical model perspective, efficient message passing algorithms like BID34 and BID17 are available in compact closed form due to their simple linear Markov structure. However, real-world sequences with long-range dependencies and complex patterns may not be well captured by Markov models. Recurrent Neural Networks (RNN), such as Long Short-Term Memory (LSTM), have become popular for sequence modeling as they directly define the distribution of each observation conditioned on the past using a neural network. RNNs offer a rich function class and allow for scalable stochastic optimization like backpropagation through time (BPTT). In this paper, a new class of models called State Space LSTM (SSL) is proposed to combine the benefits of RNNs and SSMs. SSLs can handle nonlinear dynamics like RNNs while retaining the probabilistic interpretations of SSMs by separating the state space from the sample space. Unlike SSMs, SSLs focus on modeling the sequence of latent states to represent the true underlying dynamics, rather than estimating dynamics directly from observed sequences. The State Space LSTM (SSL) model combines RNNs and SSMs by modeling the transition function between states with a neural network. Parameter estimation can be challenging due to the sequence of latent variables. In BID35, an EM-type approach is used to alternate between imputing latent states and optimizing the LSTM. However, the inference assumes a factorizable posterior through time, which is restrictive. The proposed parameter estimation scheme for State Space LSTM models avoids restrictive assumptions by sampling from the joint posterior using Sequential Monte Carlo methods. This approach leads to significant performance improvements compared to traditional methods. Parameterizing state transition and emission with neural networks has been explored in recent works, thanks to the development of recognition networks. Enriching the output distribution of RNN with flexible distributions like RBM or VAE has also gained popularity. Some studies have introduced stochasticity to RNNs by incorporating independent latent variables or attaching RNN to latent states and observations. In this approach, the transition and emission are decoupled for interpretability. In our approach, transition and emission are decoupled for efficient inference without variational assumptions. We use a stochastic EM approach, replacing full expectation with samples from SMC. SSMs and RNNs are used for sequence modeling, with SMC methods for sampling from distributions. SSMs are graphical models defining dependencies between latent states and observations. An example is the (Gaussian) LDS, where states evolve linearly over time. Inference tasks include filtering and smoothing for estimating state beliefs. SSMs allow for closed form inference tasks like filtering and smoothing, with algorithms such as the Kalman filter and RTS smoother. The EM algorithm can then be used to find the maximum likelihood estimate of parameters in LDS models. Refer to BID11 for more on learning parameters using EM iterations. RNNs aim to learn the generative distribution of p(x t |x 1:t\u22121) using a neural network with a deterministic internal state s t. The flexibility of the transformation function allows RNNs to capture long-range dependencies and learn from complex nonlinear sequences. The BPTT algorithm can be used to find the MLE of the parameters of RNN(\u00b7, \u00b7). Sequential Monte Carlo (SMC) is an algorithm that samples from a series of potentially unnormalized densities using weighted particles. LSTMs are designed to cope with vanishing gradient problems in RNNs by introducing an extra memory cell. Sequential Monte Carlo (SMC) is an algorithm that samples from unnormalized densities using weighted particles. The key to this method lies in resampling, which encourages particles with higher likelihood to survive longer. The final Monte Carlo estimate consists of only survived particle paths, chosen based on the last weights. State Space LSTM (SSL) models combine interpretability of SSMs and flexibility of LSTMs. State Space LSTM (SSL) models combine the interpretability of SSMs with the flexibility of LSTMs. The approach aims to learn dynamics in the state space rather than the sample space, without assuming linearity, Gaussianity, or Markovian transitions. Using LSTMs to model dynamics in the latent state space allows for learning from complex sequences without restrictive assumptions. The BPTT algorithm facilitates training the latent dynamics without additional approximations. The generative process involves an emission function mapping latent states to sample distribution parameters. The generative process of SSL for a single sequence involves a joint likelihood with non-Markovian state transitions, parameterized by LSTMs. Parameter estimation is done through a variational lower bound to the marginal data likelihood using a variational distribution. The generative process of SSL for a single sequence involves parameter estimation through a variational lower bound using a variational distribution. The iterative updates involve E-step for optimal variational distribution, S-step for posterior sampling, and M-step for Monte Carlo estimation. The posterior samples z 1:T are used as input for the MLE of an LSTM with the given emission model. Two examples of SSL are provided for continuous and discrete latent states. In Gaussian SSL, transition and emission are modeled using Gaussian distributions with closed form estimates for emission parameters. The MLE of \u03c6 = (C, b, R) is obtained through least squares fit in SSL. Modeling user behavior with LSTM for website sequences is challenging due to the large Internet size. An alternative approach involves operating on a \"compressed\" vocabulary using z t as a topic indicator. This method learns compression while mapping LSTM states to latent states with matrix W and bias term b. The LSTM model maps states to latent states using z t as a topic indicator, allowing for compression of vocabulary. The estimation of \u03c6 is typically done under a Dirichlet prior, and samples can be drawn from the joint posterior of z 1:T efficiently. In this work, a method based on Sequential Monte Carlo (SMC) is proposed to sample from a sequence of distributions without requiring the model to be Markovian. The approach involves approximating the posterior with weighted particles, using an importance density function f(z_t | z_1:t-1, x_t) and a specified number of particles P. This method allows for more flexibility in LSTM state transitions compared to traditional factorization assumptions. The algorithm uses Sequential Monte Carlo (SMC) with weighted particles to sample from distributions without Markovian model requirements. The choice of proposal distribution can be the transition density or the predictive distribution, leading to different particle filter variations. The predictive distribution simplifies the importance weight calculation, marginalizing over all possible particle assignments at the current time step for computational efficiency. Sequential Monte Carlo (SMC) algorithm uses weighted particles to sample from distributions without Markovian model requirements. The optimal proposal relies on efficient normalization for computational benefits. The algorithm produces a Monte Carlo approximation of the posterior and marginal likelihood, completed by a final draw from the approximate posterior. As the number of particles approaches infinity, the approximate posterior is guaranteed to converge to the true posterior. Particles P \u2192 \u221e the approximate posterior converges to the true posterior as sequence length increases, but the number of particles needed grows exponentially. Particle MCMC methods combine MCMC with SMC to avoid simulating a large number of particles. PMCMC treats particle estimates as proposals and uses a Gibbs kernel for minimal modification from basic SMC. Particle Gibbs (PG) is an algorithm derived from the Gibbs kernel, which is a conditional SMC update that maintains a fixed reference path throughout particle propagation. This modification ensures an invariant and ergodic transition kernel under mild assumptions. The final algorithm, summarized in Algorithm 2, is part of the particle SAEM BID25 BID33, incorporating stochastic EM outer iteration and non-Markovian state transition. Forward messages for specific examples are derived, such as Gaussian SSL integration and normalization preserving normality. The curr_chunk discusses the use of softmax in a proposed model for sequence tracking, emphasizing the flexibility and accuracy of the inference method. Empirical studies are presented to demonstrate the model's ability to capture nonlinear dynamics and handle complicated models. The normalization of weight distribution and messages in the state space are highlighted as manageable computations. The model opens new possibilities for interpretable yet nonlinear and non-Markovian sequence models. The curr_chunk evaluates synthetic sequence tracking, language modeling, and user modeling using complex models. SMC inference involves gradually increasing particles during training. Experiments are conducted on TensorFlow BID0 with specific hardware. The flexibility of SSL is tested with synthetic data and different dynamics in 2D space. Data points are generated with Gaussian noise added to true dynamics. The curr_chunk discusses the comparison between SSL and vanilla LSTM in capturing dynamics, highlighting that SSL can recover better from initial predictions. It also compares SMC inference method with the old BID35 algorithm in Topical SSL, showing test perplexity and number of nonzeros. In Topical SSL, the SMC inference method consistently outperforms the old factored method, with lower test perplexity and number of nonzeros in the learned word topic count matrix. LSTM, while achieving the lowest test perplexity, requires expensive linear transformations depending on vocabulary size, unlike SSL which depends linearly on the number of topics. Ablation study shows that as dataset size increases, the gap between factored approximation and accurate SMC reduces in structured natural languages, but in less structured datasets like user modeling, the factored assumption leads to poorer performance. In less structured datasets, the factored assumption leads to poorer performance in user modeling. The SMC algorithm performs better when data size is fixed and the number of topics varies. Visualizing particle paths shows the model becoming more confident about underlying topical transitions after 100 epochs, with topics concentrated on music and time. The study focused on music and time topics, confirming the effectiveness of flexible sequence modeling. An anonymized user search click history was used to evaluate model accuracy in predicting future clicks. The dataset had a 100K vocabulary and varied the number of users from 500K to 1M, with 500 fixed topics. The dataset's less structured nature made user click patterns less predictable than language modeling tasks. The new inference method showed significant improvements in accuracy. The study introduced a new inference method for Latent LSTM models, providing a more principled Sequential Monte-Carlo algorithm for posterior inference. Despite being slower, the new algorithm outperformed the factored model in terms of stability and accuracy across various datasets. Future work aims to extend the model to handle dynamically changing structured objects like time-evolving graphs."
}