{
    "title": "HJGven05Y7",
    "content": "The field of few-shot learning has advanced through meta-learning, with Model Agnostic Meta Learning (MAML) being a prominent approach. However, MAML has issues like sensitivity to neural network architectures, instability during training, and high computational costs. To address these, we propose MAML++, which improves stability, generalization, convergence speed, and computational efficiency. Few-shot learning is a method that can learn new concepts with only a handful of data-points, reducing the need for extensive data collection and labeling. This approach has the potential to advance Artificial Intelligence by addressing the shortcomings of standard deep neural networks in the small data regime. Few-shot learning is crucial for building robust machine learning models and reducing training times. It is challenging when only a few samples are available per class, making it necessary to learn from limited data. Knowledge transfer is key for model learning, but manual transfer can be time-consuming. Meta-learning can automate knowledge transfer across tasks, improving efficiency. Meta-learning involves acquiring task-specific knowledge quickly and slowly learning across-task knowledge. Recent work has shown that strided MAML++ models consistently converge fast to higher generalization accuracy without stability issues, outperforming the original strided MAML models. Meta-learning, specifically Model Agnostic Meta-Learning (MAML), aims to achieve high generalization accuracy with few-shot learning methods. MAML proposes learning an initialization for a base-model to adapt quickly to new samples from the same classes. The meta-model in MAML refers to the initialization parameters. In MAML, the meta-model initializes parameters for the base-model, used for task-specific learning on support and target sets. MAML++ improves upon MAML by offering stability, flexibility, and automatic inner loop learning. MAML++ introduces improvements in training stability, automatic hyperparameter learning, computational efficiency, and generalization performance. It achieves state-of-the-art results in few-shot learning tasks on Omniglot and Mini-Imagenet datasets, outperforming established meta-learning methods. The set-to-set few-shot learning setting BID25 frames few-shot learning as a meta-learning problem, with tasks divided into support and target sets for task-level learning and evaluation. Matching Networks BID25 achieve few-shot learning by matching target set items to support set items using cosine distance and a fully differentiable embedding function. The support set items are embedded into vectors by a neural network, and then the target set items are embedded. This approach has shown to be effective in few-shot learning tasks. The function f embeds target set items and computes cosine distances between target set embeddings and support set embeddings. Softmax function converts distances into probability distributions over support set classes. A gradient-conditional meta-learner LSTM BID19 updates a base-learner model at inference time. Predictions are made on the target set, and task loss is computed using these predictions. The meta-learner's parameters are jointly learned. The authors proposed various methods to improve meta-learning performance, such as increasing gradient update steps on the base-model and using Batch Stochastic Gradient Descent instead of LSTM for the meta-learner. These approaches led to faster learning and better generalization, achieving state-of-the-art results in Omniglot and Mini-Imagenet datasets. Additionally, in Meta-SGD, a static learning rate and update direction for each parameter in the base-model were learned, resulting in significantly improved generalization performance. Model Agnostic Meta-Learning (MAML) is a framework for few-shot learning that learns good initialization parameters for a neural network. MAML can achieve state-of-the-art results in few-shot regression/classification and reinforcement learning tasks by updating the network with a small number of gradient steps on a support set. Meta-SGD, on the other hand, shows improved generalization performance compared to MAML but doubles the model parameters and computational overhead. The inner-loop update process in Model Agnostic Meta-Learning (MAML) involves performing gradient update steps on data from a support set to obtain updated base-network parameters. The meta-objective is defined to measure the quality of an initialization across all tasks and is minimized to optimize the network's performance. The meta-objective in Model Agnostic Meta-Learning (MAML) is to optimize the initial parameter value \u03b8 0 containing across-task knowledge. The outer-loop update process minimizes this objective by updating the meta-parameters \u03b8 0 using a learning rate \u03b2 and loss on the target set for each task. MAML is a powerful framework for meta-learning but can be unstable during training due to issues like training instability and backpropagating derivatives through an unfolded inner loop. The lack of skip-connections in the standard 4-layer convolutional network used in MAML can lead to gradient issues, such as gradient explosions and diminishing gradients. Additionally, the computation of second order gradients for optimization is costly, prompting the use of first-order approximations to speed up the process. Using first-order approximations can speed up the process by a factor of three, but it may negatively impact the final generalization error. Reptile BID18 attempts to use first-order methods by applying standard SGD on a base-model and then taking a step towards the base-model parameters after N steps. Results vary compared to MAML, with some cases exceeding it and others performing worse. Approaches to reduce computation time without sacrificing generalization performance are still lacking. Another issue affecting generalization performance is the use of batch normalization in MAML experiments, where current batch statistics are used instead of accumulating running statistics. Batch normalization in MAML is less effective when using current batch statistics, as biases have to accommodate different means and standard deviations. Using accumulated running statistics can lead to convergence to a global mean and standard deviation, improving convergence speed, stability, and generalization performance. Additionally, batch normalization biases are not updated in the inner-loop, causing the same biases to be used throughout all iterations of base-models. Using a shared learning rate for all parameters and update steps can hinder generalization and convergence speed in MAML. Learning a single set of biases for all iterations of the base-model may restrict performance due to the assumption that all base-models are the same throughout inner loop updates. The authors in BID15 propose learning a learning rate and update direction for each parameter of the network to avoid manual hyperparameter searches. However, this approach increases computational effort and memory usage due to the large number of network parameters. In MAML, a fixed learning rate is used with Adam to optimize the meta-objective, with annealing the learning rate being crucial for achieving state-of-the-art generalization performance. In MAML, using a static learning rate may reduce generalization performance and lead to slower optimization. To address this issue, a method called Multi-Step Loss Optimization (MSL) is proposed, which involves minimizing the target set loss computed by the base-network after every step towards a support set task. This approach aims to improve gradient stability and overall performance of the MAML framework. The proposed Multi-Step Loss Optimization (MSL) method involves minimizing the target set loss by the base-network after each step towards a support set task. This approach aims to improve gradient stability and overall performance in the MAML framework by using a weighted sum of target set losses. The Multi-Step Loss Optimization (MSL) method addresses instability issues in MAML by using a weighted sum of target set losses. Annealed weighting is employed to prioritize later step losses during training, ensuring the final step loss receives more attention for optimal results. Reducing inner-loop updates through Derivative-Order Annealing (DA) can enhance MAML's computational efficiency. The authors propose a method to reduce computational overhead by annealing the derivative-order as training progresses. They suggest using first-order gradients for the first 50 epochs and then switching to second-order gradients for the rest of the training phase. This approach speeds up the initial epochs while maintaining strong generalization performance. No incidents of exploding or diminishing were observed in derivative-order annealing experiments. Using first-order gradients before switching to second-order gradients in training can prevent gradient explosion/diminishment issues. Implementing running batch statistics for batch normalization in MAML can alleviate undesirable effects caused by using only current batch statistics. Sharing running batch statistics across all update steps in the inner-loop fast-knowledge acquisition process is necessary for a naive implementation of batch normalization. To optimize MAML, collecting running statistics per-step for batch normalization is crucial. This approach involves instantiating N sets of running mean and standard deviation for each batch normalization layer, updating them with each optimization step. This method can enhance optimization speed and potentially improve generalization performance. To improve MAML optimization, learning biases per-step within the inner-loop update process can enhance convergence speed, stability, and generalization performance. This approach involves setting biases specific to the feature distributions seen at each step, rather than assuming similar distributions. This method aims to speed up optimization and improve generalization by adapting biases to changing feature distributions. Proposing to learn a learning rate and direction for each layer in the network, as well as different learning rates for each adaptation of the base-network. This approach aims to reduce memory and computation needed while providing flexibility in update steps. Parameters can learn to decrease learning rates at each step to help alleviate overfitting. Fixed Outer Loop Learning Rate is replaced with Cosine Annealing of Meta-Optimizer. In MAML, annealing the learning rate of the meta-optimizer using cosine scheduling can improve generalization performance without the need for hyper-parameter tuning. This approach allows the model to fit the training set more effectively, potentially leading to higher generalization power. The method was evaluated on Omniglot and Mini-Imagenet datasets. The Omniglot dataset consists of 1623 character classes with 20 instances each. 1150 classes are used for training, 50 for validation, and 423 for testing. Data augmentation includes rotating images by 90 degrees. The Mini-Imagenet dataset contains 600 instances of 100 classes from ImageNet, scaled down to 84x84. The dataset is split into 64 training classes, 12 validation classes, and 24 testing classes. A hierarchical hyperparameter search methodology was used to evaluate different methodologies on top of the baseline MAML experiments. We conducted experiments on the Mini-Imagenet dataset, implementing 6 methodologies on top of MAML. After evaluating each approach separately, we combined techniques that improved generalization performance or convergence speed. Training involved 150 epochs with 500 iterations each, evaluating model performance on the validation set after each epoch. The top 3 models per epoch were ensembled for testing on 600 tasks, distinguishing between training and evaluation tasks generated dynamically. Our proposed methodologies, trained with Adam optimizer, showed improvements over the original MAML framework. Experiment results on Omniglot dataset demonstrated superior performance compared to MAML. Notably, learned per-step per-layer learning rates contributed significantly to the enhancements. The MAML++ framework showcases notable improvements in learning rates and batch normalization methodology, achieving high accuracy in various tasks. It also demonstrates faster convergence speed and improved training stability. Additionally, MAML++ sets a new state of the art in Mini-Imagenet tasks. MAML++ sets new state of the art in Mini-Imagenet tasks, achieving high accuracy in 5-way 1-shot and 5-shot cases. It shows strong 1-shot results with reduced inner loop steps and faster convergence compared to MAML. The approach improves generalization error and stability, setting a new benchmark in few-shot tasks. The approach in few-shot learning focuses on learning per-step rates, batch normalization parameters, and optimizing per-step target losses for fast and generalizable results across Omniglot and Mini-Imagenet tasks."
}