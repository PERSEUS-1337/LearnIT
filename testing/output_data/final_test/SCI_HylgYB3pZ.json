{
    "title": "HylgYB3pZ",
    "content": "The paper identifies angle bias in MLPs with sigmoid activation functions and proposes linearly constrained weights (LCW) to reduce it. LCW helps train a 100-layered MLP more efficiently than batch normalization. Neural networks with a single hidden layer can approximate complex functions but may require an exponential number of neurons. In this paper, the vanishing gradient problem in training deep neural networks with many hidden layers is addressed. Techniques such as layer-wise pretraining, rectified linear units, variance-preserving initialization, and normalization layers are proposed to overcome this issue. The paper also identifies angle bias as a key cause of the vanishing gradient in multilayer perceptrons with sigmoid activation functions. The paper addresses the vanishing gradient problem in deep neural networks by proposing the use of linearly constrained weights (LCW) to reduce angle bias in a multilayer perceptron (MLP) with sigmoid activation functions. LCW is a weight vector with the constraint that the sum of its elements is zero, embedded into the neural network structure. This allows training deep MLPs with LCW using optimization solvers like stochastic gradient descent. Batch normalization BID7 can also correct angle bias in a neural network. Preliminary experiments show that LCW helps train deep MLPs effectively. The paper introduces linearly constrained weights (LCW) to address the angle bias problem in deep neural networks. LCW is proposed as a method to reduce angle bias in a neural network, allowing for efficient training of deep MLPs. Experimental results demonstrate the effectiveness of using LCW to train a 100-layered MLP. The paper introduces linearly constrained weights (LCW) to reduce angle bias in deep neural networks, allowing for efficient training of deep MLPs. LCW addresses the unexpected horizontal stripe pattern in the heat map of randomly generated matrices W and A. The angle bias is defined by a probability distribution P \u03b3 and the expected value of w \u00b7 a depends on the angle between w and 1 m when \u03b3 = 0. The expected value of w \u00b7 a depends on \u03b8 w as long as \u03b3 = 0, leading to angle bias in the distribution of w \u00b7 a. The angle bias is influenced by the angle between w and 1 m, resulting in a stripe pattern in W A. The generalization of Proposition 1 applies to any m dimensional distributionP with expected value \u03bc \u2208 R m. Proposition 2 discusses the bias in the distribution of w \u00b7 a based on the angle \u03b8 w unless \u03bc = 0. An experiment using an MLP on the CIFAR-10 dataset illustrates the effect of angle bias in the activation patterns of different layers. In Layer 1 of the MLP, stripe patterns caused by angle bias are seen in layers other than Layer 1. Activation values in Layer 9 remain constant regardless of input. Boxplot summaries in FIG6 show biased mean values of \u03b8 l i in layers other than Layer 1, with variance shrinking through layers. In an MLP with ReLU activation functions and 50 hidden layers, the variance of \u03b8 l i shrinks through layers. Activation patterns in layers 10, 20, 30, and 40 show stripe patterns caused by angle bias. Boxplot summaries reveal biased mean values of \u03b8 l i in layers other than Layer 1, with variance shrinking moderately. In an MLP with ReLU activation functions and multiple hidden layers, the variance of \u03b8 l i decreases across layers. Activation patterns in certain layers exhibit stripe patterns due to angle bias. Using rectified linear activation can prevent the vanishing gradients issue seen with sigmoid activation. In deep neural networks, angle bias can reduce efficiency by causing certain neurons to be consistently active or inactive. To address this, two approaches are suggested: adjusting neuron activation values towards zero or regularizing the angle between weights. A method to reduce angle bias is proposed using W LC to regulate the angle between weights. Proposition 3 states that for a random variable a following distribution P \u03b3, the expected value of w \u00b7 a is zero for w \u2208 W LC. By using weight vectors from W LC in a multi-layer perceptron (MLP), the angle bias in each layer can be reduced, as shown in the activation pattern in layers 1, 3, 5, 7, and 9 of the MLP with LCW. This approach helps address angle bias in deep neural networks. The neurons in Layer 9 of the MLP with LCW respond differently to input samples, leading to a change in network output when adjusting weight vectors in Layer 1. The angle bias is resolved in the calculation of z l i by using LCW, as seen in the activation pattern in layers after 10 epochs of training. The mean of \u03b8 l i is slightly biased according to neurons, but the variance remains consistent. The MLP with LCW resolves angle bias in z l i calculation, as shown in activation patterns after 10 epochs. LCW eliminates bias in \u03b8 l i mean, with consistent variance. LCW is used in weight vectors of ReLU MLP, showing improved activation patterns. Training with LCW involves constrained optimization to ensure weight vectors are in W LC. The proposed reparameterization technique allows training a neural network with LCW by using unconstrained optimization, embedding constraints on weight vectors into the network structure. This method reduces angle bias in z l i calculation and can be easily implemented with modern deep learning frameworks. By introducing LCW, angle bias in z l i is reduced, especially important for sigmoid activation to prevent saturation. An initialization method regularizes the variance of z l i based on a minibatch of samples. Preliminary experiments on CIFAR-10, CIFAR-100, and SVHN datasets were conducted to evaluate LCW performance compared to BN and WN. The network structure used MLPs with cross-entropy loss function. The network structure used MLPs with cross-entropy loss function. Each network had 3072 input neurons and 10 output neurons, followed by a softmax layer. Different activation functions were used. MLPs were initialized and optimized using different methods. Training was done using stochastic gradient descent with a minibatch size of 128 for 100 epochs. The experiments involved training MLPs with different activation functions on the CIFAR-10 dataset using various optimization techniques. MLP LCW and MLP BN showed successful training, while plain MLP(100, 128) struggled due to angle bias. The experiments compared deep MLPs with different optimization techniques on the CIFAR-10 dataset. MLP LCW showed higher training accuracy increase compared to MLP BN. Weight normalization reduced angle bias but did not make deep MLPs trainable. MLP LCW had lower computational overhead than MLP BN. The BN module used in experiments was developed by GPU vendors, while the WN module was developed in-house. MLP LCW showed peaks in test accuracy around 20 epochs. Further studies are needed to understand the findings. Experimental results with SVHN and CIFAR-100 datasets are reported. Training MLP LCW with 20 layers and 256 neurons per layer was unsuccessful regardless of the dataset used. The weight gradients of MLP LCW exponentially increased compared to plain MLP, potentially hindering learning due to an increase in active paths. This could be addressed by reducing inactive neurons as discussed in Section 2.3. In Section 2.3, the study aims to prevent inactive neurons by reducing angle bias in MLP LCW with rectified linear activation functions. Future work includes exploring layer-wise learning rates and weight gradient distribution regularization. The paper identifies angle bias in dot product calculations and its impact on neural network preactivation values, leading to vanishing gradients with sigmoid activation functions. To address the vanishing gradient issue in neural networks with sigmoid activation functions, linearly constrained weights are proposed to reduce angle bias. These weights can be efficiently learned through reparameterization. Preliminary experiments indicate that reducing angle bias is crucial for training deep MLPs with sigmoid activation functions."
}