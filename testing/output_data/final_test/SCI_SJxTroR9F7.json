{
    "title": "SJxTroR9F7",
    "content": "The proposed methodology, Supervised Policy Update (SPU), is a sample-efficient approach for deep reinforcement learning. It solves an optimization problem in the non-parameterized proximal policy space using supervised regression, converting it to a parameterized policy for generating new samples. SPU is applicable to both discrete and continuous action spaces, handling various proximity constraints. It addresses Natural Policy Gradient, Trust Region Policy Optimization (NPG/TRPO), and Proximal Policy Optimization (PPO) problems, outperforming TRPO and PPO in sample efficiency in Mujoco simulated robotic tasks. Experiments demonstrate that SPU outperforms TRPO in Mujoco robotic tasks and PPO in Atari games. The policy gradient problem in deep reinforcement learning involves finding a policy with high expected reward. Sample efficiency is a key issue in policy gradient methods, where new samples are required for every gradient step. Sample efficiency is crucial in costly environments like robotics. It aims to minimize the number of environment calls needed to achieve a certain performance level. The goal is to construct a new policy with the highest performance improvement possible while staying close to the original policy. The policy gradient problem in deep reinforcement learning involves finding a policy with high expected reward. To improve performance, it is desirable to limit the search to policies close to the original policy. This requires defining a distance between the current policy and a candidate new policy, and solving a constrained optimization problem to estimate the performance based on previous policy and new data. Our work introduces a new methodology called Supervised Policy Update (SPU) to address the sample efficiency problem in reinforcement learning. It aims to maximize the performance of the updated policy while ensuring it stays close to the original policy that generated the data. The implementation of SPU is slightly more involved than PPO but strikes a balance between performance and simplicity, making it suitable for solving problems beyond traditional RL testbeds. SPU optimizes over a proximal policy space to find an optimal non-parameterized policy, then converts it to a parameterized policy through supervised regression. It is simpler than NPG/TRPO and more sample efficient than TRPO in robotic tasks and PPO in video game tasks. Our paper focuses on separating the process of finding the optimal policy into two steps: first finding the optimal non-parameterized policy, then parameterizing it. We compare algorithms operating under the same constraints, with a focus on on-policy algorithms. The extension to off-policy training is left for future work. Deep reinforcement learning involves optimizing over a set of parameterized policies using neural networks. Stochastic gradient ascent is commonly used to maximize the expected discounted reward. The advantage function for a policy is denoted as A \u03c0 (s, a). The methodology applies to both finite and continuous state and action spaces. The objective J(\u03c0 \u03b8) is approximated using samples from \u03c0 \u03b8k for the future state probability distribution. Two approaches are used to approximate J(\u03c0 \u03b8) \u2212 J(\u03c0 \u03b8k), involving first-order approximation around \u03b8k and approximating the state distribution. There is a known bound for the approximation. NPG/TRPO algorithm finds gradient updates by solving sample efficiency problem with KL-divergence for policy proximity constraint in parameter space \u03b8 \u2208 \u0398. It approximates J(\u03c0 \u03b8) and D KL (\u03c0 \u03b8 \u03c0 \u03b8k) using first and second-order methods, estimates from \u03c0 \u03b8k samples, and solves for optimal \u03b8*. The TRPO algorithm finds optimal \u03b8* by considering gk, hk, and the sample average of the Hessian. SPU takes a different approach by solving the optimization problem in non-parameterized policy space and then finding a parameterized policy through supervised regression. GAC also follows a similar decomposition but with restrictions on constraint criteria and action spaces. Our approach demonstrates that decomposition alone is a useful technique for solving constrained policy optimization, unlike MPO which uses Expectation Maximization for DRL with preliminary results on discrete actions. Clipped-PPO takes a different approach to TRPO by making many gradient steps at each iteration. The Clipped-PPO method aims to keep the policy close to a previous iteration while avoiding large deviations. It differs from traditional optimization frameworks but shares similarities in spirit. Previous methods like adding a KL penalty were found to be inferior to Clipped-PPO. Our work demonstrates a new form of gradient that outperforms Clipped-PPO. The SPU methodology updates policy gradient and critic terms efficiently, exploiting past episodes and maintaining an average policy network. It aims to handle past episodes and considers bounding the joint distribution of state and action. Unlike PPO/TRPO, it has two steps for finding the optimal solution to a given constraint criterion. The SPU methodology efficiently updates policy gradient and critic terms, aiming to handle past episodes and maintain an average policy network. It has two steps for finding the optimal solution to a given constraint criterion, approximating the objective function but not the constraint like PPO/TRPO. The optimal solution for the non-parameterized problem can be determined nearly in closed form for many natural constraint criteria. The SPU methodology updates policy network \u03c0 \u03b8 to target distributions \u03c0 * (\u00b7|s i ) by minimizing a supervised loss function with stochastic gradient descent methods. Different proximity constraints lead to varied forms of the gradient update, illustrated through solving non-parameterized optimization problems. The SPU methodology updates policy network \u03c0 \u03b8 to target distributions \u03c0 * (\u00b7|s i ) by minimizing a supervised loss function with stochastic gradient descent methods. The optimization problem involves minimizing L \u03c0 \u03b8 k (\u03c0) subject to constraints, including the \"aggregated KL constraint\" and the \"disaggregated KL constraint\". The forward-KL non-parameterized optimization problem is solved exactly using the SPU framework, allowing for more precise optimization compared to heuristic approximations. The SPU methodology updates policy network \u03c0 \u03b8 to target distributions \u03c0 * (\u00b7|s i ) by minimizing a supervised loss function with stochastic gradient descent methods. Experimentally, SPU outperforms TRPO, showing significant potential. The optimal non-parameterized policy structure is given by a specific equation, and a parameterized policy \u03c0 \u03b8 is sought to minimize a loss function. The SPU methodology updates the policy network \u03c0 \u03b8 to target distributions \u03c0 * (\u00b7|s i ) by minimizing a supervised loss function with stochastic gradient descent methods. To simplify the algorithm, hyper-parameter \u03b4 is replaced with \u03bb and tuned instead. Per-state acceptance is introduced to enforce disaggregated constraints, and the approximate gradient is used for supervised training of the policy network. The stopping criterion is when training reaches a certain point. The optimal policy structure with reverse KL-divergence constraint is derived for disaggregated constraints. The non-parameterized optimal policy is found by solving a specific equation. The optimal solution is characterized by \u03bb(s) > 0. The policy structure with backward KL constraint differs from forward KL constraint. The equation has an intuitive interpretation, adjusting action probabilities based on a threshold. Additionally, a PPO-like objective can be formulated in the context of SPU. In the context of SPU, a PPO-like objective is formulated with a constraint function to optimize the policy. The optimization problem includes an aggregated constraint to ensure the updated policy remains close to the original policy. The updated policy is close to \u03c0 \u03b8 k, with the optimal solution given by a parameterized policy \u03c0 \u03b8 that minimizes mean square error over sampled states and actions. This approach, known as SPU with the L \u221e constraint, allows for exploration of other proximity constraints in the future. The methodology of SPU outperforms recent state-of-the-art methods in environments with continuous or discrete action spaces. Ablation studies and sensitivity analysis show the importance of algorithmic components and the sample efficiency of SPU compared to other methods. Implementation details are provided in Appendix D, using Mujoco simulated robotics environments as a benchmark. SPU with L \u221e and forward KL constraints outperform TRPO and PPO in Mujoco environments. SPU also outperforms TRPO by 28% after retraining for 3 million timesteps. Performance comparison is illustrated in FIG0 and FIG3 in the Appendix. Code for Mujoco experiments is available at https://github.com/quanvuong/Supervised_Policy_Update. The indicator variable in (18) enforces the per-state acceptance constraint. The variable in (18) enforces the per-state acceptance constraint. Removing this component is equivalent to removing the indicator variable. Using i D KL (\u03c0 \u03b8 \u03c0 \u03b8 k )[s i ] for dynamic stopping determines the number of training epochs. The term DISPLAYFORM1 ] is crucial for SPU performance. Per-state acceptance and dynamic stopping are important, with the former playing a central role. Hyper-parameters are retuned when a component is removed to ensure optimal performance. SPU's high performance is shown to be insensitive to hyperparameter choice by comparing it with TRPO using randomly sampled hyperparameter values. SPU outperformed TRPO in all 100 samples, with 75% and 50% showing performance improvements of at least 18% and 21% respectively. This demonstrates that SPU's superior performance is largely unaffected by hyperparameter values. SPU outperforms PPO in Atari games by 55% on average across 60 environments and 20 seeds. The comparison was made using the same network architecture and hyperparameters, showing SPU's higher sample efficiency. SPU outperforms PPO in various environments, with a larger performance gap in favor of SPU. The high performance of SPU in Mujoco and Atari domains showcases its generality and efficiency. The optimization process of SPU is shown to be convex. The optimization process of SPU is convex, with strong duality holding in the related Lagrangian problem. The problem decomposes into separate problems for each state, with the optimal solution given by a simple Lagrange-multiplier argument. The optimal solution to the decomposed constrained problem is given by a simple Lagrange-multiplier argument. The problem decomposes into separate problems for each state, with the optimization problem equivalent to a specific formula involving entropy. The problem of optimizing FORMULA2 - FORMULA1 decomposes into m separate problems, each with a feasible solution satisfying the inequality constraint. Strong duality holds for the convex problem, which can be solved by considering a Lagrangian problem with fixed \u03bb. The methodology developed in this paper applies to continuous state and action spaces, with modifications necessary for the continuous case. The definition of d \u03c0 (s) is modified to become a density function over the state space, while D KL (\u03c0 \u03c0 k ) and the approximation (8) remain unchanged. The definition of D KL (\u03c0 \u03c0 k ) and the approximation (8) remain unchanged in the non-parameterized optimization problem with aggregate and disaggregate constraints. The proofs of Theorem 1, 2, and 3 require slight modifications for continuous action spaces. The policy for Mujoco environments is parameterized by a fully connected feed-forward network. The policy for Mujoco environments is parameterized by a fully connected feed-forward neural network with two hidden layers of 64 units each. It outputs the mean of a Gaussian distribution with state-independent variable standard deviations. The action dimensions are assumed to be independent, and the probability of an action is calculated using a multivariate Gaussian probability distribution function. The baseline for advantage value calculation is also a neural network of similar size, trained to minimize the Mean Squared Error between sampled states TD\u2212\u03bb returns and predicted values. Both the policy and baseline networks have the same architecture, and Generalized Advantage Estimation is used to calculate advantage values after normalizing states. The TRPO implementation by OpenAI uses normalized advantage values for policy updates. SPU collects 2048 samples per iteration for policy and baseline network updates. Gradient descent with Adam optimizer is used for both networks. The output of the network goes through relu, linear, and softmax layers for action distribution. States are normalized before input. The TRPO implementation by OpenAI uses normalized advantage values for policy updates. States are normalized by dividing by 255 before being fed into any network. 8 processes run in parallel to collect timesteps, with each process collecting 256 samples before updating the policy and baseline network. Gradient descent is performed using Adam with a step size of 0.0001. Training is done for 10 million timesteps for both SPU and PPO, with specific parameters set for SPU. Algorithm 1 describes the forward-KL non-parameterized SPU using neural nets for policy and value approximation. Hyperparameters like learning rate, trajectory size, and training minibatch size are specified. TRPO and SPU were trained for 1 million timesteps to compare results. SPU outperforms TRPO by 28% after retraining both policies for 3 million timesteps. Performance comparison is illustrated in FIG3, and the random variable nature of SPU's improvement over TRPO is shown in Figure 4."
}