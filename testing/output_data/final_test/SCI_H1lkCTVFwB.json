{
    "title": "H1lkCTVFwB",
    "content": "Global feature pooling is a modern variant of feature pooling that offers better interpretability and regularization. While alternative pooling methods exist, such as max pooling and lp norm, averaging remains the dominant global pooling scheme in popular models. A comparison between global average and max pooling shows that max pooling encourages learning features of different spatial scales, making it more suitable for fine-grained recognition. Evaluation of nine pooling algorithms consistently demonstrates that max pooling outperforms average pooling by reducing the generalization gap across models, datasets, and image resolutions. Feature pooling has been a crucial component in visual recognition, rooted in the works of complex cells in the visual cortex and locally orderless images. Generalized pooling's performance improves consistently from average to max, reducing the generalization gap across models, datasets, and image resolutions. Combining heterogeneous pooling schemes can be challenging due to potential gradient conflicts, but the \"freeze-and-train\" trick is effective. Post-global batch normalization aids in faster convergence and enhances model performance consistently. Global feature pooling, a recent variant of feature pooling technique, defines its pooling kernel the same size as input feature map, resulting in a scalar value indicating the presence of specific features. This method serves as a strong network regularizer to prevent overfitting and is commonly used in state-of-the-art deep models for visual recognition. Fine-grained Visual Categorization (FGVC) focuses on classifying objects into subcategories. Pooling schemes like average, max, and cross-channel pooling can capture discriminative features. Higher-order pooling techniques like bilinear pooling and compact bilinear pooling aim to capture pairwise correlations and reduce computational complexity. Higher-order pooling methods like low-rank bilinear pooling, grassmann pooling, kernel pooling, and Alpha-pooling aim to reduce parameters while maintaining accuracy. Different pooling operations such as average, max, and striding are commonly used, with stochastic pooling and fractional max pooling offering alternative approaches. Global pooling layers output a vector rather than a scalar, residing in the same location as higher-order pooling methods. Fractional max pooling, spatial pyramid pooling, S3Pool, detail-preserving pooling, translation invariant pooling, k-max pooling, lp norm pooling, and soft pooling are different techniques in the pooling family that aim to find a smooth transition between average and max pooling, offering alternative approaches to reduce parameters while maintaining accuracy. Pooling techniques such as p-norm, soft pooling, mixed pooling, and gated pooling have been explored, but their application in global pooling scenarios remains largely unexplored. The choice of averaging operation in global pooling layers among highly-ranked classification models raises questions about its optimality compared to other pooling schemes like max pooling. Research has shown varying results on the effectiveness of different pooling techniques in image classification and fine-grained visual recognition. Research has shown that the selection of feature pooling significantly impacts algorithm performance, with max pooling often outperforming other pooling methods in traditional recognition frameworks. However, in squeeze and excitation networks, global max pooling has been reported to have slightly higher errors compared to average pooling. Our investigation compares global average and max pooling methods in feature learning. Max pooling produces sparser final conv-layer feature maps, focusing on part-level features, while global average pooling encourages object-level features. This is crucial for fine-grained datasets where class-specific features reside in localized object parts. In fine-grained datasets, features often reside in localized object parts. Global max pooling is found to be more discriminative and generalizes better than average pooling. Nine pooling schemes were evaluated, with max pooling, k-max pooling, and mixed pooling performing the best. Our k-max pooling model outperforms previous higher-order pooling methods, showing the need for learnable pooling optimized via gradient descent. Model performance decreases as pooling changes from max to average, raising concerns about learnable generalized pooling. Post-global batch normalization consistently improves performance and convergence. Integration of heterogeneous pooling is explored to leverage different features learned by average or max pooling. The text discusses the integration of heterogeneous pooling methods to improve model performance. It introduces the \"freeze-and-train\" trick as a way to prevent interference between different pooling methods. The resulting architecture outperforms single pooling models with minimal additional parameters. The pooling algorithms evaluated in the experiment section are described in detail. The text explains different pooling methods used in neural networks, such as average pooling, max pooling, stochastic pooling, L p norm pooling, and soft pooling. These methods manipulate feature maps to extract key information for model performance improvement. Soft pooling, logavgexp pooling, k-max pooling, mixed pooling, and gated pooling are different methods used in neural networks for pooling. Soft pooling transitions from average pooling to max pooling as \u03b2 changes. Logavgexp pooling combines average pooling and max pooling based on \u03b2. K-max pooling averages the top k activations, with k=1 being equivalent to max pooling and k=N being equivalent to average pooling. Mixed pooling combines max and average pooling based on \u03b1, where \u03b1=0 is average pooling and \u03b1=1 is max pooling. Gated pooling uses a universal weight to combine max and average pooling. The final-layer feature maps and filters of neural networks with max and average pooling are visualized to show pattern differences. Global average pooling emphasizes object-level features, while global max pooling focuses on part-level features. Feature maps indicate patterns detected by filters, with final conv-layer feature maps directly connected to the global pooling layer. Visualization examples from both pooling methods are shown for comparison. The feature maps of max and average pooling models are compared visually, showing differences in sparsity. Quantitative analysis measures sparsity using l0 norm and entropy, but struggles with noisy data. A modified approach is proposed to better reflect perceptual sparsity and noise tolerance. The proposed modified metrics, denoted as S {1,2}, aim to reflect perceptual sparsity and noise tolerance in evaluating feature maps. These metrics compute sparsity of individual feature maps using discrete entropy and thresholded l0-norm, providing a more robust analysis compared to traditional methods. The thresholded l0-norm metric is used to evaluate feature map sparsity by considering each feature map as a 1D vector. It suppresses small positive values and analyzes sparsity changes during the transition between global average and max pooling. The up-sampled feature map can be visualized as a heatmap on top of the input image to understand learned patterns in the network. The difference in learned features between global average pooling and global max pooling is evident. Global average pooling focuses on object-level features like birds, sky, grass, and water, while global max pooling emphasizes highly-localized object parts such as eyes, legs, and wing bars. In fine-grained datasets, global max pooling's attention to part-level features suggests its ability to find discriminative, class-specific features. In this section, the text discusses the comparison between global average pooling and global max pooling in terms of learned features. It also introduces the idea of combining heterogeneous pooling methods for improved performance, including an offline training baseline and end-to-end learnable methods like mixed pooling. The mixed pooling method combines the outputs of different pooling functions through a weighted sum, aiming for better performance. The text discusses different pooling strategies, including weighted sum, channel split, and branching. These strategies add hyper-parameters or parameters to the model but show little improvement over single pooling models. A modification to the branching strategy is proposed to separate gradient flow using the \"freeze-and-train\" trick. The text discusses the \"freeze-and-train\" trick for training models, where the backbone model is first trained with global average pooling, then a new linear layer with global max pooling is added and trained separately before training the whole network together. Experimental results on three datasets are provided, using ResNet-50 and Inception-v3 models with different optimizers. The text discusses training ResNet-50 and Inception-v3 models with different optimizers. Models are fine-tuned from pretrained Imagenet models with specific learning rates for different epochs. Standard image enhancement techniques are used during training with specific batch size, weight decay, and momentum. Input image size is 448x448, and testing is done on center-crop. Fair comparison requires training models from scratch. Training models from scratch is time-consuming and requires careful learning rate scheduling and hyperparameter search. Fine-tuned models may be influenced by pretrained weights, leading to similar performance. The study aims to analyze how perceptual sparsity metrics change with different k values in k-max pooling on a 448x448 input size, with k ranging from 1 to 196. The study analyzes how perceptual sparsity metrics change with different k values in k-max pooling on a 448x448 input size. Results on the Birds dataset show a monotonic increase in discrete entropy as k grows from 1 to 196. Max pooling consistently outperforms average pooling in model accuracy. In this section, evaluation results for nine global pooling schemes on three public datasets are presented. Different poolings are compared, and the influence of hyperparameters on generalized pooling is discussed. Five pooling methods with one hyperparameter each are analyzed, including l p norm pooling, soft pooling, logavgexp pooling, k-max pooling, and mixed pooling. The study explores the possibility of learnable generalized pooling through benchmark experiments. The global pooling benchmark experiment compared nine pooling methods on three datasets. K-max pooling with k=2 showed the best performance, surpassing other methods with significant accuracy increases. Training k-max pooling longer resulted in 87.2% accuracy on the Birds dataset, outperforming higher-order pooling methods for fine-grained recognition. The study compared various pooling methods for fine-grained recognition, with k-max pooling showing the best performance. Mixed pooling was the second best method, while max pooling performed similarly. Generalized pooling methods outperformed average pooling, with soft pooling and stochastic pooling being the worst performers. In a study comparing pooling methods for fine-grained recognition, k-max pooling showed the best performance, followed by mixed pooling and max pooling. Stochastic pooling surprisingly achieved the highest accuracy on the Birds dataset with Inception-v3 as the backbone model. This suggests that there may not be a universal optimal pooling scheme across all models and datasets. Average pooling was found to be suboptimal for fine-grained recognition. Max pooling consistently outperforms average pooling in fine-grained recognition tasks across different datasets and models. The study also found that localized features are better captured by max pooling compared to average pooling. Results show that max pooling performs better than average pooling across various image resolutions, confirming its superiority in model accuracy. The study validates that max pooling outperforms average pooling in fine-grained visual categorization tasks across datasets, models, and input resolutions. Different generalized pooling methods were evaluated with varied parameters, showing that they are bounded by the extremes of max and average pooling. The mean accuracy increases nearly monotonically as average pooling transforms to max pooling. Performance increases almost monotonically as average pooling transitions to max pooling, with max pooling consistently showing lower training accuracy and higher testing accuracy. Generalized pooling aims to learn an optimal hyperparameter for pooling functions, potentially outperforming both average and max pooling. Incorporating batch normalization after global pooling layers benefits all tested models, as it helps reduce output shift and aids in convergence. The reliance on \"single directions\" in the network's output indicates overfitting, and adding batch normalization after global pooling mitigates this issue. In experiments, post-global batch normalization improves model convergence and performance by approximately 1%. Heterogeneous pooling integration experiments were conducted using ResNet-50 as the backbone model. Two models with global average and max pooling were trained separately and applied as feature extractors. Multiple iterations with varied random seeds were performed for dataset enhancement, and a multi-layer perceptron (MLP) was trained using concatenated features. The multi-layer perceptron (MLP) used in the experiment has 4096 input nodes and 1024 hidden nodes, with the number of output nodes equaling the number of classes. The offline MLP classifier achieved high accuracies on fine-grained datasets, outperforming single training stage pooling models. Different pooling strategies were tested, with mixed pooling showing promising results compared to max pooling on some datasets. The study focuses on modifying the branching strategy for training a network, achieving high accuracies on three datasets. The final accuracies are 86.40%, 93.08%, and 89.97%, surpassing other methods. Visualizing conv-layer filters shows that max pooling helps learn part-level features. The study evaluated global pooling schemes for fine-grained recognition, with K-max (k = 2) pooling outperforming other models. Max pooling was found to perform better than average pooling across datasets and resolutions, displaying a monotonically increasing performance trend. The risk of overfitting with average pooling was discussed, emphasizing the importance of post-global batch normalization. The importance of post-global batch normalization is highlighted for faster convergence and improved model performance. The freeze-and-train trick is the best strategy among end-to-end learnable models. Future work should consider models learned from scratch and explore experiments on a broader set of data to generalize findings beyond fine-grained datasets."
}