{
    "title": "BJedt6VKPS",
    "content": "In this work, a set of rules for designing and initializing well-conditioned neural networks is described. The goal is to balance the diagonal blocks of the Hessian at the start of training. The conditioning of a block is related to the ratio of weight gradients to weights. For ReLU-based deep multilayer perceptrons, a simple initialization scheme using the geometric mean of fan-in and fan-out satisfies the scaling rule. This scaling principle can guide design choices for more sophisticated architectures, reducing guesswork in neural network design. In this work, a scaling quantity \u03b3 l is defined for each layer in neural networks to approximate the average squared singular value of the corresponding diagonal block of the Hessian. This quantity can be computed from the second moments of forward-propagated values and backward-propagated gradients. Networks with constant \u03b3 l are considered better conditioned, leading to the concept of preconditioned neural networks. The theory presented includes applications such as weight initialization, identifying well-conditioned networks, and improving network structures. The multilayer perceptron is used as an example, with ReLU activation functions and specific notation for each layer. Our proposed approach focuses on the singular values of the diagonal blocks of the Hessian in a multilayer perceptron network. Existing approaches like K-FAC correct the gradient step using second-order information, while our approach aims to improve the Hessian without modifying the step. The singular values of the diagonal blocks of the Hessian in a multilayer perceptron network provide insight into the conditioning of the full matrix. Balancing the magnitude of these singular values is crucial to prevent ill-conditioning. In this work, the average squared singular value of each block is used as a proxy for the full spectrum in neural networks with ReLU activation functions. The Hessian becomes ill-defined at some points, but the spectrum remains well-defined at twice-differentiable points, providing a local measure of curvature. ReLU networks are typically twice-differentiable almost everywhere, with a simple structure for the Hessian at initialization. The network's output is a piecewise-linear function fed into a final layer consisting of a loss, resulting in simplified expressions for diagonal blocks of the Hessian with respect to the weights. The network's output is considered as a composition of two functions, the current layer and the remainder of the network, written as a function of the weights. The Hessian and Jacobian of a neural network are key components in understanding its behavior. The diagonal block of the Hessian corresponding to a specific layer can be calculated using the Generalized Gauss-Newton matrix. The average squared singular value of this matrix, known as the GR scaling, plays a crucial role in analyzing the network's performance. A \"balanced\" or \"preconditioned\" network has equal scaling factors for all layers. A \"balanced\" or \"preconditioned\" network is one where the scaling factors are equal for all layers. Balancing this scaling quantity theoretically derived from the Generalized Gauss-Newton matrix can lead to optimization problems with approximately balanced Hessian blocks. However, due to various approximations made in the derivation, it is not claimed to be accurate in practice. Networks with disproportionate scaling values between layers may face convergence difficulties early on. The GR scaling equation is applied to a strided LeNet model with random input data and loss. Results show good agreement with theoretical expectations. Assumptions include element-wise i.i.d distributions and large Frobenius norm of Hessian. The pre-activations and gradients are independently distributed element-wise, with only the magnitudes correlated due to random weight matrices. Assumption A2, although problematic, simplifies the scaling of blocks without complex analysis. Similar theory can be derived for other output structure assumptions. Preconditioning is useful for various assumptions on output structure. At network initialization, the weight-to-gradient ratio captures the relative change from a single SGD step. A network with constant weight-to-gradient ratio is well-behaved under weight-decay. This ratio is equivalent to the GR scaling for MLP networks. The GR scaling for MLP networks is equivalent to the weight-to-gradient ratio at network initialization. It can be extended to convolutional layers with scaling factors and kernel widths. However, the assumption of independence of activations within a channel may lead to discrepancies in empirical estimates for convolutional layers. Padding effects are also not considered, but sequences of convolutions with the same kernel size are well-scaled against each other. The GR scaling for MLP networks can be extended to convolutional layers by adjusting the alpha parameter to correct for differences in kernel sizes. Initialization with i.i.d mean-zero random variables results in a constant GR scaling throughout the network. The geometric initialization criteria of Equation 2 are applied to bias parameters in a sequence of two layers with i.i.d, mean 0, uncorrelated, and symmetrically distributed variables. The ReLU operation affects second moments, leading to matching weight-gradient ratios. This relation also holds for sequences of strided convolutions with the same kernel size and circular padding. The initialization should be adjusted to include the kernel size. The initialization technique should be adjusted to include the kernel size, with common approaches being the Kaiming and Xavier initializations. The Kaiming technique for ReLU networks ensures constant forward-activation variance with fan-in initialization, while the fan-out variant maintains a constant back-propagated signal variance. The constant factor 2 corrects for the variance-reducing effect of the ReLU activation, with similar scaling principles observed in early neural network models using tanh activation functions. The Xavier initialization is similar to the proposed approach, balancing conflicting objectives using the arithmetic mean to maintain activation variances and back-propagated gradients variance. The literature shows confusion regarding the preference for Kaiming initialization variants. The proposed theory suggests using the geometric mean approach for balancing conflicting objectives. The Xavier initialization and the proposed approach aim to balance conflicting objectives in neural networks. The bias parameters in a network can be computed using the same proof technique as the weights. It is traditional to normalize a dataset before applying a neural network, but modern ReLU based networks question this practice. The theory provides guidance for input normalization. Our theory provides guidance for input scaling in ReLU-based networks, showing how the second moment of the input affects bias and weight parameters differently. By carefully choosing initialization, bias and weight parameters can be balanced. Using traditional normalization may result in slower learning for bias terms compared to our proposed input scaling. The behavior of a neural network is sensitive to the second moment of the outputs, especially in convolutional networks. By using geometric-mean initialization, we can control the second moment at initialization. This control may be necessary when adapting a network architecture designed under a different initialization scheme. There is currently no existing theory guiding the choice of output variance. The choice of output variance at initialization for log-softmax losses has a significant impact on back-propagated signals. Output variances of 0.01 to 0.1 are effective, and should be checked and adjusted when changing initialization schemes. Adding a layer between existing layers can be done without affecting conditioning, as long as the new layer maintains the activation-gradient second-moment product. Changing spatial dimensionality between layers affects the GR scaling. Changing spatial dimensionality between layers affects the GR scaling. The use of stride-2 convolutions and average pooling helps maintain correct scaling. Evolution in architectures shows a shift towards well-scaled building blocks like ReLU activation used in AlexNet and VGG architectures. Max-pooling and reshaping are common in these architectures before the final layers. The VGG architectures used max-pooling and reshaping before the final layers, which have been replaced in modern fully-convolutional architectures with striding and average-pooling. Recent research suggests using smaller kernel sizes for better performance. A non-convolutional ReLU network with fixed layer widths was used for multi-class classification on various datasets. The study focused on comparing optimization methods with different learning rates and initialization combinations. Results showed that geometric initialization was the most consistent. The experimental setup details and datasets used are available in the Appendix. The study found that geometric initialization was the most consistent approach among the initialization methods considered. The fan-out method, while often the best, sometimes failed to learn for certain problems, resulting in it being the worst for 9 datasets. Testing initialization methods on modern computer vision problems is challenging due to the heavy architecture search involved in current best methods. This search customizes the architecture to the initialization method used, putting other initializations at a disadvantage. Additionally, the prevalence of BatchNorm further complicates the situation as it is not addressed in the theory presented. The study found that geometric initialization was the most consistent approach among the methods considered. To provide a clear comparison, an older network, AlexNet, with a variety of filter sizes was used. Consistent kernel sizes showed negligible differences between initialization methods. The network was modified to replace max-pooling with striding and output was normalized at initialization. Alpha correction factors were added for the preconditioned variant. Preconditioned variant with alpha correction factors and geometric initialization was tested on CIFAR-10. Results showed improved training loss compared to other initialization methods, although only slightly. The scaling principle introduced allows for designing optimizable neural networks with less trial and error. This principle also automatically preconditions common network architectures. The text discusses a new initialization scheme for neural networks based on the scaling law approach. It simplifies the structure of ReLU networks for better optimization and introduces a Gauss-Newton matrix factorization for analysis. This technique aims to improve training loss and reduce the need for trial and error in network design. The (Generalized) Gauss-Newton matrix G is a positive semi-definite approximation of the Hessian of a non-convex function f, computed by factoring f into two functions where h is convex and g is approximated by its Jacobian matrix J at x. The GN matrix is closely related to the Fisher information matrix and can be used to compute diagonal blocks of the Hessian with respect to the weights Wl and inputs. The decomposition of the network structure involves a linear function and a convex function, making it convex. The Gauss-Newton approximation is not an approximation in this case. Equations for forward and backward propagation of second moments are used assuming weights are uncorrelated to activations or gradients. The weight-gradient ratio is equal to the GR scaling for MLP models. The text discusses the equivalence under zero-bias initialization and the gradient of weights in a convolutional neural network with spatial resolution \u03c1 \u00d7 \u03c1. It also considers the Jacobian of y l with respect to bias and the calculation of the product of R l with J b l r. In the context of convolutional neural networks with spatial resolution \u03c1 \u00d7 \u03c1, the text explores training residual networks without normalization layers. It discusses the necessary modifications to the network and applies a preconditioning principle to these networks. The focus is on the residual block as the building block of a ResNet model. The ResNet model utilizes residual blocks with pass-through connections, leading to an exponential increase in activation variance with depth. To mitigate this, scaling constants are introduced to each block, requiring modifications to maintain constant conditioning between blocks. The standard bottleneck block in ResNet-50 architecture includes a 1x1 convolution to reduce channels and a 3x3 convolution with equal input and output. The ResNet model uses scaling constants in each block to maintain conditioning between blocks. The block structure involves convolutions with specific scaling factors and conditioning multipliers. Initial values for the scaling factor and conditioning multiplier can be arbitrarily chosen. The weights of each convolution are initialized to ensure the output variance remains consistent. The ResNet model uses scaling constants in each block to maintain conditioning between blocks. The initialization of convolutions must be scaled down by dividing by 1 \u03b2 to ensure correct forward variance. Different scaling is needed for convolutions within residual blocks. The goal is to find a constant \u03b1 to maintain consistency between blocks. To find a constant \u03b1 for maintaining consistency between blocks in the ResNet model, scaling factors must be adjusted for convolutions within residual blocks. The scaling of convolutions is influenced by kernel sizes and initialization to ensure variance independence. The GR scaling for C0 is derived from Equation 1, with modifications needed for residual convolutions due to the non-linear response caused by residual connections. To maintain consistency in ResNet blocks, scaling factors must be adjusted for convolutions within residual blocks. The logit output of the network was scaled to have a standard deviation of 0.05 after the first minibatch evaluation for every method, with a fixed scaling constant thereafter. Weight decay of 0.00001 was applied, and LayerNorm was used to whiten the data. The network architecture used circular \"equal\" padding and ReLU nonlinearities after each convolution. It consisted of 11x11 stride-1 convolution with 3 input and 64 output channels, 5x5 stride-2 convolution with 64 input and 192 output channels, and 3x3 stride-2 convolution with 192 input and 384 output channels. Training included random augmentations, normalization to the interval [-1,+1], SGD with momentum 0.9, and a learning rate schedule with no weight decay. LIBSVM PLOTS Figure 4 shows the interquartile range of the best learning rate for each case."
}