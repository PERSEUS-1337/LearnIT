{
    "title": "BJy0fcgRZ",
    "content": "In cognitive science, understanding how people categorize information is crucial. Human learning is seen as a benchmark for artificial intelligence. Psychological research has produced various theories on categories, but validating them with real-world stimuli is challenging. Deep neural networks offer a way to represent image features, allowing for the estimation of human category structures using a blend of cognitive science and machine learning approaches. Blending human-based algorithms with deep representation learners to estimate human category structures, proving the method's feasibility. Categorization is a key problem in cognitive science, artificial intelligence, and machine learning, focusing on how we divide the world into discrete units and what we do with this information. The challenge lies in studying human categorization due to the unobservable nature of mental category representations, leading to the development of laboratory methods for estimation from human behavior. Recent work in machine learning has shown that deep learning models, like convolutional neural networks, perform well on computer vision tasks. These models can represent complex images compactly, potentially expressing human category structure. Combining experimental methods with deep learning models could help estimate the structure of human categorization. Recent work in machine learning has shown that deep learning models, like convolutional neural networks, can estimate human category structure by combining experimental methods. A proposed method uses a human in the loop to estimate distributions over complex feature spaces, aiming to capture the precise structure of human category representations. This knowledge is essential for understanding intelligent categorization behavior and guiding future work in machine learning. In a procedure, human participants are shown stimuli from two classes, A and B, with white noise overlaid, and asked to indicate the correct label. The decision boundary can be estimated by analyzing trials where noise affects the image distinction. BID15 used classification images with invertible deep feature spaces to avoid dataset bias. Category templates reduce to n A \u2212 n B in this case. Participants select images in each trial of the experiment. Participants in the experiment selected images resembling specific categories by choosing between two inverted images with feature noise. Pre-inverted images were generated offline using random feature vectors. This method produced category template images and improved machine classification boundaries. The assumption of Gaussian human category distributions with equal variance guided the alignment of vectors between means. BID5 also conducted experiments using a low-dimensional gabor PCA basis for representing black-and-white scene images. The gabor PCA basis was used to represent black-and-white scene images. Participants used judgments in an online genetic algorithm to converge to their mental image. An alternative method, MCMCP, involves humans as an acceptance function in the Metropolis-Hastings algorithm. The MCMCP method involves comparing stimuli in a parameter space for categorization. It has been used successfully to capture mental categories, estimating means, variances, and higher order moments without assumptions about category distributions. This method is a starting point for the paper. Deep convolutional neural networks like AlexNet are effective for image classification but can be difficult to interpret. Human raters participate in an experiment using an MCMC sampling loop to choose between images, which are then decoded for the next trial. Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) offer a generative approach to understanding image categories by decomposing image-label joint density. This theoretical framework allows for easy visualization of image generation constraints. In the context of image generation, perturbing images in a deep feature space can help capture essential variation for easier visualization of the latent space and learning category distributions. This method allows for meaningful changes in content compared to adding noise in the stimulus parameter space. The proposed method for learning generative image models does not require an inference network, as the latent code used to produce images is known. Deep generative networks like GANs and VAEs learn a probability distribution over latent variables, enabling the mapping of latent variables to pixels in the dataset. The proposed method for learning generative image models does not require an inference network. Deep generative networks like GANs and VAEs learn a probability distribution over latent variables to map them to pixels in the dataset. The family of deterministic functions f operates over pixels in the dataset, while the family of functions g relates to the deep latent space and human perception. The equation assumes that humans approximate the distribution of images analogously to an unsupervised learner, with the goal of encoding information in deep generative models. The proposed method for learning generative image models does not require an inference network. It assumes that psychological space y should be similar to deep latent space z for convergence using human participants. MCMCP can capture arbitrary distributions and provide better category boundaries than classification images when needed. This method has theoretical advantages over previous efforts. Using MCMC for generating images online can reduce sampling bias and address auto-correlation by removing temporally adjacent samples. Deep generator networks offer clearer samples than shallow methods and can potentially be trained end-to-end for categorizing new images. Two image generator networks were explored in experiments, utilizing a hybrid proposal distribution due to the large size of deep image embeddings compared to stimulus parameter spaces. Participants in all experiments completed 64 trials in about 5 minutes, using a hybrid proposal distribution for image comparisons. Experiments were conducted on Amazon Mechanical Turk, with data discarded if an image failed to load. The method was tested using DCGAN BID13 trained on the Asian Faces Dataset. The method was tested using DCGAN BID13 trained on the Asian Faces Dataset. Four chains for each of four categories (male, female, happy, and sad) were used, with proposals generated from an isometric Gaussian. The dataset had 50 participants and over 3,200 trials in total. MCMCP chains were visualized using Fisher Linear Discriminant Analysis in FIG1. Linear Discriminant Analysis in FIG1 shows interesting variation in chain means within categories, converging to similar regions in latent space. Mean faces for MCMCP chains appear to have converged quickly, while CI means only moderately resemble their categories. A human experiment with 30 participants showed a strong preference for MCMCP means over CI means as representations of each category, as reported in FIG2. The study compared MCMCP and CI methods for obtaining category templates, with MCMCP showing better results. A BiGAN model trained on a large dataset was used to generate competitive samples. This approach allows for comparing human and machine classification performance. Our generator network was trained using uniform noise to prevent participants from getting lost in improbable regions. Proposals are generated from an isometric Gaussian with varying standard deviations. Chains were run through an unbounded state space for two groups of categories. Group1 included bottle, car, fire hydrant, person, and television. Group2 included bird, body of water, fish, flower, and landscape. Each chain was approximately 1,040. In total, 41,600 samples were obtained from 650 participants across two groups of categories. The efficiency of the method was demonstrated by obtaining an equivalent number of trials for all categories using a BiGAN generator. The acceptance rate was around 50%, which is a common goal for MCMCP experiments. Samples for all categories are shown using Fisher Linear Discriminant Analysis. Our method efficiently estimates well-separated category means in a manageable number of trials, allowing for scalability. Unlike classification images, which show little separation with few trials, our method produces a density estimate of the entire category distribution and visualizes multiple modes within each category. The study visualizes multiple modes in category distribution using means of each component in a mixture of Gaussians density estimate. It produces realistic multi-modal mental category templates, a novel achievement in natural image categories. Quantitative assessment of obtained samples is compared to classification images using an external classification task with images scraped from Flickr. Decision rules based on MCMCP-obtained samples are tested for classification. The study demonstrates the effectiveness of decision rules based on MCMCP-obtained samples in outperforming a nearest-mean decision rule using classification images. The method leverages psychological methods and deep surrogate representations to capture human category representations, visualizing multi-modal category templates and improving classification performance benchmarks. The study shows the effectiveness of decision rules based on MCMCP samples in outperforming a nearest-mean decision rule using classification images. The framework presented can be improved as generative image models advance, and known methods for enhancing MCMC algorithms can be applied to make better use of costly human trials. However, there are limitations in the structure of the feature spaces used, which may lack expressiveness or have too many irrelevant features, requiring many trials to reach convergence. Continuing exploration of generative image models is needed to reach convergence and refine human observations. This iterative process informs new deep network developments."
}