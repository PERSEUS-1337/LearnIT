{
    "title": "r1lEjlHKPH",
    "content": "In a continual learning setting, deep neural nets struggle to retain knowledge from prior episodes when new categories are introduced. A new model proposed in this paper aims to address this issue by combining the power of deep neural nets with resilience to forgetting. The model shows improved accuracy on original classes compared to a standard deep neural net, highlighting the challenge of adapting to new tasks while retaining knowledge in deep neural networks. Continual learning aims to tackle the issue of adapting to new tasks while retaining knowledge from old ones. One major challenge is catastrophic forgetting, where a machine learning model performs poorly on old tasks after being fine-tuned for new ones. Various methods make different assumptions about task identities and boundaries, with some focusing on task learning and domain learning. In continual learning, different methods make assumptions about task identities and boundaries. Task learning assumes known task identity at test time, while domain learning removes this assumption. Class learning involves classifying among possible classes within a task, while discrete task-agnostic learning removes the assumption of known task identity at training time. Continuous task-agnostic learning assumes unknown task boundaries even at training time. In continual learning, methods vary in assumptions about task identities and boundaries. Existing methods can bias parameters towards old task values or expand model size. Care is needed when interpreting results across different papers to ensure evaluation under the same setting. Refer to (van de Ven & Tolias, 2019; Hsu et al., 2018) for more details on method performance under different settings. In this paper, a new approach is proposed for continual learning that focuses on keeping parameter changes localized to prevent neural nets from forgetting. The key idea is to regularize parameter changes in the lower layers without reducing the network's expressive power, thus improving retention without penalizing parameter changes or expanding model size. To prevent neural nets from forgetting while fine-tuning on new tasks, a solution is proposed to localize parameter changes in lower layers without compromising expressive power. By replacing the softmax activation layer with a k-nearest neighbour classifier, catastrophic forgetting can be reduced effectively. This approach offers a novel solution to continual learning. Our work proposes a novel solution to continual learning by leveraging ideas from metric learning, aiming to reduce catastrophic forgetting. Continual learning involves machine learning models retaining previously learned skills while adapting to new information. This capability is crucial in various fields such as robotics and computer vision. While retraining models on combined old and new datasets is one approach to continual learning, it is computationally expensive and infeasible due to the inability to store old data. Continual learning involves retaining previously learned skills while adapting to new information. One common shortcut is to finetune a model trained on old data with new data, but this can lead to decreased performance on old data. Recent interest in continual learning has led to methods that dynamically expand model architecture or constrain parameter values to preserve old network weights while adapting to new tasks. Methods for continual learning include preserving old network weights while adapting to new tasks through regularization-based methods like elastic weight consolidation (EWC), online EWC, and synaptic intelligence (SI). These methods protect important parameters by penalizing changes from previous task training values. Data replay-based methods, such as Learning without Forgetting (LwF) and Deep Generative Replay (DGR), generate simulated training data from previous tasks to augment training on new tasks. These methods use distillation and deep generative models to simulate input data and labels from previous task models. Other related methods also utilize training data from previous tasks. Some methods in continual learning use training data from previous tasks to augment the current training set. These methods require knowledge of task boundaries and task identities, except for the setting where neither are known. Metric learning involves learning a (pseudo-) Metric learning involves learning a distance metric, which can be combined with a k-nearest neighbors (k-NN) classifier for classification predictions. It is possible to learn a Mahalanobis distance metric to improve the accuracy of k-NN, enforcing clustering of labels with large margins between clusters. Recent work has applied similar ideas in deep neural networks, defining the distance between data points as the Euclidean distance between their embeddings. The challenge in metric learning is designing a loss function for efficient training of neural networks. Various loss functions have been proposed with varying success on different datasets. The key observation is that for a neural net to learn on non-linearly separable data, the parameters in the layers below the last layer must change. If the new task's training data is not linearly separable from previous tasks, the parameters below the last layer will be adjusted during training on the new task. To prevent catastrophic forgetting in neural networks when training on new tasks, the last layer should be made more expressive by replacing it with a nonlinear classifier like a k-nearest neighbour classifier using a learned Mahalanobis distance metric. This approach ensures that the parameters below the last layer remain stable even as the model learns new tasks. The proposed classifier utilizes a feedforward neural network with a defined loss function for training. It incorporates a k-nearest neighbor approach using a learned Mahalanobis distance metric to prevent catastrophic forgetting in neural networks. The proposed classifier uses a neural network with a loss function based on metric learning. The goal is to ensure accurate predictions for the k-nearest neighbor classifier on the last layer's feature activations. Positive and negative examples are used to train the model, with anchors representing one example per class. The loss function is a modified triplet loss to optimize the distance between examples of the same class. The training procedure involves using a modified triplet loss to encourage tight clusters of training examples of the same class. Anchors are sampled for each class, and positive and negative examples are used to train the model. The goal is to optimize the distance between examples of the same class in the neural network. During training, a batch of examples is selected to compute distances between anchors and training examples. Triplets are constructed for training, ensuring the positive example is closer to the anchor than the negative example. The dynamic margin parameter controls the distance to the farthest negative example included in the triplet. Excluding triplets where the positive example is farther from the anchor prevents mapping all examples to zeros. Closest examples to the anchor are retrieved during training. During training, the Prioritized DCI algorithm is used to retrieve closest examples to the anchor. The output embedding is normalized for efficient k-nearest neighbor search. Performance comparison on MNIST shows the proposed method outperforms other continual learning methods. The proposed method for continual learning does not assume knowledge of task boundaries. The classifier makes decisions based on cosine similarity between image embeddings. Training data is presented in stages, with Set A and Set B representing different categories. The training procedure involves training on Set A first before moving to Set B. The training procedure involves training on Set A first before moving to Set B. Evaluation on Set A is done after each step to assess performance retention. The baseline is a neural net classifier with softmax activation trained with cross-entropy loss. To train on Set B, the last layer weights are not used and initialized randomly. After training on Set B, new weights for the last layer are trained for evaluation on Set A. The proposed method replaces softmax activation with a k-nearest neighbour classifier. The proposed method uses a k-nearest neighbour classifier for training on Set B, showing better adaptation compared to other methods on CIFAR-10 data. Some methods suffer from catastrophic forgetting on Set A but adapt well to Set B. Pretraining with cross-entropy loss is used to reduce training time on large datasets. The proposed method utilizes a k-nearest neighbor classifier for training on Set B, demonstrating improved adaptation on CIFAR-10 data compared to other methods. It avoids catastrophic forgetting on Set A and shows good adaptation to Set B. Pretraining with cross-entropy loss is employed to reduce training time on large datasets. The method is compared to seven existing continual learning methods on MNIST, including EWC, online EWC, SI, DGR, DGR with Distillation, RtF, and LwF, across different datasets/splits. The CIFAR-10 dataset is split into two sets: animal categories and man-made object categories. A variant with 5 tasks is also considered, where each task corresponds to a pair of consecutive categories. ImageNet is larger and more complex, with 100 classes randomly selected for each set. The CIFAR-10 dataset is split into animal and man-made object categories. ImageNet is split into two sets: Set A with 880 non-dog categories and Set B with 120 dog breed categories. Performance comparison shows various continual learning methods' ability to adapt to new tasks without forgetting previous ones. The performance comparison between our method and the vanilla baseline on ImageNet shows the network's ability to adapt to new tasks. The network is evaluated based on top-1 percent correct on test data for the original task, with Baseline representing the network trained with cross-entropy loss. The network is fine-tuned on new tasks after being trained on the original task and uses standard network architectures proven to work well on the datasets considered. The study compares different methods for continual learning on ImageNet and MNIST datasets. The AlexNet architecture is used for ImageNet, while various methods like EWC and DGR are evaluated on MNIST. Regularization-based methods show forgetfulness of previous tasks but adapt well to new tasks, while replay-based methods perform better in retaining knowledge. The proposed method in the study can adapt to new tasks and retain knowledge from previous tasks, outperforming other methods like DGR and LwF. Results on CIFAR-10 and ImageNet datasets show that while the proposed method initially has lower test accuracy, it excels in remembering learned knowledge when trained on new data. The proposed method in the study outperforms other methods like DGR and LwF on CIFAR-10 and ImageNet datasets by adapting to new tasks and retaining knowledge from previous tasks. It achieves nearly double the test accuracy of the baseline on ImageNet and CIFAR-10, with a smaller gap on ImageNet (dog split) due to diminished similarity between sets A and B. Fine-tuning on set B may erase general image knowledge, while fine-tuning on ImageNet (random split) preserves useful concepts for classifying images in set A. Further analysis on the ImageNet dataset with random split includes varying hyperparameter settings and assessing their impact. The proposed method in the study outperforms other methods on CIFAR-10 and ImageNet datasets by adapting to new tasks and retaining knowledge from previous tasks. Increasing the number of positive examples showed a slight improvement, while decreasing the number of negative examples resulted in a slight performance drop. The method is reasonably robust to hyperparameter changes, with disabling embedding output normalization significantly dropping performance on both tasks. Removing the dynamic margin and replacing it with a static value also had an impact on performance. The study found that embedding normalization and dynamic margin are crucial for the method to adapt well to new tasks. Disabling these components significantly lowers test accuracy, especially limiting the ability to adapt to new tasks like Set B. The method successfully addresses the catastrophic forgetting problem across different datasets. The method successfully addresses catastrophic forgetting by replacing the last layer of a neural net with a non-linear classifier. Future plans include combining this approach with other methods and exploring applications in reinforcement learning."
}