{
    "title": "Skx6WaEYPH",
    "content": "In this paper, the Fourier analysis of deep ReLU neural networks reveals the presence of non-zero high frequency components contributing to vulnerability against adversarial attacks. A post-averaging technique is proposed to smooth out these components, enhancing the networks' robustness. Experimental results on ImageNet and CIFAR-10 datasets demonstrate the method's effectiveness against various adversarial attack methods. The post-averaging method defends against FGSM, PGD, DeepFool, and C&W attacks without re-training, protecting over 80-96% of adversarial samples with less than 2% performance degradation. Adversarial samples can deceive deep neural networks, leading to erroneous outputs. Research on adversarial attacks and defenses is active, with small imperceptible perturbations being common in image classification tasks. In this work, various ways to generate effective adversarial samples have been proposed to circumvent new defence methods. Different attacks are effective against different defences or datasets, leading to no consensus on the strongest attack. The proposed defence approach will be evaluated against four popular attacks for empirical analysis. Various defence mechanisms have also been proposed, including adversarial training, network distillation, gradient masking, adversarial detection, and modifications to neural networks. Many defense mechanisms have been proposed, such as adversarial training, network distillation, gradient masking, adversarial detection, and modifications to neural networks. However, these defenses have been quickly overcome by new types of attacks. There is no universally robust defense against all adversarial attacks, and the existence of adversarial samples has been investigated. Transferability of adversarial samples across different models and training sets has been observed. The existence and transferability of adversarial samples across different models and training sets have been well-documented. Adversarial vulnerability remains a challenge, with the reason still requiring further investigation. The Fourier analysis of neural networks is explored in this paper to understand the impact of small perturbations on neural network functions. The paper discusses the Fourier analysis of neural networks, highlighting the vulnerability to adversarial samples due to high frequency response components. A simple post-averaging method is proposed to address this issue without requiring re-training. Experimental results show the method effectively defends against various attacks on ImageNet and CIFAR-10 datasets, successfully protecting over 80-96% of adversarial samples. The paper discusses defending against adversarial samples in neural networks by analyzing their Fourier transform. It focuses on fully-connected neural networks using ReLU activation functions and their piece-wise linear functions in input space. The main results are presented, with more details available in the Appendix. A linear function in a fully-connected ReLU neural network is piece-wise linear, with regions divided by hyperplanes. Each region is a union of half-spaces, making mathematical analysis complex. For mathematical analysis, regions are decomposed into infinite simplices defined by linearly independent vectors. Piece-wise linear functions can be expressed as a sum of simpler functions within infinite simplices. Input to neural networks can be normalized into a unit hyper-cube for computational convenience. The piece-wise linear function can be simplified by adding hyperplanes to split the input space. Each region has a specific element with the largest absolute value. The function can be represented as a sum of functions, each transformed by an invertible matrix. The Fourier transform of fully-connected ReLU neural networks shows that high frequency components are mainly influenced by small regions in the network. This is due to the contribution of matrices A -1 q, which are related to the volume of these regions in R n. Neural networks' high frequency components are linked to small regions in the input space, which can be exploited to create adversarial samples. Each layer of a neural network divides the input space into numerous small regions, resulting in a large number of sub-regions overall. For instance, a neural network with 1000 nodes in a hidden layer and an input dimension of 200 can partition the input space into approximately 10^200 regions. Even a medium-sized neural network can divide the input space into a vast number of sub-regions, exceeding the total atoms in the universe. These regions may not have training samples, leading to arbitrary linear functions. Adversarial attacks pose a challenge as these tiny regions are prevalent throughout the input space. Tiny unlearned regions in the input space where linear functions are arbitrary contribute to the vulnerability of neural networks to adversarial samples. In deep neural networks, the linear functions in these regions are interconnected. Perturbations in input result in fluctuations in hidden node outputs, influenced by the number of hyperplanes crossed. Practical neural networks consist of tens of thousands of hyperplanes. In practical neural networks, there are tens of thousands of hyperplanes in a high-dimensional space. Even with regularization, small perturbations can easily cross many hyperplanes, leading to significant output fluctuations. For example, in ImageNet data with a VGG16 model, an average of 5278 hyperplanes are crossed per layer with a small perturbation of ||\u2206x|| 2 \u2264 0.35. This highlights the presence of unlearned tiny regions that contribute to neural network vulnerability. The proposed defense approach involves post-processing to smooth out high frequency components in neural networks, aiming to eliminate adversarial samples. This method utilizes a moving-average concept to generate predictions based on averaged values within a small neighborhood around data points. Further research is needed to effectively implement bandlimiting in neural networks to filter out decaying high frequency components and enhance defense strategies against adversarial attacks. The post-averaging technique involves smoothing out high frequency components in neural networks by averaging values within a small neighborhood around data points. This method aims to bandlimit neural networks to improve robustness against adversarial attacks. Similar ideas have been used in speech recognition to enhance defense strategies. In this work, a simple numerical method is proposed to approximate the integral for improving robustness in speech recognition. By selecting points in the neighborhood of the input, the integral is computed approximately. A sampling method based on directional vectors is used to generate samples outside the unlearned region to defend against adversarial samples. In this work, a sampling method based on directional vectors is used to generate samples for defending against adversarial attacks on ImageNet and CIFAR-10 datasets without the need for re-training neural networks. The proposed post-averaging method does not require re-training neural networks or using training data in experiments. Evaluation is done on a subset of 5000 images from the ImageNet validation set and the full test set of CIFAR-10. Pre-trained ResNet models are used for both datasets without modification. Adversarial samples are generated using Foolbox toolbox. The study tested a method against four popular adversarial attacking methods using default settings and an l \u221e norm for perturbation distance constraint. The Clean set consists of original images, while the Attacked set replaces correctly classified images with adversarial samples if generated successfully. In the experiments, the original network and the network defended by post-averaging are evaluated on both the Clean and Attacked sets. Performance is measured in terms of accuracy and defense rate. The defense approach's performance against different attacking methods is shown in Table 1. The defense approach using post-averaging is evaluated against various attacking methods. Results show high defense rates of 80-96% with minimal performance degradation in the Clean set. The approach is robust to all examined attacking methods, generating above 91 samples for each input image. Our method shows high defense rates against various attacking methods, with minimal performance degradation in the Clean set. It is particularly effective on the ImageNet dataset, defending about 95% of adversarial samples. The optimal sampling radius varies with different datasets due to their dimensionality and data sparsity. In experiments, r = 30 for ImageNet and r = 6 for CIFAR-10 achieved better performance. The model defense rate on ImageNet varies with r, with high defense rate throughout the r range [15, 30]. The model performance is not very sensitive to the number of sampling directions used (K), achieving a good defense rate with only K = 6. After evaluating the inference time for models with different K values, the post-averaging defense approach was tested against attacks with various perturbation ranges. The defense rate remained high up to = 32/255 against most attacks, except for FGSM which showed lower defending performance. This could be due to FGSM generating larger adversarial samples. In this paper, theoretical results by Fourier analysis of ReLU neural networks are presented to understand why neural networks are vulnerable to adversarial samples. The existence of tiny unlearned regions in the model function mapping may be a major reason for vulnerability. A post-averaging defense method is proposed, showing high defense rates against most attacks but lower performance against FGSM due to larger perturbations. Our defense technique has shown effectiveness against popular attack methods on ImageNet and CIFAR-10 datasets. The post-averaging method's robustness against new attack methods remains to be seen. Piece-wise linear functions and infinite simplices are defined and discussed in the context of the paper. The text discusses how a function can be formulated as a summation of linear functions in infinite simplices. It explains how a piece-wise linear function intersects with affine hyper-planes to form convex polytopes, which can be triangulated into simplices. By adding hyper-planes perpendicular to standard basis vectors, the output of a ReLU node can be represented as a sum of functions. The text discusses the Fourier transform of a function represented as a sum of linear functions in infinite simplices. It explains how the function can be computed by taking the inverse Fourier transform and finding the Fourier transform of s(x) using frequency vectors. The lemma presented shows the computation of the Fourier transform of a specific function, involving the Dirac Delta function and summation over elements. The integral result is derived for sets with two elements, one being \u03c9 r, where Ar = 0. The Fourier transform is computed using the property of Fourier transforms for radially symmetric functions."
}