{
    "title": "Hygm4cBj24",
    "content": "In this paper, a method is introduced within the continual learning paradigm that effectively forgets less useful data samples across different tasks. The method uses statistical leverage score information to measure data sample importance and adopts a frequent directions approach for life-long learning. This maintains a constant training size across all tasks and is demonstrated with experiments on variants of MNIST. In this paper, a method is introduced for lifelong learning that addresses catastrophic forgetting in machine learning models. The method uses statistical leverage scores to measure data sample importance and adopts a frequent directions approach. Experiments on MNIST and CIFAR100 datasets demonstrate its effectiveness. The typical structure of DNNs does not preserve previously learned knowledge without interference between tasks. Approaches to address this include regularization (e.g. EWC, SynInt) and architectural modification to provide more memory for new task input. Regularization and architectural modifications are used to address interference between tasks in DNNs. Memory replay stores data samples from previous tasks to retrain the new model. However, regularization can saturate learning with a large number of tasks, leading to scalability issues. Modifications like progressive networks can help resolve saturation problems but may not scale well with increasing task complexity. In this paper, a novel approach to lifelong learning with DNNs is proposed to address learning saturation and high computational complexity issues. The method involves compressing input information learned so far and from current tasks to form more efficiently condensed data samples. The compression technique is based on statistical leverage scores and frequent directions, connecting compression steps for a sequence of tasks. This approach is similar to memory replay but does not require extra memory for training, making it cost-efficient. The proposed approach in this paper aims to address learning saturation and computational complexity issues in lifelong learning with DNNs. It involves compressing input information from current and past tasks efficiently using statistical leverage scores and frequent directions. This method is akin to memory replay but does not require additional memory for training, making it cost-effective. The objective is to train a DNN over a sequence of tasks while ensuring good performance on all tasks with a fixed network architecture. The new approach OLSS aims to minimize loss by approximating task information in a streaming manner without extra memory or computation cost. It forms an approximation of task information to perform well on all tasks with a fixed network architecture. During training, the approximate i is constrained to have the same number of rows as the current task Ai. Nonlinear least squares problems can be solved using an approximation derived from linear least squares problems. Leverage score sampling is a cost-effective technique used for solving large-scale linear least squares and low-rank approximation problems. The statistical leverage score of a matrix A's i-th row is defined as U(i,:) 2 2 for i \u2208 {1, ..., n}. It indicates the non-uniformity structure of the matrix, with a higher score showing a heavier contribution to the non-uniformity. Leverage score sampling helps in selecting important samples from a dataset by sampling rows based on their scores. To embed this in a sequence of tasks, frequent directions are used to extend the concept of frequent items to matrices. The algorithm uses frequent directions to find a low-rank approximation on an expanding matrix, suitable for a continuous stream of data in lifelong learning. It involves leveraging score sampling and compression to process a sequence of tasks efficiently. The algorithm utilizes frequent directions for low-rank approximation on an expanding matrix in lifelong learning. It involves score sampling and compression to efficiently process a sequence of tasks. The model parameters are initialized, and a space parameter is set for training neural networks on the tasks. A buffer set is initialized, and SVD is performed on the matrices. Random rows are selected for training the model. The algorithm uses frequent directions for low-rank approximation in lifelong learning. It suggests using a streaming SVD method to speed up computation when dealing with large matrices. Efficient ways to approximate leverage scores are also mentioned to reduce computational costs. However, a concern is raised about the linear measure of leverage scores and the potential dependency of a data sample's importance on the DNN architecture. The proposed algorithm OLSS is evaluated on three classification tasks: Rotated MNIST, Permutated MNIST, and Incremental CIFAR100. Each task consists of a varying number of training and testing samples. In the experiments, 20 tasks are conducted, each with 5 classes and 2,500 training and 500 testing samples. A softmax layer is added to the output vector, allowing past classes to output values larger than 0. Different DNN architectures are used for rotated and permuted MNIST and incremental CIFAR100 experiments. Training is done for 5 epochs on rotated and permuted MNIST with batch size 200, and 10 epochs on incremental CIFAR100 with batch size 100. In experiments, 10 epochs with batch size 100 on incremental CIFAR100 were conducted. Algorithms compared include simple SGD predictor, EWC, GEM, and OLSS. Regularization and memory hyper-parameters were set as described. Test accuracy and computational costs were compared across algorithms. Across three benchmarks, OLSS and GEM show similar accuracy, outperforming EWC and simple SGD. However, GEM requires higher computational resources due to constraint validation and gradient projection steps. GEM's time complexity is proportional to memory buffer size, model parameters, and convergence iterations, while OLSS requires SVD or QR factorization for leverage scores computation, with lower time complexity compared to GEM. OLSS demonstrates robustness to catastrophic forgetting and positive backward transfer across tasks, especially on rotated and permuted MNIST datasets. GEM and OLSS maintain accuracy on most tasks after training on the entire task sequence, but struggle with preserving accuracy on previously trained tasks in CIFAR100. EWC shows saturation issues with increasing tasks, limiting model capacity for continual learning. The new approach addresses lifelong learning with deep neural networks by combining importance sampling and frequent directions. Results on MNIST and CIFAR100 show its effectiveness compared to state of the art methods."
}