{
    "title": "By3VrbbAb",
    "content": "Search engine users rely heavily on query completion and correction to shape their queries. The paper proposes using unsupervised deep language models to complete and correct queries with any prefix. The method addresses challenges by integrating error correction into the language model and efficiently performing CPU-based computation in real time. Experiments show a significant increase in hit rate and the ability to handle tail queries. The paper proposes a real-time search completion architecture using deep character-level language models to generate candidate completions for search queries. By integrating deep language models, the system aims to accurately predict and suggest completions for user input, improving search efficiency and user experience. The paper introduces a real-time search completion system that combines character-level language models with an edit-distance-based potential function to provide error-correcting completions for search queries. The system aims to generate high-quality completions quickly, enhancing user experience. The paper introduces a real-time search completion system that combines character-level language models with an edit-distance-based potential function to provide error-correcting completions for search queries. The method substantially outperforms highly optimized standard search completion algorithms in terms of hit rate and can execute in real time for search engines. Experiments and code are available online, with a real-time demo at http://www.deepquerycompletion.com. The text discusses database lookup methods and learning-based approaches for query completion. Database lookup involves fetching known queries matching a prefix and returning the most frequent candidates. Learning-based approaches have also been used in recent years. In recent years, various learning-based methods have been used for query completion. Different approaches such as BID14, BID8, BID10, BID13, and BID11 have been proposed, focusing on word-level translation models, code completion, neural networks, logistic regression, and character-level language models. These approaches are relevant but orthogonal to the focus on character-level modeling, beam search, and realtime completion in the current discussion. The current work focuses on character-level language models for completion, unlike previous approaches that use embeddings like word2vec. Error correction and prediction time are key drivers for this work. Previous methods include heuristic and learning-based approaches for error and spelling correction, such as generating candidate sets for common errors and using ternary search trees for spelling correction. Learning-based models are also explored for query completion. The current work focuses on character-level language models for query completion, using a learning-based approach with an n-gram Markov model. Unlike previous methods, this approach aims to maximize the probability of a recommendation being clicked when a user types a prefix string in the search engine. The current work discusses query completion using character-level language models to find top probable strings that maximize additional metrics like click-through rate. The model simplifies by focusing on the probability of the next character under the current prefix, resembling a character-level language model learned in an unsupervised manner. Character-level language models, like recurrent neural networks (RNNs) and long short term memory networks (LSTMs), are popular for high-fidelity modeling. They can be trained unsupervised on datasets of unannotated search queries to adapt to user search terms, achieving state-of-the-art performance. The model used for character-level language modeling is a recurrent neural network with hidden states, specifically an LSTM model implemented in the Keras library. It utilizes one-hot encoding of characters as input, a two-layer LSTM with 256 hidden units, and a softmax function for prediction. The model is trained to adapt to new searches by retraining on all data collected up to the current point. The language model is trained to maximize log likelihood by minimizing categorical cross-entropy loss. Queries are padded with an end-of-sequence symbol for completion prediction. The decoding problem of finding the best sequence probability is NP-hard, so approximation methods like sampling are used to generate the next character until an end-of-sequence symbol is reached. Beam search is a classic method for generating more probable sequences by performing breadth-first search and keeping the top candidates. It helps to overcome the limitations of greedy sampling and produces more intuitive results. By using beam search, a more probable set of completions can be consistently obtained compared to stochastic search. However, this method has issues with error correction and speed. A naive implementation of the model is slow, taking around one second to produce 16 completions. The next sections address these issues by introducing technical contributions. Query completion involves more than just completing a fixed prefix, as the input may contain mistakes and users may want to insert keywords. Traditionally, error correction is done by matching the input to a typo. The text discusses error correction and insertion completion using a language model-based approach. It explains how to estimate the probability of query completion by augmenting the conditional distribution with an additional probability term. This allows for handling spelling correction and insertion completion in one model. The text discusses estimating the probability of query completion by incorporating an additional probability term to handle spelling errors. The edit distance function is used to measure the distance between two strings, representing the number of errors in the original prefix. The probability of a spelling error is modeled with a constant error rate, with a 2% error rate giving \u03b1 = 4. The edit distance algorithm calculates distance between strings, with a proposed modification for search completion. It takes O(m \u00b7 m ) time to run and can be optimized to O(|C|rm) complexity by maintaining edit distances for candidates. To handle insertion completion without penalty, a new distance function called \"completion distance\" was designed. This allows for more accurate completions like \"poke go\" to \"pokemon go\". This concept can be applied beyond edit distance, such as in product search engines to optimize metrics for high-value completions. The challenge now is to implement real-time completions using this approach. The key challenge now is to perform real-time completions for query suggestions. The goal is to provide 16 completions in 20 milliseconds, but current implementations are too slow. By improving the performance by over 50 times, completion times under 20 milliseconds have been achieved. This was done by optimizing the beam search process to reduce redundant forward propagations. The algorithm illustrated below optimizes LSTM forward propagation for real-time query suggestions, reducing completion times significantly. By amortizing forward propagation over the search tree, the complexity for computing completions is reduced. The algorithm optimizes LSTM forward propagation for real-time query suggestions, reducing completion times significantly by implementing beam search and forward inference in C on the CPU. This approach is more efficient than using GPUs due to the need for complex data structures and edit distance computation. By optimizing LSTM computations with batch matrix-matrix operations and Intel MKL BLAS, significant speedups were achieved. A tweaked LSTM implementation also improved performance, with sigmoid term computation accounting for 30% of total time due to branch prediction and non-consecutive activations. By grouping together terms in the hidden state and using Intel SSE-based operations for the hard sigmoid, completion time is reduced to 13.3ms. Evaluation on the AOL search dataset BID12 shows 36M total queries, with a maximum sequence length of 60 characters. Training and testing splits involve choosing a random cutting point for each example. In the training and testing splits, queries are sorted and split based on prefix MD5 hash or timestamp. A character-level language model is trained on the queries, with an LSTM model trained for 3 epochs over the dataset. Training took 7.2 hours on a GTX 1080 Ti GPU. The character-level language model was trained for 3 epochs on the entire dataset using a 2-layer LSTM with 256 hidden dimensions and dropout of 0.5. Adam was used for training with a step size of 1e-3 and minibatch size of 256. The LSTM model was found to be prone to overfitting with 512 dimensional hidden units. In the main text, speedups achieved by different optimizations in TAB2 are summarized. Stochastic search takes three times longer than beam search due to generating longer completions. Despite being simpler, stochastic search is substantially slower with worse completions. Evaluation compares beam search to stochastic search and a trie-based completion model in TAB3. In TAB3, the performance of beam search versus stochastic search for query completion is highlighted in terms of log likelihood under the model. Beam search produces substantially better results in model likelihood and is 3x faster. Error correction was not included in this comparison. The total approach of beam search with error correction is compared to a trie-based completion model using probabilistic coverage metrics. The performance of the trie-based approach, beam search, and beam search with error correction is evaluated based on conditional probability and hit rate metrics. The models generally outperform trie-based approaches, except for probabilistic coverage on the time-based split. Error correction method shows better completions based on hit rate. The trie-based lookup method has better completions than alternative approaches, but it has notable disadvantages such as being memory intensive and not offering completions for unseen prefixes. It is not suitable for error correction and can only handle up to 2 edits, unlike our approach which can handle 4-5 edits easily. The paper presents a search query completion approach using character-level deep language models. It integrates error correction and efficiently generates candidate completions with beam search. The system works in real time with optimizations like a CPU-based custom LSTM implementation, producing better completions than simple prefix lookup."
}