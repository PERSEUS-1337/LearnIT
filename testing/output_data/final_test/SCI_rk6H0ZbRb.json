{
    "title": "rk6H0ZbRb",
    "content": "Many machine learning classifiers are vulnerable to adversarial examples due to the universal functional form of logit differences, independent of architecture, dataset, and training protocol. This leads to adversarial error scaling universally as a power-law with respect to the size of the perturbation, across various datasets, models, and attacks. The study explores the effects of reducing prediction entropy on adversarial robustness and the impact of network architectures on adversarial sensitivity. Using neural architecture search with reinforcement learning, a more robust architecture was found on CIFAR10, which is resistant to both white and black box attacks. An intriguing aspect of deep learning models in computer vision is their vulnerability to adversarial examples, which challenges our understanding of generalization and poses security risks. Techniques like adversarial training, defensive distillation, feature squeezing, and Parseval training have been proposed to enhance robustness against such attacks. The goal of this work is to study the common properties of adversarial examples across various network models and datasets. Adversarial error, caused by different attack methods, grows as a power-law with a specific exponent. Adversarial error caused by different attack methods scales as a power-law with specific exponents, indicating a mysterious commonality between models and datasets. The universality is not dependent on the specific content of the datasets or the model's ability to generalize. The success of an adversarial attack at small scales depends on the input-logit Jacobian of the model and the logits of the network. The susceptibility of machine learning models to adversarial attacks is determined by the difference between the most likely logit and the second most likely logit. Training with single-step adversarial examples can protect against large attacks but not small ones. Adversarial error scales as a power-law with specific exponents, showing a commonality between models and datasets. Our work explores adversarial robustness through neural architecture search, finding architectures robust to perturbations. We analyze the relationship between model size, accuracy, and robustness. Key contributions include studying adversarial error and logit differences across models and datasets, revealing universal patterns. Our work focuses on improving adversarial robustness through neural architecture search, demonstrating that better network architectures and entropy regularization can enhance quantitative adversarial error. We show that adversarial accuracy is strongly linked to clean accuracy but weakly correlated with model size. Our findings lead to more robust architectures against white-box and black-box attacks on CIFAR10. In Fig. 1 (a), test set adversarial error on ImageNet BID15 models follows a power law form with exponent between 0.9 and 1.1 for < 0.2. Even adversarially trained models BID6 show similar trends. Investigating different forms of adversarial error, a step l.l. attack yields a power law with a larger exponent. The adversarial error follows a power law with exponents ranging from 1.8 to 2.5 for models trained on ImageNet, MNIST, and CIFAR10. Attack protocols can change the exponent of the observed power-law. Different neural network architectures were tested on MNIST to measure their adversarial error due to FGSM on the test set. The adversarial error of various neural network architectures, including fully connected networks and convolutional networks, follows a power law with exponents between 0.9 and 1.2. A 32-layer ResNet trained on CIFAR10 exhibits a power law with an exponent of 0.99. Additionally, a fully connected network trained on MNIST with shuffled labels shows perfect accuracy on the training set. The adversarial error on MNIST with shuffled labels follows a power-law form with an exponent of 1.2. Different attack methods display universality with exponents of 1.1, 1.2, and 1.3 for L2-norm, FGSM, and PGD attacks, respectively. An explanation for this universal behavior is offered. The study explores adversarial error in neural networks, showing that it is not dependent on network specifics. It considers linear response to perturbations and examines the impact of margins on robustness. An L2-variant of the FGSM attack is studied, revealing insensitivity to perturbation choice. The minimum perturbation required to change the input classification is computed. The class assigned to an input, x, changes in neural networks. Adversarial error rate is related to clean accuracy. The output of the network is denoted as \u0177 i (x) and the logits as h i (x). Adversarial perturbations must cause an erroneous prediction. Linearized response of the network to perturbations is considered. In the linearized response of the network, the input-logit Jacobian and error of the outputs are considered. A linear model predicts logit-differences and per-class thresholds for misclassification. A linear approximation is computed to describe changes in logits for small input changes. This linear model is confirmed by the logits for a single example in FIG1 (a). The linear model accurately predicts logit dynamics for small input changes, as shown in FIG1 (a). The linear response predictions are valid for small adversarial perturbations, but evaluating the complex \u0393 i (x) remains challenging. A \"mean-field\" approximation is introduced to simplify the process. The \"mean-field\" approximation simplifies the evaluation of complex \u0393 i (x) in Eq. (6). It is observed that \u2206 12 (x adv ) goes to zero before other \u2206 1j, indicating a dominant failure mode for neural networks. A mean-field estimate for\u02c6 is derived, showing strong correlation with the approximation in FIG1 (c). This suggests that the adversarial error rate for perturbations of size will be dictated by the network-specific rescaling of \u0393. The adversarial error rate at small is influenced by the distribution of logit differences, particularly for attacks targeting the second most likely class. A mean field approximation reveals that the difference between the most likely and second most likely logits scales as O(1) as it approaches zero, indicating a finite probability of them being close together. The distribution of logit differences in neural networks shows inherent uncertainty, with power-law tails observed in adversarial error rates for different architectures and datasets. The mean field approximation suggests a finite probability of the most likely and second most likely logits being close together. The distribution of logit differences in neural networks exhibits power-law tails with positive exponents, which increase with the layer index. Empirical results from ImageNet and MNIST datasets confirm these findings across various network architectures, indicating that adversarial error rates follow a power-law form regardless of model complexity or dataset dimensionality. The distribution of logit differences in neural networks shows power-law tails with positive exponents, increasing with the layer index. This phenomenon is observed in various network architectures on ImageNet and MNIST datasets, leading to adversarial error rates following a power-law form. The challenges in adversarial robustness are not solely due to model complexity or dataset dimensionality, but also stem from the abundance of examples with small \u2206 12 values, making it easier to fool the model at test-time. The distribution of \u2206 1j for trained models resembles that of i.i.d. random logits, indicating a lack of strong correlations between logits during training. Defensive distillation, inspired by distillation methods, aims to improve adversarial robustness by incorporating information about ratios of incorrect classes. Further research on the distributions of logit differences during training of distillation networks could provide valuable insights. Entropy regularization with a \u03bb = 4.5 is proposed to increase adversarial robustness in neural networks by penalizing the entropy of softmax outputs. This regularization term aims to increase the confidence of the network on each sample, leading to increased logit differences. The approach is shown to improve robustness for both regularly trained and adversarially trained networks on various datasets. Trained networks for permutation invariant and regular MNIST show improved robustness with entropy regularization, despite a slight decrease in clean accuracy. Adversarial training methods, including entropy regularization, are implemented to enhance adversarial accuracy. The PGD attack adversarial accuracy on permutation invariant MNIST is compared between models trained with and without adversarial training. In investigating adversarial training effects, the distribution of \u2206 1j for two networks trained on permutation invariant MNIST is plotted in FIG3. Entropy regularization shifts margins to larger values and reduces samples with small \u2206 1j. Larger networks are observed to be more robust against adversarial examples, regardless of adversarial training. In investigating adversarial training effects, larger networks are observed to be more robust against adversarial examples. A study is conducted to determine if network architectures play a role in adversarial robustness and to find a more robust model on CIFAR10 through neural architecture search with reinforcement learning. The search space is restricted to simplify the process, and the number of prediction steps is increased to regain complexity. In experiments, child models are trained with clean and adversarial examples to increase complexity. Models are trained for 10 epochs on a set of 25 thousand samples, with half being adversarially perturbed. The best model is selected based on FGSM adversarial accuracy and filters are scaled up for further training. After training child models with clean and adversarial examples, the best model is selected based on adversarial accuracy. The models are then enlarged and trained for 100 epochs on the full training set. The performance of these models is evaluated on a test set, with a comparison to a vanilla NAS baseline model. Experiment 1 results show the adversarial accuracy of NAS Baseline and another architecture found in the experiment, both trained with the same adversarial procedure. The architectures are trained with adversarial training, resulting in a slight decrease in clean accuracy. Models with the best adversarial accuracy at = 8 are reported. Adversarially trained models have a clean accuracy of 95.1% on the test set, while non-adversarially trained models reached 95.3% accuracy. Using PGD adversarial examples in training leads to more robust architectures. Experiment 2 results show improved performance in robustness. Black-box attacks are conducted on the network. In Experiment 2, the architecture reaches a 17% higher adversarial accuracy on PGD examples compared to BID8. Despite using clean examples in half of the minibatches, the model matches BID8's accuracy on white-box PGD attacks. The model is more robust against white-and black-box attacks, with a clean accuracy 5.9% higher. The NAS Baseline model has 4.9 million trainable parameters, while the models from Experiments 1 and 2 have 2.3 million and 3.5 million parameters, respectively. The best architecture from Experiment 2 and NAS Baseline are presented in Appendix FIG3. Performance statistics of child models during NAS are also studied. In Experiment 1, 9360 child models were trained for 10 epochs. The correlation between adversarial accuracy and the number of trainable parameters is weak, but strongly correlated with clean accuracy. Larger networks like Inception v3 and ResNet benefit from more parameters, unlike most child models during NAS. High clean accuracy does not guarantee adversarial robustness, as shown by the variance in adversarial accuracy among models with good clean accuracy. The experiments showed that larger networks like Inception v3 and ResNet benefit from more parameters, unlike most child models during NAS. High clean accuracy does not guarantee adversarial robustness, as shown by the variance in adversarial accuracy among models with good clean accuracy. The range of adversarial accuracies in the histogram of models with over 85% clean accuracy is 22% with a standard deviation of 2.6%. The study focused on common properties of adversarial examples across different models and datasets, highlighting the importance of architecture in adversarial robustness. The text discusses the computation of adversarial perturbations on the logits of a neural network using Jacobian and error terms. It also explores the distribution of perturbations and the impact on network performance. The study emphasizes the importance of network architecture in achieving adversarial robustness. The study computes the joint distribution between ranked logits in a neural network, emphasizing the combinatorial factor in the calculation. The distribution over perturbations is also discussed, highlighting the impact on network performance. The study analyzes the distribution over perturbations in a neural network, focusing on the impact on network performance. The equation for the distribution is computed and analyzed for small perturbations, revealing differences in architecture compared to previous models found by NAS."
}