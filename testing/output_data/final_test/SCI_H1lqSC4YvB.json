{
    "title": "H1lqSC4YvB",
    "content": "The reparameterization trick in variational inference is a valuable tool but is limited by standardization transformation. This paper introduces a generalized transformation-based gradient model that combines control variate advantages. A new polynomial-based gradient estimator is proposed with better theoretical performance than the reparameterization trick under certain conditions. In variational inference, a new polynomial-based gradient estimator is proposed with better theoretical performance than the reparameterization trick under certain conditions. It has lower gradient variance, enabling faster inference in machine learning tasks. The paper focuses on variational inference tasks in machine learning, aiming to approximate the posterior distribution in probabilistic models. It introduces a parameteric family of distributions to optimize the Kullback-Leibler divergence. The method aims to improve the optimizer for a wider range of distribution families. The paper introduces a generalized transformation-based gradient model (G-TRANS) for optimizing a wider class of parameteric distribution families in variational inference tasks. It replaces parameter-independent transformations with a more flexible approach, providing a powerful and elegant way to construct gradient estimators. The paper introduces a generalized transformation-based gradient model (G-TRANS) for optimizing parameteric distribution families in variational inference tasks. It connects to the score function method and reparameterization trick, and proposes a novel polynomial-based gradient estimator. The paper is organized into sections reviewing stochastic gradient variational inference, proposing the generalized transformation-based gradient, introducing the polynomial-based G-TRANS gradient estimator, evaluating its performance on synthetic and real data, and reviewing related works. The paper discusses stochastic optimization in variational inference tasks, focusing on maximizing the evidence lower bound (ELBO) to find the best variational parameter \u03b8. It mentions the intractability of computing the expectation and introduces gradient estimators like the score function method and reparameterization trick. These methods are commonly used in constructing Monte Carlo estimators for the exact gradient of the ELBO. The score function method and reparameterization trick are popular in variational inference for obtaining unbiased stochastic gradients of the ELBO. The score function estimator is a general way to compute gradients, but suffers from high variance, requiring variance reduction methods like Rao-Blackwellization and control variates. Reparameterization trick involves assuming an invertible and continuously differentiable transformation for gradient estimation. The reparameterization trick involves transforming the variational distribution into a distribution that does not depend on the variational parameter, resulting in lower variance gradient estimators. This method is not as widely applicable as the score function method but can be used for commonly used distributions like the Gaussian distribution. However, finding appropriate standardization functions for distributions like Gamma, Beta, or Dirichlet can be challenging due to the involvement of special functions. Theorem 3.1 states that the probability density function of \u03c1, defined by \u03c6(z, \u03b8), is denoted as w(\u03c1, \u03b8). The G-TRANS gradient estimator is derived from the gradient \u2202L \u2202\u03b8 with v \u03b8 satisfying an unbiasedness constraint. The score function and reparameterization gradients are special cases of the G-TRANS model when the standardization function \u03c6 does not depend on the parameter \u03b8. The G-TRANS model's standardization function \u03c6 does not depend on the parameter \u03b8, resulting in a gradient estimator with the same variance as the score function estimator. The transport equation is introduced by Jankowiak & Obermeyer (2018) and is determined by the unique 1-dimensional solution for standardization distributions that do not depend on \u03b8. The G-TRANS model connects to the control variate method through a complex differential structure, allowing for a combination of advantages from CV and generalized reparameterization. Theorem.3.1 transforms the unbiased reparameterization procedure by finding the appropriate velocity field that satisfies the unbiasedness constraint. Variational optimization theory can be applied to find the velocity field with the least estimate variance, but the solution to the Euler-Lagrange equation is impractical due to the presence of f(z) in the integrand. The introduction of the velocity field offers a more elegant and flexible way to construct a gradient estimator. The G-TRANS model introduces a polynomial-based gradient estimator that is superior to existing frameworks. It assumes a factorized base distribution and considers an ad-hoc velocity field family to ensure unbiasedness. The resulting gradient estimator is more general than the score estimator. The G-TRANS model introduces a polynomial-based gradient estimator that is more general than existing methods. It considers a special family of the velocity field and focuses on distributions with analytical high order moments. The gradient estimator is based on the polynomial velocity field and is constructed using a sample drawn from the distribution. The polynomial-based G-TRANS gradient estimator, derived from a sample drawn from q(z, \u03b8), outperforms the reparameterization gradient estimator under certain conditions by having a smaller variance. This is proven analytically by reorganizing the expression Var(\u2212 \u2202f \u2202zi ). By choosing a suitable polynomial, a better gradient estimator can be obtained according to Proposition 4.2. Adjusting the value of C i further enhances the performance of the estimator. In this section, a Dirichlet distribution is used to approximate the posterior distribution for a probabilistic model with a multinomial likelihood and Dirichlet prior. The difficulty of estimating coefficients for a polynomial approximation is highlighted, leading to a focus on cases where k is less than 2 in practical experiments. The text discusses using Gamma distributions to simulate Dirichlet distributions in a model with a multinomial likelihood and Dirichlet prior. It focuses on constructing a gradient estimator for the factorized distribution using shape parameters and approximating the derivative of the lower incomplete gamma function. The approximation error is small for specific ranges of parameters. The text discusses constructing a gradient estimator for the factorized distribution using shape parameters and approximating the derivative of the lower incomplete gamma function. Results show that the G-TRANS gradient estimator has lower variance compared to the RSVI method. Our G-TRANS gradient estimator outperforms the IRG method for large \u03b1 1 values, with no significant difference for small \u03b1 1 values. The performance is studied on the Sparse Gamma DEF model with the Olivetti faces dataset, following the Sparse Gamma DEF setting specified by Naesseth et al. (2017). The model used in the experiment consists of 3 layers with different numbers of components. Various priors are set for weights and local variables. A specific step-size sequence is utilized, and the best result is achieved with a certain parameter value. The experiment achieved the best result with B = 4 using the G-TRANS gradient estimator. G-TRANS outperformed RSVI, ADVI, BBVI, and G-REP in accuracy and ELBO improvement. It showed faster speed than IRG initially but slowed down later due to decreasing step size. Research is exploring extending the reparameterization trick to a wider range of distributions. Our gradient model provides a more elegant expression of the generalized reparameterized gradient compared to G-REP. It hides the transformation behind the velocity field, avoiding the costly computation of the Jacobian matrix. The RSVI also develops a similar gradient model using rejection sampling techniques. Our gradient estimator offers a deterministic procedure to reduce gradient variance without introducing additional stochasticity. It is closely related to the path-wise derivative model by Jankowiak & Obermeyer (2018), but our model allows for generalized transformation-based gradients. The implicit reparameterization gradient (IRG) differentiates from the path-wise derivative by using a different method for multivariate distributions. Other works have addressed limitations of standard reparameterization, but involve expensive computation not suitable for large-scale variational inference. Schulman et al. (2015) expressed the gradient similarly to G-REP but lacked necessary details for general variational inference tasks. Our proposed G-TRANS gradient model extends reparameterization to a larger class of variational distributions, providing a flexible way to construct gradient estimators with lower variance compared to existing methods. The G-TRANS estimator offers lower gradient variance, enabling faster convergence. Future work includes constructing G-TRANS estimators for distributions lacking analytical high-order moments and utilizing approximation theory to find effective high-order polynomial functions. Additionally, constructing velocity fields with optimal transport theory shows promise. The proof concludes with the derivation of the transport equation for the reparameterization trick, showing independence of the standardization distribution with \u03b8. The focus is on the 1-dimensional case with an unbiased constraint, leading to the consideration of the term (r \u03b8 (z, \u03b8)) 2. The complexity of the equation is highlighted, emphasizing the boundedness of h(z, \u03b8) for verification purposes. In real-world practice, verifying the boundedness of h(z, \u03b8) is crucial. Taking the dual polynomial velocity field v \u03b8 dp in the G-TRANS framework leads to a dual result to Proposition 4.2. If Cov(P k \u2202 log q(z,\u03b8) \u2202\u03b8 , (2f \u2212 P k ) \u2202 log q(z,\u03b8) \u2202\u03b8 ) > 0, then the gradient estimator from the dual polynomial velocity field has lower variance than the score function gradient estimator. The proof is similar to that of Proposition 4.2."
}