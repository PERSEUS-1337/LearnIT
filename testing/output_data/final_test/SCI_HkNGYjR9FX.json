{
    "title": "HkNGYjR9FX",
    "content": "Recurrent neural networks (RNNs) are effective in processing sequence data but are complex and memory intensive. To address these issues, a method is introduced to learn binary and ternary weights during training, reducing hardware requirements. This approach replaces multiply-accumulate operations with simple accumulations, benefiting custom hardware in silicon area and power consumption. Performance evaluation using LSTMs and GRUs shows competitive results in sequence classification and language modeling. Our method achieves competitive results on tasks using binary/ternary weights, with custom hardware for accelerating LSTM computations. LSTMs with binary/ternary weights offer up to 12x memory saving and 10x inference speedup compared to full-precision hardware. CNNs have surpassed human-level accuracy in various tasks by obtaining hierarchical representations. The computational complexity of CNNs is dominated by convolutional layers, while RNNs have shown success in modeling temporal data. Both networks are typically over-parameterized and suffer from high computational complexity. Deploying CNNs and RNNs on mobile devices with limited hardware resources is challenging. Techniques such as approximating the weight matrix with a lower rank matrix have been introduced to address these issues. In previous studies, it was found that many parameters in DNNs are noncontributory and can be pruned without affecting accuracy. Quantization methods have been introduced to reduce the bitwidth of weights/activations, leading to energy and power reductions. Quantization has been shown to be the most beneficial approach for hardware implementations. However, existing quantization techniques have focused on optimizing CNNs or fully-connected networks, neglecting the potential benefits for RNNs in processing sequential data. In previous studies, it was found that many parameters in DNNs are noncontributory and can be pruned without affecting accuracy. Quantization methods have been introduced to reduce the bitwidth of weights/activations, leading to energy and power reductions. However, RNNs have received less attention for hardware implementations compared to CNNs and fully-connected networks. BID18 introduced a binarized RNN with promising results on simple tasks, but it struggles with tasks requiring large inputs/outputs. BID45 and BID20 introduced multi-bit quantized RNNs that match accuracy performance with full-precision counterparts using up to 4 bits for data representations. This paper proposes a method to learn recurrent binary and ternary weights in RNNs during training, eliminating the need for excessive bitwidth. This paper introduces a method for learning recurrent binary and ternary weights in RNNs, reducing computation time and memory footprint during inference. Experimental results show that binary/ternary models can achieve near state-of-the-art performance on various sequential tasks. The paper introduces custom hardware to accelerate recurrent computations in RNNs with binary or ternary weights, achieving near state-of-the-art results with reduced computational complexity. Binarization process involves converting full-precision weight matrix elements to binary or ternary values, with specific methods like BinaryConnect and TernaryConnect. The TernaryConnect method assigns ternary values to weight elements to improve precision accuracy. Ternary weight networks (TWNs) learn scaling factors to minimize distance between full-precision and ternary weights. DoReFa-Net can learn different bitwidths for weights, activations, and gradients. TTQ method uses asymmetric scaling parameters for quantization. Loss-aware binarization (LAB) and its extensions, such as loss-aware quantization (LAQ) and the use of multiple binary weight matrices like in BID45, aim to minimize loss in binarized weights and improve prediction accuracy. These methods involve ternarizing weights with asymmetric scaling parameters and utilizing binary search trees for efficient binary code derivation. While reducing bitwidth, they also increase the number of parameters and operations. Among various methods, BID45 and BID17 targeted RNNs to reduce computational complexity and improve prediction accuracy. BID45 and BID17 showed promising results on specific temporal tasks, with BID17 matching the performance of full-precision models on word-level language modeling when using k = 4. However, there are no binary models that can match the performance of full-precision models on word-level language modeling tasks. The literature lacks binary/ternary models that can perform various temporal tasks while achieving similar prediction accuracy as full-precision models. The remarkable success of RNNs in processing variable-length sequences is hindered by the exploding gradient problem. Various RNN architectures like LSTM and GRU were introduced to address this issue. This paper focuses on LSTM to learn recurrent binary/ternary weights. The LSTM architecture involves hidden states, gates, and updates regulated through gates. The main computational core of LSTM is dominated by a matrix. The LSTM architecture's main computational core is dominated by matrix multiplications. To reduce computational complexity and memory accesses, recurrent weights can be binarized or ternarized. However, methods that ignore the loss of the binarization process struggle to binarize LSTM weights. BinaryConnect is an example where weights are binarized during forward computations, and both full-precision and binarized weights can be used for inference after training. The binarized LSTM fails to control the flow of information due to issues with gates and hidden states, leading to problems in performing sequential tasks. The binarized LSTM struggles with information flow control due to gate and hidden state issues, affecting sequential task performance. Batch normalization is proposed to address this by learning binarized/ternarized recurrent weights, enhancing network robustness to hyperparameter settings and input distribution changes. The statistics E(x) and V(x) represent mean and variance estimations of the unnormalized vector for the current minibatch. Batch normalization is commonly used in RNNs to improve convergence speed and performance. The goal is to represent each weight as a scaled factor \u03b1, normalized to compute the probability of binary or ternary values for each element of the weight matrix W. The text discusses the process of binarization and ternarization of weights in a neural network. It explains how the weights are normalized and then sampled from a distribution to obtain binarized/ternarized weights. Batch normalization is used to regulate the scale of these weights during vector-matrix multiplications. Additionally, it mentions that full-precision weights are used during parameter updates. In the process of updating parameters, full-precision weights are utilized due to their small values. The gradient estimation for full-precision weights is done using Eq.(1) as binarization/ternarization functions are not differentiable. Batch normalization, while helpful, slows down training due to additional computations. The performance of LSTMs with binary/ternary weights on various temporal tasks is evaluated, with details on hyperparameters and tasks deferred to Appendix C. For character-level modeling, the aim is to predict the next character, evaluated in bits per character (BPC) on datasets like Penn Treebank, War & Peace, and Linux Kernel. Our quantization experiments on Penn Treebank, War & Peace, and Linux Kernel datasets show that our binary/ternary models outperform existing methods in prediction accuracy. The LSTM models used have 1000, 512, and 512 units for the respective datasets. Our ternarized model achieves the same BPC values as the full-precision model on War & Peace and Penn Treebank datasets. Our ternarized model achieves comparable accuracy to the full-precision model on the Text8 dataset for character-level language modeling. Despite using binary or ternary weights, our models perform well compared to full-precision models. In word-level modeling, predicting the next word is the main goal, but dealing with large vocabulary sizes makes model quantization challenging. The task involves dealing with large vocabulary sizes, making model quantization challenging. BID45 introduced an alternating method for multi-bit quantization of LSTMs, but it only matched full-precision performance with 4 bits. There is a significant performance gap between 2-bit quantization and full precision. To demonstrate the effectiveness of a new method, a small LSTM size of 300 is used for comparison with medium and large models introduced by BID47. Performance results are summarized in TAB2. Our binarized/ternarized models outperform the alternating method using 2-bit quantization in terms of perplexity and memory size. The medium-size model with binary weights also shows improvement over the 4-bit quantization method. Models with recurrent binary and ternary weights perform comparably to full-precision counterparts in the MNIST classification task. Training is done using an LSTM with 100 nodes and a softmax classifier layer. Test performance of models with recurrent binary/ternary weights is reported in TAB3. The ternary model achieves the same accuracy as the alternating method with 2\u00d7 fewer operations. Attentive Reader, introduced by Hermann et al., uses an attention mechanism to answer questions about real news articles. Our quantization method is shown to be effective when training Attentive Reader with recurrent binary/ternary weights. Our method for learning recurrent binary/ternary weights on the CNN corpus replicates Attentive Reader and achieves similar accuracy with a 32\u00d7 smaller memory footprint. Batch normalization is proposed to address quantization loss in LSTM weights, improving accuracy in various temporal tasks compared to binaryconnect. Our method for learning binary/ternary weights outperforms existing quantization methods in literature, achieving comparable accuracy to full-precision models. The LSTM layer in our model uses deterministic values of -1, 0, or 1 for weights, unlike CNNs or fully-connected networks. The proposed LSTM model trained with binary/ternary weights can only perform inference computations with those weights. The distribution of weights is dominated by non-zero values in the model with ternary weights. The effect of probabilistic quantization on prediction accuracy was evaluated using a ternarized network trained for language modeling tasks. The prediction accuracy variance due to stochastic ternarization was found to be very small and negligible. Similar behavior was observed for other temporal tasks in the study. The proposed training algorithm for LSTM models with binary/ternary weights shows fast convergence and good generalization over long sequences. The model converges faster than full-precision LSTM initially, with a slower convergence rate later to prevent early overfitting. It also generalizes well over longer sequences and focuses on relevant information for predicting the next target character, improving accuracy as sequence length increases. Our method of binarizing/ternarizing LSTM models shows fast convergence and good generalization over long sequences. It can also be applied to other recurrent architectures like GRUs. We tested our method on character-level language modeling tasks using GRUs on different corpora, showing successful binarization/ternarization of recurrent weights. Additionally, we investigated the impact of batch sizes on prediction accuracy. The LSTM model trained on character-level language modeling tasks showed that batch normalization cannot be used for a batch size of 1, leading to lower prediction accuracy compared to models without batch normalization. Binarized/ternarized models improved prediction accuracy with increasing batch sizes, while the baseline model's accuracy decreased. These models can be utilized in various dataflows like DaDianNao and TPU. The effectiveness of LSTMs with recurrent binary/ternary weights is evaluated by building a binary/ternary architecture over DaDianNao, a highly efficient dataflow for DNNs. DaDianNao achieves significant speedup and energy reduction compared to GPUs. Hardware techniques can further enhance computation speed, such as skipping zero-valued weight computations. DaDianNao utilizes DRAM to store weights/activations and provide memory bandwidth for each MAC unit. Two ASIC architectures implementing Eq. (2) are considered for evaluation. For the low-power implementation of two ASIC architectures implementing Eq. (2), 100 MAC units are used with a 12-bit fixed-point representation for weights and activations. This ensures no prediction accuracy loss in the full-precision models. In LSTMs with recurrent binary/ternary weights, a 12-bit fixed-point representation is used for activations, and multipliers in the MAC units are replaced with low-cost multiplexers to maintain prediction accuracy. The low-power inference engine for binary/ternary models in TSMC 65-nm CMOS technology results in significantly lower power consumption and silicon area compared to full-precision models. By using recurrent binary/ternary weights, up to 10\u00d7 more MAC units can be instantiated, leading to a 10\u00d7 speedup in inference computations at 400 MHz. In this paper, a method is introduced that learns recurrent binary/ternary weights to speed up computations by up to 10\u00d7 compared to full-precision models. The proposed training method generalizes well over long sequences and various temporal tasks. It also benefits custom hardware implementations by reducing memory bandwidth and replacing full-precision multipliers with hardware-friendly multiplexers. Two ASIC implementations are introduced for this purpose: low-power and efficient. The ASIC implementations introduced in the paper include low-power and high-throughput designs, with the former saving up to 9\u00d7 power consumption and the latter speeding up recurrent computations by a factor of 10. The probability density curves in Figure 4 compare the BinaryConnect LSTM with its full-precision counterpart on the Penn Treebank character-level modeling task, showing differences in how gates control the flow of information. The output gate allows all information through, gate g blocks all information, and the forget gate f struggles to decide. Binarization changes gate and hidden state probabilities during training. Learning recurrent binary/ternary weights involves forward and backward propagation. Forward propagation includes batch-normalizing vector-matrix multiplication results with binarized/ternarized weights. Backward propagation involves normalization of unit activations. During backward propagation, gradients are computed for each parameter of each layer and updates are made using a learning rule. Full-precision weights are used for parameter updates, with recurrent weights being binarized/ternarized during forward propagation. Batch normalizing the state unit can be optionally used for better control. The Penn Treebank corpus is split for training, validation, and test sets, with an LSTM model and softmax classifier used for the task. Cross entropy loss is minimized on minibatches of size 64. The curr_chunk discusses the training process of a binarized model on different corpora, including the Linux Kernel, Leo Tolstoy's War and Peace, and Text8 datasets. It mentions the use of an LSTM layer of size 512 followed by a softmax classifier, with specific details on vocabulary sizes and learning rates. The probability density of the model's states/gates is illustrated in FIG5, showing their control over information flow. The curr_chunk discusses training models on Text8 and Penn Treebank datasets using LSTM layers of different sizes and specific training parameters like learning rates and update rules. The curr_chunk discusses training models on MNIST dataset using LSTM with 100 nodes, Attentive Reader architecture, and binary/ternary architecture implemented in VHDL. The models are trained with specific parameters like learning rates, batch sizes, and update rules. The Genus Synthesis Solution utilizes TSMC 65nm GP CMOS technology to accelerate computations on binary and ternary weights, showing a 10\u00d7 and 5\u00d7 speedup compared to full-precision models. The latency of the proposed accelerator is depicted in Figure 7."
}