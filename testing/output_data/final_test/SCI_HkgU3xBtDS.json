{
    "title": "HkgU3xBtDS",
    "content": "Reinforcement learning methods using game tree search have been successful in games like chess, shogi, and Go. This research focuses on improving episode generation coordination to balance exploitation and exploration. Experiments show robust performance compared to Alpha Zero. Computer programs beating human players in these games is a significant achievement in computer science. This study proposes new methods for improving performance in two-player deterministic games using reinforcement learning and game tree search. The focus is on developing more robust algorithms that do not require human intervention for hyperparameter tuning. The goal is to achieve better final performance in game environments like Markov decision processes. Reinforcement learning improves agent's policy and value estimation in environments. Model-free reinforcement learning with deep neural networks has shown success in video game environments. This study focuses on two-player deterministic games, where model-based reinforcement learning can also be applied. Forward planning using a tree structure can be used for optimal results. The game tree represents the winning player and optimal action in two-player deterministic games. Minimax search can determine the winner without searching all states. Alpha-beta search reduces unnecessary search but may not be feasible for complex games. Various methods, including algorithmic improvements and function approximation, have been developed for realistic time execution and performance. Monte Carlo tree search is an effective method for forward planning in games. Monte Carlo tree search (MCTS) is widely used in game planning, with variants like the PUCT algorithm described in this report. Starting from an empty game tree, MCTS adds new nodes to expand the tree in each simulation. The core of MCTS lies in deciding action responses on the game tree based on feedback from the approximation function. The Monte Carlo tree search uses the bandit algorithm to decide action responses on the game tree, balancing exploration and exploitation. PUCT algorithm employs the PUCB formula for selection, asymptotically finding the optimal action. Additionally, the new estimated value at the root node is returned in the algorithm. The PUCT algorithm uses the PUCB formula for action selection in the game tree, balancing exploration and exploitation. Noise is added to the policy at the root node to encourage more exploratory action selection. This study follows the method of adding noise generated by the Dirichlet distribution. Agents benefit from being able to perform game tree search using the true environment model. The PUCT algorithm improves strategy and value estimation through game tree search. Reinforcement learning studies like Alpha Zero and EXpert ITeration have been successful in handling various games. Alpha Zero continuously updates approximation function using Monte Carlo tree search. Algorithm 3 describes the single thread version of Alpha Zero for training neural nets. In the previous method, the neural net is trained using Monte Carlo tree search. The study suggests that the neural network's policy may lead to weak states during the learning process. A method to improve explorability at the root node is proposed, accelerating learning speed in Go. Additionally, a new approach to applying Monte Carlo tree search to episode generation is introduced, resulting in a new probability distribution and state value estimation. The Monte Carlo tree search introduces a new probability distribution and state value estimation, functioning as a policy and value estimation method. In the proposed approach, the search is conducted on the master game tree, with values stored in each node corresponding to the ordinal Monte Carlo tree search. The goal is for the master game tree to converge to the optimal strategy over time. The proposed method named MbM (MCTS-by-MCTS) uses the master game tree to refine the performance of Monte Carlo tree search. Function approximation and PUCT are applied to the master game tree to increase the accuracy of policy and value estimation. Training is done to improve the accuracy of neural nets in the master game tree search. The proposed MbM method uses the master game tree to enhance Monte Carlo tree search performance by updating values through training neural nets. The method calculates a weighted average between old and new values to absorb result fluctuations and emphasizes newer estimations as neural networks progress. The master game tree is updated using a simple average, summing the state value estimates of new nodes and propagating differences through the tree. The proposed method uses the master game tree to enhance Monte Carlo tree search performance by updating values through training neural nets. The NextPathByMasterTree() function generates the opening action sequence by descending the tree. In an experiment with tic-tac-toe, the proposed method and AlphaZero were compared after 20,000 battles. Key points compared were the ratio of defeating random players and perfect players. Tic-tac-toe is a simple task where a perfect strategy can be obtained through minimax search. The experiments compared the proposed method with AlphaZero in tic-tac-toe after 20,000 battles. Different strategies were tested, including adding Dirichlet noise to the policy. Varying the initial values of episode generation temperature parameters showed significant performance differences. The initial values of episode generation temperature parameters were varied in the experiments. Other experimental settings and hyperparameters included attenuation rate, C in PUCB, noise to the policy, number of epochs, number of episodes per epoch, input features, details of neural nets, training data, and optimization. The experimental results are plotted for analysis. The experimental results comparing AlphaZero and the proposed method MbM show that MbM outperforms AlphaZero in reaching almost optimal strategies under all conditions. The performance of MbM is equal to or better than AlphaZero, suggesting that the difference in performance may be due to the explorability of episode generation by adding noise to the policy in the master game. The study compared the performance of MbM and AlphaZero in reaching optimal strategies. MbM outperformed AlphaZero, with the addition of noise to the policy enhancing explorability. Different versions of MbM were tested, with MbM-NoNoise performing worse than AlphaZero. MbM-UniformRelaxation showed improvement over AlphaZero but was not as good as MbM. The study focused on Tic-tac-toe as a simple task to demonstrate these findings. The study compared MbM and AlphaZero in reaching optimal strategies for Tic-tac-toe. Results showed that adding noise to the policy in MbM enhanced explorability. However, MbM-NoNoise performed worse than AlphaZero due to early policy convergence. Further examination is needed to improve the strategy applied to the master game tree. In this study, the proposed method aims to enhance the exploratory nature of the game tree. The experimental results suggest that the method can yield robust outcomes for larger games, even with varying temperature parameters. Future research should focus on real-time improvement speed compared to previous methods, aiming for better stability and performance in a wider range of domains."
}