{
    "title": "H1bhRHeA-",
    "content": "Recent neural network and language models are using softmax distributions with a large number of categories, making the calculation of the softmax normalizing constant expensive. This has led to the development of biased estimates of the softmax. This paper introduces two unbiased algorithms for maximizing the softmax likelihood, which outperform competitors on real-world datasets. The softmax model calculates the probability of a random variable taking on a label based on covariates and parameters. The paper focuses on maximizing the ridge-regularized maximum log-likelihood problem with large N, K, D in modern applications like natural language processing and recommendation systems. Stochastic Gradient Descent is commonly used, but the cost of calculating the normalizing sum can be expensive for large K, D. Various approximations have been proposed to address this issue. Various approximations have been proposed to address the expensive cost of calculating the normalizing sum for large N, K, D in maximizing the ridge-regularized maximum log-likelihood problem. These include tree-structured methods, sampling methods, self-normalization, and alternative models like the spherical family of losses. However, these approximations are computationally tractable but biased and do not converge to the optimal solution. BID16 managed to recast the problem as a double-sum over N and K, making it amenable to Stochastic Gradient Descent with reduced per iteration cost. The paper addresses the instability of vanilla SGD in optimizing double-sum formulations of softmax likelihood. Two algorithms are developed: U-max, ensuring bounded gradients and convergence with small learning rates, and Implicit SGD, known for stability and similar convergence properties. Implicit SGD updates for the double-sum formulation can be efficiently computed. The paper introduces the U-max algorithm to stabilize SGD updates and the Implicit SGD algorithm for efficient computation with bounded step size. Both algorithms outperform existing methods, with Implicit SGD achieving significantly lower log-loss. The paper introduces U-max and Implicit SGD algorithms, showing that both outperform previous methods. Implicit SGD has the best performance. The key is to represent the SGD method as a double-sum over datapoints and classes, optimizing a jointly convex function in u and W. The paper introduces U-max and Implicit SGD algorithms, showing that both outperform previous methods. Implicit SGD has the best performance. The key is to represent the SGD method as a double-sum over datapoints and classes, optimizing a jointly convex function in u and W. The challenge in optimizing f using SGD is the problematically large magnitude gradients, leading to computational overflow and algorithm issues. The paper introduces U-max and Implicit SGD algorithms to optimize the double-sum formulation f(u, W). These methods address issues with large gradient magnitudes in vanilla SGD, leading to computational problems and slow convergence. The goal is to design reliable and efficient algorithms for optimization. The paper proposes U-max and Implicit SGD algorithms to optimize the double-sum formulation f(u, W), which aims to address issues with large gradient magnitudes in vanilla SGD for faster convergence. The paper introduces U-max and Implicit SGD algorithms to optimize the double-sum formulation f(u, W) for faster convergence by addressing issues with large gradient magnitudes in vanilla SGD. The U-max algorithm adjusts u i to decrease the objective function f(u, W) and converge to the optimum, with a rate at least as fast as vanilla SGD. U-max algorithm resolves large gradient issues by controlling gradient magnitude with parameter \u03b4. The learning rate \u03b7 t must be \u2264 \u03b4 2 /(4B 2 f ) for convergence. Implicit SGD is another method for handling large gradients. Implicit SGD, also known as an \"incremental proximal algorithm,\" differs from vanilla SGD by updating \u03b8 (t+1) on both sides of the equation. It is more robust to learning rate variations and has smaller step sizes, making it preferable in some cases. Implicit SGD, also known as an \"incremental proximal algorithm,\" updates \u03b8 (t+1) on both sides of the equation, with smaller step sizes and more robust to learning rate variations compared to vanilla SGD. The step size in Implicit SGD is asymptotically linear, preventing computational overflow. In each iteration, one datapoint i and one class k = y i are sampled, with no ridge regularization, resulting in a step size magnitude of O( DISPLAYFORM4. The challenge lies in computing a solution to (9), which is problem-dependent. Implicit SGD, also known as an \"incremental proximal algorithm,\" updates \u03b8 (t+1) on both sides of the equation with smaller step sizes, making it more robust to learning rate variations compared to vanilla SGD. The computation of a solution to (9) is problem-dependent, and the Implicit SGD update can be computed within accuracy in runtime O(n(n + m)(D + n log( \u22121 )). The log( \u22121 ) factor in the update equation comes from applying a first-order method, which may result in slower iterations compared to vanilla SGD or U-max. In the special case of n = m = 1, the bisection method can provide an explicit upper bound on the optimization cost. Implicit SGD, also known as an \"incremental proximal algorithm,\" updates \u03b8 (t+1) on both sides of the equation with smaller step sizes, making it more robust to learning rate variations compared to vanilla SGD. The computation of a solution to (9) is problem-dependent, and the Implicit SGD update can be computed within accuracy with only two D-dimensional vector inner products and at most log 2. This method is robust to learning rate variations, does not have overflow issues, and its updates can be computed in roughly the same time as vanilla SGD. The study compared U-max and Implicit SGD on real-world datasets, investigating the performance difference between two double-sum formulations. Various datasets were used, with features normalized to unit L2 norm and dimension truncated for manageability. The datasets were pre-separated into training and test sets. The focus was on optimizing the softmax likelihood over the training set. Algorithms were compared to state-of-the-art methods with runtime O(D) per iteration. Competitors included NCE, IS, and OVE. Standard settings were used for these biased algorithms. Implicit SGD and U-max were implemented with specific parameters. The U-max algorithm was tested with specific parameters, including n = 1, m = 1, and \u03b4 = 1. Ridge regularization parameter \u00b5 was set to zero. Each algorithm ran for 50 epochs on datasets with a learning rate decrease of 0.9 per epoch. Prediction error and log-loss were recorded at 10 evenly spaced epochs. Gradient magnitude varied among algorithms, affecting the learning rate. Datasets were obtained from specific sources, with BID16 having a slower runtime compared to other methods. Additional experiments were conducted with n = 1, m = 1, showing minimal impact. The U-max algorithm was tested with specific parameters, including n = 1, m = 1, and \u03b4 = 1. Ridge regularization parameter \u00b5 was set to zero. Each algorithm ran for 50 epochs on datasets with a learning rate decrease of 0.9 per epoch. Prediction error and log-loss were recorded at 10 evenly spaced epochs. Gradient magnitude varied among algorithms, affecting the learning rate. Datasets were obtained from specific sources, with BID16 having a slower runtime compared to other methods. Additional experiments were conducted with n = 1, m = 1, showing minimal impact. When setting n = 1, m = 1 in the methods, there was virtually no difference except for slower runtime. Performance of NCE with n = 1, m = 1 and n = 100, m = 5 on the Eurlex dataset showed very little difference. Tuned initial learning rates for each algorithm on each dataset were displayed in Table 2. Vanilla SGD had stability issues with a learning rate of 10 \u22123 on certain datasets. The experiments were run on 10% of the training data with specific initial learning rates. The U-max algorithm, tested with specific parameters, ran for 50 epochs with a learning rate decrease of 0.9 per epoch. Comparison to state-of-the-art shows Implicit SGD outperforms on most datasets, converging faster and achieving lower log-loss after 50 epochs. U-max also performs well compared to previous methods. The U-max algorithm, tested with specific parameters, ran for 50 epochs with a learning rate decrease of 0.9 per epoch. Comparison to state-of-the-art shows Implicit SGD outperforms on most datasets, converging faster and achieving lower log-loss after 50 epochs. U-max also performs well compared to previous methods. Additionally, Vanilla SGD's performance is better than the previous state-of-the-art but worse than U-max and Implicit SGD. The difference in performance between vanilla SGD and U-max can largely be explained by vanilla SGD requiring a smaller learning rate to avoid computational overflow. The sensitivity of each method to the initial learning rate can be seen in Appendix G, where the results of running each method on the Eurlex dataset with different learning rates is presented. The proposed double-sum in U-max outperforms the double-sum of Raman et al. on the Eurlex dataset, with smaller magnitude gradients and faster convergence. U-max and Implicit SGD algorithms optimize softmax likelihood efficiently, requiring only O(D) computation per iteration and converging to the optimal softmax MLE. The new method efficiently optimizes softmax for large datasets and classes, outperforming previous state-of-the-art. Implicit SGD can be applied to various neural networks, including word2vec models. The optimal values of parameters are bounded, and the objective is strongly convex. The optimal values of parameters are bounded, and the objective is strongly convex, leading to efficient optimization of softmax for large datasets and classes. The text discusses the equation for L(W) in stochastic composition optimization, highlighting the role of variable v_i and updates for w_yi. The text discusses the convergence of U-max to the softmax optimum in stochastic composition optimization, proving the claim made in Theorem 1. Lemma 5 states that if u_i is less than a certain value, then setting u_i to a specific value is equivalent to a Taylor series expansion. The text proves Theorem 1 by showing that the rate of convergence of U-max is at least as fast as standard SGD. The proof involves bounding the value of f at each iterate and using the projection of \u03b8 onto a convex set \u0398. The text also discusses the strong convexity of f in Theorem 2. The Implicit SGD update is derived for different scenarios, starting with the case of a single datapoint and class without a regularizer. The update involves optimizing variables to minimize the stochastic gradient. The Implicit SGD update involves optimizing variables to minimize the stochastic gradient, with the optimization problem reducing to finding the optimal values of variables in the direction of x i. The solution for a can be found by setting its derivative equal to zero. The solution for variable a can be obtained by setting its derivative to zero. It can be expressed in terms of the Lambert W function. To find the optimal values of variables, we need to minimize over u i using the bisection method. The bounds of the bisection interval are calculated to ensure fast convergence. The gradient of u i is monotonically increasing in a. If the derivative in (16) is negative at DISPLAYFORM6, the interval size must be less than log(2). The gap is upper bounded by log(2(K - 1)) + |x i (w k - w yi) - \u0169 i|. If the derivative in (16) is positive at u i = \u0169 i, u i is upper bounded by \u0169 i. The solution for a is strictly monotonically increasing as a function of the right side of the equation. The solution for a is strictly monotonically increasing as a function of the right side of the equation, allowing for the calculation of lower and upper bounds on the optimal value of u i. This enables the use of the bisection method for efficient computation. Implicit SGD with a single datapoint and sampled class has a bounded step size magnitude with respect to w. The update process differs when there are multiple datapoints, classes, and a regularizer involved, requiring pre-computation for a low-dimensional representation of x. The process involves pre-computation for a low-dimensional representation of x values in mini-batches, integrating out u i for each datapoint, and using first order or quasi-Newton methods for optimization. Mini-batches are defined by partitioning datapoint indices and selecting sets of classes that are not equal to any labels in the batch. The objective is written in terms of an expectation of functions corresponding to mini-batches. The process involves pre-computation for a low-dimensional representation of x values in mini-batches, integrating out u i for each datapoint, and using first order or quasi-Newton methods for optimization. Mini-batches are defined by partitioning datapoint indices and selecting sets of classes that are not equal to any labels in the batch. The objective is written in terms of an expectation of functions corresponding to mini-batches. In terms of expectation functions for mini-batches, j is sampled with probability p j = |S j |/N and C is sampled uniformly from C j. The regularizing constant \u03b2 k is determined by E[I[k \u2208 C \u222a S j ]\u03b2 k ] = 1. The optimization problem simplifies to solving for u Sj terms, decomposing into separate optimization problems for each u i in S j. The solution involves setting the derivative of u i to zero, yielding a solution with the Lambert W function. The optimization problem involves decomposing into perpendicular and parallel components, with the perpendicular component yielding an optimal value w through calculus. The parallel component is simplified using an orthonormal basis, leading to an optimization problem involving specific matrices and factors. The optimization problem involves calculating derivatives and inner products to find the optimal solution for the parallel component of w. Using an orthonormal basis simplifies the calculation process, allowing for the use of standard first-order methods to solve the problem efficiently. The optimization problem involves reconstructing the optimal solution for the parallel component of w using sparse updates to save computation time. Calculating the terms involves inner products of D-dimensional vectors and finding the orthonormal basis Vj using the Gram-Schmidt process. The optimization problem involves reconstructing the optimal solution for the parallel component of w using sparse updates to save computation time. The Gram-Schmidt process is used to compute the orthonormal basis {Vj : j = 1, ..., J} as a pre-processing step for defining fixed mini-batches. The cost of calculating inner products and optimizing the Implicit SGD update formula is O(n(n + m)(D + n log( \u22121 )). It is crucial to initialize the optimization procedure at a point where the gradient is small to avoid numerical issues. The optimization problem involves reconstructing the optimal solution for the parallel component of w using sparse updates. Results of using different learning rates for each algorithm on the Eurlex dataset are presented, including Implicit SGD, NCE, IS, OVE, and U-max algorithms. NCE and NCE (1,1) have similar performance."
}