{
    "title": "Sk6fD5yCb",
    "content": "Binary Deep Neural Networks (BDNNs) optimize computational performance and memory usage in prediction phases. Convolutional Neural Networks (CNNs) can be implemented using binary representations with Espresso, a compact library for forward propagation of CNNs in less than 400KB. Espresso offers GPU parallelism and CPU implementation, with special layers for BCNNs using bit-packing for efficient execution. Espresso is a compact library for optimized binary neural networks, providing speed-up in matrix-multiplication routines and reducing memory usage. It outperforms existing implementations by a significant margin and is released under the Apache 2.0 license. Convolutional Neural Networks have advanced computer vision, surpassing human capabilities. Deep Neural Networks have been successful in speech recognition and automated translation. However, DNNs require excessive memory and power, limiting their use on mobile devices. Memory constraints on mobile platforms hinder the deployment of trained DNNs, and their computational intensity drains battery life. Reducing computational load improves energy efficiency and enables more applications, like real-time object classification on mobile. In order to reduce the computational and memory requirements of Deep Neural Networks (DNNs), specialized hardware or quantized networks can be used. Quantized networks store parameters as small integers instead of floating point numbers, increasing efficiency without the need for specialized hardware. This approach focuses on binary deep neural networks (BDNN) to achieve faster predictions and free up computational resources for other tasks like speech recognition. Binary deep neural networks (BDNN) use 1-bit integers to reduce memory usage and speed up execution time. While only proof-of-concept implementations have been published, an optimized framework called Espresso has been developed for BDNNs to achieve state-of-the-art performance with minimal memory footprint. This framework enables further research on BDNNs and their practical applications. Espresso is an optimized framework for Binary Deep Neural Networks (BDNNs) that focuses on forward-propagation for improved performance with minimal memory usage. It supports both dense and convolutional layers, simplifying deployment on mobile or embedded devices. This work is a stepping stone towards optimizing training routines for BDNNs. Improving DNN performance can be done at hardware or software levels. Hardware-wise, dedicated chipsets can outperform CPUs/GPUs. Software-wise, simplifying architectures, pruning weights/filters, weight sharing, and quantizing network weights can lead to more efficient computations. Quantized networks aim to train DNNs with weights that do not significantly impact classification accuracy. Quantized networks aim to train DNNs with weights that do not significantly impact classification accuracy. BID5 showed that binary networks can achieve near state-of-the-art results on standard datasets like MNIST and CIFAR-10. Binarized CNNs have also shown promising results on massive datasets like ImageNet using binarized versions of popular DNN architectures. BinaryNet is a technique introduced to train DNNs with weights and activations constrained to {\u22121, +1}, achieving nearly state-of-the-art accuracy for MLP training on MNIST and CNN training on CIFAR-10. The authors also propose a binary optimized implementation of matrix multiplication resulting in 7\u00d7 faster performance than the base-line non-optimized implementation. Their core contributions involve replacing Floating-point Multiply and Add operations with XNORs and bit-counts. Espresso provides tools for executing forward-propagation of DNNs, with a focus on convolutional neural networks in computer vision. The emphasis is on convolutional neural networks for computer vision applications, which are less memory efficient and more computationally intensive than traditional machine learning algorithms. Identifying the memory and computational bottlenecks of DNNs is crucial for practical application. The focus is on GPU-optimized BDNN architectures, with support for CPU and GPU implementations. Espresso's implementations of tensors and layers come in three variants: CPU, GPU, and GPU opt. Espresso enables the design of hybrid DNNs with CPU, GPU, and GPU opt layers. The computational bottleneck lies in dot products and dense linear algebra operations, crucial for deep learning networks. The efficient computation of dot-product involves Floating-point Multiply and Add (FMA) operations. Espresso improves computational performance by using bit-packing, optimized memory management, and custom CUDA kernels. It eliminates the need for costly memory re-arrangements and implements a custom memory allocator to enhance GPU performance. Matrix multiplications are efficiently performed using CUDA. In BDNNs, FMA operations are replaced by XNOR and bit-count for faster computations. XNOR offers higher throughput and can execute multiple dot-products simultaneously. The network is binarized, memory layout is compressed, and input data is re-interpreted for efficient execution. The BDNN utilizes efficient execution of dot-products by re-interpreting input data for fixed-precision processing. The weights and activations are encoded as {0, 1} for hardware implementation. Bit-packing reduces memory usage by 32\u00d7 and enables processing multiple values simultaneously. This is particularly beneficial for dot-products, allowing computation of 64 element vectors with just one XNOR operation. The BDNN efficiently computes dot-products using bit-packing, reducing memory usage and enabling processing of multiple values simultaneously. Binary input data is split into bit-planes for optimized computation, allowing for efficient matrix operations in BDNNs. During training a Binary Deep Neural Network (BDNN), the gradient is computed with binary weights but accumulated with floating point precision for reliable updates. The straight-through estimator is used to handle issues with the derivative of the sign function. Weights are clipped to [-1, 1] during training to prevent excessive growth of floating point weights. The framework includes tensors, layers, and the network as principal components. The framework includes tensors, layers, and the network as principal components. Tensors are n-dimensional matrices used for storing inputs, weights, and activations. Layers process input tensors to produce output tensors, while a network consists of a concatenation of layers. Tensors are stored in memory using row-major order with interleaved channels. The notation A m,n,: indicates all channels of the (m, n)-th element. Espresso also defines bit-packed tensors for GPU optimizations. Bit-packing in Espresso optimizes tensor layout for GPU performance, with different packing directions based on the number of channels. This layout allows for efficient memory access in convolutional layers, especially for retrieving pixel neighborhoods. The packing direction is chosen based on the layer type, with n being the most efficient direction for other layers. Espresso provides various layer types such as Input, Convolutional, Pooling, Dense, and Batch-normalization. Each layer has specific characteristics like size, tensor parameters, and output. The API defines forward functions for computing layer outputs and applying non-linearity. Convolutional layers in Espresso use 2D convolutions through matrix multiplications for high data reuse. Computation is optimized for CPU and GPU performance by sectioning data in cache-friendly amounts. Espresso optimizes computational performance by using matrix multiplication to express convolutions. The unrolling procedure transforms tensors into matrices for efficient processing. CUDA kernels are provided for GPU and CPU implementations. Matrix-vector multiplications are essential for dense and CNN layers, utilizing the OpenBLAS library for CPU architecture. Espresso optimizes computational performance by using matrix multiplication for convolutions. The CUDA kernels for GPU and CPU architectures are implemented using OpenBLAS library for dense and CNN layers. Register blocking optimization is utilized for matrix multiplication, with modifications for binary data representation and FMA operations. The text discusses optimizing computational performance in CNN layers by using XNOR and bit-count with packed tensors. It addresses the issue of zero-padding in convolutional GPU layers, treating data as binary to fix corner-cases in post-processing without affecting the convolution kernel code. The correction matrix in Espresso A DNN is computed during GPU opt layer loading, involving the convolution of layer weights with a padded zero-tensor. Converting a network to Espresso involves loading layers at runtime from a parameters file that specifies storage format and weights. Training with BinaryNet and converting parameters to Espresso format is done using a utility script. Framework performance is evaluated based on average computational time for tasks, tested on an NVIDIA GeForce GTX 960 with 2GB RAM and Intel dual-Xeon X5660 @ 2.80. Espresso is evaluated on an NVIDIA GeForce GTX 960 with 2GB RAM and Intel dual-Xeon X5660 @ 2.80 GHz. Three evaluations are performed: matrix multiplications, forward-propagations of MLP on MNIST dataset, and forward-propagations of CNN on CIFAR-10 dataset. Comparison is made with BinaryNet and BDNN implementations. Espresso outperforms BinaryNet in computation speed, especially in dense matrix multiplication, by approximately 8 times. The evaluation focuses on real-time performance using MNIST and CIFAR-10 datasets with batch-size of one. Espresso's optimized kernels and use of register blocking result in a significant speed improvement, achieving a memory DRAM throughput of 40 GB s \u22121 for reads and 5 GB s \u22121 for writes. The 64-bit packing leads to a \u2248 25% speed improvement compared to BinaryNet's 32-bit kernel. In evaluations over the MNIST dataset, Espresso shows a consistent speed-up of \u2248 68\u00d7 compared to BinaryNet in classification execution time. Espresso's binary optimization achieves a speedup of \u2248 68\u00d7 compared to BinaryNet. The GPU opt implementation leads in performance evaluation, with a speedup of \u2248 12\u00d7 on an NVIDIA GTX 960. Through binary optimization, performance is further increased to \u2248 15\u00d7. This is attributed to the use of binary-optimized layers, optimized kernels for matrix multiplication, and Espresso's computational capabilities. Binary optimized layers in Espresso reduce the need for frequent parameter binarization during forward method calls, leading to improved performance. Specific layers are designed to pack parameters only once during network loading. Espresso's optimized kernels for matrix multiplication outperform BinaryNet's, with a slight difference in row-packing speed and a significant improvement in column-packing efficiency. This approach results in a performance gain of approximately 15%. Espresso achieves a 15% performance gain by using matrix-matrix multiplication kernels for dense layers with batch size 1. It also leverages binary optimization in the first layer by splitting input data into bit-planes, resulting in a 3x performance boost compared to non-optimized networks. The GPU optimized implementation of a binary-optimized CNN layer achieves significantly better performance compared to non-optimized networks. It requires significantly less memory, with a saving of approximately 31 times the amount of memory. The implementation follows the VGGNet-like CNN architecture and shows a 16 times speed-up compared to CPU in the MLP test. The gains are attributed to the parallelism of unrolling and pooling, as well as the higher memory throughput of the GPU. The GPU optimized implementation of a binary-optimized CNN layer in Espresso shows a \u2248 5\u00d7 performance gain compared to non-optimized networks. It also requires significantly less memory, with a saving of \u2248 31\u00d7 amount of memory. The framework supports both traditional DNNs and BCNNs for heterogeneous deployment on CPU and GPU, including popular CNN architectures. Espresso framework supports CNNs in addition to MLP networks, surpassing state-of-the-art MLP implementations. It is efficient, lightweight, and self-contained, utilizing CUDA kernels for GPU computation. Future plans include adding training capabilities and conducting performance comparisons on larger datasets."
}