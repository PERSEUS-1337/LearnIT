{
    "title": "RL33301",
    "content": "Program evaluations, including randomized controlled trials (RCTs), are crucial in informing and influencing policy decisions. Recent attention in the federal government has focused on RCTs, with various bills in Congress prioritizing their use in evaluating programs for offender reentry and low-income families. The report discusses the importance of randomized controlled trials (RCTs) in program evaluations, highlighting their role in informing policy decisions. It also addresses concerns about the reliability and interpretation of evaluations presented to Congress to influence decision-making. The report outlines the basic attributes of RCTs, ways to judge their quality, and diverse views on their practical capabilities and limitations in program evaluation. Randomized controlled trials (RCTs) are discussed as a form of program evaluation in the report, addressing concerns about reliability in decision-making. The report also highlights recent attention on RCTs in education policy and the President's budget proposal. Potential issues for Congress, oversight of policy areas, and pending legislation are identified. A glossary is provided for clarity in program evaluation vocabulary. Stakeholders, including citizens and elected officials, have an interest in government program performance and results. Stakeholders seek answers about defining public policy problems, program effectiveness, federal program management, stakeholder impact, unintended consequences, future government activities, resource allocation, evaluation quality, weaknesses in studies, and federal agency program evaluations. Program evaluations play a crucial role in policy discussions by addressing questions about assessing programs, research quality, agency capacity, independence, and the role of agencies in evaluation. These evaluations are of interest to Congress, stakeholders, and agency leaders, and can inform oversight and lawmaking processes. Program evaluations are crucial in policy discussions, involving various actors such as interest groups, think tanks, academics, legislators, and federal agencies. These actors bring evaluations to emphasize findings supporting their positions, with different approaches used in studies. The term program evaluation is practical in addressing questions and informing oversight and lawmaking processes. The term program evaluation is interpreted in various ways, with no consensus definition. It can refer to government policies, activities, projects, laws, or functions that someone may wish to evaluate. Evaluation is defined as an inquiry process for collecting evidence and drawing conclusions about the quality or significance of a program. Congress defined program evaluation for the Government Performance and Results Act of 1993. Program evaluation, as defined by the Government Performance and Results Act of 1993 (GPRA), is an assessment of how Federal programs achieve their intended objectives through objective measurement and systematic analysis. GPRA mandates executive branch agencies to develop strategic plans, performance plans, and program performance reports. Evaluation may not always be quantifiable due to the diversity of government activities. It involves investigating a program's operations and results to inform conclusions at specific points in time. Evaluation is a cumulative process over time, informing policy makers and Congress in their decision-making. However, viewpoints on program evaluations can be contentious, with disagreements on methods, quality, and interpretation of findings. It is crucial for policy makers to be informed consumers of evaluations. This report discusses different types of program evaluation methods, including RCTs, but does not provide an overall taxonomy. Practitioners and theorists have varying categorizations and inconsistent terminology when describing program evaluations. The report discusses the RCT method for program evaluation, which aims to estimate the impact of an intervention on a specific outcome of interest. An RCT randomly assigns subjects to treatment or control groups to compare the effects of the intervention. The RCT method compares two groups to measure the program's impact. Other types of program evaluation, such as observational designs, can also be used to address evaluation questions. Quasi-experimental designs are studies that estimate treatment impact without random assignment. Some lack a control group or measure outcomes before treatment. They are considered a form of observational design or impact analysis. Program evaluation often involves judging program effectiveness through interviews, observations, document reviews, and case studies. Nonexperiment methods, such as quasi-experiments, are used alongside systematic reviews to assess programs. Congress plays a role in making program evaluation policy and scrutinizing program evaluations. Congress plays a role in making program evaluation policy, including funding and methods for evaluations. It is important for policymakers to understand the practical capabilities and limitations of different evaluation methods. Members of Congress rely on specific program evaluations to inform their policy making and oversight of federal policies. Evaluation information can vary in quality, depth, and relevance, posing challenges for decision-makers. Understanding how to assess these aspects of evaluation methods can be beneficial for Congress in critically evaluating program evaluations. Program evaluations can provide a deeper understanding of policy problems, suggest ways to improve programs, and inform resource allocation. Evaluations can be controversial due to differing goals, values, and ethics influencing assessments of a program's merit and worth. Program evaluations assess the intrinsic and extrinsic value of a program to individuals and society. Evaluations help clarify program accomplishments and inform policy makers, but merit and worth can be subjective. Program evaluations assess the impact of a program on outcomes of interest by comparing results with and without the intervention. Subjects are randomly assigned to treatment and control groups to measure the effects of the intervention. Random assignment is used in program evaluations to ensure that treatment and control groups are statistically equivalent at the beginning of the study. The control group simulates what would have happened to the treatment group without the intervention, allowing for the estimation of the intervention's impact. This impact reflects the average of subjects who experienced different outcomes, making it easier to attribute observed differences to the intervention rather than initial group differences. Various statistical tools can be used to estimate whether observed differences are likely due to the intervention or to chance in a Randomized Controlled Trial (RCT). The quality of an RCT is assessed by internal validity, external validity, and construct validity. Internal validity measures the confidence in stating that the impact found was caused by the intervention being studied. For example, if an RCT shows that juveniles attending an aftercare program have lower recidivism rates than those who did not attend, it demonstrates the impact of the program. Internal validity in a Randomized Controlled Trial (RCT) assesses the methodological rigor of the study to determine if observed differences are due to the intervention or other factors. It depends on factors like random assignment, group size, and study design to ensure reliable conclusions about causation. The internal validity of a Randomized Controlled Trial (RCT) depends on factors like group size, random assignment, and study design to ensure reliable conclusions about causation. Factors such as attrition, contamination, and researcher bias also impact the study's validity. External validity in research refers to the extent to which the findings of a study can be applied to other settings, times, or groups of subjects. It involves assessing whether the intervention being studied can be replicated with confidence and whether it will have a similar impact in different situations or environments. The external validity of a study depends on factors like the confidence in replicating an intervention and potential deviations that may affect the study's outcome in other settings. The external validity of a study depends on factors like the way subjects were selected and the potential differences among populations and environments. Multi-site RCTs with diverse subjects are more generalizable. RCTs can be more generalizable if subjects are randomly selected from a certain population. Attrition can impact external validity, even if equivalent among treatment and control groups. Researchers describe interventions, subjects, and local environments to address these considerations. Construct validity is an important aspect of evaluation, focusing on accurately naming and measuring attributes or programs. It involves judging whether measures truly reflect what they are intended to measure, such as student achievement versus social capital. This type of validity ensures that a study evaluates the question it claims to be evaluating. Construct validity is crucial in evaluation, ensuring that measures accurately reflect what they are intended to measure. Actors in the policy process may have differing views on measuring student achievement or program success. When construct validity is applied to a program, it assesses how well the program aligns with its intended operation and causal mechanisms for achieving outcomes. Understanding these mechanisms can help improve the program or replicate its success in different circumstances. High internal, external, and construct validity are essential for the quality of a Randomized Controlled Trial (RCT). Internal validity ensures that impacts are due to the intervention studied, external validity ensures results can be replicated for other subjects, and construct validity confirms that outcomes measure what they are intended to measure and that the program caused the theorized impact. Randomized Controlled Trials (RCTs) are often seen as having high internal validity but can lack external validity, leading to doubts about the replicability of results. RCTs with low internal validity may not provide reliable evidence of impact, while those with high internal validity but low external validity may show impact for one population but not generalizability. Construct validity and mechanisms of causation can be challenging to establish in Randomized Controlled Trials (RCTs). Complementary evaluation methods may be necessary to ensure the intervention caused an impact as theorized. Some commentators suggest that quality assessment of RCTs should consider internal validity, external validity, statistical analysis, and ethical aspects. Claims about RCTs' practical capabilities and limitations should be considered in comparison with other research methods. Claims about RCTs' practical capabilities and limitations, in comparison with other research designs, have been controversial. Some observers view RCTs as the best way to determine a program's \"effectiveness,\" but there are challenges in interpreting terminology. Advocates have suggested RCTs for government social and economic activities. Some observers advocate funding government activities based on proven effectiveness through RCTs and quasi-experiments, while others may have different thresholds for what constitutes \"proven.\" RCTs and non-RCTs are seen as competitors or substitutes in judging effectiveness, with some valuing RCTs and quasi-experiments for impact estimation. Other methods may be deemed unreliable and of little value. Observational and qualitative methods are also considered appropriate for estimating impacts in various circumstances. Observational and qualitative methods are seen as complements to RCT designs, capable of casting doubt on findings, questioning inferences, establishing theory, and increasing external validity. Well-designed and implemented RCTs are considered the \"gold standard\" for making causal inferences about interventions, providing the most valid estimate of impact for a large sample of subjects. Different observers may prefer quantitative or qualitative methods, but RCTs are valued for their ability to establish causation in certain circumstances. RCTs, particularly double-blind ones, are considered the \"gold standard\" for determining the impact of interventions. They are extensively used in the medical field for evaluating drugs and procedures to identify the most effective treatments. Rigorously estimating impact is crucial for measuring a program's effectiveness and comparing different treatments. Non-random assignment studies are contrasted with RCTs in terms of validity and reliability. Studies that rely on non-random assignment or no control group can be subject to threats to internal validity, such as selection bias. There is consensus among program evaluation theorists that more RCTs should be performed in social science-related areas. More RCTs should be conducted in social science-related areas as part of a broad portfolio of evaluation strategies. However, there is less consensus on the proportion of evaluations that should be RCTs compared to other designs, and the conditions under which RCTs would be most valuable and likely to result in valid findings. Economists argue that RCTs have a preferred role in estimating impacts due to their high internal validity, but certain quasi-experimental methods can also validly estimate impacts under specific conditions. Some argue that quasi-experimental methods can provide valid impact estimates when RCTs are not feasible due to various factors. However, there are concerns that RCTs and quantitative evaluation methods may exclude qualitative research from being considered scientific or funded. Scholars and practitioners have also highlighted limitations of RCTs in certain areas of public policy, despite their strength in making cause-effect inferences. RCTs are considered strong for making cause-effect inferences for large samples if well-designed, but are challenging to implement. Some argue that there is a gap between theoretical capabilities and practical results of RCTs. Certain policy areas find it difficult to implement RCTs successfully, leading to skepticism of them being the \"gold standard.\" Some researchers suggest that well-designed observational studies can produce similar results to RCTs. Poorly designed RCTs can lead to inaccurate results. Poorly designed RCTs can yield inaccurate results and may not always support causal inferences for small sample sizes or individual cases. Some observers believe RCTs have limitations in external validity and may require support from other evaluation methods. RCTs can be seen as impractical, unethical, time-consuming, and costly compared to other research designs. RCTs are well-suited for certain questions but may not assess how and why impacts occur, program modifications, cost-effectiveness, unintended consequences, or validity of measures. Observational or qualitative designs are considered more appropriate for addressing these types of questions. A single well-designed study is rarely sufficient to reliably support decisions. In health care, reliance on a single RCT study is discouraged due to concerns about its robustness and external validity. The Cochrane Collaboration notes that most individual RCTs are not considered strong enough against chance effects and often lack quality. The overwhelming amount of health care information available, including from RCTs, is often of poor quality. In addressing questions of study quality, researchers use hierarchies of evidence and systematic reviews. There is debate on defining \"quality\" in evaluations, with some advocating for RCTs at the top due to high internal validity. Concerns about individual study quality have led to a reorientation in health care evaluation approaches, as seen with the USPSTF. The USPSTF, in response to limitations of the hierarchical grading system based on research design, now considers internal validity as a key factor in evaluating study quality. They have listed a hierarchy of research designs, with RCTs at the top, but also emphasize other inputs in assessing study quality. The USPSTF considers various research designs for study quality, including RCTs, quasi-experiments, observational studies, uncontrolled experiments, and expert opinions. They prioritize internal validity but also value external validity. The USPSTF considers different research designs for study quality, prioritizing internal validity but also valuing external validity. Researchers and decision makers often consider other studies, including non-RCTs, to ensure against chance and inform decision making. Systematic reviews are embraced in the healthcare field to identify knowledge gaps and draw conclusions from multiple studies. Systematic reviews are used in the healthcare field to identify knowledge gaps and draw conclusions from available evidence, helping to inform decision making about research priorities and provision of health care to patients. Researchers place systematic reviews as an additional category above RCTs in an evidence hierarchy, focusing on potential internal validity. Meta-analysis is also considered at the top of this hierarchy. A meta-analysis is a type of systematic review that uses statistical methods to derive quantitative results from multiple sources of evidence. Conducting and interpreting systematic reviews can be challenging and require caution as they may not be comprehensive for all stakeholders. Systematic reviews focus on specific questions and outcomes of interest, but may not cover all important outcomes. This can raise issues in evaluating public policies. The challenges of evaluating public policies arise from the diverse outcomes of interest and differing stakeholder perspectives on outcome measures. Systematic reviews in healthcare also face issues of external validity due to limited information for assessment. Systematic reviews have been less common in social science-related areas compared to health care, possibly due to the scarcity of RCTs and other evaluations in policy areas related to social sciences. This is evidenced by the significantly higher number of RCT studies published in medical literature compared to social sciences. Reasons for the relative lack of systematic reviews in the social sciences, compared to health care, include less funding for evaluation, more challenging research settings, resistance to using RCTs, disagreements on evaluation methods, and less interest from policy makers. Efforts to increase systematic reviews in social science have led to the creation of the Campbell Collaboration, which focuses on promoting the use of RCTs in defining evidence. The Campbell Collaboration focuses on promoting the use of RCTs in defining evidence in the social sciences. They also allow implementation studies and qualitative research in systematic reviews. Two contexts where RCTs have been highlighted are program evaluation policy in education and the citation of individual studies in policy and budget proposals to Congress. These cases are not analyzed in detail in the report, but the issues identified could be applied to them. The U.S. Department of Education established a priority for using Randomized Controlled Trials (RCTs) in evaluating education programs, stating that RCTs are best for determining project effectiveness. This priority, set by the Secretary of Education, has sparked controversy in the education policy and evaluation field. The U.S. Department of Education established a priority for using Randomized Controlled Trials (RCTs) in evaluating education programs, citing the Elementary and Secondary Education Act of 1965 (ESEA) as the statutory authority. This final priority aims to ensure federally funded projects are evaluated using scientifically based research methods. The statutory definition of scientifically based research includes research that employs observational or experimental methods, as well as research evaluated using experimental or quasi-experimental designs. The definition does not prioritize experimental designs over observational designs, except when comparing experimental and quasi-experimental designs. The notice of priority from the Department of Education (ED) emphasized the importance of random assignment and quasi-experimental designs in scientifically based research. The Education Sciences Reform Act of 2002 (ESRA) also highlighted the significance of randomized controlled trials (RCTs) in program evaluation. ESRA established the Institute of Education Sciences (IES) and outlined its functions. The Education Sciences Reform Act of 2002 (ESRA) emphasizes the use of random assignment experiments and other designs to establish causal relationships in IES-funded research. ESRA's definition for scientifically valid education evaluation requires the use of experimental designs, including random assignment, to estimate program impacts. The Education Sciences Reform Act of 2002 (ESRA) prioritizes random assignment experiments and other research methodologies for strong causal inferences in IES-funded research. The definition of effectiveness in the ED priority generated debate, with some interpreting it as synonymous with impact. However, there were interpretations that went beyond impact, suggesting that RCTs were the best method. During the comment period for the proposed priority, nearly 300 parties submitted comments to the Department of Education (ED). The comments were analyzed in the final priority notice, which revealed more criticism than support for the priority. Despite the substantive nature of the comments, ED decided not to make changes to the priority. Supporters and opponents of the priority agreed that more Randomized Controlled Trials (RCTs) were necessary in certain education policy situations. Many supporters of the ED priority believe more RCTs are needed in education policy to estimate impacts of interventions, while critics argue RCTs have been oversold and unjustifiably de-emphasize other evaluation designs. The ED priority for randomized experimental designs in research and evaluation projects could significantly impact education programs and policy by influencing the allocation of research funds and shaping future choices. Critics argue that this priority may detract from overall evaluation priorities. The ED prioritizes randomized experimental designs in research and evaluation projects to encourage scientifically based methods in federal education programs. The department aims to fund programs with strong evidence of effectiveness and eliminate those that do not meet this criteria. The definition of \"work\" in this context may refer to increasing student achievement. The ED prioritizes randomized experimental designs in research and evaluation projects to fund programs with strong evidence of effectiveness. The word \"work\" may indicate merit and worth, and a guidance document was issued to help distinguish practices supported by rigorous evidence. The Department of Education prioritizes randomized experimental designs to fund programs with strong evidence of effectiveness. A guidance document defines \"evidence\" as estimations of impact and mentions the What Works Clearinghouse to evaluate the strength of evidence for educational interventions. The What Works Clearinghouse (WWC) was established in 2002 by the U.S. Department of Education to provide trusted scientific evidence on effective education practices. The WWC prioritizes certain evaluation designs that provide strong evidence of impacts, excluding other types of evaluations. The Government Accountability Office (GAO) defines outcome evaluation to include assessments of unintended effects and program processes. The U.S. Department of Education (ED) plans to develop \"what works\" guides informed by the What Works Clearinghouse (WWC), which prioritizes certain evaluation designs for determining program effectiveness. ED and Institute of Education Sciences (IES) determinations of \"what works\" may primarily rely on studies with randomized controlled trials (RCTs) and certain quasi-experiments, excluding other types of program evaluations. The WWC website presents a different definition for scientifically based research compared to NCLB, focusing on ESRA standards for IES-funded research. The IES-funded research standards require random assignment experiments for claims of causal relationships, with other designs to eliminate competing explanations. The ED priority extends this requirement to the entire Education Department, linking WWC to NCLB's scientifically based research definition. The Bush Administration elevated RCTs as the preferred way to evaluate federal executive branch programs under the PART initiative, using it to determine overall effectiveness and justify budget proposals. The PART initiative, introduced by the OMB during the FY2004 budget cycle, assesses federal programs representing 20% of the budget each year. Programs are rated based on a questionnaire, producing a numerical score between 0% and 100% to determine overall effectiveness. Ratings were released with budget proposals from FY2004 to FY2007, with plans for further assessments in subsequent years. The OMB's PART initiative assesses federal programs on a scale of 0% to 100% to determine their effectiveness rating. Ratings include \"effective\" (85% to 100%), \"moderately effective\" (70% to 84%), \"adequate\" (50% to 69%), and \"ineffective\" (0% to 49%). Programs without acceptable performance measures are labeled as \"results not demonstrated,\" indicating unknown effectiveness rather than ineffectiveness. Disagreements on performance measures can lead to this designation. The Administration's PART initiative evaluates federal programs to determine their effectiveness. The PART aims to guide funding decisions and prioritize programs that work. In the context of the FY2006 budget proposal, 48 out of 99 discretionary programs were proposed for termination to focus resources on effective programs. In FY2006, nearly half of proposed terminations were in the Department of Education, citing lack of performance information or rigorous evaluations. The White House sent a list of terminations accepted by Congress, with 17 of 49 proposed ED terminations accepted, including a 56% cut in Even Start. The Administration proposed terminating or reducing 141 programs in the FY2007 budget. The Administration proposed terminating or significantly reducing 141 programs government-wide, with 42 programs within the Department of Education targeted for termination. These programs were deemed ineffective and lacked evidence of effectiveness, aligning with the Department's goal to eliminate support for ineffective programs. The Office of Management and Budget emphasized the use of Randomized Controlled Trials (RCTs) as the preferred method for evaluating program effectiveness in 2004. The document highlighted Randomized Controlled Trials (RCTs) as the best method for evaluating program effectiveness, endorsed by the Coalition for Evidence-Based Policy (CEBP) and the Office of Management and Budget (OMB). However, RCTs may not be suitable for every program, so agencies may need to consider alternative evaluation methodologies. In OMB's What Constitutes document, the term effectiveness is used in two senses: as a synonym for merit or worth, and as demonstrating impact. These concepts may not always align, as merit can be judged by factors beyond impact on a specific outcome. The document clarifies the definition of effectiveness in some cases, but in others, it may be open to interpretation by agencies. The curr_chunk discusses the importance of evaluating federal programs for effectiveness and impact. It emphasizes the need for agencies to use appropriate evaluation methods, such as Randomized Controlled Trials (RCTs), to demonstrate program impact. The key focus is on measuring program effectiveness by assessing the outcomes and impact of the program interventions. The curr_chunk emphasizes the importance of Randomized Controlled Trials (RCTs) in evaluating program effectiveness and impact. RCTs are considered the gold standard for assessing intervention effectiveness in various fields. The guidance from OMB for the FY2006 budget's PART also highlights RCTs as the preferred method for evaluating program impact. The guidance emphasizes the importance of Randomized Controlled Trials (RCTs) as the most significant aspect of program effectiveness. It suggests that RCTs are the highest quality, unbiased evaluation method to demonstrate program impact, but acknowledges that they may not be suitable for every program. In such cases, well-designed quasi-experimental studies are recommended as an alternative method to assess impact. The FY2007 PART guidance appears to have slightly changed its tone regarding RCTs compared to the FY2006 guidance. The guidance on program evaluation emphasizes the importance of Randomized Controlled Trials (RCTs) as particularly well suited for measuring impacts. It directs agencies and OMB to consult with evaluation experts when RCTs are not feasible, but OMB's judgments on evaluation designs have sometimes overridden agency decisions. Disputes have arisen over program success judgments, such as the controversy surrounding the Community Development Block Grant (CDBG) program's PART rating. The article discusses the Community Development Block Grant (CDBG) program and the FY2006 budget proposal to cut and consolidate 18 community and economic development programs. OMB's Deputy Director for Management, Clay Johnson III, mentions that political views and differing views on program goals may influence the evaluation process. Johnson emphasizes the importance of focusing on outcomes rather than outputs in housing programs. The Vocational Education State Grants program within ED, proposed for termination in the FY2007 budget, was deemed \"ineffective\" by the Administration based on the NAVE study. The study found no evidence that high school vocational courses contribute to academic achievement or college enrollment, leading to the program's termination justification. The benefits of vocational education are evident in increased earnings, with students earning almost 2 percent more for each additional high school vocational course. This applies to a diverse group of students, including those in postsecondary education, economically disadvantaged students, students with disabilities, and both men and women. Vocational courses taken in high school improve later earnings, especially for those who participate intensively in vocational programs. Vocational courses in high school improve later earnings but do not impact academic achievement or college transitions. The success of the program depends on policymakers' priorities. The PART assessment released in FY2007 did not mention earnings benefits, and disputes over program goals can arise. The debate over vocational education program goals questions the validity of program evaluations. Proponents of the PART commend its transparency and focus on program performance in budget discussions. Critics argue that the Administration's criteria for evaluation may not align with Congress's intended program goals. The report discusses how the Office of Management and Budget (OMB) has influenced program goals and measures, potentially conflicting with agency-developed goals under the Government Performance and Results Act (GPRA). It highlights the prominence of Randomized Controlled Trials (RCTs) in program evaluation policy and their use in justifying policy and budget proposals to Congress. The focus on RCTs raises various issues for Congress, including structural requirements, constraints, and broader program evaluation concerns. Congress may need to address these issues when developing legislation or overseeing programs. When developing program evaluation policy, Congress may need to consider various issues related to the parameters and risks of Randomized Controlled Trials (RCTs). Key parameters include random assignment, cost, time frame for producing findings, and ethical considerations. The central attribute of an RCT is the random assignment of subjects to treatment and control groups for making inferences about interventions. Random assignment in Randomized Controlled Trials (RCTs) allows researchers to infer the impact of interventions with reliable statistical tools. However, in some cases, like studying the impact of environmental policies on global warming, random assignment may not be feasible. When considering program evaluation policy, Congress should question the practicality of random assignment. When evaluating programs, it is important to consider if Randomized Controlled Trials (RCTs) are suitable. Congress should prioritize funding for quasi-experiments in addition to RCTs due to their cost. Large multi-site RCTs, especially in education, can be expensive, costing millions of dollars. Cost is a significant factor in conducting evaluations of federal programs. Large multi-site RCTs in education can cost millions, while smaller scale RCTs with 100-200 individuals may cost hundreds of thousands. Quasi-experiments are often cheaper but come with different benefits and risks. Funding constraints can limit the ability to conduct evaluations, leading to trade-offs in resource allocation. The opportunity cost of pursuing evaluations can be high due to varying benefits and costs depending on agency needs. Congress must weigh benefits against costs considering potential design flaws or contamination. Alternative evaluation designs may be considered depending on internal validity advantages of RCTs. Alternative evaluation designs, such as quasi-experimental studies, may be considered as alternatives to RCTs depending on Congress's specific evaluation objectives. RCTs can take a long time to yield results, which may not align with the need for timely policy decisions. However, the benefits of waiting for more evidence to make informed decisions must be weighed against the risk of evaluation information becoming outdated. When considering whether to direct the use of RCTs or other evaluations of public programs and policies, Congress might consider ethical duties to protect study participants' privacy and ensure informed consent. Oversight may be required to fulfill these duties and address privacy issues in program evaluations. Privacy issues may arise in an RCT if program evaluations capture information about individuals that could be misused or released publicly. Safeguards are required for collecting information, as mandated by the Privacy Act. This includes conditions for disclosing personally identifiable information, accounting for disclosures, specifying collection purposes, and enforcement measures. When a non-federal entity conducts a program funded by an agency, Privacy Act coverage is often outlined in contracts. Access to government programs in an RCT may be an issue if a control group is denied access. The question arises whether the benefits of testing a program outweigh denying access. Informed consent may also be a concern in RCTs, especially when enrolling only willing participants. The use of informed consent in RCTs and program evaluations may not always be required by law, but it ensures participants understand and agree to their involvement. Implementing individual protections in these studies could involve oversight to ensure privacy and ethical considerations are met. The oversight for program evaluations includes protections for participants' privacy and informed consent. This review process, though time-consuming and costly, can benefit studies by improving design. Congress and agencies have implemented various oversight mechanisms, such as grant competitions and peer review. They may also focus on issues like study interpretation and implementation during policy-making. When presenting program evaluations to Congress for policy decisions, it is important to consider the evaluation's internal validity. Contamination is a major threat to the internal validity of RCTs and quasi-experiments, while other designs face threats like selection bias. To avoid contamination, well-designed and implemented RCTs are crucial. Well-designed and implemented RCTs aim to insulate treatment and control groups from events that could affect outcomes differently. Insulating the study from unforeseen variables that may impact groups differently is challenging in social science research. Subjects in the treatment group being aware of their inclusion in a special program can lead to psychological effects not part of the treatment, affecting behavior. Control group subjects learning they are not in the treatment group may seek alternative treatments or undermine the intervention. In evaluating a crime prevention program with an RCT, perpetrators moving between experimental and control districts can contaminate results. Ensuring proper delivery of intended treatment to all subjects or sites can be challenging in some studies. Proper delivery of intended treatment is crucial to avoid contamination in program evaluation studies. Questions arise about the confidence Congress should have in the internal validity of studies and the ability to avoid contamination in certain policy areas. This impacts resource allocation and evaluation structure. When Congress is presented with a program evaluation to justify a policy position, the external validity of the evaluation becomes crucial. Generalizability of the intervention's findings to other circumstances is uncertain without further analysis. Congress should consider if the intervention can be replicated in different times or places for the findings to be applicable elsewhere. When evaluating program interventions for policy decisions, Congress must consider the generalizability of findings. Single-site RCTs may not always replicate impacts across different subjects, times, and circumstances. Multi-site RCTs are more generalizable but also more costly and challenging to conduct. Multiple impact analysis studies are recommended to ensure reliable findings. Complementary studies, such as observational or qualitative evaluations, are often necessary to understand an intervention's mechanism, unintended consequences, and conditions for success. Congress may scrutinize evaluation findings for external validity to other times, conditions, and subjects, especially when setting evaluation policy for an agency or program. Congress may need guidance on evaluation methods for program generalizability. Issues to consider include the focus on RCTs in evaluation policies and the inclusion of other evaluation methods in a broader framework. The report discusses the need for different types of evaluations to address diverse stakeholder needs in government programs and policies. Various evaluation techniques have been developed to answer key questions, with some types complementing each other. However, experts and practitioners may disagree on the practical capabilities and limitations of these evaluations due to finite resources. The report highlights the challenges in determining appropriate evaluation methods due to limited resources. It discusses the need for different types of evaluations to address stakeholder questions and potential gaps in perspectives. Congressional concerns include how the executive branch pursues evaluations under the PART initiative and how Congress oversees the Administration's efforts for budget and performance integration. In the education policy arena, questions arise about the appropriate implementation by the Department of Education. The Department of Education's implementation of program evaluation aspects under NCLB and ESRA is questioned, along with the prioritization of RCTs and quasi-experiments. Congress may need to provide guidance on program evaluation policy and methods for other policy areas. Stakeholders presenting evaluations to influence Congress should ensure they address key questions and provide a comprehensive view. Many observers emphasize the importance of policymakers being informed consumers of evaluation information to justify policy recommendations. However, the vocabulary of program evaluation can be confusing, with differing definitions for terms like \"effective.\" The term effectiveness can refer to a program's merit, goal achievement, impact on outcomes, or a mix of these concepts. Determining the effectiveness of a program can vary based on different definitions and perspectives. Stakeholders may have implicit assumptions about the best evaluation methods, leading to potential disagreements among experts. Congress may need to scrutinize these aspects closely to ensure a comprehensive evaluation process. In evaluating program effectiveness, stakeholders may have differing opinions on the best evaluation methods. Questions should be raised about the underlying reasons for claiming a method as \"best\" and whether there are alternative approaches that should be considered. This scrutiny is essential for ensuring a comprehensive evaluation process. In the policy process, Congress should use evaluation information from various actors to gain insights into policy problems and make better-informed decisions. The use of evaluation can be controversial, with some advocating for performance-based budgeting and evidence-based policy to determine a program's future funding or existence based on its effectiveness. Different actors do not have consensus definitions on what constitutes \"performance\" and \"evidence\" in decision making. There is debate on the role of past performance evaluation in shaping future strategies, with arguments for and against its predictive value. The process of strategic decision making involves various factors such as research, forecasting, risk assessment, professional judgment, intuition, and values. Social science has shown that there are practical limits to rationality in decision making, which should be considered by Congress when addressing policy questions. When considering policy questions, Congress should take into account various factors such as how RCT studies are used, if agencies and OMB are utilizing them appropriately, and what other factors should be considered. Program evaluations can offer insight into policy issues, but may not always be comprehensive, accurate, or unbiased. Congress should not solely rely on a single program evaluation for high confidence in findings. Program evaluations may not always be unbiased or well-designed, leading to flawed or inaccurate results. The way a program evaluation question is framed can influence how a program is portrayed, especially in terms of criteria for success. There may be varying views on how to judge a program's success, making it unclear if a study's research question covers all necessary aspects for a valid evaluation. Despite widespread consensus on how to judge success in program evaluations, there is still a possibility of inaccurate or flawed information due to random chance or unforeseen events. Concerns about data quality and the potential for false positive or false negative results exist, raising questions about the confidence Congress should have in evaluation information, including from RCTs. Social science researchers have recommended strategies to address these uncertainties. Social science researchers recommend that consumers of evaluation information consider the practical capabilities and limitations of different evaluation methods, scrutinize claims of validity, look for multiple studies or systematic reviews, and utilize resources like GAO for interpretation. Congress should assess if federal agencies have the capacity and independence to conduct and present evaluations objectively. The Government Accountability Office (GAO) has raised concerns about the limited resources spent on program evaluation in federal agencies. Despite recent efforts to emphasize program evaluation, it is unclear if agencies have the capacity to properly conduct, interpret, and use evaluations. This lack of capacity raises questions for Congress about the effectiveness of program evaluation within federal agencies. The Government Accountability Office (GAO) has raised concerns about the limited resources spent on program evaluation in federal agencies. Congress questions if agencies have the capacity to use evaluation information effectively for decision-making and if they have the necessary independence to present objective findings to Congress. Congress may need to address issues of evaluation capacity. Congress has established agencies like the Agency for Healthcare Research and Quality (AHRQ) and the Institute of Education Sciences (IES) to focus on evaluation issues in various policy areas, beyond just federal programs. These agencies have a mission to conduct research, synthesize evidence, and advance quality in healthcare and education. The Agency for Healthcare Research and Quality (AHRQ) and the Institute of Education Sciences (IES) have a mission to provide national leadership in expanding fundamental knowledge and understanding in education and healthcare. Their focus goes beyond federal programs to encompass research and evaluations throughout entire policy areas. The vocabulary of program evaluation can be technical and complex, making it challenging for consumers to understand. Clear definitions and distinctions between key terms are crucial for interpreting study findings accurately and scrutinizing evaluations effectively. The appendix provides definitions for recurring terms in evaluations to help readers interpret findings accurately and scrutinize evaluations effectively. It emphasizes the importance of clear definitions and distinctions between key terms in program evaluation. Construct validity refers to the extent to which a study evaluates what it claims to evaluate, while contamination in an RCT refers to unintended factors that may affect the treatment and control groups differently. It is important to insulate these groups from contaminating events to ensure accurate outcomes. RCTs should ensure proper administration of treatment to prevent contamination. Control group is chosen randomly and does not receive the program being studied. Effect is the outcome following a cause, while effectiveness refers to achieving specific goals. Evaluation involves collecting evidence to draw conclusions about value and merit. The curr_chunk discusses external validity, which refers to the extent to which a study's findings can be applied to other settings or groups. It also mentions the Government Performance and Results Act (GPRA) of 1993, a law requiring federal agencies to develop strategic plans and performance reports. Impact measurement evaluates how a program intervention affected the outcome of interest for a large group of subjects, on average, compared to what would have happened without the intervention. It may not always be quantifiable due to the diverse activities of the federal government. Impact measurement evaluates the effects of a program intervention on a group of subjects, determining if the intervention caused the observed impact. Internal validity in an RCT refers to the confidence in attributing the impact to the intervention. Meta-analysis uses statistical methods to analyze multiple sources of evidence quantitatively. Observational design helps explain cause-and-effect relationships without replicating an RCT. In an RCT, stakeholders may have different outcomes of interest related to a program. Performance measurement involves monitoring program operations and reporting progress towards goals. Performance measurement is distinct from program evaluation and can include statistical information. In program evaluation, synonyms for program include treatment and intervention. Program evaluation is an assessment of how Federal programs achieve intended objectives under the Government Performance and Results Act of 1993. It informs conclusions at particular points in time and is a cumulative process over time. Different types of program evaluation are categorized by practitioners and theorists, sometimes referred to as designs or methods. Qualitative evaluation involves judging program effectiveness through interviews, observations, document reviews, and case studies. Quasi-experimental design estimates treatment impact without random assignment to control groups. Quasi-experiments may lack a control group and do not always measure outcomes before treatment. They are considered a form of observational design by some, while others place them in a separate category. Random assignment involves assigning subjects by chance to control and treatment groups. Random selection is different from random assignment and involves drawing a sample from a larger population. Randomized controlled trials use random assignment to evaluate programs by comparing treatment and control groups. Randomized controlled trials (RCT) compare treatment and control groups to estimate program impact. Terms like randomized field trial (RFT) are used interchangeably with RCT. Double-blind studies conceal treatment allocation from both subjects and researchers. Single-blind studies hide treatment information from subjects. Statistical significance is crucial in RCT findings. Statistical significance in RCTs indicates confidence in treatment impact, with a 95% probability level. Significance at the .05 level suggests a 1 in 20 chance of the observed difference occurring by chance. However, significance does not guarantee a large or important impact. Systematic reviews are structured literature reviews addressing specific questions. Systematic reviews are structured literature reviews that analyze evidence to answer specific questions. They involve searching, applying criteria, appraising literature, and synthesizing data to formulate findings. External validity is left for readers to assess in terms of how applicable the evidence is to their circumstances. Systematic reviews analyze evidence to answer specific questions, often focusing on RCTs and various types of studies. An RCT involves a treatment group randomly assigned to a program, with subjects as the unit of analysis. Validity includes internal, external, and construct validity. Worth refers to the overall value of a program to society, often paired with merit."
}