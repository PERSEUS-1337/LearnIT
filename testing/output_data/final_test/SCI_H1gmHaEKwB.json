{
    "title": "H1gmHaEKwB",
    "content": "Model compression is essential for deploying neural networks on devices with limited resources. Most compression methods lack guarantees on the trade-off between compression rate and approximation error for new samples. We propose an efficient neural pruning algorithm based on the coreset framework, ensuring a trade-off between compression rate and approximation error for any test sample. Our method is data-independent and guarantees accuracy for any input, including adversarial ones. Demonstrated effectiveness on popular network architectures like LeNet-300-100 on MNIST. Neural networks have become popular in machine learning, with overparametrization leading to better convergence and generalization. However, the high memory and computational costs limit their use in small devices. Various approaches to compress existing models have been proposed to reduce network sizes without significant accuracy loss. Various model compression heuristics have been successfully applied to neural network models, but they lack strong provable guarantees on the trade-off between compression rate and approximation error. Data-dependent methods for model compression are vulnerable to adversarial attacks. A network compression framework should ideally provide provable guarantees. Our proposed network compression framework aims to provide provable guarantees on the tradeoff between compression rate and approximation error. It is based on the theory of coresets, which reduce massive inputs to smaller instances while maintaining a good approximation of the original set. By treating neurons of a neural network as inputs in a coreset framework, we can efficiently reduce the number of neurons in each layer while maintaining performance. The coreset algorithm selects neurons in each layer and determines new weights connecting them to the next layer. It is applied layer by layer from bottom to top, with the coreset size linked to approximation error. This data-independent compression method differs from data-dependent approaches, like Baykal et al. (2018), which focus on weight coresets rather than neuron coresets. Neural pruning reduces weight tensors size while maintaining network density. Implementing pruned networks requires no extra effort. Coresets of neurons outperform sampling-based coresets in compression without sacrificing accuracy. Fast construction, taking 56 sec to compress each dense layer in VGG-16 network. The neural pruning algorithm offers a trade-off between compression rate and output approximation error using coresets. Coresets have been successful in data reduction for various applications, relying on sensitivity framework for importance measure. Weight pruning is a method used to reduce redundancy in overparameterized neural networks by removing unimportant connections through sparsity enforcement, typically done via L1 regularization. However, studies have shown that after fine-tuning, L2 regularization may outperform L1 regularization as there is no benefit to pushing values towards zero compared to pruning small weight connections. Neural pruning methods like low-rank approximation and weight quantization show high compression rates, but lack strong provable worst-case analysis. Weight pruning leads to irregular network structures, making computational savings challenging. Neural pruning methods, such as filter pruning in CNNs, reduce tensor size by identifying and removing weak neurons or channels based on their contribution to discriminative power. These data-dependent methods lack guarantees for future inputs and may involve retraining the network multiple times. Channel pruning methods aim to minimize reconstruction error between pruned and pre-trained models, lacking guarantees on accuracy and compression trade-offs. In contrast, coreset-based model compression, like Baykal et al. (2018), constructs coresets of neurons instead of weights, creating a data-independent coreset. Dubey et al. (2018) introduces a coreset compression method for neural networks that is data-dependent. The method involves adding a sparsity constraint to k-means coresets and weighting filters based on their activation magnitudes. A pre-processing step eliminates filters based on their activation norms. The algorithm compresses each layer of the network from bottom to top, with a theoretical analysis provided for the construction. The proposed construction aims to create a small coreset that approximates the input data with bounded error. The ideal goal is shown to be impossible in Theorem 6, but Theorem 7 and Corollary 8 present a method to construct a small coreset with controlled error. Algorithms are provided for constructing coresets for single neurons and multiple neurons in a layer. The concept of a weighted set is defined, and Algorithm 1 outlines the coreset construction process. A coreset is constructed from a weighted set in a query space with an input set, objective function, and models. It provides a good approximation to the input set for any query. An \u03b5-coreset with multiplicative guarantees is defined as a weighted set that approximates the query space with controlled error. An \u03b5-coreset of (P, w, X, f) is a weighted set (Q, u) that approximates the query space with controlled error. The size of coresets depends on the complexity of the activation function and a defined supremum. The VC-dimension of the query space (P, X, f) is O(d) for common activation functions. Theorem 5 by Braverman et al. (2016) bounds the size of a coreset for a query space with additive error. It states that with a sample of m points from P, a (1 + \u03b5)-multiplicative factor approximation is provided for every query. Theorem 6 shows that small coresets cannot be computed for certain activation functions like ReLU, even with constraints on input and test set sizes. It introduces the concept of additive \u03b5-error as a better alternative to multiplicative error. Theorem 7 states that for a query space with certain constraints, the size of the coreset is bounded. The proof is provided in Appendix A.2. The result is generalized to include negative weights and any bounded activation function. The size of the coreset is bounded by a sufficiently large constant determined from the proof. Algorithm 2 and Corollary 9 explain how to compute a single coreset for multiple weighted sets, illustrating layer pruning in a neural network. The coreset size is bounded by a large constant determined from the proof. Neural pruning with coresets is tested on LeNet-300-100 and VGG-16 models. Compression rate of Neuron Coreset is compared to other sampling schemes like uniform sampling, percentile, and SVD for matrix sparsification. In experiments using PyTorch on a Linux Machine with Intel Xeon CPU and Nvidia GPUs, coresets pruned 90% of parameters without accuracy loss in a LeNet-300-100 network trained on MNIST data. VGG-16 model with 5 blocks of layers was trained and tested on CIFAR-10. Neural pruning was applied to dense layers, resulting in a 75% decrease in parameters with a slight improvement in classification accuracy. Analysis of the trade-off between approximation error and coreset size was conducted. In a study on neural pruning, different tests were conducted to evaluate approximation error and coreset size. Tests included varying weight distributions and subset sizes of neurons. The evaluation used images from the MNIST test set as queries. Our coresets outperformed other methods in tests on neural pruning for approximation error and coreset size. The compression framework involves selecting neurons for each layer and fine-tuning. The algorithm excelled, especially at high compression rates, compared to well-known algorithms. Our coreset algorithm, which involves selecting neurons using Algorithm 2 followed by fine-tuning, outperformed other methods in neural pruning tests for approximation error and coreset size. Ablation analysis on LeNet-300-100 trained on MNIST showed that the coreset selection provides better accuracy compared to uniform sampling, requiring less fine-tuning. Our proposed neural pruning algorithm based on the coreset framework shows high compression rates with no accuracy loss on ReLU networks. The method requires less fine-tuning compared to uniform sampling and offers provable trade-offs between compression rate and approximation error. Future work will extend the framework to pruning filters in CNNs and other architectures. The proposed neural pruning algorithm based on the coreset framework achieves high compression rates without sacrificing accuracy on ReLU networks. It requires less fine-tuning than uniform sampling and provides provable trade-offs between compression rate and approximation error. Future work will expand the framework to pruning filters in CNNs and other architectures. Applying Theorem 1 with X = B \u03b2 (0) yields that, with probability at least 1 \u2212 \u03b4, \u2200x \u2208 B \u03b2 (0), Equation 6 is derived by separating sums into points with positive and negative weights and applying Cauchy-Schwarz inequality. Points with positive and negative weights are then bounded separately using Theorem 7."
}