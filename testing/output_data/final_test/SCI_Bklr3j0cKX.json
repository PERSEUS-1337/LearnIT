{
    "title": "Bklr3j0cKX",
    "content": "This work introduces Deep InfoMax (DIM), a method for unsupervised learning of representations by maximizing mutual information between input and output of a neural network encoder. Incorporating locality knowledge in the input improves representation quality for downstream tasks. DIM outperforms popular unsupervised methods and competes with fully-supervised learning on classification tasks, offering new possibilities for representation learning objectives. The goal of deep learning is to find useful representations by training an encoder to maximize mutual information between inputs and outputs. Recent advances allow for effective computation of mutual information in high-dimensional settings. Maximizing mutual information between local regions of the input and the representation can greatly improve quality for classification tasks. Deep InfoMax (DIM) combines mutual information maximization with prior matching to constrain representations according to desired statistical properties. This approach is related to the infomax optimization principle, leading to the formalization of DIM, which estimates and maximizes mutual information between input data and learned high-level representations. Deep InfoMax (DIM) maximizes mutual information between input data and learned high-level representations. It uses adversarial learning to constrain representations with desired statistical characteristics. New measures of representation quality are introduced, including Mutual Information Neural Estimation (MINE) and a neural dependency measure (NDM). DIM is compared to other unsupervised methods for learning representations. Classic methods like independent component analysis (ICA) and self-organizing maps lack the same capabilities. Deep neural networks have higher representational capacity compared to classic methods like ICA and self-organizing maps. Recent approaches include deep volume-preserving maps, deep clustering, noise as targets, and self-supervised learning. Generative models play a role in building representations, with mutual information being crucial for the quality of the learned representations. Reconstruction error in generative models can be linked to mutual information. The reconstruction error in generative models can be related to mutual information. Models with reconstruction-type objectives provide guarantees on the information encoded in their representations. Adversarial models also use mutual information to train an encoder/decoder to match joint distributions or minimize reconstruction error. Methods based on mutual information have a long history in unsupervised feature learning, with the infomax principle advocating maximizing mutual information between input and output. Mutual Information Neural Estimation (MINE) and Deep InfoMax (DIM) are approaches that estimate mutual information of continuous variables, with DIM using Jensen-Shannon divergence for stability and better results. DIM can work with various MI estimators and leverage local structure in the input to improve representations for classification. Data augmentations and transformations can prevent degenerate solutions in discrete MI. Unsupervised clustering and segmentation can be achieved by maximizing MI between images associated by transforms or spatial proximity. The study explores representations learned from different MI objectives focusing on local or global structure, essential for training representations for diverse applications. Contrastive Predictive Coding (CPC) is a mutual information-based approach that maximizes MI between global and local representation pairs. Unlike DIM, CPC processes local features sequentially to build partial \"summary features\" for predicting future local features. In contrast, DIM uses a single summary feature to predict all local features simultaneously in a single step. Deep InfoMax (DIM) aims to predict all local features at once using a single estimator, incorporating occlusions during training. The base encoder model encodes an image into a feature map of M \u00d7 M vectors, summarized into a single feature vector Y. DIM utilizes a global mutual information objective to train the network, passing high-level feature vector Y and lower-level feature map through a discriminator. Training involves generating fake samples by combining feature vectors with feature maps from other images. The encoder is trained to maximize mutual information between input and output. It defines a family of encoders over a domain and range of a function. The encoder generates encodings from input space samples, creating a distribution over encodings. This approach can be adapted for temporal data as well. The encoder is trained to maximize mutual information between input and output, following the Deep InfoMax (DIM) framework. It involves mutual information maximization and statistical constraints to shape the output of the encoder. The approach is based on Mutual Information Neural Estimation (MINE) and aims to find parameters that maximize mutual information. The approach involves training a classifier to distinguish between joint and marginal samples, encoding images to feature maps, and using a discriminator function modeled by a neural network to estimate and maximize mutual information. The approach involves training a classifier to distinguish between joint and marginal samples, encoding images to feature maps, and using a neural network to estimate and maximize mutual information by optimizing non-KL divergences. The Jensen-Shannon-based estimator is compared to the DV-based estimator in the context of minimizing total correlation. It is shown to be more stable and works better in practice. Noise-Contrastive Estimation (NCE) is used as a bound on mutual information in the DIM framework. The key difference between DV, JSD, and infoNCE formulations lies in the expectation over P/P. The JSD-based objective mirrors the original NCE formulation in Gutmann & Hyv\u00e4rinen (2010), with DIM setting the noise distribution to the product of marginals over X/Y and the data distribution to the true joint. The infoNCE formulation follows a softmax-based version of NCE, similar to ones used in language modeling, with strong connections to binary cross-entropy in noise-contrastive learning. Implementations of JSD and infoNCE are similar and can reuse most of the same code. In experiments, infoNCE often outperforms JSD on downstream tasks, but this effect diminishes with more challenging data. However, infoNCE and DV require a large number of negative samples to be competitive. Generating negative samples becomes cumbersome with increasing batch size. DIM with JSD loss outperforms infoNCE as the number of negative samples decreases. The objective in Eq. 3 maximizes mutual information between input and output, but this may not always be desirable depending on the task. In order to obtain a representation more suitable for classification, maximizing the mutual information between the high-level representation and local patches of the image is favored. The encoder can selectively pass information through based on the input, favoring shared aspects across patches. The local DIM framework presented in FIG0 encodes input to a feature map reflecting useful structure, then summarizes it into a global feature. The MI estimator maximizes the average estimated MI between global/local pairs, promoting shared information across the input. Multiple easy-to-implement architectures have been successful in optimizing this \"local\" objective. Further implementation details are provided in the App. (A.2). The DIM framework encodes input to a feature map reflecting useful structure and summarizes it into a global feature. It imposes statistical constraints on learned representations by training a discriminator to estimate divergence and then minimizing this estimate. This approach is similar to adversarial autoencoders but without a generator. Deep InfoMax (DIM) is a framework that encodes input into a feature map, imposes statistical constraints on learned representations, and trains a discriminator to estimate divergence. It can be used with global and local mutual information maximization and prior matching objectives. The hyperparameters in DIM affect the learned representations significantly. Additionally, prior matching alone can be used to train a generator of image data. The evaluation of representations relies on various proxies such as linear separability and mutual information with class labels. Some works have explored transfer learning tasks by freezing encoder weights and training a neural network classifier using the representation as input. Others have directly measured mutual information between labels and representations. Some studies have measured mutual information between labels and representations, while others have used mutual information neural estimation to directly measure the mutual information between the input and output of the encoder. Additionally, a discriminator can be trained to assess the independence of representations by generating factor-wise independent distributions. The Neural Dependency Measure (NDM) evaluates the KL-divergence between original and shuffled representations to measure factor dependency. Evaluation metrics include linear and non-linear classification using SVM and neural networks, as well as semi-supervised learning. The curr_chunk discusses various methods for evaluating separability and mutual information in neural networks, including semi-supervised learning, MS-SSIM, Mutual Information Neural Estimate (MINE), and Neural Dependency Measure (NDM). These methods involve fine-tuning encoders, training discriminators, and measuring KL-divergence between representations. The experiments were conducted on multiple datasets excluding CelebA. In experiments on various datasets excluding CelebA, separate classifiers were built for high-level vector representation, the output of the previous fully-connected layer, and the last convolutional layer. Model selection was based on averaging the last 100 epochs, with a uniform dropout rate and decaying learning rate schedule to prevent over-fitting. Different objectives were used for DIM, with a compact uniform distribution chosen as the prior. DIM(L) outperformed other priors like Gaussian, unit ball, or unit sphere on all datasets, except for CPC. It performs as well as or better than a fully-supervised classifier without fine-tuning, indicating high-quality representations. However, fully supervised classifiers can achieve better results with specialized architectures and data augmentations. Competitive results on CIFAR10 have been achieved in different settings. Our STL-10 results are state-of-the-art for unsupervised learning, supporting the hypothesis that the local DIM objective is effective in extracting class information. InfoNCE performs best, with diminishing differences compared to JSD with larger datasets. DV competes with JSD on smaller datasets but performs worse with larger datasets. CPC and DIM performance improved significantly with a strided crop architecture, outperforming BiGAN. For CIFAR10 and STL-10, DIM(L) and CPC used infoNCE with an \"encode-and-dot-product\" architecture. CPC utilized 3 networks for predicting local feature maps. Without data augmentation, CPC underperformed DIM(L) on CIFAR10. On STL-10 with data augmentation, CPC and DIM showed competitive performance. The DIM model enhances performance by computing less global features conditioned on 3x3 blocks of local features, maximizing mutual information. This approach utilizes a single MI estimator across all possible 3x3 blocks of local features, resulting in significant performance improvements. The model was tested on CIFAR10 using a ResNet-50 encoder and on STL-10 with a similar architecture. Additionally, a version of DIM computes the global representation from a 3x3 block of local features. The DIM model, which computes global representation from 3x3 blocks of local features, is competitive with CPC in certain settings. However, it performed worse than using fully global representation with CIFAR10. DIM slightly outperforms CPC, suggesting that strictly ordered autoregression may not be necessary for some tasks. Linear separability, reconstruction, mutual information, and dependence were evaluated with the CIFAR10 dataset using support vector machines and MINE with a decaying learning rate schedule. The MINE model used a decaying learning rate schedule to reduce variance and achieve faster convergence. MS-SSIM correlated well with the MI estimate from MINE, indicating good pixel-wise information encoding. Models showed lower dependence compared to BiGAN, with DIM showing potential when combining local and global objectives. Reconstruction-based models like VAE and AAE scored high for MI, and augmenting DIM with input occlusion for global features was considered. Further analysis can be found in the ablation studies and nearest-neighbor analysis in the appendix. Augmenting DIM involves adding input occlusion for computing global features and maximizing mutual information between local features and spatial coordinates. These additions improve classification results. For occlusion, part of the input is randomly occluded when computing global features, while local features are computed using the full input. Maximizing mutual information between occluded global features and unoccluded local features encourages encoding shared information across the entire image. Additionally, the model's ability to predict coordinates of local features is maximized by minimizing crossentropy. This task can be extended to maximize conditional mutual information between pairs of local features given global features. The Deep InfoMax (DIM) method aims to maximize conditional mutual information between pairs of local features and their relative coordinates, given global features. This approach enhances feature learning by encoding images based on distributions over relations among higher-level features. The method incorporates input occlusions and coordinate prediction tasks, which are extensions of inpainting and context prediction tasks for self-supervised learning. Deep InfoMax (DIM) introduces a method for learning unsupervised representations by maximizing mutual information, emphasizing locally-consistent information across structural \"locations\". The relationship between Jensen-Shannon divergence (JSD) and pointwise mutual information (PMI) is explored through marginal and conditional distributions. The relationship between Jensen-Shannon divergence (JSD) and pointwise mutual information (PMI) is examined by comparing the joint and product of marginals, which represent mutual information (MI). The difficulty of computing continuous MI is addressed by assuming a discrete input with uniform probability and a randomly initialized joint distribution. The experiments involved sampling from a uniform distribution, applying dropout for sparsity, and computing KL and JSD divergences between joint and marginal distributions. Results show a monotonic relationship between KL and JSD, with distributions having high mutual information also having high JSD. Architectural details for the experiments are provided, with code for running Deep Infomax (DIM) available. The encoder used is similar to a deep convolutional GAN (DCGAN) discriminator. The experiments involved using an encoder similar to a deep convolutional GAN (DCGAN) discriminator for CIFAR10 and CIFAR100 datasets, and an Alexnet architecture for other datasets. ReLU activations and batch norm were applied to hidden layers. The output of all encoders was a 64-dimensional vector. Strong monotonic relationship between mutual information and JSD was observed. The global mutual information objective involves encoding input into a feature map and further encoding it using linear layers to get a representation. A fully-connected network with hidden layers is used for scoring pairs of feature map and vector. Fake samples are generated by combining global and local features from different images. The architecture featured in the results includes a 1 \u00d7 1 convnet with two 512-unit hidden layers as discriminator. Another tested architecture involves embedding global and local features in a higher-dimensional space for efficient evaluation of pair-wise scores using dot products. This approach can represent arbitrary classes of non-linear pair-wise functions. The architecture involves a Concat-and-convolve and Encode-and-dot-product approach for scoring real and fake feature map pairs. Matching the encoder output to a prior is also part of the process. The architecture involves training a discriminator to classify real and \"fake\" samples from the encoder output, while the encoder aims to deceive the discriminator. The global and local features are obtained using fully connected neural networks, with details provided in the experiment tables. The outputs of these networks are combined through matrix multiplication, allowing for efficient computation of positive and negative examples simultaneously. This architecture is highlighted in the main classification results. The architecture involves training a discriminator to classify real and \"fake\" samples from the encoder output. The feature map for the local objective can be taken from any level of the encoder, while for the global objective, the last convolutional layer is used. Different layers are found to work best for different datasets, likely due to the size of receptive fields. The discriminator in the prior matching architecture is a fully-connected network with two hidden layers. Generative models follow a setup similar to Donahue et al. Generative models were trained using a DCGAN generator with Adam optimizer. Contrastive Predictive Coding utilized a GRU-based PixelRNN with matching dimensions for hidden units. JSD-based estimator outperformed infoNCE and DV-based estimators by excluding positive samples in the product of marginals. In our implementation, infoNCE was sensitive to the number of negative samples for estimating the log-expectation term. With a high sample size, infoNCE outperformed JSD on many tasks, but performance dropped quickly with fewer images. DV was outperformed by JSD even with the maximum number of negative samples, and was highly unstable as the number of negative samples decreased. Accuracies were averaged over the last 100 epochs for infoNCE, JSD, and DV DIM losses. In our implementation, infoNCE was sensitive to the number of negative samples for estimating the log-expectation term. JSD is insensitive to the number of negative samples, while infoNCE shows a decline as the number of negative samples decreases. DV also declines, but becomes unstable as the number of negative samples becomes too low. Nearest-neighbor analysis of DIM's representations shows that DIM(L) has a more interpretable structure across images, but using only consistent information across patches may lead to shared patterns among nearest neighbors. The text discusses the use of nearest-neighbor analysis to examine shared patterns across patches in representations. Heatmaps were generated using bilinear interpolation and thresholded for clarity. An ablation study on CelebA was conducted to analyze the impact of global and local parameters on classification accuracy. The study highlights the importance of hyperparameters in influencing the encoder's representational characteristics. The results of an ablation study for DIM on CIFAR10 show that good classification performance depends on the local term \u03b2, while good reconstruction relies on the global term \u03b1. A combination of \u03b1 and \u03b2 yields higher MINE estimates for mutual information. In CelebA, the global objective has a stronger impact on classification accuracy compared to other datasets. A schematic of learning the Neural Dependency Measure is shown in Figure 12. The text discusses encoding inputs into representations, shuffling features for discrimination between real and fake labels, and analyzing Neural Dependency Measures (NDMs) for various \u03b2-VAE models. Increasing \u03b2 values show a trend towards stronger independence estimates. Experimental details on occlusion and coordinate prediction are presented. Experimental details on occlusion and coordinate prediction tasks are presented. Occlusions were randomly applied to input images, ensuring at least one visible 10x10 block of pixels. For absolute coordinate prediction, global and local features are sampled from the data distribution. The text discusses the sampling of global and local features for relative coordinate prediction using an MLP with two hidden layers. The prediction function is implemented with ReLU activations and batchnorm, and gradients are computed by marginalizing over all local features associated with a global feature. The text discusses using a predictive model with the same architecture as described previously to train a high-quality image generator. The model selects source and target local features to compute gradients and utilizes a one-dimensional mixture of two Gaussians to map to a generator function. The text introduces a generator function Z \u2192 X with input z drawn from a simple prior distribution. It utilizes a predictive model to find parameters \u03b8 and introduces discriminators to optimize the JSD f-GAN lower bounds for training. The text discusses training the generator in the JSD f-GAN framework to align the first-order moment of the distributions. It highlights the importance of overlap between the encoder targets V P and V Q for stable gradient flow during training. The encoder targets V P and V Q should overlap for stable gradient flow during training. The generator and encoder use a ResNet architecture, with contractive penalty on the encoder and gradient clipping on the discriminators. Training was done on LSUN BID10, CelebA, and Tiny Imagenet datasets. Samples of generated results from three different methods, NS-GAN-CP, WGAN-GP, and mapping to two Gaussians, were compared after 100 epochs. The qualitative results showed no significant differences. The generator mapping to two Gaussians produced highly realistic images competitive with other methods. Quantitative comparison with NS-GAN-CP and WGAN-GP showed that while the method did not surpass them, it came reasonably close."
}