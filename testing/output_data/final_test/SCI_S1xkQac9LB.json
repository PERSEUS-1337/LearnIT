{
    "title": "S1xkQac9LB",
    "content": "The limitations of BLEU and ROUGE metrics are reviewed, criteria for a good metric are proposed, and concrete ways to improve them are suggested. In this work, the performance of evaluation metrics like BLEU and ROUGE in machine translation and summarization is assessed. The limitations of these metrics have been criticized by the academic community. The study aims to provide an empirical criticism of BLEU and ROUGE and establish criteria for better evaluation metrics. In this work, an empirical criticism of BLEU and ROUGE evaluation metrics is formulated, proposing criteria for sound evaluation metrics. Recent advances in NLP are used to design a data-driven metric addressing weaknesses in BLEU and ROUGE. These traditional metrics have been widely used in NLP tasks for almost two decades, but their correlation with human judgement has been extensively criticized in the literature. The literature extensively criticizes the shortcomings of BLEU and its low correlation with human judgment. Various studies have found that BLEU does not correlate with human judgment on adequacy or fluency in machine translation and text simplification tasks. Language modeling, as explained by Radford et al., has become crucial in NLP tasks. Two leading architectures for language modeling are Recurrent Neural Networks (RNNs) and Transformers. RNNs process input tokens one by one through time, while Transformers learn dependencies between tokens using an attention mechanism. New evaluation metrics like BERTscore and Sentence Mover's Similarity use word embeddings and cosine similarity to measure similarity between sentences. Another method, RUSE, proposes embedding both sentences for evaluation. The RUSE method proposes embedding sentences separately and pooling them before using a pre-trained MLP for prediction on various tasks. A quality estimator metric is suggested for language evaluation, aiming to go beyond just architecture specifications. Limitations of BLEU and ROUGE are discussed, highlighting how they can assign high scores to semantically opposite translations/summaries and low scores to semantically related ones. These metrics may also give high scores to unintelligible translations/summaries. The limitations of BLEU and ROUGE scores are discussed, including insensitivity to negation, word permutation, and unintelligible sentences. Higher order BLEU scores aim to address these issues but may increase sensitivity to paraphrasing. To improve evaluation metrics for language, first principles criteria are established, emphasizing correlation with human judgement in semantic similarity and the ability to distinguish between different translations/summaries. The curr_chunk discusses the implementation of a scorecard for evaluating sentence similarity, emphasizing the need to correlate with human judgment and distinguish between logical contradictions, unrelated sentences, and agreements. Experiments are conducted using BLEU, ROUGE, and a neural evaluator, RoBERTa, fine-tuned for sentence similarity prediction. The goal is for the similarity metric to align closely with human judgment. The curr_chunk evaluates the performance of BLEU and ROUGE metrics compared to a RoBERTa model for semantic similarity on the STS-B benchmark. It also discusses the importance of differentiating between contradiction, neutral, and entailment in sentence evaluation using the MNLI dataset. The RoBERTa model outperforms BLEU and ROUGE, showing little correlation with human judgment. In this work, a framework is established to assess metrics comparing the quality of reference and hypothesis summary/translations. Random corruptions are introduced to sentence pairs from the MNLI dataset, and Spearman's ranked correlation and Kendall's \u03c4 are used for evaluation. The study highlights the potential of recent Transformers to replace BLEU and ROUGE metrics."
}