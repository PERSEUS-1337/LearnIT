{
    "title": "HklA93NYwS",
    "content": "Deep neural networks have achieved state-of-the-art performance in various fields, but they need to be scaled down for real-world applications. Knowledge transfer methods like knowledge distillation (KD) have gained attention for reducing network size while maintaining performance. This work focuses on changing the structure of a teacher network to mimic multiple teacher networks without extra resources, using stochastic blocks and skip connections. In the training phase, sub-networks are generated by dropping stochastic blocks randomly to create a teacher network. This allows training the student network with multiple teacher networks, enhancing the student network's performance. The proposed structure improves student networks on benchmark datasets, addressing the challenge of limited memory and computational resources in embedded systems. In this work, the focus is on making a single teacher network act as multiple teacher networks by incorporating stochastic blocks and skip connections. This approach aims to enhance student networks' performance without the resource burden of using multiple teacher networks. In this work, stochastic blocks and skip connections are added to a teacher network to simulate multiple teacher networks using the same resources. Stochastic blocks randomly drop in the training phase but maintain valid paths for reliable performance. Sub-networks are generated for each batch, acting as individual teacher networks for the student network throughout training. In this study, sub-networks are generated by dropping blocks from a network with a proposed structure, resulting in multiple teacher networks providing different knowledge to a student network. The student networks trained with this method outperform those trained with traditional knowledge transfer methods. Knowledge transfer methods (KD, AT, ML) have been applied over CIFAR-100 and tinyimageNet datasets. Recent studies have focused on transferring knowledge from teacher networks to student networks using softened outputs. This approach has gained attention for its effectiveness in improving student network performance. Knowledge transfer methods have been applied to improve student network performance by transferring knowledge from teacher networks using softened outputs. Various techniques such as using intermediate outputs, attention maps, flows calculated by Gram matrix, and mutual learning have been explored for effective knowledge transfer. A new paradigm of bidirectional knowledge transfer involves multiple teacher networks improving student networks. Dissimilarity between teacher networks provides extensive knowledge to enhance student networks. Research shows that neural networks can be further improved with the help of multiple networks for vision tasks. Multiple teacher networks are more helpful than a single teacher in speech recognition. Deploying these methods is demanding due to additional resources. In the proposed structure, multiple valid paths are generated to transfer reliable and diverse outputs to the student network for flexible knowledge. In reinforcement learning, encouraging high entropy output distribution improves exploration and prevents early convergence for enhanced performance. Penalizing confident outputs and smoothing labels are also strategies used in the process. In the context of deep neural network training, penalizing confident outputs and smoothing labels have been proven effective strategies. Regularizing high confident outputs prevents over-fitting and enhances network adaptivity. Individual networks with higher entropy outputs are better teachers for student networks than ensemble networks. To mimic the effect of multiple teacher networks, stochastic blocks and skip connections can be added to a single teacher network. In the context of deep neural network training, penalizing confident outputs and smoothing labels have been effective strategies. To mimic the effect of multiple teacher networks, stochastic blocks and skip connections can be added to a single teacher network. Multiple sub-networks can serve as multiple teacher networks, utilizing skip connections to prevent vanishing gradient problems and enabling deeper network training. Residual networks with skip connections can be viewed as ensembles of multiple paths of different lengths. In residual networks, changing the structure or deleting blocks does not significantly harm performance. Even if blocks are dropped, valid paths still exist due to skip connections, generating multiple neural networks with adequate performance. This stochastic approach in training allows for the creation of multiple sub-networks with skip connections to prevent vanishing gradient problems and enable deeper network training. In training neural networks, a linear decay mode is used to set the survival probability of each block, with a trade-off between quantity and quality of sub-networks. The optimal survival probability varies for teacher and student pairs, with a range of [0.5, 0.9] tested to improve student networks. Sub-networks can act as teacher networks to provide independent knowledge for student networks. Residual networks have shown competent performance with dropped blocks. CNNs based on residual networks may exhibit similar characteristics. The accuracy of pre-trained networks is tested by dropping each block for different network types. Figure 2 displays the accuracy results for residual network of 32 layers, mobile network, and wide residual network 28-10. In Figure 2, sto networks are stronger against dropping blocks than basic networks. Teacher networks are pre-trained with the proposed structure to observe the impact of fatal blocks. Sub-networks sometimes predict correctly with high entropy outputs, making it easier for student networks to learn. Regularizing neural networks to be less confident improves performance. Similar results are observed in deep mutual learning. Using an ensemble of n networks as a teacher in deep mutual learning is less helpful than using n individual networks as teachers. The ensemble results in low entropy outputs, reducing the salient cues for knowledge transfer. Dropping blocks of the teacher network is akin to using individual networks, allowing sub-networks to provide rich knowledge to student networks. The original network's knowledge is fully utilized in generating sub-networks that share parts of the original network but offer different knowledge. The difference in knowledge is similar to that of individual neural networks. In deep mutual learning, the teacher and student networks exchange knowledge. The teacher network is modified for distillation techniques like KD and AT. Both networks need to be pre-trained for mutual learning to work effectively. In deep mutual learning, the teacher and student networks exchange knowledge through distillation techniques like KD and AT. However, when both networks are not pre-trained, the student network may receive random knowledge from randomized sub-networks of the teacher network, hindering its improvement. To address this, a single sub-network generated by stochastic drop is used as the teacher network for each batch in our simulation. The proposed method is evaluated using CIFAR-100 and tiny imagenet datasets. The Tiny Imagenet dataset is a downscaled version of ImageNet, with 64x64 RGB images from 200 classes split into 100,000 train and 10,000 test images. Data augmentation is applied to CIFAR-100 with horizontal flips and random crops. Networks are trained for 200 epochs with batch size 128 and decreasing learning rate. For Tiny Imagenet, no augmentation is used, and networks are trained for 100 epochs with batch size 128 and decreasing learning rate. Stochastic gradient descent optimizer with momentum 0.9 is utilized. In the simulation, CNNs like wrn, resnet, mobilenet, and vgg net are used with a modified proposed structure for knowledge transfer methods on CIFAR-100. Results show that the proposed structure improves student network performance, with ResNet 32 showing over 5% accuracy improvement when trained with the proposed structure compared to pure WRN 28-10. Attention Transfer (AT) results also demonstrate the effectiveness of the proposed structure in improving performance. The proposed structure, utilizing residual networks and wide residual networks, improves student networks in knowledge transfer methods. Results show further improvement over the pure AT method, with both teacher and student networks benefiting. The simulation results on tiny imagenet demonstrate the effectiveness of the proposed structure in peer learning paradigm. The simulation results on tiny imagenet show the impact of knowledge transfer methods on student networks. Significant blocks in certain models affect network performance, leading to the exploration of using different sub-networks as teacher networks. In this study, the impact of knowledge transfer methods on student networks is explored using different sub-networks as teacher networks. Multiple teacher networks are proposed to improve a student network without additional resources, leading to better performance compared to conventional transfer methods. The proposed structure can be easily applied to other tasks such as segmentation or object detection."
}