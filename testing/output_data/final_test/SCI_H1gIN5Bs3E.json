{
    "title": "H1gIN5Bs3E",
    "content": "Multi-output learning allows for considering similarities between tasks using regularizers. A generalization to a continuum of tasks is proposed using vector-valued RKHSs. Hyperparameters in loss functions can capture aspects like outlier tolerance, smoothness, sparsity, and structured-sparsity inducing norms. Support vector machines, One-Class Support Vector Machine (OCSVM), confidence exemplified by Quantile Regression (QR), and importance of different decisions in Cost-Sensitive Classification (CSC) are key concepts in machine learning. Multi-Task Learning provides a way to benefit from relationships between tasks while preserving local properties of algorithms. Extending multi-task learning to handle any hyperparameter value is a natural progression. Takeuchi et al. (2013) extended multi-task learning by considering an infinite number of parametrized tasks. In 2013, Takeuchi et al. extended multi-task learning to handle an infinite number of parametrized tasks through Parametric Task Learning (PTL). They relaxed the affine model assumption on tasks and piecewise-linear assumption on loss, proposing Infinite Task Learning (ITL) using Vector-Valued Reproducing Kernel Hilbert Space (vv-RKHS). This approach moves from single parameterized tasks to ITL through multi-output learning. In the task, a random variable (X, Y) with joint distribution P(X,Y) is considered, along with n i.i.d. training samples. The goal is to estimate the minimizer of the expected risk over a hypothesis class H, by solving a regularized empirical risk minimization problem. Two examples are provided, including Quantile Regression where the hyperparameter \u03b8 is in the range (0, 1). In Quantile Regression, the goal is to predict the \u03b8-quantile of the output distribution. Density Level-Set Estimation involves separating outliers from inliers using techniques like OCSVM. The unsupervised learning problem is solved by minimizing a regularized empirical risk over parameters. The focus is not on a single hyperparameter value (\u03b8) and associated risk. In contrast to single hyperparameter values, Multi-Task Learning (MTL) considers joint solutions for multiple tasks by encoding similarities through a kernel on hyperparameters. Tasks share the same input space and training samples, with each task specified by a hyperparameter value. This approach, referred to as multi-output learning, aims to provide consistent solutions across tasks. Infinite Task Learning is a novel framework that aims to handle new tasks after the learning phase without being limited to predefined hyperparameter values. It involves learning a function-valued function to handle a continuum of tasks. Our framework generalizes the Parametric Task Learning approach by allowing a wider class of models and relaxing the hypothesis of piece-wise linearity of the loss function. It benefits from the functional point of view, allowing the design of new regularizers and the imposition of various constraints on a continuum of tasks. For example, ensuring the continuity of the function for similar tasks and imposing shape constraints in quantile regression. In the supervised setting, the integrated loss function is introduced for nested level sets. The true risk is minimized by searching for a pointwise minimizer in a rich enough space. The importance of the prediction at different hyperparameter values is encoded by a probability measure. In the context of nested level sets in the supervised setting, the integrated loss function minimizes the true risk by finding a pointwise minimizer in a rich space. The empirical counterpart involves a variety of penalty terms, with Reproducing Kernel Hilbert Spaces offering a flexible approach. This section focuses on solving the ITL problem with a vv-RKHS model family and various penalty examples, supported by representer theorems for computational tractability. In the context of nested level sets in the supervised setting, the integrated loss function minimizes the true risk by finding a pointwise minimizer in a rich space. The empirical counterpart involves penalty examples and representer theorems for computational tractability. Solving Eq. (6) can be challenging due to the integral over \u03b8, but Quasi Monte Carlo (QMC) methods offer efficient optimization over vv-RKHSs and enable generalization guarantees. In this work, the \u0398 \u2192 Y mapping is described by an RKHS H k\u0398 with a scalar-valued kernel k \u0398. The x \u2192 (hyperparameter \u2192 output) relation is modelled by the Vector-Valued Reproducing Kernel Hilbert Space using the decomposable Operator-Valued Kernel. This kernel provides benefits and defines a function space with a known structure, allowing elements in H K to be considered as mappings from X to H k\u0398. There is an isometry between H K and H k X \u2297 H k\u0398. The isometry between H K and H k X \u2297 H k\u0398 allows for flexibility in analysis and design of penalization schemes. Regularizers like Ridge Penalty and L 2,1 -penalty are discussed for different tasks, with H K constraining solutions within a finite radius in the vv-RKHS. The L 2,1 -penalty is more suitable for DLSE, preserving the \u03b8-property. Shape constraints ensure monotonicity in QR, with penalization for negative derivatives. Representer Theorems offer flexibility in regularizer design. The advantage of using vv-RKHS as a hypothesis class is the finite-dimensional representation of the ITL solution under mild conditions. The Representer Theorem states that under certain conditions, there is a unique solution for the minimization problem in DLSE. This approach is related to Joint Quantile Regression but allows for predicting quantile values at any point between 0 and 1. In contrast to Joint Quantile Regression, \u221e-QR allows predicting quantile values at any point between 0 and 1, even outside the range used for learning. The approach specializes to q-OCSVM by choosing a constant kernel. Operator-Valued Kernels have also been used in functional outputs, providing an exact finite representation of the solution. The decomposable kernel choice simplifies the prediction complexity for multiple quantiles. Excess Risk Bounds: Generalization error analysis for QR with Ridge regularization and without shape constraints is extended to Infinite-Task Learning. The interplay between training samples (n) and integral locations (m) in the QMC scheme drives excess risk to zero, with \u03bb = \u03bb(n, m). The efficiency of the ITL scheme for QR has been tested on various benchmarks, with results summarized in Table S.1 for 20 real datasets from the UCI repository. Additional experiments on the non-crossing property on a synthetic dataset are shown in Fig. S. The expression of the pinball loss and a proposition regarding random variables and conditional quantile functions are discussed. The efficiency of the ITL scheme for Quantile Regression (QR) has been tested on various benchmarks, with results summarized in Table S.1 for 20 real datasets from the UCI repository. Additional experiments on the non-crossing property on a synthetic dataset are shown in Fig. S. The technique used for solving non-smooth optimization problems associated with QR, DLSE, and CSC tasks involves the L-BFGS-B optimization scheme, known for its efficiency in large-scale learning. The technique used for solving non-smooth optimization problems associated with Quantile Regression, DLSE, and CSC tasks involves the L-BFGS-B optimization scheme. An infimal convolution is applied to non-differentiable terms of the objective function. The complexity per L-BFGS-B iteration is O(n 2 \u221a n), assuming m = O( \u221a n). The efficiency of the non-crossing penalty is demonstrated on a synthetic dataset with 40 points. The efficiency of the proposed scheme in quantile regression was evaluated through an experimental protocol involving dataset splitting, hyperparameter optimization, and regression learning. Results were compared with two other methods, JQR and IND-QR, through 20 simulations and statistical testing. The \u221e-QR method was shown to estimate the entire quantile function, offering a more comprehensive solution. The \u221e-QR method estimates the whole quantile function, solving a more challenging task. Performance in terms of pinball loss is comparable to JQR on most benchmarks, except for 'crabs' and 'cpus' datasets. \u221e-QR outperforms IND-QR on eleven datasets and JQR on two datasets, showing the efficiency of the constraint based on the continuum scheme. Generalization error analysis will be done using uniform stability, with a reference to vv-RKHS for generalization bounds. The study focuses on the stability of an algorithm that returns a specific output given a dataset. The algorithm uses losses defined for inputs in the output space of the vv-RKHS. Assumptions are listed for the theorem used, and the OVK shape used is described. Assumptions 1 and 2 are satisfied for the chosen kernel, and assumption 3 involves the application being \u03c3-admissible. The application (y, h, x) \u2192 (y, h, x) is \u03c3-admissible, convex with respect to f, and Lipschitz continuous with Lipschitz constant \u03c3. A learning algorithm is \u03b2-uniformly stable with respect to the loss function if it satisfies certain conditions. Quantile Regression is discussed in this context. The loss function for Quantile Regression is \u03c3-admissible with \u03c3 = 2\u03ba\u0398. The hypothesis is verified for y \u2208 [\u2212B, B]. For a training set S = ((x1, y1), ..., (xn, yn)) and \u03bb > 0, it holds that h * S is the output of the algorithm. Assumption 4 is satisfied for \u03be = 2B + \u03baX\u03ba\u0398B\u03bb. The QR learning algorithm defined in Eq. (9) satisfies Assumption 4 for \u03be = 2B + \u03baX\u03ba\u0398B\u03bb. With probability at least 1 - \u03b4, it holds for all n \u2265 1 on the drawing of samples. The Hardy-Krause variation of a function f is defined as the quantity sup. The function's importance lies in the Quasi Monte-Carlo setting, where Proposition 3.3's bound is relevant only if f has finite Hardy-Krause variation. Lemma S.5.7 states assumptions on scalar kernels k X and k \u0398, with a focus on bounding the integral supremum. The function's importance lies in the Quasi Monte-Carlo setting, where Proposition 3.3's bound is relevant only if f has finite Hardy-Krause variation. Lemma S.5.7 states assumptions on scalar kernels k X and k \u0398, with a focus on bounding the integral supremum. Moreover, in the subdivision considered, all zeros of the residuals y \u2212 h * S (x)(\u03b8) are present, ensuring y \u2212 h * S (x)(\u03b8 i+1 ) and y \u2212 h * S (x)(\u03b8 i ) are always of the same sign. The calculation involves studying the residuals and their possible outcomes, with bounded partial derivatives and conditions for different scenarios. In the Quasi Monte-Carlo setting, Proposition 3.3's bound is relevant for functions with finite Hardy-Krause variation. Assumptions on scalar kernels are outlined in Lemma S.5.7, focusing on bounding the integral supremum. The residuals y \u2212 h * S (x)(\u03b8 i+1 ) and y \u2212 h * S (x)(\u03b8 i ) are always of the same sign in the considered subdivision. The calculation involves studying residuals, bounded partial derivatives, and conditions for different scenarios, leading to an asymptotic behavior as n, m \u2192 \u221e. The asymptotic behavior of the estimated model by \u221e-OCSVM is assessed using the \u03b8-property. Raw inputs were used for the datasets (Wilt, Spambase) without preprocessing. The input kernel was the exponentiated \u03c7 2 kernel with a Gaussian kernel chosen for k \u0398. The efficiency of the proposed continuum approach for density level-set is illustrated in FIG6. The efficiency of the proposed continuum approach for density level-set estimation is illustrated in FIG6."
}