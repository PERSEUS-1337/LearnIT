{
    "title": "Hyee-0VFPH",
    "content": "We introduce a novel algorithm, Stochastic Geodesic Optimization (SGeO), inspired by a geodesic optimization approach. SGeO incorporates an adaptive coefficient with Polyak's Heavy Ball method to control parameter updates based on optimization path changes. Experimental results demonstrate SGeO outperforms first-order methods on convex functions and deep Autoencoder benchmarks, competing with the second-order method K-FAC. Additionally, integrating Nesterov style lookahead gradient (SGeO-N) shows significant improvements. Gradient Descent with Momentum and its variants are popular for optimizing neural networks, despite the existence of second-order methods like Hessian-Free optimization and Natural Gradients. Nesterov's accelerated gradient has been effective in deep neural network optimization, but may not perform optimally for strongly convex functions. The search for optimization methods combining the efficiency of first-order methods and the effectiveness of second-order updates continues. In this work, an adaptive coefficient for the momentum term in the Heavy Ball method is introduced to combine first-order and second-order methods. The algorithm, Geodesic Optimization (GeO) and Stochastic Geodesic Optimization (SGeO), effectively weights the momentum term based on the change in direction on the loss surface. This approach provides implicit local curvature information without the need for expensive second-order information. Experiments demonstrate the effectiveness of the adaptive coefficient on both strongly-convex functions with Lipschitz gradients and general non-convex problems, such as deep Autoencoders. GeO can significantly speed up optimization processes. In this paper, Geodesic Optimization (GeO) and Stochastic Geodesic Optimization (SGeO) are introduced as algorithms that combine first-order and second-order methods to optimize deep Autoencoders. SGeO is shown to effectively handle ill-conditioned curvature and reach lower reconstruction error compared to first-order methods like Heavy Ball and Nesterov. Additionally, SGeO consumes less memory than second-order methods like K-FAC. The paper also discusses the adaptive coefficient designed for strongly-convex problems and its modification for general non-convex cases. Related work in the literature is also reviewed. The algorithm Geodesic Optimization (GeO) and Stochastic Geodesic Optimization (SGeO) are introduced to optimize deep Autoencoders by combining first-order and second-order methods. The approach involves following geodesics on the loss surface guided by the gradient, approximating the geodesic equation iteratively using a quadratic. The conjugate gradient method is seen as a momentum method with a learning rate and momentum parameter. The Hessian calculation is avoided due to its high computational cost. The Hessian calculation is avoided by using a line search to determine the step size in optimization algorithms. Various approximations to the parameter \u03b3 have been proposed, with an adaptive coefficient incorporating a notion of direction change in the algorithm's path. The adaptive coefficient \u03b3 C t is proposed to be applied to the Heavy Ball method as a momentum term in optimization algorithms. It reinforces the effect of previous updates when directions align and decreases when they don't, with 0 \u2264 \u03b3 C t \u2264 2. This coefficient is based on implicit second-order information and is particularly effective in strongly-convex functions with Lipschitz gradients. GeO (Geodesic Optimization) is an algorithm that incorporates an adaptive coefficient \u03b3 for strongly convex and Lipschitz functions. Nesterov's momentum can be added to GeO by modifying the gradient calculation. However, this approach may not work well for non-convex functions like those in neural network optimization. The algorithm SGeO (Stochastic Geodesic Optimization) proposes altering the adaptive coefficient for non-convex functions in neural network optimization. It reinforces the previous direction to avoid sudden changes in gradient, using minibatches to increase efficiency. The algorithm SGeO focuses on stabilizing updates in non-convex optimization by emphasizing unit vectors for the gradient and previous updates. It integrates Nesterov's lookahead gradient and discusses related work on large-scale optimization techniques for neural networks. The optimization techniques discussed include Duchi et al.'s method that extends gradient descent, Adadelta, RMSprop, Adam, Adaptive Restart, AggMo, and AMSGrad. These methods improve upon previous algorithms by adjusting the learning rate, keeping track of past gradients, resetting momentum, and damping oscillations. They are orthogonal to the SGeO algorithm's focus on stabilizing updates in non-convex optimization. Recent works have focused on accelerating gradient descent methods, with proposals such as adaptive methods to improve Nesterov's algorithm convergence rate for strongly convex functions. Differential equations have been used to model Nesterov, with suggestions that all accelerated methods have a continuous time equivalent defined by a Lagrangian functional. In a recent work, Defazio (2018) proposes a differential geometric interpretation of Nesterov's method for strongly-convex functions with links to continuous time differential equations. Second-order methods like Hessian-Free optimization and the natural gradient method reformulate gradient descent using concepts in differential geometry. K-FAC approximates the Fisher. SGeO is a method that derives curvature information implicitly using the change in direction, unlike K-FAC which approximates the Fisher information matrix explicitly. It was evaluated on strongly convex functions with Lipschitz gradients and benchmarked against Heavy-Ball and Nesterov's algorithms. The problems used were Anisotropic Bowl, Ridge Regression, and Smooth-BPDN, with different learning rates and momentum parameters set based on Lipschitz and strong-convexity parameters. The adaptive parameter \u03b3 t for Fletcher-Reeves is set to \u03b3 and for GeO and GeO-N is \u03b3 C t = 1 \u2212\u1e21 t \u00b7d t. The functionspecific parameter \u03b1 is set to 1, 0.5 and 0.9 in that order for the following problems. Anisotropic Bowl is a bowl-shaped function with Lipschitz continuous gradients. GeO-N and GeO converge in 82 and 205 iterations, while Heavy-Ball and Fletcher-Reeves take around 2500 and 3000 iterations. The Ridge Regression problem involves a linear least squares function with Tikhonov regularization. The measurement matrix A, response vector b, and ridge parameter \u03b3 are defined. The function f(\u03b8) is a positive definite quadratic function with unique solutions. Results show that Fletcher-Reeves outperforms other methods in terms of convergence, except for gradient descent. Smooth-BPDN is a smooth and strongly convex version of the BPDN problem, solved using Nesterov's method with a tolerance set to f(\u03b8) - f*N < 10^-12. GeO-N and GeO converge in 308 and 414 iterations, outperforming other methods, with Fletcher-Reeves closest at 569 iterations. SGeO is evaluated on benchmark deep autoencoder problems using MNIST, FACES, and CURVES datasets. The study evaluated different optimization algorithms for deep autoencoder problems using benchmark datasets. Baselines included SGD-HB, SGD-N, and K-FAC, while SGeO was also tested. Results were presented in Figures 4 to 6, focusing on training error rather than generalization. The algorithms were implemented using MATLAB on a single machine with specific hardware specifications. The study compared optimization algorithms for deep autoencoder problems, focusing on training error over generalization. All methods used the same parameter initialization scheme. SGeO utilized a fixed momentum parameter and a multiplicative schedule for the learning rate. The decay parameter (\u03b2) was set to 0.95 and a search was done for the momentum parameter \u00b5 in {0.999, 0.995, 0.99}. The minibatch size was 500 for all methods except K-FAC, which used an exponentially increasing schedule. The Blk-Tri-Diag version of K-FAC was used without iterate averaging, achieving results comparable to SGeO-N in terms of reconstruction error. SGeO outperforms SGD with Nesterov's momentum and K-FAC in MNIST and FACES experiments. Nesterov style lookahead gradient accelerates training for MNIST and CURVES datasets. SGeO and SGeO-N achieve low error rates in MNIST after 900 seconds of training. A novel algorithm based on adaptive coefficients for the Heavy Ball method is proposed. SGeO outperforms first-order methods like SGD with Nesterov's momentum and shows comparable performance to second-order methods like K-FAC. It has minimal computational overhead and opens new directions in high-dimensional optimization research, particularly in neural network optimization. Future work will focus on analyzing its theoretical properties and applying it to other machine learning paradigms. The per-iteration results for autoencoder experiments are shown in Figures 7 to 9, comparing to K-FAC. Generalization experiments on the test set are included, with SGeO-N showing better predictive performance on the CURVES dataset. The focus is on optimization rather than generalization, with algorithms tuned for training set performance. The algorithms are tuned for best performance on the training set, with overfitting addressed through regularization and small validation sets. The adaptive coefficient behavior is illustrated in Figures 13 (a) and (b), showing its usage in the optimization process. The values of the adaptive coefficient during optimization are included from experiments. The scaled Goldstein-Price function with multiple local minima and plateaus is used to compare different optimization methods. Nesterov's method gets stuck at local minima, while Geodesic is able to escape and recover effectively. Geodesic-N shows smoother paths due to lookahead gradient effects."
}