{
    "title": "SkluFgrFwH",
    "content": "Learning Mahalanobis metric spaces is a significant problem with various applications. Algorithms like ITML and LMNN have been developed for this purpose. The optimization problem aims to minimize violated similarity/dissimilarity constraints, with a FPTAS solution for fixed ambient dimensions. The algorithm is parallelizable, robust against noise, and shows promising results on synthetic and real-world datasets. Metric learning is a fundamental computational primitive with various applications and has received significant attention in the literature. The input consists of objects with pairwise similarity and dissimilarity constraints. The goal is to find a mapping into a target metric space that satisfies the constraints. In Mahalanobis metric learning, the input space is in R^d and the algorithm aims to minimize violated constraints. The text discusses Mahalanobis metric learning in the context of optimizing a mapping in a target metric space. The goal is to find a matrix that minimizes violated constraints, maximizing mapping accuracy. An approximation algorithm is developed for this optimization problem, running in near-linear time for any fixed ambient dimension. The algorithm utilizes tools from geometric approximation algorithms and linear programming theory in small dimensions. The text discusses a randomized algorithm for learning d-dimensional Mahalanobis metric spaces, with the ability to compute mappings with high accuracy in near-linear time. The algorithm can handle various forms of regularization and has shown significant performance improvements in practice. It is evaluated on synthetic and real-world datasets, comparing favorably against existing algorithms. The text introduces a novel algorithm for learning Mahalanobis metric spaces efficiently. It aims to minimize violated pairwise distance constraints directly, without using convex relaxation. The algorithm's details, practical improvements, and proof of Theorem 1.1 are discussed in subsequent sections. In Section 4, an approximation scheme for Mahalanobis metric learning in d-dimensional Euclidean space with nearly-linear running time is presented. Linear metric learning is shown to be an LP-type problem, defined by constraints and optimal solutions. Monotonicity and locality axioms characterize an LP-type problem on a set of constraints. Learning Mahalanobis metric spaces can be expressed as an LP-type problem, defined by constraints and optimal solutions. The problem involves basis computation, violation testing, and initial basis computation. Constraints are defined in a positive semidefinite matrix, with H representing a set of constraints. The learning process involves rewriting equations and defining constraints for each element in H. The problem involves defining constraints in a positive semidefinite matrix for a set of constraints H. Feasible solutions can be associated with matrices satisfying the constraints. Choosing a vector w and a random vector r defines an LP-type problem with combinatorial dimension Opd 2 q. The assertion holds with high probability if each ri is chosen with precision of \u2126plog nq bits. The locality axiom (A2) holds with high probability. The set A_F has at most n facets and there are at least two distinct infimizers for wpA_Gq when A_G is considered. The probability that r is orthogonal to any single hyperplane is at most 2\u00b4c log n \" n\u00b4c. The combinatorial dimension, \u03ba, is bounded by Opd 2 q constraints, as each optimal solution can be uniquely specified by at most Opd 2 q constraints. This is achieved by specifying ellipsoids E_A for each set of constraints F \u010e H, where each ellipsoid is uniquely determined by pd`3qd{2 distinct points on its boundary. The combinatorial dimension is bounded by Opd 2 q constraints, with each optimal solution specified by at most Opd 2 q constraints. The basis computation step involves iteratively removing constraints that do not decrease the optimum cost until a minimal set is obtained. The algorithm uses an exact algorithm for the special case and presents an approximation scheme based on Mahalanobis metric learning as an LP-type problem. Welzl (1991) developed a randomized linear-time algorithm for minimum enclosing ball and ellipsoid problems, extending to general LP-type problems. This algorithm applies to Mahalanobis metric learning as an LP-type problem, resulting in a linear time algorithm for R^d. The procedure Exact-LPTMLpF;Hq in Algorithm 1 implements this on a set of constraints F\u2286H, with the procedure LPTMLpF;Bq handling constraints F, B\u2286H to output a solution A\u2286R^d\u02c6d. The procedure Basic-LPTMLpBq computes LPTMLpH; Bq with a cardinality of B at most the combinatorial dimension Opd 2 q. This can be implemented using one initial basis computation (B0) and Opd 2 q basis computations (B2), taking total time d Op1q. An exact algorithm for Mahalanobis metric learning is known to lead to a nearly-linear-time approximation scheme for LP-type problems, as detailed in Har-Peled (2011). The algorithm outputs a basis that violates at most p1`\u03b5qk constraints in F with high probability. For Mahalanobis metric learning, Algorithm 2 provides an approximation guarantee. The proof of the main result follows from Lemmas 2.2 and 2.3. Algorithm 2, LPTML(F), is an approximation algorithm for Mahalanobis metric learning. The LP-type algorithm described in the previous section can be extended to handle regularization on the matrix A. By introducing a regularizing term in the objective function, the algorithm can minimize the total number of constraints violated by A. A regularized version of Mahalanobis metric learning aims to minimize the objective function cost 1 pAq plus a regularization term costpAq`\u03b7\u00a8regpAq, for some \u03b7 \u0105 0. The LP-type algorithm can be extended to handle regularization on matrix A by minimizing the objective function cost 1 pAq plus a regularization term costpAq`\u03b7\u00a8regpAq, for some \u03b7 \u0105 0. The regularizer regpAq can be expressed as a linear function on the entries of A, such as trpAq, with polynomially bounded coefficients. The randomized algorithm for learning d-dimensional Mahalanobis metric spaces can compute a solution A with cost 1 pAq \u010f p1`\u03b5qc\u02da in time d Op1q nplog n{\u03b5q Opdq, with high probability. The regularizer regpAq in the LP-type algorithm for matrix A can be polynomially bounded, allowing for the omission of certain terms without affecting the result. By introducing constraints and heuristics, the algorithm's performance in practical scenarios has been significantly improved. The algorithm's performance is significantly improved by using heuristics such as move-to-front and pivoting, which have been previously used in algorithms for linear programming and minimum enclosing ball. The move-to-front heuristic reorganizes an ordered list of constraints during the algorithm's run, while the pivoting heuristic selects the constraint that is \"violated the most\" to add to the basis. The algorithm utilizes heuristics like move-to-front and pivoting to enhance performance. It involves approximate counting for dissimilarity constraints and parallelization by distributing sub-problems to different machines. Early termination is used to achieve high accuracy with fewer iterations. The algorithm in Algorithm 2 can be parallelized by distributing sub-problems to different machines. Practical improvements have been implemented and experiments have been conducted on synthetic and real-world data sets. The experimental setting and main findings are discussed, focusing on a classification task involving labeled points in R d with similarity and dissimilarity constraints. Various algorithms are used to learn a Mahalanobis metric for the input point set. The implementation and documentation can be found in supplementary material 1. The algorithm in Algorithm 2 can be parallelized by distributing sub-problems to different machines. Practical improvements have been implemented and experiments have been conducted on synthetic and real-world data sets, including Iris, Wine, Ionosphere, and Soybean datasets from the UCI Machine Learning Repository. A synthetic dataset was also used, involving sampling points from two Gaussians in R2 and applying a linear transformation. This transformation affected the accuracy of k-NN classification with k=4. The linear transformation reduced the accuracy of k-NN classification from 1 to 0.68. Synthetic data was modified with adversarial noise before the transformation. The algorithm was compared against ITML and LMNN, with Algorithm 2 minimizing pairwise distance constraints. The accuracy of k-NN classification was examined, and the performance of LPTML was compared to ITML and LMNN after 2000 iterations. LPTML was compared to ITML and LMNN after 2000 iterations. LPTML achieves comparable accuracy to ITML and LMNN on real-world and synthetic data sets. However, LPTML outperforms ITML and LMNN on the Synthetic + Adversarial Noise data set due to its resistance to noise-induced biases."
}