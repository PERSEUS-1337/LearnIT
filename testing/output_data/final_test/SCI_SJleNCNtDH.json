{
    "title": "SJleNCNtDH",
    "content": "We study intrinsic motivation as an exploration bias for reinforcement learning in sparse-reward synergistic tasks, where multiple agents must collaborate to achieve a goal. Our approach incentivizes agents to take actions that have unpredictable effects, either based on true states encountered or a dynamics model trained with the policy. This method is validated in robotic bimanual manipulation tasks with sparse rewards. Our approach in robotic bimanual manipulation tasks with sparse rewards yields more efficient learning than training with only sparse rewards or using surprise-based intrinsic motivation. Synergistic behavior is crucial in multi-agent environments like a team of robots playing soccer. Learning policies from sparse, binary rewards is challenging due to exploration being a bottleneck for positive reinforcement. Videos of the project are available on the webpage: https://sites.google.com/view/iclr2020-synergistic. In sparse-reward multi-agent environments, exploration is a significant challenge, especially for encouraging synergistic behavior. Reward shaping using intrinsic motivation has been proposed as a solution, showing improved performance in various domains. This approach incentivizes agents to take surprising actions, aiding in exploration. In this paper, an alternative strategy for encouraging synergistic behavior in multi-agent tasks is proposed. The method involves rewarding agents for joint actions that lead to different results compared to if those actions were done individually. For example, in the task of twisting open a water bottle, two agents working together achieve results that would not be possible if they acted alone. The approach incentivizes synergistic behavior by rewarding joint actions that lead to different outcomes than individual actions. For example, in the task of lifting a heavy bar, a composition of single-agent models predicts partial lifting, while a two-agent model predicts full lifting. Training the policy to prefer such actions biases towards synergistic behavior. The text discusses intrinsic motivation in incentivizing synergistic behavior by leveraging the difference between true and predicted action effects. It proposes two formulations, one based on individual-agent predicted effects and the other on joint and compositional prediction models. The latter formulation requires training a forward model but is analytically differentiable with respect to the action taken. The text explores leveraging the difference between true and predicted action effects for improved sample complexity in the policy gradient framework. It focuses on five simulated robotic tasks with sparse rewards, showing that shaping the reward via intrinsic motivation leads to more efficient learning compared to training with only the sparse reward signal. Intrinsic motivation based on prediction error yields more efficient learning than sparse reward training or shaping rewards via surprise. This approach encourages synergistic behavior in multi-agent reinforcement learning. In this paper, intrinsic motivation formulations for multi-agent synergistic tasks are proposed. Efficient exploration in multi-agent settings has been a focus, with strategies like lookahead-based exploration and social motivation being explored. Our work incentivizes synergy by rewarding agents for actions that impact the world in unique ways. Bimanual manipulation in robotics involves complex control strategies drawing on hybrid force-position theory. In this work, an exploration strategy based on intrinsic motivation is described to solve synergistic tasks like bimanual manipulation via model-free reinforcement learning. The goal is to enable learning in settings with sparse extrinsic rewards, addressing the exploration bottleneck in such scenarios. In the absence of extrinsic rewards, intrinsic rewards guide exploration towards \"interesting\" actions for synergistic task completion. Formulations in Sections 3.1 and 3.2 operationalize this insight, making learning more efficient by guiding exploration and allowing for analytical gradients computation. These formulations aid in efficiently learning task policies. In the absence of extrinsic rewards, intrinsic rewards guide exploration towards synergistic task completion. Formulations aid in efficiently learning task policies by biasing exploration with an intrinsic reward function. The proposed approach involves an intrinsic reward function that encourages synergistic actions among agents to facilitate more efficient learning. This concept can be extended to settings with multiple agents, aiming to incentivize actions that have a significant impact on the environment when agents collaborate. Leveraging the difference between outcomes of actions executed together versus sequentially as a reward signal is a key insight in this approach. The intrinsic reward function encourages synergistic actions among agents by using single-agent prediction models to measure prediction errors and pretraining agents with data of random interactions before placing them in multi-agent environments. The intrinsic reward function encourages synergistic actions among agents by using single-agent prediction models to measure prediction errors and pretraining agents with data of random interactions before placing them in multi-agent environments. While random interactions sufficed to learn prediction models, alternative single-agent exploration strategies could be used. The intrinsic reward for synergy requires observing outcomes of actions, unlike human judgment of synergy in tasks. The non-dependence of intrinsic reward on outcomes is scientifically interesting and practically desirable. The intrinsic reward function promotes synergistic actions among agents by utilizing single-agent prediction models to measure prediction errors. To enhance sample-efficient learning, a joint prediction model is trained to predict the next environment state based on the states and actions of both agents. This allows for the formulation of a second intrinsic reward function that rewards actions where the expected outcome differs from the outcome when the agents act together. The second formulation of intrinsic reward, r intrinsic 2 (s, a, \u00b7), is based on the disparity between joint and compositional models' predictions. This formulation is analytically differentiable with respect to the action executed, allowing for more informative gradient updates in the learning algorithm. In contrast to curiosity-based methods, which reward surprising behavior, this approach encourages agents to find new behaviors that differ from expected outcomes. Our focus is on synergistic multi-agent tasks with sparse rewards, aiming to bias exploration for sample-efficient learning. We use deterministic regressors for model-based reinforcement learning and train the joint prediction model f joint and task policy \u03c0 \u03b8 to maximize total shaped reward r full. Our focus is on synergistic multi-agent tasks with sparse rewards, aiming to bias exploration for sample-efficient learning. The reward r full is a combination of intrinsic and extrinsic rewards across an episode. We train the joint prediction model f joint and policy \u03c0 \u03b8 simultaneously using transition samples to ensure efficient learning. The second intrinsic reward formulation leverages differentiability with respect to actions to enhance policy gradient methods. The gradient for the expected reward over trajectories can be written as shown in Appendix B. In practice, the policy gradient algorithm is treated as a black box, with additional gradients added to those yielded by the algorithm. This approach may double-count certain gradients but has minimal impact on training. New intrinsic rewards are designed for use in multi-agent sparse-reward tasks, alongside extrinsic rewards. In multi-agent sparse-reward tasks, bimanual manipulation tasks and a multi-agent locomotion task are considered. The proposed formulations are compared to baselines without intrinsic rewards, with alternative intrinsic reward formulations, and ablations to understand different intrinsic reward formulations and the impact of partial differentiability. Tasks include bottle opening, ball pickup, corkscrew rotating, bar pickup, and ant push. All tasks require effective use of both agents and are simulated in MuJoCo. The tasks involve bimanual manipulation with two Sawyer arms and an object on a table. Tasks include bottle opening by rotating a bottle cap, and ball pickup. Sparse binary rewards are used to encourage synergistic behavior between agents. The tasks involve various bimanual manipulation challenges such as lifting a slippery ball, rotating a corkscrew, lifting a heavy bar, and moving a block with two ants. The objects' locations and sizes are varied across episodes to provide different levels of difficulty. The ants work together to push a block to prevent it from toppling over. Experimental results for a three-agent ant push are provided. The internal state of each agent includes joint positions, velocities, and object pose. A simple Euclidean metric is used for the state space. Forward models predict changes in the object's pose. A discrete library of skills with continuous parameters is provided for learning within the environments. The stochastic policy maps a state to a distribution over actions. The stochastic policy in our study maps states to distributions over skills for agents A and B, with means and variances for continuous parameters. Skills can be hand-designed or learned from demonstration. If a skill cannot be executed or if arms are about to collide, the reward is 0. See Appendix C for more details. The network architecture for bimanual manipulation tasks involves 4-layer fully connected neural networks with 64-unit hidden layers and ReLU activations. Training details include pretraining single-agent models on experience samples without extrinsic rewards and optimizing the joint model and policy concurrently. The joint model and policy are optimized concurrently, relying only on extrinsic rewards. Intrinsic motivation as surprise is not effective for synergistic tasks, as it encourages unpredictable actions. A trade-off coefficient of \u03bb = 10 is set, using stable baselines implementation of proximal policy optimization (PPO). The study compares different baselines for skill discovery in multi-agent tasks, including random policy, separate-agent surprise, extrinsic reward only, and non-synergistic surprise. The non-synergistic surprise baseline aims to optimize for extrinsic reward and joint surprise without explicitly encouraging multi-agent behavior. The study compares different baselines for skill discovery in multi-agent tasks, showing that synergistic intrinsic rewards improve sample efficiency. Separate-agent surprise policies struggle due to the need for coordination, while extrinsic reward only policies perform better with enough training samples. Pretraining the joint model does not significantly improve performance compared to the proposed method. Using synergistic intrinsic rewards accelerates learning and improves sample efficiency compared to non-synergistic rewards. Policies utilizing these rewards outperform baseline policies by encouraging synergistic behavior, leading to better task performance with fewer samples. Non-synergistic surprise models tend to exploit the joint model rather than behave synergistically, resulting in decreased performance when combined with extrinsic rewards. Our method's competitive advantage over baseline persists even with additional interactions for pretraining the joint prediction model without extrinsic rewards. Analytical gradients improve sample efficiency by changing the reward function and optimization method. The impact of changes in using r intrinsic 2 without analytical gradients is compared to previous results. Leveraging analytical gradients makes r intrinsic 2 more sample-efficient, leading to better optimization. Formulation of intrinsic motivation without extrinsic reward shows agents learning synergistically but not solving the task. Videos of results can be found on the project webpage. Additionally, a plot of policy performance versus different settings of \u03bb is provided in Appendix D. In this work, a formulation of intrinsic motivation encourages synergistic behavior for learning sparse-reward tasks like bimanual manipulation and multi-agent locomotion. Significant benefits were observed compared to non-synergistic forms of intrinsic motivation. The formulation focuses on actions that require collaboration between agents, and there is potential to extend this concept to encourage action sequences. Future work could explore learning a diverse set of policies to discover a broad range of synergistic skills. Expanding the domains to involve more complex tasks is also suggested. The training algorithm described in Section 3.3 focuses on optimizing the objective by writing it in a specific way. The algorithm involves trajectories up to a certain timestep, excluding the future actions. The inner expectation is dependent on the policy parameter \u03b8, representing the expected reward of the current state. The algorithm aims to improve state representations for more complex object types like deformable objects, which lack a natural notion of 6D pose. The algorithm focuses on optimizing the objective by defining the expected reward of the current state with respect to the policy parameter \u03b8. The inverse kinematics feasibility checks allow the system to rule out impossible end effector poses, leading to 0 reward. The action space in the ant push environment includes pre-training four skills for moving in different directions with continuous parameters. The policy architecture involves selecting movement direction and magnitude, with skills and parameters sampled from Gaussian distributions. An experiment was conducted to analyze the impact of the trade-off coefficient \u03bb on performance. The experiment studied the impact of the trade-off coefficient \u03bb on policy performance. When \u03bb = 0, agents act synergistically without solving the task. Results with \u03bb = 10 showed that high extrinsic rewards drive behavior towards task completion. The approach can be extended to more agents, like A, B, and C. The study focused on the importance of the ordering of single-agent forward models as the number of agents increases. Testing was done on a three-agent version of the ant push environment, showing promising results. Future work will explore scalability and the impact of increasing the number of agents on performance. In a three-agent version of the ant push environment, random actions rarely lead to success due to the low likelihood of finding a valid sequence of joint actions. Our proposed bias towards synergistic behavior serves as a useful form of intrinsic motivation for exploration in this environment."
}