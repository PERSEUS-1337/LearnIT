{
    "title": "H1gL-2A9Ym",
    "content": "Neural message passing algorithms for semi-supervised classification on graphs have achieved success by considering the relationship between graph convolutional networks (GCN) and PageRank. A new propagation scheme based on personalized PageRank, called personalized propagation of neural predictions (PPNP), and its fast approximation, APPNP, have been developed. These models outperform previous methods, offering a large, adjustable neighborhood for classification and easy integration with any neural network. Graphs are widely used in various real-world applications and are described through scientific models. Deep learning approaches have been successful in solving important graph problems such as link prediction, graph classification, and semi-supervised node classification. Different methods leverage deep learning algorithms on graphs, including node embedding methods and approaches that use both graph structure and node features. Several deep learning approaches leverage graph structure and node features, including spectral graph convolutional neural networks, message passing algorithms, and neighbor aggregation via recurrent neural networks. Message passing algorithms have gained attention for their flexibility and performance, with improvements such as attention mechanisms, random walks, and edge features. However, these methods only utilize limited neighborhood information for each node, limiting the model's understanding, especially for nodes on the periphery. Increasing the neighborhood size for algorithms like Graph Convolutional Networks (GCN) can lead to oversmoothing, affecting performance for nodes on the periphery. GCN converges to a random walk's limit distribution with more layers, which doesn't consider the starting node's neighborhood. This results in deteriorating performance for a high number of layers. To address this issue, a solution is proposed. The paper proposes an algorithm using personalized PageRank for Graph Convolutional Networks to prevent oversmoothing with a high number of layers. It introduces a teleportation scheme to balance locality and information leverage, allowing for infinitely many propagation steps without oversmoothing. The algorithm separates the neural network from the propagation scheme, improving performance for nodes on the periphery. The proposed algorithm separates the neural network from the propagation scheme, allowing for a higher range without changing the network. It enables independent development of the propagation algorithm and neural network for predictions. Adding the propagation scheme during inference improves network accuracy, requiring fewer parameters and less training time compared to other models. The model achieves state-of-the-art results with linear computational complexity in the number of edges. The study introduces a message passing model using graphs with text-based features. It defines the graph G = (V, E) with nodes V, edges E, feature matrix X, class matrix Y, and adjacency matrix A. The Graph Convolutional Network (GCN) is a widely used algorithm for semi-supervised classification. The model separates the neural network from the propagation scheme, achieving state-of-the-art results with linear computational complexity. The study introduces a message passing model using graphs with text-based features. It defines the graph G = (V, E) with nodes V, edges E, feature matrix X, class matrix Y, and adjacency matrix A. The Graph Convolutional Network (GCN) is a widely used algorithm for semi-supervised classification. The model separates the neural network from the propagation scheme, achieving state-of-the-art results with linear computational complexity. The symmetrically normalized adjacency matrix with self-loops, diagonal degree matrix Dij = k\u00c3 ik \u03b4ij, and trainable weight matrices W0 and W1 are used in two GCN-layers considering only neighbors in the two-hop neighborhood. A message passing algorithm like GCN cannot be trivially expanded to use a larger neighborhood due to oversmoothing and increased depth and number of learnable parameters. The study focuses on the influence score of nodes in a k-layer GCN, showing that information spreads in a random walk-like manner. By considering a modified k-step random walk distribution, the model achieves personalized propagation of neural predictions. The convergence of the random walk probability distribution to the limit distribution is discussed in the context of graph properties. The text discusses how the convergence of the random walk probability distribution to the limit distribution can be achieved by solving an equation. This global property is independent of the random walk's starting node and is unsuitable for describing the root node's neighborhood. By connecting the limit distribution with PageRank, a personalized PageRank approach is proposed that considers the root node using a teleport vector. Our adaptation of personalized PageRank involves using a teleport vector to preserve the node's local neighborhood in the limit distribution. The influence score of a root node on another node is determined by the personalized PageRank, which can be adjusted by the teleport probability \u03b1. The fully personalized PageRank matrix is obtained by substituting the indicator vector with the unit matrix, allowing for the calculation of the influence score between nodes. Personalized propagation of neural predictions (PPNP) utilizes influence scores for semi-supervised classification by generating predictions for each node based on its features and propagating them via a personalized PageRank scheme. PPNP's model equation involves a feature matrix X and a neural network f \u03b8 generating predictions H \u2208 R n\u00d7c independently for each node, allowing for parallelization. This separates the neural network used for prediction from the propagation matrix used. PPNP separates the neural network used for generating predictions from the propagation scheme, allowing for flexibility in prediction methods. The model is trained end-to-end, with gradients flowing through the propagation scheme during backpropagation. This enables the use of deep convolutional neural networks for graph predictions. The model's accuracy is significantly improved by adding propagation effects in backpropagation. Directly calculating the fully personalized PageRank matrix is computationally inefficient, leading to high complexity and memory requirements. To address this, the equation can be viewed as a variant of topic-sensitive PageRank, allowing for an approximate computation of topic-sensitive PageRank in PPNP. The APPNP model achieves linear computational complexity by approximating topic-sensitive PageRank via power iteration, using a random walk with restarts. Each power iteration step is calculated using a specific formula, retaining graph sparsity without constructing a large matrix. The convergence of this iterative scheme can be demonstrated by analyzing the resulting series. This propagation scheme does not require additional parameters for training. The PPNP model does not need extra parameters for training, unlike models like GCN. It can propagate far with few parameters, showing significant benefits. The model uses fixed-point iterations and a predetermined iteration (adapted personalized PageRank) for propagation. The neighborhood influence on each node can be adjusted with the teleport probability \u03b1, allowing for model customization. The model allows for customization via the teleport probability \u03b1, adjusting neighborhood sizes for different network types. Various works have enhanced message passing algorithms with skip connections, but limitations persist in the number of layers used. Combining message passing with co-and self-training has shown improvements, offering a solution to the range problem. The model improves classification results by combining co-and self-training, similar to other semi-supervised models. It simplifies architecture by decoupling prediction and propagation, avoiding oversmoothing issues seen in deep GNNs. PPNP increases range without extra layers, making it easier and faster to train compared to deep GNNs. Our work focuses on establishing a thorough evaluation protocol for message-passing algorithms. We conduct experiments 100 times on random splits and initializations, ensuring data consistency between visible and test sets. This approach aims to address issues such as overfitting and hyperparameter tuning, which can significantly impact algorithm performance. The study focuses on evaluating GCN-like models rigorously by using consistent hyperparameters, conducting grid search for optimization, applying early stopping criterion, calculating confidence intervals via bootstrapping, and reporting p-values of a paired t-test. Four text-classification datasets are used for evaluation. The study evaluates GCN-like models using text-classification datasets such as CITESEER, CORA-ML, and PUBMED citation graphs. The graphs use a bag-of-words representation of paper abstracts as features. Larger graphs have average shortest path lengths between 5 and 10, making a regular two-layer GCN insufficient. Baseline models compared include GCN, N-GCN, and others. The study evaluates various GCN-like models on text-classification datasets using citation graphs with bag-of-words features. Models include GCN, N-GCN, GAT, bt. FP, and JK. Hyperparameters and regularization techniques are detailed in the study. The study evaluates GCN-like models on text-classification datasets using citation graphs with bag-of-words features. APPNP with \u03b1 = 0.1 and K = 10 power iteration steps is used, while \u03b1 = 0.2 is used on the MICROSOFT ACADEMIC graph. Results show significant improvements over baseline models on all datasets. Thorough setup reveals advantages of PPNP and APPNP when hyperparameters are optimized. The study evaluates GCN-like models on text-classification datasets using citation graphs with bag-of-words features. Optimized hyperparameters and multiple data splits are crucial for model performance. A statistically rigorous evaluation is necessary for conclusive model comparison. Training time per epoch is compared, showing differences in scalability and robustness among methods like PPNP, APPNP, and GAT. APPNP scales well to large data but is 25% slower than GCN due to more matrix multiplications. It performs significantly faster than GAT, even with improved training time. In sparsely labeled settings, PPNP and APPNP dominate due to better information propagation. The performance gap between APPNP and GCN tends to increase for nodes far away from the training set, benefiting from the increased range. The accuracy depends on the number of power iterations, with personalized propagation showing improved accuracy compared to standard propagation. The personalized propagation principle with a teleport probability \u03b1 \u2208 [0.05, 0.2] improves accuracy and convergence speed. Using a moderate number of power iterations (e.g. K = 10) approximates exact PPNP effectively. Adjusting the teleport probability for different datasets is crucial, with higher \u03b1 values enhancing convergence speed. The personalized propagation principle with a teleport probability \u03b1 improves accuracy and convergence speed. PPNP and APPNP are trained end-to-end, with the propagation scheme affecting the neural network f \u03b8 during training and the classification decision during inference. Investigating the model's performance without propagation shows the value of this addition. Different scenarios are explored, including training with APPNP, inference without APPNP, and using APPNP for both training and inference. The regular APPNP, which always uses propagation, achieves the best results. Skipping propagation during training can significantly reduce training time for large graphs. The model can be combined with pretrained neural networks to improve accuracy. Propagating during training can lead to large improvements, indicating applicability to online/inductive learning. Personalized propagation of neural predictions (PPNP) is introduced in this paper. In this paper, personalized propagation of neural predictions (PPNP) and its fast approximation, APPNP, are introduced. The model decouples prediction and propagation, solving the limited range problem in message passing models. It outperforms state-of-the-art methods for semi-supervised classification on multiple graphs. Future work could involve combining PPNP with more complex neural networks for applications in computer vision or natural language processing. The method involves personalized PageRank and its convergence properties. The matrix existence is determined by the determinant condition. The iterative equation for APPNP converges as the propagation steps increase. The sampling procedure involves splitting data into visible and test sets, with different label sets used in each experiment. 20 nodes per class are used for training, 500 nodes for early stopping, and the remaining nodes for validation or test sets. 20 random seeds are used for splits, fixed across runs for consistency. Each experiment is run with 5 random initializations on each data split, totaling 100 runs per experiment. The experiment involves running 100 runs with 5 random initializations on each data split. The early stopping criterion uses a patience of p = 100 and a maximum of n = 10,000 epochs. The parameter set with the highest accuracy is chosen, with ties broken by selecting the lowest loss. TensorFlow is used for all experiments except bootstrapped feature propagation. Adam optimizer with a learning rate of l = 0.01 and cross-entropy loss are used for all models. The optimized GCN model uses two layers with 64 hidden units, dropout on the adjacency matrix with 0.5 dropout rate, and L2 regularization parameter of 0.02. N-GCN utilizes 16 hidden units, 4 heads per random walk length, and random walks of up to 4 steps. GAT model uses original hyperparameters with L2 regularization parameter of 0.001 and learning rate of 0.01. The bootstrapped feature propagation model uses specific hyperparameters such as a return probability of \u03b1 = 0.2, 10 propagation steps, and 10 bootstrapping steps with r = 0.1n training nodes added per step. Training nodes are selected based on low entropy predictions and class proportions. The jumping knowledge networks employ three layers with h = 64 hidden units each, L2 regularization with \u03bb = 0.001, and dropout with d = 0.5 on all layers except the adjacency matrix. The model does not have stochasticity in its initialization and is run once per train/early stopping/test split. The average improvement in percentage points of APPNP over GCN depending on the distance from the training nodes on different graphs is shown in Figure 13. The y-axis scale varies per graph."
}