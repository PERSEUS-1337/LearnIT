{
    "title": "rJg76kStwH",
    "content": "Markov Logic Networks (MLNs) combine logic rules and probabilistic graphical models for knowledge graph problems. Graph Neural Networks (GNNs) are efficient for large-scale graph problems but lack explicit logic rule incorporation. ExpressGNN, a GNN variant, combines MLNs and GNNs for effective probabilistic logic reasoning. Knowledge graphs play a crucial role in various applications like question answering and information retrieval. To improve their quality and derive new facts, additional processing such as link prediction and record de-duplication is necessary. Markov Logic Networks (MLNs) combine logic rules and probabilistic graphical models for tasks on knowledge graphs, allowing for generalization with limited labeled data. However, inference in MLNs is computationally intensive. In this paper, the authors propose ExpressGNN, a variant of graph neural networks that can efficiently incorporate prior knowledge encoded in logic rules. This method aims to combine the strengths of MLNs and GNNs for effective data-driven learning. ExpressGNN is a variant of graph neural networks that efficiently incorporates logic rules for MLN inference. It enables efficient learning and inference on large knowledge graphs, combining logic rules and data supervision while maintaining a compact and expressive model. Logic rules are used for reasoning in knowledge graphs, but they struggle with uncertainty. Probabilistic graphical models like Markov Logic Networks have been proposed to combine relational and probabilistic approaches for effective reasoning tasks. MLNs handle noise in logic rules and knowledge graphs but face computational challenges due to exponential costs. Various works aim to enhance MLNs in accuracy and efficiency, yet scalability remains an issue. ExpressGNN framework addresses scalability by utilizing efficient stochastic training and compact parameterization. ExpressGNN is a framework that combines Graph Neural Networks (GNNs) with first-order logic rules to leverage domain knowledge, unlike existing data-driven approaches. It addresses scalability issues in handling noise in logic rules and knowledge graphs by using efficient stochastic training and compact parameterization. ExpressGNN combines Graph Neural Networks with first-order logic rules to leverage domain knowledge. It addresses scalability issues in handling noise in logic rules and knowledge graphs by using efficient stochastic training and compact parameterization. Knowledge graph embedding methods like TransE, NTN, DistMult, ComplEx, and RotatE design scoring functions for relational patterns but lack consistent probabilistic models. In contrast, ExpressGNN has a consistent probabilistic model and can incorporate knowledge from logic rules. Neural Network (pLogicNet) integrates knowledge graph embedding methods with MLNs using GNNs to capture structural knowledge. It can trade-off model compactness and expressiveness by tuning dimensionality. Knowledge graph consists of entities (constants) and relations (predicates) defined in first-order logic. In general, predicates have asymmetric arguments. Ground predicates are binary random variables used in MLN, with M d ways to ground a d-ary predicate. Each observed fact in knowledge bases is a truth value assigned to a ground predicate. Unobserved facts are treated as latent variables in the open-world paradigm. Knowledge bases are represented by bipartite structures. In a knowledge base represented by a bipartite graph, facts are latent variables connected to constants through edges. Markov Logic Networks use logic formulae to define potential functions in undirected graphical models, where predicates and assignments of constants play a crucial role. Markov Logic Networks (MLN) define a joint distribution over observed and unobserved facts using logic representations. Ground formulas have constants assigned to all arguments, with potential functions defined by formulas. The weight w f can be adjusted to incorporate more complex entities like images or texts. The weight w f in Markov Logic Networks (MLN) represents the confidence score of a formula. MLN graphs are denser than knowledge graphs, with higher-order polynomials in the number of entities. The variational EM framework for MLN inference and learning is introduced, utilizing ExpressGNN as a key component. Markov Logic Networks model the joint probabilistic distribution of observed and latent variables. Training involves maximizing the log-likelihood of observed facts, but direct maximization is intractable due to the need to compute the partition function and integrate over variables. The variational EM algorithm is used to optimize the variational evidence lower bound (ELBO) of the data log-likelihood. The E-step infers the posterior distribution of latent variables, minimizing KL divergence between Q \u03b8 (H|O) and P w (H|O). The M-step learns weights of logic formulae in MLN, optimizing P w to maximize data log-likelihood. Mean-field distribution approximates true posterior due to computational complexity of exact inference in MLN. To scale up graphical models like latent Dirichlet allocation for large text corpora, mean-field variational distribution independently infers unobserved predicates using factorized Bernoulli distributions. Deep learning models parameterize the variational posterior, with graph neural networks as the inference network for compact yet expressive approximations. The ELBO can be reorganized with a fixed w f in the E-step. The ELBO can be reorganized with a fixed w f in the E-step to address the challenge of intractable computational cost by sampling mini-batches of ground formulae to approximate exponential summations with a controllable number of terms. In each optimization iteration, a batch of ground formulae is sampled to compute terms in the objective function. The mean-field approximation decomposes the global expectation into local expectations. The second term involves computing a local sum of entropy using the posterior of latent variables. A supervised learning objective can be added for tasks with sufficient labeled data. The overall E-step objective function is enhanced with this addition. The E-step objective function combines logic rules and labeled data supervision. In the M-step, weights of logic formulae are learned in Markov Logic Networks. The partition function Z(w) is optimized using pseudo-log-likelihood to handle the intractable number of terms. In the E-step, the objective function combines logic rules and labeled data supervision. In the M-step, weights of logic formulae in Markov Logic Networks are optimized using gradient descent. The derivative is computed based on observed or latent facts, and gradients are efficiently computed on the Markov blanket of each variable. A different sampling scheme is used for computational efficiency, considering observed truth values and drawing samples from the variational posterior for latent variables. The ELBO of fully observed ground formulae depends on the formula weight, making it computationally intractable to use all possible ground predicates for gradient computation. In the E-step, the objective function combines logic rules and labeled data supervision. In the M-step, weights of logic formulae in Markov Logic Networks are optimized using gradient descent. To tackle the computational challenge, a small subset of ground predicates is considered, each capable of determining the truth value of a ground formula. This subset allows for efficient estimation of gradients with lower computational cost. In the neural variational EM framework, the inference network needs to be expressive and efficient in approximating the true posterior distribution. The inference network, ExpressGNN, utilizes graph neural networks with tunable embeddings to model the posterior and capture structure knowledge encoded in the knowledge graph. It consists of three parts: a vanilla GNN on the knowledge graph, tunable embeddings, and the use of embeddings to define the variational posterior. The GNN parameters are shared across the computational graph. ExpressGNN is a compact model with shared parameters for the entire graph, augmented with tunable embeddings for each entity. The tunable embeddings increase model expressiveness, with the number of parameters being O(kM). The model assigns similar embeddings to similar entities in the knowledge graph, while tunable embeddings provide additional capacity to encode entity-specific information. The total trainable parameters in ExpressGNN are O(d^2 + kM), allowing for a trade-off between model complexity. ExpressGNN combines GNN with tunable embeddings to balance model compactness and expressiveness. The model can save parameters by reducing k for large-scale problems. Theoretical analysis shows GNN's expressive power in mean-field inference, highlighting the benefit of combining GNN and tunable embeddings in ExpressGNN. Studies reveal GNN's limitations in distinguishing nodes, such as in graph isomorphism checks. In a knowledge graph, GNN fails to distinguish nodes with opposite relations, requiring more powerful variants like ExpressGNN for isomorphic node detection. ExpressGNN is a more efficient and memory-efficient variant of GNN, using tunable embeddings to enhance model compactness and expressiveness. It directly operates on the knowledge graph, making it suitable for industry-scale problems and capable of capturing structural knowledge. ExpressGNN is evaluated on benchmark datasets including UW-CSE, Cora, synthetic Kinship datasets, and FB15K-237. The experiments are conducted on a GPU-enabled Linux machine with Intel Xeon processors and 256GB RAM using PyTorch and Adam optimizer for training. ExpressGNN is trained with Adam optimizer and the same computational resources are allocated for all experiments. Default hyperparameters are used for competitor methods. Initial learning rate is set to 0.0005 and decayed by half every 10 epochs without validation loss improvement. A two-layer MLP with ReLU activation function is used for each embedding update step in the GNN model. Different MLP configurations are learned for each dataset. Hyperparameters are tuned using the original validation set or the smallest subset for each dataset. ExpressGNN configuration search includes embedding size, split point of tunable and GNN embeddings, update steps, and batch size. Weights are fixed as 1 for inference and initialized as 1 for learning experiments. \u03bb is set to 0 for inference and 1 for learning. Evaluation compares ExpressGNN with MLN methods on various datasets. Ablation study explores GNN and tunable embeddings trade-off. Experiment settings involve fixed hyperparameters and Adam optimizer training. The experiment settings involve fixed hyperparameters and Adam optimizer training. MLN can handle open-world settings in a probabilistic framework, unlike closed-world settings. Prediction tasks include deduplicating entities in Cora and predicting male or female in Kinship. The ExpressGNN-E method predicts the truth of query predicates with different entities. Inference accuracy results are reported on three benchmark datasets. ExpressGNN-E outperforms baseline methods on Cora and UW-CSE datasets, achieving good accuracy. The Kinship dataset, with increasing entities, shows consistent performance. The Kinship dataset shows increasing entities with linear growth on sets S1-S5. HL-MRF achieves perfect accuracy on S1-S4 but struggles with the largest set S5. ExpressGNN-E yields similar results, with inference efficiency demonstrated in Fig. 4. ExpressGNN-E outperforms baseline methods on UW-CSE and Kinship datasets, maintaining scalability as data size grows. Some baseline methods become infeasible for larger sets, while ExpressGNN-E shows consistent performance. ExpressGNN-E can trade-off model compactness and expressiveness by tuning GNN dimensionality and embeddings. Ablation study on Cora dataset shows GNN64+Tune4 performs comparably to Tune64 but better than GNN64. GNN64+Tune4 has fewer parameters to train when entities are large, aligning with theoretical analysis that GNN alone lacks expressiveness. ExpressGNN seeks a combination of two types of embeddings to balance model compactness and expressiveness. The best configuration varies based on the task goal. Evaluation on the FB15K-237 dataset shows promising results compared to state-of-the-art methods. Two versions of ExpressGNN are tested: ExpressGNN-E for inference-only and ExpressGNN-EM for inference-and-learning tasks. ExpressGNN-E and ExpressGNN-EM are evaluated on a prediction task to generate a rank list over possible instantiations of a relation. Evaluation metrics include Mean Reciprocal Ranks (MRR) and Hits@10. ExpressGNN is compared with state-of-the-art methods due to its scalability on the dataset. ExpressGNN is compared with various state-of-the-art methods for knowledge base completion, showing superior performance. ExpressGNN-EM achieves the best results by learning the weights of logic rules, outperforming all baseline methods including MLN. ExpressGNN outperforms baseline methods like TransE and RotatE by leveraging logic rules in addition to labeled data. It shows better performance with smaller training data, highlighting the benefit of using logic rules when data is limited for supervised learning. In practical scenarios, a large portion of relations in knowledge bases are long-tail, with few facts. A zero-shot learning dataset based on FB15K-237 was created to test model performance on relations with insufficient training data. Supervised relational learning methods perform poorly on sparse long-tail relations, while ExpressGNN, which combines logic rules and neural networks, shows better performance in zero-shot learning. ExpressGNN combines logic rules and neural networks for probabilistic logic reasoning, addressing scalability issues with efficient training. It leverages GNNs to capture structural knowledge in knowledge graphs, achieving better performance on long-tail relations. ExpressGNN improves model compactness and expressiveness by tuning the dimensionality of the GNN and the embedding part. Extensive experiments on benchmark datasets demonstrate its effectiveness. Examples show that GNN embeddings alone may not be expressive enough. Benchmark datasets used include UW-CSE and Cora for experiments. The Cora dataset contains citations to computer science research papers and is split into five subsets based on research fields. A synthetic dataset similar to the Kinship dataset was introduced, with controllable entities representing kinship relationships in the Alyawarra tribe. The generation of the synthetic dataset involves splitting entities into groups representing different generations and kinship relationships. The knowledge base completion benchmark FB15K-237 is a challenging variant of FB15K, constructed by removing near-duplicate and inverse relations. It is split into training/validation/testing sets, with statistics shown in Table 5. Logic formulae examples from four benchmark datasets are listed in Table 7. ExpressGNN is compared with five probabilistic inference methods in Sec. 6.1 under open-world conditions. In Sec. 6.1, ExpressGNN is compared with five probabilistic inference methods under open-world semantics, which is different from the closed-world setting typically used due to scalability issues. Results show a fair setup for competitor methods, with AUC-PR scores better under open-world setting. The original datasets were collected and evaluated under closed-world assumption, which may not hold true in real-world scenarios. In real-world scenarios, the closed-world assumption is unlikely to hold true for large-scale knowledge bases like Freebase and WordNet. Therefore, the open-world setting is more reasonable. Logic formulae are used in benchmark datasets, with hand-coded rules for UW-CSE, Cora, and Kinship, and candidate rules generated for FB15K-237 using Neural LP. In the open-world setting, logic formulae are used in benchmark datasets like UW-CSE, Cora, and Kinship. For FB15K-237, rules with high confidence scores are selected and de-duplicated to generate 509 logic formulae. Theorem states that two latent random variables have the same posterior distribution in any MLN if certain conditions are met. Logic formulas can be represented as factor graphs connecting constants to predicates or predicate negation. The logic formula factor graph connects constants to predicates or predicate negation. Each formula can be represented by a factor graph, with distinctive constants instantiated from templates. The MLN induced by a formula results in different posteriors for specific predicates. The construction of a factor graph for a formula is illustrated, showing the differentiation of edges based on argument position. The proof constructs a formula using G * c1:n and defines its value based on observed variables. If the MLN only contains this formula, nodes must be distinguishable due to their connections in the factor graph representation. The formula value is defined based on observed variables in the MLN. Eq. 10 defines a formula where constants can be replaced, while Eq. 11 represents a ground formula with specific arguments. There is no formula containing r(c1,...,cn) based on a certain condition."
}