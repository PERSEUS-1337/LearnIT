{
    "title": "BJxDNxSFDH",
    "content": "Recent few-shot learning algorithms have enabled models to quickly adapt to new tasks based on only a few training samples. In this paper, a few-shot meta-learning system focusing on regression tasks is proposed. The model reduces the degree of freedom of the unknown function by representing it as a linear combination of sparsifying basis functions. A Basis Function Learner network encodes basis functions for a task distribution, and a Weights Generator network generates the weight vector for a novel task. The model outperforms current state-of-the-art meta-learning methods in various regression tasks. Few-shot learning methods focus on quickly adapting to new tasks with minimal training examples. A proposed few-shot regression model aims to learn to learn by efficiently adapting to new tasks rather than just predicting outcomes for specific inputs. The model proposed is a few-shot learning model for regression tasks. It consists of a Basis Function Learner network and a Weights Generator network to estimate F(x) with few samples. The model is evaluated on various regression tasks including sinusoidal regression, 1D heat equation tasks, 2D Gaussian distribution tasks, and image completion on MNIST and CelebA datasets. The paper introduces a few-shot regression model using sparsifying basis functions learned from data. The model includes a Basis Function Learner network and a Weights Generator network for predicting regression tasks with limited data. Experiments were conducted on various tasks including sinusoidal regression, heat equation tasks, Gaussian distribution tasks, and image completion on MNIST and CelebA datasets. The paper introduces a few-shot regression model using sparsifying basis functions learned from data. Regression problems have been extensively studied in machine learning and signal processing. The model reformulates regression as a few-shot learning problem, enabling it to perform regressions on tasks sampled from the same distribution. Deep neural networks rely heavily on labeled data, but learning from limited labeled data is gaining attention. Meta learning is a prominent approach for this challenge. Meta learning, also known as learning to learn, focuses on developing an adaptive model for various tasks. It has shown promise in style transfer, visual navigation, and few-shot learning problems. One-shot classification was introduced by Lake et al. in 2011, leading to the creation of the Omniglot dataset for few-shot classification. Various meta learning methods have since emerged to tackle few-shot problems, with some learning a similarity metric between new test examples and few-shot training samples for predictions. Few-shot learning methods utilize different approaches such as optimization-based techniques and generative models to improve model performance in tasks with limited training data. Various methods like learning optimal model initialization, adapting model parameters effectively, and employing LSTM for optimization algorithms have been explored. Additionally, generative models have been proposed to overcome limitations in few-shot settings. Neural Processes algorithms model output distributions using Deep Neural Networks, similar to Variational Autoencoders. Unlike existing works, our model employs a deterministic approach by learning basis functions for output distribution modeling without latent vectors. Our model uses a sparse linear combination of basis functions for predictions in few-shot regression, outperforming Neural Processes. The approach is compared to dictionary learning but differs in its continuous nature and limited sample observations. In few-shot regression, the goal is to develop a model that can quickly regress to various equations with minimal training samples. Each regression task consists of training and validation samples, with a continuous distribution of tasks. The main idea is to model the unknown function y = F(x) with a small number of training samples, making it a challenging task. In few-shot regression, the task is to regress to various equations with minimal training samples. The unknown function F(x) is modeled with a sparse representation using a linear combination of basis functions. This approach aims to approximate F(x) with a few samples, making it a challenging yet important task. When modeling the unknown function F(x) as a sinusoid, using the Fourier basis allows for a sparse representation with only a few significant weights. This reduces the degree of freedom of F(x) and makes it possible to estimate F(x) accurately with a small number of samples. The approach involves learning a set of basis functions {\u03c6 i (x)} from training tasks to achieve sparse representation for any task. The Basis Function Learner Network encodes a set of functions {\u03c6 i (x)} for sparse representation in tasks drawn from p(T). A Weights Generator Network maps training samples to a constant vector w. The model predicts using w T \u03a6(x) and is trained with a loss function consisting of three terms. The loss function for training the model includes a mean-squared error term between validation set labels and predictions, along with penalty terms on the weight vectors to encourage sparsity and reduce variance. The full loss function incorporates L1 and L2 regularization terms, similar to Elastic Net Regression. The loss function for training the model includes penalty terms on weight vectors to encourage sparsity and reduce variance, incorporating L1 and L2 regularization terms like Elastic Net Regression. The model learns parameters for the Basis Function Learner network and Weight Generator network. In this section, experiments were conducted using different regression tasks to evaluate the method. The learning rate was set to 0.001, and the Adam Optimizer was used for optimization. The experiments were implemented using the Tensorflow library. Specific experiments on the 1D Heat Equation and 2D Gaussian regression tasks are detailed in the appendix. The Basis Function Learner for 1D Regression tasks consists of two fully connected layers with 40 neurons. For 1D Regression tasks, the Basis Function Learner has two fully connected layers with 40 hidden units. The model is evaluated on sinusoidal regression with parameters A, b, and \u03c9. Training is done with batch size 4 and 60000 iterations for 5, 10, and 20 shot cases. Comparison is made with other few-shot learning methods like Meta-SGD and MAML. In the experimental setup, the model is compared against various few-shot learning methods like Meta-SGD, MAML, and Neural Processes. Two variants of the model are presented differing in the size of the Weights Generator, with details on the architecture provided for each variant. The model is compared against various few-shot learning methods like Meta-SGD, MAML, and Neural Processes. Two variants of the model are presented with different sizes of the Weights Generator, each with specific architecture details. The \"large\" model has 5 fully connected layers with 40 hidden units each for fair comparison with BMAML and EMAML, which use ensemble methods with 10 model instances. An alternative Sinusoidal Regression task is evaluated with expanded ranges for b and \u03c9, added noise term, and 1000 tasks during training. An ensemble version of the model is also tested with 10 separate instances trained on the same tasks. The model is trained on 1000 tasks with 10 separate instances, and results are aggregated by taking the mean of predictions. Mean-squared error is evaluated for 10 shot and 5 shot cases, showing outperformance in sinusoidal tasks. The method is also tested on MNIST and CelebA datasets for image data comparison. During meta-training, K points are sampled from images of size 28x28 in MNIST and 32x32 in CelebA to form regression tasks. MSE is evaluated on the remaining pixels in the meta-testing stage. The method is compared with NP family models using deeper network structures with 5 fully connected layers and 128 hidden units. The comparison is fair in terms of network complexity. Our method, with 4 fully connected layers and 128 hidden units, outperforms two NP methods and achieves MSE close to the most recent ANP on 10,000 tasks from the meta-testing set. The high-dimension predictions on CelebA image data demonstrate the effectiveness of our approach. ANP's cross-attention improvement could potentially be applied to our method as well. Further analysis shows our method learns sparsifying basis functions for regression tasks. Our method, with 4 fully connected layers and 128 hidden units, outperforms two NP methods and achieves MSE close to the most recent ANP on 10,000 tasks from the meta-testing set. The high-dimension predictions on CelebA image data demonstrate the effectiveness of our approach. ANP's cross-attention improvement could potentially be applied to our method as well. Further analysis shows our method learns sparsifying basis functions for regression tasks, demonstrating the sparsity of basis functions by using only a fraction of the full set for prediction. Our method demonstrates the ability to contain most information in a few terms, allowing for a good approximation of F(x) with K samples. Ablation studies test design choices, focusing on self-attention operations in the Weights Generator and penalty terms in the loss function. An experiment replaces self-attention with fully connected layers, showing the impact on the Weights Generator. The study compares the performance of a simpler model with self-attention operations on a sinusoidal regression task. Different penalty terms were tested on the generated weights vector, including L1-norm, L2-norm, and a combination of both. The results show that adding self-attention operations improves performance, and the sparsity of the weights vectors for each variant is demonstrated. The study compares different penalty terms on weight vectors for a sinusoidal regression task. Results show that combining L1 and L2 penalties gives the best performance. The model with both penalties maintains high sparsity with near-zero weights. A few-shot meta learning system focusing on regression is proposed. Our few-shot meta learning system for regression tasks is based on a linear representation of basis functions. We use a Basis Function Learner network to encode the basis functions and a Weight generator network to generate weights. The model shows competitive performance in various regression tasks, with only 22 out of 40 basis functions being non-zero, demonstrating the sparsity enforced by our method. The learned basis functions in the few-shot meta learning system correspond to components of sinusoidal functions, representing peaks and troughs. Experiments show the impact of removing certain basis functions on sinusoidal regression predictions. The study illustrates the importance of basis functions in sinusoidal tasks, showing that removing key functions can significantly impact predictions. The Weights Generator Network architecture includes self attention blocks and fully connected layers for generating weights. The Weights Generator Network architecture includes self-attention blocks and fully connected layers for generating weights. The residual connection is passed through a layer normalization operation. The input to the first self-attention block is always the input to the Weights Generator network, while subsequent blocks take the outputs of the previous block. The self-attention operation involves transforming the input into query, key, and value vectors, which then undergo a scaled dot-product self-attention operation. The method is also evaluated on a 1D Regression task, specifically the 1D heat Equation task. The 1D heat equation task involves a rod with fixed temperature ends and a heat source applied at a random point. The goal is to model the temperature at each point of the rod over time until equilibrium is reached. The experiment sets the rod length to 5 and samples points on the heat equation curve. The model is evaluated on 10 shot and 5 shot cases, compared to EMAML and BMAML on regression tasks. The evaluation results of our method on the 2D Gaussian regression task are compared to EMAML and BMAML, along with an ensemble version. The model is trained to predict the probability distribution function of a two-dimensional Gaussian distribution with specific mean and standard deviation ranges. Results are presented in Table 6. We also compare our method to EMAML and BMAML on the Advanced Sinusoidal task, evaluating on 10, 20, and 50 shot cases. Additionally, qualitative results on CelebA datasets are provided in Figure 7, noting the complexity of RGB images as 2D functions. The results on CelebA datasets show visually better regression results compared to NP and CNP. Predictions using the first S largest weights indicate that the 2D image function is usually predicted with less than 50 weights, suggesting efficient representation. The proposed sparse linear representation framework for few shot regression resembles dictionary learning, focusing on learning dictionaries of atoms for signal representations. In few shot regression, the goal is to predict the entire continuous function y = F(x) with only a few samples given, making it significantly different and more challenging than typical dictionary learning algorithms. In few-shot regression, the basis matrix \u03a6 is encoded by the Basis Function Learner network, allowing for prediction of continuous functions with limited samples."
}