{
    "title": "S1lNWertDr",
    "content": "Learning long-term dependencies in recurrent neural networks (RNNs) is a challenge. Hierarchical RNNs (HRNNs) address this by using shortcuts in the hierarchy. This paper shows that in deep HRNNs, gradients can be replaced by locally computable losses, reducing memory requirements significantly. RNNs are powerful for real-world tasks like machine translation and language modeling but face challenges in training due to issues with learning long-term dependencies and memory requirements in the standard training algorithm TBPTT. The standard training algorithm for RNNs requires memory that grows linearly with sequence length, making it challenging to train RNNs with large hidden states on long sequences. TBPTT delays parameter updates until full forward and backward passes are completed, leading to slow learning for long sequences. Various approaches have been developed to address the issue of vanishing/exploding gradients in RNN architectures. Hierarchical RNNs (HRNNs) offer a solution to the vanishing/exploding gradient problem by splitting the network into levels updated at decreasing frequencies. Lower levels can be decoupled from higher levels in HRNNs, allowing for locally computable losses and easier training with memory. The decoupled HRNNs allow for training with memory decreased exponentially in depth compared to standard HRNNs. Local losses from decoder networks force hidden states to contain all relevant information. Performance is comparable to standard HRNNs in tasks with long-term dependencies and deep hierarchies. This RNN architecture with short gradient paths addresses memory efficiency issues and may help solve other challenges in the future. Several techniques have been proposed to address memory issues in TBPTT, such as trading memory for computation by storing only certain hidden states and recomputing missing ones on demand. Online optimization algorithms like Real Time Recurrent Learning (RTRL) have tackled memory problems and update locks. However, large computation costs and noise in approximations have made these algorithms impractical so far. Decoupled Neural Interfaces, introduced by Jaderberg et al. (2017), involve training a neural network to predict incoming gradients for parameter updates. HRNNs, studied extensively over the years, include approaches like updating hierarchies based on prediction errors and Clockwork RNNs with modules updated at decreasing frequencies. Various methods explicitly incorporate hierarchical data structures. Various methods have been proposed to incorporate hierarchical data structures in neural networks. Chung et al. (2016) introduced an architecture where the network extracts the hierarchical structure itself. Models that can learn useful hierarchies may improve performance. Auxiliary losses in RNNs, as presented by Schmidhuber (1992) and Goyal et al., have been used to improve generalization and sequence learning capabilities. Training of latent variables can be facilitated by introducing auxiliary losses in neural networks, as demonstrated by Goyal et al. (2017) and Trinh et al. This approach improves optimization and generalization of LSTMs by reconstructing or predicting input subsequences. Unlike previous methods, our approach uses auxiliary losses to replace gradient paths in hierarchical models, allowing for the discarding of hidden states in lower hierarchical levels. Our approach introduces an auxiliary loss in hierarchical recurrent neural networks (HRNN) to improve optimization and generalization. By cutting gradient flow and using local computation, we reduce memory requirements during training. The HRNN consists of lower and upper RNNs, with the lower RNN receiving input x and generating output y. The HRNN model consists of lower and upper RNNs. The lower RNN receives input x and generates output y, while the upper RNN updates its hidden state every k steps. The model can be extended to deeper hierarchies and allows for flexible update frequencies based on input structure. TBPTT involves unrolling the network for T steps in the forward pass. The HRNN model involves unrolling for T steps in the forward pass, with hidden states stored in memory. Gradients are computed in the backward pass using these hidden states, with restricted gradients termed as \u2202 symbol. The resulting gradients are not true gradients, except at the time-step when the upper RNN ticks. This approach is used to train the gradient restricted HRNN (gr-HRNN). The restricted gradients can be computed with memory proportional to k for the lower RNN and 2T/k for the upper RNN, improving memory efficiency by a factor of 2/k compared to standard TBPTT. This approach can be recursively applied to deep HRNNs, resulting in memory requirements of (l-1)k for the hidden states of lower RNNs. In the gr-HRNN, an auxiliary loss term is introduced to ensure that the hidden state contains all information about the last k inputs. This loss comes from a feed-forward decoder network and is added to the combined loss of the HRNN with a hyper-parameter \u03b2. In the gr-HRNN, an auxiliary loss term is introduced to ensure the hidden state retains information about recent inputs. This loss is computed locally in time, minimizing memory requirements during training. Experimental evaluation of the gr-HRNN with auxiliary loss shows promising results on tasks requiring learning long-term dependencies. The HRNN model has variations like HRNN, gr-HRNN, and mr-HRNN, each with different approaches to handling gradients and auxiliary loss. These models help in understanding the importance and effects of individual components. The HRNN model has variations like HRNN, gr-HRNN, and mr-HRNN, each with different approaches to handling gradients and auxiliary loss. The models are trained using a batch size of 100 and the Adam optimizer with specific parameters. The network for the auxiliary loss is a two-layer network with specific units and loss functions. In the copy task, the network is presented with a binary string and should output the same string in the same order. The maximum sequence length a model can solve is referred to as L max. Results for a sequence of length 100 are summarized in Table 1. The copy task requires exact storage of the input sequence over many steps to assess how long dependencies a model can capture. Both our model and the model using all gradients achieve similar performance, limited by the number of unrolled steps in training. The auxiliary loss is necessary for better performance, as the model without it performs poorly. Our model outperforms the model with all gradients and is robust to the choice of auxiliary loss weight. The HRNN uses the capacity of the decoder network and is unrolled for T = 200 steps. Gradients are propagated through the whole sequence for sequence lengths at most 100. Model parameters are updated once per batch in the pixel MNIST classification task. The network must predict the class label of presented MNIST digits, with pixels in fixed random order. Long-term dependencies are required for accurate predictions, especially with default permutation. Results show model performance comparable to HRNN with true gradient, highlighting the importance of auxiliary loss. The gr-HRNN model outperforms HRNN with all gradients in tasks like character-level language modeling on the Penn TreeBank corpus. Short-term dependencies dominate long-term ones in this task, and all 3 models perform similarly. The gr-HRNN model performs well in character-level language modeling tasks on the Penn TreeBank corpus, even without gradients or auxiliary loss for long-term dependencies. Short-term dependencies dominate in this task, and the hierarchical structure of the data is explicitly provided to the model by updating the upper RNN once per word and the lower RNN once per character. The gr-HRNN model excels in character-level language modeling tasks on the Penn TreeBank corpus by incorporating a hierarchical structure. The model uses an upper RNN with 512 units and unrolls for 50 characters to handle task complexity. It can generalize to deeper hierarchies, where decoder networks predict elements of input sequences or hidden states of lower level RNNs. The gr-HRNN model excels in character-level language modeling tasks on the Penn TreeBank corpus by incorporating a hierarchical structure. Our approach generalizes to deeper hierarchies using the copy task with sequence length 500. We test a HRNN with 3 levels, comparing the use of auxiliary loss in the lowest and middle layers. The RNNs have 64, 256, and 1024 units. Without the auxiliary loss in the middle layer, the network cannot solve the task. In hierarchical RNNs, adding an auxiliary loss to the middle layer enables the model to solve tasks perfectly. This approach allows for memory savings by not propagating gradients from higher to lower levels. Experimental results show that memory-efficient HRNNs with auxiliary loss perform similarly to memory-heavy HRNNs and outperform HRNNs with the same memory budget on various tasks, including deeper hierarchies. Combining high capacity RNNs with techniques to address parameter update lock could enable the use of these models for tasks with long-term dependencies. Resolving the parameter update lock issue is crucial for learning long-term dependencies in RNNs. The proposed technique of replacing gradients in HRNNs with locally computable losses may help in addressing this problem. Replacing gradients in HRNNs with locally computable losses may help solve the parameter update lock issue, which is crucial for learning long-term dependencies in RNNs. This approach could be a stepping stone towards addressing this problem, but further work is needed in this area."
}