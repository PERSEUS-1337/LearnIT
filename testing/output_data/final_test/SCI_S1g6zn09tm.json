{
    "title": "S1g6zn09tm",
    "content": "We propose a fully-convolutional conditional generative model, the latent transformation neural network (LTNN), for real-time view synthesis. The proposed methodology introduces the conditional transformation unit (CTU) to learn latent space transformations for specified target views. It includes a consistency loss term, a task-divided decoder, and an adaptive discriminator to improve training. The model outperforms state-of-the-art results in multi-view reconstruction, view synthesis, and object rotation tasks, with a 30% reduction in computational demand for inference. Generative models are effective for representing complex datasets and generating realistic samples. Conditional models can control properties of generated data. Some models incorporate encoding components to preserve input data structure while allowing for specified target properties. These inference models aim to maintain input features while controlling certain characteristics. In this work, a general framework is proposed for effectively performing inference with conditional generative models by strategically controlling the interaction between conditioning information and latent representations. A conditional transformation unit (CTU) is introduced to navigate the latent space structure, improving the performance of conditional generative models. The CTU, a collection of convolutional layers, approximates latent space operators to map encoded inputs to specified targets. A consistency loss term guides CTU mappings during training. A conditional discriminator unit (CDU) improves the network's ability to identify and eliminate transformation artifacts. RGB balance parameters allow for global color balance adjustments. The conditional transformation unit \u03a6 constructs mappings in the latent space for high-level attribute changes. The conditional transformation unit \u03a6 utilizes mappings in the latent space to produce attribute changes in decoded outputs, using conditioning information to select convolutional weights. It aims to align generated images with the true data distribution by adjusting RGB parameters globally and estimating local pixel values efficiently. Additionally, a task-divided decoder is introduced to learn shape and color properties simultaneously. The text introduces a framework for color inference with three network components dedicated to different tasks. It also presents a conditional discriminator unit to improve adversarial training by eliminating transformation-specific artifacts in generated images. These contributions significantly enhance the network's overall performance. The resulting latent transformation neural network (LTNN) has shown significant improvement in performance through various experiments, outperforming existing models in multi-view reconstruction, view synthesis, and object rotation synthesis. The CTU conditioning framework allows for additional conditioning information without affecting the network's speed. BID4 has developed a generative model for generating images of chairs, tables, and cars with specified attributes controlled by transformation and view parameters. The framework allows for synthesizing objects based on pre-defined models using transformation and view parameters. Conditional generative models rely on additional data for geometric prediction tasks and cannot be trained using images alone. Some networks enforce specific organizational structures in the latent space but require detailed labels for supervision, making them challenging for general tasks like training with real images. The appearance flow network (AFN) by Zhou et al. (2016) predicts rotated viewpoints of objects from images using geometric concepts unique to rotation. The conditional variational autoencoder (CVAE) incorporates conditioning information for synthesizing attribute changes in an identity-preserving manner. CVAE-GAN adds adversarial training to improve prediction quality. The conditional adversarial autoencoder (CAAE) by Zhang et al. (2017) models age progression/regression in human faces by concatenating conditioning information with the input's latent representation. The proposed model incorporates conditioning information by defining dedicated, transformation-specific convolutional layers at the latent level, allowing the network to integrate age information before decoding. The proposed LTNN framework incorporates conditioning information through transformation-specific convolutional layers at the latent level, enabling the synthesis of multiple views from a single input. This approach outperforms existing inference-based conditional models in various view synthesis tasks while requiring fewer FLOPs for inference. The methods used to define the LTNN model, including network structure, conditional transformation unit mappings, discriminator unit implementation, and network loss function, are detailed in this section. The proposed LTNN framework incorporates conditioning information through transformation-specific convolutional layers at the latent level for view synthesis tasks. The model workflow includes encoding input images, selecting conditional filter weights, decoding to obtain pixel value maps, and refining predictions using RGB balance parameters. Real and generated images are passed to a discriminator for loss computation and weight updates using ADAM optimization. The proposed LTNN framework incorporates conditioning information through transformation-specific convolutional layers for view synthesis tasks. It involves encoding input images, selecting conditional filter weights, decoding for pixel value maps, and refining predictions with RGB balance parameters. Loss computation and weight updates are done using ADAM optimization and backpropagation. Detailed network structure is provided in the appendix. Generative models aim to disentangle latent space for attribute modification. Nonlinear layers are used to process conditioning information at the latent space level in conditional generative frameworks. Conditioning information is combined with input features to enhance feature extraction. The proposed LTNN network design incorporates conditioning information using a conditional transformation unit (CTU) with distinct convolutional mappings in the latent space. Each CTU mapping has its own collection of convolutional filter weights and Swish activations for view point estimation. The proposed LTNN network design incorporates conditioning information using a conditional transformation unit (CTU) with distinct convolutional mappings in the latent space. The filter weights and Swish parameters of each CTU mapping are selectively updated based on the conditioning information provided. CTU mappings transform the encoded input to produce high-level view or attribute changes upon decoding, achieved by introducing a consistency term in the loss function. The discriminator in the adversarial training process also receives conditioning information to specify the attempted transformation. The proposed model incorporates a context-aware discriminator structure, the Conditional Discriminator Unit (CDU), to identify unrealistic artifacts in the transformation process. Each viewpoint has its own CDU. The model uses adversarial loss as the primary component, with additional loss terms for structural reconstruction and consistency. The final loss function for the encoder and decoder components includes a consistency term to guide mappings towards approximations of latent space mappings. This term enforces that the transformed encoding approximates the encoding of the target image during training. The decoding process is divided into tasks for estimating refinement maps, pixel values, and RGB color balance. This decoupled framework for estimation aids the network in its tasks. The decoding process involves convolutional layers and bilinear interpolation to upsample latent information. Two transpose convolutional layers are used for task separation, one for refining pixel values and the other for predicting pixel-values. RGB balance parameters adjust color channels, and the final output is the Hadamard product of the refinement map and RGB-rescaled value map. The network uses refinement maps to mask values outside the target object, focusing on object shapes during training. Experimental results show sharp drop-offs along boundaries, with synthesized views showing higher accuracy for pose estimation. Various experiments were conducted, including hand pose estimation and synthesis of rotated views of rigid objects. The study involved experiments on hand depth image data, synthesis of rotated views of objects, and modification of attributes on synthetic face datasets. Models were trained using 80% of the datasets, with evaluation based on L1 mean pixel-wise error and SSIM. Ground truth data was available for most experiments, except for the real hand dataset where an indirect metric was used for evaluation. The study involved experiments on hand depth image data, synthesis of rotated views of objects, and modification of attributes on synthetic face datasets. Evaluation of the proposed framework was done with existing works using comparison groups for experiments with non-rigid and rigid objects. Additional experiments compared the proposed CTU conditioning method with conventional concatenation methods. The LTNN model was trained using a synthetic dataset for the hand pose experiment due to the unavailability of ground truth predictions for the real NYU hand dataset. The LTNN model was trained using a synthetic dataset for hand pose estimation, providing ground truth coordinates for evaluation. The model generated 9 different views for input into a pose estimation network, showing substantial improvement over existing methods. Real-time performance was demonstrated at 114 fps without batching and 1975 fps with a mini-batch size of 128. The LTNN model, trained on a synthetic dataset for hand pose estimation, outperformed existing methods by generating 9 different views for input. It achieved real-time performance at 114 fps without batching and 1975 fps with a mini-batch size of 128. In experiments with real faces and objects, the LTNN model significantly outperformed other state-of-the-art models in both L1 and SSIM metrics. The LTNN framework excels in rigid-object transformations using a single image input, surpassing other methods in L1 metric and comparable SSIM scores. Unlike competitors, it doesn't require additional depth or 3D models for training, making it practical for real datasets. In 360\u00b0 view estimation for chairs, the proposed model outperforms existing methods with the least computational cost. The proposed method for 3D chair 360\u00b0 view synthesis outperforms others in terms of parameters, FLOPs, and inference times. It can handle diverse attribute modification tasks and allows for near continuous attribute changes. Multiple modifications can be synthesized simultaneously using distinct CTU mappings within the framework. The framework introduced in this work incorporates conditioning information into generative models using CTUs and a consistency loss term. It also utilizes a context-aware discriminator to enhance adversarial training performance. The framework has been tested on various tasks and has shown superior performance compared to existing methods. The framework incorporates conditioning information using CTUs and a context-aware discriminator to enhance training performance. It has shown superior performance on various tasks compared to existing methods. The CTU maps latent features and noise vector for face attribute tasks, while the decoder processes features with convolution transpose layers for value estimation and refinement maps. For the ALOI data experiment, the network structure follows IterGAN Galama & Mensink (2018) with additional Block v1 layer for stereo face dataset BID5. The encoder includes efficient feature extraction blocks with dense connections, normalized using batch normalization. The minimalist decoder design is inspired by BID24, using standard convolutional layers and transpose convolutional layers for value estimation. The method implemented in TensorFlow uses 5x5 filters for value-estimation and refinement maps. Parameters are initialized using variance scaling. Training is done with SGD and ADAM optimizer with specific parameters. One-sided label smoothing is used for discriminator training. Datasets are normalized to [0, 1]. The LTNN model allocates output channels for non-identity filters based on the total number of channels specified. In the Block v1 layer, 31 out of 32 channels are non-identity filters with Swish activation functions. Filters with multiple convolutional layers do not use activation functions for intermediate layers. A kinematic hand model with 33 degrees of freedom generated 200,000 distinct hand poses with nine depth images from different viewpoints. Testing was done on MSRA and NYU BID33 hand datasets using LTNN multi-view predictions for hand pose estimation. The accuracy of predicted angles was assessed to evaluate network predictions. The LTNN model is capable of making simultaneous predictions of multiple viewpoints, while Pix2Pix BID13 and IterGAN BID6 networks produce a single synthesized view. To ensure fair comparisons, each model was trained to produce a single 30\u00b0 rotated view of ALOI objects. Only two CTU mappings were trained: one for identity and one for the rotated view. Experiment results on unseen objects from the ALOI dataset are shown in Figure A.8. Our method outperforms IterGAN and Pix2Pix in generating sharper and more realistic views of chairs from the ShapeNet BID2 3D model repository. The dataset used for real face experiments includes 1000 face images with background and different views, which were processed to reduce noise and perform face segmentation. Training images were resized to 128 \u00d7 128 \u00d7 3 resolution for model training. The curr_chunk discusses the training process for face images, including resizing to 128 \u00d7 128 \u00d7 3 resolution, rendering faces at different age ranges and lighting directions, and varying face orientation. It also mentions converting input images to grayscale for colorization demonstration and provides ablation/comparison results. Additionally, it touches on the task-division decoding procedure and the use of refinement maps to produce masks resembling target objects' shapes. The curr_chunk discusses the use of refinement maps for local color balancing and artifact removal in generated images. It also mentions near-continuous attribute modification through piecewise-linear interpolation in the latent space. The curr_chunk discusses predicting elevation changes using linear interpolation in the latent space of network representations. This allows for continuous attribute changes to be approximated. The proposed framework enables near-continuous attribute modification through piecewise-linear interpolation in the latent space, producing modified images with variations in light direction, age, azimuth, and elevation. This is achieved by using 9 CTU mappings and linearly interpolating between discrete transformation encodings. Additional results can be found in Section A.7.6."
}