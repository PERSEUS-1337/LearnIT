{
    "title": "SyeLno09Fm",
    "content": "A challenge in applying reinforcement learning to real-world problems is defining a reward function. Inverse reinforcement learning (IRL) infers a reward function from expert behavior, but collecting diverse datasets can be costly. This study proposes using demonstrations from other tasks to improve reward function inference efficiency. Our method efficiently recovers rewards from images for novel tasks, similar to learning a prior. Reinforcement learning algorithms have the potential to automate decision-making tasks across various domains, but defining accurate reward functions can be challenging for real-world tasks. Inverse reinforcement learning (IRL) infers reward functions from task demonstrations, addressing challenges in specifying rewards for high-dimensional observation spaces like images. Learning reward functions from demonstrations covering real-world task variability is costly. Expressive function approximators may seem reasonable for learning complex functions from high-dimensional observations, but in practice, rewards functions are rarely learned due to the prohibitive cost of providing diverse demonstrations. In the \"few-shot\" domain, recovering a good reward function with expressive function approximators is difficult. Prior solutions relied on low-dimensional linear models with handcrafted features. This work proposes learning expressive features that are robust with limited demonstrations by leveraging common structure in related tasks. For example, in a robot navigating through a home, there is a structure amid the space of useful behaviors, even if the reward function varies. The approach aims to discover common structures among different tasks and encode them to infer reward functions from a few demonstrations. It addresses over-fitting in few-shot Inverse Reinforcement Learning by learning a \"prior\" that constrains possible reward functions within a few steps of gradient descent. The approach aims to learn reward functions efficiently by using meta-training to establish a rich \"prior\" for goal inference. This method enables effective few-shot learning when initializing Inverse Reinforcement Learning in a new task, with the key contribution being an algorithm that efficiently learns new reward functions. The proposed approach aims to learn deep neural network reward functions efficiently from raw pixel observations, outperforming existing methods. It is based on the maximum entropy framework for Inverse Reinforcement Learning and addresses the under-specified nature of IRL by using high-dimensional input and fully convolutional networks. Incorporating fully convolutional networks into the MaxEnt IRL framework, BID52 proposed a method for learning from high-dimensional input for navigation tasks. Other approaches like guided cost learning (GCL) and adversarial IRL have also utilized neural network rewards. This work focuses on effectively utilizing prior demonstration data from other IRL tasks to learn new tasks with non-linear reward functions. Our work aims to distill meta-training tasks into a prior that can efficiently learn rewards for new tasks, different from previous approaches that incorporate shared prior over reward functions. The goal is to use the distilled meta-training tasks to efficiently learn on new tasks, rather than acquiring good reward functions for explaining the meta-training tasks. Our approach aims to efficiently learn on new tasks by distilling meta-training tasks into a prior, different from previous methods. Various meta-learning approaches have been proposed, including memory-based methods, learning optimizers/initializations, and comparing datapoints in a learned metric space. Future work can explore adapting these approaches for broader applicability in Inverse Reinforcement Learning (IRL). The standard Markov decision process (MDP) is defined by the tuple (S, A, p s , r, \u03b3) where S and A denote the set of possible states and actions, r : S \u00d7 A \u2192 R is the reward function, and \u03b3 \u2208 [0, 1] is the discount factor. In Inverse Reinforcement Learning (IRL), the goal is to recover the unknown reward function from a set of expert demonstrations. The approach builds on the maximum entropy (MaxEnt) IRL framework to model the probability of trajectories. In Inverse Reinforcement Learning (IRL), the reward learning problem is formulated as a maximum likelihood estimation (MLE) in an energy-based model. Learning in energy-based models is common in various applications, but in IRL, it is typically done with a small number of example demonstrations. This work aims to integrate information from prior tasks to constrain the optimization in the regime of limited demonstrations. The goal is to address this issue by providing a way to meta-learn and improve learning efficiency. Meta-learning algorithms aim to optimize learning efficiency on new tasks by generalizing to new tasks rather than new data points. In the meta-learning setting, there are meta-training and meta-test sets drawn from a task distribution. During meta-training, the learner learns task structures to efficiently learn from limited examples when presented with a new task. One approach to meta-learning is to directly parameterize the learner for few-shot learning settings. One approach to meta-learning is to parameterize the meta-learner with a neural network conditioned on task training data and test task inputs. This model is optimized using log-likelihood across all tasks. Another approach is to learn components of the learning procedure, such as initialization or optimization algorithm. This work extends the model agnostic meta-learning approach by learning an initialization adapted by gradient descent. In meta-IRL, the goal is to learn reward functions across tasks to infer new task rewards from expert demonstrations. This involves optimizing parameters for gradient descent on held out tasks, akin to adding a constraint in a multi-task setting. The aim is to learn a prior over human intentions to effectively determine new task rewards with minimal demonstrations. During meta-IRL, the goal is to learn reward functions across tasks to infer new task rewards from expert demonstrations. This involves optimizing parameters for gradient descent on held out tasks. In meta-training, tasks are used to encode common structure so that the model can quickly acquire rewards for new tasks from just a few demonstrations. The algorithm must infer the parameters of the reward during the meta-test phase. During meta-IRL, the algorithm must infer reward function parameters for new tasks from expert demonstrations. The MaxEnt IRL loss is used to compute updated parameters for the reward function, enabling quick acquisition of rewards for new tasks. The algorithm in meta-IRL infers reward function parameters for new tasks from expert demonstrations using the MaxEnt IRL loss. The goal is to efficiently adapt to new tasks at meta-test time by encoding prior information over the task distribution in the learned reward prior. This is achieved by finding parameters that lead to a reward function with high likelihood for test demonstrations. The algorithm in meta-IRL infers reward function parameters for new tasks from expert demonstrations using the MaxEnt IRL loss. It aims to efficiently adapt to new tasks at meta-test time by encoding prior information over the task distribution in the learned reward prior. This involves finding parameters that lead to a reward function with high likelihood for test demonstrations. The optimization problem for \u03b8 is defined to minimize the IRL loss for a set of test demonstrations. The method extends the MAML algorithm to the inverse reinforcement learning setting, which is challenging due to the computation involved in computing the MaxEnt IRL gradient and MAML objective. The algorithm in meta-IRL infers reward function parameters for new tasks from expert demonstrations using the MaxEnt IRL loss. It involves computing the meta-gradient for the objective by performing updates in expected state visitations and state visitation distributions of the expert. This allows recovery of the IRL gradient for efficient adaptation to new tasks at meta-test time. Meta-testing involves leveraging the meta-trained parameters to enable fast, few-shot IRL of novel tasks. State visitations are computed from available demonstrations for each task. The IRL gradient is recovered for efficient adaptation to new tasks at meta-test time. Our approach involves computing state visitations from demonstrations for each task and using them to compute the gradient for parameter adaptation. It is beneficial to take more gradient steps during meta-testing, with performance improving up to 20 steps. The objective optimizes for parameters that enable efficient adaptation and generalization on various tasks. This approach expresses a \"locality\" prior over reward function parameters. Our approach involves learning a distribution over demonstrations \u03c4, with a prior over \u03c6T used for MAP inference during meta-test. In the context of maximum likelihood estimation, fast adaptation in MAML can be seen as performing MAP inference over \u03c6 with a Gaussian prior. This connection is based on the relationship between early stopping and regularization, as discussed in Santos (1996). The interpretation of MAML as imposing a Gaussian prior on parameters is exact for quadratic likelihoods, but only an approximation for non-quadratic likelihoods. Despite limitations in complex parameterizations like deep function approximators, early stopping and initialization can still be viewed as a form of prior. This suggests potential extensions to incorporate Bayesian approaches for reward and goal inference. The evaluation aims to test the hypothesis that leveraging prior task experience facilitates reward learning for new tasks. Our method leverages prior task experience for reward learning on new tasks with few demonstrations. We compare it with alternative algorithms using multi-task experience. The comparison evaluates learning performance starting from a learned initialization versus starting from scratch. This study introduces a new meta-inverse reinforcement learning problem with no prior work addressing it. Comparisons with black-box meta-learning methods highlight the importance of incorporating the IRL gradient into the meta-learning process. Our method incorporates the IRL gradient into the meta-learning process, implicitly conditioning on demonstrations through gradient updates. We compare to an RNN-based meta-learner and optimize for effective parameter initialization for the IRL problem. Our method incorporates the IRL gradient into the meta-learning process, implicitly conditioning on demonstrations through gradient updates. We compare with finetuning an initialization obtained with the same set of prior tasks, but with supervised pretraining. This comparison evaluates the benefits of optimizing explicitly for weights that perform well under fine-tuning. The environment and evaluation involve a navigation domain called SpriteWorld, where a convolutional neural network is trained to map image pixels to rewards. The task visuals are inspired by Starcraft, presenting a challenging experiment that requires learning rewards directly from raw pixels. The task visuals are inspired by Starcraft and involve applying learning algorithms for micromanagement. Tasks require navigating to goal objects while exhibiting terrain preferences. Evaluations are done in new test environments to ensure correct visual cue learning. MandRIL shows capability of overfitting in both test settings. The underlying MDP structure of SpriteWorld is a grid with unique tasks generated by randomly choosing 3 sprites from a total of 100 sprites. One sprite is designated as the goal for navigation, while the other two are obstacles. The agent navigates to a designated object while avoiding obstacles, optimizing on a meta-training set to generalize to rearranged objects. Performance is measured using expected value difference. The evaluation protocol involves testing on new tasks unseen during meta-training, including tasks with new sprite combinations and placements, as well as tasks with entirely new sprites referred to as \"out of domain objects.\" Each task is evaluated by adapting the reward based on demonstrations provided, and then testing the adapted reward on a new environment with different sprite positions. Our approach, MandRIL, shows superior performance in both in-distribution and out-of-distribution sprite settings during meta-test evaluations. Even with minimal demonstrations, MandRIL outperforms other meta-learning methods that tend to overfit on training data. Learning the reward function from scratch proves to be a competitive baseline as the number of demonstrations increases. Learning the reward function from scratch is a competitive baseline, matching MandRIL's performance with 20 or more demonstrations. MandRIL outperforms other methods with minimal demonstrations and excels in out-of-distribution settings. Pre-training on all tasks hinders adaptation, while pre-training on a single task does not improve performance. MandRIL surpasses all methods in meta-test evaluations. Our approach ManDRIL outperforms other methods by explicitly optimizing initial weights for fine-tuning, showing robust performance improvement. Comparisons against mean gradient and single task pre-training show less effectiveness, indicating that fine-tuning reward functions learned in this manner is not optimal. Our approach ManDRIL outperforms other methods by optimizing initial weights for fine-tuning, robustly improving performance. It enables few-shot learning for reward functions of new tasks through a novel formulation of inverse reinforcement learning that encodes common structure across tasks. Leveraging data from previous tasks, we can effectively learn deep neural network reward functions from raw pixel observations for new tasks with only a handful of demonstrations. This paves the way for future work in environments with unknown dynamics or more probabilistic approaches to reward and goal inference. The input to our reward function is an 80 \u00d7 80 RGB image, with an output space of 20\u00d720 in the underlying MDP state space. Q-iteration is used to compute the optimal policy in our experiments. In our experiments, we parameterize the reward function for all our reward functions starting from the same base learner. The neural network architecture includes convolutional layers with varying filter sizes and strides, followed by an LSTM implementation for predicting agent location. Conditioning the initial hidden state on image features did not improve performance in our conditional model. In the demo conditional model, spatial information of demonstrations is preserved by feeding state visitation map as an additional channel to the image. Different hyperparameters were tested in the LSTM learner, but they did not impact performance. Bias transformation BID12 did not help in the experimental setting. Sprites in the environment are extracted from StarCraft files. The sprites used in the environment were extracted from StarCraft files. 100 random units were used for meta-training, with evaluation on 5 randomly selected sprites. A meta-training set of 1000 tasks was created for computational efficiency. The sprites were divided into buildings and characters, with characters having multiple poses and buildings having a single pose. Units were randomly placed during meta-training to avoid obstacles, and terrain was randomly generated using different tiles. The environment terrains were randomly generated using a graph traversal algorithm with specific constraints on neighbor tiles. Various buildings were used in the experiments, such as academy, barracks, beacon, and more. The list of characters and buildings used in the experiments is provided. The meta-test training performance with varying numbers of demonstrations is shown, indicating that as the number of demonstrations increase, all methods are able to perform well in terms of training performance. The quality of reward function is defined by parameter \u03b8 on task T with MaxEnt IRL loss. The gradient is calculated using Jacobian matrix and state visitations. Updated parameters are obtained after a single gradient step. The MaxEnt IRL loss is computed over trajectories. The MaxEnt IRL loss, L test T, is minimized over parameters \u03b8 by computing the gradient and using chain rule. Notations for partial derivatives and gradients are defined. Focus is on the k \u00d7 k-dimensional matrix of partial derivatives in Eq. 12. The chain rule is applied to expand the second term in derivatives. Notations for partial derivatives and gradients are defined, including the k \u00d7 |S||A|-dimensional matrix of second-order partial derivatives. The gradient of the expectation E \u03c4 [\u00b5 \u03c4 ] with respect to the reward function r \u03b8 is computed by expanding the expectation. The chain rule is applied to expand the second term in derivatives, involving partial derivatives and gradients. The gradient of the expectation with respect to the reward function is computed by expanding the expectation. The formula for DISPLAYFORM7 is derived in this context."
}