{
    "title": "SkeUG30cFQ",
    "content": "Recent results in linear algebra show that matrices can be decomposed into diagonal and circulant matrices, leading to the development of efficient deep neural network architectures. This paper explores the approximation capabilities of Deep diagonal-circulant ReLU networks, demonstrating that they can approximate deep ReLU networks with bounded width and small depth. Recent progress in deep neural networks has led to an increase in model sizes, with state-of-the-art architectures having millions to billions of parameters. While large models are more accurate, they are challenging to train and deploy, especially on mobile devices with limited resources. Structural properties in linear algebra are commonly exploited to address these challenges. In linear algebra, structural properties of matrices are used to speed up computations and reduce memory usage. BID7 proposed a network architecture with circulant matrices to drastically reduce model size. Cheng et al. showed that this architecture is almost as accurate as the original network. BID23 leveraged a result to design Deep diagonal-circulant ReLU networks, with good experimental results. In this paper, the authors bridge the gap between empirical results and theoretical capabilities of Deep diagonal-circulant ReLU networks. They prove that these networks can approximate any dense neural network by decomposing matrices into diagonal and circulant matrices. This result is more practical than previous methods, allowing for low rank SVD decomposition while controlling approximation error. Thorough experiments were conducted on synthetic and real data. The authors conducted experiments on synthetic and real datasets to demonstrate the tradeoff between accuracy and model size by adjusting the number of factors in matrix decomposition. They also evaluated the applicability of this approach on neural network architectures for video classification on the Youtube-8m Video dataset. The experiment showed that Deep diagonal-circulant ReLU networks can train more compact neural networks for large-scale scenarios. Various techniques have been proposed to build compact deep learning models, including model distillation. Several approaches have been proposed to compress neural networks, including reducing memory at the level of individual weights or weight matrices. Some researchers focus on designing compact models from the start to reduce memory footprint. One technique involves compressing weight matrices using hashing functions, but it may have poor performance on modern GPU devices due to irregular access. In theory, replacing weight matrices in fully connected layers with circulant matrices can improve performance on modern GPU devices. BID7 observed that fully connected layers are often used for dimensionality reduction. Cheng et al. proposed using random circulant matrices in place of weight matrices, resulting in models with good accuracy. Training the circulant matrix weights with the rest of the network using gradient-based optimization further improves performance. Replacing weight matrices in fully connected layers with circulant matrices can improve performance on modern GPU devices. Circulant matrices are trained with the rest of the network using a gradient-based optimization algorithm, suggesting they offer more than simple random projections. Other structured matrices like Toeplitz, Vandermonde, and Fastfood transforms have also shown promising results in neural network architectures. Despite the lack of theoretical insight, deep neural networks with structured matrices have shown good approximation capabilities. The universal approximation theorem by BID5 states that a neural network with at least 1 hidden layer and sigmoid nonlinearity can approximate any function. However, this theorem does not limit the width of the network or consider the training procedure. Recent theoretical work has focused on evaluating the expressiveness of neural networks based on their width and depth. Circulant matrices, a special type of Toeplitz matrix, are able to model various linear transforms effectively. Circulant matrices are compact and can represent linear transforms efficiently. They can be combined with diagonal matrices to approximate any matrix with high precision. The product of a circulant matrix and a vector can be simplified to an element-wise product in the Fourier domain, reducing complexity from O(n^2) to O(nlog(n)). Circulant matrices, combined with diagonal matrices, can approximate any matrix with high precision. Recent theoretical work on Deep diagonal-circulant ReLU networks shows that 2-layer networks of unbounded width are universal approximators. However, the properties of these networks with bounded width are not well understood. Deep diagonal-circulant ReLU networks with bounded width are explored for their approximation capabilities. The networks are defined using circulant matrices and diagonal matrices, with depth and width denoted by l and n respectively. The analysis aims to determine if these networks are universal approximators and what functions they can approximate. ReLU networks are universal approximators, with a proposition linking standard deep neural networks to Deep diagonal-circulant ReLU networks. Bounded depth Deep diagonal-circulant ReLU networks are proven to be universal approximators on any compact set. These networks can approximate any Deep ReLU network and are not necessarily more compact. The previous corollary shows that a n-wide Deep ReLU network with l layers can be decomposed into a Deep ReLU network with l(2n\u22121) matrices. Empirical evidence suggests that bounded width and small depth Deep diagonal-circulant ReLU networks perform well. The following theorem discusses the approximation properties of these networks. Deep diagonal-circulant ReLU networks with bounded width and small depth can approximate deep ReLU networks with low-rank matrices. The proposition requires k to divide n, but it is conjectured to hold without this condition. Using low-rank approximators for matrices in neural networks can control approximation error. The Deep diagonal-circulant ReLU networks can approximate deep ReLU networks with low-rank matrices. The networks use low-rank approximators to control approximation error. For any k dividing n, there exists a network with width n and depth m = 4(k + 1)n that satisfies certain conditions. Each matrix can be replaced by a product of k diagonal-circulant matrices. The experiments presented aim to investigate the impact of increasing the number of diagonal-circulant factors on network accuracy. Results align with theoretical analysis. Additionally, a deep diagonal-circulant neural network architecture is proposed for video classification, based on a state-of-the-art model by Abu-El-Haija et al. The architecture, based on a model proposed by Abu-El-Haija et al., involves large layers made more compact using circulant matrices. The approach shows good accuracy and can create a more compact network. The experimental setup includes a dataset with 10000 examples, training a neural network with 3 hidden layers, and comparing it with a Deep diagonal-circulant ReLU network. Results show the loss comparison between the dense architecture and the Deep diagonal-circulant ReLU networks. The Deep diagonal-circulant ReLU networks achieve over 90% compression rate with a significant loss in accuracy for factors up to 16. Increasing factors improves accuracy but hinders convergence, limiting training to 32 layers. To balance compression and accuracy, circulant-diagonal ReLU decomposition can be selectively applied. The applicability of diagonal-circulant ReLU networks is demonstrated in a large-scale video classification architecture trained on the Youtube-8M dataset. The architecture proposed by BID2 and improved by BID22 includes video and audio frame embeddings. The model architecture consists of three blocks of layers. The experiment aims to evaluate the trade-off between compression rate and accuracy using circulant-diagonal ReLU decomposition on certain layers. TensorFlow Framework BID0 is used for all experiments with specific hyperparameters. The experiment used Adam optimizer with specific hyperparameters and evaluated the effect of circulant-diagonal ReLU decomposition on different layers. Results showed a compression rate of 9.5 for the compact fully connected layer with similar performance, while the compact DBoF and MoE achieved higher compression rates at the cost of accuracy. The study demonstrated that Deep diagonal-circulant ReLU networks can be used in large-scale machine learning applications by trading off model size for accuracy. The model with a compact FC converges faster, while the compact DBoF and MoE show higher compression rates but lower accuracy. The theoretical study showed that these networks are bounded width universal approximators, with the ability to calculate error bounds based on network depth and weight matrices' singular values. The study demonstrated the use of Deep diagonal-circulant ReLU networks in machine learning applications. The networks are bounded width universal approximators with the ability to calculate error bounds based on network depth and weight matrices' singular values. Theoretical results show the convergence properties of these networks. The study showcased the use of Deep diagonal-circulant ReLU networks in machine learning. These networks are universal approximators with error bounds based on network depth and weight matrices' singular values. Theoretical results demonstrate the convergence properties of these networks. Let A be a matrix of rank k in C n\u00d7n, where n is divisible by k. There exists a sequence of 4k + 1 matrices B 1 , . . . , B 4k+1, with B i being circulant if i is odd, and diagonal otherwise. The SVD decomposition of M results in a product of 4k + 2 matrices, alternating between diagonal and circulant. The product of RO will have the same rank as R, with the last n \u2212 k columns being zeros. The values of d ij are unknown but can be computed. The linear equation system W RO = U can be formulated to find the values of d ij. The bloc-diagonal matrix consists of Toeplitz matrices induced by subsequences of length k of r. Each bloc is a permutation of the identity matrix, making the block diagonal matrix invertible. The block diagonal matrix is invertible, allowing for the solution of linear equations to find values for d. The matrix can be factorized into circulant and diagonal matrices, representing A with a product of 4k + 2 matrices. Low rank matrices in a neural net can be decomposed into diagonal and circulant matrices, connected to form the neural net. The recurrence relation formula can be used to bound errors. The formula for bounding errors in a neural net is given by a recurrence relation. It states that for any pair of vectors a, b in Cn, ReLU(a) - ReLU(b) \u2264 a - b. This proof involves a SVD approximation of rank k for a matrix A in Cn\u00d7n with singular values \u03c31...\u03c3n."
}