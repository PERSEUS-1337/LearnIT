{
    "title": "HJxeGb5pTm",
    "content": "Open information extraction (OIE) systems extract relations and arguments from natural language text in an unsupervised manner. OPIEC is the largest OIE corpus publicly available, extracted from English Wikipedia, with over 340M triples and valuable metadata. It complements existing OIE resources and contains provenance information, confidence scores, and semantic annotations. The OPIEC corpus is analyzed by comparing its content with knowledge bases like DBpedia and YAGO, revealing that most facts between entities present in OPIEC. The OPIEC corpus, extracted from English Wikipedia, contains unique facts not found in DBpedia or YAGO. OIE facts differ in specificity and open relations are polysemous. OPIEC is a valuable resource for automated knowledge research. Open Information Extraction (OIE) involves extracting relations and their arguments from text in an unsupervised manner, producing structured (subject, relation, object)-triples. These extractions are valuable for tasks like knowledge base construction, question answering, event schema induction, and improving OIE systems. Derived resources from OIE extractions include entailment rules, question paraphrases, Relgrams, and OIE-based embeddings. In this paper, a new OIE corpus called OPIEC is introduced, extracted from English Wikipedia using the MinIE OIE system. OPIEC is the largest publicly available OIE corpus with over 340M triples, providing detailed metadata, syntactic and semantic annotations, entity information, and confidence scores for each extraction. The OPIEC corpus was analyzed for its contents and usefulness. A subcorpus called OPIEC-Clean was created, containing triples with relations between concepts. OPIEC-Clean is smaller but still four times larger than prior OIE corpora. The OPIEC corpus, compared to DBpedia and YAGO knowledge bases, shows that most facts cannot be found in them. OIE facts differ in specificity, and open relations are highly polysemous. The OPIEC-Linked subcorpus contains 5.8M triples with linked Wikipedia articles for disambiguation. The OPIEC corpus, along with its subcorpora, is a valuable resource for research on automated knowledge base construction. It focuses on large-scale OIE corpora that do not use predefined sets of arguments and relations. The corpus contains open relations between arguments of various entity types. The OPIEC corpus, along with related OIE corpora like ReVerb, PATTY, WiseNet, WiseNet 2.0, DefIE, and KB-Unify, organizes open relations in relational synsets and integrates multiple OIE corpora. OPIEC and OPIEC-Clean are significantly larger than KB-Unify in terms of triples, distinct arguments, and relations. The OPIEC corpus, created using the MinIE extractor, contains more extractions than prior resources. Most corpora use automated methods for entity disambiguation, but this may introduce errors and limit the corpus to confidently linked arguments. OPIEC did not perform automatic disambiguation, retaining Wikipedia links when available. The OPIEC corpus contains almost 6M triples with disambiguated arguments via golden links. OPIEC and ReVerb provide confidence scores for extractions to measure extraction accuracy. Extractor confidence is crucial for downstream applications. The OPIEC corpus, constructed from English Wikipedia articles, contains syntactic and semantic annotations for extractions, including part-of-speech tags, lemmas, and attribution. Semantic annotations simplify triples and provide contextual information. The pipeline overview shows the process of constructing OPIEC. The pipeline used for constructing the OPIEC corpus from English Wikipedia includes Apache Spark for distributed corpus construction. A modified version of WikiExtractor was used to extract plain text and retain internal Wikipedia links as metadata annotations. Custom entity linkers can be integrated into the pipeline using these annotations. The NLP pipeline used on preprocessed Wikipedia articles included tokenization, sentence splitting, part-of-speech tagging, lemmatization, named entity recognition, temporal tagging, and dependency parsing. Triples were extracted using the OIE system MinIE, based on ClausIE, which minimizes extractions into compact triples by removing unnecessary words and providing semantic annotations. MinIE extracts triples from input sentences and adds semantic annotations such as polarity, modality, attribution, and quantities. For example, in the sentence \"David Heyman said that Gilderoy Lockhart will probably not be played by Kenneth Branagh,\" the extracted triple is (\"Gilderoy Lockhart\"; \"be played by\"; \"Kenneth Branagh\") with annotations indicating negative possibility and positive certainty attribution to David Heyman. MinIE-SpaTe enhances MinIE's output by adding spatial and temporal annotations, creating spatial/temporal triples. These annotations differentiate between entire triples, arguments, and references. The system utilizes syntactic information, SUTime, and NER for precision. MinIE-SpaTe enhances MinIE's output by adding spatial and temporal annotations to triples. Temporal annotations provide context for the entire triple, including temporal modifiers. The system uses the TIMEX3 format BID29 to represent temporal information. Additionally, temporal annotations are provided for arguments that contain temporal information referring to a phrase rather than the whole triple. MinIE-SpaTe enhances MinIE's output by adding spatial and temporal annotations to triples. It extracts triples with temporal annotations, including temporal references as subjects or objects. The system uses TIMEX3 format to represent temporal information and retains NER types provided during preprocessing. During postprocessing, wrong triples are removed, and remaining triples are annotated with a confidence score to ensure Wikipedia links are not broken up. MinIE-SpaTe retains NER types during preprocessing and extracts triples with temporal annotations. The system filters incorrect triples and assigns confidence scores to ensure Wikipedia links are preserved. Triple extraction errors are analyzed using a logistic regression classifier trained on labeled datasets. Features like clause type and conjunction processing are crucial in determining correct extractions. In postprocessing, triples are rearranged to avoid splitting links across constituents. Metadata like provenance, syntactic and semantic annotations, and confidence scores are retained for each triple. Details on metadata fields can be found in Appendix B. The OPIEC-Clean and OPIEC-Linked subcorpus were constructed by filtering the OPIEC corpus based on specific rules. Basic statistics and information about the corpus sizes and extracted triples are provided. The OPIEC corpus contains extractions produced by MinIE-SpaTe, with some triples being challenging for downstream applications. A substantial portion of the triples in the OPIEC corpus are challenging for downstream applications due to under-specificity, particularly caused by a lack of coreference information. This includes triples with personal pronouns, determiners, and Wh-pronouns as arguments, making coreference resolution a difficult problem. Coreference resolution is crucial for improving the recall of OIE systems, especially for entity mentions like works of art. A preliminary study found that a significant percentage of OPIEC triples have similar token lengths and confidence scores across different corpora. The OPIEC corpus contains triples with annotations, with a small percentage involving named entities in both subject and object. Many triples have complicated expressions in their arguments, making them difficult to handle. The OPIEC corpus is valuable for research on improving complex OIE extractions rather than for downstream tasks. The OPIEC-Clean corpus is derived from OPIEC by removing underspecified and complex triples. Triples are considered clean if arguments are linked entities or match Wikipedia page titles, not split across constituents, and have a non-empty object. MinIE is a clause-based OIE system that extracts subject-relation pairs without objects. The OPIEC-Clean corpus is easier to work with than the full OPIEC corpus, containing clean triples that are shorter, have higher confidence scores, and are targeted towards downstream applications and research in automated knowledge base construction. The OPIEC-Linked corpus is a subset of OPIEC-Clean with linked arguments, making it the largest corpus to date with golden triples. The OPIEC-Linked corpus, a subset of OPIEC-Clean, is the largest corpus with golden disambiguation links for arguments. About 49% of triples in OPIEC have semantic annotations, increasing to 58% in OPIEC-Linked. Annotations mainly refer to quantities, space, or time, providing context for extractions. Negative polarity and possibility modality annotations are less common, possibly due to the factual nature of Wikipedia articles. The distribution of annotations differs significantly for OPIEC-Linked compared to OPIEC and OPIEC-Clean. The OPIEC-Linked corpus shows a drop in quantity annotations due to linked phrases lacking quantities. It has a higher fraction of spatial triples, attributed to Wikipedia's location pages. In OPIEC-Clean, around 42% of arguments are recognized as named entities, with person being the most frequent NER type. In OPIEC-Clean, 58% of arguments are not typed, mostly concepts not recognized by the NER system. The top-10 most frequent arguments not typed are film, population, city, village, father, song, company, town, album, and time. 18% of triples have two typed arguments, 66% have at least one typed argument, and 34% do not have recognized named entity arguments. Tab. 3 shows the most frequent open relations between arguments recognized as persons and/or locations. The most frequent relation between persons is \"have\", which is highly polysemous, while other relations like \"marry\" and \"be son of\" are less ambiguous. The OPIEC corpora provides all open relations between recognized argument types along with their frequencies and confidence scores for correct extraction. Evaluation showed that 71% of triples were correctly extracted, with precision calculated within ten equi-width intervals. The precision within ten equi-width intervals was calculated for various relations, with a high correlation between confidence score and precision. OPIEC-Clean and OPIEC-Link had a larger fraction of high-confidence triples compared to the raw OPIEC corpus. The OPIEC corpus includes triples with lower confidence scores, which may still provide valuable information for downstream applications. A comparison is made between OIE triples in OPIEC, DBpedia BID0, and YAGO BID17 knowledge bases to assess complementarity. Various relations like \"MusicalArtist spouse\" are analyzed for information extraction. The OPIEC corpus contains triples with lower confidence scores, which can still be valuable for downstream applications. The alignment of OIE triples from OPIEC-Linked to YAGO is discussed, focusing on statistics that do not require full disambiguation of open relations. The OPIEC corpus contains triples with lower confidence scores, valuable for downstream applications. 29.7% of OIE triples in OPIEC-Linked have a KB hit in DBpedia or YAGO, with most having exactly one hit. 70.3% of linked triples do not have a KB hit. Tab. 4 shows the most frequent open relations aligned to DBpedia relations. The OPIEC corpus contains triples with lower confidence scores, valuable for downstream applications. 29.7% of OIE triples in OPIEC-Linked have a KB hit in DBpedia or YAGO, with most having exactly one hit. 70.3% of linked triples do not have a KB hit. The frequencies of open relations do not directly correspond to KB relations, as open relations can be ambiguous or more specific than KB relations. The top-100 most frequent open relations constitute roughly 38% of the OPIEC-Clean corpus, showing skewed relation frequencies. OPIEC-Linked is used as a proxy for the number of DBpedia hits. The OPIEC corpus contains triples with lower confidence scores, valuable for downstream applications. OPIEC-Linked is used as a proxy for the number of DBpedia hits of relations, showing that there is a substantial amount of information present in OIE triples not found in KBs. Open relations like \"have\" align with multiple DBpedia relations, indicating they should not be directly mapped to KB relations. The most frequent open relations in OPIEC-Clean are \"be\" and \"have\", which are aligned with numerous DBpedia relations. The OPIEC corpus contains triples with lower confidence scores, valuable for downstream applications. It is noted that open relations like \"have\" align with multiple DBpedia relations, indicating they should not be directly mapped to KB relations. YAGO provides date facts and meta-facts, which correspond to temporal and spatial annotations in OIE triples. OPIEC contains spatial or temporal triple annotations, with 645,525 triples selected for comparison with YAGO date facts. Out of these, 2,613 are temporal and 2,629 are spatial. Analysis reveals reasons for spatio-temporal information absence in KBs, such as missing or indirectly available data. An example is the OPIECLinked triple (\"Iain Duncan Smith\", \"is leader of \", \"Conservative Party\") with temporal annotations (pred=\"from\", 2001) and (pred=\"to\", 2003), where YAGO lacks this information. YAGO contains KB hits with temporal annotations, but they are less specific and lack temporal information compared to OIE data. The low number of KB hits indicates that OIE data provides valuable spatial and temporal information that is potentially very valuable. The OPIEC dataset contains valuable spatial and temporal annotations for automated KB completion tasks. More than half of the triples in OPIEC-Linked without a KB hit refer to top-100 relations in OPIEC-Clean, indicating additional facts not present in DBpedia. Disambiguation is a challenge, with 60% of randomly sampled triples considered correctly extracted. In a sample of high-confidence triples, 80% were correctly extracted, highlighting the potential and challenges of utilizing OIE knowledge. The OPIEC corpus, extracted from Wikipedia, contains hundreds of millions of triples with rich metadata. A data profiling study showed that most open facts in OPIEC do not overlap with DBpedia and YAGO knowledge bases, providing complementary information. Open relations in OPIEC are often more specific, generic, or correlated to KB relations. The corpus and its derived statistics are valuable for automated KB construction and downstream applications. The OPIEC corpus, extracted from Wikipedia, contains millions of triples with rich metadata, providing valuable data for automated KB construction and downstream applications. An independent study demonstrated the utility of OPIEC in entity-aspect linking."
}