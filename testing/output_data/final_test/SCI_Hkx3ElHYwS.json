{
    "title": "Hkx3ElHYwS",
    "content": "Network quantization is a model compression technique essential for neural network deployment. A new technique allows training quantization-friendly networks directly convertible to accurate quantized networks without fine-tuning. This method enables quantizing weights and activations to 4 bits, achieving high efficiency and improved accuracy compared to other fully quantized networks. For example, achieving 66.68% top-1 accuracy on ImageNet using ResNet-18, surpassing previous state-of-the-art accuracy of 61.52%. Neural network quantization is a technique to reduce the size of deep networks by using efficient integer arithmetic on quantized versions of model weights and activations. Extensive experiments were conducted to test the efficacy of the method, including ablation studies to improve training stability and accuracy. The codebase and trained models are available on GitHub. Research on network quantization has been intensive in recent years, with works focusing on quantizing parts of the network while leaving some operations unquantized. Neural network quantization reduces network size by using efficient integer arithmetic on quantized weights and activations. Some works quantize parts of the network, while others quantize the entire network, termed strict quantization. Strict quantization is more challenging to maintain accuracy. State-of-the-art 4-bit networks incur large accuracy losses compared to full precision. State-of-the-art 4-bit networks suffer significant accuracy drops compared to full precision models. Training low-bitwidth strictly quantized networks involves quantizing pre-trained models and fine-tuning with STE for gradient updates, facing challenges due to the initial model not being quantization-friendly. Fine-tuning a model, especially at low bitwidths, is challenging due to the lack of accurate gradient information. The process may require substantial changes to transform the initial model into an accurate quantized model. Fine-tuning using Straight-Through Estimator (STE) involves updating a model internally represented with floating point values using gradients computed at the nearest quantizations. GQ-Net is a guided quantization training algorithm designed to address the challenges of fine-tuning models at low bitwidths. It aims to produce an accurate and quantization-friendly full precision model by minimizing error with training labels and distributional differences between model outputs and quantized versions. This approach guides the optimization process towards an accurate model suitable for quantization. GQ-Net aims to optimize a model that is accurate and quantization-friendly by minimizing error and distributional differences. The approach leads to a quantized model that is accurate and does not require further fine-tuning, reducing training time significantly. GQ-Net's technique is independent and can be used with other quantization methods. The guided training technique can be applied to network pruning. GQ-Net outperforms existing methods in quantization accuracy, even with 4-bit weights and activations. The models are publicly available and validated on ImageNet with ResNet-18 and MobileNet-v1/v2. GQ-Net achieves high accuracy using 4-bit quantization on ResNet-18 and MobileNet-v2, outperforming previous methods. The technique utilizes layer-wise quantization, enhancing efficiency. Neural network quantization has been extensively studied, with various approaches like binary, ternary, or multi-bit quantization for weights, activations, and gradients. Various works have quantized model weights, activations, and gradients to reduce model size and accelerate training. Methods include binary, ternary, or multi-bit quantization, as well as techniques like layer-wise quantization. Training methods like straight through estimators have also been used to improve network quantization. Several works have focused on quantizing all parts of a network to support deployment using only integer arithmetic units and avoid the complexity of additional floating point units. This includes methods such as binary, ternary, or multi-bit quantization, as well as techniques like layer-wise quantization. Some approaches involve re-calibrating or modifying the original network to maintain accuracy with minimal data. Several works have focused on quantizing all parts of a network to support deployment using only integer arithmetic units and avoid the complexity of additional floating point units. Gysel et al. (2016) proposed network inference using dynamic fixed-point arithmetic based on model weight distribution. Jacob et al. (2018) and Krishnamoorthi (2018) introduced quantization training algorithms for Tensorflow-Lite, generating strictly quantized networks for hardware implementation. Louizos et al. (2019) suggested a training method for strictly quantized models. Recent work includes parameterized quantizers with learnable upper bounds and basis for fixed-point arithmetic execution. Jung et al. (2018) proposed optimizing weight scaling. The GQ-Net architecture proposed by Jung et al. (2018) involves optimizing weight scaling and quantization ranges jointly from task losses. It includes an L-layer neural network with computations in floating point arithmetic, quantizers for weights and activations, and methods to optimize these parameters. The GQ-Net architecture proposed by Jung et al. (2018) involves optimizing weight scaling and quantization ranges jointly from task losses in an L-layer neural network. The network quantizes weights and inputs iteratively, computes quantized activations using integer or fixed point arithmetic, and constructs a loss function incorporating both training loss and a loss capturing the difference between the full precision model and the quantized model. The GQ-Net architecture optimizes weight scaling and quantization ranges in an L-layer neural network. It involves quantizing weights and inputs iteratively, computing quantized activations, and minimizing a loss function that pushes the floating point and quantized models to behave similarly. Training involves taking mini-batches of samples and labels to update the parameters and minimize the loss. The GQ-Net architecture optimizes weight scaling and quantization ranges in an L-layer neural network by iteratively quantizing weights and inputs. Weight scheduling for L f and L q parameters captures the relative importance of cross entropy and KL divergence errors during training. Experimentation showed that dynamically modifying the values of \u03c9 f and \u03c9 q during training can produce higher accuracy than using static values. Modifying values of \u03c9 f and \u03c9 q during training can lead to higher accuracy than using static values. A schedule alternating between setting \u03c9 f and \u03c9 q several times can drive the model to a high accuracy region before finding a quantization-friendly model. This approach results in better performance than static schedules by reducing interference between L f and L q in the loss function. During training, adjusting \u03c9 f and \u03c9 q values can improve accuracy. Alternating between these values multiple times helps the model reach a quantization-friendly state, reducing interference between L f and L q in the loss function. Optimizing L results in reduced accuracy due to conflicting updates to W from gradients of L f and L q terms. A heuristic approach based on updating W to minimize L f and independently updating \u0174 to minimize L q has shown to enhance accuracy. The GQ-Net utilizes layer-wise linear quantizers with learnable boundaries for weights and activations. Parameters include quantization bitwidth and upper/lower quantization boundaries. Uniform quantization is preferred for hardware implementation. The GQ-Net uses layer-wise linear quantizers with learnable boundaries for weights and activations. Gradients propagate through the non-differentiable operator using the straight through estimator during training. Alternatingly optimizing weights and parameters improves accuracy, as jointly optimizing them resulted in unstable training. Batch normalization is crucial in large model training, but for GQ-Net which trains both floating point and quantized models simultaneously, adjustments are needed. Activations from these models follow different distributions, so normalizing them with the same statistics can hinder training. Instead, treating activations of different numerical precision as from different domains, similar to multi-domain, is more effective. To improve GQ-Net's accuracy, activations of different numerical precision are treated as from different domains and normalized with separate statistics. This adjustment minimizes storage overhead and enhances performance in both full-precision and quantized settings. Validation experiments on ImageNet using ResNet-18, MobileNet-v1, and MobileNet-v2 architectures show improved top-1 and top-5 accuracies. Unlike recent works, the order of Conv, BN, and ReLU layers remains unchanged. We replaced BatchNorm layers with SyncBatchNorm in models like ResNet-18, MobileNet-v1, and MobileNet-v2. Quantization was done using parameterized linear quantizers with a bitwidth of 4. Weight and activation quantizers were initialized differently. All layers, including the first and last, were quantized. Training protocol remained the same. Training was conducted on 32 distributed GPUs with a mini-batch size of 64, stopping after 120 epochs. Model weights were optimized using the Kaiming-normal scheme and SGD with Nesterov momentum. The learning rate warmed-up from 0.2 to 0.8 and decayed twice by a factor of 0.1. Quantization parameters were optimized using Adam without weight decay. GQ-Net was validated by comparing it with other quantization methods like White Paper, Integer-only, and RelaxedQuant. Results were evaluated for full precision accuracy using different training protocols for ResNet-18. Our method outperforms state-of-the-art quantization methods for ResNet-18 and MobileNets models, achieving higher accuracy with lower bitwidths. For example, our 4-bit model surpasses comparison methods even at higher bitwidths, with significant improvements in top-1 accuracy. The effectiveness of GQ-Net components was also validated, showcasing its superiority in network compression. The effectiveness of GQ-Net components was validated by progressively adding them to the vanilla quantization-friendly training protocol for ResNet-18. By updating model weights and quantizer parameters alternately, quantization accuracy improved by +4.24% over the vanilla protocol. Combining gradients in training steps led to suboptimal quantized weights. Dynamic adjustment of weights improved quantization accuracy by +0.23%. ResNet-18 and MobileNet-v2 implementations were used, with GQ-Net components progressively applied for improved accuracy. The GQ-Net components were progressively applied in the training process, with dynamic adjustments made to weights for improved quantization accuracy. Blocking the gradient from L q caused by \u03c3(x L ) further enhanced quantization accuracy by +0.66%. Using different running statistics for full precision and quantized activations in BatchNorm layers improved accuracy. Parameterized quantizers were replaced with fixed quantizers, resulting in improved quantization accuracy. In this paper, GQ-Net is introduced as a method for training accurate quantized neural networks. It balances full precision accuracy and similarity between full precision and quantized models to optimize the process. By tuning the weights of these factors, GQ-Net achieves fully quantized networks with high accuracy surpassing the state of the art. Future work includes further adjustments to GQ-Net components, combining it with other quantization techniques, and applying similar methodologies to other neural network optimization problems."
}