{
    "title": "SygvZ209F7",
    "content": "The backpropagation algorithm is considered biologically implausible due to its requirement for symmetric weight matrices in the feedforward and feedback pathways. Two biologically-plausible alternatives, feedback alignment and sign-symmetry, have shown comparable learning capabilities to BP on small datasets. However, recent studies have found that these alternatives perform significantly worse than BP on larger datasets like ImageNet. Sign-symmetry and feedback alignment were tested on ImageNet and MS COCO datasets with different network architectures. Networks trained with sign-symmetry showed classification performance close to BP-trained networks, setting a new benchmark for biologically-plausible learning algorithms. The debate continues on whether deep learning models truly reflect how the brain learns, given the challenges of implementing backpropagation in the brain. Backpropagation, a key component in deep networks, is challenging to implement in the brain due to hardware constraints. However, research has shown that relaxing weight-symmetry requirements in error backpropagation can still lead to effective learning in neural networks, even outperforming traditional backpropagation on datasets like MNIST and CIFAR. The success of asymmetric feedback algorithms like \"sign-symmetry\" and \"feedback alignment\" suggests that learning can occur even with inaccurate estimation of error derivatives. Feedback alignment proposes that feedforward weights align with random feedback weights for learning signals. However, recent research indicates that these biologically-plausible algorithms do not generalize well to larger datasets like ImageNet and perform worse than backpropagation. The study by Bartunov et al. found that feedback alignment performed poorly on locally-connected networks due to restrictions on weight sharing. They did not test sign-symmetry, which may be more effective than feedback alignment. The current study re-evaluates the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using standard ConvNet architectures. The study re-evaluates the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using standard ConvNet architectures, finding that sign-symmetry can train networks on both tasks effectively. Additionally, partial feedback alignment in the last layer can achieve better performance than backpropagation. These results suggest that biologically plausible learning algorithms are viable for training artificial neural networks and modeling brain learning mechanisms. The text discusses how learning occurs in the brain through feedforward and feedback steps in neural networks. It introduces equations describing the computation process and different approaches like sign-symmetry and feedback alignment for weight matrices. These methods aim to mimic biological learning mechanisms in artificial neural networks. Alignment BID17 uses a fixed random matrix as the feedback weight matrix B. Through training, W is adjusted so that eTWBe > 0, where e is the error in the network's output. This condition implies that the error correction signal Be lies within 90\u00b0 of eTW. Different training settings were tested on ResNet-18 BID6 on ImageNet, including backpropagation, sign-symmetry, and feedback alignment for convolutional and fully-connected layers. The code for these algorithms is available on GitHub. In sign-symmetry and feedback alignment, feedback weights were initialized differently. Standard training parameters were used for backpropagation, ResNet-18, and AlexNet with various learning algorithms. AlexNet was slightly modified for experiments. In experiments using sign-symmetry and feedback alignment, feedback weights were initialized differently. A batch size of 256 and a learning rate decay of 10-fold every 10 epochs were used for training for 50 epochs. Sign-symmetry performed nearly as well as backpropagation, while feedback alignment outperformed previous results. Despite not accurately propagating error gradients, sign-symmetry only slightly underperformed backpropagation on a large dataset. The skip-connections in ResNet help prevent gradient degradation. Sign-symmetry performed well in a modified AlexNet without skip connections. Feedback alignment showed better learning than previous results. In a network trained with sign-symmetry, the feedback weight always tracks the sign of the feedforward weight, potentially reducing the burden on the feedforward weight. Batch-Manhattan SGD was proposed to stabilize training with asymmetric feedback algorithms, but standard SGD worked better for sign-symmetry. Future experiments are needed to draw stronger conclusions on the effects of Batch-Manhattan. The experiment assessed the effectiveness of sign-symmetry in training complex object detection networks using RetinaNet on the MS COCO dataset. Three training settings were compared, including backpropagation for all layers, backpropagation for the last layer in both subnets and sign-symmetry for the rest of the layers, and backpropagation for the last layer in both subnets and feedback alignment for the rest of the layers. In experiments with sign-symmetry for training object detection networks on COCO, results show a performance gap compared to backpropagation. The network was initialized with a pretrained ResNet-18, trained with SGD, and followed FPN architecture design choices optimized for backpropagation. The study focused on the performance of sign-symmetry in training networks on the COCO dataset. Results showed that alignment angles between feedforward and feedback weights decreased for the last 3 layers but increased for other layers during training. In comparison, backpropagation-trained networks showed an increase in alignment angles for all layers. During training, feedforward weights tend to become sparse and aligned with standard basis vectors, while feedback weights remain on a diagonal. The average kurtosis of feedforward weights increased, indicating more dispersed values. Sign-symmetry and backpropagation led to weights with similar magnitudes, suggesting efficient learning in the network. Further research is needed to understand how sign-symmetry influences learning algorithms. The study found that biologically-plausible learning algorithms like sign-symmetry and feedback alignment can effectively learn on ImageNet. Sign-symmetry outperformed feedback alignment and approached backpropagation in performance. Bartunov et al. did not test sign-symmetry, which may explain the conflicting conclusions. Additionally, Bartunov et al. only tested locally-connected networks on ImageNet, limiting their size due to separate weight storage requirements. During training, alignment angles between feedforward and feedback weights changed in different ways for sign-symmetry and backpropagation. The kurtosis of feedforward weight matrices increased. Model capacity was reduced, potentially affecting feedback alignment performance. Using backpropagation in the last layer improved feedback alignment performance. Sign-symmetry and feedback alignment relax the tight coupling of weights between feedforward and feedback pathways. Feedback alignment requires no relation between the two, while sign-symmetry requires antiparallel connections. Empirically, feedback alignment performance is not ideal on challenging problems. The antiparallel wiring of feedforward and feedback pathways in the brain can be achieved using chemical signals to guide axon targeting. Consistent neuron outputs simplify the network structure, as shown in Figure 4. Incorporating this constraint is important for future biological network models. The antiparallel wiring of feedforward and feedback pathways in the brain can be achieved using chemical signals to guide axon targeting. Figure 4 indicates the relative ease of wiring sign-symmetry in the brain. This hypothesis is falsifiable by experimental data in the near future. Sign-symmetry removes weight magnitude transport but relies on \"sign transport.\" Recent progress in imaging can trace axons longer than 1 mm in large brain volumes. Testing the reality of sign-symmetric wiring in the brain is challenging due to current imaging limitations. However, specific wiring can achieve sign-symmetry in feedback pathways, potentially eliminating the need for \"sign transport\" during learning. Future research could focus on evaluating the capacity of sign-fixed networks to learn. The guidance for establishing an antiparallel wiring pattern in the brain involves specific receptor-ligand recognition. By expressing receptors and ligands in a certain pattern, a sign-symmetric feedback network can be created. This network can be implemented using orthogonal ligand-receptor pairs, with each synapse being exclusively positive or negative. Only a few pairs are needed to implement all possible connectivities in this scheme. These ideas present testable hypotheses for brain wiring. The biological implementation of feedback alignment and sign-symmetry involves synaptic connections between neurons A and B being updated based on the activity of a third neuron C representing the error of neuron B. Neuron C's connection to B with a fixed weight can induce changes in B's electric potential, leading to Long-term Potentiation or Long-term Depression. The biological plausibility of ResNet as an unrolled recurrent network in the visual cortex has been discussed, but the mechanism of backpropagation through time remains unclear. The biological plausibility of implementing backpropagation through time in the brain has been discussed, addressing issues with online learning, recurrent architecture, and training normalization statistics. Various biological constraints such as weight-sharing in convolutional layers, temporal dynamics, spiking neurons, and sample inefficiency in deep learning have been considered. Removing weight transport is seen as a step towards biological plausibility, but further work is needed to develop a truly plausible and empirically-verified model of learning in the brain. Recent work evaluated sign-symmetry and feedback alignment for training ResNet and AlexNet on ImageNet and RetinaNet on MS COCO. Sign-symmetry performed nearly as well as backpropagation on ImageNet, while modified feedback alignment showed improved performance. Both algorithms had reasonable results on MS COCO with minimal tuning, suggesting that biologically-plausible learning algorithms like sign-symmetry remain promising for training artificial neural networks."
}