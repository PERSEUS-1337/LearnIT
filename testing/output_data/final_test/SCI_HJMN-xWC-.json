{
    "title": "HJMN-xWC-",
    "content": "Convolutional neural networks and recurrent neural networks are designed for spacial and sequential data, while standard feed-forward neural networks lack adaptability to feature correlations. A new unsupervised method, Backbone-Skippath Neural Networks (BSNNs), automatically determines network structure from data, resulting in better classification performance with fewer parameters compared to FNNs. BSNNs achieve better classification performance with fewer parameters compared to FNNs. The interpretability of BSNNs is also superior. CNNs and RNNs have made breakthroughs in machine learning tasks, utilizing well-designed network structures and parameter sharing schemes. CNNs connect each neuron to a local region in the input volume, utilizing the local properties of spacial data for effective feature extraction. In contrast to CNN and RNN, FNN's network structure is simple with fully connected layers, leading to high connection redundancies and ignoring correlations in data, weakening the model's strength as a feature extractor. The goal is to learn parsimonious deep feed-forward neural networks with few parameters to prevent overfitting and enable running on mobile devices. Parsimonious FNNs can have flexible structures for different tasks, improving data fitting and interpretability. The narrow fully-connected layers in the Backbone path are Skip-paths with sparse connectivity. Learning parsimonious FNNs is challenging due to the need for sparse connections between layers. Network pruning can help achieve this but starting from a larger network can waste computations. In this paper, data is assumed to be generated by a sparse probabilistic model with multiple layers of latent variables, and the feed-forward network approximates the relationships between observed and latent variables. The method presented in the probabilistic model significantly reduces parameters in FNNs while maintaining good performance in 17 classification tasks. Network structure learning for FNNs has previously relied on constructive algorithms that gradually add neurons to the network, requiring manual design strategies. The authors propose a structure learning method for RBM-like models that automatically determines hidden units and connections between layers. Reinforcement learning and genetic algorithms are also used for learning complex structures in CNNs, but these methods require many training runs. Network pruning is suggested as an alternative to constructive algorithms for network structure learning. Network pruning is a method that starts from a large network and prunes connections or neurons to achieve structure learning. Various pruning algorithms like Optimal Brain Damage and Dynamic Network Surgery have been proposed to efficiently prune connections or neurons in neural networks. One drawback is the time-consuming retraining process involved in some methods. Neurons pruning methods have also been tested, but all pruning methods require starting from a network structure. The main drawback of network pruning methods is the need to start from a larger network than necessary, leading to wasted computations on useless connections or neurons. A new method called Parsimonious Structure Analysis (PSA) introduces a model consisting of a wide, deep, but sparse feed-forward path called the Backbone, and multiple narrow fully-connected paths called Skip-paths, forming the Backbone-Skippath Neural Network (BSNN). PSA addresses the challenge of structure learning for neural networks by learning the Backbone and Skip-paths separately. Structure learning for neural networks is challenging as features may not always have clear relationships like in convolutional networks. In convolutional layers, units are only connected to strongly correlated units in the layer below, reducing parameters. This concept can be applied to feed-forward neural networks, where hidden units should only connect to strongly correlated units below. However, discovering these correlations in feed-forward networks is not easy. In Parsimonious Structure Analysis (PSA), Hierarchical Latent Tree Analysis (HLTA) is used to identify these correlations. In PSA, Hierarchical Latent Tree Analysis (HLTA) is utilized to identify co-occurrence patterns among units and construct hidden units to explain these patterns. PSA aims to discover correlations among input variables and group highly correlated ones together by starting with the two most correlated variables and expanding the group as needed. The process involves computing mutual information between observed variables, selecting the pair with the highest mutual information as seeds for a new variable group, and adding new variables to the group based on their mutual information. PSA adds variables from S to G in descending order of mutual information. Two models are built with G as observed variables: one with a single latent variable and one with two latent variables. BIC scores are computed and compared to determine if the two latent variable model is significantly better. If not, correlations are well modeled with a single latent variable. If the test fails, a finalized variable group is identified from the subtree in the two latent variable model. PSA introduces a latent variable for each group of strongly correlated observed variables. Progressive EM algorithm is used to estimate parameters in the models. The variables in each group are partitioned into disjoint groups, and mutual information among the latent variables is computed. The tree structure for the Backbone path is expanded by adding new connections based on empirical conditional mutual information. The latent tree model is constructed using the Chow-Liu tree method. Parameter estimation is done with the EM algorithm. A deep model is built layer by layer to capture long-range correlations among variables. The deep structure is constructed by converting latent variables into observed ones and learning additional layers of latent variables. The models are stacked to form a three-layer network. The deep structure is built layer by layer to capture long-range correlations among variables, resulting in a three-layer network. The hierarchical latent tree model is constructed using the Chow-Liu tree method, with parameter estimation done through the EM algorithm. Additional links are introduced to model interactions not captured by the tree structure, by considering adding connections for each latent variable to link it to more nodes at the previous level based on correlation strength. The hierarchical latent tree model is expanded using the conditional mutual information to connect the top nodes between layers. The resulting structure is used for the Backbone, but its sparsity may limit capturing global features. To address this, Skip-paths are introduced to the BSNN model structure. The BSNN model structure includes a Backbone path with a sparse structure and Skip-paths for feature extraction. A classification or regression layer can be added on top of all paths for training using back-propagation algorithms. The method is evaluated in 17 classification tasks for non-spatial or non-sequential data. Our proposed method learns the structure of feedforward neural networks from data, reducing model complexity and parameters while maintaining classification performance. The experiment includes 12 chemical compounds classification tasks and 5 text classification tasks using publicly available datasets. The chemical compounds dataset consists of 12,000 environmental compounds with 12 different toxic effects to predict. The study involves predicting 12 toxic effects for chemical compounds through binary classification tasks. Sparse features are filtered out, and remaining features are rescaled. A training set, test set, and validation set are used, with experiments repeated three times. Additionally, 5 text classification datasets are utilized, with the top 10,000 frequent words selected as vocabulary. Experiments are repeated three times, and results are compared with standard models. In the experiment, Backbone-Skippath Neural Network (BSNN) is compared with standard Feed-forward Neural Networks (FNNs) and Pruned FNNs in 17 classification tasks. BSNN keeps only 5% of connections in the Backbone path and limits units in Skip-paths to 100. FNN is a fully-connected neural network with linear layers and activation functions. Pruned FNN is trained by pruning weak connections after training a fully-connected FNN from scratch. The experiment compares Backbone-Skippath Neural Network (BSNN) with standard Feed-forward Neural Networks (FNNs) and Pruned FNNs in 17 classification tasks. BSNN retains only 5% of connections in the Backbone path and limits units in Skip-paths to 100. FNN is a fully-connected neural network with linear layers and activation functions, while Pruned FNN is trained by pruning weak connections after training a fully-connected FNN from scratch. Structure learning for BSNNs is done using PSA, automatically determining the number of layers, hidden units, and sparse connections between layers. The best structure for FNNs is determined through grid-search over hyper-parameters like the number of hidden units and layers. Both \"rectangle\" and \"conic\" network shapes are tested for FNNs. The experiments compare Backbone-Skippath Neural Network (BSNN) with standard Feed-forward Neural Networks (FNNs) and Pruned FNNs in 17 classification tasks. FNNs with conic shape decrease hidden units layer by layer in a geometric progression. Pruned FNNs are obtained by pruning weak connections from the best FNN model. PyTorch 3 is used for all experiments with ReLUs as activation functions and Adam as the optimizer. Validation loss is monitored during training to select models. Classification results on the Tox21 dataset are shown in TAB2. BSNNs achieve competitive performance. The structures of FNNs are tuned individually for each task, with BSNNs achieving better AUC scores on most classification tasks. BSNNs consistently have fewer parameters than FNNs, yet still achieve higher classification accuracy in some tasks and comparable accuracy in others. This confirms that BSNNs can achieve high performance with fewer parameters. The backbone path in BSNNs captures most of the information in data and acts as a main part of the model. Removing narrow skip-paths and training the model shows that the backbone path alone achieves AUC scores or accuracies slightly worse than BSNNs. The number of parameters in the sparse path is much smaller than BSNNs and FNNs. Without the backbone, the model's performance is significantly worse. The results highlight the importance and effectiveness of the backbone path in BSNNs. The effectiveness of the backbone path in BSNNs is highlighted by the slightly worse performance of the backbone path alone compared to BSNNs. The new model BSNN-FC maintains similar classification results to BSNN despite keeping only 5% of connections in the sparse path. Pruned FNNs are also compared, showing that our structure learning method successfully removes unnecessary connections in BSNN-FC. The comparison between BSNNs and pruned FNNs shows that BSNNs have higher AUC scores in most classification tasks. The interpretability of hidden units in BSNNs and FNNs is measured by sorting words based on correlations with hidden units. The similarity between pairs of words in the top-10 list characterizing a hidden unit is used to measure interpretability. The interpretability of hidden units in BSNNs and FNNs is measured using a word2vec model trained on Google News datasets. The interpretability score is defined as the average similarity of characterizing words, with higher similarity indicating similar contexts. Sogounews dataset is excluded due to Chinese vocabulary. Top-layer hidden units are considered for comparison of interpretability scores among different models. BSNNs outperform FNNs and Pruned FNNs in interpretability, showing superior coherency and compactness in characterizing hidden units. Pruned FNNs reduce interpretability. Qualitative results show meaningful hidden unit characterizations in BSNNs for different datasets. The proposed BSNNs show better model interpretability compared to standard FNNs. The unsupervised structure learning method utilizes correlation information to create parsimonious deep feed-forward networks with fewer parameters while maintaining good classification performance. This approach also improves model interpretability, addressing an important issue in deep learning. The proposed method improves model interpretability in deep learning by creating parsimonious networks with correlation information. Future work will extend this approach to other network types like RNNs and CNNs."
}