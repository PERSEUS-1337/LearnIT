{
    "title": "BJl6t64tvr",
    "content": "Using adaptive gradient methods in machine learning is believed to negatively impact generalization. This belief is being re-examined both theoretically and experimentally, considering recent insights and trends. Adaptive optimizers in deep learning have been extensively studied and are considered essential for optimization. Recent experiments show that improved training performance with adaptive optimizers does not necessarily lead to overfitting, especially in modern deep learning settings. A \"user's guide\" to adaptive optimizers is proposed, including modifications to AdaGrad to address its empirical shortcomings. These methods scale step sizes based on observed gradients, leading to accelerated optimization and robustness to hyperparameter choices. In recent years, there has been a debate between adaptive gradient methods and \"vanilla\" stochastic gradient descent in deep learning. While SGD may be slower to converge, it is argued to find solutions that generalize better. This has caused a divide between academic research and practitioners, with the latter group still valuing adaptive gradient methods as an important tool in deep learning. This work aims to revisit the generalization performance of adaptive gradient methods empirically and explore overlooked factors. Our work investigates the generalization performance of adaptive gradient methods, addressing factors that can impact optimization trajectory and potentially bridge performance gaps with SGD. We discuss the inconsistent evidence for a generalization penalty and highlight the brittleness of simple experiments and abstractions. The study replicates experiments from Wilson et al. (2017) and finds that they are affected by unknown hardware and software differences. Adaptive methods can either fail or succeed dramatically compared to SGD in certain theoretical settings. The innocuous initial accumulator value hyperparameter can destroy adaptivity at smaller parameter scales. Using the proposed \"\u03b5 = 0\" variant of AdaGrad is recommended for large-scale NLP. Differences between Adam, AdaGrad, and RMSprop are not fundamental, and with enough tuning, all three can be equally good candidates in optimizer search. Adaptive regularization algorithms like AdaGrad, RMSprop, and Adam have been widely cited and are commonly used in deep learning frameworks. Recent literature shows that adaptive methods, particularly Adam, remain relevant in various fields such as NLP and deep generative modeling. These algorithms have been extensively studied and continue to be considered as strong candidates in optimizer search. Recent works have focused on quantifying the generalization properties of SGD, including stability, implicit bias, and fine-grained analysis for neural networks. There is also growing interest in understanding the interpolation regime for overparameterized function fitting, with SGD being a key focus. Recent works have focused on quantifying the generalization properties of SGD, including stability, implicit bias, and fine-grained analysis for neural networks. Empirical questions on the generalization of adaptive gradient methods have been raised, with suggestions to switch from Adam and SGD during training. Various studies have highlighted issues with adaptive methods like Adam failing to converge in certain settings, leading to the development of alternative approaches like \"superconvergence.\" Sociological problems in research on optimizer selection have also been noted, prompting the creation of benchmarking suites for fairer hyperparameter searches. In a stochastic optimization setting, adaptive gradient methods are defined for tasks where the expectation is over a random variable z with an unknown distribution D. The algorithm is given a sample z1,...,zT \u223c D and aims to produce a point w \u2208 Rd with a population loss close to the minimizer. Stochastic optimization methods like Stochastic Gradient Descent (SGD) use iterative first-order optimization with a sequence of iterates w1,...,wT, updating at each step using the stochastic gradient. Adaptive gradient methods automatically adjust to gradient and parameter variations, with a unique step size for each gradient entry. These methods include SGD, momentum, AdaGrad, RMSprop, and Adam, all unified by a common update equation. In the context of adaptive gradient methods like SGD, momentum, AdaGrad, RMSprop, and Adam, various parameter settings are summarized for common optimization algorithms. The discussion includes lesser-known practices that have shown consistent results in large-scale experiments, emphasizing simplicity without adding extra hyperparameters or alternatives. The AdaGrad update includes a parameter for inversions, with the original proposal including for theoretical presentation convenience. In practice, the parameter for inversions in AdaGrad should be tuned depending on the problem instance. Default values in standard implementations tend to be high, potentially leading to AdaGrad behaving like SGD with a fixed learning rate. This can result in loss of adaptive properties, as seen in Figure 4 where second moments of the gradient are small. Choosing a large epsilon in second-moment-based methods may actually be a beneficial feature, allowing for smooth interpolation with SGD. In the NLP setting, it is suggested to remove the hyperparameter for diagonal-matrix adaptive methods and use the Moore-Penrose pseudoinverse for the AdaGrad update instead of the full inverse. This adjustment ensures no movement in coordinates where the gradient has been 0, without affecting AdaGrad's regret guarantees. Further analysis in Appendix B confirms that standard AdaGrad regret bounds remain valid with this modification. AdaGrad, RMSprop, and Adam are key optimization algorithms in machine learning. AdaGrad does not include momentum and uses a per-parameter learning rate based on accumulated gradient squares. RMSprop uses exponential moving averaging, while Adam adds momentum and a bias-correction factor to RMSprop. Implementation of RMSprop can vary, with TensorFlow including momentum and Keras API not. AdaGrad can be extended to incorporate heavy-ball momentum. The Adam optimizer includes bias correction terms that can be seen as an update to the learning rate. For typical values of \u03b21 = 0.9 and \u03b22 = 0.999, the learning rate multiplier resembles an external warmup. Adding a warmup phase externally to Adam complicates hyper-parameter tuning. Instead of using bias correction in Adam optimizer, it is suggested to disable it completely and implement a warmup schedule instead. This warmup schedule helps in adjusting the learning rate during the initial stages of training, especially for adaptive optimizers. The warmup allows the learning rate to increase gradually while the gradient norm decreases sharply. Learning rate decay schedules are crucial hyperparameters for optimizers. While domain-specific schedules have been developed for stochastic gradient algorithms, adaptive algorithms like Adam or RMSprop require an external learning rate decay schedule. This is because they do not have a data-dependent decay like AdaGrad. In experiments, the implicit decay of AdaGrad was sufficient for superior performance in training a machine translation model. However, an external decay rate was needed for training Resnet-50 on ImageNet-2012. The study focused on optimization methods for deep models in natural language processing and image recognition, specifically training a Transformer model for machine translation using the Transformer-Big architecture. The Transformer model with 1024 model dimensions, 8192 hidden dimensions, and 16 attention heads was trained on the WMT'14 English to French dataset. AdaGrad with \u03b5 = 0 and momentum outperformed Adam, while SGD with momentum, plain AdaGrad, and RMSprop performed worse. Adaptivity and momentum were found to be effective in training these models. An \"\u03b5 = 0\" variant of AdaGrad was proposed and empirically motivated for improved performance. The Transformer model was trained on the en\u2192fr dataset using AdaGrad with varying \u03b5 values. Lowering \u03b5 to 10^-7 showed significant convergence improvement. Visualizing the gradient values at step t = 116200 revealed small magnitudes. The choice of \u03b5 is crucial, leading to the removal of the dependency altogether. Additionally, a ResNet-50 architecture was trained on the Imagenet-2012 dataset for image classification. The training setup for the Transformer model includes the use of a TPU v3 Pod with a batch size of 16386. Comparing optimization methods, AdaGrad initially achieves 63.94% test accuracy but improves to 74.76% with an external decay rate added at epoch 50. Tuning the learning rate schedule is crucial for achieving high accuracy in tasks. In replicating experiments from Wilson et al. (2017) on CIFAR-10 classification with a VGG network, the baseline SGD reached 75% test accuracy. Some findings were sensitive to hyperparameter tuning and changes in deep learning software and hardware. The experiments were successfully replicated using the same codebase and hyperparameter settings. In character-level language modeling, optimal hyperparameter settings were successfully replicated but resulted in different conclusions between SGD and Adam. Software version discrepancies may explain the differences. In generative parsing using the Penn Treebank, the model converged with each optimizer, but differences were observed in two settings. The experiments conducted showed discrepancies in optimizer performance and reproducibility issues in deep learning, with non-convergence observed in learning rates for SGD and RMSprop. Additionally, a fatal memory leak was encountered when training with DyNet 2.1 setup. The experiments revealed issues with optimizer performance and reproducibility in deep learning, including non-convergence with certain optimizers. The study highlighted challenges in credible optimizer evaluation, similar to previous research on random-seed tuning. The findings suggest technical complications in generalization, particularly in character-level language modeling with LSTM layers. In the context of deep learning experiments, issues with optimizer performance and reproducibility were revealed, including non-convergence with certain optimizers. The study emphasized challenges in credible optimizer evaluation, similar to previous research on random-seed tuning. The experiments highlighted technical complications in generalization, especially in character-level language modeling with LSTM layers. In a discussion on convex problems, it was noted that the generalization performance of AdaGrad and SGD can vary significantly depending on the instance, emphasizing the nuanced nature of understanding their performance. The construction provided by Wilson et al. (2017) for overparameterized linear regression was briefly reviewed, with the importance of various properties of the precise instance determining which algorithm performs better. The feature vector x is structured with \"dummy\" coordinates set to 1, followed by blocks of coordinates that represent the value of y. AdaGrad explores a solution space within the subspace of the sign vector of X y, treating the first three coordinates equally. However, this approach leads to high generalization error as AdaGrad fails to extract true label information from these coordinates. The example is adapted from the original AdaGrad paper, illustrating challenges in overparameterized settings. In an overparameterized setting, AdaGrad quickly adapts to new vectors by setting the coordinate to 1, reducing training error to 0 after one epoch. The average test error becomes roughly (1 - c). Comparatively, SGD with optimal decay scheme reaches a reduced learning rate after cd/2 steps. SGD with optimal 1/ \u221a t decay scheme reduces the learning rate to O(1/ \u221a d) after cd/2 steps, resulting in a test error of at least \u223c (1 \u2212 c/2) after cd steps. This is lower than AdaGrad's error at the same stage, requiring \u2126( \u221a d) more steps to achieve the same test error as AdaGrad."
}