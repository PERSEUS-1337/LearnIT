{
    "title": "SJfWKsC5K7",
    "content": "This paper presents a method to quantitatively and semantically explain the knowledge encoded in a convolutional neural network (CNN). The study proposes distilling knowledge from the CNN into an explainable additive model to provide a quantitative explanation for the CNN prediction. Experimental results demonstrate the effectiveness of the method in analyzing the bias-interpreting problem and guiding the learning of the explainable additive model. In this paper, the focus is on explaining the specific rationale of each network prediction semantically and quantitatively. The goal is to provide clear visual concepts for explaining the logic of each network prediction, aiming to satisfy specific demands in real applications and earn trust from people by diagnosing feature representations inside neural networks. In this study, a new neural network called an explainer network is proposed to explain CNN predictions by clarifying visual concepts that activate the network. This approach aims to decompose the prediction score into components of these visual concepts, addressing challenges faced by current algorithms. Additionally, pre-trained models detecting visual concepts are used to explain the logic of the performer network's predictions. The explainer network is designed to mimic the logic of the performer's predictions by using pre-trained visual concepts. It decomposes the prediction score into multiple value components based on specific visual concepts, providing quantitative semantic explanations for the performer. The explainer network mimics the performer's logic in predictions using visual concepts, providing quantitative explanations without ground-truth annotations. It serves as a semantic paraphrase of the performer's feature representations, helping understand the logic behind predictions. The explainer may not fully replicate the performer's prediction score due to representation capacity limits. The explainer network aims to explain the performer's predictions using visual concepts. Challenges arise from bias-interpreting, where the explainer may selectively choose visual concepts. To address this, two types of losses for prior weights of visual concepts are proposed to guide the learning process. Our \"semantic-level\" explanation for CNN predictions differs from traditional studies of neural networks, focusing on explaining the performer without compromising its discrimination power. This approach resolves the conflict between interpretability and discrimination power of features. Our strategy of explaining the performer resolves the conflict between interpretability and discrimination power. Quantitative explanations have practical value in critical applications, helping to earn trust. Clarifying the logic of network predictions can diagnose feature representations and identify problematic features. The study focuses on explaining CNN predictions using a new method that distills knowledge from a pre-trained performer into an interpretable explainer. Novel losses are developed to address biasinterpreting problems, with promising experimental results. The proposed method offers a generic solution for interpreting neural networks and has been successfully applied to various benchmark CNNs for different applications. The paper discusses the visualization of feature representations in neural networks to understand their inner workings. Various techniques like gradient-based visualization and up-convolutional nets have been used to analyze network inputs and outputs. Network-attack methods have also been employed to diagnose CNN blind spots and biased representations due to dataset bias. The paper discusses techniques for visualizing feature representations in neural networks to understand their inner workings. Previous methods analyzed neural networks at the pixel level, but did not summarize network knowledge into clear visual concepts. BID1 defined six types of semantics for CNN filters, while BID41 proposed a method to compute image-resolution receptive fields of neural activations. Other studies retrieved middle-layer features representing clear concepts, such as objects, scenes, and colors. Unlike previous studies, the focus is on analyzing the quantitative contribution of each semantic concept to predictions. The scope of network interpretability is to learn interpretable feature representations in neural networks in an un-/weakly-supervised manner. Various models like Capsule nets and interpretable RCNN have learned interpretable features in intermediate layers. InfoGAN and \u03b2-VAE have learned disentangled codes for generative networks. Interpretable CNNs have learned filters in intermediate layers to represent object parts. However, interpretable features may lack discrimination power. Explaining neural networks via knowledge distillation is a recent direction, with BID40 using a tree structure to summarize CNN predictions. In contrast to previous models, our research focuses on quantitatively explaining each CNN prediction. We distill knowledge from a pre-trained performer to an explainable additive model, overcoming bias interpretation challenges. By learning n neural networks pre-trained to detect visual concepts, we successfully explain network knowledge using human-interpretable semantic concepts. Our method involves training n neural networks to detect visual concepts and explain prediction scores of a performer without needing annotations on training samples. The goal is to use inference values of visual concepts to interpret the performer's output. The method involves training neural networks to detect visual concepts and explain prediction scores without annotations. An additive explainer model decomposes prediction scores into components of visual concepts, providing quantitative contributions to the final prediction. The explainer may not recover all information from the performer. The explainer model aims to estimate weights that minimize the prediction score difference between the explainer and the performer. Different input images result in different weights, reflecting various decision-making modes of the performer. Another neural network, the explainer, is designed to estimate these weights using the input image. The explainer is trained with knowledge-distillation loss to learn the distribution of visual concepts. To address bias-interpreting issues in learning, a loss function is used to guide the process by incorporating prior weights that represent the relationship between the performer's prediction and visual concepts. These prior weights are approximated with assumptions and penalize the difference between the weights and the weight distribution. Different input images have different prior weights to avoid significant loss. In order to address bias-interpreting issues in learning, a loss function is utilized to guide the process by incorporating prior weights that represent the relationship between the performer's prediction and visual concepts. These prior weights are approximated with assumptions and penalize the difference between the weights and the weight distribution. Different input images have varying prior weights to avoid significant bias. Two different ways of computing the weights are introduced, with a decreasing weight for the loss function in early epochs and a shift towards distillation loss for higher accuracy in later epochs. Two types of losses for prior weights are designed, one using cross-entropy and the other using MSE loss. In certain applications, a non-linear activation layer is added to ensure \u03b1 i \u2265 0, with \u03b1 = log[1 + exp(x)]. Two techniques are introduced to compute rough prior weights w efficiently. In Case 1, interpretable filters in intermediate conv-layers of the performer are discussed, focusing on learning CNNs for object classification with exclusive triggers for specific object parts. The text discusses interpreting the score of an object using elementary scores for object parts in CNNs. It focuses on annotating object parts represented by filters and learning the additive contribution of each filter to the performer output. The network prediction is represented using feature maps and activation units in the conv-layer. Confidence in detecting object parts is measured using the Jacobian. The text discusses approximating filter weights in CNNs using the Jacobian and normalization. It also explores interpreting network predictions by sharing features among visual concepts in intermediate layers. The text discusses approximating filter weights in CNNs using the Jacobian and normalization. It also explores interpreting network predictions by sharing features among visual concepts in intermediate layers. We estimate a numerical relationship between \u0177 and the score of each visual concept y i using a Taylor series. Two experiments were conducted to interpret different benchmark CNNs for various applications, demonstrating the broad applicability of the method. The first experiment explained object part detection, while the second explained face attribute prediction. Explanations obtained were evaluated qualitatively and quantitatively. In the experiment, CNNs were used to classify target animals using interpretable filters in the top conv-layer. Different types of CNNs, including AlexNet and VGG networks, were utilized with varying numbers of filters in their top conv-layer. Skip connections in residual networks made learning part features more challenging. In the experiment, CNNs were used with different numbers of filters in the top conv-layer to classify target animals. The masked output of the top conv-layer was used to compute {y i }. The 152-layer ResNet BID10 3 was used to estimate weights of visual concepts. Positive images were used to represent object parts of the target category. Classification accuracy and relative deviations of the explainer and performer were measured. Negative prior weights were filtered out to ensure a positive relationship between \u0177 and y i. Cross-entropy loss was applied to learn. The study used CNNs with varying filter numbers in the top conv-layer to classify animals. The ResNet model estimated weights of visual concepts, and cross-entropy loss was applied to learn the explainer. Evaluation included correctness of estimated explanations and measuring bias using entropy of contribution distribution. Visualizations like grad-CAM were used to validate explanations. The study proposed metrics for evaluating the performer's information not captured by visual concepts, including prediction accuracy and relative deviation. The relative deviation on images reflected inference patterns not modeled by the explainer, with average deviation across all images used to assess visual concept representation power. In an experiment, a CNN based on VGG-16 was trained on the CelebA dataset to estimate 40 face attributes. A target attribute was selected from global features like attractive, heavy makeup, male, and young. Explainers were created for each target attribute using a 152-layer ResNet structure. In Experiment 2, a 152-layer ResNet structure was used to estimate weights for the attractive attribute prediction. Prior weights were computed using the Case-2 implementation with a 4096-dimensional output as the shared feature. The L-2 norm loss was used to learn explainers, and the evaluation metric from Experiment 1 was utilized. The figure illustrates quantitative explanations for object classification and face-attribution prediction by performers, showing contributions of different object parts. The figure illustrates contributions of different object parts to the classification score. Object parts all made positive contributions, with bars indicating elementary contributions from features of face attributes. A comparison was made between the method used and a traditional baseline, showing less bias-interpreting issues with the method. Table 2 evaluates the representation capacity of visual concepts using classification accuracy and relative deviations of the explainer. Our method demonstrated less bias-interpreting issues compared to the baseline, as shown in Fig. 3 and Fig. 9 in the appendix. We focused on explaining CNN predictions semantically and quantitatively, distilling knowledge from a pre-trained performer. Our method involves distilling knowledge from a pre-trained performer into an interpretable additive explainer. The explainer decomposes the performer's prediction score into value components from semantic visual concepts to compute quantitative contributions. This strategy maintains the discrimination power of the performer and has been applied to various benchmark CNN performers in preliminary experiments. The objective is not to achieve super accuracy in classification but to mimic the logic of the performer and produce similar prediction scores. The biggest challenge is avoiding over-interpretation when using an additive explainer for interpretation. In this study, two losses were designed to address bias-interpreting challenges when using an additive explainer to interpret a neural network. The classification accuracy of the explainer and performer was measured, showing that the additional loss for bias-interpreting successfully overcame the problem without decreasing the explainer's accuracy. Interestingly, the explainer sometimes outperformed the performer in classification, a phenomenon also observed in previous research. The distillation process removed abnormal features, boosting the student network's robustness. BID40 used a tree structure to summarize inaccurate rationales of CNN predictions. The assumption of feature significance being proportional to the Jacobian w.r.t. the feature is problematic. Our method focuses on providing quantitative explanations for specific samples, using an additive model to achieve more convincing results compared to the baseline. We compared contribution distributions of visual concepts between our method and the baseline, showing that our method offers a more reasonable distribution. Additionally, we provided quantitative explanations for object classification by assigning contributions of filters. Our method focuses on providing quantitative explanations for object classification by assigning contributions of filters to object parts. The CNN used similar object parts to classify objects, as shown in the grad-CAM visualization of feature maps. Each filter in the top conv-layer represented a specific object part, annotated based on visualization results. Our method assigns contributions of filters to object parts based on visualization results. The order of layers was changed without affecting performance. The distillation process did not use ground-truth labels for classification, using a threshold for decision boundary instead. The decision boundary for classification was set at \u03c4 \u2248 0 to maximize accuracy, ensuring a fair comparison between the performer and the explainer."
}