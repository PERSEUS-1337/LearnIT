{
    "title": "Sygfa739LS",
    "content": "In compressed sensing, a new sparse signal recovery algorithm using reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) is developed. The algorithm learns how to choose the next support sequentially, outperforming existing methods like orthogonal matching pursuit (OMP). Empirical results demonstrate the superior performance of the proposed RL+MCTS algorithm in reconstructing high dimensional sparse signals from a small number of observations. Recent advancements in machine learning have introduced new approaches to signal recovery algorithms in compressed sensing. Two well-established methods, Orthogonal Matching Pursuit (OMP) and Basis Pursuit (BP), are commonly used as baseline methods for comparison. The proposed RL+MCTS algorithm outperforms OMP and BP in reconstructing high dimensional sparse signals from limited observations. Recent advancements in machine learning have introduced new approaches to signal recovery algorithms in compressed sensing. Works have applied ANNs, RNNs, and generative models like Autoencoder and GANs for encoding and decoding signals. Unlike previous works, the innovation here focuses on signal recovery algorithms rather than signal encoding or measurement matrix design, dealing with general sparse signals. The text discusses signal recovery algorithms in compressed sensing, focusing on general sparse signals. It introduces a reinforcement learning approach using Monte Carlo Tree Search (MCTS) to address the signal recovery problem. Experimental results show that the proposed RL+MCTS algorithm outperforms traditional methods like OMP and BP for various matrix sizes. The approach aims to reduce computational complexity for higher signal sparsity in compressed sensing. The text discusses using RL and MCTS to solve the sparse signal recovery problem in compressed sensing by sequentially choosing columns of A to minimize the 0-loss. The MDP for compressed sensing defines states as pairs (y, S) where y is the observed signal and S is the selected columns of A. Terminal states occur when |S| = k. In compressed sensing, states are defined as pairs (y, S) where |S| = k or ||A S x s \u2212 y|| 2 2 < . The MDP transition is deterministic, and the reward function is defined with fixed hyperparameters. A policy is learned via RL and MCTS to select columns of A for sparse signal recovery. Training data is generated for this purpose. The training data for compressed sensing is generated by creating k-sparse signals x 0 and computing y = Ax 0. A policy network, combined with MCTS, selects columns sequentially until k columns are chosen. This strategy shifts computational complexity from testing to training, similar to previous work. A neural network models the policy and state-value function for the RL+MCTS algorithm in sequential decision making for compressed sensing. The policy \u03c0 \u03b8 (a|s) defines a probability over actions for a state s, with V \u03b8 (s) representing long-term rewards. Input features for the network are designed based on OMP, selecting columns based on the largest component in |\u03bb s |. The RL+MCTS algorithm aims to train the policy network iteratively by selecting the next column index with the largest component in |\u03bb s |. It uses Monte Carlo Tree Search (MCTS) to generate training samples and update neural network parameters. This process is similar to the AlphaGo Zero algorithm. In the training phase, the RL+MCTS algorithm uses MCTS and the current f \u03b8 to generate new training samples. MCTS can also be used in the testing phase to enhance performance by constructing a search tree. To reduce training complexity, a maximum depth d is fixed for the MCTS tree, with the remaining levels rolled out using the OMP rule. In the training phase, the RL+MCTS algorithm uses MCTS and the current f \u03b8 to generate new training samples. MCTS can also be used in the testing phase to enhance performance by constructing a search tree. The remaining levels of the tree are rolled out using the OMP rule to select columns until a total of k columns are chosen. Experimental results for the proposed RL+MCTS algorithm are compared against OMP and BP methods. Training and testing are done on matrices of size 7 \u00d7 15 and 15 \u00d7 50. The RL+MCTS algorithm is evaluated with reduced complexity by expanding the tree to depth d and following the OMP rule until a terminal state is reached. The experiment results for the RL+MCTS algorithm are shown for a 10 \u00d7 100 matrix. Two models are trained: one without tree depth constraint and one with tree depth d = 6, resulting in a 40% reduction in training time per sample. Testing involves selecting columns with and without MCTS, using different tree depths and MCTS settings. The experiment results for the RL+MCTS algorithm show that training with a fixed tree depth of d = 6 yields favorable results compared to OMP and BP. Average prediction times per signal are provided in Table 1, with testing speeds on a less powerful machine than the training setup. The testing speeds can potentially be improved with better hardware. The proposed RL+MCTS algorithm shows promising results compared to existing sparse signal recovery algorithms like OMP and BP. It offers potential for further optimization and research, with the possibility of improving features and leveraging training sample efficiency. In this appendix, hyper-parameters of experiments are included to improve performance and training sample efficiency. Broader settings of problems like noisy observations and varying observation matrices are actively investigated."
}