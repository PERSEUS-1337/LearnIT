{
    "title": "H1cKvl-Rb",
    "content": "We propose leveraging an ensemble of $Q^*$-functions for improved exploration in deep reinforcement learning, adapting bandit algorithms to the $Q$-learning setting. Our exploration strategy is based on upper-confidence bounds (UCB) and shows significant gains on the Atari benchmark. Deep Q-learning (DQN) is a leading technique in learning mappings from high-dimensional observations to actions, but challenges remain in sample efficiency and convergence. Optimal exploration techniques from the bandit setting do not straightforwardly extend to deep RL. Bootstrapped DQN is a previous approach in this domain. Bootstrapped DQN adapts bandit algorithms to deep reinforcement learning by using an ensemble of Q*-functions for exploration. It approximates the posterior over Q* instead of MDPs, using a multi-headed neural network. Empirical findings suggest that different initializations for the heads yield the best performance. The paper introduces new algorithms that improve performance on the Atari benchmark by utilizing the Q-ensemble approach. The algorithms include Ensemble Voting and UCB exploration strategy, which construct uncertainty estimates of Q-values for action selection. The approach is based on bandit algorithms and Markov decision processes. The curr_chunk discusses a Markov decision process (MDP) with state and action spaces, transition and reward functions, discount factor, and initial state distribution. It introduces the Q-function for a policy and the optimal Q* -function satisfying the Bellman equation. Watkins and Dayan's early optimality result in reinforcement learning is also mentioned. Watkins and Dayan (1989, 1992) proved that online Q-learning converges to the optimal policy when every state is visited infinitely. However, Watkins' Q-learning can be slow in MDPs with random exploration. Later work introduced reinforcement learning algorithms with fast convergence. These methods use exploration strategies to encourage the agent to visit new state-action pairs. For example, R-MAX assumes infrequently-visited states offer maximum reward, and delayed Q-learning initializes high Q-values to ensure sufficient exploration. Theoretically sound RL algorithms are not always computationally practical in deep RL. In deep RL, exploration methods like -greedy and Boltzmann are used due to the impracticality of theoretically sound RL algorithms. Some approaches involve constructing an exploration bonus based on density or dynamics models, but a drawback is that the exploration may focus on irrelevant aspects of the environment. Earlier works on Bayesian reinforcement learning, such as BID6 and BID7, studied Bayesian Q-learning in model-free and model-based settings. BID6 learned the distribution of Q * -values through Bayesian updates, while BID7 updated the posterior distribution of the MDP and solved the Q * values at every step. Strens (2000) proposed posterior sampling for reinforcement learning (PSRL), which takes a single sample of the MDP from the posterior in each episode. Recent works have established near-optimal Bayesian regret bounds for episodic RL. Recent works have established near-optimal Bayesian regret bounds for episodic RL, experimenting with low-dimensional problems due to the intractable computational cost for high-dimensional RL. Inspired by PSRL, Osband et al. (2014) proposed randomized least-square value iteration for linearly-parameterized value functions, while Bootstrapped DQN Osband et al. (2016) applies to Q-functions parameterized by deep neural networks. Bootstrapped DQN BID16 maintains a Q-ensemble with a multi-head neural net structure to parameterize multiple Q-functions. Bootstrapped DQN diversifies Q-ensemble through independent initialization and different samples for each Q-function. Q-functions are trained simultaneously with a random mask to update only if nonzero. Target value calculation avoids overestimation using Double DQN approach. Experiments on Atari games show all Q-functions trained with the same samples but different initializations. The ideal Bayesian approach to reinforcement learning is to maintain a posterior over the MDP, but due to limited computation, it is more practical to use a Q-ensemble to approximate the posterior over the Q*-function. Ensemble Voting is the proposed method, where each Q-function proposes an action based on its Q-value, and the agent chooses the action by majority vote. Ensemble Voting is a proposed method in reinforcement learning where each Q-function takes a Bellman update based on a minibatch of transitions. The Q-ensemble {Q k } uses a target network for stability, with differences in parameters only from random initialization. The deep neural network parametrization introduces nonconvexity, causing the Q-ensemble {Q k } to not converge to the same Q-function during training. Bagging by updating each Q k with an independently drawn minibatch led to inferior learning performance. In supervised learning, deep ensembles with random initializations outperform bagging. Ensemble sampling is developed for bandit problems with deep neural network policies. Algorithm 1, using ensemble voting, is shown to be superior to bootstrapped DQN in experiments. In the next section, an optimism-based exploration strategy is proposed using UCB algorithms adapted from the bandit setting. The algorithms maintain an upper-confidence bound for each arm, allowing the agent to optimistically choose the arm with the highest UCB at each time step. At time step t, an arm is pulled based on empirical reward and variance, with UCB exploration in RL. The agent selects actions maximizing UCB incorporating standard deviation. Algorithm 2 includes UCB exploration with hyperparameter \u03bb for control. Performance comparison on Atari games is done using consistent parameters. Algorithm 1 and Algorithm 2 are evaluated on Atari games using UCB exploration with hyperparameter \u03bb. The experiments are conducted on the OpenAI Gym platform with 40 million frames and 2 trials on each game. Performance is compared against Double DQN and bootstrapped DQN to isolate the impact of UCB exploration. In this study, Algorithm 1 and Algorithm 2 are compared using UCB exploration with hyperparameter \u03bb on Atari games. The comparison includes Double DQN and bootstrapped DQN to assess the impact of exploration methods. Ensemble Voting and ucb exploration outperform prior methods in terms of maximal mean reward in Atari games. UCB exploration achieves the highest maximal mean reward among four algorithms in most games evaluated, outperforming Double DQN, bootstrapped DQN, and Ensemble Voting. In comparison with A3C+, UCB exploration achieves the highest average reward in 28 games with significantly fewer training frames. Our approach outperforms A3C+ in 10 games. Comparison results are aggregated into four categories: Human Optimal, Score Explicit, Dense Reward, and Sparse Reward. The Atari games are categorized accordingly. In this section, a posterior update formula for the Q * -function is derived under full exploration assumption, dependent on the transition Markov chain. The posterior update is approximated with Q-ensembles {Q k }, showing that the Bellman equation emerges as the approximate update rule for each Q k. The MDP is specified by transition probability T and reward function R, with a joint distribution considered over (Q *, T). Sampling (s, a) according to a fixed distribution, the corresponding reward r and next state s form a transition \u03c4 = (s, a, r, s) for updating the posterior of (Q *, T). The Q* -function posterior is updated using Bayes' formula, with approximations made due to the high-dimensional space of (Q*, T). The joint posterior of Q* and T is calculated after observing a randomly sampled transition \u03c4. The Q* -function posterior is updated by sampling independently initialized Q* -functions and updating them as more transitions are sampled. The agent chooses actions based on a majority vote from each Qk. The update rule for Qk after observing a new transition is derived, with a lower bound of the posterior calculated. The Q* -function posterior is updated by sampling independently initialized Q* -functions and updating them as more transitions are sampled. The update rule for Qk after observing a new transition involves maximizing the lower-bound of the posterior distribution. To overcome tractability issues, the posterior update is approximated by reusing one-sample next state s from \u03c4 and taking a gradient step on Qk+1. In this section, an \"InfoGain\" exploration bonus is studied, which encourages agents to gain information about the Q* -function. The approach involves updating Q* -functions using an experience replay buffer and sampling transitions for each update. The total information gain is calculated based on the posterior distribution of Q* after observing a sequence of transitions. The Ensemble Voting Algorithm does not maintain the posterior distribution, so an InfoGain exploration bonus is defined to measure disagreement among {Q k}. This bonus is monotonous with respect to the residual information in the posterior distribution. Boltzmann distributions are computed for each Q k, and the average KL-divergence is calculated to determine the exploration bonus. The exploration bonus encourages the agent to explore where {Q k} disagree, controlled by a hyperparameter \u03c1. A temperature parameter T controls sensitivity to discrepancies among {Q k}. Algorithm 3 incorporates InfoGain exploration bonus into Algorithm 2, with varying hyperparameters \u03bb, T, and \u03c1 for each game. Performance of combined UCB+InfoGain exploration is demonstrated in FIG4. Combining UCB and InfoGain exploration in Algorithm 3 with \u03c1 = 1 and T = 1 does not uniformly improve the learning curve, as shown in FIG4. InfoGain exploration has varying impacts on individual games, with UCB exploration sufficient for some games like Demon Attack and Kangaroo, while InfoGain further improves learning in games like Enduro, Seaquest, and Up N Down. The effectiveness of InfoGain exploration depends on the temperature parameter T, which varies across games. FIG5 illustrates the behavior of ucb+infogain exploration with different temperature values, showing that tuning the InfoGain exploration bonus with the appropriate temperature can enhance learning in games requiring extra exploration like ChopperCommand, KungFuMaster, Seaquest, and UpNDown."
}