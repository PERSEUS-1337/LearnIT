{
    "title": "ry_WPG-A-",
    "content": "The study challenges the information bottleneck theory of deep learning, which claims deep networks go through fitting and compression phases related to generalization performance. The research shows these claims are not universally true, as neural nonlinearity plays a significant role in the information plane trajectory. The study challenges the information bottleneck theory of deep learning, showing that compression phase in neural activations is not always related to generalization performance. Nonlinearities like ReLU do not lead to compression. Compression does not necessarily improve generalization. Stochasticity in training does not cause compression. Hidden representations may compress task-irrelevant information concurrently with fitting process. Deep neural networks are widely used for various tasks. This paper examines the dynamics of learning through information theory, focusing on representation learning in deep neural networks. Representation learning in deep neural networks involves each layer acting as a set of summary statistics that contain some information from the input while retaining as much as possible about the target output. The information in hidden layers can be measured during learning, providing insights into the optimization process. The information bottleneck theory sets a fundamental limit on how much input compression and target output information a representation can achieve, serving as a benchmark for comparing different architectures and algorithms. In deep neural networks, trajectories in the information plane consist of two phases: an initial \"fitting\" phase where mutual information increases, and a subsequent \"compression\" phase where mutual information decreases. This compression phase is believed to contribute to the generalization performance of deep networks, possibly due to stochastic gradient descent. Analytical methods and simulations are used to study these phenomena. In Section 2, it is shown that compression in neural networks is mainly due to the tanh activation function, while ReLU does not exhibit compression. The relationship between compression and nonlinearity is discussed, along with the dissociation between generalization and compression. Deep linear networks do not compress during training in the examined setting. The impact of stochasticity on compression is investigated in Section 4. In Section 4, the study investigates if stochasticity in training causes compression in the information plane. Results show comparable compression with both full batch gradient descent and stochastic gradient descent, indicating stochasticity is not the main factor. The phases of stochastic gradient descent occur even in networks that do not compress, suggesting they are not causally related to compression. In Section 5, a scenario with task-relevant and task-irrelevant input dimensions is studied, showing a drop in hidden-layer mutual information with the task-irrelevant subspace. The study found that hidden-layer mutual information with the task-irrelevant subspace decreases during training, while overall information with the input increases. Task-irrelevant information is compressed simultaneously with the boosting of task-relevant information. The importance of noise assumptions in information theoretic analyses of deep learning systems is highlighted, casting doubt on the generality of the IB theory for explaining generalization performance in deep architectures. Changing the activation function can significantly alter a network's trajectory in the information plane. The study replicated the experimental setup of Shwartz-Ziv & Tishby (2017) using a neural network with 7 hidden layers for binary classification. Mutual information of network layers was calculated by binning neuron activations. Compression was only observed in the final classification layer. The study replicated Shwartz-Ziv & Tishby's experimental setup using a neural network with 7 hidden layers for binary classification. Mutual information was calculated by binning neuron activations, showing compression only in the final classification layer. The code was modified to train deep networks with rectified linear activation functions, allowing potentially unbounded positive activities for ReLU networks. Mutual information was calculated by discretizing hidden layer activity into 100 evenly spaced bins. The study replicated Shwartz-Ziv & Tishby's experimental setup using a neural network with 7 hidden layers for binary classification. Mutual information was calculated by binning neuron activations, showing compression only in the final classification layer. The code was modified to train deep networks with rectified linear activation functions, allowing potentially unbounded positive activities for ReLU networks. Maximum activity values were used to discretize the hidden layer activity. The mutual information with the input monotonically increases in all ReLU layers, with no apparent compression phase. Larger networks on the MNIST dataset were also trained, showing that tanh networks compressed but ReLU networks did not. The choice of nonlinearity substantively affects the dynamics in the information plane. The impact of neural nonlinearity on mutual information dynamics is studied using a minimal three neuron network model. A scalar Gaussian input distribution is fed through a weight and passed through a neural nonlinearity to yield hidden unit activity. The mutual information between the binned hidden layer activity and input can be calculated exactly in this setting. The choice of nonlinearity significantly affects the dynamics in the information plane. The impact of neural nonlinearity on mutual information dynamics is studied using a three neuron network model. The probabilities of hidden unit activity landing in specific bins can be calculated exactly for monotonic nonlinearities. Mutual information with the input first increases and then decreases for tanh nonlinearity, but always increases for ReLU nonlinearity. For large weights, the tanh hidden unit saturates, leading to a discrete variable with low entropy. ReLU nonlinearity can also compress information as inputs are binned, causing hidden units to saturate. The issue is that actual mutual information is infinite unless noise is added to the network. The actual mutual information I(h; X) is infinite without added noise to the network's hidden layers. Networks with noise in processing can have finite mutual information. Binning or adding noise to activations is necessary to compute a finite mutual information, but this is arbitrary and not part of the network's operation. The binning procedure can be seen as implicitly adding noise to the hidden layer activity. The binning procedure is crucial for obtaining a finite mutual information value in deterministic systems. However, noise and binning are not present in the networks considered by Shwartz-Ziv & Tishby (2017), raising questions about the influence of noise on generalization performance in deep learning systems. Comparing different architectures may be affected by the addition of noise. The addition of noise in deep learning systems may affect the comparison of different architectures in terms of mutual information. Even networks with identical input-output maps can exhibit varying robustness to noise. Approaches focusing on the weights obtained during training may avoid this issue. When a tanh network saturates its nonlinear units after being initialized with small weights, it enters a compression period impacting mutual information. During training, tanh networks enter a compression phase due to nonlinearity-based saturation, slowing down the process. This compression behavior is attributed to double-saturating nonlinearities and the binning methodology used to calculate mutual information. Other mechanisms, such as highly correlated activations or projecting out irrelevant input directions, could also lead to compression. In a student-teacher setup, a neural network learns to approximate the output of another network, allowing for exact calculation of generalization performance and mutual information without binning. This approach can be applied to deep linear neural networks to study generalization dynamics. In a student-teacher setup, a linear teacher neural network generates input and output examples for a deep linear student network to learn. The teacher weights define the rule to be learned, with a signal to noise ratio determining the strength of the rule relative to approximation error. The student network is trained using a dataset generated by the teacher network, minimizing mean squared error between outputs. The student network is a deep linear neural network with multiple layers and linear activation functions. A deep linear network with linear activation functions computes outputs using a linear function. Despite the simplicity, deep linear networks can learn complex nonlinear patterns. The optimization problem is nonconvex, leading to potential overtraining. The generalization error can be easily calculated, and the mutual information between input and output can be precisely determined due to Gaussian distribution in hidden layers. The deep linear network with linear activation functions computes outputs using a linear function. Gaussian noise is added to the hidden layer activity for calculating mutual information. The network has an input layer of 100 units, 1 hidden layer of 100 units each, and one output unit. The deep linear network with 100 units in the input layer, 1 hidden layer of 100 units each, and one output unit was trained using batch gradient descent on a dataset of 100 examples. The network behaves similarly to a ReLU network, generalizes well with minimal overtraining, and does not exhibit compression. Overtraining is worse when the number of inputs matches the number of training samples, but can be reduced by adjusting the number of samples. In the case of linear networks, overfitting occurs when the number of samples matches the size of the network, with no compression observed in the information plane. In a linear analysis of a generic setting, networks show similar information dynamics but different generalization performance. There are no additional mechanisms causing compression during learning, and generalization can vary widely. Input dimensions have equal variance, and weights are independently drawn. Real-world tasks may involve higher variance inputs that are more relevant. This possibility was not explored in this study. In nonlinear networks, training with 30% of the data led to modest overtraining. The information bottleneck theory suggests that randomness in stochastic gradient descent drives the compression phase during training. SGD optimization has two phases: \"drift\" where gradients' mean is large, and \"diffusion\" where it becomes smaller, signaling compression initiation. The authors propose compression should start after the transition to a low signal-to-noise ratio. During the diffusion phase, weights evolve stochastically following a Fokker-Planck equation to maximize entropy subject to a training error constraint. This leads to maximizing the entropy of inputs given hidden layer activity, with a constraint on mutual information for small training error. The diffusion phase drives weights to a maximum entropy distribution while minimizing conditional uncertainty about inputs given hidden layer activity. However, there is no guarantee that weights sampled from this distribution will maximize the entropy of inputs across different training runs. The stochasticity of SGD is not necessary for compression. Two training procedures are compared: offline SGD and batch gradient descent (BGD). BGD uses the full training dataset without randomness in updates. Information dynamics of tanh and ReLU networks are compared in FIG5. The study compares information dynamics in tanh and ReLU networks using offline SGD and batch gradient descent. Compression in tanh networks is consistent across methods, indicating randomness in training does not significantly contribute to compression. Gradient signal-to-noise ratio transitions during learning are observed, but not causally related to compression. Generalization is discussed in Appendix I with examples from the MNIST dataset and linear networks. Our study explores generalization in linear networks without compression on the MNIST dataset. We investigate the impact of task-relevant and task-irrelevant inputs on information plane dynamics, showing that good generalization can occur by ignoring noise-contributing inputs. The dynamics of a deep linear neural network trained on a task with task-relevant and task-irrelevant inputs show that while there is no overall compression phase, the information specifically about the task-irrelevant subspace does compress over training. This compression occurs simultaneously with fitting to the task-relevant information, suggesting that when a task requires ignoring certain inputs, the information in the task-relevant subspace increases robustly. However, the information specifically about the task-irrelevant subspace does compress after initially growing as the network is trained. This indicates that compression dynamics in the information plane are not a general feature of deep networks, but are influenced by nonlinearities. Compression dynamics in the information plane are not a general feature of deep networks, but are influenced by the nonlinearities employed. Doublesaturating nonlinearities lead to compression, while single-sided saturating nonlinearities like ReLUs do not compress in general. Stochasticity in the training process does not contribute to compression. Generalization performance does not always track information plane behavior, questioning the causal link between compression and generalization. The information bottleneck principle suggests that compression in neural networks may not always be visible in hidden layers and can offer new insights for training algorithms in stochastic networks. This challenges the idea that compression dynamics are a general feature of deep networks, as they are influenced by the type of nonlinearities used. The study investigates compression in neural network layers with specific activation functions. Results using different estimators for tanh and ReLU functions are shown. Double-saturating nonlinearities can lead to compression, while single-sided ones do not. The KDE estimator assumes hidden activity is a mixture of Gaussians, suitable for the analysis. The assumption in the present setting is that input activity is distributed as delta functions at each example in the dataset, with hidden layer activity being a deterministic function of the input. Gaussian noise is added for analysis purposes, resulting in a mixture of Gaussians distribution for the hidden activity. In the present setting, noise is added solely for analysis purposes, not during training or testing. Mutual information with input and output can be calculated based on the dataset and network architecture. Mutual information was estimated using data samples from the test set. The noise variance \u03c3 2 = 0.1 was used to estimate mutual information from data samples in the test set. Results show compression in tanh networks but not in ReLU networks, with less pronounced compression in the KDE method. The network was trained using SGD with minibatches of size 128, and smaller layer sizes were chosen in the top three hidden layers for accurate kernel density estimation. MNIST results are from a single training run, with more detailed results provided in FIG12 for tanh activation and FIG0 for ReLU activation. The evolution of cross entropy loss and mutual information between input and hidden layers is shown in FIG12 for tanh activation and FIG0 for ReLU activation. The mutual information is estimated using nonparametric KDE and binning methods, with the entropy of inputs as an upper bound on mutual information. The fourth row visualizes the dynamics of SGD updates during training, showing the norm of weights, mean updates, standard deviation of updates, and gradient SNR for tanh and ReLU networks. The gradient SNR exhibits a phase transition during training, despite the increase in weights' norms. The norm of weights in each layer increases during training, with a phase transition observed in the gradient SNR. Mutual information between input and layer activity is estimated using nonparametric methods. The nonparametric KDE estimator is used to estimate mutual information between input and layer activity. The gradient SNR shows a phase transition during training, with weight norm dynamics increasing in each layer. Results from the kernel MI estimator using different activation functions are also shown. The softplus function is a smoothed version of ReLU, retaining more information with no compression. The nonparametric MI estimator focuses on compression phenomenon between input and hidden layer activity. The MI between hidden representation and input would be infinite without noise assumptions. The Kraskov method is used instead of specific noise assumptions. The Kraskov method is used to estimate the entropy of hidden representations T without specific noise assumptions. Compression in the layer entropy H(T) indicates compression in mutual information. The Kraskov estimate involves parameters like dimension, number of samples, and distance to nearest neighbors. The entropy dynamics over training for tanh and ReLU networks trained on a dataset with the network architecture in Shwartz-Ziv & Tishby (2017) show compression in tanh layers but not in ReLU layers. The Kraskov estimator yields similar results to binning and KDE strategies in estimating entropy using nearest-neighbor distances. The entropy dynamics of neural networks during training show compression in tanh layers but not in ReLU layers. The necessity of noise assumptions for nontrivial information analysis is discussed, along with issues related to discrete entropy versus continuous entropy. The mutual information between hidden layer activity and input is a key focus. The mutual information between hidden layer activity h and input X is calculated using entropy. For discrete variables, entropy is determined by the probability of symbols. In the case of continuous variables, entropy can be negative or infinite. When h is continuous, the mutual information can be infinite as long as h has finite entropy. The mutual information between hidden layer activity h and input X is calculated using entropy. For continuous variables, entropy can be negative or infinite. To ensure finite mutual information, some noise in the mapping is required. This noise assumption is not present in actual neural networks during training or testing. The mutual information between hidden layer activity h and input X is calculated using entropy. To ensure finite mutual information for continuous variables, noise in the mapping is required. Binning the values of h into a discrete variable T allows for the use of discrete entropy, but the network operates on continuous variables. The choice of binning strategy is similar to adding noise in the continuous case, as it is solely for calculating mutual information. The binning methodology for hidden layer activity involves using evenly spaced bins based on the type of activation function used. For bounded activations like tanh, bins are set between the minimum and maximum limits of the function. For unbounded activations like ReLU, bins are determined after training by identifying the minimum and maximum hidden activation values. This approach allows for accurate mutual information estimation without restricting the activation function's magnitude during training. Another strategy involves binning based on a neuron's net input rather than its activity. The binning strategy for hidden layer activity involves using evenly spaced bins based on the activation function. For tanh, bins are set between the minimum and maximum limits of the function, capturing more information as network weights grow larger. This approach avoids compression in most layers and results in information plane dynamics without exhibiting compression. The implementation of a neural network on digital hardware involves finite precision binning, with high resolution compared to other methods. Fine-grained binning can result in little to no change in information during training. Noise/binning assumptions have consequences for data processing analysis. The implementation of a neural network on digital hardware involves finite precision binning, with high resolution compared to other methods. Fine-grained binning can result in little to no change in information during training. Noise/binning assumptions have consequences for data processing analysis, as the data processing inequality does not apply to noisy/binned mutual information estimates. Compression is observed in the highest and smallest layers near the end of training due to strong saturation of tanh. Markov chain for DISPLAYFORM5 implies I(X; h 1 ) \u2265 I(X; T 2 ). Mutual information is no longer invariant to invertible transformations of hidden activity h when noise is added. Invariance to reparametrizations is lost when noise is present in hidden representations. After adding noise, the mutual information is no longer invariant to reparametrizations of hidden activity. For example, scaling the weights in a network affects the mutual information, even though the function computed remains the same. The mutual information of a network can be affected by scaling the internal layer's weights, even though the function computed remains the same. Neural saturation is related to compression in mutual information, as weights typically increase in size during training to engage nonlinearity like tanh for learning nonlinear tasks. The Rademacher complexity of neural networks with small weights must be low. Networks trained in this paper increase the norm of their weights. The mutual information between hidden representation and output can be calculated. Information plane dynamics for stochastic and batch gradient descent learning in a linear network are shown. Compression in mutual information is related to neural saturation. During training, neural networks switch between two phases based on the gradient signal-to-noise ratio. The \"drift\" phase has a high SNR and no compression, while the \"diffusion\" phase has a low SNR and compression occurs. This phenomenon is related to the two phases of gradient descent described in the literature. During training, neural networks switch between two phases based on the gradient signal-to-noise ratio. The gradient SNR is calculated for each layer, showing a step-like transition to a lower value during training for both tanh and ReLU networks. The norm of the weights is also plotted, revealing a similar qualitative pattern. During training, neural networks switch between two phases based on the gradient signal-to-noise ratio. The SNR undergoes a transition from high to low over training for both tanh and ReLU networks. This two-phase nature of gradient descent holds across different settings examined. The SNR transition is not related to compression, as ReLU networks showing the transition do not compress. A minimal model in a three-neuron linear network further demonstrates the generality of the two-phase gradient SNR behavior independent of compression. The gradient signal-to-noise ratio undergoes a two-phase transition during neural network training, regardless of compression. The weight norm increases as training progresses, even without compression. This behavior is observed in various settings, indicating a consistent two-phase structure in the gradient SNR. The gradient signal-to-noise ratio behavior is not causally related to compression dynamics, indicating that saturating nonlinearities are the primary source of compression."
}