{
    "title": "SkgsQ8LK_E",
    "content": "In this paper, a novel framework is presented to disentangle an object's content and style in an unsupervised manner using a two-branch Autoencoder. One branch focuses on structural content by projecting latent factors into a structured point tensor, while the other branch learns style information. This framework effectively separates content-style representation without human annotation, showing superior disentanglement and visual analogy quality on four image datasets. The research presents a novel framework for unsupervised disentanglement of object style and content using a two-branch Autoencoder. It aims to separate content and style in a latent space without human annotation, achieving superior disentanglement and visual analogy quality on various image datasets. The proposed two-branch Autoencoder framework aims to separate object style and content in a latent space without human annotation. The structural content branch discovers meaningful structural points to represent object geometry, while the style branch learns complementary style representation through asymmetric settings and layer-wise softmax operator. The proposed method uses prior losses to constrain structured point tensors for high repeatability and uniform distribution. A KL divergence loss and skip-connections are introduced for disentangled representation of content and style. Results show effective disentanglement on cat face dataset and superior performance on various datasets. The architecture of the model relies on prior knowledge to constrain learning and disentanglement of structural content information. The loss function is formulated with special consideration on prior, following the VAE framework. The model assumes latent variables z and y are generated from prior distributions, and x follows a conditional distribution. The Bayesian formulation maximizes log-likelihood over observed samples. Our framework, following an Autoencoder architecture, involves two branches: the structural content branch enforces a Gaussian spatial probability distribution, while the style branch learns a complementary style representation. The model addresses intractable integrals by introducing an approximate posterior to estimate using evidence lower bound (ELBO). The final loss is obtained by splitting the p(x|y, z) term, consisting of prior loss L prior, reconstruction loss L recon, and KL loss L KL. The content representation y is structured as a soft latent point tensor and re-projected onto a Gaussian spatial probability distribution space. The hourglass network outputs landmark heatmaps h, with each channel representing a structural point's spatial location. The content representation y is structured as a soft latent point tensor and re-projected onto a Gaussian spatial probability distribution space. Landmark heatmaps h are generated by the hourglass network, with each channel representing a structural point's spatial location. A Separation Loss is introduced to ensure adequate coverage of the object of interest in each heatmap during training. The content representation y is structured as a soft latent point tensor and re-projected onto a Gaussian spatial probability distribution space. Landmark heatmaps h are generated by the hourglass network, with each channel representing a structural point's spatial location. A Concentration Loss is added to encourage the variance of activations to be small, focusing on a single location. The second term optimizes the reconstruction loss of the whole model, denoted as generator G. The work investigates a different prior to disentangle visual content and style. The reconstruction loss for the generator G is designed to improve perceptual quality by incorporating feature information from a VGG-19 model pre-trained on ImageNet. Adversarial loss can also be added for further enhancement, but in this work focused on disentanglement, only the reconstruction loss is utilized. The distribution q(z|x, y) is modeled as a parametric Gaussian by the encoder. The encoder network models q(z|x, y) as a Gaussian distribution to sample the style code z. The prior p(z|y) is estimated by the encoder network as well. By minimizing the KL divergence between q(z|x, y) and p(z|y), z is regularized. The framework ensures z is from x and the Decoder D \u03b8 recovers information of x. To encourage z to learn a complementary of y, the network fuses the encoded content representation with the style code z. The network fuses the encoded content representation with the style code z using skip-connections between the encoder and decoder. This design guides z to encode more style information complementary to content. Input images are cropped and resized to 256x256 resolution, and a one-stack hourglass network is used as a geometry extractor. The 2D-Gaussian map with \u03c3 = 4 is used for visualization. The network uses stride-2 convolution for downsampling to obtain style and content representations. A symmetrical deconvolution network with skip connection is used for reconstruction. The architectures and hyperparameters are detailed in the appendix. Unsupervised Feature Disentangle focuses on learning disentangled representations in an unsupervised manner. Various methods have been proposed, but interpreting the learned factors remains a challenge. In recent works, efforts have been made to improve disentangling latent factors. BID21 and BID9 assign codes to specific factors of variation, while BID30 uses a 3DMM model and GANs for face property representation. BID4 divides latent variables into discrete and continuous ones. In a new approach, one branch of representation is given a more complex prior to focus solely on object shape information. Pose Synthesis: Recent advancements in GANs research have enhanced pose-guided image generation. BID20 pioneered pose image synthesis using U-Netlike networks, with subsequent works achieving better results in human pose and face generation. BID5 utilized a conditional U-Net for shape-guided image generation, while BID17 and BID24 integrated geometric information into the image generation process. However, existing methods often rely on extensive annotated data or pre-trained landmark estimators. Unsupervised Structure Learning: The unsupervised learning of object structures is a key focus in computer vision, with early works concentrating on keypoints detection and descriptor matching. Recent approaches, such as BID12 and BID36, demonstrate the potential for end-to-end structure learning in Autoencoder formulations, while BID31 adopts a deformable template paradigm for representation. Our work differs from existing methods by explicitly learning complementary style representations in a two-branch VAE framework, leading to clear disentanglement. We evaluate our method on four datasets: MNIST-Color, 3D Chair, and Cat & Dog Face, covering both synthesized and real-world data. The study evaluates a framework for disentanglement ability and generation quality using various datasets like MNIST-Color, 3D Chair, Cat & Dog Face, and CelebA. Qualitative evaluations include conditional sampling, interpolation, retrieval, and visual analogy, while quantitative evaluations involve content and style consistency metrics. The study evaluates disentanglement ability and generation quality using various datasets like MNIST-Color. It demonstrates diversity in conditional generation results with successfully disentangled content and style. The model learns disentanglement spontaneously without prior knowledge of the digit in the image. Linear interpolation results show reasonable coverage of the manifold. Interpolation results demonstrate smooth color transitions in the latent space while maintaining digit information. Nearest neighbor retrieval experiments show the disentangled ability of the learned representation, allowing for semantic and visual retrieval. Quantitatively, Recall@1 is reported as the most challenging metric in TAB2 (a). The content representation outperforms image pixel and style representation in visual retrieval tasks. Visual analogy results on MNIST-Color and 3D Chair show successful maintenance of detail components. Comparison with other unsupervised representation learning methods in terms of perceptual quality is shown in FIG4. Our approach demonstrates superior geometry maintenance compared to other methods like VAE, \u03b2-VAE, BID8, and InfoGAN. We showcase the disentanglement ability for the azimuth factor on the 3D Chair dataset and highlight better geometry preservation. Additionally, we will show the scalability of our model on real-world datasets such as Cat, Dog Face, and CelebA. Our model successfully extends unsupervised disentanglement to photo-realistic generation with 256 \u00d7 256 resolution. It accurately captures structural information of images, allowing for style transformation while maintaining geometry shapes. Qualitative evaluation shows swapping of geometry and style information on Cat dataset, demonstrating the ability to swap expression, head-pose, facial action, and hair texture. Structural heatmaps encode geometry cues effectively, providing a prior for identity, head pose, and expression in query images. The model successfully extends unsupervised disentanglement to photo-realistic generation with 256 \u00d7 256 resolution, capturing structural information for style transformation while maintaining geometry shapes. It can swap expression, head-pose, facial action, and hair texture on real-world images, with structural heatmaps providing a prior for identity, head pose, and expression in query images. The model can generalize to various real-world images with large variations, such as mouth-opening, eye-closing, tongue-sticking, and exclusive style. Content and style consistency of analogy predictions are evaluated due to the absence of ground-truth targets. The evaluation of content and style consistency in analogy predictions is done using content and style similarity metrics. Content consistency is assessed using mean-error of landmarks detected by a landmark detection network trained on manually annotated data. Public cat facial landmark annotations are sparse, so 10k cat faces were manually annotated with 18-points for training. In the evaluation, a landmark detection network trained on 10k manually annotated cat faces with 18-points is used for content consistency assessment. The celebA dataset is evaluated using a model with 68-landmarks. Results on the testing sets of both datasets are reported in TAB1, comparing with two state-of-the-art unsupervised structure learning methods. Content and style consistency are evaluated using content and style similarity metrics, landmark detection error, SSIM, and IS. Our novel model based on Autoencoder framework aims to disentangle object representation by content and style without the need for annotation. It demonstrates superior performance in content/style consistency and generation quality. The effects of VGG loss and KL loss on generated images are evaluated using Structural Similarities (SSIM) and Inception Scores (IS) on the Cat dataset. Our work suggests using strong priors to restrict latent variables as an effective tool for disentangling, opening up potential research topics for the future. The latent variables are a potential tool for disentangling object representation. Experimentation has been done on near-rigid objects, but learning on deformable objects remains a challenge. Content-invariant style representation shows promise for recognition tasks."
}