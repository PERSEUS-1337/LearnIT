{
    "title": "ryY4RhkCZ",
    "content": "Building robust online content recommendation systems involves complex interactions between user preferences and content features. The field has evolved with new Deep Learning models to capture non-linear interactions. A novel method, Deep Density Networks (DDN), addresses challenges in online recommendations by predicting probability densities of CTR for more efficient exploration of the feature space. Results show the effectiveness of DDN in a real-world recommendation system serving billions of recommendations daily. Taboola's content discovery platform aims to match content to users who are likely to engage with it using computational models. Content recommendations are shown in widgets at the bottom of articles on various websites, serving billions of recommendations per day to hundreds of millions of active users. Traditionally, recommender systems have been modeled in a multi-arm bandit setting to balance exploitation and exploration strategies. One approach to multi-arm bandit problems is the -greedy algorithm, balancing exploitation and exploration to maximize long term reward. Techniques like Upper Confidence Bound (UCB) and Thompson sampling use prediction uncertainty to efficiently explore the feature space. In this paper, a unified deep neural network model called DDN is introduced to incorporate measurement and data uncertainty for online recommendations. The model can be trained end-to-end and aids in exploitation/exploration selection strategy. A mathematical formulation is presented to deconvolve measurement noise and provide data uncertainty predictions. The benefits of using DDN in a real-world content recommendation system are demonstrated. Deep learning models have revolutionized recommender systems by capturing complex user-item relations and integrating various data sources. Traditionally, recommender systems have been modeled in a multi-arm bandit setting to maximize long term rewards. Bayesian neural networks have been used in this context. Bayesian neural networks have been applied in recommender systems to maximize long term rewards. Various methods such as Bayes by Backprop, Monte Carlo dropout, and separating uncertainty into model and data uncertainty have been proposed to address computational costs and model exploration/exploitation trade-offs. In this paper, the LinUCB algorithm is proposed to support models based on contextual features, addressing measurement noise and noisy labels. Measurement noise is modeled using a Gaussian model and combined with a MDN. Taboola's recommender system generates revenue through online advertisers paying CPC for user clicks on recommendations, measured in RPM. Taboola's recommendation engine focuses on predicting CTR to rank content recommendations based on RPM. The system partitions into candidation and ranking phases to handle millions of recommendations within strict time constraints. CTR prediction relies on features like recommendation creatives and click statistics. Taboola's recommendation system utilizes a small list of recommendations stored in distributed databases worldwide, continuously recalculated by servers. When a request is made, frontend servers retrieve the list and rank recommendations based on user features using a DNN. The system constantly evaluates new recommendations, split into exploration and exploitation modules to maximize RPM. In this paper, the focus is on the candidation phase and CTR prediction task of the recommendation system. The Deep Density Network (DDN) is introduced to handle measurement noise and data uncertainty in a unified model, improving robustness to noisy data. The model combines content-based and collaborative filtering approaches, using two separate subnets to model the target and context. This enables more efficient exploitation/exploration selection strategies. The model uses two separate subnets to model target and context features, incorporating content-based and collaborative filtering approaches. The target subnet processes content features and additional inputs, while the context subnet handles context features like device type. A Deep Density Network fuses the target and context feature descriptors to predict CTR and uncertainty. Historical data of target and context pairs is used for training. To train models, historical data of target and context pairs (t, c) with empirical CTR is used. Models are optimized for CTR prediction using Maximum Likelihood Estimation (MLE). Uncertainty is divided into measurement and data uncertainty, with measurement uncertainty dependent on the number of recommendations and data uncertainty being inherent noise in observations. Observations show that data uncertainty is divided into homoscedastic and heteroscedastic uncertainty. Homoscedastic is constant across all inputs, while heteroscedastic varies depending on the inputs. Algorithms like -greedy choose actions without specific preferences for successful targets or information gain. Selecting non-greedy actions based on their potential optimality, considering both expectation and variance of CTR estimates, is beneficial. Estimating uncertainty is crucial for decision-making. Estimating uncertainty enables adaptive selection between exploitation/exploration using the UCB algorithm. Probabilistic modeling assists in sampling targets with high potential value. Model uncertainty captures confidence about different values in the feature space. Model uncertainty can capture confidence in different feature values, but it is computationally expensive with dropout. Exploring the feature space by setting Out Of Vocabulary (OOV) values below a threshold shows larger uncertainty. Data and measurement uncertainties are modeled together using random variables Y, Y*, and . Data uncertainty is modeled by placing a distribution over the model output and learning it based on inputs. The text discusses using a Mixture Density Network (MDN) to model Y* and estimate the expected value and standard deviation. By combining MDN with a Gaussian model, both data and measurement uncertainties can be deconvolved and modeled using a single model. The text introduces a model combining MDN and a Gaussian model for probability estimation in training. Historical data from a browsed website is used, with experiments on REG, MDN, and DDN models for CTR estimation. In order to compare models fairly, hyper-parameters were tuned individually through random search iterations. Evaluation was done using Mean Square Error (MSE) and online A/B testing with average RPM across publishers. An online throughput metric was used to measure exploration effectiveness, as offline simulation was not feasible due to high turnover in recommendation pool. The throughput metric measures the contribution of models to target-publisher pairs. Feature importance is evaluated by analyzing the effect on prediction when changing parameters in deep learning networks. In the analysis, the impact of hiding features on prediction uncertainty is evaluated. Features have a significant effect on data uncertainty, with new advertisers showing a higher impact than new targets. Comparing MDN and DDN models trained on different datasets is also discussed. In TAB0, MDN and DDN models are compared on datasets D1 and D2 with varying levels of noise. DDN outperforms MDN by 2.7% on D1 and 5.3% on D2, showing the importance of integrating measurement noise in modeling. In TAB2, DDN performs best in online RPM, surpassing MDN and REG by 1.7% and 2.9% respectively, due to its ability to attenuate loss during training and converge to better parameters. The Deep Density Network (DDN) has shown superior performance compared to other models, such as MDN, in handling measurement noise and converging to better parameters. By adjusting the parameter a, DDN can prioritize information gain over RPM, leading to improved throughput. The trade-off induced by a = 0.5 resulted in a good throughput gain with minimal RPM cost. DDN is a unified DNN model capable of predicting probability distributions and handling data uncertainties effectively. The Deep Density Network (DDN) is effective in modeling non-linearities and complex target-context relations, incorporating higher level representations of data sources. It outperformed REG and MDN models in online experiments, showing a 5.3% improvement on a noisy dataset and RPM improvements of 2.9% and 1.7% respectively. By utilizing DDN's data uncertainty estimation and UCB strategy, there was a 6.5% increase in targets throughput with only a 0.05% RPM decrease."
}