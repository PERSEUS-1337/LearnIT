{
    "title": "HJMINj05tQ",
    "content": "Su-Boyd-Candes (2014) linked Nesterov's method to an ordinary differential equation (ODE). By adding a Hessian damping term to the ODE, Nesterov's method can be obtained as a discretization. Similarly, for strongly convex functions, adding a Hessian damping term to Polyak's ODE results in Nesterov's method. Both second order ODEs can be represented as first order systems. Liapunov analysis is applied to accelerate convergence rates in continuous and discrete time. It can also be extended to stochastic gradients, treating the full gradient case as a special instance. This results in a unified approach to convex acceleration across different scenarios. Su et al. (2014) connected Nesterov's method for convex, L-smooth functions with a second-order ordinary differential equation (ODE). They did not demonstrate that Nesterov's method arises as a discretization of the ODE. By considering a perturbation of the ODE with a Hessian damping term, it was shown that Nesterov's method in the strongly convex case arises as a discretization of a second-order ODE. This perturbation accelerates gradient descent in continuous time. The analysis unifies the continuous time ODE with the algorithm, including full gradient acceleration as a special case. Acceleration of stochastic gradient descent has been established by previous studies. The continuous time approach in optimization involves rewriting systems involving \u2207f and replacing it with g = \u2207f + e to achieve accelerated convergence rates. The condition on |e| is crucial for faster convergence than stochastic gradient descent. The renewed interest in continuous time methods began with recent works by Su et al. (2014) and others. Continuous time optimization methods have been around for a long time, with Polyak's method (1964) related to accelerating solutions of linear partial differential equations. The Liapunov approach for acceleration in solving linear partial differential equations was introduced by Young in 1954. Polyak (1987) and Nemirovskii et al. (1983) provided continuous time interpretations of Newton's method and mirror descent algorithm, respectively. Stuart & Humphries (1996) studied the preservation of properties in discretizations of dynamical systems. The Hessian term in (H-ODE-SC) can be represented as a first-order system. The forward Euler method can be used to discretize the system (1st-ODE) with a constant time step, h, to obtain Nesterov's method. The method involves defining y k as a convex combination of x k and v k, and evaluating gradients at y k. This approach differs from the standard forward Euler method by evaluating \u2207f at y k instead of x k, but it is still an explicit method. More advanced methods are discussed in Scieur et al. (2017). The discretization of (H-ODE) given by (FE-C)(1) with h = 1/ \u221a L and tk = h(k+2) is equivalent to the standard Nesterov's method. The second order ODE (H-ODE-SC) can be represented as a first order system and discretized using a forward Euler method with a constant time step h to obtain Nesterov's method. The forward Euler method is applied for a small time step h in the first-order ODE. Nesterov's method for strongly convex functions is recalled, and the discretization of the second-order ODE is equivalent to Nesterov's method. Liapunov analysis for convex cases in continuous and discrete time is defined, with results already known and proofs provided in the appendix. The proof of the rate using a Liapunov function can be found in BID4. The discrete Liapunov function was used to prove a rate in the strongly convex case. The proof can be found in Wilson et al. (2016, Theorem 6) and in Appendix E. Theorem 6 provides results for stochastic gradient descent in continuous and discrete time, including the accelerated case. Stochastic gradients are discussed with a gradient plus error term, and the accelerated rate requires fast decreasing error terms. The combination of gradient reduction and momentum is also mentioned. The text discusses the requirements for stochastic gradient descent, emphasizing the need for fast-decreasing error terms. It also explores the use of larger minibatches to achieve this. The Forward Euler scheme is modified, and a Liapunov function is used to study the convergence rate. The sequence of error terms is shown to satisfy a specific condition, with an example provided for SGD. The text discusses the need for faster-decreasing error terms in stochastic gradient descent compared to regular SGD. It introduces a discrete Lyapunov function and explores the use of larger minibatches. The sequence of error terms satisfies a specific condition for accelerated SGD. In the framework, a discrete Lyapunov function is defined, leading to a convergence result for sequences generated by the scheme (Sto-FE-SC). The proof of the new result is included, showing the convergence of the sequences. In the framework, a discrete Lyapunov function is defined for sequences generated by the scheme (Sto-FE-SC), leading to convergence results. The proof includes showing the convergence of the sequences using strong convexity and gradient descent. In the proof, by utilizing L-smoothness and convexity of f, it is shown that for a solution x of a certain equation with initial data x0, if the error function e satisfies a specific condition, then the function E(t, x(t)) is decreasing. This leads to the conclusion that the supremum of |x - x*| remains bounded. The discretization of the equation is also discussed, defining E_k for k \u2265 1. In this section, an error e(t) is considered in the calculation of the gradient. A perturbation of the system is studied, leading to the definition of a Lyapunov function. The function E_k is defined for k \u2265 1, and a discrete Gronwall's inequality is used to show that E_k is bounded below. The text discusses the perturbed system of (1st-ODE-SC) with a locally integrable function e, defining a continuous time Liapunov function E(x, v) and a perturbed Liapunov function E(t, x, v). The function E_k is defined for k \u2265 1, and a discrete Gronwall's inequality is used to show that E_k is bounded below. The text discusses the perturbed Liapunov function E(t, x, v) for the system (1st-ODE-SC) and proves the boundedness of the solution. Using Gronwall's inequality and previous results, it concludes with a corollary for the solution with initial conditions. The text discusses the perturbed Liapunov function E(t, x, v) for the system (1st-ODE-SC) and proves the boundedness of the solution. It uses Gronwall's inequality and previous results to establish a corollary for the solution with initial conditions. The proof involves strong convexity, L-smoothness, and estimating differences of E in terms of gradients evaluated at y k. The proof of Proposition 3.4 involves estimating the linear term y k \u2212 x k , \u2207f (y k ) in terms of y k \u2212 x * , \u2207f (y k ) with a correction controlled by the gap and the quadratic term in E. The largest choice of h is h = 1 \u221a L, leading to (10) using the monotonicity of the expression on the right hand side."
}