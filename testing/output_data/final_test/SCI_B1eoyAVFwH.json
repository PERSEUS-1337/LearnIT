{
    "title": "B1eoyAVFwH",
    "content": "Multi-task learning aims to reduce data, parameters, and training time compared to single-task models. However, implementing this efficiently is challenging due to the need for a suitable architecture that can handle multiple tasks without excessive computational requirements. To address this, a method for automatically searching over multi-task architectures is proposed, considering resource constraints. This method involves parameterizing feature sharing strategies and quick evaluation of architectures through feature distillation, leading to optimized parameter-efficient multi-task models. Benchmarking on Visual Decathlon shows the effectiveness of this approach. In this work, the focus is on automatically searching for high-performing multi-task architectures that balance task resource requirements while maintaining high performance. Multi-task learning offers benefits in terms of generalization, robust feature representations, and resource efficiency. By leveraging similarities across tasks, multi-task models require less data, fewer training iterations, and fewer parameters compared to task-specific models. The goal is to find architectures that offer the best performance while considering resource constraints. Neural architecture search (NAS) automates the search for models that balance performance trade-offs within resource constraints. Multi-task architectures support separate outputs for different tasks through unique execution paths in the network. Tuning optimal configurations manually is challenging due to the vast space of options available for tweaking architectures. During inference, unused parts of the network can be ignored by either pruning out nodes or zeroing out their activations, leading to improved parameter efficiency in multi-task architectures. The goal is to optimize computational efficiency by reducing average node use per task and effectively identifying which tasks require more capacity. Performance is influenced by how nodes are shared across tasks, with uncertainty on whether sharing parameters would benefit or interfere. In this work, effective partitioning strategies are identified to maximize performance and reduce computation per task by partitioning individual feature channels within a layer. This approach offers more control over computation and sharing between tasks compared to existing methods that allocate entire layers. The main challenge in finding effective multi-task architectures is the vast number of possibilities for feature performance. In this work, the focus is on identifying effective partitioning strategies to maximize performance and reduce computation per task by partitioning individual feature channels within a layer. The proposed parameterization of partitioning strategies aims to streamline the search space and express key features more compactly. Additionally, a surrogate operation is commonly used in NAS to evaluate sampled architectures, training a smaller model for a shorter number of iterations to predict the final performance of the full model. In this work, a strategy for evaluating multi-task architectures using feature distillation is proposed to provide faster feedback on partitioning strategies. The parameterization aids automatic architecture search by representing sharing strategies compactly. Results on Visual Decathlon show the effectiveness of identifying trade-offs between parameter use and performance on image classification datasets. In the context of evaluating multi-task architectures, the discussion focuses on designing deep learning architectures for vision tasks. Key factors to consider include task ordering and fixed vs learned strategies. The importance of training on all tasks simultaneously to avoid catastrophic forgetting is highlighted. The contributions in this work aim to streamline the process of finding task-specific solutions by utilizing a shared network architecture. This approach, commonly used in reinforcement learning, involves training a single agent to perform multiple tasks. Distillation of multiple models into one is a technique employed to train these shared models effectively. In the context of training shared models for multiple tasks, various methods are used to incorporate new tasks without disrupting the original task-solving ability. These methods include freezing network weights, masking specific filter weights, introducing auxiliary layers, and dynamically expanding network capacity for each new task. The goal is to build on a fixed model, requiring new tasks to perform original task computations while adding task-specific steps. Multi-task architectures are built from parallel layers, with cross-stitch networks computing activations for each task. Different approaches involve searching for optimal execution paths for tasks using reinforcement learning or evolutionary algorithms. Parameter sharing trade-offs can occur at a more detailed level by dividing individual feature channels instead of entire layers. Neural Architecture Search focuses on three main areas: search space, optimization, and sample evaluation. A continuous search space allows for slight changes in resource allocation across tasks, offering new optimization alternatives. Leading approaches use reinforcement learning or genetic algorithms for NAS, but the tradeoffs between these methods are unclear. The effectiveness of random sampling and evolutionary strategies optimization is tested for Neural Architecture Search. Evaluating samples involves using cheaper evaluation methods like preserving weights, successive halving, and modeling expected performance of sampled architectures. The use of distillation for evaluation is investigated. During evaluation, binary masks are used to focus on specific tasks by zeroing out irrelevant channels in the feature tensor. Gradients are calculated with backpropagation, but weights can be preserved by zeroing out their gradients based on a backward mask. This approach captures training dynamics in multi-task architectures and allows for transferring features from tasks like ImageNet classification to new datasets. During evaluation, binary masks are used to focus on specific tasks by zeroing out irrelevant channels in the feature tensor. The backward mask, m b, ensures that ImageNet weights remain untouched during finetuning on a new task. Allocating resources at the channel level offers advantages such as fine-grained control over weight allocation and easy network pruning post-training. Masks are applied every other layer to optimize resource usage, allowing for efficient utilization of compute at test time while benefiting from joint training with other tasks. Applying masks at every other layer allows for efficient resource allocation, with only one quarter of the original model being used. To find the best masks for each task, a binary matrix is defined to specify partitioning masks. However, optimizing over this matrix is challenging due to the large space of discrete values and the need to account for redundancy. Explicitly considering the overlap of channels between masks is necessary to ensure diverse degrees of feature sharing. To efficiently allocate resources, masks are applied at every other layer, reducing the model size. To find optimal masks, a matrix is used to specify partitioning, with consideration for channel overlap. The matrix P determines feature channels and sharing between tasks, guiding the selection of masks for diverse feature utilization. Sampling strategies directly from P helps identify matching masks, with adjustments made to address ill-posed spaces. The representation of matrix P allows for efficient resource allocation by specifying partitioning and feature sharing between tasks. It is low dimensional and interpretable, providing insights into network capacity usage. The diagonal of P gives an average node usage per task, which is used to compare resource efficiency of different partitioning strategies. Optimization over different parameterizations of P involves choosing samples and evaluating them in a black box optimization setting. In a black box optimization setting, we evaluate and compare different samples to find good constraint matrices. One strategy is random sampling, which has been shown to be effective in architecture search work. Random samples help map out the search space and identify key choices affecting performance. A random matrix P is sampled with values from 0 to 1 to cover a large portion of the space. Evolutionary strategies are used to bias or restrict samples of a random matrix P for a specific resource target. Gradient-based optimization is employed to search over parameterizations, with a focus on using as few channels as necessary per task. A weighted average of random directions in the parameter space is computed to update parameters, along with an additional L2 weight regularization term. An L2 weight regularization term is added to reduce the number of channels used per task, without affecting overall accuracy. This optimization strategy is enabled by the parameterization defined earlier. It allows for exploring multi-task architectures more efficiently compared to existing methods that focus on coarse selection of computational blocks. Different partitioning schemes need to be evaluated to determine the most effective approach. To efficiently explore multi-task architectures, feature distillation is proposed to observe the representational capacity of partitioned layers. By focusing on a few layers, total computation and the number of weights are reduced. Distilling to intermediate layer activations provides a more direct training signal than a final classification loss. This approach aims to determine the effectiveness of different partitioning schemes without the need for expensive model training to convergence. Feature distillation is used to observe the representational capacity of partitioned layers in multi-task architectures. New layers are initialized and loaded with reference models for each target task. Input is passed through task-specific pretrained models up to a target depth, then through subsequent layers and new shared layers. Intermediate features in the new layers are masked based on proposed partitioning. Mean-squared error loss supervises shared layers to match output features of reference teacher models. Distillation effectiveness is measured by replacing original pretrained layers with new shared layers and evaluating model accuracy. Feature distillation is utilized to assess the representational capacity of partitioned layers in multi-task architectures. The process involves producing features at a target depth, passing them through residual blocks of both teacher and student models with a mask applied, and comparing features using MSE loss. This method allows for quick evaluation of partitioning strategies without being limited by individual pre-trained models' performance. The distillation process is run for a brief interval to provide feedback on different parameterizations, leading to a significant reduction in evaluation time for masking strategies. In multi-task architectures, the role of parameterization and distillation is crucial for minimizing task computation and parameter use while maintaining high accuracy. Experiments are conducted on the Visual Decathlon dataset, which consists of various computer vision classification datasets. A shared ResNet model is initialized with separate output layers for each task, with feature partitioning applied to the last third of the model. Training involves alternating mini-batches from each dataset using standard cross-entropy. In multi-task architectures, parameterization and distillation play a crucial role in minimizing task computation and parameter use while maintaining high accuracy. The study involves alternating mini-batches from different datasets and applying a standard cross-entropy classification loss. The focus is on the relationship between performance and feature restriction in a multi-task setting, aiming to reduce interference across tasks and improve overall performance. The goal is to determine the best performance possible as average feature use is reduced further. The study focuses on distillation in multi-task architectures to minimize computation and parameter use while maintaining accuracy. Distillation involves initializing child layers with pretrained layers from an ImageNet model to accelerate the process. The distillation procedure takes just one minute on a P100 GPU. Comparison is made between distillation and full training, with validation accuracy after 5k and 10k iterations. The appendix provides further details. The distillation procedure allows for sampling and comparing parameterizations, providing more confidence in top-performing models. Randomly sampling parameterizations helps map out performance of different strategies. Fast feedback with distillation and effective search space coverage produce information with fewer samples and less time. Choice of partitioning has limited impact at high levels of feature use. Partitioning well and specifying the degree of sharing between tasks are crucial for achieving high performance. Different partitioning strategies with varying levels of sharing can significantly impact performance. Feature sharing should be parameterized for optimization, as shown in Figure 4. Per-task results in Figure 5 demonstrate that every task benefits from using more features. In a study on feature sharing among tasks, it was found that using more features benefits every task. However, as the level of shared features increases, individual tasks may suffer due to interference. Evolutionary strategies can further optimize performance by pushing the edge of performance even higher. Evolutionary strategies optimize performance by quickly identifying samples for best accuracy and making slight changes for improvements. Adjusting weight decay controls resource use for the final partitioning strategy, allowing easy tuning to meet specific resource needs. The best parameterization outperforms baselines in partitioning strategies, showing superior validation accuracy. The best parameterization outperforms baselines in partitioning strategies, reducing average feature use while maintaining overall performance. Simple tasks requiring fewer channels are separated to improve performance on harder tasks like CIFAR100, Flowers, and Omniglot. In this work, efficient multi-task architecture search is explored to quickly find high-performance models under a limited per-task budget. A novel strategy for searching over feature partitioning is proposed, determining network capacity allocation for each task and parameter sharing. A compact representation is designed as a search space, estimating performance of partitioning schemes using feature distillation. Operations are defined to convert the search space into a constraint matrix for feasible masks. The text discusses the remapping of off-diagonal elements in a matrix to ensure that there is no more overlap between tasks than the channels used by any one task. This process helps measure the correlation of feature activations across task-specific ResNet models. The features produced are highly correlated through the first two-thirds of the model, with greater task differentiation observed towards the end. For full model training, batch normalization statistics are controlled to prevent early blocks. Multi-task training involves using a batchsize of 64 with SGD and momentum at a learning rate of 0.05 for 100k iterations, dropping to 0.005 at iteration 75k. Training is done on a single Nvidia P100 GPU following Visual Decathlon splits. Separate batch normalization statistics per task and maintaining gradient statistics with momentum are key for model convergence. During training, a curriculum strategy is used to select mini-batches from tasks based on their training accuracy. This approach has shown effectiveness in multi-task reinforcement learning. Experiments are conducted using a pretrained ImageNet model, excluding ImageNet in partitioning experiments to focus on interactions of other datasets. The main hyperparameters determining performance were learning rate and a temperature term controlling task sampling. Grid search was used to find the final values. Validation accuracy was averaged across 5 trials. To simplify experiments, the first two-thirds of the network were frozen and shared, with partitioning only on the last third. Updating weights of the last block focused on task-specific features without limiting representational capacity. ImageNet-pretrained ResNet model was used in all experiments. In experiments, an ImageNet-pretrained ResNet model with three computational blocks is used. The first two blocks are frozen, and partitioning is done only on the last set of layers. Feature differences are compared across finetuned task-specific models to analyze how task feature use diverges after finetuning. Task features are compared after passing a shared image into each model, controlling for batch normalization statistics. The study used an ImageNet-pretrained ResNet model with three computational blocks. Feature differences were compared across task-specific models, showing divergence in feature use after finetuning. Distillation training was conducted for 3000 iterations without an accuracy-based curriculum, focusing on the last set of layers. Distillation training was performed on the last four ResNet layers with a batch size of 4 and a learning rate of 1, reduced by a factor of 10 at iteration 2000. Scores were averaged across three trials, with optimization curves based on 1000 samples run on machines with 4 P100 GPUs. The process took just over four hours, involving sampling 16 random parameter directions at each step for evaluation. A gradient descent step with a learning rate of 0.1 was applied to the current parameters, ensuring values remained between 0 and 1. During distillation training, a sigmoid operation was tested to constrain values between 0 and 1 without affecting optimization performance."
}