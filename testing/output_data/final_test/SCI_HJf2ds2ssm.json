{
    "title": "HJf2ds2ssm",
    "content": "Neural networks can produce unexpected results on inputs far from the training distribution. ODIN is a method for out-of-distribution detection that achieves good performance for image classification tasks. In this paper, ODIN is adapted for sentence classification and word tagging tasks. ODIN scores can be used as a confidence measure for predictions on in-distribution and out-of-distribution datasets. English-LinES treebank contains sentences from literature, while English-EWT dataset is larger and more diverse. The datasets are described, with sentences represented as words and embeddings used for bidirectional LSTM. ODIN is adapted for sentence classification and word tagging tasks. It uses a cross-entropy loss function for sentiment analysis and a dense layer for POS tagging. Hyperparameters like perturbation magnitude and temperature are chosen based on OOD detection performance. Results for OOD detection and rank correlation coefficients are shown in tables. Table 4 displays rank correlation coefficients for PbThreshold and ODIN methods. Temperature scaling improves out-of-distribution detection, with higher temperatures saturating at T reaching thousands. Perturbations have a positive effect on sentiment analysis but not on POS tagging. ODIN outperforms PbThreshold for POS tagging tasks. ODIN scores on en-LinES are indicative of network performance on OOD samples. Circle size in the development sets of en-LinES and en-EWT is proportional to sample numbers. In this work, ODIN out-of-distribution detection method was adapted for sentence classification and sequence tagging tasks. It outperformed the PbThreshold baseline as an OOD detector. The study aimed to evaluate the confidence scores produced by these methods for neural model predictions. Other OOD detection methods for NLP tasks remain untested, and sequence-to-sequence tasks were not covered in the analysis."
}