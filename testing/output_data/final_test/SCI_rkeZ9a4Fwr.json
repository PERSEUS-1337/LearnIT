{
    "title": "rkeZ9a4Fwr",
    "content": "This paper discusses the robustness of VAEs to adversarial attacks and introduces Seatbelt-VAE, a new hierarchical disentangled VAE designed to be more robust. Unsupervised learning of disentangled latent variables in generative models is still a research challenge. The text discusses how VAE-derived models aim to reward independence between latent variables to achieve better disentangled representations. Factor VAE, \u03b2-TCVAE, and HFVAE have shown that up-weighting the total correlation term can improve the disentanglement compared to \u03b2-VAEs. Disentangled representations offer more informative and robust latent space representations, increasing the model's robustness. Having a disentangled hidden layer in a discriminative deep learning model enhances robustness to adversarial attacks, particularly in applications where the encoder's output is utilized downstream. Adversarial attacks on deep generative models aim to deceive the model by distorting the input image to match a target image in the latent-space representation. This type of attack is relevant for various tasks such as text classification, discrete optimization, image compression, and perceptual processing. \u03b2-TCVAEs are more robust to latent-space attacks than standard VAEs, improving the agent's robustness to domain shift. However, imposing additional disentangling constraints degrades image quality. Powerful models may help mitigate this issue. The text discusses the robustness of Seatbelt-VAEs to latent-space adversarial attacks on different models, including Chairs, 3D Faces, and CelebA. The Seatbelt-VAE is compared to \u03b2-TCVAE and \u03b2-TCDLGM models, showing different levels of penalization on the adversarial distortion. The Seatbelt-VAE is described as a hierarchical disentangled VAE, drawing on previous works like Ladder VAEs and BIVA. The Seatbelt-VAE is shown to be more robust to adversarial attacks compared to \u03b2-TCVAEs and \u03b2-TCDLGMs. The key focus is on the robustness of disentangled representations, with the introduction of Seatbelt-VAE designed to increase robustness to various types of attacks while improving perceptual quality of reconstructions. Variational autoencoders (VAEs) are a deep extension of factor analysis suitable for high-dimensional data like images. They involve a joint distribution over data x and latent variables z, with parameters represented by deep nets. Exact inference is intractable, so VAEs use amortised stochastic variational inference with an approximate posterior distribution. The reparameterisation trick is used to take gradients through Monte Carlo samples. In a \u03b2-VAE, a free parameter \u03b2 is introduced. In a \u03b2-VAE, a free parameter \u03b2 multiplies the D KL term in the objective function, which remains a lower bound on the evidence. Decompositions of the evidence lower bound shed light on its meaning, with the dataset-level evidence lower bound defined over a dataset D of size N. The decomposition includes the average encoding distribution and the total correlation for the encoding distribution. The total correlation (TC) for q \u03c6 (z) is a key aspect in Factor and \u03b2-TCVAEs, with an objective defined as L \u03b2TC (\u03b8, \u03c6, D) = 1 + 2 + 3 + B + \u03b2 A. A differentiable, stochastic approximation to E q \u03c6 (z) log q \u03c6 (z) is provided by Chen et al. (2018) for training. The Deep Latent Gaussian Model (DLGM) by Rezende et al. (2014) introduces a hierarchy of conditional stochastic variables in the generative model. The Deep Latent Gaussian Model (DLGM) by Rezende et al. (2014) introduces a chain factorization with Gaussian distributions parameterized by deep nets. To perform amortized variational inference, a recognition network is introduced, but training DLGMs can be challenging due to difficulties in learning informative latent variables. In a DLGM, a single-layer VAE can train in isolation within a hierarchical model, leading to the collapse of entire layers of latent variables. This phenomenon motivates the development of Ladder VAE and BIVA. Novel hierarchical disentangled VAEs aim to disentangle only the top-most latent variables. The Seatbelt-VAE model, inspired by BIVA, conditions the likelihood on all latent variable layers to prevent collapse in a hierarchical disentangled VAE. By increasing conditional dependencies and nodes, the model's safety against adversarial attacks, noise, and decreases in perceptual quality improves as \u03b2 increases. Free-bits regularization helps address optimization challenges in DLGMs. Training VAEs and derived models commonly involves stochastic gradient ascent on the ELBO with minibatches of data. Minibatch estimators are derived to handle large mixture distributions in objective functions. Hierarchical VAEs using Minibatch Weighted Sampling estimator for \u03b2-TCVAEs have been proposed. Adversarial attacks on VAEs have been studied, with two attack modes identified. Attackers aim to manipulate a large number of pixels in generative models compared to discriminative models. Adversarial attacks on VAEs involve two modes: output attack and latent attack. The output attack aims to change pixel values in the output to alter the content of the reconstruction. The latent attack aims to find adversarial examples that closely match the initial image in appearance. The latent-space adversarial objective for a single stochastic layer VAE prioritizes smaller distortions by penalizing the L2 norm of d. Studies have shown that latent attacks are as or more effective than output attacks for single layer VAEs, depending on model architecture. The attacker's strategy depends on the model architecture, with different approaches for DLGMs, \u03b2-TCDLGMs, and Seatbelt-VAE. Targeting individual latent layers can affect the effectiveness of the attack, with a focus on matching the bottom latent layer for DLGMs and \u03b2-TCDLGMs. In Seatbelt-VAE, attacking all layers is more effective than targeting top or base layers individually. Four tranches of experiments were conducted to explore these strategies further. In a series of experiments, the study demonstrates the robustness of Seatbelt-VAEs and \u03b2-TCDLGMs to adversarial attacks compared to vanilla VAEs. These disentangled models also show increased resistance to unstructured noise, with Seatbelt-VAEs being the most robust. The effect of disentangling on model weights sparsity is also examined across different datasets. The study used the same encoder and decoder architectures as Chen et al. (2018) for each dataset. The Mutual Information Gap (MIG) at the top layer of each model is included in the Appendix. Existing disentangling metrics may not directly apply to hierarchical models. Various models were trained with different penalizations, and the final ELBO of the trained models is plotted in Figure 3 without the additional \u03b2 penalization. Increasing the \u03b2 penalisation for Seatbelt-VAEs does not significantly degrade the model's quality, as shown by the ELBO. Reconstructions from a Seatbelt-VAE with L = 4 and \u03b2 = 20 maintain facial identity better than those from a \u03b2-TCVAE, especially in preserving finer facial features. The study evaluates the resistance of Seatbelt-VAEs to increasing \u03b2 penalisation through adversarial attacks on various datasets. The results show that increasing \u03b2 does not significantly degrade the model's quality, as indicated by the ELBO. The reconstructions from Seatbelt-VAEs maintain facial identity better than \u03b2-TCVAEs, especially in preserving finer facial features. The study evaluates the effectiveness of adversarial attacks on disentangled models, showing that latent space attacks are less effective. Attacks on Seatbelt-VAEs with high \u03b2 and L values often result in outputs resembling the original inputs. Additional examples of attacks on various datasets are provided in the Appendix. The study evaluates adversarial attacks on disentangled models, showing that they are less effective. Attacks on \u03b2-TCVAEs become harder as \u03b2 increases, with higher \u2206 latent values compared to standard VAEs on Chairs and 3D faces. Output attacks are also less effective. Figures 5 and 6 illustrate these findings. The study shows that \u03b2-TCDLGMs and Seatbelt-VAEs offer significant protection against adversarial attacks, with larger values of metrics corresponding to less successful attacks. Depth and disentangling together provide the most effective defense. In the Appendix, the study evaluates the robustness of models to random noise by adding noise to datasets and analyzing the performance of different models. Seatbelt-VAEs and \u03b2-TC models show improved robustness to noise, while \u03b2-TCDLGMs perform worse. See Figure 8 for comparisons of models under different noise levels. The robustness of disentangled models to adversarial attacks may be due to their resistance to random perturbations. Regularization in the form of D KL terms in the auto-encoder view helps in disentangling, with L 2 regularization leading to orthogonal latent projections in linear autoencoders. Increasing \u03b2 in \u03b2-TCVAEs and Seatbelt-VAEs results in higher L 2 norm of weights in networks. The L2 norm increases for the encoder and decreases for the decoder in \u03b2-TCVAEs and Seatbelt-VAEs for Chairs, 3D Faces, and CelebA. The changes are generally greater for \u03b2-TCVAE than Seatbelt-VAE, as the former interacts directly with the disentangled representation. This increase in robustness to adversarial attacks is strongest for attacks via the latent space. Recent work by Shamir et al. (2019) provides a theoretical grounding for using stochastic methods to defend against adversarial inputs in deep neural network classifiers. They propose using more robust VAEs, like \u03b2-TCVAEs, for this purpose. Seatbelt-VAE, a hierarchical VAE disentangled on the top-most layer with skip connections down to the decoder, is introduced as a potential solution. The hierarchical VAE with skip connections increases robustness to adversarial attacks and noise, serving as an effective denoising autoencoder. The model's performance under attack is reflected in its ability to defend against uncorrelated noise. The application of \u03b2-TC decomposition to the likelihood is discussed, leading to the ELBO for Seatbelt-VAEs. The i = 1 case is covered in the appendix of Chen et al. (2018). We repeat the argument for i = 1 and then cover the case i > 1 for models with factorisation of q \u03c6 (z|x). During training, a minibatch is sampled and E q \u03c6 (z 1 ) log q \u03c6 (z 1 ) is estimated. Each member of the minibatch is the result of sequentially sampling along a chain. During training, a minibatch {z M } is sampled by sequentially selecting values from a set {z i\u22121 n }. The model estimates E q \u03c6 (z i ) log q \u03c6 (z i ) and samples z i k from q \u03c6 (z i |z i\u22121 k ). Runs were conducted on Azure cloud system with NC6 GPU machines. For \u03b2-TCDLGMs and Seatbelt-VAEs, mappings q \u03c6 (z i+1 |z i ) and p \u03b8 (z i |z i+1 ) are implemented as MLPs with decreasing hidden layer dimensions. To train the model, ADAM with default parameters and a learning rate of 0.001 was used. Data was preprocessed to fall within the interval -1 to 1. CelebA and Chairs datasets were downsampled and cropped. The L2 distance between input x and its reconstruction, adversarial objectives, and log likelihood were evaluated. Latent attacks targeted at z1 in DLGMs were found to be highly effective. Latent attacks on z1 in DLGMs are highly effective, becoming less effective as \u03b2 increases but more effective as L increases. The reduction in sample diversity is observed for L = 1 (\u03b2-TC VAE) but not for \u03b2 = 10, L = 4 Seatbelt-VAE. Mutual Information Gap decreases with an increase in |z|, leading to degenerate latent representations. Degenerate latent representations occur when different units in z have similar mutual information to the same ground truth factors. The red line in a) is at |z| = 6, the number of ground-truth factors of variation for dSprites."
}