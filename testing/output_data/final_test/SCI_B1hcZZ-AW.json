{
    "title": "B1hcZZ-AW",
    "content": "In this paper, a principled method for learning reduced network architectures using reinforcement learning is introduced. The approach involves taking a larger 'teacher' network as input and outputting a compressed 'student' network. Two recurrent policy networks are used in different stages to aggressively remove layers and carefully reduce the size of the network. Our approach involves using reinforcement learning to compress a larger 'teacher' network into a smaller 'student' network. By utilizing policy gradients, we can achieve compression rates of over 10x for models like ResNet-34 while maintaining similar performance. Additionally, pre-trained policies on smaller networks can accelerate training on larger networks, making deep convolutional networks more efficient for deployment on smaller devices. Researchers have developed model compression techniques based on Knowledge Distillation to compress a large (teacher) network to a smaller (student) network using various training techniques. State-of-the-art knowledge distillation methods require hand-designed architectures for the student model, making it difficult to identify optimal network architectures. To address this, a more principled approach using reinforcement learning for network architecture compression is presented. In a principled approach to network architecture compression, a reinforcement learning method is used to create a high-performance student model from knowledge distilled from a larger teacher model. The process involves converting the teacher network to a student network using a Markov Decision Process (MDP) where actions like removing filters or reducing layer sizes transform the architecture. The selection of actions is guided by a policy \u03c0(a|s). The policy \u03c0(a|s) guides the selection of actions in reinforcement learning to learn an optimal policy based on a reward function. The proposed approach addresses scalability issues by introducing a two-stage action selection mechanism for network architecture compression. The proposed approach introduces a two-stage action selection mechanism for network architecture compression using reinforcement learning. It involves selecting a macro-scale \"layer removal\" action followed by a micro-scale \"layer shrinkage\" action to efficiently explore reduced network spaces. The generated network architectures are trained with Knowledge Distillation BID12, and a novel compression reward function is proposed to encode both compression rate and student model accuracy. The approach introduces a method for compressing network architectures using reinforcement learning. It involves a two-stage action selection mechanism for layer removal and shrinkage to explore reduced network spaces efficiently. The compression policies demonstrate generalization across networks with similar architectures, accelerating the reinforcement learning process. Pruning-based methods are discussed for preserving important weights and removing redundant ones. Knowledge distillation involves training a smaller network to mimic a larger \"teacher\" network. This approach allows for greater flexibility in finding the optimal architecture for a given dataset and constraints. BID12 demonstrated that training the student to learn from both the teacher and training data outperforms models trained using only training data. In this approach, Knowledge Distillation with an intermediate hint layer is used to train a thinner but deeper student network to outperform the teacher network. The paper focuses on automating Knowledge Distillation by training a policy to learn the optimal student architecture. The experiments show that the learned architectures outperform previous approaches. Architecture search has been a key focus in exploring neural network design space. The previous work focused on maximizing performance in architecture search, while our goal is to find a compressed architecture with reasonable performance. Unlike existing methods, we use the teacher model as the search space instead of building networks from scratch. Leveraging the teacher model's components, which achieve high accuracy, we aim to create a compressed architecture efficiently. Our goal is to learn an optimal compression strategy via reinforcement learning, systematically reducing a Teacher network to output a small Student network. The decision process is modeled as a Markov Decision Process (MDP), with states representing possible reduced network architectures derived from the Teacher model. Actions involve layer removal or shrinkage policies. The network architecture is transformed using a set of actions, including layer removal and parameter reduction. The state transition dynamic is deterministic, with a discount factor of 1 for equal reward contribution. Rewards are assigned based on the network architecture's score, with 0 for incomplete states. The reinforcement learning approach aims to learn an optimal policy for maximizing total reward in network architecture transformation. A two-stage procedure is proposed to address credit assignment challenges in the action space design. The reinforcement learning approach proposes a two-stage procedure for network architecture transformation. The first stage involves selecting actions to keep or remove layers, while the second stage selects actions to adjust configuration variables. This process efficiently explores the state space to find the optimal student network. In the layer removal stage of student-teacher reinforcement learning, actions are taken to keep or remove layers based on binary decisions. The Bidirectional LSTM policy observes hidden states and information about the current layer to make these decisions. Information about the layer includes type, kernel size, stride, padding, and number of outputs. More complex architectures like ResNet are modeled using skip connections. In the layer removal stage of student-teacher reinforcement learning, actions are taken to keep or remove layers based on binary decisions. The policy network uses skip connections in more complex architectures like ResNet. The trajectory length for layer shrinkage is determined by the number of configuration variables for each layer. The policy observes hidden states, previously sampled actions, and current layer information to make decisions. The parameterization of the current layer information includes appending the previous action in an autoregressive manner. The action space for layer shrinkage is defined as a range of values, with each action corresponding to shrinking a layer parameter. The reward function design is crucial for learning policies, as it distinguishes between good and bad architectures. The goal of model compression is to maximize compression while maintaining high accuracy, penalizing models with high compression and low accuracy more harshly. In model compression, the reward function is defined to balance compression and accuracy. The reward is computed as a product of compression and accuracy rewards, encouraging models to maintain accuracy while optimizing for compression. The compression reward is calculated using a non-linear function to bias the policy towards accurate models with high compression. The compression ratio C is defined as the ratio of trainable parameters between student and teacher models. The accuracy reward Ra is normalized with respect to the teacher model. Degenerate architectures may receive a reward of -1. The approach allows for incorporating pre-defined hardware or resource budgets without additional hyperparameters. Our approach incorporates pre-defined hardware or resource constraints by rewarding architectures that meet them and discouraging those that do not. The reward function is redefined to relax hard constraints, allowing for early exploration and gradual constraint incorporation over time. This flexibility enables the production of models viable in various settings, unlike conventional compression techniques. Our approach incorporates pre-defined hardware or resource constraints by rewarding architectures that meet them and discouraging those that do not. The optimization procedure for our stochastic policies, \u03c0 remove and \u03c0 shrink, involves training policy networks using the REINFORCE policy gradient algorithm to maximize expected rewards over sequences of actions. The optimization procedure for stochastic policies involves training policy networks using the REINFORCE algorithm to maximize expected rewards. A state-independent baseline function is used to reduce variance, and an Actor-Critic policy was tested but had limitations. Student models are trained using logits from a teacher model to incorporate dark knowledge and regularize students. The student model is trained to minimize mean L2 loss on training data using logits from the teacher model. Final student models are trained with hard and soft labels, showing high performance on multiple datasets and teacher architectures, often surpassing the teacher model. The method demonstrates competitive performance compared to current baseline methods of model compression. The study demonstrates competitive performance in model compression, even under highly resource-constrained conditions. Experiments on MNIST and CIFAR-10 datasets show that policies learned by the approach effectively remove redundancies from large network architectures. The SVHN dataset contains colored digit images for training and testing, while CIFAR-100 has 100 classes with a steeper size-accuracy tradeoff. The approach shows solid results despite limitations and is tested on the Caltech-256 dataset for sparse data circumstances. In sparse data circumstances, experiments were conducted on the Caltech-256 dataset with 256 classes and 30607 images. Networks were trained from scratch for standardized comparisons across datasets. An experiment on the ImageNet32x32 dataset with 1.28 million training images and 1000 object classes was also performed. Student models were trained for 5 epochs in the following experiments. In sparse data circumstances, experiments were conducted on the Caltech-256 dataset with 256 classes and 30607 images. Networks were trained from scratch for standardized comparisons across datasets. An experiment on the ImageNet32x32 dataset with 1.28 million training images and 1000 object classes was also performed. Student models were trained for 5 epochs in the following experiments. The layer removal and layer shrinkage policy networks were trained using the Adam optimizer with a learning rate of 0.003 and 0.01 respectively. Both recurrent policy networks were trained using the REINFORCE algorithm (batch size=5) with standard backpropagation through time. A grid search was done to determine the ideal learning rate and batch size. In this section, we evaluate the ability of our approach to learn policies to find compressed architectures without any constraints. The compression performance of teacher networks was evaluated using Conv4 and VGG-13 networks. Results showed improvement over time in compression, accuracy, and reward. Experiments on CIFAR-10 dataset with VGG-19, ResNet-18, and ResNet-34 networks also demonstrated high performance of learned student networks despite a 10x compression rate. The compressed ResNet-34 student model outperforms the ResNet-18 model despite having fewer parameters, showing promising results across different datasets. The ResNet-34 model outperforms the ResNet-18 model due to more residual blocks. Caltech-256 experiments show performance with limited training data. The approach avoids overfitting but struggles with less data. An experiment on ImageNet32x32 dataset resulted in a 30.87% accuracy for the teacher model after 40 epochs. After training for 40 epochs, our approach achieved a top-1 accuracy of 30.87%, finding a compressed model with similar performance (-0.65% drop). The runtime for 100 iterations of layer removal on a ResNet-34 teacher was 272 hours. Comparison to pruning and Knowledge Distillation methods was done, focusing on compression rate defined by parameter ratio. Our approach involves pruning based on BID24, removing redundant weights or filters from the network. Pruning is stopped when accuracy drops below 1% of the student model or the number of parameters is less than our method. The best performing model is selected after 5 rounds of pruning, showing superior results compared to baseline models on both datasets. Operating on the architecture space of the model may yield more consistent results than directly manipulating the weight space. Our method involves pruning redundant weights or filters from the network based on BID24, rather than directly manipulating the weight space. We compared models generated by our method to hand-designed models with similar parameters on CIFAR-10 and SVHN datasets. The models were trained to convergence with Knowledge Distillation, showing superior results. In Iandola et al. FORMULA0, a model with 727K parameters was adapted to CIFAR-10. VGG-small, FitNet, and SqueezeNet were benchmarked on CIFAR-10 and SVHN datasets for fair comparison. Results show that the method outperforms hand-designed models despite having fewer parameters. Model selection is crucial in Knowledge Distillation. Adding a size constraint via a reward function makes optimization more challenging. The results in TAB4 demonstrate that compression policies can produce sensible results despite constraints. The approach may be viable in practice, but reusing compression policies for new teacher architectures needs further investigation. Training a policy on one teacher model and applying it to another tests its general compression strategy. The initialized policy is expected to converge to a locally optimal policy with enough iterations. Performance measures over the first 10 policy update iterations are provided in TAB5. While there is a slight drop in accuracy with pretrained policies, the average reward is higher. Transfer learning from a smaller model to a larger model shows promising results in accelerating the policy search on deep networks. A novel method for compressing neural networks is introduced. The text introduces a novel method for compressing neural networks using a two-stage layer removal and layer shrinkage procedure. The approach efficiently learns to search model architectures by leveraging signals for accuracy and compression as supervision. The method performs well across various datasets and architectures, demonstrating generalization capabilities through transfer learning. It can also incorporate practical constraints like power or inference time, showing potential for real-world applications. The Actor-Critic algorithm replaces b k with V \u03b8 k to estimate the value function at each time step. Results on the MNIST dataset show slight stability improvement compared to vanilla REINFORCE, while CIFAR-10 results indicate better stability but lower performance. Grid search was used to select learning rates and batch sizes, with graphs showing convergence rates. The grid search on the MNIST dataset using the VGG-13 network determined lr=0.003 as the optimal learning rate and a batch size of 5 as the most effective. The implementation details include a removal policy with 2 hidden layers and 30 hidden units trained with Adam optimizer, and a shrinkage policy with 2 hidden layers and 50 hidden units also trained with Adam optimizer at lr=0.003. All experiments were conducted in PyTorch with 1 NVIDIA TitanX GPU. The policies were implemented with 2 hidden layers and 50 hidden units, trained with the Adam optimizer and a learning rate of 0.1. Each policy was trained for at least 100 epochs with a batch size of 5 rollouts. MNIST Teacher models were trained for 50 epochs with a starting learning rate of 0.01 and a batch size of 64. CIFAR-10/100 Teacher models were trained for 150 epochs with a starting learning rate of 0.001 and a batch size of 128. SVHN Teacher models were trained for 150 epochs with a starting learning rate of 0.001. The training procedures for Caltech256 and ImageNet32x32 models involved adjusting learning rates and using data augmentation techniques like mean subtraction, horizontal flipping, and random cropping. The Caltech256 models were trained from scratch with a learning rate reduction from 0.01 to 0.001 after 50 epochs. The ImageNet32x32 ResNet-34 teacher model was trained for 40 epochs with a learning rate reduction by a factor of 5.0 every 10 epochs. The design of the reward function for model compression involves penalizing models with high compression but low accuracy, and ensuring that the reward increases with both compression and accuracy. A symmetrical reward function based on validation accuracy and compression can lead to undesired outcomes. The reward function for model compression penalizes low accuracy with high compression and ensures increasing rewards with both accuracy and compression. Non-linear rewards outperform naive ones, with more complex functions also showing promise. Degenerate cases receive a fixed reward of -1, such as empty architectures. Empty architectures may output \"remove\" actions for each layer, resulting in no trainable parameters. Removing too many layers in a CNN can lead to large feature map sizes before fully connected layers, making training impractical. Specialized architectures like ResNet have inter-layer dependencies that must be met. Total training time estimates are provided for training policies. The approximate time taken to train layer removal policies for architecture optimization varies based on hardware and dataset. Training each student model for a few epochs to determine rewards can be computationally expensive. Initializing models with random weights may be an efficient evaluation method. Initializing models with random weights could be an efficient way to evaluate architectures, along with using hypernetworks for better initialization. Selecting an informative subset of the dataset and exploring transfer learning with pretrained policies on different architecture search problems are also interesting directions to consider."
}