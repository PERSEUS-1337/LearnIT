{
    "title": "BkxDKREtPS",
    "content": "We propose a new representation called the one-pixel signature to analyze convolutional neural networks (CNNs). This signature is created by generating an adversarial value pixel-by-pixel, showcasing the largest change in class prediction. It is independent of CNN architecture choices and can efficiently detect backdoored CNN classifiers. Classic networks like LetNet, VGG, AlexNet, and ResNet exhibit unique characteristics in their signature images. The proposed one-pixel signature representation can efficiently detect backdoored CNN classifiers, independent of architecture choices. CNNs have been widely used in various applications such as image labeling, object detection, and generative models. Despite significant progress in practical CNN-based methods for real-world applications like image captioning and 2D to 3D estimation/reconstruction, there is a lack of rigorous mathematical understanding and analysis for CNN classifiers. Issues such as model/data complexity, robustness, convergence, and invariants need further exploration. Additionally, a new problem has emerged where CNNs are trained with a backdoor, known as BadNets. This poses a security risk as illustrated by a backdoored/Trojan CNN classifier in Fig. 1. During training, a CNN classifier learns to make predictions matching ground-truth labels. However, if under a Trojan/backdoor attack, the CNN classifier becomes vulnerable and makes unexpected adverse predictions. Limited success in detecting backdoored CNNs has been achieved. One-pixel signature is developed to identify backdoored CNNs without accessing network architecture or model parameters. The effectiveness of using one-pixel signature for detecting backdoored CNNs under a Trojan attack is demonstrated. Various network architectures are studied, and the one-pixel signature is shown to be applicable for defending against Trojan attacks on object detectors. The representation is easy to compute and agnostic to specific CNN architectures and parameters, making it useful for analyzing both CNNs and standard classifiers. The backdoored CNN, known as CNNT rojan, is trained maliciously by inserting a \"virus\" pattern during training to force misclassification. The goal is to create a unique signature for CNN classifiers that is easy to compute and universal across network architectures. This signature differs from visualizing CNN filters or optimizing neural structures. In object recognition, a signature can be defined by searching for invariants in the filtering scale space. In object recognition, signatures can be defined by searching for invariants in the filtering scale space. Various methods have been proposed for characterizing neural networks, each with distinct definitions and methodologies. Some approaches map network parameters to an embedding space for optimization, while others study network backdoors. Existing methods have explored network representation similarity and pathologies of the hypothesis space, but their applicability to the network backdoor problem is unclear due to limitations such as fixed network type and computational complexity. Adversarial attacks, including white-box and black-box ones, are also related to this area of research. Adversarial attacks aim to build robust CNNs against adversarial inputs, while Trojan attacks focus on defending/detecting compromised CNN classifiers. Malicious manipulation of training data can create backdoors in networks, with attacks involving changing network parameters and settings. The use of one-pixel signatures can illustrate classical CNN architectures and recognize their types. The closest related work is BadNets, which focuses on presenting similar concepts. Our goal is to develop a representation as a hallmark for a neural network classifier that should be revealing, agnostic to architecture, with low computational and representation complexity, applicable to both whitebox and black-box inputs. We propose a one-pixel signature inspired by object signatures and one-pixel attack methods. The signature is generated for a given CNN classifier by visiting each pixel in an image and finding the largest possible change to the prediction. The signature for a pixel is the largest change to the prediction for that pixel, resulting in signature images for a CNN classifier. The goal is to map a CNN classifier to produce a signature of K image channels. The signature is defined as a default image with pixel values in the range of [0, 1]. The classifier generates classification probabilities based on the pixel values, changing only the value of a specific pixel while keeping the rest constant. The signature images for a CNN classifier are computed by changing the value of a specific pixel while keeping the rest constant. The significance of each pixel in predicting the k-th class is determined independently, reducing computation complexity. The overall complexity to obtain a signature is O(m \u00d7 n \u00d7 K \u00d7 V), with V as the search space for image intensity. The signature can be computed for a blackbox classifier without access to model parameters. The neural network backdoor/Trojan attack problem involves a developer creating a backdoored classifier that behaves normally on the test dataset but produces malicious predictions for specific compromised images. This can occur without the customer's knowledge, leading to potential security risks. The task is to defend against Trojan attacks on CNN classifiers by detecting backdoors. Trojan attacks involve manipulating training data to inject backdoors without compromising classification accuracy on the original dataset. The backdoored model should maintain high accuracy on normal input but successfully classify input with the \"virus pattern\". A CNN Trojan detector is trained to identify Trojan attacks in CNNs. The one-pixel signature is used to characterize different CNN architectures and detect backdoored classifiers. The one-pixel signature is used to differentiate CNN architectures like LeNet, ResNet, AlexNet, and VGG. Results show it can uniquely identify network architectures regardless of the dataset. Signatures of classic CNN models trained on ImageNet are visualized for class \"tench\". In the process of signature generation, v value is updated for all three channels simultaneously to reduce computational cost. One-pixel signature can detect trojan attacks in backdoored CNN architectures by differentiating between models with \"vaccine\" patterns and normal models. A classifier can be trained to detect a backdoored CNN network without knowing the architecture or the \"virus\" pattern by differentiating one-pixel signatures of vaccinated models from normal models. The experiment mainly used the MNIST dataset for training and testing data generation to evaluate the Trojan detector. Random patterns were used as \"vaccine\" to create CNNT rojan for training, while \"virus\" patterns were used to generate testing CNNT rojan. Two sets of 250 CNN models were trained - one injected with Fashion-MNIST images as \"virus\" patterns and the other with the original MNIST dataset, labeled as CNN Trojan. The experiment involved training a set of 250 CNN models with random patterns as \"vaccine\" to create backdoored CNNs for testing. Different patterns, parameter initializations, architectures, or learning strategies were used for each backdoored CNN. Clean CNNs were also trained without the vaccine patterns. One-pixel signatures were obtained for each CNN, and a Vanilla CNN classifier was trained as a Trojan detector using these signatures as input. The experiment involved training 250 CNN models with random patterns as \"vaccine\" to create backdoored CNNs. A Vanilla CNN classifier was then trained to detect Trojans using one-pixel signatures as input. The experiment involved training 250 CNN models with random patterns as \"vaccine\" to create backdoored CNNs. The results showed a successful detection rate of approximately 90% for different models using one-pixel signatures. The layout of clean and Trojan models' signatures were visually different, demonstrating the effectiveness of the detection method. The experiment demonstrated that one-pixel signatures can achieve a high detection rate for Trojans even without knowing the network architecture. Additionally, the potential applicability of one-pixel signatures for detecting backdoored object detectors was illustrated using images from the Open Image Dataset V4. The study showed that one-pixel signatures can differentiate models trained on different datasets, such as MNIST and Fashion-MNIST, using LeNet-5 and ResNet-8 architectures. Results from 1000 models of each architecture trained on both datasets were compared, along with 2000 models trained with mixed architectures. The signatures were fed into a Vanilla CNN classifier for evaluation, indicating the ability to identify the dataset used for training. The study demonstrated that one-pixel signatures can distinguish models trained on different datasets and architectures. A Trojan detector trained on data with vaccine patterns showed success in detecting single target backdoors but struggled with all-to-all attacks, particularly on the MNIST dataset. The detector's weakness was exposed in scenarios where all labels were maliciously altered, indicating potential compromise. In this paper, a novel framework is developed to unfold convolutional neural network classifiers by designing a revealing, easy to compute, agnostic, and canonical signature applicable to black-box networks. The one-pixel signature is shown to be informative for classic CNN architectures and for addressing backdoor detection challenges. It is agnostic of the classifier's architecture and can be extended to traditional machine learning classifiers. Signature images are generated for Random-Forest, SVM, decision tree, and Adaboost classifiers. The study focuses on benchmarking various Trojan attack strategies, including poisoning training data with multiple trojan patterns in changing sizes and locations. Using the MNIST dataset and LeNet-5 model, the results of different poisoning strategies were analyzed, showing that altering the size, location, and number of patterns can still lead to successful attacks. The study analyzed different Trojan attack strategies on the MNIST dataset using the LeNet-5 model. Changing the location and number of patterns can still generate successful backdoor models. Trojan patterns in the Testing set differ from those in the Training set, leading to better generalization. Moving patterns globally failed to generate backdoors as the model didn't converge. Backdoored CNN models referred to in the paper can respond to different trojan patterns in a local region of the image. The attack scheme minimally affects classification accuracy on the original dataset. The backdoor trigger pattern should perturb the original image minimally and preserve primary features. The results show a more detailed trojan insertion design. The study evaluated Trojan attack strategies on the MNIST dataset using the LeNet-5 model. Trojan insertion was done in each class to assess feasibility and detection effectiveness. Single class detection rate was around 98%, while mixed class detection rate was similar. Two methods for injecting backdoors were tested, both resulting in models functioning normally. The models could classify good samples accurately, but the signatures could reflect the presence of backdoors. The study analyzed the impact of training epochs on model signatures, showing stronger features with higher validation accuracy. Signatures become more defined and converged as more epochs are trained. Results from a ResNet-50 model on cifar-10 dataset at different epochs are illustrated. Different CNN classifiers on MNIST dataset show varying accuracy rates. The Faster RCNN model for object localization includes convolution layers, a Regional Proposal Network (RPN), and fully connected neural networks for classification. The model rescales images to 300 pixels on the shorter side and uses anchor sizes of 64^2, 128^2, and 256^2 pixels with aspect ratios of 1:1, 1:2, and 2:1. The classifier after ROI Pooling layer is used to generate a fixed size signature for the model. After ROI Pooling layer with pre-trained weights, a 7*7 signature image is obtained through a one-pixel method."
}