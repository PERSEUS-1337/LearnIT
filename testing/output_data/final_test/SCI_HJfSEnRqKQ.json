{
    "title": "HJfSEnRqKQ",
    "content": "Active learning with partial feedback (ALPF) addresses the mismatch between the form of labels and annotations in multiclass classification. The learner actively selects examples and asks binary questions to uncover partial labels, allowing for more efficient annotation of examples. Active learning with partial labels involves a sampling strategy to choose example-class pairs and learning from partial labels between rounds. Experiments on Tiny ImageNet show a 26% improvement in top-1 classification accuracy compared to baselines, with 30% of the annotation budget needed for full dataset annotation. ALPF-learners fully annotate TinyImageNet at 42% lower cost, challenging the idea that labels should only be solicited for hard examples. Active Learning (AL) aims to increase data efficiency by strategically selecting examples for annotation. Large-scale multi-class annotation is not atomic, requiring simpler feedback mechanisms like yes/no questions. Annotation pipelines filter candidates and use class hierarchies to reduce costs. Real-world annotation costs can vary per example. ALPF proposes Active Learning with Partial Feedback to reduce annotation costs by selecting examples and questions strategically. Learners choose (example, class) pairs, receiving binary feedback to obtain partial labels. The focus is on hierarchically-organized collections with atomic and composite classes. Active Learning with Partial Feedback (ALPF) aims to reduce annotation costs by strategically selecting examples and questions. The focus is on hierarchically-organized collections, using active questions to determine class membership efficiently. This approach involves selecting samples at random and then asking questions actively until finding the correct label. Active Learning with Partial Feedback (ALPF) aims to reduce annotation costs by strategically selecting examples and questions. ALPF learners have the option of choosing a different example for the next binary query, leading to efficient learning from partially-labeled data. The predictive distribution is parameterized by a softmax over all classes, converting the multiclass problem to a binary classification problem on a per-example basis. Active Learning with Partial Feedback (ALPF) introduces acquisition functions for soliciting partial labels, such as expected information gain (EIG), EDC, and ERC. ALPF learners are evaluated on CIFAR10, CIFAR100, and Tiny ImageNet datasets using WordNet for label hierarchy. The experiments simulate rounds of active learning with a small initial amount of data. In ALPF, feature vectors x and labels y are denoted by x \u2208 R d and y \u2208 Y, where Y = {{1}, ..., {k}}. The agent starts with an unlabeled training set D = {x 1 , ..., x n }. Composite classes are defined as a pre-specified collection C = {c 1 , ..., c m }, including atomic and composite classes. Partial labels, like \u1ef9 i \u2282 {1, ..., k}, do not fully indicate the underlying atomic class. In ALPF, an ALPF learner eliminates classes to obtain successively smaller partial labels until only the exact label remains. The learner interacts with annotators by choosing questions from a set of all pairs of examples and composite classes. The questions involve asking if an example contains a specific class. The ALPF learner eliminates classes to obtain smaller partial labels until only the exact label remains. The learner selects pairs for labeling and updates the partial label based on binary feedback, re-estimating the model and selecting another question. The ALPF learner uses partial labels to train its model, updating it periodically in batch-mode. The goals are to minimize error on labeled data and fully annotate datasets efficiently. Experimental results show the effectiveness of the ALPF strategy in learning from partial labels. The ALPF learner utilizes partial labels for training, updating the model periodically in batch-mode. The model estimates conditional probabilities of atomic classes and optimizes by minimizing log loss. The loss function simplifies to standard cross entropy when all examples are fully labeled. The ALPF learner uses partial labels for training and updates the model periodically in batch-mode. The model estimates conditional probabilities of atomic classes and optimizes by minimizing log loss. The loss function simplifies to standard cross entropy when all examples are fully labeled. Labels with probability 1 result in no update. Models are updated when predictions disagree with the current partial label, and Expected Information Gain (EIG) quantifies uncertainty in the predictive distribution. EIG is maximized when p(c, x, \u03b8) = 0.5. Expected Remaining Classes (ERC) is a heuristic that aims to quickly reach exactly-labeled examples by selecting those with the fewest expected remaining classes. This heuristic is inspired by BID2 and minimizes uncertainty in the predictive distribution. Expected Decrease in Classes (EDC) is a sampling strategy aimed at reducing the number of potential classes by selecting the question that leads to the smallest reduction in potential labels. ALPF algorithms are evaluated on various datasets with different sizes and classes, using the Wordnet hierarchy to determine the set of possible binary questions for each dataset. The number of questions for each dataset varies, with binary questions between re-trainings also differing. Warm-starting each learner with a small percentage of training examples is crucial. The experiments use the ResNet-18 architecture without fine-tuning techniques to focus on active learning. Some leaderboard scores may be higher due to pre-training on the full ImageNet dataset. The study uses the ResNet-18 architecture without fine-tuning techniques for active learning. They warm-start experiments with 5% labeled data and iterate until all examples are labeled, updating models in rounds. Training includes data augmentation techniques and the Adam optimizer outperforms SGD when learning from partial labels. The study uses ResNet-18 architecture for active learning, starting with 5% labeled data and iterating until all examples are labeled. The classifier is re-trained from scratch at each round with random initialization. Faster convergence by initializing with the previous best classifier leads to worse performance due to over-fitting. Optimization is terminated after 75 epochs. More frequent re-training did not show any benefit. Learning from partial labels is demonstrated with the loss function, showing efficacy in a partially labeled dataset simulation. The study demonstrates the effectiveness of learning from partial labels in a simulated dataset. By using a WordNet-derived hierarchy, experiments show that incorporating coarse-grained partial labels improves model accuracy. The performance of classifiers with and without partial labels is compared, with the observation that accuracy improves with additional partial labels, but diminishes as the labels become coarser. The study shows that model accuracy improves with coarse-grained partial labels. Different active learning approaches are compared, including baseline random sampling, active questions learners, and ALPF learners who can freely choose examples and questions. ALPF learners can freely choose any example-question pair at each turn, encountering partial labels during training. Experiments evaluate classification and annotation perspectives, measuring top-1 accuracy and the number of questions needed to label all training examples. Results show that AQ significantly improves over baseline methods, sampling examples randomly and labeling to completion before moving on. ALPF-ERC outperforms AQ methods by 4.5% on Tiny ImageNet at 30% budget. Comparisons between ERC, EDC, and uncertainty sampling are made. ERC performs well without warm-starting, while EIG benefits from a 5% warm-start.ERC may focus excessively on easy classes, as shown in an adversarial dataset test. ALPF-ERC outperforms AQ methods by 4.5% on Tiny ImageNet at 30% budget. ERC works well without warm-starting, EIG benefits from a 5% warm-start, and EDC struggles without warm-starting. ALPF learners using EIG select high-entropy data, while EDC and ERC choose examples with lower entropy predictions. The surprising performance of EDC and ERC may be due to the cost structure of ALPF.ERC focuses on \"easy\" examples, as seen in a test on a simulated dataset. ERC focuses on \"easy\" examples, testing its behavior on a simulated dataset with trivially easy classes. EIG splits its budget evenly among classes, while EDC and ERC concentrate on different classes at different stages. In the adversarial case, EIG quickly learns easy classes, while EDC and ERC exhaust easy ones first. EDC and ERC label all data with less total cost than EIG, but this behavior may be costly with trivial classes and a large unlabeled dataset relative to the budget. The text discusses binary identification and active learning methods. Binary identification involves finding optimal strategies using yes/no questions, while active learning selects examples based on uncertainty. Cost-aware sampling heuristics are mentioned, but dynamic cost changes during training are not addressed. Deep Active Learning (DAL) has recently emerged as a research area, exploring schemes combining traditional heuristics with pseudo-labeling and using Monte Carlo samples for uncertainty estimates. DAL has shown success in NLP tasks, with studies on sentiment classification. BID28 explores active learning for sentiment classification by proposing a new sampling heuristic. BID26 achieved state-of-the-art performance on named entity recognition using only 25% of the training data. BID13 and BID14 investigate uncertainty measures in neural network predictions. Papers like BID10, BID21, and BID2 focus on learning from partial labels, with different approaches to address the problem. Active learning with partial feedback introduces new algorithmic challenges such as changing partial labels and biased selection. Experiments validate this framework on large-scale classification benchmarks, showing that ALPF learners fully label data with 42% fewer binary questions compared to traditional active learners. Diagnostic analysis suggests starting with \"easier\" examples in ALPF can be more efficient. In experiments comparing models with warm-starting on i.i.d. data, ERC benefits while EIG and EDC perform worse without warm-starting. Increasing warm-starting from 5% to 10% does not improve performance further. Re-initializing and training models from scratch on Tiny ImageNet every 30K questions is crucial for good performance. Updating the model by fine-tuning 5 epochs after every 3K questions results in 10X faster model updating frequency, benefiting ALPF-EDC and ALPF-ERC."
}