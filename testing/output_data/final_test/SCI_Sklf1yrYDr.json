{
    "title": "Sklf1yrYDr",
    "content": "Ensembles of neural networks improve accuracy and predictive uncertainty by averaging predictions from multiple networks. However, the cost of training and testing an ensemble increases linearly with the number of networks. BatchEnsemble is a novel ensemble method with lower computational and memory costs compared to typical ensembles. It uses a unique weight matrix approach and is highly parallelizable. Across various tasks, BatchEnsemble shows competitive accuracy and uncertainties, with a 3X speedup at test time and 3X memory reduction for an ensemble of size 4. Additionally, BatchEnsemble performs well in lifelong learning scenarios. BatchEnsemble is a novel ensemble method with lower computational and memory costs compared to typical ensembles. It shows competitive accuracy and uncertainties, with a 3X speedup at test time and 3X memory reduction for an ensemble of size 4. Ensembling is a common technique in machine learning literature to improve performance by combining outputs of multiple models. Deep neural networks trained with different random seeds can converge to different local minima, leading to disagreements in predictions even if the models have converged. Ensembles of neural networks benefit from this by achieving better performance through averaging or majority voting on outputs. Diverse ensemble members lead to improved performance, as shown by Lakshminarayanan et al. (2017). Deep ensembles have been shown to provide reliable predictive uncertainty estimates and achieve the best performance on out-of-distribution uncertainty benchmarks. However, their computational and memory costs increase linearly with ensemble size, limiting their practical use beyond supervised learning tasks. In this paper, the authors introduce BatchEnsemble, a more parameter efficient ensemble model for lifelong learning. They address the computational and memory bottleneck by using a novel weight generation mechanism. Unlike traditional ensembles, BatchEnsemble reduces testing and memory costs significantly. BatchEnsemble is a mini-batch friendly ensemble model that reduces testing and memory costs significantly. It shows the best trade-off among accuracy, running time, and memory on various deep learning tasks. Additionally, BatchEnsemble is effective in uncertainty evaluation, lifelong learning, and can scale up to 100 sequential learning tasks without catastrophic forgetting. In this section, relevant background on ensembles, uncertainty evaluation, and lifelong learning is discussed for the proposed method, BatchEnsemble. Bagging, or bootstrap aggregating, combines different models to improve generalization performance. Ensembles perform at least as well as individual models, with the best performance achieved when members make independent errors. Extensive research has been done on ensembles for model improvement. Ensemble research focuses on reducing cost at test and training time. Methods include compressing complex ensembles into smaller models, distilling knowledge into a single neural network, forming ensembles with different training checkpoints, and using models with different regularization for better performance. Snapshot ensemble involves training a single model with cyclic learning rates. Ensemble methods like Snapshot ensemble and fast geometric ensemble aim to improve model performance by creating multiple local minima solutions or modes that can be connected by simple curves. Implicit ensembles, such as Dropout and MC-dropout, provide uncertainty estimates and improve calibration in deep neural networks. Bayesian neural networks and deep ensembles are used to model uncertainty in neural networks. Deep ensembles provide well-calibrated uncertainty estimates and are a simple alternative to Bayesian neural networks. Various metrics are used to measure the quality of uncertainty estimates, including the contextual bandits benchmark and Expected Calibrated Error (ECE). In lifelong learning, models train on tasks sequentially without. In lifelong learning, models train on tasks sequentially without access to previous tasks' data. The core difficulty is \"catastrophic forgetting,\" where neural networks tend to forget what they have learned. Previous work on alleviating this issue includes regularizing updates on the current task and maintaining a memory buffer for previous tasks' data. In lifelong learning, models combat catastrophic forgetting by penalizing the gradient on the current task to preserve memory. Different approaches include combining experience replay algorithms and increasing model capacity as new tasks are added, such as with progressive neural networks (PNN). Some methods expand the model in a more parameter efficient way, like applying group sparsity regularization. In lifelong learning, models combat catastrophic forgetting by penalizing the gradient on the current task to preserve memory. Different approaches include combining experience replay algorithms and increasing model capacity as new tasks are added, such as with progressive neural networks (PNN). Yoon et al. (2017) and Xu & Zhu (2018) apply group sparsity regularization to efficiently expand model capacity. They illustrate how ensemble weights are generated for multiple ensemble members, using shared and fast weights. Vectorization allows for parallel computation of ensemble weights within a device, making the mechanism parallelizable. Ensemble weights are generated for multiple ensemble members using shared and fast weights. Vectorization enables parallel computation of ensemble weights within a device, making the mechanism parallelizable. The next layer's activations for each ensemble member in a mini-batch are obtained by computing Eqn. 5, allowing efficient implementation of ensemble using GPU parallelism. During testing, the average of predictions of each ensemble member is taken. During testing, BatchEnsemble efficiently computes predictions by repeating the input mini-batch for each ensemble member, reducing computational cost. The only additional computation is the cheap Hadamard product. However, a limitation is that each ensemble member receives only a portion of input data if the minibatch size remains the same as single model training. Increasing the batch size in BatchEnsemble ensures each ensemble member receives the same amount of data as single model training, reducing computational overhead. Memory overhead is minimal as only vectors need to be stored, resulting in almost no additional memory cost compared to storing full weight matrices. For example, a BatchEnsemble of ResNet-32 with size 4 incurs only 10% more parameters. BatchEnsemble of ResNet-32 with size 4 incurs 10% more parameters compared to naive ensemble methods. The memory cost of ensemble methods limits their application in scenarios like multi-task and lifelong learning. BatchEnsemble allows for independent models for each task, with each ensemble member handling a lifelong learning task. BatchEnsemble allows for independent models for each task, with each ensemble member handling a lifelong learning task. It prevents catastrophic forgetting and has less memory consumption compared to progressive neural networks. BatchEnsemble can easily scale to up to 100 tasks and adapt to new tasks by training only fast weights. BatchEnsemble allows for independent models for each task, preventing catastrophic forgetting and having less memory consumption. It can scale to up to 100 tasks and adapt to new tasks by training only fast weights. However, limitations include only rank-1 perturbations for each task and shared weight training only on the first task, hindering explicit transfer between tasks. BatchEnsemble enables independent models for each task, preventing catastrophic forgetting and reducing memory consumption. It can handle up to 100 tasks and adapt to new tasks by training fast weights. However, explicit transfer between tasks is hindered by limitations such as only rank-1 perturbations for each task and shared weight training only on the first task. No lateral connections were needed for Split-CIFAR100 and Split-ImageNet, leaving room for future improvements in lifelong learning approaches. BatchEnsemble is showcased for lifelong learning on Split-CIFAR100 and Split-ImageNet datasets, which involve multiple tasks with new classes introduced sequentially. The Split-ImageNet dataset has more classes and higher image resolutions compared to Split-CIFAR100. More details about these datasets are provided in Appendix A. In lifelong learning on Split-CIFAR100, T = 20 tasks are considered using ResNet-18 with task-specific final dense layers. BatchEnsemble is compared to PNN, vanilla neural networks, and EWC. Results show accuracy, forgetting, and cost metrics. BatchEnsemble achieves comparable accuracy as PNN with a 4X speed-up and 50X less memory consumption. It also maintains the no-forgetting property of PNN, making it the best trade-off among all methods compared. On Split-ImageNet with 100 tasks, BatchEnsemble has a 20% parameter overhead compared to vanilla ResNet-50. BatchEnsemble achieves comparable accuracy as PNN with a 4X speed-up and 50X less memory consumption. It also maintains the no-forgetting property of PNN. Two baselines, BN-Tuned and naive ensemble, are used for comparison. Validation accuracy on each subsequent task is the evaluation metric. BatchEnsemble is evaluated on Transformer and large-scale machine translation tasks WMT14 EN-DE/EN-FR with an ensemble size of 4. Results show faster convergence and lower validation perplexity compared to a single model. The BatchEnsemble Transformer outperforms the big Transformer in validation perplexity, indicating potential for larger Transformers. Comparison with dropout ensemble shows BatchEnsemble's superiority. Evaluation on CIFAR-10/100 dataset with ResNet32 shows promising results. In this section, we compare BatchEnsemble to MC-dropout for achieving 100% training accuracy on CIFAR100 using ResNet32. Additional dense layer and dropout are added to match parameters. Training iterations are increased for BatchEnsemble, which is faster than training individual models sequentially. BatchEnsemble is faster than training individual models sequentially, even with longer training time. Increasing mini-batch size can lead to the same performance without needing more training iterations. Table 2 shows BatchEnsemble outperforms single models and MC-dropout in accuracy. Naive ensemble, with individually trained models, serves as an upper bound for comparison. BatchEnsemble's utility extends to uncertainty modeling in contextual bandits, as shown in Appendix D. It also maintains diversity among ensemble members for predictive tasks. In the context of BatchEnsemble's efficiency and performance in predictive tasks, the evaluation of uncertainty modeling on bandit data is highlighted. The importance of reliable uncertainty estimation in contextual bandits is emphasized, with the experiment showing that ensemble methods, like Dropout, can be effective for uncertainty modeling. Thompson sampling from ensemble members is used, and the competitive performance of Dropout on bandit problems supports the use of ensemble methods for uncertainty estimation. BatchEnsemble, an efficient method for ensembling and lifelong learning, achieves the best mean value on bandits tasks with ensemble sizes 4 and 8. It outperforms Dropout in terms of average performance and shows promising accuracy on CIFAR-10 corrupted dataset. Combining BatchEnsemble with dropout ensemble improves uncertainty prediction and removes computation and memory bottlenecks of typical ensemble methods. BatchEnsemble is a method for ensembling and lifelong learning that shows potential for improving lifelong learning. It performs well on CIFAR datasets and machine translation tasks. The datasets used include CIFAR-10, CIFAR-100, WMT16 English-German, and WMT14 English-French. The preprocessing schemes involve zero-padding, random crop, and horizon flip. Source and target tokens are processed into shared sub-word tokens. The dataset used for training includes WMT14 English-French with 36M sentence pairs and CIFAR-100 images split into tasks for classification. Each task consists of 100/T classes, simplifying the problem to a classification task. Standard data pre-processing techniques are applied. The training set undergoes standard data pre-processing techniques such as padding, random crop, and random horizontal flip. The dataset is split into tasks with each task containing 1000/T classes of images. BatchEnsemble implementation details are discussed, including weight regularization for ensemble members. In practice, regularizing the shared weight while leaving the fast weights unregularized works equally well. Diversity can be encouraged by initializing fast weights randomly and training ensemble members with different sub-batches. Machine Translation involves training Transformer models for specific steps to reach targeted perplexity on validation. Experiments were conducted using 4 NVIDIA P100 GPUs, with the BLEU score of Big Transformer on English-German task shown in Figure 5. Despite BatchEnsemble having lower perplexity, a better BLEU score was not observed. The dropout rate for Transformer base was 0.1 and 0.3 for Transformer big on English-German, while remaining 0.1 on English-French. Classification involved training the model with a mini-batch size of 128 and maintaining a standard learning rate schedule for ResNet. The learning rate decreases from 0.1 to 0.01, from 0.01 to 0.001 at halfway and 75% of training. Weight decay coefficient is 10^-4. Ensemble size is 4 with 32 training examples per member. Training budget is 250 epochs. Recent lifelong learning methods like Dynamically expandable networks and Reinforced continual learning show competitive performance. These methods are memory efficient compared to progressive neural network. All three methods yield similar accuracy in Split-CIFAR100 task. BatchEnsemble is compared to PNN in terms of accuracy and cost on Split-CIFAR100. BatchEnsemble shows better computational and memory efficiency, especially on large lifelong learning tasks like Split-ImageNet. The results are similar to previous studies, with BatchEnsemble performing well on out-of-distribution examples from unseen classes. Ensembles of models, including BatchEnsemble and naive ensemble, show higher uncertainty on unseen classes compared to single models. The ensemble weight generation mechanism does not degrade uncertainty modeling. Expected Calibration Error (ECE) calculations on CIFAR-10 and CIFAR-100 further support this finding. BatchEnsemble outperforms single neural networks in calibration, as shown in Table 6b. ECE is used to measure calibration, with BatchEnsemble showing better predictions. The calibration of different methods is evaluated on the CIFAR-10 corruption dataset, which contains various image corruptions. BatchEnsemble is compared to dropout ensemble, with naive ensemble as an upper bound. BatchEnsemble achieves superior calibration compared to dropout ensemble, especially as skew intensity increases. It requires fewer forward passes for optimal performance and is more cost-effective in testing time. Combining BatchEnsemble and dropout ensemble improves calibration and memory efficiency, demonstrating their complementary nature. The diversity among ensemble members is crucial for accurate predictions, particularly on CIFAR-10 test examples where single models fail but ensembles succeed. In Figure 8, examples from the final models are randomly selected and their prediction maps are plotted. The ensemble model outperforms individual models on CIFAR-10 classification task. BatchEnsemble is compared to a naive ensemble of small models on CIFAR-10/100 dataset, showing better performance with similar memory consumption. The results in Table 5 demonstrate that BatchEnsemble outperforms a naive ensemble of small models in terms of accuracy with the same memory budget."
}