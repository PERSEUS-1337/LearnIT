{
    "title": "ByOnmlWC-",
    "content": "Genetic algorithms are commonly used in optimization problems, but have not been effective in deep reinforcement learning due to parameter crossovers. Genetic Policy Optimization (GPO) introduces a new genetic algorithm for deep policy optimization using imitation learning and policy gradient methods. Experiments on MuJoCo tasks show that GPO outperforms state-of-the-art policy gradient methods. Reinforcement learning (RL) has made significant progress in various domains such as games, locomotion control, visual-navigation, and robotics using deep neural networks (DNNs) as powerful functional approximators. Policy gradient methods are used to optimize policies by estimating gradients from rollout trajectories. Evolution strategies (ES) is a stochastic optimization technique that searches the policy space without gradient backpropagation. It samples parameter vectors, evaluates fitness, and updates the parameter distribution iteratively. Examples include Covariance Matrix Adaptation Evolution Strategy (CMA-ES) and recent advancements in policy optimization. Genetic algorithms (GAs) are heuristic search techniques inspired by natural selection, evolving genotypes using mutation, crossover, and selection operators. Unlike ES, GAs use the crossover operator to increase diversity in the population. The crossover operator in genetic algorithms is often used on the parameter representations of parents, but it can be problematic for nonlinear neural networks. NeuroEvolution of Augmenting Topologies (NEAT) offers a solution by evolving neural networks through evolutionary algorithms like GA, but it has limitations in deep reinforcement learning for complex tasks. Designing an efficient crossover operator is crucial for combining parent policies represented by neural networks in policy optimization. Genetic Policy Optimization (GPO) introduces a new genetic algorithm for deep policy optimization. It utilizes imitation learning for policy crossovers in the state space and advanced policy gradient methods for mutation, resulting in superior performance on continuous control tasks. Genetic Policy Optimization (GPO) outperforms policy gradient methods in continuous control tasks by maximizing rewards in a Markov Decision Process (MDP) environment with high-dimensional, continuous state and action spaces. The goal is to learn a policy that maximizes expected rewards starting from an initial state. Policy-based RL methods aim to find an optimal policy directly in the policy space by parameterizing the policy and using gradient descent methods to optimize it. The REINFORCE algorithm calculates the gradient using the likelihood ratio trick and updates the policy parameters based on the approximation to the policy gradient. The advantage actor-critic (A2C) algorithm utilizes the state value function to enhance performance. The A2C algorithm reduces gradient variance by using the state value function. TRPO and PPO are methods that control policy updates to prevent large changes. PPO is a first-order approximation of TRPO that penalizes KL divergence changes between old and updated policies. The penalty weight \u03b2 is adaptive and adjusted based on observed change in KL divergence after multiple policy update steps. Evolutionary algorithms are gaining interest in RL for policy search procedures. Neuroevolution uses evolutionary algorithms to generate neural network weights and topology. NEAT has been successfully applied in control tasks. NEAT and its extensions like HyperNEAT have been used for policy optimization in control tasks and learning to play Atari games. CoSyNE algorithm has shown favorable results compared to Q-learning and policy-gradient based RL algorithms. Evolution Strategies have also been proposed for black-box policy optimization. CMA-ES samples candidate parameter vectors using a Gaussian perturbation on the mean vector, updating it based on the fitness of the candidates. Cross-Entropy methods have shown effectiveness in simple environments. Existing neuroevolution algorithms perform crossover and mutation on policy networks with fixed topology. In this work, policy gradient algorithms are used for efficient mutation of high-dimensional policies, implementing a crossover operator. The procedure for policy optimization involves evolving policies through selection, crossover, and mutation operators. The algorithm starts with an ensemble of parents and adds the resulting children to the population for genetic optimization. Different crossover strategies for neural network policies are illustrated in Figure 1. The distribution plot next to each policy shows the state-space where high returns are achieved. Our proposed state-space crossover operator aims to improve policy behavior. Policies are mutated individually using standard policy gradient methods. Parents are selected based on fitness to create new child policies through crossover. The population for the next generation is obtained by combining all policies. The algorithm terminates after k rounds of optimization, considering policies parameterized using deep neural networks. If the policy is Gaussian, the network outputs the mean and standard deviation of each action. Combining two DNN policies to create a child policy that absorbs the best traits of both parents is complex. Different crossover strategies are illustrated in Figure 1, showing neural network policies and state-visitation distribution plots. The state-visitation distributions indicate good policies of the parents. The state-visitation distributions of parents indicate good policies, with non-overlapping distributions showing good state-to-action mapping. Crossover in parameter space creates a DNN child policy by copying edge weights from parents, but this may lead to low performance due to complex interactions. Ideal crossover in state-space combines regions from both parents for better performance. In this work, a new crossover operator using imitation learning combines traits from both parents to generate a high-performance child policy. The crossover is done in the behavior or state visitation space, not directly in the parameter space. The effect of these crossovers is quantified by mixing DNN pairs and measuring policy performance in a simulated environment. Imitation learning includes behavioral cloning and inverse reinforcement learning, with behavioral cloning used in this study. The paper introduces a new crossover operator using imitation learning to generate a high-performance child policy by combining traits from both parents in the behavior space. Policy gradient algorithms are utilized for mutation of neural network weights, maintaining genetic diversity in the population. The genetic algorithm achieves similar or higher sample efficiency compared to state-of-the-art methods. The section details three genetic operators for mixing input policies in the state space. The paper introduces a new crossover operator using imitation learning to generate a high-performance child policy by combining traits from both parents in the behavior space. The operator mixes two input policies \u03c0 x and \u03c0 y in statespace to produce a new child policy \u03c0 c with identical network architecture. The child policy is learned using a two-step procedure involving training a two-level policy \u03c0 H and a binary policy \u03c0 S. Training objective for \u03c0 S is weighted maximum likelihood using trajectories from the parents involved in the crossover. The paper introduces a new crossover operator using imitation learning to generate a high-performance child policy by combining traits from both parents in the behavior space. The operator mixes two input policies \u03c0 x and \u03c0 y in statespace to produce a new child policy \u03c0 c with identical network architecture. The child policy is learned using a two-step procedure involving training a two-level policy \u03c0 H and a binary policy \u03c0 S. The log loss for \u03c0 S is calculated using high-reward trajectories from the parents to avoid negative behavior transfer. The information from \u03c0 H is distilled into \u03c0 c using imitation learning, where trajectories from \u03c0 H are used as supervised data to train \u03c0 c. The paper introduces a new crossover operator using imitation learning to generate a high-performance child policy by combining traits from both parents in the behavior space. To avoid compounding errors due to state distribution mismatch between the expert and the student, the Dataset Aggregation (DAgger) algorithm is adopted. Training involves sampling trajectories from the current student, labeling actions using the expert, and minimizing the KL-divergence loss to achieve a policy that performs well under its own induced state distribution. The paper introduces a new crossover operator using imitation learning to generate a high-performance child policy by combining traits from both parents in the behavior space. This operator modifies each policy of the input policy ensemble by running iterations of a policy gradient algorithm, leading to genetic diversity and good exploration of the state-space. The gradients for policy \u03c0 i are calculated using high-variance gradients estimated from rollout trajectories. The paper introduces a new crossover operator using imitation learning to generate a high-performance child policy by combining traits from both parents in the behavior space. It utilizes an MLP to model the critic baseline for advantage estimation and discusses the differences between PPO and A2C in terms of policy updates. Data-sharing among policies in the ensemble is also highlighted for off-policy learning. A larger data-batch improves gradient estimation and stabilizes learning in policy gradient methods. The operator returns policy-couples for crossover based on fitness evaluation. The crossover operator selects couples with maximum fitness based on performance and diversity criteria. A trade-off between high performance and diversity is achieved through a linear combination of fitness measures. Initially, a higher weight is given to diversity to explore the state-space, which is later annealed to encourage high-performance policies. The GPO algorithm is tested on continuous control benchmarks using Gaussian control policies. Experiments include analyzing the crossover operator, learning curves, comparisons with baselines, and scalability discussions. All experiments are conducted using the OpenAI rllab framework on 9 locomotion tasks in the MuJoCo physics simulator. The GPO algorithm uses Gaussian control policies with neural networks for mean parameterization. PPO and A2C are used as policy gradient algorithms for mutation. The crossover operator is evaluated on the HalfCheetah environment to measure its efficacy. The GPO algorithm utilizes Gaussian control policies with neural networks for mean parameterization, while PPO and A2C serve as policy gradient algorithms for mutation. The crossover operator is tested on the HalfCheetah environment to assess its effectiveness in improving policy performance. The performance of various policies involved in 8 different crossovers is plotted, showcasing the average episode reward for child policies after state-space and parameter-space crossovers compared to the parents. Additionally, state visitation distributions for high reward rollouts are analyzed for the parent and child policies. The crossover operator in the GPO algorithm is tested on the HalfCheetah environment using Gaussian control policies with neural networks for mean parameterization. The performance of child policies after state-space and parameter-space crossovers is compared to the parents, showing that state-space crossover often leads to improved performance while parameter-space crossover can result in low performance. High reward rollouts from the child policy with state-space crossover visit regions frequented by both parents. In this subsection, policies obtained with state-space crossover visit regions frequented by both parents, unlike parameter-space crossover where policies meander in regions with weak supervision. GPO is compared to standard policy gradient algorithms, running for 12 rounds with a population size of 8 and simulating 8 million timesteps. Two baselines using the same data amount are also compared: Single trains 8 independent policies without crossover, while Joint trains a single policy with policy gradient. The GPO algorithm outperforms Single and Joint baselines in various environments when using PPO as the policy gradient method. GPO shows better performance in Walker2D and HalfCheetah environments, despite Joint having a larger batch-size for gradient updates. The GPO algorithm outperforms baselines in Walker2D and HalfCheetah environments, showing better exploration and exploitation. Results indicate genetic algorithms could compete with state-of-the-art policy gradient methods. Ablation studies measure performance impact of different operators. The GPO algorithm outperforms baselines in Walker2D and HalfCheetah environments, showing better exploration and exploitation. Results indicate genetic algorithms could compete with state-of-the-art policy gradient methods. Ablation studies measure performance impact of different operators, such as crossover, selection, and data-sharing during mutation in GPO. The symbols C, S, and M are used to denote the presence of these components in the algorithm, with each component providing some improvement over training an ensemble of policies that do not interact. The GPO algorithm outperforms baselines in Walker2D and HalfCheetah environments, showing better exploration and exploitation. Results indicate genetic algorithms could compete with state-of-the-art policy gradient methods. Ablation studies measure performance impact of different operators, such as crossover, selection, and data-sharing during mutation in GPO. Each component applied in isolation provides improvement, with data-sharing having the highest benefit. Combining components leads to better performance, and using all components results in GPO with the best performance. The SELECTION operator selects high-performing individuals for crossover in every round of Algorithm 1. Natural selection weeds out poorly-performing policies during the optimization process. Figures show the average episode reward for policies in the ensemble at the final round of GPO, which are more robust compared to policies trained using the Single baseline. Experimenting with varying population sizes for GPO shows consistent performance with the same batch-size for gradient steps. Genetic Policy Optimization (GPO) is a scalable optimization procedure that combines evolutionary algorithms and reinforcement learning. It efficiently performs policy crossover in state space using imitation learning and mutates policy weights with advanced policy gradient algorithms. The experiments demonstrate the benefits of crossover in state-space for deep neural network policies. GPO shows improved exploration and exploitation in Walker2D and HalfCheetah environments, outperforming state-of-the-art policy gradient methods. The algorithm utilizes genetic operators like MUTATE and CROSSOVER, which are well-suited for multiprocessor parallelism. Genetic Policy Optimization (GPO) combines evolutionary algorithms and reinforcement learning for efficient policy crossover in state space. Experiments show GPO outperforms state-of-the-art policy gradient methods in MuJoCo locomotion tasks. Future advances in policy gradient methods and imitation learning may further enhance GPO's performance in challenging RL tasks. The crossover stage in Genetic Policy Optimization (GPO) involves training the binary policy (\u03c0 S) and imitation learning using expert trajectories from the parents. The dataset is filtered based on trajectory rewards, and supervised training is done for 100 epochs. Expert trajectories are obtained from \u03c0 H, and imitation learning is performed for 10 iterations using existing expert trajectories and new trajectories sampled from the student policy \u03c0 (i) c. The student policy is updated by minimizing the KL divergence objective over the transitions sampled from the expert. The training process involves 10 epochs with Adam, and the wall-clock time for GPO is compared to a Joint baseline. Mutate takes up a significant portion of the total time due to data-sharing overheads, but there is room for efficiency improvement by using MPI. The efficiency of Mutate can be improved by using MPI. Joint trains 1 policy with 8\u00d7 the number of samples as each policy in the GPO ensemble. Sample-collection exploits 8-way multi-core parallelism by simulating multiple independent environments in separate processes. Results show that a population of 32 is competitive in terms of sample complexity. The sample complexity for a population of 32 is competitive with the default GPO value of 8."
}