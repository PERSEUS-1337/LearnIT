{
    "title": "rJIgf7bAZ",
    "content": "Abstraction is crucial for intelligent learning systems to make complex decisions. The options framework allows for abstraction over sequences of decisions, but current models require options to be predefined, limiting scalability. Learning options directly from interaction with the environment is preferred, but remains challenging. A novel policy gradient method is developed in this work for automatic learning of policies with options, improving all options available to an agent simultaneously. The options framework allows for abstraction over sequences of decisions, improving all options available to an agent simultaneously. Experimental results show that the learned options can be interpreted and the method is more sample efficient than existing methods, leading to faster and more stable learning of policies with options. Learning temporal abstraction in reinforcement learning has been challenging, with existing methods relying on predefined sequences of actions. Recent developments, such as the Options framework, offer a more flexible approach to learning temporal abstraction. Prior work focused on discovering subgoals in state space to construct options, but this often requires a model of the environment dynamics. Our contributions aim to address this limitation by proposing alternative methods for learning temporal abstraction. Our contributions build on the Options framework to learn control abstractions without relying on a model of environment dynamics. We introduce an algorithm that optimizes returns by learning a set of options and relaxing key assumptions from previous work. This approach improves sample efficiency and allows for seeding control policies from expert demonstrations. The set of options is viewed as latent variables that represent the agent's behavior, enhancing the learning of temporal abstraction in reinforcement learning. Our algorithm improves sample efficiency by simultaneously enhancing all potential options and the policy over options in an end-to-end manner. It is evaluated on continuous MDP benchmark domains and compared to previous reinforcement learning methods. Recent research in option discovery focuses on learning options to reach specific subgoals within the environment, with some work concentrating on discrete state and action spaces while others explore subgoal states in continuous spaces using clustering or spectral methods. BID6 proposes defining subgoals of new policies based on existing options' initiation conditions, requiring a system model or assumptions about the environment. Training policies for each subgoal independently can be data and time-intensive. Another approach involves optimizing function approximation parameters for hierarchical policies, incorporating option indexes into states or trajectories. Option termination, selection, and inter-option behavior depend on system state and option index. This method is suggested for learning hierarchical model parameters in the option-critic architecture. In the option-critic architecture, BID10 proposes using neural networks for option-specific value functions and explicit state space partitioning for policy specialization. Options are treated as latent variables, requiring linear state features for policy optimization using an expectation-maximization approach. This method relies on information from the entire trajectory before policy improvement, eliminating online updates. In the option-critic architecture, BID10 suggests using neural networks for option-specific value functions and state space partitioning for policy specialization. BID5 also utilizes neural network policies in imitation learning. Various works in hierarchical reinforcement learning explore different approaches, such as higher-level policies setting goals for lower-level policies or specifying a prior over lower-level policies for different tasks. The agent interacts with the environment at discrete time steps, observing the state vector at each step. The agent interacts with the environment by observing states and selecting actions based on a stochastic policy. The goal is to maximize the expected return per trajectory using policy gradient methods. Policy gradient methods optimize \u03c1 by performing stochastic gradient ascent on the parameters \u03b8 of a family of policies \u03c0 \u03b8. The full return likelihood ratio gradient estimator is a simple and general policy gradient estimator that can be importance sampled if observed trajectories are not generated from the agent's policy. The policy gradient theorem expands on this, providing a gradient estimate with lower variance. The options framework allows for abstraction over sequences of decisions in RL. The options framework in RL involves abstraction over sequences of decisions. Agents have access to a set of options with their own policies, initiation sets, and termination functions. Options are selected and executed based on policies, with the policy over options used to choose the next option after termination. The options framework in RL involves abstraction over sequences of decisions. Agents select options based on policies and termination functions. To learn options using a policy gradient method, all aspects of the policy are parametrized. The goal is to optimize the agent's performance with respect to a set of policy parameters by maximizing the expected return of trajectories in the MDP sampled using the current policy. The Inferred Option Policy Gradient algorithm aims to maximize expected performance by increasing the probability of visiting highly rewarded state-action pairs. This is achieved by updating all options that could have generated the state-action pair through a differentiable inference step. The algorithm optimizes the policy end-to-end by decomposing the probability of trajectories and using the \"likelihood ratio\" method to estimate the gradient from samples. Unlike the REINFORCE policy gradient, actions are not independent even when conditioned on states. The Inferred Option Policy Gradient algorithm maximizes expected performance by updating options through a differentiable inference step. The inner gradient is computed by marginalizing over hidden options at each time step, allowing for gradient descent to maximize the objective using sampled data. The variance of the estimator can be reduced through inclusion of a constant baseline. The algorithm for learning options to optimize involves simplifying the estimator by removing terms from the gradient estimator and using a state-dependent baseline. The value function is learned using gradient descent on the mean squared prediction error of Monte-Carlo returns. Estimating the value function can also be done using other standard methods such as LSTD or TD(\u03bb). The algorithm for learning options to optimize returns involves initializing parameters randomly, sampling actions according to the current intra-option policy, updating parameters based on sampled episodes, and evaluating the effectiveness of the algorithm and learned options. The algorithm's performance was evaluated in various continuous control environments in the OpenAI Gym using the MuJoCo physics simulator. The environments included Hopper-v1, Walker2d-v1, HalfCheetah-v1, and Swimmer-v1, all requiring the agent to operate joint motors to move with penalties for unnecessary actions. The algorithm (IOPG) was compared with option-critic (OC) and asynchronous actor-critic (A3C) methods. The study compared the performance of IOPG with option-critic (OC) and asynchronous actor-critic (A3C) methods in various continuous control environments. The algorithms used multiple agents operating in parallel, with similar parametrized actor architectures and neural network structures. The option-critic algorithm did not require SMDP-level value function approximation, and all three algorithms closely followed the model architecture of BID18. The study compared IOPG with OC and A3C methods in continuous control environments. Policies and termination networks were implemented using linear layers. Option termination was determined by a linear sigmoid layer. The policy over options was represented by a linear softmax layer. The value function was represented by a linear layer. Weight matrices were initialized with normalized rows. RMSProp was used for optimization. Entropy regularization was employed to encourage exploration. IOPG, OC, and A3C methods were compared in continuous control environments using linear layers for policies and termination networks. IOPG showed faster learning than A3C, with all options able to utilize gathered data. OC had slower learning due to options not being learned simultaneously. Visualization of options over states was performed to understand their nature. Visualization of options over states was performed using T-SNE BID8 on a random subsample of states in the last 8000 frames. The results show that different options are active in different regions of the state space, indicating spatial coherence and structure in the policy. Options likely have temporal coherence as well, representing abstract behaviors that extend over several actions. Additional analyses of the learned options are displayed in FIG4. In the Walker2d environment, agents with four or eight options perform equally well, while having only two options leads to sub-optimal performance. This suggests that only three of the options learned are useful due to the simplicity of the environment. The options learned by IOPG are temporally extended, with termination decreasing as options improve. The new algorithm introduced in this paper, called inferred option policy gradients, allows for learning hierarchical policies within the options framework. Gradients are propagated through a differentiable inference step, enabling end-to-end learning of option policies, selection, and termination probabilities. Unlike other algorithms, this approach ensures that options do not become 'responsible' for unlikely states or actions they generated, encouraging them to specialize more effectively. The new algorithm, inferred option policy gradients, enables learning hierarchical policies within the options framework. Specialization in a part of the state space increases interpretability of options. Performance in the Walker2d environment improves with multiple options available. Option selection becomes more focused during training, with a few options being consistently chosen. The probability of staying in an active option increases as options improve, indicating temporal extension. Learning with inferred options is faster than learning with an option-augmented state space. Learning with inferred options is faster than using a comparable non-hierarchical policy gradient method, despite having more parameters. Option inference encourages intra-option learning, allowing multiple options to improve from a single learning experience, resulting in a speed-up. Future work aims to quantify the transferability of learned options between tasks and explore an online, actor-critic version for continuous learning in infinite-horizon problems."
}