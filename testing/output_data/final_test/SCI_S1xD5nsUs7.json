{
    "title": "S1xD5nsUs7",
    "content": "In this paper, the authors investigate lossy compression of deep neural networks (DNNs) using weight quantization and lossless source coding for memory-efficient deployment. They introduce universal DNN compression through universal vector quantization and source coding, specifically focusing on universal randomized lattice quantization of DNNs. Experimental results show compression ratios of $47.1$ and $42.5$ for compressing the 32-layer ResNet and AlexNet models, respectively. In this paper, DNN weight quantization is explored for compressing deep neural networks, along with weight pruning, to generate compact models. Various techniques such as soft weight sharing and vector quantization are discussed for efficient deployment. Weight pruning and lattice quantization are extensively studied methods for compression. This paper explores DNN weight quantization for compressing deep neural networks, along with weight pruning, to create compact models. It discusses techniques like soft weight sharing and vector quantization for efficient deployment. The efficiency of lattice quantization depends on source statistics, while randomized lattice quantization with uniform random dithering provides near-optimal performance for any source distribution. The paper introduces a universal DNN compression framework involving universal quantization and lossless source coding techniques. It also proposes a fine-tuning algorithm for vector quantized DNNs to recover accuracy loss. Vector quantization involves generating n-dimensional vectors from N weights, with the gain of fine-tuning increasing as the vector dimension grows. Randomized lattice quantization achieves universally good performance regardless of source statistics at any rates by adding uniform random dithers to n-dimensional vectors. This method ensures quantization error independence from source statistics. After adding uniform random dithers to n-dimensional vectors, lattice quantization is applied, followed by fine-tuning the vector quantization codebook to recover the loss after quantization. The codebook overhead limits the gain of vector quantization in practice, especially as the dimension n increases. After adding uniform random dithers to n-dimensional vectors, lattice quantization is applied, followed by fine-tuning the vector quantization codebook to recover the loss after quantization. The codebook is fine-tuned separately by dividing weights into nk VQ groups and updating their shared quantized values based on the average gradient of the network loss function. Individual quantized vectors are also updated following their shared quantized vectors in the codebook. In randomized lattice quantization, average gradients are computed for the updates. Universal source coding algorithms are more practical than Huffman coding as they do not require estimating source statistics. They use dictionary-based coding with a smaller codebook overhead. In lattice quantization, average gradients are computed from the network loss function after canceling random dithers, while shared values are fine-tuned before canceling dithers. The indices from the lattice quantization output codebook are inputted into a universal source coding scheme for compression. Using randomized lattice quantization, the decoder decompresses the fine-tuned lattice quantization output from the compressed stream and cancels the dithering vectors to obtain the deployed weights of the universally quantized DNN. The proposed DNN compression scheme is evaluated without pruning for the ResNet-32 model on CIFAR-10 dataset, considering two cases for (randomized) lattice quantization with different uniform boundaries. The quantization bin size \u2206 is the same for both cases, with case (b) placing the zero element at the center of the middle bin. Randomized lattice quantization shows good performance in both cases, benefiting from uniform dithering before quantization. Vector quantization outperforms scalar quantization, especially at high compression ratios. Compression ratios for pruned ResNet-32 and AlexNet BID20 models are summarized in TAB1. The proposed universal DNN compression scheme with the bzip2 BID17 algorithm achieves significant compression ratios of 47.10\u00d7 for ResNet-32 and 42.46\u00d7 for AlexNet. This method offers a better trade-off between compression rate and accuracy loss compared to other compression techniques. The weights of the pre-trained ResNet-32 model are vectorized for vector quantization, followed by lattice quantization or randomized lattice quantization. In this experiment, randomized lattice quantization is compared to lattice quantization for DNN compression. Randomized lattice quantization performs well for all cases, while lattice quantization only performs well when quantization bins are optimized for weight distribution. Randomized lattice quantization is applicable to any network models without optimization and provides a good rate-distortion trade-off. The performance of universally quantized ResNet-32 models before and after fine-tuning the codebook is shown in FIG2. The gain of vector quantization is more significant after fine-tuning, especially as the vector dimension increases. Pruning, quantization, and source coding contribute to the compression ratio improvement for the AlexNet model. Using universal quantization and bzip2 source coding, a compression ratio of 42.46 is achieved with a top-1 accuracy of 57.02%. Comparisons are made between Huffman coding and Lempel-Ziv-Welch (LZW) and bzip2 source coding algorithms. In experiments, LZW and bzip2 with Burrows-Wheeler transform BID16 offer better compression ratios than Huffman coding."
}