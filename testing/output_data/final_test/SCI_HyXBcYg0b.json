{
    "title": "HyXBcYg0b",
    "content": "Graph-structured data like social networks, brain networks, and gene regulatory networks have sparked interest in applying deep learning to graph domains. This paper focuses on designing neural networks for graphs of variable length to tackle tasks such as vertex classification, graph classification, regression, and generative tasks. The comparison between recurrent neural networks (RNNs) and convolutional neural networks (ConvNets) for graph learning tasks is explored. Existing graph RNN and ConvNet architectures are reviewed, and extensions of LSTM and ConvNet to graphs of any size are proposed. Controlled experiments are designed to solve basic graph problems. The proposed graph ConvNets outperform graph RNNs by 3-17% in accuracy and are 1.5-4x faster. They are also 36% more accurate than variational techniques. The most effective graph ConvNet architecture includes gated edges and residuality, which provides a 10% performance gain. ConvNets and RNNs are successful in computer vision and natural language processing tasks but require regular data domains. In contrast, graph data is not regular but heterogeneous. Neural network techniques for graphs go beyond computer vision and natural language processing, focusing on heterogeneous graph domains like social networks and gene regulatory networks. Different neural network architectures are classified based on fixed length graphs and variable length graphs. Convolutional neural networks on spectral graph theory have been developed for fixed length graphs, utilizing graph Laplacian and Chebyshev polynomials for linear complexity. Filters for spatial localization were explored in various ways by different studies. BID9 utilized Chebyshev polynomials for linear complexity on sparse graphs, BID21 focused on narrow-band frequencies with Cayley polynomials, and BID27 addressed multiple fixed graphs. BID19 simplified spectral convnets by using 1-hop filters for semi-supervised clustering. Other works introduced generic formulations for graphs with variable lengths, such as BID12 and BID29 using recurrent neural networks. BID22 extended this with a GRU architecture, BID30 introduced a vanilla graph ConvNet for communication tasks, BID25 added an edge gating mechanism for semantic role labeling, and BID4 designed a novel architecture. In this work, the focus is on studying neural networks like RNNs and ConvNets in the context of graphs with arbitrary length. The text discusses existing techniques, new graph NN models, and numerical experiments. A standard RNN for word prediction in natural language processing is considered, where the feature vector for each word is computed based on the previous step and the current word. The neighborhood concept for regular RNNs is defined as the previous step in the sequence. The notion of neighborhood in graph RNNs is defined by the graph structure. The feature vector for a graph RNN includes a data vector and feature vectors of neighboring vertices. Properties of f G-RNN include locality, weight sharing, and independence of graph length. The mapping f takes an unordered set of neighboring vertex feature vectors and a data vector to define the feature vector in a graph RNN. The neighborhood transfer function in graph RNNs, referred to as f G-RNN, maps neighboring vertices and a data vector. In a graph, neighbors are not distinguishable unless edges are weighted or annotated. The center vertex is the only special vertex around which the neighborhood is built, leading to a generic formulation. Early work on graph RNNs for arbitrary graphs used a vanilla RNN with a multilayer perceptron to define the feature vector h i. The iterative scheme proposed by BID22 for minimizing Eq. (4) involves the use of gated recurrent units (GRU) and Hadamard point-wise multiplication. This model has been applied in NLP tasks and quantum chemistry for fast organic molecule properties estimation. Tree-Structured LSTM model by BID31 extends the original LSTM model to a tree-graph structure, allowing for efficient updating of feature vectors without the need for iterative processes. The model utilizes a gate function f ij to control information flow between neighboring nodes, making it suitable for tasks in quantum chemistry and NLP. The information flow between neighbors in learning systems on graphs is crucial, with some neighbors being irrelevant. Different approaches like adding a gated mechanism to ConvNets have been used for tasks like language modeling and image generation. In computer vision, feature vectors are updated in neighborhoods of pixels using non-linear transformations. In ConvNets, feature vectors are updated in neighborhoods of pixels using non-linear transformations. The neighborhood transfer function in graph ConvNets maps input vectors to neighboring vertices' feature vectors. The neighborhood transfer function in graph ConvNets maps input vectors to neighboring vertices' feature vectors. Graph ConvNets use a special vertex as the center around which the neighborhood is built. Different instantiations of graph ConvNets have been proposed for tasks like traffic control and communication between agents. The proposed Graph LSTM extends the Tree-LSTM to arbitrary graphs and multiple layers, using an iterative process to solve equations. It leverages the vanilla graph ConvNet architecture and edge gating mechanism for graph learning tasks. The most generic formulation of a graph ConvNet with edge gating property is defined in Eq. (9). A multi-layer gated graph ConvNet using residual networks is also formulated, incorporating the identity operator between convolutional layers. The subgraph matching problem is addressed, aiming to find vertices of a given subgraph in larger graphs with variable sizes. This task is essential for graph neural networks, with subgraphs and larger graphs generated using the stochastic block model. The stochastic block model (SBM) is used to assign communities to nodes in a random graph. Experiments involve generating a subgraph with 20 nodes using SBM with q = 0.5. Larger graphs consist of 10 communities with sizes between 15 and 25, each with p = 0.5. The noise level q is 0.1, and the signal on graphs is randomly generated. Neural networks take variable-sized graphs as inputs and output vertex classification vectors using fully connected layers. The output of neural networks for variable-sized graphs is simple fully connected layers from hidden states. The learning schedule includes a maximum of 5,000 iterations with a decreasing learning rate based on loss. The loss is cross-entropy with 2 classes weighted by sizes, and accuracy is measured using a confusion matrix. Algorithms are optimized with a budget of parameters and number of layers fixed. The neural networks for variable-sized graphs use fully connected layers from hidden states. The budget of parameters is fixed at B = 100K and number of layers at L = 6. Optimizers and learning rates are manually selected to minimize loss. Different architectures perform well with specific optimizers and learning rates. The first experiment focuses on shallow graph neural networks with a single layer L = 1, varying noise levels. Hyper-parameters are selected accordingly. Neurons H are automatically computed for each architecture to meet the budget. Performance varies with different architectures, with graph ConvNets outperforming BID30 and BID25. Performance decreases with increased noise. Multiple layers are crucial compared to shallow networks, with models benefiting from more layers. RNN-based architectures see a decrease in performance with a large number of layers, while ConvNet architectures benefit from larger values of L. The proposed graph ConvNet performs slightly better than BID30 and BID25, and all ConvNet models are faster. In the third experiment, algorithms are evaluated for different parameter budgets. The proposed graph ConvNet performs best for a large budget and is faster than RNNs. The influence of hyper-parameter T on BID22 and graph LSTM is also examined. Computational time increases with larger T values. The section also considers the semi-supervised clustering problem. In this section, the semi-supervised clustering problem is addressed by finding 10 communities on a graph with one label per community. The task is more discriminative compared to previous clustering tasks, with 10 clusters to find. SBM is used to generate graphs with variable community sizes, and different algorithms such as BID22, BID30, BID25, and gated ConvNets are effective for this task. Graph LSTM and Li et al. (2016) use different optimization methods and T values. The experiments in this section compare ConvNet architectures with RNNs, showing that ConvNets perform better with increasing layers. Graph ConvNets and BID25 outperform other architectures for a fixed number of layers. ConvNets also converge faster than RNNs, especially for semi-supervised tasks. Additionally, learning-based approaches are compared to non-learning variational methods, with variational Dirichlet problem solving showing promising results. This work explores the choice of graph neural network architectures for solving learning tasks with graphs of variable length. The experiments report an average accuracy of 45.37% using learning techniques with one label per class. The best learning model achieves 82% accuracy, outperforming variational techniques that do not use ground truth data. The test complexity of learning techniques is O(E), where E is the number of edges in the graph, providing an advantage over variational Dirichlet models with O(E 3/2 ) complexity. In controlled experiments, graph ConvNets showed increasing accuracy with deeper networks, unlike graph RNNs. Gated graph ConvNets and residuality were explored for improved performance. Residuality allows stacking more layers, boosting accuracy by 10% with over 6 layers. Future work will focus on domain-specific problems in chemistry, physics, and neuroscience."
}