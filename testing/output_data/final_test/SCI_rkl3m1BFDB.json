{
    "title": "rkl3m1BFDB",
    "content": "Saliency maps are often used to explain deep reinforcement learning agents' behavior, but they can be subjective and unfalsifiable. An empirical approach using counterfactual reasoning shows that saliency map explanations are not always supported by experiments. They are best seen as exploratory rather than explanatory tools, producing heatmap-like visualizations to highlight important regions in visual input for deep network image classification. Saliency methods in computer vision and reinforcement learning generate maps to explain how agents choose actions in RL applications. RL systems present unique challenges due to their temporal and interactive nature, where deep models select sequential actions with long-term effects. Saliency maps in RL are used to assess internal representations and behavior over multiple frames in the environment. In this work, a methodology grounded in counterfactual reasoning is developed to evaluate the explanations generated using saliency maps in deep RL. The study aims to assess the applicability of saliency maps for explaining the behavior of RL agents, focusing on how they choose actions over multiple frames in the environment. The study introduces a new method to assess saliency maps in deep RL agents by experimentally evaluating the correspondence between pixel-level and semantic-level inferences. Saliency maps from a Breakout game frame show asymmetry in importance regions for action selection and reward estimation. The Breakout game requires using a paddle to hit bricks on the screen. Saliency maps show the importance of pixels for action selection. The study evaluates the correspondence between pixel-level and semantic-level inferences in deep RL agents playing Breakout. The study explores how technical details of reinforcement learning interact with saliency maps to understand their utility and pitfalls. Reinforcement learning enables agents to learn effective actions in an environment through repeated interactions and rewards. Deep reinforcement learning uses neural networks to represent policies for maximizing rewards in high-dimensional environments. Deep RL systems use neural networks to represent policies for interacting with high-dimensional environments like Atari games. Saliency maps are generated by interventions at the pixel level to understand the impact on action selection. Changing the conditional probability distribution of \"Pixels\" can be done by altering the pixel's color, adding a gray mask, blurring a small region, or masking objects with the background color. These interventions simulate the absence of pixels on the network's output but may produce images inconsistent with the generative process. Human observers combine saliency maps, agent behavior, and semantic concepts to explain agent behavior. Counterfactual reasoning is used to explain decision boundaries of deep models. Humans use counterfactuals to reason about outcomes and enabling conditions. Saliency maps offer pixel-level counterfactuals, but interventions at the pixel level may not be enough to explain agent behavior according to semantic concepts. Identifying the functional relationship between semantic concepts and pixels can be challenging. Interventions at the pixel level may not fully explain agent behavior based on semantic concepts. Three classes of interventions are distortion, semantics-preserving, and fat-hand, each affecting pixels, semantic concepts, and game state differently. Saliency maps primarily result in distortion interventions, with some methods also producing object-based interventions. Saliency maps primarily result in distortion interventions, with some methods potentially producing semantics-preserving or fat-hand interventions as well. Counterfactual evaluations using semantics-preserving interventions may be more suitable for testing hypotheses of behavior accurately. A survey of recent conference papers in deep RL focused on those using saliency maps to explain or make claims about agent behavior, citing work on Jacobian Saliency. Recent research has explored different methods for generating saliency maps in deep RL, including perturbation, object detection, and attention-based techniques. These maps have been used in 46 claims from 11 papers to explain agent behavior. In Appendix C, a set of claims is provided, categorized into three types of saliency map usage. The claims interpret salient areas as indicators of agent focus and propose hypotheses about the learned policy by analyzing saliency patterns and agent behavior. For example, one claim notes that a Breakout agent focuses on the paddle. Another claim suggests that the displayed score is a key factor in agent focus. The survey highlights the limited empirical verification of explanations generated from saliency maps in machine learning. Only 7% of claims attempt to validate the interpretations, with some trying to manipulate input semantics to assess the agent's response. Subjectivity and speculation in explanations have been critiqued in the field. The limited empirical verification of explanations from saliency maps in machine learning is concerning. Saliency methods lack the ability to formalize abstract concepts like \"aiming\" in Breakout, leading to subjective conclusions. Unfalsifiable interpretations of saliency map patterns, such as mistaking a diver for an enemy, raise questions about the validity of these explanations. The curr_chunk discusses how researchers use saliency maps to infer representations and processes behind an agent's behavior in complex processes like video games. Researchers' expectations bias the existence and nature of these mappings, but the validity of such inferences has not been empirically evaluated. The curr_chunk discusses generating falsifiable hypotheses from saliency maps and an intervention-based approach to verify them. Saliency maps may not directly relate to semantic concepts but can still be useful for exploring hypotheses about agent behavior. Claims informed by saliency maps involve semantic concepts, learned representations, and behavior. Semantic concepts can be visually identified from the pixel output, and interventions on game state can help assess the relationship between saliency and learned semantic representations. The curr_chunk discusses creating scientific claims from saliency maps by linking salient concepts to learned representations and agent behavior. Using a standard pattern, researchers can infer that if concept X is salient, the agent has learned representation R, resulting in behavior B. An example is given with Breakout, where identifying bricks leads to the agent aiming at tunnels. This format allows for falsifiable hypotheses to be constructed. The curr_chunk discusses evaluating saliency maps by intervening on the RL environment to assess the relationship between learned representations and pixel input. By directly manipulating the game state, researchers can analyze saliency under counterfactual conditions, providing a new approach compared to prior work focusing on pixel input manipulation. This method allows for a more in-depth understanding of the generative process of the pixel image. The curr_chunk discusses using TOYBOX to generate interventional data under counterfactual conditions by manipulating the game state to assess saliency on semantic concepts. Saliency on concept set X is evaluated by intervening on the state to determine patterns of saliency. The curr_chunk discusses conducting case studies on Atari games Breakout and Amidar to evaluate the relationship between semantic concepts and saliency maps. The studies use observed saliency maps to generate hypotheses and analyze the agent's learned representation. The deterministic nature of the games allows for stable interpretation of the network's action selection. In a case study on Atari games Breakout and Amidar, saliency maps are used to evaluate the relationship between semantic concepts and the network's action selection. The agent's learned representation is analyzed based on observed saliency maps, with a focus on methods of explanation rather than the explanations themselves. In one case study, the behavior of the agent in Breakout is evaluated, showing how the agent identifies a partially complete tunnel and maneuvers the paddle accordingly. By translating the brick configurations horizontally, the salience of the tunnel is observed to change, with different levels of prominence under left and right translations. In Amidar, the agent's learned representation is analyzed using saliency maps. The yellow sprite indicating the agent's location is consistently salient, while the displayed score also shows varying levels of saliency throughout the game. This leads to multiple hypotheses about the agent's learned representation. The agent's learned representation in Amidar is analyzed using saliency maps. Hypotheses suggest the agent associates increasing score with higher reward and has created a lookup table for its actions. To evaluate, interventions on score were designed, showing the impact on reward and saliency intensity. Interventions on displayed score affect agent behavior differently based on reward, leading to varied performance outcomes. Despite similar saliency maps, performance degradation varies among interventions, indicating that agent behavior is not solely determined by salience intensity patterns. The salience intensity patterns are similar for interventions on displayed score, resulting in differing levels of degraded performance but producing similar saliency maps. Weak correlations exist between reward differences and saliency differences under intervention, with insignificant p-values for most interventions. The high correlation between processes does not imply causation, and interventions can help identify common causes. In Amidar, enemies close to the player tend to have higher saliency, leading to successful avoidance of enemy collision. A hypothesis is generated based on this observation. Data is collected on player-enemy distance and enemy saliency to test the correlation without direct intervention on the game state. The study analyzed the correlation between enemy distance and saliency in the game. Results showed no significant correlation between the two, indicating no causal dependence. Intervening on enemy positions did not show an increasing trend in saliency for closer enemies. Additionally, no correlation was found in the enemy distance experiments for saliency. Spurious correlations can occur between player-enemy distance and saliency, and human observers are susceptible to identifying them. Thinking counterfactually about explanations from saliency maps helps evaluate them empirically, highlighting the challenges in drawing conclusions from saliency maps. The evaluation highlights the limitations of using saliency maps as explanatory tools and emphasizes the need for combining them with other methods for reliable inference. The framework for generating falsifiable hypotheses and distinguishing between explanation components can aid in experimental evaluation. The proposed methodology aims to address challenges in drawing conclusions from saliency maps. The methodology presented in this work can be extended to other vision-based domains in deep RL. Intervention-based experimentation is proposed as a primary tool to evaluate hypotheses from saliency maps. Alternative methods, such as evaluating statistical dependence, can help identify false hypotheses earlier. Limited evaluation may be possible in non-intervenable environments, but may be more tedious to implement. The interventions in Case Study 1 can be replicated in an observation-only setting by manipulating pixel input. The proposed methodology for evaluating explanations from deep RL agents is not model dependent and can be applied to recurrent deep RL agents. However, it may not work for repeated interventions on recurrent agents due to their memorization capacity. Prior work has suggested alternatives to saliency maps for explaining deep RL agents, such as Toybox for behavioral experiments. Saliency maps have been evaluated and critiqued in the deep network literature by various researchers. Saliency maps have been evaluated and critiqued in the deep network literature. Kindermans et al. (2019) and Adebayo et al. (2018) demonstrate their utility by adding random variance in input. Seo et al. (2018) provide a theoretical justification and hypothesize a correlation with model interpretation. Samek et al. (2017) and Hooker et al. (2018) evaluate existing methods for image classification. Jain & Wallace (2019) and Brunner et al. (2019) critique network attention for explanation. A survey is conducted on saliency map uses, proposing a methodology for evaluation and examining the correspondence between pixel-level and semantic concept-level inferences. Results show saliency maps may not accurately reflect learned representations. Our results suggest caution in relying on saliency maps to indicate causal relationships between semantic concepts and agent behavior. It is recommended to use saliency maps as an exploratory tool rather than an explanatory one. Example saliency maps for Amidar using perturbation, object, and Jacobian methods are shown. The A2C model is used to train RL agents on Breakout and Amidar, with each agent trained for 40 million iterations. The survey of recent literature assessed the use of saliency maps in interpreting agent behavior in deep RL. 46 claims from 11 papers were selected, excluding model-specific mapping methods. The papers appeared in various conferences and platforms. The study found that saliency maps should be used as an exploratory tool rather than an explanatory one. The agent enters a 'tunneling mode' during gameplay, focusing on specific pixels for estimation. There are differences in policy saliency between models, with one model paying limited attention to certain features. Further investigation is recommended to train an agent without certain input pixels. When analyzing visualizations on Seaquest, it was observed that the agent did not understand the importance of resurfacing when the oxygen is low, as indicated by low oxygen states for all actions. This suggests a lack of comprehension of the need to ascend to the surface to avoid suffocation. The agent fails to understand the importance of resurfacing when oxygen is low, leading to suffocation as the major cause of death. The model learns to focus on task-relevant elements in ATARI games, such as the player, enemies, power-ups, and score. In games like Ms Pacman and Alien, the model scans for paths without enemies or ghosts, avoiding them when detected. Additionally, the agent learns to place trip-wires strategically to trigger specific actions when game objects cross them. The system uses trip-wires strategically in games like Space Invaders and Breakout to trigger specific actions when game objects cross them. It employs different modes to make decisions, including content-specific, mixed, and location-based queries. The attention agent in the game focuses on specific areas along the player's trajectory, while the baseline agent is more focused on the immediate area in front of the player and the score. The attention agent also considers possible future paths and focuses on the upper part of the screen after destroying bricks. This attention persists even after the ball moves elsewhere, indicating a tunneling behavior where the agent strikes where it has already hit. The causal graphical model can be extended to different domains. The causal graphical model can be extended to different domains in RL where the input is an image. Game state includes underlying state variables for Breakout. Results show weak correlation between differences in reward and saliency for interventions. The study evaluated the relationship between player-enemy distance and saliency in Amidar using perturbation and Jacobian saliency maps. Jacobian saliency performed poorly in the intervention-based experiment, indicating no impact of distance on saliency. Regression analysis showed no correlation between observational and interventional distributions. Enemy 1 was found to be more salient throughout the game. The results showed a weak effect for both observational and interventional data. The study found a weak causal dependence of saliency on distance-to-player, with small effect strength and correlation coefficients."
}