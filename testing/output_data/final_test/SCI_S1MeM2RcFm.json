{
    "title": "S1MeM2RcFm",
    "content": "Deep Neural Networks (DNNs) are used in cloud servers and autonomous agents for their high performance. Models can be deployed in white-box or black-box settings, with a concern for protecting against Intellectual Property (IP) infringement. BlackMarks is a multi-bit watermarking framework for black-box scenarios, using the owner's binary signature to embed watermarks in the model. BlackMarks is a multi-bit watermarking framework for black-box scenarios, embedding watermarks in DNNs using the owner's binary signature. The watermark is encoded in the model's output activations through fine-tuning with a WM-specific regularized loss. BlackMarks preserves DNN functionality with minimal WM embedding overhead. Developing a high-performance DNN is costly due to the need for massive data processing and computing resources. Digital watermarking is used to protect DNNs as intellectual property. In the multimedia domain, DNN watermarking techniques are still in early stages, aiming to prove model ownership while maintaining functionality and resisting attacks. Techniques are categorized as 'white-box' (requiring model internals) or 'black-box' (using output predictions). White-box methods have higher capacity but limited applicability. The curr_chunk discusses the development of BlackMarks, an end-to-end black-box watermarking framework that enables multi-bit watermark embedding in Machine Learning models. This approach combines the advantages of white-box and black-box watermarking methods, offering higher capacity and requiring only model predictions for implementation. BlackMarks is an end-to-end black-box watermarking framework that enables multi-bit watermark embedding in Machine Learning models. It offers higher capacity and only requires model predictions for implementation. The framework improves model robustness against adversarial attacks and provides new perspectives for model designers in the deep learning domain. Digital watermarking techniques involve two phases: WM embedding and WM extraction. A constraint-based watermarking system uses the original problem as cover constraints to hide the owner's WM signature. The IP designer creates a stego-key and additional constraints to embed the WM, resulting in a stego-solution that satisfies both original and WM-specific constraints. Effective watermarking methods must meet criteria such as imperceptibility, robustness, verifiability, capacity, and low overhead. DNN watermarking for IP protection is gaining interest among researchers and practitioners, with BID31 taking the first step in this direction. BID31 and BID27 introduce DNN watermarking techniques by embedding watermarks in intermediate layers and activation maps, respectively. Zero-bit watermarking methods are proposed for black-box scenarios, with BID20 using adversarial samples to embed watermarks in the decision boundary. Detection of watermarks is done through null hypothesis testing based on the model's response to query images. BlackMarks propose using incorrectly classified images as trigger images to embed watermarks in a model. Three watermark generation algorithms are suggested, and the watermark is embedded by training the model with the WM set. Detection is done by querying the model with the WM set and thresholding the accuracy. The framework of BlackMarks is shown in FIG1. BlackMarks framework involves two main phases: WM embedding and WM extraction. Verifiability and integrity are crucial factors in designing a practical DNN watermarking methodology. Verifiability ensures accurate extraction of the embedded signature, while integrity minimizes false alarms in IP infringement detection. BlackMarks meets all requirements and ensures that the accuracy of the neural network is not compromised during watermark embedding. The watermark embedding in the DNN network should not degrade its performance. Watermark extraction should have minimal false negatives and be detectable with the right keys. The embedded watermark should withstand model modifications and not falsely claim authorship of unmarked models. The methodology should be able to embed a large amount of information with low communication and computational overhead. The watermark should be secure against brute-force attacks and leave no trace in the DNN. Validation should include testing against different types of attacks like model fine-tuning. The attack methods on DNN watermarking include re-training the model, parameter pruning, and watermark overwriting. BlackMarks exploits the non-convex optimization problems in DL models to embed the watermark signature. The workflow of watermark embedding and extraction is detailed, with a focus on image classification. BlackMarks introduces a method for watermark embedding in deep neural networks, specifically focusing on image classification. The approach involves encoding owner-specific watermark information in the output activations of the model. This one-time post-processing step is performed locally by the owner before model distribution. The method is designed to utilize the unused space within the high dimensional DNN for watermark embedding. BlackMarks designs an encoding scheme to map class predictions to binary bits by clustering output activations. The key generation module uses this scheme, owner's private signature, and original training data to generate WM key image-label pairs through targeted adversarial attacks. BlackMarks deploys targeted adversarial attacks to craft WM key images and labels for model authentication. The source and target classes for the WM image are determined based on the encoding scheme. The framework is compatible with various targeted adversarial attack methods for key generation, aiming to design specific WM key images as queries for model authentication. BlackMarks uses targeted adversarial attacks to create WM key images for model authentication. The WM signature and key generation parameters are kept secret. Adversarial samples may lead to false positives in WM detection, so the key size is set larger to filter out transferable keys near decision boundaries. Model fine-tuning is done to seamlessly encode WM information. BlackMarks incorporates an additive WM-specific embedding loss during DNN fine-tuning to encode WM information seamlessly. The total regularized loss includes the embedding strength \u03bb to control the contribution of the additive loss. Hamming Distance is used as the loss function to measure the difference between the extracted signature and the true signature. This retraining procedure resembles 'adversarial training' without the additional regularization loss. Existing zero-bit black-box watermarking papers leverage 'adversarial training' to ensure high classification accuracy on the WM trigger set. BlackMarks introduces an additive embedding loss for multi-bit watermarking, focusing on the difference between decoded signatures and original ones. After fine-tuning the model, indices of correctly classified initial WM keys are identified and high transferability WM key images are removed. In the process of watermark embedding, a subset of candidate WM keys is selected based on the owner's key size. The signature is extracted from the remote DNN by querying the model with WM key images and decoding the corresponding predictions to obtain the recovered signature. The computation and communication overhead of watermark extraction are analyzed in this section. The owner can prove authorship if the Bit Error Rate (BER) is zero, computed by comparing the true and extracted signatures. The computation cost involves decoding prediction responses and comparing signatures, while the communication overhead is equal to the key length. The communication overhead for watermark extraction is determined by the key length. BlackMarks' performance is evaluated on various datasets and neural network architectures, proving its effectiveness. Fidelity is maintained as the accuracy of the target neural network is not significantly degraded after watermark embedding. BlackMarks ensures fidelity by optimizing for classification accuracy and WM-specific loss during embedding. It acts as a regularizer to prevent overfitting and enables robust watermark extraction for ownership verification. The robustness of BlackMarks is evaluated against removal attacks such as parameter pruning and model fine-tuning. Parameter pruning, model fine-tuning, and watermark overwriting are attacks on the BlackMarks framework. Fine-tuning involves retraining the model to remove watermark information, while parameter pruning sparsifies the weights in the target DNN. Fine-tuning does not affect the watermark detection rate, as shown in the results. The BlackMarks framework is vulnerable to attacks such as parameter pruning and watermark overwriting. Pruning involves setting small weight values to zero, leading to a drop in accuracy that can't be compensated for by fine-tuning. Watermark overwriting allows an attacker to corrupt the original watermark by embedding a new one, posing a threat to the model owner's signature and key generation parameters. The attacker can generate new WM keys to fine-tune the model, aiming to disturb the WM extraction. BlackMarks shows verifiability and robustness against WM overwriting attacks, with zero BER in the overwritten DNN. The security of BlackMarks's WM key set relies on uncertainties in the key generation process. BlackMarks is secure against brute-force attacks as the key generation parameters and WM signature are kept secret. Integrity is maintained by ensuring that unmarked models do not falsely claim authorship. Evaluation of BlackMarks' integrity involves comparing decoded signatures with the owner's signature, with results summarized in TAB4. The BlackMarks watermarking method ensures security against brute-force attacks and maintains integrity by preventing unmarked models from falsely claiming authorship. Evaluation results in TAB4 show that BlackMarks yields low false positive rates when querying unmarked models with watermark keys. Additionally, BlackMarks has a higher capacity compared to existing zero-bit black-box watermarking methods, allowing for the embedding of more complex signatures. The amount of information carried by the owner's watermark signature can be measured by entropy. The BlackMarks watermarking method ensures security and integrity, with high capacity for complex signatures. Entropy measures the information in the owner's watermark signature. The framework allows for embedding multi-bit watermarks in a black-box setting, with low false positive rates. The runtime overhead for watermark embedding is analyzed, with the WM inserted through fine-tuning of the target DNN. The computation overhead for embedding a watermark using BlackMarks is determined by computing the additive loss term during DNN training. There is no communication overhead for watermark embedding as it is done locally by the model owner. The normalized runtime ratio of fine-tuning the pre-trained model with the watermark-specific loss is measured. Results show that BlackMarks incurs a reasonable additional overhead for watermark embedding, even for large benchmarks, with as low as 2.054% overhead. The study examines the impact of watermark embedding on a model's robustness against adversarial attacks. Results show that the watermarked model outperforms the unmarked model in accuracy against different white-box attacks. This improvement is attributed to the regularization loss in watermark embedding, which enforces correct predictions on training data and watermark keys. BlackMarks improves model robustness against adversarial attacks, similar to adversarial training. Plans to extend to multi-user setting for fingerprinting. BID3 introduces collusion-resilient DNN fingerprinting in white-box setting. BlackMarks proposes black-box multi-bit watermarking for IP protection. Shows feasibility of black-box fingerprinting methods. Provides first evidence of embedding multi-bit information using model's predictions. Comprehensive evaluation of BlackMarks' performance on various benchmarks. BlackMarks embeds robust watermarks in DNN predictions with low overhead. It outperforms existing techniques and enables black-box fingerprinting. For ImageNet dataset, code-bit clusters are larger, reducing collision probability. JSMA is not used due to memory constraints. Model fine-tuning is required for watermark embedding. In experiments, the WM embedding process involves fine-tuning a pre-trained model with a regularized loss function for 15 epochs. The learning rate is reduced by a factor of 10, ensuring the WM key is encoded in output activations without compromising accuracy. Additional experimental results supporting BlackMarks' performance are provided in this section. Supplementary experimental results support BlackMarks' performance. Robustness against model fine-tuning attack is demonstrated in TAB6, showing zero BER even after fine-tuning. Evaluation of BlackMarks' integrity in TAB7 reveals non-zero BER, indicating respect for integrity requirement with different WM key sizes. The BlackMarks watermarking technique shows respect for integrity requirements with various key lengths. Additionally, the watermarking process enhances the model's robustness against adversarial attacks, leading to higher classification accuracy on adversarial samples compared to unmarked models."
}