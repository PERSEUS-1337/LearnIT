{
    "title": "Skeh-xBYDH",
    "content": "This work explores the theoretical understanding of neural networks, focusing on one hidden layer networks and the efficiency of standard SGD training in producing generalization guarantees for symmetric functions. Empirical verification shows that this efficiency is not present with randomly chosen initial conditions. The importance of symmetry in neural network design is emphasized, with the study highlighting the role symmetry plays in enhancing network performance. The study focuses on the role of symmetry in neural network initialization, showing its importance for successful performance. It examines the aspects of expressiveness, training, and generalization in neural networks with one hidden layer, emphasizing the significance of proper initialization. The study explores the role of symmetry in neural network initialization, emphasizing its importance for successful performance. It focuses on a class of symmetric functions that are invariant under permutations of input coordinates, such as the parity and majority functions. Minsky and Papert (1988) showed that the parity function cannot be represented using a network with limited connectivity. The study emphasizes the importance of symmetry in neural network initialization for successful performance, focusing on symmetric functions like parity and majority functions. It is shown that the parity function cannot be represented using a network with limited connectivity. In contrast, a fully connected network with one hidden layer and common activation functions like sign, sigmoid, or ReLU only requires O(n) neurons. The training process efficiently converges to zero empirical error with proper initialization, leading to small true error as well. The study highlights the importance of symmetry in neural network initialization for successful performance, focusing on symmetric functions like parity. It shows that a network cannot learn parity from a random initialization. The number of parameters in the network is \u2126(n^2), indicating potential overfitting with small sample sizes. The initialization phase plays a crucial role in achieving generalization guarantees, even with a small sample size. The study discusses the challenges of learning parity with neural networks, emphasizing the importance of symmetry in initialization. It mentions that a small sample size is not sufficient for learning parity, and highlights the theoretical explanations provided by Shamir (2016) and Song et al. (2017). The framework discussed does not accurately represent neural network training with SGD. It questions if one SGD update equals a single query and highlights differences in dataset training methods. The model assumes adversarial noise and the lower bound does not capture efficient training processes. Theorem 1 demonstrates efficient learning of parity with symmetry-based initialization. In essence, parity cannot be learned as part of P but can be learned as part of S. The proof for why the hardness proof for P cannot be applied to S lies in the statistical dimension difference between the two classes. The analysis shows that with a small step size h in SGD, the output neuron reaches a good state while the hidden layer remains stable. Experimenting with larger values of h shows that although training error is zero, true error increases. The hidden layer neurons create an embedding of the input space into R. The hidden layer defines an \"embedding\" of the input space into R, which changes over time with training examples. If the embedding has a good margin, training with standard SGD converges quickly. Previous works have studied the expressiveness and limitations of neural networks. Several works have studied the training phase of neural networks, including analyses of SGD for general neural network architecture and the study of neural tangent kernels (NTK). Du et al. (2018) showed that randomly initialized shallow ReLU networks can nullify the training error under certain conditions. Several works have studied the training phase of neural networks, including analyses of SGD for general neural network architecture and the study of neural tangent kernels (NTK). Du et al. (2018) showed that randomly initialized shallow ReLU networks can nullify the training error under certain conditions. Their analysis focuses on optimization over the first layer, with the output neuron weights fixed. Chizat and Bach (2018) provided another analysis, while Allen-Zhu et al. (2018b) demonstrated that over-parametrized neural networks can achieve zero training error. Zou et al. (2018) provided guarantees for zero training error with separated classes and positive margins. Convergence and generalization guarantees for neural networks were also explored in subsequent works. Margins are emphasized in learning, as seen in the works of Sokolic et al. and Bartlett et al. Sokolic et al. (2017), Bartlett et al. (2017), and Sun et al. (2015) provided generalization bounds for neural networks based on margin at the end of training. Elsayed et al. (2018), Romero and Alquezar (2002), and Liu et al. (2016) proposed different training algorithms to optimize the margin. Song et al. (2017) and Shamir (2016) showed the difficulty of neural networks learning parities using statistical queries (SQ). Abbe and Sandon (2018) demonstrated a similar phenomenon in a more realistic setting. In a more realistic setting, considering symmetries in neural networks can be crucial for success. Efficient representations for symmetric functions by networks with one hidden layer are described, focusing on sigmoid and ReLU activation functions. The building blocks of symmetric functions are indicators of |x| = i for i \u2208 {0, 1, . . . , n}. The symmetric functions can be efficiently represented by neural networks with one hidden layer using sigmoid and ReLU activation functions. The building blocks are indicators of |x| = i for i \u2208 {0, 1, . . . , n}. A network with one hidden layer of n + 3 ReLU neurons and one bias neuron can represent any function in S. The coefficients of the ReLU gates are 0, \u00b11, \u00b12 in this representation. The section describes a small network with one hidden layer that efficiently learns symmetric functions using a small number of examples via SGD. The architecture, initialization, and loss function are specified for convergence of SGD, requiring \"good\" initial conditions. The chosen initialization depends on the activation function, resembling Lemma 2 for ReLU. Understanding the function class in terms of \"representation\" helps in choosing the network architecture. The network has one hidden layer with weights denoted by w_ij, bias denoted by b_i, and matrices denoted by W and B. The network is initialized with weights and biases for the hidden layer and output neuron. The hinge loss function is used for SGD, with a focus on the margin of the hidden layer. The set V in R d is defined based on the weight matrix and bias vector between the input and hidden layers. The hidden layer neurons define an \"embedding\" of the input in Euclidean space. A partition that agrees with the function has a margin lower bound of \u2126(1/ \u221a n). The weights of the hidden layer, when fixed, allow Theorem 1 to hold for SGD with batch size 1. Theorem 1 holds for SGD with batch size 1, but this is not enough to prove it. Novikoff (1962) showed that the perceptron algorithm makes few mistakes for linearly separable data with large margin. Running SGD with hinge loss is similar to a modified perceptron algorithm. Novikoff's proof can be extended to any \u03b2 > 0 and batch size, leading to Theorem 2 for margin \u03b3 > 0 and step size h > 0. The modified perceptron algorithm achieves a margin of at least \u03b3\u03b2 h 2\u03b2 h+(Rh) 2 with a limited number of updates. When running SGD on the entire network, the layers interact, and update rules are defined for the weights. The weights of the last layer play a key role in the dynamical system defined by the update rules. The weights of the last layer in a neural network are updated similarly to the modified perceptron algorithm. If the network has a good representation of the input, the interaction between layers during training does not affect this representation. Lemma 4 states conditions for linear separability and margin in a neural network with ReLU activation. The text discusses the efficiency of SGD iterations in training neural networks. It highlights key results such as the maximum distance moved by each node, the norm of the network, and the number of updates needed for training to end. The goal is to identify a geometric structure that facilitates efficient learning. The proof involves running SGD on i.i.d. examples from an unknown distribution, with specific step size and loss function parameters. The network is expected to correctly classify sample points and output the correct values for unseen data points. The text discusses the efficiency of SGD iterations in training neural networks, focusing on the geometric structure that aids efficient learning. It mentions the initialization of the network embedding space, VC dimension, and the modified perceptron algorithm for separating embedded samples. The goal is to prove the efficiency of the training process with a limited number of updates. After K \u2264 20R 2 /\u03b3 2 = O(n 4 ) updates, (M * , b * ) separates the embedded sample V * S with a margin of at least 0.9\u03b3/3. Lemma 4 states that as long as less than K = O(n 4 ) updates were made, the elements in V moved at most O(1/n 2 ). At the end of training, the embedded sample V S is separated with a margin of at least 0.9\u03b3/3 with respect to the hyperplane defined by M * and B * . Each v * x for x \u2208 X moved at most O(1/n 2 ) < \u03b3/4, ensuring the network has zero empirical error. The proof shows that the network's functionality depends on at most n + 1 examples. The parameter \u03b2 = R 2 h in the proof helps bound the distance elements in V move during training. Experimental results were conducted using a network with one hidden layer, ReLU activation, and hinge loss with \u03b2 = n 3 h. SGD with mini-batch size one was used, showing similar behavior with larger batches, different activations, and loss functions. Training error graphs are included in the appendix. The appendix presents training error and true error graphs versus training epoch. Theoretical results and initialization performance are validated in Figure 2. Two settings were tested: training only the second layer and training both layers with a step size h = n \u22126. SGD continues after minimizing empirical error due to parameter \u03b2 > 0. Experimenting with step sizes in Figure 3 shows varying performance. The training error converges faster to zero with larger step sizes, but at the expense of true error. Setting \u03b2 = 0 does not affect performance, as theory requires \u03b2 > 0 while practice allows \u03b2 = 0. Learning parity is challenging with small sample sizes, where training error can be minimized but true error remains high. As sample size increases, nullifying training error becomes harder. Initialization helps minimize both training and true error quickly. Initialization plays a critical role in minimizing training and true error quickly, even when data is corrupted. Symmetries are important in designing neural networks, as demonstrated by the ability to learn any symmetric function with proper initialization. However, the parity function or random symmetric functions cannot be learned with random initialization. The works Shamir (2016) and Song et al. (2017) discussed the limitations of using the language of SQ to explain the inner mechanism of network training. It was proven that standard SGD training efficiently reduces true error in a special case, but the challenge remains in extending these results to general neural networks. Future research should focus on identifying geometric states of the network that ensure fast convergence and generalization. The proof follows from observations regarding the weight of elements in X and the maximal distance of their embeddings. The norm of weights W and B is assumed to grow at a maximal rate during optimization. The maximal norm R(t) of the embedded space X at time t is bounded, with the assumption that R X > 1. The spectral norm of a matrix is at most its 2-norm. Assuming worst-case scenario where R(t) grows monotonically at a maximal rate, using the modified perceptron algorithm with choice \u03b2 = R^2h. Sample size of 10n with input dimension n = 30."
}