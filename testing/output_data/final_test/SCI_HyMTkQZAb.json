{
    "title": "HyMTkQZAb",
    "content": "K-FAC is a 2nd-order optimization method that approximates the Fisher information matrix for neural network optimization. Originally for fully-connected networks, it has been extended to handle convolutional networks and now RNNs with a novel FIM approximation. Our method outperforms SGD with momentum and Adam on RNN training tasks by efficiently computing cross-covariances and inverses. Neural networks require effective training methods due to their widespread use. Second-order optimizers like RMSprop and Adam offer marginal improvements over well-tuned SGD, but are computationally expensive for large networks. Non-diagonal curvature matrix approximations have shown potential in optimization methods for neural networks. New methods like K-FAC, Natural Neural Nets, and Practical Riemannian Neural Networks have achieved state-of-the-art performance in training tasks. These methods are motivated by past issues with scalability and the success of centering and normalization techniques. The K-FAC method has been extended to handle convolutional networks and a new family of curvature matrix approximations for recurrent neural networks has been developed. The focus is on approximating the Fisher information matrix through various assumptions on the network's gradients. Our main technical contribution is an approximation using a linear Gaussian graphical model to describe the statistical relationship between gradient contributions in RNN training tasks. The loss function is defined as L(y, f(x, \u03b8)) = -log r(y|f(x, \u03b8)), where r is the density function of the predictive distribution R. The objective is to minimize the expected loss h(\u03b8) = EQ[L(y, f(x, \u03b8)] over the training distribution Q on x and y. The Fisher information matrix F is associated with the model's predictive distribution Py|x(\u03b8) and is defined as cov(D\u03b8, D\u03b8). The Fisher information matrix is defined using Py|x(\u03b8) and the natural gradient is F\u22121\u2207h, used in natural gradient descent. Natural gradient descent has advantages of parameterization invariance and Fisher efficient convergence, but with caveats highlighted by BID23. The natural gradient method is a 2nd-order method that minimizes a local quadratic approximation to the objective function. It uses the Fisher matrix as a substitute for the Hessian matrix, providing better pre-asymptotic convergence rates compared to stochastic 2nd-order methods like SGD with Polyak averaging. Theoretical advantages of 2nd-order methods are pronounced when gradient noise is mitigated with large mini-batches. Natural gradient methods justify computing the natural gradient as (F + \u03bbI) \u22121 \u2207h instead of F \u22121 \u2207h, providing update damping/regularization for neural networks with millions of parameters. The K-FAC method uses a block-diagonal approximation of the Fisher matrix for neural networks with millions of parameters. It approximates blocks as Kronecker products between smaller matrices to reduce computational cost. The Kronecker product of matrices B and C, denoted by B \u2297 C for matrices B \u2208 R m\u00d7n and C of arbitrary dimensions, is a block matrix defined by DISPLAYFORM0. The matrices A and G can be estimated using simple Monte Carlo methods and averaged over lots of data by taking an exponentially decaying average across mini-batches. This is the basic Kronecker factored approximation related to the Natural Neural Nets approach. The approximation in BID24 neglects higher-order cumulants of as and gs, assuming they are Gaussian distributed. Inversion and multiplication by F involve inverting factor matrices A and G and matrix-matrix multiplications. Operations are computationally feasible due to small factor matrix dimensions. Additional approximations like iterative inversion or Kronecker-factorization can be applied. Inverses computation can be spread across optimizer iterations at the cost of introducing some staleness into estimates. The Kronecker-factored approximation can be extended to convolutional layers by assuming uncorrelated gradient contributions from different spatial locations and spatially homogeneous statistics. This new approximation, called \"KFC\", is derived to handle multiple mappings and different network architectures like RNNs and CNNs. The main technical contribution of this paper is a family of Kronecker-based approximations of F for RNNs. These approximations involve various assumptions, including spatially uncorrelated derivatives and spatial homogeneity. The approximations also include a new approach that models the statistical structure between gradient contributions from different time-steps using a linear Gaussian graphical model. The gradient of the single-case loss with respect to W can be written as DISPLAYFORM1 where D t W = g t a t denotes the contribution to the gradient from time-step t. Let F T denote the conditional Fisher of DW for a particular value of T. We make the simplifying assumption that T is independent of the w t 's, leading to eqn. 3 as DISPLAYFORM1. Independence of T and the w t 's is a reasonable approximation assumption to make. Temporal homogeneity is a mild approximation assumption that assumes the statistical relationship between words at different time steps only depends on their time distance. This assumption allows for a simplified notation in equations, similar to the steady-state assumption in dynamical systems. The steady-state assumption in dynamical systems assumes that the Markov chain reaches equilibrium. If external inputs reach steady-state and are pair-wise independent, the assumption is accurate for most states. Extending the temporal homogeneity assumption to the a t 's and g t 's allows for well-defined notation. Assuming all training sequences have the same length, F will be the sum of 2T0 + 1 Kronecker products. Without additional structure, inverting such a sum is not efficient. Decomposition-based methods can invert sums of two Kronecker products, but there is no known efficient algorithm for three or more. Additional approximating assumptions are needed to proceed, such as assuming independent or uncorrelated contributions to the gradient. The assumption of independent or uncorrelated contributions to the gradient allows for simplification of the formula for efficient multiplication of F^-1 by a vector. This approximation is similar to the original K-FAC method for fully-connected layers, but additional assumptions are required to make the formula tractable. In this section, a less severe approximation is considered for computing a tractable F^-1 by assuming a linear Gaussian graphical model for the statistical relationship of the w t 's. The model has a one-dimensional chain structure corresponding to time. This approach differs from previous methods and requires additional mathematical machinery. The model has a one-dimensional chain structure corresponding to time. Variables evolve forward in time according to a square matrix equation. The decision of directed or undirected edges is irrelevant for the Fisher approximation. The model extends infinitely in both directions, with indices in the range (-\u221e, \u221e). The model structure requires \u03a8 to have spectral radius < 1 for well-defined correlations between gradient contributions at different time-steps. The Back-prop Through Time process involves external temporal quantities like inputs x and activations, acting as hidden variables with their own dynamics. The external quantities, such as the unknown process generating true x's, may not have Markovian dynamics, but the w t 's should be approximately Markovian if they encode relevant information. The linear-Gaussian assumption is necessary for tractable expectations. Define transformed versions of F T and \u03a8 as shown in the appendix. In the appendix, DISPLAYFORM1 and DISPLAYFORM2 are used to compute F \u22121, which can be recovered via a simple relation. However, simplifying the formula for efficient computation of F \u22121 when \u03a8 is a Kronecker product is challenging due to the appearance of \u03a8 and its transpose. Two simplifying assumptions, \"Option 1\" and \"Option 2,\" are explained in the next subsections. Option 1 involves DISPLAYFORM5 if V 1 is the cross-moment over time. In the next subsections, Option 1 involves DISPLAYFORM5 if V 1 is the cross-moment over time, leading to the symmetric property of \u03a8. The eigen-decomposition of \u03a8 can be efficiently evaluated when it is a Kronecker product, simplifying the computation process. The interpretation of \u03a8 as the transition matrix of an LGGM is discussed in the proof of Proposition 1. Linear dynamical systems with symmetric transition matrices are common in machine learning and related fields. These matrices allow for modeling exponential decay of signal components over time, but not rotations between components needed for oscillating signals. It is important for the observed matrix to be exactly symmetric for calculations to be accurate. If it is not, symmetrizing or approximating the matrix can be done to ensure accuracy. When dealing with linear dynamical systems with symmetric transition matrices, it is crucial for the observed matrix to be exactly symmetric for accurate calculations. One option is to approximate the matrix by rescaling it with a factor T to ensure proper scaling characteristics. This approximation yields a simple expression that can be efficiently evaluated, especially when dealing with Kronecker product matrices. However, this approximation may break down if some linear components have temporal autocorrelations close to 1 and T is relatively small. The approximation of rescaling the matrix with a factor T can lead to overcounting temporal correlation when some components have autocorrelations close to 1 and T is small. To compute the error, estimates of the Kronecker factors V0 and V1 are used to estimate \u03a8 as \u03a8 = V1V0^(-1). The factors are estimated using exponentially decayed averages over mini-batch estimates, computed by averaging over cases and summing across time-steps. The spectral radius of \u03a8 is less than or equal to 1, ensuring the Gaussian graphical model is well-defined. The spectral radius of \u03a8 being less than 1 ensures the Gaussian graphical model is well-defined. Efficient implementation assuming V 0 and V 1 are Kronecker-factored is discussed, with full pseudo-code provided in the appendix. The novel curvature matrix approximations for RNNs were evaluated within the \"distributed K-FAC\" framework BID2 on two different RNN training tasks. The 2nd-order statistics were accumulated through an exponential moving average during training, and factored Tikhonov damping was applied. Experiments were conducted on a single machine with 16 CPU cores and a Nvidia K40 GPU. The Nvidia K40 GPU is used for experiments, with additional computations done on CPUs asynchronously. Updates are computed using stale values to minimize computational overhead. A step-size selection technique from BID2 is adopted for larger learning rates. Hyperparameters include max learning rate, trust-region size, momentum, damping constants, decay-rate for second-order statistics, and hyperparameters of baseline methods. The second-order statistics and hyperparameters of baseline methods were tuned using grid search. A two-layer RNN with LSTM architecture was used for word-level language modeling on the PTB dataset. Gradients were computed using truncated back-propagation with a sequence length of 35 timesteps. The learning rate was adjusted using a decaying schedule. The optimizer was replaced with a modified distributed K-FAC optimizer using RNN Fisher approximations for experiments. The experiments involved using a K-FAC optimizer with RNN Fisher approximations on two different sizes of the same LSTM architecture. By treating the weight matrices separately, smaller Kronecker factors were obtained, making computations cheaper. However, the large size of the Fisher blocks for the input and output layers posed challenges due to the vocabulary size. The input vector's associated factor was found to be diagonal, simplifying computations. The experiments involved using a K-FAC optimizer with RNN Fisher approximations on two different sizes of the same LSTM architecture. The input vector's associated factor was found to be diagonal, simplifying computations. The large factor associated with the output was approximated as diagonal for efficiency. Parameter updates of the method required 80% more time than SGD updates, but made more progress. Adam outperformed SGD initially, but SGD had lower loss at the end of training. Layer-normalization technique BID3 sped up Adam but hurt SGD performance. The proposed method outperformed both Adam and SGD even with layer-normalization. The study compared optimization performance of different methods, finding that 2nd-order methods tended to overfit more than SGD. It is suggested that explicit regularization or model modifications could address this issue. The study compared optimization performance of different methods, finding that 2nd-order methods tended to overfit more than SGD. To further investigate the optimization performance, a small two-layer LSTM with 128 units was used on the Penn-TreeBank dataset. Results showed that K-FAC updates were more time-consuming but had a significant advantage over Adam in terms of wall-clock time. Additionally, the approach was applied to the Differentiable Neural Computer model for learning algorithmic programs. The DNC model is designed to solve structured algorithmic tasks by using an LSTM to control an external read-write memory. The model was trained on a simple repeated copy task, where it needs to recreate a series of two random sequences. The DNC model significantly outperforms the Adam baseline in a repeated copy task, showing improvement in update count but only modest improvement in wallclock time. The iterations were more time-consuming due to different trade-offs in gradient computation. With careful engineering to reduce communication costs, a larger model and dataset could lead to a bigger improvement in wall-clock time. The new family of approximations to the Fisher information matrix for RNNs allows for training with the K-FAC optimization method, reducing convergence time and wallclock time in distributed training setups. Jason Weston, Sumit Chopra, and Antoine Bordes provide supplementary computations and proofs for the new approximations. The transformed linear-Gaussian graphical model derived a simple formula for \u03a8, with variables represented as \u0175 t = V \u22121/2 0 w t. The 2nd-order moments of the transformed variables are given by V 0 = I, obeying V 1 = V 0. The spectral radius of \u03a8 remains less than 1, ensuring the well-defined nature of the model. The transformed model is isomorphic to the original one, with all relationships holding by replacing quantities with their transformed versions. Rational functions can be evaluated with matrix arguments by using matrix operations like multiplication and inversion. Sums and inverses of matrices are co-diagonalizable when the matrices themselves are. When matrices are co-diagonalizable, there is no ambiguity in mixing commutative and non-commutative algebra. The value of a function f(x) with a matrix argument B may be undefined due to division by zero or non-convergence. The expression for F can be rewritten using Proposition 3. The eigendecomposition of \u03a8 allows evaluation of \u03b6T(\u03a8) based on the spectral radius. Proposition 2 approximates F using matrix operations. Proposition 3 states that for x \u2208 C, x = 0, and T a non-negative integer, DISPLAYFORM0 holds. This can also be expressed using the geometric series formula 1\u2212x before computing the derivative, resulting in DISPLAYFORM2. After computing the derivative, we have DISPLAYFORM3 and DISPLAYFORM4, utilizing the geometric series formula. The spectral radius of DISPLAYFORM1 needs to be bounded, which can be achieved by showing \u03c1(A1A\u221210) \u2264 1 and \u03c1(G1G\u221210) \u2264 1 for A i and G i computed using the estimation scheme outlined in Section 3.5.4. In Section 3.5.4, the exponentially decayed average of mini-batch averages of estimators is discussed. The matrix is a positively-weighted linear combination of terms, leading to the inequality \u03c1(A1A\u221210) \u2264 1. This is proven using Lemma 1 for real, symmetric, positive semi-definite block matrices. The statement is equivalent to proving X 2 = \u03c3 max (X) \u2264 1, where \u03c3 max (X) is the largest singular value of X. By using Schur complements, it is shown that the block matrix is positive semidefinite. When the block matrix is positive definite, the inequalities become strict. The goal is to efficiently compute the matrix-vector product of an arbitrary vector with the inverse Fisher approximation F \u22121. To efficiently compute the matrix-vector product of an arbitrary vector with the inverse Fisher approximation F \u22121, we need to compute F \u22121 z, where z is given as a matrix Z. The matrix-vector product involves multiplying the vector by U, then by diag(\u03b3(\u03c8)), and finally by U. The eigendecompositions of A 0 and G 0 can be used to compute V, while \u03a8 can be written as a Kronecker product. The task is to compute the matrix-vector product where U diag(\u03c8)U is the eigendecomposition of \u03a8, and \u03b3(x) is defined accordingly. To efficiently compute F \u22121 z, we multiply the vector by U, then by diag(\u03b3(\u03c8)), and finally by U. The eigendecomposition of \u03a8 can be computed using its Kronecker product structure. This involves computing the eigendecompositions of each factor and multiplying z by U = U A \u2297 U G using a specific identity. In summary, F \u22121 z can be computed in matrix form. To efficiently compute (I \u2212\u03a8)z and (I \u2212\u03a8 \u03a8) \u22121 z, we use matrix operations and eigendecompositions. The matrix form of (I \u2212\u03a8)z is Z \u2212\u03a8 G Z\u03a8 A, while (I \u2212\u03a8 \u03a8) \u22121 z involves multiplying by (E A \u2297 E G) and diag(vec(ll \u2212 m G m A)) \u22121. The matrix form involves element-wise division by ll \u2212 m G m A, followed by E A \u2297 E G. This is denoted as B C for element-wise division of matrix B by matrix C."
}