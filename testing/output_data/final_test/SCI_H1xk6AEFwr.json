{
    "title": "H1xk6AEFwr",
    "content": "Generative seq2seq dialogue systems are trained to predict the next word in dialogues, learning from large conversation datasets to generate various responses. However, this flexibility can lead to undesirable responses and nonsensical output. To address this, a classifier is trained to select from a list of predefined responses based on conversation context. This approach allows for experts to edit and improve responses without retraining the classifier or affecting old training data. Human evaluation of 775 doctor/patient conversations shows that a discriminative approach improves responses compared to a generative model. Task oriented dialogue systems, like those for restaurant reservations, are not suitable for primary care medical conversations due to the complexity of building a knowledge base. The generative models (GM) in dialogue systems do not require labeling or structured representations but lack mechanisms for ensuring high-quality responses. GPT2 has been shown to produce racist output with just four-word sequences. Checking for curse words in generated utterances is not sufficient to prevent subtle failure modes. Typos and inaccuracies in training data can be reproduced by the model during inference. Our discriminative approach aims to improve generative models by using a set of high-quality exemplar responses that are factual, sensible, and grammatical. Experts can edit these exemplars before or after training to ensure quality. Response classes can be updated or removed to redirect the model towards more productive content without retraining. The paper proposes a system for generating reasonable responses across multiple domains by creating non-overlapping response groups with weak supervision. It utilizes a pretrained similarity model to cluster similar responses and a human to merge the most frequent clusters into larger response classes. This approach is expected to be beneficial in task-oriented settings such as patient diagnostics and customer service interactions. Section 2 discusses related conversational agents and methods, while Section 3 details the approach of creating response classes to cover various conversational contexts. Section 4 explains the main results and experiments comparing response quality using different classification architectures. Wolf et al. (2019) utilized a generative transformer approach called \"TransferTransfo\" to win the 2018 NeurIPS PersonaChat competition. The model starts training with pretrained weights from GPT2 transformer and finetunes with PersonaChat data using next-utterance classification and language modeling losses. Our approach involves utilizing the hidden state of the final self-attention block of the transformer for backpropagation of losses into the original weights. Comparison is made with a modified version of this approach in Section 4. Previous studies have focused on conditioning generation on source URLs and controlling repetition, specificity, response relatedness, and question asking. Our approach differs by removing the generative decoding step, allowing direct control of responses and faster processing. Our approach involves utilizing the hidden state of the final self-attention block of the transformer for backpropagation of losses into the original weights. Comparison is made with a modified version of this approach in Section 4. Previous studies have focused on conditioning generation on source URLs and controlling repetition, specificity, response relatedness, and question asking. Our approach differs by removing the generative decoding step, allowing direct control of responses and faster processing. Ranking Models, including Dinan et al. (2018) and Zhou et al. (2018), operate on datasets of (context, candidate response, y) triples, where y is a binary label indicating whether the candidate response was the one observed in the conversational data. These models enjoy the same control guarantees as our discriminative approach, since they select responses from a fixed bank, but inference requires both a system to retrieve candidate responses and one forward pass for every candidate response. Our approach chooses between all candidate responses in one forward pass. The closest work to ours is Wan & Chen (2018)'s AirBNB customer service chatbot, which also uses a discriminative approach, but does not attempt to cover the whole response space and differs architecturally. Whereas our approach restricts the output space to 187 responses that attempt to cover the whole output. The authors restrict the output space to 187 responses, choosing from 71 investigative questions representing clusters. They improve class diversity by ensuring each issue type is represented by at least one cluster. They replace the hierarchical context encoder with a pretrained language model for better performance and faster training time. The authors aim to use the last t turns of conversational context to suggest a response for a doctor to send to a patient. They create response classes based on interchangeable doctor utterances and train a classifier to predict a response class given the context. The goal is to generate response classes with low overlap and good coverage for better classifier performance. The authors propose a five-stage procedure for suggesting responses to doctors based on conversational context. They preprocess responses, encode them as vectors using different models, and compute similarities within a semantic neighborhood to make subsequent steps computationally cheaper. The authors suggest a five-stage procedure for recommending responses to doctors based on conversational context. They preprocess responses, encode them as vectors using various models, and compute similarities within a semantic neighborhood to streamline subsequent computational steps. They utilize a supervised similarity model, BERT, to predict the semantic similarity of response pairs and employ Agglomerative Clustering on a sparse distance matrix to merge clusters with dissimilarity below a certain threshold. Optional step includes manually merging clusters into response classes. The dataset contains clusters with centroid text and occurrences of cluster constituents, which are sorted by frequency. The labeler decides if the centroid text belongs to an existing response class or creates a new one. Responses with similar impact on the user are merged into the same class. Skipping step 5 uses clusters as response classes. A discriminative response suggestion model is trained to associate conversational context with response classes. A response suggestion model is trained to associate conversational context with response classes. Context-response pairs are labeled if the true response is a member of a response class. The model is based on ULMFit approach with modifications for larger batch size, using a pretrained AWD-LSTM language model. Special tokens are inserted to encode information about speaker changes. For language model finetuning, 300,000 doctor/patient interactions were used, resulting in 1.8 million rounds of exchanges covering various medical domains. The most recent 100,000 interactions were utilized for response class generation, yielding 72,981 labeled pairs for classification training. The number of turns per conversation and turn length varied widely. Automated clustering in response generation yielded 40,000 clusters with high similarity but many overlaps. Manual labeling created 187 groups from the most frequent clusters, leaving 90% of responses unlabeled. The clustering procedure may not merge interchangeable clusters due to sentence encoders focusing on meaning rather than conversational impact. The manual merging step in response evaluation does not need to be fully completed. Medical doctors assessed response quality compared to true responses. Results are shown in Tables 2 and 5, with accuracy for comparing classifiers in Tables 3 and 4. Tables 3 and 4 compare classifiers on the same dataset for accuracy on unseen labeled data. A generative baseline using AWD-LSTM language model is employed before classification finetuning. Hierarchical ULMFit outperformed hierarchical QRNN, while flat ULMFit performed better than its hierarchical counterpart. The performance of Hierarchical ULMFit was decreased due to the large variance in turn lengths, requiring more padding and smaller batch sizes. A pretrained double-headed transformer showed similar accuracy to ULMFit on conversation data. The study compared the performance of different architectures on conversation data. It was found that using more than 6 turns of conversation history did not significantly improve classification accuracy. Additionally, opting out of suggesting a response when the model's predicted probability is low led to a decrease in bad suggested responses. The study analyzed the performance of different architectures on conversation data, finding that using more than 6 turns of history did not significantly improve accuracy. Opting out of suggesting a response when the model's predicted probability is low reduced bad suggested responses. Restricting evaluations to the most confident contexts decreased bad suggested responses from 11% to 1.8%. The amount of manual labeling needed varied based on the dataset used for training discriminative models. In a study on conversation data, using more than 6 turns of history did not significantly improve accuracy. Opting out of suggesting a response when the model's predicted probability is low reduced bad suggested responses. Evaluators found that asking for information already given was a common error for both generative and discriminative models. The discriminative model had no nonsensical responses, unlike the generative model. In this work, a classification model is proposed to generate useful responses in various contexts while maintaining control over the quality of responses. The model leverages pretraining techniques and restricts generations to a set of high-quality responses. It addresses common errors such as asking the same question twice and aims to improve response quality compared to a generative model. Future work on the proposed classification model includes addressing the creation of response classes and testing the control for flexibility tradeoff in other conversational domains."
}