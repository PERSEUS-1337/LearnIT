{
    "title": "r1ehEgHKwH",
    "content": "Graph-based dependency parsing involves two main steps: an encoder generates feature representations for parsing substructures of the input sentence, and a decoder constructs the parse tree with the highest total score. Recent advancements in neural encoding have significantly improved parsing accuracy, leading to a decreased use of high-order decoding techniques. It is believed that contextualized features from neural encoders can replace the need for high-order decoding. This study evaluates different combinations of neural and non-neural encoders with first- and second-order decoders to analyze their effectiveness with varying training data sizes. In this study, the effectiveness of different combinations of neural and non-neural encoders with first- and second-order decoders is analyzed for dependency parsing. It is found that a strong neural encoder with first-order decoding performs well with large training data, while a non-neural encoder with a second-order decoder is more effective with small training data. In this paper, the focus is on the impact of neural techniques on parsing accuracy. The use of neural encoders has led to a significant increase in accuracy, while high-order decoding techniques are now considered less necessary due to the comprehensive information captured by neural encoders. The paper evaluates different combinations of neural and non-neural encoders with first-and second-order decoders for parsing performance. Powerful neural encoders reduce the need for high-order decoding with sufficient training data. However, with smaller training data, the advantage of a neural encoder with a second-order decoder diminishes. For limited training data, a simple non-neural encoder with a high-order decoder is preferred for its robustness and performance. Graph-based dependency parsing involves finding a maximum-spanning tree in a directed graph. The process of parsing involves formulating the task as a search for a maximum-spanning tree in a directed graph. This search consists of computing scores for substructures in the graph and finding the tree with the highest score. The tokens and dependency arcs in an input sentence are represented as vertices and edges in the graph, which is factorized into substructures. These substructures are scored using a function, and the max-spanning tree is determined by combining these scores. The parsing process is carried out by an encoder and decoder, with the encoder implementing the scoring function. The neural encoder uses a neural network to score substructures in a sentence. The decoder, based on dependency arc factorization, can be first-order or non-projective using algorithms like Eisner or Chu-Liu-Edmonds. Time complexity is O(n^3) for Eisner and O(n^2) for Chu-Liu-Edmonds. The paper discusses second-order projective decoding in dependency parsing using a modified Eisner algorithm. It focuses on projective dependency parsing due to the NP-hard nature of non-projective decoding. Different learning methods for graph-based dependency parsers are also explored, including margin-based loss and gradient-based methods like Adam Kingma & Ba. The paper explores second-order projective decoding in dependency parsing using a modified Eisner algorithm. It discusses different learning methods for graph-based dependency parsers, including margin-based loss and gradient-based methods like Adam Kingma & Ba. Two types of neural encoders are used in the experiment: a one-layer LSTM encoder and a two-layer LSTM encoder, with the latter having higher expressive power. The computation of substructure scores follows previous work by Kiperwasser & Goldberg (2016). The computation of substructure scores in dependency parsing involves concatenating word embeddings and POS tags for each position in the input sentence. This information is passed through Bi-LSTM layers to obtain contextual feature vectors. A multi-layer perceptron (MLP) is used to compute scores for each position in a substructure, which are then concatenated to form the representation for the entire substructure. Another MLP is used to calculate the substructure score based on its representation. The representation of dependency arcs in second-order cases involves positions i, j, k. A non-neural encoder generates a sparse feature vector using a manual template. The first-order decoder is based on the Eisner algorithm, which defines complete and incomplete spans for dynamic programming. The second-order decoder in dependency parsing involves combining spans in a specific way to form a complete span that covers the entire sentence. This process is based on the Eisner algorithm and utilizes both complete and incomplete spans. The decoder traces back the formation of the last complete span to recover the dependency parse tree. The decoding procedure in dependency parsing involves extra dependency arcs, increasing time complexity to O(n^4). Different encoders and decoders are evaluated on treebanks across languages. PTB includes the English WSJ corpus annotated by SD, while other treebanks are annotated in UD v2.0. Pruning sentences longer than 40 in training for efficiency due to slow decoding of long sentences. The training data for dependency parsing involves pruning sentences longer than 40 for efficiency. Different encoder and decoder configurations are implemented in PyTorch, following a similar approach to Kiperwasser & Goldberg (2016) with minor modifications. Parsers are trained for 40 epochs with a batch size of 10 sentences, using specific dimensions for word embedding, POS tag embedding, and LSTM hidden units. The Adam method is adopted for parameter optimization. In this study, the Adam method is used for parameter optimization in PyTorch for dependency parsing. Evaluation focuses on Unlabeled Attachment Score (UAS) as it is independent of label prediction. The complexity of models affects the amount of data needed for training. The study evaluates different encoder-decoder combinations on large training datasets from PTB and UD-Czech-CAC treebanks. Results show that increasing encoder complexity generally improves performance, while higher order decoders outperform first-order decoders. The advantage of second order decoding is more pronounced for nonneural encoders compared to neural encoders. The impact of powerful LSTM encoders on the usefulness of high-order decoders is further investigated. The study evaluates different encoder-decoder combinations on large training datasets from PTB and UD-Czech-CAC treebanks. Results show that increasing encoder complexity generally improves performance, while higher order decoders outperform first-order decoders. In Figure 2, parsing accuracy of each encoder-decoder combination on gold dependencies of different lengths is shown. Neural encoders generally outperform nonneural ones except for short dependency lengths. A significant gap in parsing accuracy between LSTM-2+FO and LSTM-2+SO is observed for longer dependency lengths, attributed to the inferiority of the first-order decoder in finding very long-term dependencies. The study evaluates different encoder-decoder combinations on medium and small training datasets from various treebanks. Results show that increasing encoder complexity generally improves performance, while higher order decoders outperform first-order decoders. Training data sizes impact the results, with most languages lacking annotated corpora the size of PTB. Evaluation is done on selected medium-sized treebanks and experiments are conducted with smaller datasets as well. In the evaluation of encoder-decoder combinations on medium and small training datasets from various treebanks, increasing encoder complexity generally improves performance. Higher order decoders tend to outperform first-order decoders. The size of the training data impacts the results, with most languages lacking annotated corpora the size of PTB. Notably, in some cases, a non-neural encoder can outperform neural encoders when the decoder is set to second order, possibly due to the fine-grained POS tag set in PTB providing informative features. Additionally, LSTM-2+FO replaces LSTM-2+SO as the best performing combination in some treebanks. In low-resource settings, NLP systems are tested on small training datasets. Various treebanks are sampled to reduce training data size, with subsets randomly selected. Experimental results are presented in Table 4, showing the impact of decreased training data on encoder-decoder combinations. LSTM-2+FO outperforms LSTM-2+SO in some treebanks, possibly due to the complexity of the models and limited training data availability. The experimental results in Table 4 show significant differences with small training data. Non-neural encoders combined with second order decoders perform better in most treebanks compared to neural encoders. The non-neural encoders demonstrate better data-efficiency and stability, as shown in Figures 3 and 4. The LSTM encoders are more data-hungry than the non-neural encoder, performing better with large data but worse with small data. The two-layer LSTM encoder outperforms the one-layer LSTM encoder. The second-order decoder is consistently better than the first-order decoder when combined with the non-neural encoder. The second-order decoder performs better than the first-order decoder when combined with non-neural encoders, but the data-efficiency may depend on the encoder used. Further experimentation is needed to understand this phenomenon. With large data, neural encoders and high-order decoders are recommended for best parsing accuracy, while small data benefits from traditional non-neural encoders and high-order decoders. To improve parsing accuracy, one should consider using a traditional non-neural encoder with a high-order decoder. Future work may involve exploring second-order sibling decoding, third-order decoding, neural encoders with biaffine and triaffine score computation, and transition-based dependency parsers."
}