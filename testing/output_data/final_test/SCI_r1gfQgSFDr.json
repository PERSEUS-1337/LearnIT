{
    "title": "r1gfQgSFDr",
    "content": "Generative adversarial networks have advanced in image generation, but their use in audio has been limited. Autoregressive models like WaveNet are still dominant in generating audio signals like speech. Introducing GAN-TTS, a Generative Adversarial Network for Text-to-Speech. Our architecture includes a conditional feed-forward generator for speech audio and an ensemble of discriminators analyzing audio realism and utterance correspondence. GAN-TTS performance is evaluated using subjective human evaluation and quantitative metrics, showing high-fidelity speech generation comparable to state-of-the-art models. GAN-TTS is parallelizable and efficient, unlike autoregressive models. Text-to-Speech (TTS) task involves converting text into speech audio. The Text-to-Speech (TTS) task has seen progress with neural autoregressive models like WaveNet, SampleRNN, and WaveRNN. Recent research focuses on improving parallelism in TTS models by predicting multiple time steps in parallel using flow-based models. This approach allows for more efficient running on modern hardware. An alternative approach for parallel waveform generation using Generative Adversarial Networks (GANs) is explored in this paper. GANs are known for their success in generative modeling of images but have had limited success in audio generation tasks. The paper introduces GAN-TTS, a text-conditional high-fidelity speech synthesis system using GANs with a feed-forward generator and multiple discriminators. The paper introduces GAN-TTS, a text-conditional speech synthesis system using GANs with a feed-forward generator and multiple discriminators. It proposes quantitative metrics for speech generation based on FID and KID, replacing the Inception image recognition network with the DeepSpeech audio recognition network. The best-performing model achieves a MOS of 4.2, comparable to WaveNet MOS of 4.4, establishing GANs as a viable option for efficient TTS. Neural models for audio generation focus on likelihood-based approaches, using autoregressive models or invertible feed-forward neural networks. Invertible models can be trained by distilling autoregressive models for mode-seeking behavior in conditional generation settings. Many audio generation models focus on realistic speech signals corresponding to text, without modeling every possible variation in the data. Adversarial models exhibit similar behavior without distillation and invertibility requirements. These models operate in the waveform domain, directly modeling the amplitude of the waveform over time, unlike discriminative audio models that operate on time-frequency representations and discard phase information. When generating audio signals, the phase component is crucial for realism, but some models like Tacotron and MelNet use the Griffin-Lim algorithm to reconstruct missing phase information. Other models like Deep Voice 2 & 3 and Tacotron 2 first generate a spectral representation and then use an autoregressive model to convert it into a waveform. This helps correct any imperfections in the generated spectrograms. Generative Adversarial Networks (GANs) have achieved state-of-the-art results in image and video generation, as well as unsupervised feature learning. The discriminator network provides a gradient signal to the generator to differentiate between real and generated samples. Limited work has shown good performance of GANs in audio generation, with exceptions like WaveGAN and GANSynth. WaveGAN generated raw audio for spoken commands, while GANSynth produced state-of-the-art results for single note recordings from musical instruments. Neekhara et al. proposed an adversarial vocoder model for synthesizing magnitude spectrograms from mel-spectrograms. In non-visual domains, GANs have limited application at large scale. Various approaches have been proposed to enhance speech synthesis, such as using GANs to avoid oversmoothing of speech parameters and replacing probabilistic loss with adversarial versions. GANs have also been used for voice conversion directly on raw waveforms. Training GANs at scale for audio generation has shown potential for extensive improvements. Training GANs at scale has shown potential for extensive improvements in various domains. Different discriminators have been used for images, with some focusing on separate resolutions and others combining 3D and 2D discriminators. Adversarial feature learning involves combining outputs from multiple discriminators to differentiate between joint distributions. Discriminators operating on input windows have been used in adversarial texture synthesis and image translation. Text-to-speech models are trained on a dataset containing high-fidelity audio of human speech. The dataset for training text-to-speech models contains high-fidelity audio of human speech with linguistic features and pitch information. It includes 567 features and 44 hours of audio clips spoken in North American English. The generator network learns to convert linguistic features into speech. The generator network converts linguistic features and pitch into raw audio at 24kHz, using dilated convolutions to capture long-term dependencies. The architecture of generator G consists of seven blocks, each containing two residual blocks with increasing dilation factors in the convolutions. The GBlock architecture consists of increasing dilation factors and Conditional Batch Normalisation. Each GBlock contains two skip connections and gradually upsamples the temporal dimension of hidden representations. The final convolutional layer produces a single-channel audio waveform. An ensemble of Random Window discriminators is used instead of a single discriminator. The ensemble of Random Window Discriminators (RWDs) operates on randomly sub-sampled fragments of real or generated audio samples. It consists of multiple discriminators with different window sizes and conditioning options, leading to faster training compared to conventional discriminators. The number of discriminators only affects training computation requirements, as only the generator network is used during inference. The Random Window Discriminators (RWDs) ensemble uses random windows of varying sizes for faster training and reduced computational complexity. In the first layer, input waveforms are reshaped to a constant temporal dimension. Conditional discriminators assess if generated audio matches input conditioning by aligning random windows with the conditioning. The Random Window Discriminators (RWDs) ensemble uses random windows aligned with conditioning frequency to preserve correspondence between waveform and linguistic features. Conditional discriminators are constrained to conditioning signal frequency (200Hz), while unconditional discriminators sample at full 24kHz frequency, increasing training data. The ensemble discriminator combines 10 different Random Window Discriminators (RWDs) using waveform and linguistic features. The architecture includes fully-convolutional structures with average pooling and a deterministic discriminator for ablation study. Discriminators consist of blocks similar to the generator's blocks, with differences in batch normalization. The DBlocks in the conditional and unconditional RWDs differ in the placement of linguistic feature embedding. Downsample factors in the middle DBlocks match linguistic feature frequency. Conditional RWDs gradually downsample input waveform until matching conditioning, then use conditional DBlock. DBlocks' convolutions follow a pattern of 1, 2, 1, 2 dilation factors. The discriminator in the Multiple Random Window Discriminator architecture operates on a small window with a pattern of 1, 2, 1, 2 dilation factors. It combines outputs from 5 unconditional and 5 conditional discriminators for evaluation using Mean Opinion Scores and quantitative metrics. Human evaluators rated the naturalness of sentences on a Likert scale compared to WaveNet and Parallel WaveNet. Our model can generate audio samples of arbitrary length using a convolutional masking trick. Human evaluators scored sentences up to 15 seconds long using quantitative metrics like FDSD, cFDSD, KDSD, and cKDSD, inspired by evaluation metrics for GANs in image generation. The Fr\u00e9chet distance (FID) and Maximum Mean Discrepancy (MMD) are computed between reference and generated distributions using representations from a pre-trained Inception network. For speech, features are extracted from DeepSpeech2 and used to compute FID and MMD metrics. Kilgour et al. (2019) proposed a similar metric called Fr\u00e9chet Audio Distance using a network trained on the AudioSet dataset. In this study, a network trained for speech recognition is used for audio event classification. Two variants of metrics, conditional (cFDSD, cKDSD) and unconditional (FDSD, KDSD), are computed to compare real and generated samples. The metrics evaluate distances between conditional and unconditional distributions, with 10,000 samples drawn independently for estimation. The study compares real and generated samples using conditional and unconditional metrics, with 10,000 samples drawn independently for estimation. The use of Fr\u00e9chet and Kernel distances is discussed, highlighting the unbiased estimator of MMD for kernel-based distances. Experiments comparing GAN-TTS with WaveNet and architectural choices validation are also detailed. The study compares real and generated samples using conditional and unconditional metrics, with a focus on the use of multiple RWDs with different downsampling factors. Ablations of the best-performing model were conducted, confirming the importance of combining multiple RWDs. The results show that models using unconditional RWDs outperformed those that did not, with the deterministic full discriminator achieving the worst scores. Models using unconditional RWDs outperformed those that did not, with combinations of different window sizes proving beneficial. The downsampling model with a base window size of 240 performed the best among the 10-RWD models. There was a noticeable correlation between human evaluation scores (MOS) and the proposed metrics, indicating their suitability for evaluating neural audio synthesis models. The effectiveness of RWDs over the full discriminator may be due to the simplicity of the distributions they discriminate between and the variety of samples they can draw from these distributions. The largest window discriminators in the best model discriminate between distributions supported on R 3600, with 371 and 44,401 different windows that can be sub-sampled from a 2s clip. The Generator has a larger receptive field and fewer FLOPs compared to Parallel WaveNet. Discriminators in the ensemble compare windows of shorter sizes, making training faster than with FullD. The proposed GAN-TTS model for raw audio text-to-speech generation is trained with ensembles of Relative Wavelet Distances (RWDs), which results in faster training compared to FullD. The generator has 30 layers, while the discriminators have depths ranging from 11 to 17 layers. The model shows stable training with gradual improvement in sample quality and decreasing metric values, even after 1 million training steps. Unlike traditional text-to-speech models, GAN-TTS is adversarially trained and utilizes a feed-forward convolutional network for efficient audio generation. Our model utilizes an ensemble of unconditional and conditional Random Window Discriminators to assess the realism of generated speech and its correspondence with input text. We introduced quantitative metrics for speech generative models and demonstrated their effectiveness through experimental evaluation. These metrics will be shared with the machine learning community. Our results show the feasibility of text-to-speech conversion. The text discusses the architecture details of a text-to-speech generation model using GANs. The generator is a fully-convolutional network capable of generating samples of arbitrary length but requires zero-padding for fixed-size tensor processing. Masking is used to address issues with non-zero values propagating outside the border between input and padding. The text discusses the architecture of GAN-TTS's Generator, which includes 30 layers mostly consisting of dilated residual blocks. Conditional discriminators combine waveform and linguistic features after downsampling the temporal dimension. Masking is used to address issues with non-zero values propagating outside the border between input and padding. The text discusses downsampling linguistic features in the GAN-TTS architecture using prime divisors of 120/k. Evaluation metrics extract high-level features from raw audio using the pre-trained DeepSpeech2 model. The text discusses downsampling linguistic features in the GAN-TTS architecture using prime divisors of 120/k. Evaluation metrics extract high-level features from raw audio using the pre-trained DeepSpeech2 model, which includes preprocessing layers. The function DS computes 1600 features for each 20ms window with 10ms overlap, and then averages the features along the temporal dimension. Fr\u00e9chet distance and MMD can be computed using specific estimators, with the polynomial kernel k(x, y) being used. FID estimates for real data are biased, while KID does not suffer from this issue. The proposed DeepSpeech metrics, like KID, use unbiased estimators to avoid bias in scoring real data for conditional distances. In a conditional text-to-speech setting, bias in estimators can lead to positive values for cFDSD, while cKDSD values would be close to zero with unbiased estimators. The functions G and DS are defined for generating audio features, and real examples are jointly sampled for evaluation. In the context of generative models of audio, the \u00b5-law transform is utilized without quantization for continuous domain operation. The transform is applied with specific \u00b5 values for different bit encodings, showing improved performance in early experiments. Our early experiments demonstrated that models generating \u00b5-law transformed audio outperformed those using non-transformed waveforms. We utilized a 16-bit transformation and trained the models with a single discriminator step per generator step, adjusting the learning rates accordingly. The models were trained using the hinge loss, a batch size of 1024, and the Adam optimizer with specific hyperparameters. Spectral normalization and orthogonal initialization were applied to both the generator and discriminator, following previous research. The models were trained on Cloud TPU v3 Pods with data parallelism over 128 replicas for 1 million updates, taking approximately 48 hours. The pseudocode for training GAN-TTS is presented in Algorithm 1, and Figure 4 illustrates the gradual decrease of cFDSD during training."
}