{
    "title": "SygHGnRqK7",
    "content": "In federated learning, data is distributed across servers, making data exchange impractical. A Bayesian nonparametric framework is developed for federated learning with neural networks. Each server trains local neural network weights, and an inference approach synthesizes a global network without data pooling. The approach is tested on image classification datasets, showcasing its effectiveness in scenarios where data aggregation is costly. Federated learning overcomes challenges of centralized data by using algorithms that learn from separate data sources with different distributions. This approach avoids communication costs, time sensitivity, and privacy concerns associated with pooling data from wearable devices. The text discusses a probabilistic federated learning framework that trains and aggregates neural network models on separate data sources without centralizing data. It matches local model parameters across sources to construct a global network using a Beta-Bernoulli process. The construction of a global network in a probabilistic federated learning framework allows for flexible growth or shrinkage based on data needs. No assumptions are made about data distribution or local learning algorithms, which can be adapted as necessary. Communication is only required after local algorithms converge, leading to compressed global models with fewer parameters. This approach is more efficient than naive ensembles of local models during inference. The paper discusses the possibility of learning a compressed federated model without accessing original data, aiming to improve the performance of local networks. It introduces the Beta-Bernoulli process, describes the model for federated learning, and evaluates the proposed approach in Section 4. Limitations are discussed in Section 5. The approach in Section 4 builds on tools from Bayesian nonparametrics, specifically the Beta-Bernoulli Process and Indian Buffet Process. A random measure Q is drawn from a Beta Process with weights and atoms. Atoms are selected using a Bernoulli process with base measure Q. The Beta-Bernoulli process and Indian Buffet Process are used to select measures in a hierarchical construction. The Indian Buffet Process is described as customers arriving at a buffet and choosing dishes to sample. The IBP specifies a distribution over sparse binary matrices with infinitely many columns and was originally demonstrated for latent factor analysis. Our work focuses on extending the Indian Buffet Process (IBP) to federated learning of neural networks, a recent application of the IBP to distributed topic modeling has been done using the Beta-Bernoulli Process (BBP). Federated learning has gained attention in the machine learning community, with different approaches like multi-task learning and support vector machine (SVM) models being explored. Our work extends the Indian Buffet Process to federated learning of neural networks, which differs from previous approaches that use simple averaging of local learner weights. Unlike other methods, our framework is non-parametric and allows the federated model to adjust its complexity based on varying data complexity. Previous work on distributed deep learning is also considered. The curr_chunk discusses the application of Bayesian nonparametric machinery to federated learning with neural networks, aiming to identify subsets of neurons in local models that match with others to form an aggregate model. This approach differs from previous works on distributed deep learning, which focus on scalable training from large data with frequent communication between nodes. The curr_chunk discusses forming an aggregate model by matching subsets of neurons in local models. It focuses on training J Multilayer Perceptrons with one hidden layer each and learning a global neural network with weights and biases. The approach is based on Bayesian nonparametric machinery in federated learning with neural networks. The curr_chunk explains that the ordering of neurons in the hidden layer of a Multilayer Perceptron (MLP) is permutation invariant. This perspective justifies viewing hidden layers in neural networks as feature extractors, with each hidden neuron representing a new feature. The last layer of a neural network is seen as a softmax regression, but neural networks outperform basic softmax regression due to supplying high-quality features constructed from input features. Each hidden neuron in the MLP acts as a parameterization of the corresponding neuron's feature extractor. The curr_chunk discusses the construction of global feature extractors (neurons) by modeling the grouping and combining of feature extractors from multiple MLPs using a Hierarchical BBP based model. The generative model involves drawing global atoms (hidden layer neurons) from a Beta process prior. The construction of global feature extractors involves drawing neurons from a Beta process prior with a base measure. Batch specific distributions over global atoms are generated, and a subset of global atoms is selected for each batch. Observed local atoms are considered as noisy measurements of the corresponding global atoms. The algorithm for Maximum a posteriori estimation of global atoms involves maximizing the posterior of random variables that match observed neurons to global atoms. The MAP estimate is derived using Gaussian-Gaussian conjugacy, with an objective function to be maximized. The algorithm for Maximum a posteriori estimation of global atoms involves maximizing the posterior of random variables that match observed neurons to global atoms. Using optimization techniques, the objective function is derived and solved iteratively by fixing all but one variable at a time. The process involves rearranging terms and modifying the objective function to find the optimal assignment for B j. The algorithm for Maximum a posteriori estimation of global atoms involves maximizing the posterior of random variables to find optimal neuron matching assignments using the Hungarian algorithm. The assignment cost objective is solved iteratively by fixing variables one at a time, leading to the overall single layer inference procedure. Our approach involves converting neurons in each server to weight vectors using corresponding neurons in the output layer, forming a cost matrix for matching with the Hungarian algorithm. Matched neurons are aggregated and averaged to create a new layer in the global model. This model can handle single layer neural networks of any width, but we extend the approach to deep architectures by defining a generative model of deep neural network weights. In deep architectures, a generative model of deep neural network weights is defined from outputs back to inputs. The top-down approach involves considering global atoms as vectors of outgoing weights from a neuron. Each layer is generated using a model similar to the single layer case, with a Hierarchical Beta-Bernoulli process construction for selecting atoms. The number of neurons on each layer controls the dimension of the atoms. The generative process in deep architectures involves generating layers using a Hierarchical Beta-Bernoulli process. The inference procedure follows a top-down approach, inferring the matching of layers starting from the top layer. The per-layer inference is a straightforward copy of the single layer case. The text discusses the extension of a modeling framework to handle streaming data in federated learning settings, specifically focusing on wearable devices. The Bayesian paradigm is used to process temporal data with memory constraints, where the posterior of one step becomes the prior for the next step. The approach is generalized from a single hidden layer model to a streaming setting, with the ability to extend to a multilayer scenario. The text discusses the extension of a modeling framework to handle streaming data in federated learning settings, specifically focusing on wearable devices. The differences in the generative model effect (2) and (3), which become: We derive cost expression for the streaming extension in the Supplementary. To verify our methodology we simulate federated learning scenarios using two standard datasets: MNIST and CIFAR-10. Two partition strategies are of interest: homogeneous and heterogeneous. In empirical studies, the framework aggregates local neural networks into a global neural network that competes with ensemble methods. Experiments show that PFNM is the best approach under certain constraints, important for federated learning and model averaging of neural networks. A good neural network averaging approach can also serve as initialization for Knowledge Distillation. Federated Averaging with shared initialization quickly degrades for more than 2 networks or different class distributions. PFNM does not require shared initialization and can produce meaningful averages of many neural networks. Nonparametric clustering of weight vectors based on DP-means is also compared as an alternative approach. The DP-means method, specifically DP-means BID3 BID0, is proposed as an alternative to Beta Process construction. Test set performance of local models is compared using PFNM, which consistently outperforms baselines. The size of the global model is controlled by 0 / J, with smaller values potentially impacting performance quality. Each local neural network is trained for 10 epochs with 100 neurons in this experiment. The J local neural networks are trained for 10 epochs with 100 neurons each. For Federated averaging, 300 neurons per local network are considered to increase global model capacity. The maximum number of neurons for PFNM and DP-means is capped at 700. Global model sizes are summarized in FIG6 and 2d, showing significant compression compared to the maximum of 100J. The experiments demonstrate efficient model averaging of neural networks in the parameter space by considering permutation invariance of hidden neurons. The global model learned by PFNM can be used as a final solution for federated learning or as an initialization for better performance with distributed optimization, Federated Averaging, or Knowledge Distillation. The text discusses additional baselines for federated learning, including Uniform ensemble (U-Ens) and Weighted ensemble (W-Ens), which aim to aggregate multiple learners. U-Ens averages class probabilities from batch neural networks, while W-Ens weights class probabilities based on instances. However, these approaches have drawbacks such as high computational costs. Knowledge distillation (KD) is an extension of ensemble learning, where a smaller neural network mimics the behavior of an ensemble. The distributed optimization approach downpour SGD (D-SGD) requires frequent communication between batch servers and the master node for gradient updates. In experiments, D-SGD communicated once per training epoch, while other methods communicated only after training batch neural networks. Probabilistic Federated Neural Matching (PFNM) is compared against four extra-resource baselines. In experiments, D-SGD initially improves with increasing number of batches but then drops in performance as batch size decreases, affecting gradient quality. Ensemble approaches perform well with a collection of weak classifiers and do not degrade noticeably in performance. The advantage of ensemble approaches is that they perform well with weak classifiers and do not degrade noticeably in performance. However, there is a high computational burden when making predictions with batch neural networks. Weighted ensemble performs worse than uniform ensemble on heterogeneous CIFAR-10 case, possibly due to low quality batch networks. In a second experiment with multilayer batch neural networks, the multilayer PFNM can handle deep networks and outperform D-SGD. In this work, PFNM is highlighted as a strong solution for federated learning applications with limited prediction time and expensive communication. The study focuses on developing models for matching fully connected networks and emphasizes the importance of convergent local neural networks. Future work aims to explore more sophisticated ways to handle uncertainty in small batch weights. The matching approach is unsupervised, but incorporating supervised signals may enhance performance. The curr_chunk discusses the potential benefits of incorporating supervised signals in unsupervised matching approaches for neural networks. It also mentions the extension of the modeling framework to other architectures like CNNs and RNNs. The goal of MAP estimation is highlighted, focusing on maximizing the posterior probability of latent variables in neural networks. The curr_chunk discusses optimization of neural network weight estimates using global atoms. It outlines a method for finding MAP estimates based on Gaussian-Gaussian conjugacy and simplifies the optimization process. The approach involves iterative optimization by fixing all but one global atom at a time. The curr_chunk discusses rearranging the optimization process for neural network weight estimates using global atoms. It simplifies the objective function by focusing on solving for B j and rewrites it as a linear sum assignment problem. The method involves considering B j as the last batch and optimizing for B j to find MAP estimates. The curr_chunk discusses the linear sum assignment problem for optimizing neural network weight estimates using global atoms. It concludes with a visual illustration of the multilayer inference procedure and details provided in Algorithm 1. The curr_chunk discusses the Bayesian inference for the streaming extension of the model, focusing on the cost matrix and matching neurons. The global layer is formed by aggregating and averaging matched neurons, which is then used to match the next lower layer in a multilayer setting. The cost for finding B j,s is calculated, with a heuristic used for terms not available in closed form in Bayesian nonparametric literature. In the experiments, data partitioning for federated learning was analyzed using two strategies for MNIST and CIFAR-10 datasets. Homogeneous partitioning involved equal class distributions and batch sizes, while heterogeneous partitioning allowed for imbalanced class distributions in batches. Mean accuracies and standard deviations were obtained from 10 trials for each strategy-dataset pair. Code to reproduce the results will be released after the review period. In heterogeneous partitioning, batches can have imbalanced class distributions and variable sizes. A Dirichlet distribution is used to allocate instances to batches, with some batches potentially missing examples of certain classes. Batch networks are trained using PyTorch BID21 and Adam optimizer BID20. Parameter settings are summarized for reproducibility. The ensemble procedure is formally defined with probability distributions over classes. The ensemble procedure is defined with probability distributions over classes. It includes a uniform ensemble prediction and a weighted ensemble prediction based on the number of examples per class in each batch. Additionally, a knowledge distillation approach involves training a master neural network to minimize cross entropy with outputs from batch neural networks. The architecture of the master network is manually set with 500 neurons per layer. The master network in the knowledge distillation approach pools input data from all batches to the master server. PyTorch and Adam optimizer were used for training, with weights updated at the end of each epoch. The master network and batch networks have 50 neurons per layer, with no performance improvement observed when increasing neuron count. Our nonparametric model for matching neural networks infers the master network size from batch weights. The cost term expression controls the discovery of new neurons, limiting growth to avoid impractically large networks. Network sizes learned by PFNM are summarized in experiments with increasing batches and hidden layers. The maximum size is 50 neurons per batch per layer. The master network size in PFNM is inferred from batch weights, limiting growth to 50 neurons per batch per layer. The network is more compact compared to stacking batch neural networks. Communication rounds in D-SGD for federated learning are minimized to produce competitive results. Test accuracy on MNIST with heterogeneous partitioning is shown for comparison with ensemble based methods. In PFNM, communication rounds in D-SGD are minimized for competitive results. Streaming federated learning is simulated with heterogeneous partitioning of MNIST data. PFNM-Streaming method is evaluated for accuracy in a setup with J = 3 batches and S = 15 steps. Our method, PFNM-Streaming, updates the global neural network by matching based on cost computations and initializing batch neural networks for the next step. D-SGD updates global neural network weights after each step and uses them as initialization for batch neural networks. Ensemble methods are extended to streaming by updating local neural networks sequentially as new data becomes available. The PFNM-Streaming method updates the global neural network by matching based on cost computations and initializing batch neural networks for the next step. The models have three parameters: prior variance of weights, control for discovering new neurons, and variance of local neural network weights. Empirical analysis shows that the performance is robust to parameter values, with parameter 2 having a slightly higher impact. The parameters are set to 10 and 1 for 2 and 0 respectively in all experiments. In experiments, parameter 2 has a higher impact on performance. For homogeneous partitioning, 2 has minimal effect, while for heterogeneous partitioning, it is more noticeable but all values lead to competitive performance."
}