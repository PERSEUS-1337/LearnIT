{
    "title": "r1g4E3C9t7",
    "content": "Recent studies have highlighted adversarial examples as a threat to neural network models and downstream applications. This paper explores using temporal dependency in audio data to improve robustness against adversarial attacks in automatic speech recognition tasks. Results show that temporal dependency provides discriminative power against audio adversarial examples and is resistant to advanced attacks. The study explores using temporal dependency in audio data to enhance robustness against adversarial attacks in automatic speech recognition systems. Results indicate that leveraging domain-specific data properties can help mitigate the impact of adversarial examples on Deep Neural Networks (DNNs) used in various machine learning applications. While DNN architectures vary, attacking methodology for generating adversarial examples involves maximizing training loss or optimizing attack objectives. Unique data properties, like temporal dependency in audio data, can potentially enhance discriminative power against adversarial inputs. The paper investigates the robustness of automatic speech recognition models against adversarial examples by exploring input transformation techniques and utilizing temporal dependency for discriminative power. Experimental results show limited improvement with transformation techniques against recent attack methods, but demonstrate the potential of temporal dependency in enhancing model robustness. The paper explores the robustness of automatic speech recognition models against adversarial attacks, demonstrating promising results in identifying non-adaptive and adaptive attacks. The proposed method also shows resistance against strong adaptive attacks, with potential for extending the approach to other domains. This research sheds new light on designing defenses against various types of data attacks. The paper discusses different types of attacks for generating audio adversarial examples, such as Speech-to-Label and Speech-to-Text attacks. Various methods have been proposed to create adversarial examples, including genetic algorithms and probabilistic loss functions. Additionally, research has been done on adversarial training and data augmentation to improve model robustness. The proposed approach focuses on gaining discriminative power against adversarial examples through embedded temporal dependency. The curr_chunk discusses the unique properties of audio data, specifically the temporal dependency in waveforms that is leveraged by recurrent neural networks. It questions whether temporal dependency can improve model robustness similar to spatial consistency in images. Temporal dependency in audio data is explored to improve model robustness against adversarial attacks. The study aims to transfer lessons from image adversarial examples to the audio domain and investigate the discriminative power of temporal dependency. Experimental results show that audio input transformation is not effective against adversarial attacks, unlike in the image domain. This research highlights the importance of unique data properties in building robust machine learning models and can aid in studying more complex data types like videos or multimodal cases. Input transformation is a common defense technique in the image domain, involving feature transformation on raw images to disrupt adversarial perturbations before neural network processing. Methods like bit quantization, image filtering, and autoencoder reformation are used. However, these methods can be bypassed by subsequent or adaptive attacks, and may cause obfuscated gradients, leading to false robustness. Athalye et al. have shown that gradient obfuscation can be circumvented, making input transformation vulnerable to adversarial examples. In contrast to input transformation methods vulnerable to adversarial examples, this paper proposes leveraging temporal dependency in audio data for detecting adversarial examples in automatic speech recognition tasks. The methodology effectively discriminates against audio adversarial examples while maintaining recognition performance on normal examples. The experimental results demonstrate that an adaptive adversarial attack cannot bypass the proposed temporal dependency-based defense method in automatic speech recognition. The weakness of defense techniques in image cases is likely to transfer to the audio domain. Leveraging temporal dependency in ASR can lead to effective defense approaches against adaptive adversarial attacks. In an attempt to discriminate audio adversarial examples, primitive signal processing transformations were applied to audio inputs. These transformations include quantization, local smoothing, and downsampling to disrupt adversarial perturbations and reduce their impact on the input space. In an effort to defend against audio adversarial examples, signal processing techniques such as downsampling and using an autoencoder to remove adversarial noises have been employed. The 16kHz audio data is downsampled to 8kHz before signal recovery, and a sequence-to-sequence autoencoder is used to process frame-level pieces of the audio. The study explores the impact of adversarial perturbations on temporal dependency in audio sequences. A method is proposed where the first k portion of the audio sequence is inputted into an ASR to obtain transcribed results. The consistency between the transcribed results of the prefix and the entire sequence is compared using word error rate as the distance metric. This comparison aims to assess the effect of adversarial perturbations on temporal dependency. The study investigates the impact of adversarial perturbations on temporal dependency in audio sequences. It is hypothesized that the consistency between the transcribed results of the prefix and the entire sequence may be affected by added perturbations aiming to alter the ASR output. Experimental results will focus on datasets, target learning models, attack methods, and evaluation metrics for defense/detection effectiveness against different attacks. The study evaluates defense/detection methods against adversarial attacks on audio sequences. Autoencoder defense is ineffective for adversarial audios and may harm benign instances. Input transformation is less effective for audio than images. The proposed TD method effectively detects adversarial audios across various learning tasks. Strong adaptive attacks against the TD method are unsuccessful. In experiments evaluating defense methods against adversarial attacks on audio, the TD method effectively detects adversarial audios. Strong adaptive attacks are unsuccessful in generating effective adversarial audio against TD. Case studies are provided to understand TD's performance, measuring effectiveness on various adversarial audio generation methods using different datasets like Speech Commands, LibriSpeech, and Mozilla Common Voice. The Common Voice dataset by Mozilla contains human speech audio samples. The dataset used for attacks has an average duration of 3.998s. The Speech Commands dataset consists of 65000 audio files with single commands like \"yes\" and \"no\". DeepSpeech is used for speech-to-text tasks, while a convolutional model is used for audio classification. The Command Song attack performance is evaluated on Kaldi speech recognition. The Commander Song attack targets audio from popular songs for speech-to-text translation. The attack generates adversarial audio with characteristics that can be played over the air. The effectiveness of the generated adversarial audios is measured since the Commander Song codes are not available. Optimization based attack against speech-to-text translation (Opt): The targeted speech-to-text attack uses CTC-loss in a speech recognition system as an objective function and solves the task of adversarial attack as an optimization problem. Evaluation metrics like word error rate (WER) and character error rate (CER) are used to measure the recovery efficiency of defense methods. WER and CER are commonly used metrics to measure the error between recovered text and the ground truth in word or character level. The effectiveness of transformations against speech-to-text attacks is evaluated by comparing the translation distance between instances and ground truth before and after transformation. A controlled experiment involves calculating effectiveness ratios for benign and adversarial instances using distance functions like WER and CER. The proposed TD method is a data-specific metric for detecting adversarial audio, with evaluation based on the area under curve (AUC) score. The proposed TD method is the first data-specific metric to detect adversarial audio, focusing on true positive detection without affecting false positives. Evaluation is based on AUC, comparing temporal dependency using WER, CER, and LCP. Defense methods include autoencoder and input transformation for classification and speech-to-text attacks. Commander lacks training data for autoencoder, while GA and Opt have sufficient data. The input transformation for audio classification targeted attacks was evaluated against the GA attack in BID0. The attack success rate was 84% on average, but decreased to 2.1% with the Quantization-256 input transformation. 63.8% of the adversarial instances were converted back to their original label. The effects of input transformation on audio classification accuracy were minimal, with a slight decrease from 89.2% to 89.0%. Input transformation was found to be more effective in mitigating adversarial perturbations in classification tasks compared to speech-to-text tasks. The input transformation method was also evaluated against the Commander Song attack BID36, with a reported 91% attack detection rate. In the study, the Quan-256 input transformation achieved a 100% detection rate for characterizing adversarial examples. Primitive signal processing methods were evaluated for effectiveness in mitigating audio attacks, with results shown in tables TAB1 and A2. The effectiveness ratio of the transformations was quantified using WER and CER metrics. The study evaluated input transformations like Median-4, Downsampling, and Quan-256 to reduce adversarial perturbations in audio. While these transformations were effective in defending against adversarial audios, adaptive attacks were still possible. MagNet BID25 used an autoencoder to mitigate adversarial perturbations in non-adaptive adversarial images. The study applied an autoencoder structure for audio to defend against adversarial attacks. The MagNet-like method was used to transform audio spectrum maps, but the effectiveness was limited compared to defending adversarial images. Autoencoder did not perform well against classification attacks, only reducing the attack success rate to 8.2%. The autoencoder method did not perform well in reducing attack success rates compared to other input transformation methods. It showed poor performance in transforming benign instances and failed to recover adversarial audio. This highlights the limitations of using non-adaptive additive adversarial perturbations to bypass the autoencoder on audio data. The proposed TD detection method will be evaluated on different attacks to demonstrate its effectiveness. The TD method is evaluated on speech-to-text attacks (Commander and Opt) to demonstrate its effectiveness. In the Commander attack, generated adversarial audio inconsistency is detected with k = 1 2. The Opt attack shows empirical performance in distinguishing adversarial audios using WER, CER, and LCP metrics. The study evaluates the TD method on speech-to-text attacks (Commander and Opt) using WER, CER, and LCP metrics. Different adversarial targets are considered, and AUC scores are reported for detection results. Results show high AUC scores using WER as the detection metric, with AUC reaching 0.936 on Common Voice and 0.93 on LIBRIS. Varying values of k show consistent results, with AUC reaching 0.969 when k = 4/5 based on CER. In this section, adaptive attacks are applied against input transformations to evaluate the robustness of the defense methods. Opt is used as the strongest attack for adaptive attacks on speech-to-text translation. The experiments' structure is listed in TAB4, with results suggesting that the temporal dependency based method is effective in characterizing adversarial audio attacks. The autoencoder defense method almost fails against different attacks, highlighting the importance of input transformation based defense and TD detection. The study evaluates the robustness of input transformations as defenses against adaptive attacks. Three input transformation methods - Quantization, Local smoothing, and Downsampling - are used to generate adaptive attacks. The optimization-based attack aims to minimize perturbation while maximizing adversarial impact. The attack is adjusted for quantization transformation by incorporating the quantization parameter q. This approach ensures that adversarial audios are resistant to quantization transformations. When q is large enough, distortion increases but transformation becomes ineffective due to information loss. Adaptive attacks are conducted on sampled elements for downsampling transformation. Local smoothing is differentiable for average smoothing, allowing effective gradient passing. Median smoothing attack involves converting gradient back to median for updating, similar to maxpooling layer's backpropagation. Adaptive attacks are implemented successfully. The adaptive attack was implemented on audio samples from LIBRIS and Common Voice datasets using input transformation methods like down-sampling, smoothing, and quantization. Decibels were used to measure the perturbation magnitude, and the relative perturbation was calculated. The effectiveness of the adaptive attack was demonstrated based on the measured criteria. The adaptive attack was effective with reasonable perturbation levels, as shown in Table 6. Adversarial audios had distortion levels tolerable to human ears, mostly within -15dB to -45dB range. Strong adaptive attacks were conducted against the temporal dependency based method to evaluate its robustness. Three types of strong adaptive attacks were proposed to explore the method's temporal consistency. The text discusses attacking audio by dividing it into two parts, applying perturbations to each part individually, and then combining them. Two methods are proposed: attacking the first part while leaving the second unchanged, and attacking the entire audio while silencing the second part. This approach aims to balance attack success rates for different sections of the audio. The attack success rate for segment and concatenation attacks on audio remains low, with reasons including temporal dependency and recognition process disruptions. Examples of failed attacks are provided in the Appendix. The failure of the concatenation adaptive attack highlights the importance of temporal dependency in audio. Concatenating separate parts breaks the perturbation, making the adaptive attack inefficient. This has negligible effects on benign audio, showing promise for detecting adversarial audio. Different section portions are evaluated for combination attacks, with varying results based on the attacker's strategy. The study evaluates the effectiveness of adaptive attacks in audio data. Results show that using the same key for attack and defense yields good performance, while using different keys results in failure. Randomly sampling defense keys makes attacks difficult, improving model robustness. For multiple keys, attacker success decreases as the number of keys increases. The paper suggests leveraging temporal dependency in audio for characterizing adversarial examples. Temporal dependency in audio is resistant to adaptive adversarial attacks, showcasing its power in characterizing adversarial examples. The method proposed does not require model retraining and offers insights into enhancing adversarial robustness."
}