{
    "title": "B1lGU64tDr",
    "content": "Real-world dynamical systems often involve multiple stochastic subsystems that interact with each other, making modeling and forecasting challenging. The relational state-space model (R-SSM) uses graph neural networks (GNNs) to simulate joint state transitions of correlated objects, enhancing the modeling of multi-object dynamics. By combining GNNs with SSM, R-SSM incorporates relational information effectively. The model is augmented with normalizing flows for vertex-indexed random variables and auxiliary contrastive objectives to aid learning. Empirical evaluation on synthetic and real time series datasets demonstrates the utility of R-SSM in capturing complex system dynamics. State-space models (SSMs) are used to analyze dynamical systems and sequence data, with deep SSMs incorporating neural networks for improved modeling of multi-object systems in various fields like physics, ecology, and finance. Deep SSMs use neural networks for flexible state transitions and emissions, with GNNs being a promising choice for modeling multi-object systems. AR models based on recurrent GNNs are seen as special instances of SSMs but are limited by deterministic state transitions and oversimplified observation distributions. In this study, the relational state-space model (R-SSM) is introduced, utilizing graph normalizing flow (GNF) to capture joint evolutions of correlated stochastic subsystems. Structured posterior approximation and auxiliary training objectives are developed for learning R-SSM using variational inference. Experimental results demonstrate competitive test likelihood and prediction performance compared to GNN-based AR models. The paper introduces the relational state-space model (R-SSM) using graph normalizing flow (GNF) to capture joint evolutions of correlated stochastic subsystems. It includes sections on necessary preliminaries, formal introduction of R-SSM, related work, and experimental evaluation. An attributed directed graph is defined by a 4-tuple, and notation for vertices and edges is explained. Graph Neural Networks (GNNs) are neural networks designed for processing graph-structured data and supporting relational reasoning. They focus on updating vertex representations in a graph by considering nearby vertices and using a multi-round message passing paradigm. This involves vertices sending messages to neighbors, aggregating received messages, and updating their own representations. In message-passing GNNs, basic blocks are defined using Equations (1) and (2) with a multi-head attention mechanism. Equation (3) utilizes either a RNN cell or a residual block. A GNN stacks separately-parameterized MHA blocks to compute H iteratively. State-space models analyze dynamical systems with latent states {z_t} and exogenous inputs {u_t}, parameterized by static parameter \u03b8. State-space models (SSMs) analyze dynamical systems with latent states {z_t} and exogenous inputs {u_t} parameterized by static parameter \u03b8. The latent state process has an initial density z1 \u223c \u03c0\u03b8(\u00b7|u1) and a transition density zt+1 \u223c f\u03b8(\u00b7|z\u2264t, u\u2264t+1). Noisy measurements of the latent state are observed through an observation density x_t \u223c g\u03b8(\u00b7|z\u2264t, u\u2264t). Deep SSMs use RNNs to compress z\u2264t (and u\u2264t) into fixed-size vectors for tractability. R-SSM enables multiple deep SSMs to communicate, and normalizing flows are utilized for invertibility. Normalizing flows are invertible transformations that can convert simple probability densities into complex ones. By chaining together invertible mappings with efficient determinants, expressive distributions can be constructed. This approach is particularly useful for dynamical systems with multiple interacting objects that are observed simultaneously. Our model uses Graph Neural Networks (GNNs) to capture interactions between objects in a multi-object dynamical system. The system's interaction structure is represented as a directed graph, where each object is a vertex and edges indicate relationships between objects. A relational state-space model utilizes graph neural networks to capture interactions between objects in a dynamical system. Sparse graph structures are preferred for modeling systems with many objects due to their stronger relational inductive bias and computational efficiency. The model assumes correlated dynamical subsystems evolving jointly under the coordination of GNNs, with each object corresponding to a vertex in the graph. The relational state-space model uses graph neural networks to capture interactions between objects in a dynamical system. It introduces global and local latent variables to represent shared and individual states. An R-SSM factorizes the joint density of observations and latent states using matrix notation. A GNN with RNN cells compresses past dependencies into context vectors, generating observations from a diagonal Gaussian distribution parameterized by an MLP output. The generative model involves a multilayer perceptron (MLP) output and a local transition density f \u22c6 \u03b8. The local observation distribution g \u03b8 can be Gaussian or a mixture of logistic distributions. The N + 1 latent state processes interact through a graph neural network (GNN), allowing vertices to depend on each other's state trajectories. An R-SSM can be seen as an ordinary SSM where the entire graph evolves together. The generative model involves a multilayer perceptron (MLP) output and a local transition density. The N + 1 latent state processes interact through a graph neural network (GNN), allowing vertices to depend on each other's state trajectories. An R-SSM can be seen as an ordinary SSM where the entire graph evolves together. The joint density of latent states and observations factors as the integral is intractable, so variational sequential Monte Carlo (VSMC) is used to maximize a variational lower bound on the log marginal likelihood. The variational lower bound is obtained using biased gradient estimator \u2207L Proposal design, with proposal distributions structured for all time steps. The proposal for Z t depends on information up to time t and shares parameters with the generative model. B t acts as a belief state summarizing past observations deterministically. Graphical structure of proposal design is shown in Figure 1c. The proposal design for all time steps involves a Graph Normalizing Flow (GNF) to model joint state distributions for correlated random variables indexed by graph nodes. This approach introduces inter-object dependencies without complicating the learning process. The Graph Normalizing Flow (GNF) is used to model joint state distributions for correlated random variables indexed by graph nodes. The GNF coupling layer splits the input into two parts and uses GNNs to enforce equivariance. The GNF combines a coupling layer with an affine layer and a 1\u00d71 convolution layer to increase model expressivity. Multiple GNFs can be stacked to construct transition and proposal densities. Message passing inside coupling layers transforms noise, while 1\u00d71 convolution layers eliminate manual dimension permutation. Initial experiments showed issues with posterior collapse in learning R-SSM. In initial experiments, learning R-SSM faced posterior collapse issues, a common problem in VAE training. This phenomenon causes the variational posterior approximation to degenerate into the prior early on, leading to training getting stuck in undesirable local optima. Additionally, a challenge in likelihood-based training of deep sequential models was observed, where the model tended to capture only short-term local correlations for smooth observations. To address these issues, the CPC approach was used to force latent states to perform two auxiliary tasks. To address posterior collapse and short-term correlation capture issues in learning R-SSM, the latent states are forced to perform two auxiliary contrastive prediction tasks. Future observations of each vertex are summarized into a vector using a backward RNN at each time step. Two auxiliary CPC objectives are defined to encourage encoding useful information and reflect interaction effects. Negative samples are selected from future summaries of other vertices within the minibatch. GNNs are used in neural physics simulators and dynamics models, providing a framework for learning on graph-structured data. The objective to maximize involves tunable hyperparameters \u03b21 and \u03b22, with the estimation procedure described in the appendix. GNNs are integrated into various models for multi-agent forecasting, including seq2seq framework and generative adversarial networks. R-SSM introduces structured latent variables for uncertainty representation. GNNs are also combined with sequential latent variable models in works like R-NEM, NRI, SQAIR, VGRNN, MFP, and Graph VRNN. The latent variables in existing models like R-NEM, NRI, and Graph VRNN are discrete, while our model uses continuous latent variables to represent object states. Our model introduces a global latent state process and explores normalizing flows and contrastive objectives. It differs from Graph VRNN by incorporating a hierarchical structure and additional features. Deep latent variable models (LVMs) for sequential data have gained interest, utilizing neural networks to incorporate stochastic latent variables into vanilla RNNs or developing deep state space models (SSMs) with flexible transition and emission distributions parameterized by neural networks. Various works have focused on approximate inference and parameter estimation methods for nonlinear SSMs in the literature. Incorporating stochastic latent variables into RNNs and developing deep state space models (SSMs) with flexible distributions have been extensively studied. VSMC combines VI and SMC, addressing the posterior collapse problem. Z-forcing predicts future summaries directly, but the backward RNN may degenerate easily. R-SSM is implemented using TensorFlow Probability library. Experiments validate R-SSM's performance on a toy dataset. In Section 5.2, R-SSM is compared with state-of-the-art sequential LVMs for multi-agent modeling on a basketball gameplay dataset, and the effectiveness of GNF is tested through ablation studies. In Section 5.3, the prediction performance of R-SSM is compared with strong GNN-based seq2seq baselines on a road traffic dataset. Detailed model architecture and hyperparameter settings for each dataset are provided in the Appendix. Values reported with error bars are averaged over 3 or 5 runs. A simple toy dataset is constructed to illustrate the capability of R-SSM using the symmetric stochastic block model. In a dataset where each vertex belongs to one of K communities, vertices are connected with probability p0 if in the same community, p1 otherwise. A covariate vector is attached to each vertex, and 10K examples are generated for training. Comparing R-SSM with baselines like VAR, VRNN, and GNN-AR, the dataset is challenging for common models to fit. The models VAR, VRNN, GNN-AR, and R-SSM are evaluated based on three metrics: LL, MSE, and CP. Results show that VAR performs poorly, VRNN underfits and struggles to generalize, while GNN-AR and R-SSM show better performance. The dataset used for evaluation is challenging for common models to fit. In contrast, GNN-AR and R-SSM generalize well and achieve higher test log-likelihood. The importance of latent variables for capturing uncertainty in stochastic multi-object systems is highlighted. Training dynamics without L aux 1 easily get stuck in posterior collapse, while adding L aux 2 improves test likelihood. R-SSM is compared with MI-VRNN for multi-agent trajectories in basketball gameplay. The dataset includes 107,146 training examples. The MI-VRNN model, denoted as MI-VRNN, is compared with R-SSM for multi-agent trajectories in basketball gameplay. The dataset consists of 107,146 training examples and 13,845 test examples with 2D trajectories of players and the ball. Ablation studies are conducted to validate the proposed ideas, with test likelihood bounds and rollout quality metrics reported in Table 3. The VRNN baseline by Zhan et al. (2019) is also included for comparison. The R-SSMs outperform baselines in test log-likelihood. Adding L aux 1 is necessary for successful training. Training with L aux 2 and adding GNFs improve results. R-SSM with 8 GNFs achieves higher likelihood than with 4 GNFs. Rollouts from the model match ground-truth better in OOB rate. Preliminary results for trajectory setting are also provided in Table 3. In addition to the trajectory of the ball, R-SSM is compared with GVRNN using a complete graph of ball and players as input. GVRNN outperforms R-SSM due to its GNN-based observation model, encoding past observations into the prior, and implementation tricks like predicting changes in observations. Traffic speed forecasting on road networks is a challenging task due to complex spatiotemporal interactions. R-SSM is compared to GNN-based seq2seq baselines on the METR-LA dataset, showing comparable performance. The dataset includes 4 months of 1D traffic speed measurements from 207 sensors aggregated into 5-minute windows. R-SSM utilizes conditional inputs G = (V, E, V, E) and U 1:T, where E connects sensors based on road network distance, V stores sensor positions and embeddings, E stores road network distances, and U 1:T provides time information. R-SSM is trained on small time windows spanning 2 hours and evaluated on a 7:1:2 split for training, validation, and test. The comparison of mean absolute forecast errors (MAE) for forecast horizons of 15, 30, and 60 minutes shows that R-SSM delivers comparable short-term forecasts but slightly worse long-term forecasts compared to DCRNN and GaAN. The results are attributed to the training methods and the complexity of stochastic systems. In this work, a deep hierarchical state-space model is presented where state transitions of correlated objects are coordinated by graph neural networks. The model is trained on small time windows and evaluated on forecast horizons. The study highlights the challenges of multi-step prediction in stochastic systems and suggests using probabistic forecasts for better uncertainty estimates. Future work aims to improve the multi-step prediction ability of deep SSMs. The model introduces graph normalizing flow to enhance transition density and posterior approximation. Experiments show superior performance on time series tasks. Future work includes testing on high-dimensional data and incorporating visual learning and discrete latent variables. Kingma & Dhariwal (2018) proposed an element-wise affine layer for activation normalization, while Hoogeboom et al. (2019) parameterized the invertible linear transformation using QR decomposition."
}