{
    "title": "BkVvwj0qFm",
    "content": "The Geometric Operator Convolutional Neural Network (GO-CNN) framework integrates domain knowledge by replacing the kernel of the first convolutional layer with a geometric operator function. It adapts to various problems and outperforms common CNNs on CIFAR-10/100 despite having fewer trainable parameters. Theoretical analysis shows convergence and generalization error bounds between GO-CNNs and common CNNs. The Geometric Operator Convolutional Neural Network (GO-CNN) framework improves on common CNNs by reducing dependence on training examples, enhancing adversarial stability, and integrating domain knowledge. CNNs have limitations in utilizing prior information and generalizing with little data due to overfitting. Research has focused on modifying CNNs to address these issues. Geometric operators like Sobel, Schmid, and HOG extract edge and texture information from images for various computer vision tasks. They rely on domain knowledge and prior information but cannot adjust parameter values automatically. The method proposed in this paper combines geometric operators with Convolutional Neural Networks to improve data learning. It directly constructs geometric operator convolution and integrates it into the network, aiming to reduce parameter redundancy and achieve global optimization. The Geometric Operator Convolutional Neural Network integrates geometric operators into a new framework, achieving global optimizations and reducing parameter redundancy. It enhances adversarial stability and has broad customization capabilities for diverse problems. Convolutional Neural Networks (CNNs) have been successful in classification and recognition tasks. Researchers use domain knowledge and prior information tailored to each specific task to improve classification accuracy. This can be done through traditional image processing algorithms as preprocessing steps or to initialize convolution kernels. Different pre-processing models, such as filters or feature detectors, have been used to enhance CNN accuracy, like the Gabor filter with CNN BID10, which is based on human vision. The Gabor filter, Fisher vectors, sparse filter Banks, and the HOG algorithm are used with CNNs to improve accuracy in texture representation and discrimination. Some methods use traditional image processing algorithms for multi-scale feature extraction. Geometric operators are also widely used in these algorithms. Geometric operators like the Gabor filter are commonly used in traditional image processing algorithms to improve accuracy. Researchers have experimented with incorporating Gabor filters in the first convolutional layer of CNNs to eliminate pre-processing overhead and enhance classification accuracy. Additionally, some studies have focused on using filters to initialize multiple convolutional kernels, such as creating kernels in four directions using the Gabor function. These methods aim to improve the initialization process in Convolutional Neural Networks. The Geometric Operator Convolutional Neural Network integrates geometric operators into a CNN to reduce parameter redundancy and enhance model transformation ability. Different operators like the Roberts, Laplace, and Gabor are used based on application scenarios. The GO-CNN proposed in this paper integrates geometric operators like SIFT, Roberts, and Laplace into CNNs for pattern recognition and image enhancement. The framework involves convolving geometric operators such as Gabor and Schmid to study frequency characteristics of local range signals. The Gabor filter, originally proposed as the \"Window\" Fourier transform by BID11 in 1946, combines spatial and frequency signal processing. It uses a gaussian window to extract local information in both the spatial and frequency domains. The Gabor filter, like the Fourier transform, extracts local information in the frequency domain. It can be integrated into CNN due to similarities with convolution kernels. The Gabor operator can be integrated into CNN to extract directional correlation texture features from an image. Two main binding methods are discussed: preprocessing the image with the Gabor operator before feature extraction by CNN, or convoluting the Gabor operator to form a convolution layer and integrating it into CNN. The Gabor kernel function has 5 parameters learned and regenerated into an m\u00d7m kernel, replacing common convolution kernels to form a convolutional layer. The Schmid operator, similar to the Gabor operator, is integrated into a Convolutional Neural Network to improve adversarial stability to rotation and enhance image feature extraction. It has rotation invariance and is convoluted to reduce the number of trainable parameters in the convolutional layer. The Schmid kernel function, with two parameters, is learned and generated to replace common convolution kernels in the network. In this paper, Schmid kernels are used to replace common convolution kernels in a convolutional layer. Different geometric operators can be replaced with geometric operator convolutions, making them customizable. The question addressed is how to combine multiple geometric operators with common CNNs to form GO-CNNs. The visualization of the first layer of convolution kernels maintains geometric characteristics, so the convolution kernel in the first convolutional layer is replaced by a kernel generated from geometric operators. In this study, Schmid kernels are utilized to replace common convolution kernels in a convolutional layer, creating a Geometric Operator Convolutional Neural Network (GO-CNN). Kernels in the first convolutional layer are generated from various geometric operator functions, known as generator functions, and concatenated to form a complete convolutional kernel. This unique approach defines the forward propagation of the GO-CNN, where the gradient of loss is transferred to the convolution kernel during backward propagation. The Geometric Operator Convolutional Neural Network (GO-CNN) utilizes Schmid kernels in place of common convolution kernels. The convolution kernel is generated by geometric operator functions and the gradient is transferred to the parameters of each convolution kernel during training. The effectiveness of GO-CNN for computer vision tasks is theoretically proven to be maintained despite a decrease in trainable parameters. The input is denoted by DISPLAYFORM0, with corresponding labels {y i |y i = 0 or 1}. The loss function is Mean Square Error. The output of the neural network is \u1ef9 i for each input I i, and the empirical loss function is defined. Parametric Convolutional Kernel Space is defined as a function mapping vectors to matrices, with n as the parameter number, m as the kernel size, and od as the output dimension. The generator function f generates convolutional kernels with fewer parameters, reducing the trainable parameters of GO-CNN. GO-CNN, a variant of CNN, uses a parametric convolutional kernel space to reduce trainable parameters. The first convolutional layer's kernel is generated from this space, making GO-CNN similar to common CNN but with a different kernel in the first layer. If the function of the first layer is not injective, meaning different inputs can lead to the same output, the network takes on a different characteristic. The first convolutional layer in GO-CNN must have an injective function to avoid errors. The kernel of the layer needs to satisfy a specific condition to be injective, ensuring different inputs lead to unique outputs. The 3x3 kernel generated by the Gabor kernel function satisfies a specific condition for injectivity in the convolutional layer of a GO-CNN. If a kernel in the parametric convolutional kernel space fails to meet this condition, it is not considered well-defined. The set of all well-defined GO-CNNs is denoted as G*f. When the generator function is the Gabor kernel function, the GO-CNN is well-defined. Theorem 1 states that for common CNNs, if the first fully connected layer is wide enough, the empirical loss of a well-defined GO-CNN can be controlled. The generalization error of well-defined GO-CNNs is almost the same as common CNNs, as shown in Theorem 2. The goal is to identify which GO-CNNs are well defined. The generalization error of well-defined GO-CNNs can be controlled if the first fully connected layer is wide enough. GO-CNN with Gabor kernel function as the generator function is well defined. If there are many generator functions in the first convolutional layer and a sufficient number of kernels generated by Gabor kernel function, the GO-CNN is also well defined. The GO-CNN, denoted by G, has kernels generated by function f tj. If a Gabor kernel function exists and enough kernels are generated, G is well defined. Experimental details are provided for the GO-CNN with experiments on CIFAR-10/100 datasets. The network uses half trainable Schmid kernels and half trainable Gabor kernels in the first layer. The GO-CNN, denoted by G, uses Schmid kernels and half trainable Gabor kernels in the first layer. Experimental results show that GO-CNN achieves similar accuracy and generalization error bound as common CNNs on CIFAR-10/100 datasets. In practical applications with limited annotated data, GO-CNN performs well. In numerical experiments with CIFAR-10/100 and MNIST datasets, the test set is used to train the model, while the train set is used to evaluate it. The basic network structure used for the MNIST dataset is LeNet, and in GO-CNN, the first convolutional layer is replaced by an operator convolutional layer with trainable Gabor and Schmid kernels. The accuracy of MNIST and CIFAR-10/100 datasets is compared in TAB2. The GO-CNN outperforms the common CNN in accuracy on MNIST and CIFAR-10/100 datasets, especially when the train set size is reduced. It shows better performance in predicting unknown data and is less affected by adversarial attacks. Additionally, when the test set is rotated, the GO-CNN performs 1.21% better than the common CNN, demonstrating its stability. The GO-CNN enhances adversarial stability of rotated and Gaussian disturbance samples, showing a 1.21% and 0.6% improvement over common CNN, respectively. It utilizes a priori knowledge from medicine for better recognition in intelligent medical diagnoses of bone fractures. Despite fewer trainable parameters, GO-CNN maintains similar accuracy and slightly lower generalization error compared to common CNN. The Geometric Operator Convolution Neural Network (GO-CNN) reduces dependence on training samples and enhances adversarial stability. It replaces the kernel in the first convolutional layer with geometric operator functions, providing customization for diverse situations and theoretical guarantees. Future exploration includes a more appropriate geometric operator convolution block and an explanation of Gabor and Schmid operators. The Geometric Operator Convolution Neural Network (GO-CNN) replaces the kernel in the first convolutional layer with geometric operator functions, providing customization for diverse situations and theoretical guarantees. Gabor filters extract directional correlation texture features from images, while Schmid operators exhibit rotation invariance. The proposition is proven by showing that if I * w = 0, then I = 0. The proposition is proven by showing that if I * w = 0, then I = 0, where I is a matrix defined in a specific way. Different parameters are chosen to discuss the equations derived from the pixel generator function. The equations show relationships between various parameters like \u03b8, x, y, \u03b3, and \u03c3. From Eqn.13.8, we derive equations involving various parameters like a, h, and \u03bb. By combining these equations, we find that certain values equal zero, leading to specific relationships between variables. By combining equations, it is found that certain values equal zero, leading to specific relationships between variables in common CNN structures. The convolution kernel and fully connected layers are defined with sigmoid activation functions, resulting in defined output before and after activation. After activation, denoted by F(x), the common CNN is defined as F = {F} with output before and after activation as Fi, Fi. For a GO-CNN G, the convolutional kernel is defined as kg, and weights and biases are denoted as G i, G i. Neuron numbers are maintained for each layer in common CNN and GO-CNN, with different approximation abilities. The width of each layer is defined, and the convolution kernel for common CNN is denoted as kF. Sigmoid activation is represented by \u03c3. The biases and activation functions of the convolutional and fully connected layers in common CNN and GO-CNN are defined. Neuron numbers are maintained for each layer in both networks. Neuron numbers are defined for each layer in common CNN and GO-CNN. The width of each layer is determined. The mapping function between input and output is established. The GO-CNN is shown to be injective. The network can be treated as a one hidden layer neural network. Theorems and inequalities are proven. The GO-CNN is proven to be injective and can be treated as a one hidden layer neural network. Theorems and inequalities are established, leading to the conclusion that recognizing objects in a scene relies on prior information rather than domain knowledge. The Geometric Operator Convolutional Neural Network's recognition effect for object recognition tasks is highlighted. The Geometric Operator Convolutional Neural Network's recognition effect for object recognition tasks is explored using CIFAR-10 and CIFAR-100 datasets. ResNet18, ResNet34, and ResNet50 BID14 frameworks are used with data augmentation techniques. Stochastic gradient descent optimization with momentum is employed, and images are normalized to a 0-1 distribution during training. Batch size is 100 for both testing and training. The Geometric Operator Convolutional Neural Network (GO-CNN) is trained using a batch size of 100, initial learning rate of 0.1, and weight decay of 0.0005. The learning rate decreases by one fifth every 60, 120, and 160 epochs. Performance is evaluated after 200 epochs, showing that GO-CNN achieves similar accuracy to common CNN. T-SNE visualization is used for data mapping in two or three dimensions. In this paper, a two-dimensional T-SNE visualization is used to display CIFAR-10 features extracted by the Geometric Operator Convolutional Neural Network. The features are clearly separated in the visualization, making them more distinguishable and easier to classify compared to features extracted by a common Convolutional Neural Network. The MNIST dataset is also discussed, which consists of handwritten images with ten classes and is used for numerical experiments. The MNIST dataset contains 10,000 images and is used for numerical experiments with the adaptive moment estimation BID18 optimization algorithm. Image padding is increased to 32\u00d732 during training, with a batch size of 100, initial learning rate of 0.001, and weight decay of 0.0005. The learning rate remains constant until 20,000 iterations are reached. The algorithm's final performance evaluation is based on averaging results over five runs on one test set. In China, medical imaging resources are limited in small to medium-sized cities, leading patients to seek treatment in larger cities with better resources. In China, patients often travel to big cities for better medical treatment due to limited resources in smaller cities. Doctors use texture information from images for fracture diagnosis, with preprocessing done by Schmid operators. The GO-CNN proposed integrates geometric operator preprocessing into deep networks for global parameter learning without preset parameters. The study used the GO-CNN model with trainable Schmid kernels for intelligent fracture diagnosis. Data from X-rays at Hainan People's Hospital was used to train three models, with 4,016 fracture patches created for training balance. Five experiments were conducted using SGD algorithm and finetune strategy with a batch size of 50. The study utilized the GO-CNN model for fracture diagnosis, training with data from Hainan People's Hospital. The algorithm's performance was evaluated after 12,000 iterations, showing the GO-CNN as the most accurate. The two-stage method had higher fracture recall than the CNN, emphasizing the importance of medical domain knowledge. Additionally, the GO-CNN outperformed the two-stage method in fracture recall, indicating its utilization of medical knowledge. The Geometric Operator Convolutional Neural Network utilizes medical knowledge for fracture diagnosis, achieving global optimization in deep neural networks."
}