{
    "title": "SJl47yBYPS",
    "content": "The field of Deep Reinforcement Learning (DRL) has seen a rise in popularity of maximum entropy reinforcement learning algorithms due to their superior sample efficiency. This paper aims to understand the contribution of the entropy term in Soft Actor Critic (SAC) algorithms, showing that it addresses the bounded nature of action spaces. A proposed normalization scheme allows for streamlined algorithms to match SAC performance. Experimental results suggest a need to reconsider the benefits of entropy regularization in DRL. Additionally, a non-uniform sampling method for selecting transitions from the replay buffer is introduced. Off-policy Deep Reinforcement Learning algorithms like TD3 aim to improve sample efficiency by reusing past experience. The algorithm introduces clipped double-Q learning, delayed policy updates, and target policy smoothing, outperforming SAC and achieving state-of-the-art performance on continuous control tasks. Recently, maximum entropy RL algorithms, such as Soft Actor Critic (SAC), have gained popularity for their superior sample efficiency and performance on Mujoco environments. SAC combines off-policy learning with maximum entropy RL, showing better results than DDPG and TD3 on high-dimensional tasks like Humanoid. This paper aims to understand the impact of the entropy term on maximum entropy algorithms' performance. The entropy term in Soft Actor Critic (SAC) addresses the bounded nature of action spaces in Mujoco environments, ensuring exploration even with squashing. A new algorithm, Streamlined Off Policy (SOP), uses a standard objective without entropy but includes a normalization scheme to maintain exploration. The paper introduces Streamlined Off Policy (SOP) as an alternative to maximum entropy RL algorithms like SAC for Mujoco benchmarks. SOP does not use entropy maximization but achieves similar sampling efficiency and robustness. Additionally, SOP combined with non-uniform sampling achieves state-of-the-art performance. Anonymized code for reproducibility is provided. The environment is modeled as a Markov Decision Process (MDP) with continuous state and action spaces, a reward function, transition function, and discount factor. The goal is to maximize the expected discounted return for a policy \u03c0. Optimal policies exist for both finite and continuous spaces. In Reinforcement Learning (RL) with unknown environments, the focus is on finding the optimal policy. In RL with unknown environments, exploration is necessary to learn a suitable policy. In DRL with continuous action spaces, the policy is modeled by a parameterized policy network. Additive random noise is used for exploration during training, while testing involves setting the noise to zero. Maximum entropy reinforcement learning optimizes policies to maximize expected return and entropy. In maximum entropy RL, the objective is to maximize the entropy of the policy to encourage exploration while balancing with the reward. The policy network outputs a vector for actions and a vector for exploration, allowing for a wider exploration of the environment. The paper discusses the use of standard additive noise exploration in Mujoco benchmarks to achieve performance comparable to maximum entropy RL. It also addresses the squashing exploration problem in DRL algorithms, where actions need to be selected within certain bounds before being taken. The paper discusses the challenges of squashing actions in Deep Reinforcement Learning (DRL) algorithms, highlighting the potential negative impact on additive-noise exploration strategies. The paper discusses the squashing exploration problem in DRL algorithms, emphasizing its negative impact on exploration strategies. Algorithms like DDPG and TD3 may be impaired by squashing exploration, while SAC is a maximum-entropy based off-policy DRL algorithm that shows good performance in Mujuco benchmark environments. The SAC algorithm is highlighted for its state-of-the-art performance in Mujoco benchmark environments. The entropy term in SAC helps address the squashing exploration problem, ensuring sufficient exploration in bounded action spaces. A comparison is made between SAC with adaptive temperature and SAC without entropy on Humanoid-v2 and Walker-v2 environments, showing SAC outperforming SAC without entropy in the Humanoid environment. SAC outperforms SAC without entropy in the Humanoid environment, but for Walker, SAC without entropy performs nearly as well as SAC. The importance of entropy maximization varies between environments, with Humanoid having 17 action dimensions and Walker having 6. Representative results for one dimension in both environments are shown in Figure 2. In the Humanoid environment, SAC with entropy shows small |\u00b5 k | values fluctuating significantly, allowing for exploration in action space. In contrast, SAC without entropy has huge |\u00b5 k | values leading to actions being clustered at either M or -M, resulting in no exploration. For Walker, both algorithms show sensible \u00b5 k values in the range [-1,1], leading to appropriate actions. The benefit of maximum entropy RL in SAC for Mujuco environments is resolving the exploration problem. In some environments like Walker, policy network outputs are sensible, maintaining exploration and good performance without entropy maximization. However, in environments like Humanoid, entropy maximization is necessary to reduce output magnitudes for exploration and good performance. The question arises if a streamlined off-policy algorithm can match SAC's performance without entropy maximization. Without entropy maximization, policy network output values can become persistently huge, leading to insufficient exploration. A simple solution is to normalize the output values when they collectively become too large. The Streamlined Off Policy (SOP) algorithm is essentially DDPG with a simple normalization scheme applied to the policy network outputs. The Streamlined Off Policy (SOP) algorithm is a modification of DDPG and TD3, incorporating normalization, clipped double Q-learning, and target policy smoothing. It uses tanh squashing instead of clipping and eliminates entropy terms, temperature adaptation, and delayed policy updates. Additionally, it explores variations with different normalization schemes, such as inverting gradients. The SOP algorithm modifies DDPG and TD3 by incorporating normalization, clipped double Q-learning, and target policy smoothing. It uses tanh squashing, eliminates entropy terms, temperature adaptation, and delayed policy updates. Gradients suggest increasing action magnitudes, downscaled within boundaries, and inverted outside boundaries. More implementation details can be found in the Appendix. The SOP algorithm modifies DDPG and TD3 by incorporating normalization, clipped double Q-learning, and target policy smoothing. Results show that SOP, SAC, and IG have similar sample-efficiency performance and robustness across all environments. TD3+ has slightly weaker asymptotic performance for Walker and Humanoid, while IG initially learns slowly for Humanoid but gives similar asymptotic performance. This confirms that with a simple output normalization scheme in the policy network, the performance of SAC can be achieved without maximum entropy RL. In the Appendix, an ablation study for SOP shows a significant performance drop without double Q-learning or normalization. Removing target policy smoothing results in a minor drop in some environments. A new sampling scheme called Emphasizing Recent Experience (ERE) achieves state-of-the-art performance for the Mujoco benchmark. ERE is a general method applicable to any off-policy algorithm, simple to implement, and introduces only one additional hyper-parameter. During the parameter update phase, mini-batches are gradually sampled more aggressively from recent data. During the parameter update phase, mini-batches are sampled more aggressively from recent data based on a hyper-parameter \u03b7 \u2208 (0, 1]. This sampling scheme emphasizes recent data, gradually reducing the range of sampling to prioritize newer information and avoid over-writing parameter changes. The sampling scheme in parameter updates emphasizes recent data to avoid over-writing changes made by older data. By using this scheme, new information can be quickly obtained from recent data while maintaining acceptable approximations for states visited in the past. When replacing uniform sampling with ERE, the expected number of times a data point is sampled remains the same for all data points in a fixed buffer. When using the ERE sampling scheme in parameter updates, older data points are sampled more frequently than newer ones. This can hinder learning as newer data may contain more valuable information. ERE adjusts the sampling curve to prioritize newer data, accelerating the learning process by allowing them to be sampled soon after collection. Further algorithmic details and comparisons are provided in the Appendix. ERE is compared to exponential sampling and Prioritized Experience Replay. SOP+ERE shows significant performance improvement in all environments, especially in Ant and Humanoid. SOP+ERE achieves state-of-the-art performance, with improvements of 21% and 24% over SAC in Ant and Humanoid, respectively. In recent years, there has been significant progress in improving the sample efficiency of DRL for continuous robotic locomotion tasks with off-policy algorithms. There is also a significant body of research on maximum entropy RL methods and prioritized experience replay (PER) as an alternative to uniform sampling from a replay buffer. PER uses the absolute TD-error of a data point as the measure for priority, with higher priority data points having a higher chance of being sampled. The prioritized experience replay method assigns higher priority to certain data points, improving algorithms like DQN and DDQN. Other methods like ACER and RACER also optimize replay buffer usage for algorithms such as DDPG, NAF, and PPO. Different buffer sizes were tested in De Bruin et al. (2015). In De Bruin et al. (2015), large replay buffers with diverse data improved performance. Hindsight Experience Replay (Andrychowicz et al., 2017) prioritizes trajectories with lower density estimation for multi-goal, sparse reward environments. A new algorithm without entropy maximization matches SAC's performance for Mujoco benchmarks. Results suggest a need to reconsider entropy regularization in DRL. Combining the streamlined algorithm with non-uniform sampling achieves state-of-the-art Mujoco benchmark performance. In an ablation study, the importance of output normalization, double Q networks, and randomization in the SOP algorithm for Mujoco benchmark performance is examined. Results show that double Q-networks and output normalization are critical for achieving good performance. Output normalization and double Q networks are critical for achieving good performance in Mujoco benchmarks. Target policy smoothing slightly improves performance. Hyperparameters for SOP, SOP+ERE, and SOP+PER are shown in Table 2. Different hyperparameters are used for adaptive SAC and TD3 implementations. Hyperparameter search is discussed for clarity, fairness, and reproducibility. In early experiments, values for \u03b7 in the ERE scheme were tested on the Ant environment, with 0.995 found to work well. For the PER scheme, a search was conducted for \u03b2 1, \u03b2 2, and learning rate values on Ant. The exponential sampling scheme involved searching for \u03bb values in Ant based on plotted probabilities of sampling. In early experiments, values for \u03b7 in the ERE scheme were tested on the Ant environment, with 0.995 found to work well. For the PER scheme, a search was conducted for \u03b2 1, \u03b2 2, and learning rate values on Ant. The exponential sampling scheme involved searching for \u03bb values in Ant based on plotted probabilities of sampling. In some early experiments with SAC, \u03c3 = 0.3 was found to give good performance without entropy and with Gaussian noise. Values were searched for HalfCheetah-v2, TD3, and TD3+ algorithms for data collection and target policy smoothing. In the Inverting Gradient method, the tanh function is removed from the SOP and replaced with Inverting Gradients to bound the actions. This method has been found to achieve stronger performance compared to Zeroing Gradients and Squashing Gradients. The output of the policy network is denoted as p in this implementation. The Inverting Gradient method replaces the tanh function with Inverting Gradients to bound actions. During exploration, p is the mean of a normal distribution for sampling actions. The IG approach involves backpropagating gradients from the Q network to the policy network, computing ratios for each action dimension, and updating the policy network parameters accordingly. Efficient implementation and exploration of sampling schemes are discussed. The text discusses the implementation of the proportional variant of Prioritized Experience Replay with SOP, focusing on redefining the absolute TD error and calculating priority values for sampling data points. It also explains the computation of sampling probabilities and importance sampling weights based on the SOP algorithm. The text introduces the SOP algorithm with Prioritized Experience Replay (SOP+PER), comparing it to ERE which does not require a special data structure. Various variants of SOP+PER were tested, but results were inconclusive, leading to a decision to keep the algorithm simple. ERE implements an exponential sampling scheme where recent data points have higher sampling probabilities. The SOP algorithm with Prioritized Experience Replay (SOP+PER) was compared to ERE, which uses an exponential sampling scheme for data points. Results showed that SOP+EXP improved performance, especially in the Humanoid environment, but SOP+PER did not provide a significant boost. It was challenging to find hyperparameter settings that worked well for all environments with SOP+PER. Further research is needed to understand why PER does not work well with SOP. Further research is needed to adapt Prioritized Experience Replay (PER) for environments with continuous action spaces and dense reward structures. The data sampling process in PER is influenced by the hyperparameter \u03b7, with recent data points having a higher probability of being sampled. Different \u03b7 values are needed based on the agent's learning speed and the obsolescence of past experiences. Adapting \u03b7 to the learning speed is crucial for PER to work effectively in various environments with different reward scales and learning progress. Performance is defined as the training episode return, with \u03b7 0 being the initial hyperparameter value. In Prioritized Experience Replay (PER), the hyperparameter \u03b7 is adapted based on the agent's learning speed. The adaptive scheme adjusts \u03b7 to either facilitate quick learning from new data or utilize the stabilizing effect of uniform sampling from the buffer. Programming details in the ERE scheme involve sampling from the entire buffer and gradually shrinking the sampling range, even when the buffer is not full. This ensures consistency in computing based on a buffer size of 1M data points. In Prioritized Experience Replay (PER), the hyperparameter \u03b7 is adapted based on the agent's learning speed. The adaptive scheme adjusts \u03b7 to either facilitate quick learning from new data or utilize the stabilizing effect of uniform sampling from the buffer. The design can be modified to compute c k based on the current amount of data points in the buffer. The sampling range can be shrunk linearly, but it gives less performance gain. The number of updates after an episode is set to be the same as the number of timesteps in that episode. A formula for c k is provided based on the number of mini-batch updates and the max size of the replay buffer. Uniform sampling is done in the first update, and c k can become small for some mini-batches when \u03b7 is small. In Prioritized Experience Replay (PER), the hyperparameter \u03b7 is adapted based on the agent's learning speed. The adaptive scheme adjusts \u03b7 to facilitate quick learning or stabilize sampling. The recent performance improvement is computed by comparing current and past episode returns. ERE has lower programming complexity as it requires minimal adjustments to mini-batch sampling. In contrast to ERE, PER requires a sum-tree data structure for efficient operation. The exponential sampling scheme is easy to implement but may have high computation overhead. To enhance efficiency, an approximate sampling method is used by segmenting the replay buffer and sampling uniformly from each segment. ERE's additional computation is minimal in terms of complexity and wall-clock time. In practice, there is no significant difference in computation time between SOP and SOP+ERE. However, PER incurs a notable overhead on SOP, running twice as long. The overhead grows linearly with the mini-batch size, especially in Mujoco environments due to their smaller state space dimension. The exponential sampling scheme incurs minimal extra computation when using an approximate sampling method. The proposed normalization scheme and Inverting Gradients (IG) method are simple to implement with negligible computation overhead. IG implementation is slightly more complex but can be optimized to avoid large computation overhead. Efficient implementation code is publicly available for reproduction. Results show a performance boost for SOP and IG with the ERE scheme. Comparing TD3 with TD3+ normalization scheme, results indicate improvement with TD3+. After comparing TD3 with TD3 plus their normalization scheme (referred to as TD3+), significant performance improvement is seen in the Humanoid environment. The actions selected during training SAC with and without entropy are examined in Humanoid and Walker2d environments, with action dimensions of K = 17 and K = 6, respectively. Results show that entropy maximization is crucial for Humanoid but less so for Walker2d, as seen in the fluctuation of action values. The |\u00b5 k | values are significant for both algorithms in all dimensions. Actions are clustered at either M or -M. Walker has sensible |\u00b5 k | values for all dimensions."
}