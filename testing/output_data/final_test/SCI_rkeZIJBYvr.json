{
    "title": "rkeZIJBYvr",
    "content": "The existing meta-learning approaches for few-shot classification assume a fixed number of instances and classes per task, limiting their ability to adapt to varying scenarios. To address this, a novel meta-learning model is proposed to balance meta-knowledge and task-specific learning within each task. This model uses Bayesian inference and variational inference to determine the optimal approach for each task. Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) is validated on task- and class-imbalanced datasets, outperforming existing meta-learning approaches. Researchers have explored meta-learning to generalize well over tasks with small data regimes. Meta-learning approaches aim to utilize meta-knowledge across tasks to compensate for the lack of training data. Existing methods have focused on scenarios with equal training instances per class, which is restrictive. In real-world scenarios, tasks may have different training instances and class imbalances, as well as out-of-distribution tasks. This realistic setting challenges the model's ability to adapt to new tasks effectively. In real-world scenarios, tasks may have varying training instances and class imbalances, as well as out-of-distribution tasks. To optimally leverage meta-learning under these imbalances, the model should adaptively decide how much meta-knowledge to use for each task and class. In real-world scenarios, tasks may have varying training instances and class imbalances, as well as out-of-distribution tasks. To optimally leverage meta-learning under these imbalances, a novel Bayesian Task-Adaptive Meta-Learning (Bayesian TAML) framework is introduced. This framework learns task-specific balancing variables to adaptively balance between meta-knowledge and task-specific updates, addressing imbalances and distributional discrepancies. The framework utilizes set-representations for each task to convey useful statistics about the task or class distribution, and learns the distribution of three balancing variables to decide how far to deviate from the meta-knowledge during task-specific learning. The Bayesian TAML framework introduces three balancing variables: 1) distance from meta-knowledge for tasks with varying shots, 2) class-dependent learning rate for handling class imbalance, and 3) task-dependent attention mask for modifying shared parameters based on task needs, especially useful for out-of-distribution tasks. Validated on Omniglot, mini-ImageNet, and heterogeneous datasets. Our Bayesian TAML framework addresses task and class imbalance in meta-learning with heterogeneous datasets, improving performance significantly. It introduces balancing terms for handling imbalances and out-of-distribution tasks. Our contribution includes considering a novel problem of meta-learning under realistic task distributions and proposing Bayesian TAML for effective meta-learning with such imbalances. Our Bayesian TAML framework adapts the meta-learner and task-specific learner differently for each task and class, significantly outperforming existing models in imbalanced few-shot classification tasks. Meta-learning involves memory-based, metric-based, and optimization-based methods to generalize over task distributions. Meta-learning approaches such as MAML and Meta-SGD optimize shared parameters for fast adaptation to new tasks. Episodic training is used to train models over multiple tasks. Existing methods focus on scenarios with balanced class distributions, while our approach considers imbalanced few-shot classification tasks, adapting the meta-learner and task-specific learner differently for each task and class. Task-adaptive meta-learning aims to improve performance by modifying meta-learning models for specific tasks. Different approaches have been proposed, such as learning temperature scaling parameters, generating task-specific network parameters, and using task-specific parameter producers. These methods aim to enhance performance in scenarios with varying shots per class and out-of-distribution tasks. Our model focuses on balancing between meta-learning and task-specific learning, unlike existing approaches that only consider few-shot learning with a fixed number of instances per class. Recent work includes a probabilistic version of MAML and Bayesian MAML with variational inference frameworks. The model proposed by et al. (2018) utilizes Bayesian modeling to balance between meta-learning and task-specific learning. It aims to estimate parameters for novel tasks rapidly under a decision theoretic framework, considering uncertainties in few-shot classification tasks. The approach leverages uncertainties of the meta-learner and gradient-direction to achieve this balance. The model is based on the model-agnostic meta-learning (MAML) by Finn et al. (2017), where the goal is to meta-learn initial model parameters to generalize over task distributions efficiently. The MAML framework aims to optimize the gradient-based meta-learning objective by adapting the initial point \u03b8 for task-specific predictors. However, it has limitations in handling task/class imbalance and out-of-distribution tasks, such as fixed inner-gradient steps and stepsize \u03b1 across all tasks. The MAML framework has limitations in handling class imbalance and out-of-distribution tasks. To address this, three balancing variables are introduced to tackle these issues. To address limitations in handling class imbalance and out-of-distribution tasks within the MAML framework, a method is proposed that involves simulating task and class imbalance simultaneously. This is achieved by sampling a uniform-random number of instances for each class and introducing a clipping function and task-dependent learning-rate decaying factor. Additionally, varying the learning rate of class-specific gradient updates for each task-specific gradient update step is used to handle class imbalance. The proposed method addresses class imbalance and out-of-distribution tasks in the MAML framework by simulating task and class imbalance simultaneously. This involves sampling instances for each class, introducing a clipping function, and using a task-dependent learning-rate decaying factor. Additionally, varying the learning rate of class-specific gradient updates for each task-specific update step is utilized to handle class imbalance. The framework includes an additional task-dependent variable that weights the initial parameter according to task usefulness, with an emphasis on meta-knowledge when the dataset is similar and less emphasis when unfamiliar. This behavior is implemented using Bayesian modeling on the latent variable, leading to a unified framework with a recursive update rule for task-specific predictors. The proposed method utilizes a multi-dimensional global learning rate vector and Bayesian modeling for three latent variables to improve inference quality. The variables share the same inference network pipeline to minimize computational cost. The generative process for each task involves a complete data likelihood with a shared deterministic parameter across all tasks. The proposed method uses Bayesian modeling and a shared deterministic parameter for tasks to maximize log-likelihood. Amortized variational inference is employed with a tractable approximate posterior, simplifying the inference framework. The objective remains a valid lower bound of the log evidence. The proposed method utilizes Bayesian modeling and a shared deterministic parameter for tasks to maximize log-likelihood. Amortized variational inference simplifies the inference framework, resulting in a valid lower bound of the log evidence. The modified objective focuses on the test examples, with a low-variance estimator for the lower bound in meta-learning. The final form of the meta-training minimization objective involves Monte-Carlo approximation. The variational distribution q(\u03c6 \u03c4 |D \u03c4 ; \u03c8) aims to capture statistical information in the dataset D \u03c4 for solving imblanace and out-of-distribution problems. DeepSets is commonly used as a set-encoder to transform each instance in the set. DeepSets is used as a set-encoder to transform instances in a set, but for classification datasets, DeepSets needs to be structured hierarchically. However, there are limitations with sum-pooling in DeepSets, especially when describing distributions. Mean-pooling can help, but it doesn't recognize the number of elements in a set, which is crucial for encoding imbalance. To address limitations in encoding imbalance, higher-order statistics like sample variance, skewness, and kurtosis are proposed in addition to the sample mean. A new encoder network, StatisticsPooling(\u00b7), combines these statistics for class imbalance and task imbalance. The method is validated in imbalanced scenarios. In imbalanced scenarios, tasks can have different shots and distributions. 4-block CNNs with 64 channels are used for Omniglot and MNIST, reduced to 32 channels for other datasets. Imbalanced Omniglot has 1623 classes with 20 training instances each. Tasks are generated with 10-way classification and 5 queries per class, with varying training instances sampled. Models are trained on Omniglot and evaluated on both Omniglot and MNIST. The study evaluates models on Imbalanced tiered-ImageNet and MNIST datasets for out-of-distribution tasks. Tasks involve 5-way classification with varying training instances per class. Models are trained on tiered-ImageNet and evaluated on both tiered-ImageNet and mini-ImageNet. Additionally, the model is tested on a heterogeneous dataset combining Omniglot and VGG flower. In a challenging multi-dataset setting, a Bayesian TAML model outperforms all baselines, especially on out-of-distribution tasks like Fashion MNIST. The effectiveness of each balancing variable is validated through ablations, with meta-training conditions set to Omniglot 5-way any-shot classification. By adding a Bayesian component to Meta-SGD, out-of-distribution tasks like MNIST can be effectively handled. The distribution of the mask for OOD tasks is skewed towards zero, as shown in Figure 4. The effectiveness of each balancing variable is validated through ablations in a challenging multi-dataset setting. The effectiveness of balancing variables f(\u03b3 \u03c4) and g(\u03c9 \u03c4) in handling task and class imbalance is demonstrated through ablations in a multi-dataset setting. f(\u03b3 \u03c4) adjusts for inter-task imbalance, while g(\u03c9 \u03c4) rescales class-specific gradients for class imbalance. Results show larger gains for 1-shot tasks, indicating the usefulness of meta-knowledge in improving performance on smaller tasks. The model significantly outperforms baselines, especially under high class imbalance. Bayesian modeling is shown to be effective in addressing the imbalance problem, particularly for out-of-distribution tasks like MNIST. Our proposed Bayesian TAML effectively addresses class/task imbalance, reacting more sensitively with Bayesian modeling. An ablation study validates the Set of Sets dataset encoding scheme as highly effective for generating balancing variables, outperforming other encoding methods. Bayesian TAML effectively balances meta-learning and task-adaptive learning by encoding datasets into hierarchical set-of-sets representations. A Bayesian framework infers balancing variables for parameter attention masks, learning rate decay, and class-specific learning rates. The model outperforms existing methods on imbalanced few-shot classification tasks, handling task and class imbalances effectively. This work advances the application of meta-learning to real-world problems. Our work advances the application of meta-learning to real-world problems by describing baseline models such as Meta-Learner LSTM, Prototypical Networks, and MAML. These models aim to perform few-shot classification tasks using different approaches. The curr_chunk discusses different variations of the MAML model, including a base MAML with a learnable learning-rate vector, a model proposed by Lee & Choi (2018) with task-specific parameters, a probabilistic version by Finn et al. (2018) modeling task-adaptive inner-gradient steps, and a Bayesian TAML model for adaptive balancing. The curr_chunk describes the settings for realistic any-shot classification, including modifications to the episode generating strategy for imbalanced Omniglot data. The metalearning rate, total iterations, and inner gradient step size are specified for different models, with the number of inner-gradient steps set to 5. The meta-batch size is kept at 1 for all experiments to observe the impact of imbalance scenarios. We trained models on tiered-ImageNet with modified episode generation for C-way classification. The network architecture includes an inference network generating balancing variables. Average pooling is used before the shared encoder for large inputs like tiered-ImageNet and mini-ImageNet. The network architecture includes an inference network generating balancing variables. Attaching average pooling before the shared encoder reduces computation cost and improves performance. The Set-ofSets structure proposed in the main paper is justified based on DeepSets' permutation invariance properties. The main theorem states that a function operating on a set is valid if it can be decomposed appropriately. The main theorem states that a function operating on a set is valid if it can be decomposed appropriately into a composite function operating on a set of sets, with specific nonlinearities. The comparison between two approximation schemes for evaluating test example predictions at meta-testing time shows that MonteCarlo integration performs better than naive approximation, especially with out-of-distribution tasks like MNIST and mini-ImageNet. The predictive distributions involve higher uncertainty for OOD tasks, making it beneficial to consider the large variance. In the study, the model's performance was evaluated on tasks like Omniglot and Mini-ImageNet using a 4-block CNN architecture. The number of inner-gradient steps was set to 5 for both meta-training and meta-testing. Results were compared with existing meta-learning approaches, showing competitive performance. Bayesian TAML outperforms most baseline models in conventional fixed-way fixed-shot classification tasks, except for the mini-imagenet 5-way 1-shot experiment where BMAML performs the best. The model is compared with applying linear interpolation between initial parameters \u03b8 and random initialization. At meta-training time, a single sample MC approximation is used for computational efficiency. The model's performance is 68.20 \u00b1 0.66% in the \"higher way\" setting of 20-way training for 5-way testing."
}