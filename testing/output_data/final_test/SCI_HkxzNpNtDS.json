{
    "title": "HkxzNpNtDS",
    "content": "Recent research focuses on natural language grounded navigation in photo-realistic environments, aiming to bridge the gap between seen and unseen environments. This is achieved through a multitask navigation model trained on Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, as well as environment-agnostic representations for navigation policy to generalize better on unseen environments. Our environment-agnostic multitask navigation model bridges the performance gap between seen and unseen environments, outperforming baselines on unseen environments by 16% on VLN and 120% on NDH. This establishes a new state of the art for the NDH task, focusing on natural language grounded navigation in photorealistic environments. The challenge in indoor navigation tasks lies in the agent's poor performance in unknown environments due to data scarcity and the high cost of collecting new data. Unlike vision-only navigation tasks, natural language grounded navigation relies on human interaction and communication, making it difficult to generalize policies efficiently. In this paper, the focus is on developing a generalized multitask model for natural language grounded navigation tasks like Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH). The goal is to address issues of generalization and data scarcity by efficiently transferring knowledge across tasks and solving multiple tasks with one agent simultaneously. The current models face limitations due to restricted house scans in datasets like Matterport3D, hindering their performance in unknown environments. The study proposes an environment-agnostic learning method to improve generalization in navigation tasks. By learning visual representations that are invariant to specific environments, the agent can better navigate unseen environments. This approach introduces multitask and environment-agnostic training regimes, showing effectiveness in VLN and NDH tasks. The multitask navigation model excels in executing language guidance in indoor environments, outperforming single-task models by a large margin. It also reduces the performance gap between seen and unseen environments and achieves a new state of the art in NDH with over 120% improvement in goal progress. Vision-and-Language Navigation task involves an agent navigating in photo-realistic environments based on natural language instructions. This work focuses on Vision-and-Language Navigation (VLN) tasks, specifically Room-to-Room (R2R) navigation, using multitask learning and environment-agnostic approaches. It differs from previous methods by incorporating dialog history for navigation guidance in indoor environments. The Cooperative Vision-and-Dialog Navigation (CVDN) dataset involves interactive language assistance for indoor navigation in photo-realistic home environments. The Navigation from Dialog History (NDH) task requires an embodied agent to infer navigation actions towards a goal room containing a target object based on a dialog history between humans cooperating in the task. Multitask learning involves training multiple tasks jointly to prefer hypotheses that explain all tasks simultaneously, leading to more generalized solutions. It has been successful in various fields such as natural language processing, speech recognition, computer vision, drug discovery, and Atari games. Deep reinforcement learning methods used for natural language grounded navigation tasks are known to be data inefficient. This work introduces multitask reinforcement learning. In this work, multitask reinforcement learning is introduced to improve data efficiency by positive transfer across related tasks. Environment-agnostic techniques, such as Model-Agnostic Meta-Learning and domain-agnostic methods, are utilized to learn representations that can be generalized on unseen environments. The distributed actor-learner navigation learning framework is designed to train models for language grounded navigation tasks like VLN and NDH. It utilizes off-policy correction methods to efficiently scale reinforcement learning methods to thousands of machines and supports various supervision strategies. The framework is built using TensorFlow and supports ML accelerators. Our environment-agnostic multitask navigation model adapts the reinforced cross-modal matching (RCM) model for tasks like NDH and VLN. It shares learnable parameters and uses a gradient reversal layer to learn environment-agnostic representations. An environment classifier predicts the agent's location during training. The environment-agnostic multitask navigation model uses an environment classifier to predict the agent's location during training. It employs interleaved multitask data sampling and reward shaping to optimize different modules simultaneously. The discounted cumulative reward function R is implemented for VLN and NDH tasks, with behavior cloning used to model relevant state-action spaces. Reinforcement learning aids the agent in recovering from errors in unseen scenarios. During multitask navigation model training, a mixed strategy of reinforcement learning and behavior cloning is used to update the policy \u03c0. To improve generalizability, a latent environment-agnostic representation is learned to remove irrelevant environment-specific features. This helps prevent overfitting to specific environments and aids in modeling the navigation policy. The policy module models p(a t |z t , s t ) and p(z t |s t ), while an environment classifier and gradient reversal layer are used to learn an environment-agnostic representation. The environment classifier predicts the house identity, and the gradient reversal layer aids in adversarial learning by maximizing the environment classification loss. The text discusses increasing the entropy of the classifier in an adversarial learning manner by minimizing classification loss conditioned on the latent representation. Natural language guidance is tokenized and embedded into n-dimensional space, with vocabulary restrictions and out-of-vocabulary token mapping. Bi-directional LSTM is used for token sequence encoding. The text discusses the agent perceiving a 360-degree view at each time step, encoded using LSTM to create an attention-pooled representation. The policy module includes a cross-modal attention unit and an action predictor. The policy module includes a cross-modal attention unit and an action predictor. It learns a policy that maps natural language instructions and initial visual scenes to a sequence of actions. The action space consists of navigable directions from the current location, with the model predicting the probability of each direction using a bilinear dot product. The classifier generates a probability distribution over house labels using a 2-layer bi-directional LSTM for the instruction encoder and a 2-layer LSTM for the visual encoder. The environment classifier has one hidden layer of size 128 units and an output layer with the number of classes. During training, some episodes aim to increase the agent's likelihood of choosing human actions. The agent's likelihood of choosing human actions is increased during training episodes. Evaluation metrics include Path Length (PL) and Navigation Error (NE) on Validation Seen and Validation Unseen datasets for the VLN task. The evaluation metrics for the navigation model include Error (NE), Success Rate (SR), Success weighted by Path Length (SPL), and Coverage weighted by Length Score (CLS). The agent's progress in the NDH task is measured by the reduction in distance to the goal region. Training the navigation model using environment-agnostic learning (EnvAg) and multitask learning (MT-RCM) shows a significant reduction in the agent's performance gap. The evaluation metrics for the navigation model include Error (NE), Success Rate (SR), Success weighted by Path Length (SPL), and Coverage weighted by Length Score (CLS). Training the navigation model using multitask learning and environment-agnostic learning results in a significant reduction in the agent's performance gap between seen and unseen environments. The techniques are complementary, with improved performance on unseen environments when trained together. MT-RCM + EnvAg outperforms the state-of-the-art on NDH validation unseen dataset by more than 120% and surpasses the RCM baseline on VLN validation unseen dataset by more than 16%. Multitask learning boosts under-represented tokens by increasing training samples. Dialog history impact on NDH task is studied using different parts of the dialog. Results show MT-RCM model outperforms competitors on VLN and NDH tasks in unseen environments. Multitask learning reduces performance gap between agent's seen and unseen environments for VLN and NDH tasks. Success rate of MT-RCM on VLN task increases with richer dialog history. Shared language encoder for both tasks outperforms separate encoders, emphasizing parameter sharing importance. Both VLN and NDH tasks benefit from environment-agnostic learning. Environment-agnostic learning benefits VLN and NDH tasks, leading to more generalizable policies compared to environment-aware learning. Training with the opposite objective of predicting navigation environments results in overfitting on the training dataset. The model learns to represent visual inputs from the same environment closer together, leading to a clustering effect. Incentivizing the agent to get closer to the goal room is more effective than the exact goal location for the NDH task. Ablation studies consistently support this approach across different parts of the dialog history. The model trained using multitask learning approach learns a generalized policy for natural language grounded navigation tasks, closing the gap between seen and unseen environments. It effectively transfers knowledge across tasks and outperforms baselines by a significant margin. Future extensions include adapting the approach to other language-grounded navigation datasets. The results show that agents rewarded for proximity to the goal room outperform those rewarded for exact goal location. Models with shared language encoder perform better than those with separate encoders. Multitask learning and environment-agnostic methods reduce the performance gap between seen and unseen environments."
}