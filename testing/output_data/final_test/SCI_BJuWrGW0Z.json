{
    "title": "BJuWrGW0Z",
    "content": "Neural program embeddings are commonly used for program analysis tasks like synthesis, repair, code completion, and fault localization. Existing embeddings are limited by focusing on syntax, while our proposed semantic embedding learns from program execution traces to capture program semantics more accurately. This approach utilizes sequential tuples of live variable values, making it a better fit for Recurrent Neural Networks. Our evaluation of different syntactic and semantic program embeddings shows that semantic embeddings outperform syntactic embeddings in classifying errors in student programming submissions. Additionally, we enhance a search-based program repair system with predictions from semantic embeddings, leading to significantly improved search efficiency. Recent breakthroughs in deep learning techniques have sparked interest in applying them to programming languages and software engineering. Key areas include program classification, similarity detection, program repair, and program synthesis. Neural networks are used with syntax-based program representations, such as convolutional neural networks over abstract syntax trees (ASTs) for classifying programs based on functionalities. Techniques like DeepFix, SynFix, and sk p focus on neural program repair using sequences of tokens for program representation. Program synthesis techniques in MOOC assignments typically represent programs as sequences of tokens. While some methods generate programs using input-output pairs, these representations may not accurately capture program properties. Syntax-based program representations have limitations due to the gap between program syntax and Bubble Insertion. Due to the significant gap between program syntax and Bubble Insertion, their execution traces for the input vector A = [8, 5, 1, 4, 3] are displayed, highlighting the syntactic differences between the two algorithms. This gap illustrates the difference between static expression and dynamic execution in program statements at runtime. At runtime, program statements are not interpreted in the order of token sequences due to control-flow structures. Program dependency is crucial for defining semantics but not utilized in token sequences or ASTs. An example with a max function illustrates data dependency and control flow in program execution. The execution of the statement depends on the evaluation of the if condition, where the max value is control-dependent on the item itself. Minor syntactic discrepancies can lead to significant semantic differences in program semantics. This weakness affects deep learning techniques using syntax-based program representation. Dynamic program embeddings were evaluated in automated program repair. Our dynamic program embeddings outperform syntax-based embeddings in classifying student programming mistakes and improving program correction efficiency. These embeddings can also benefit program analysis tasks like synthesis, fault localization, and similarity detection. The paper introduces dynamic program embeddings learned from runtime execution traces to address limitations of syntax-based program representations. These embeddings outperform syntactic embeddings in predicting student programming mistakes and can enhance program repair systems. The approach is based on dynamic program analysis techniques and offers improvements in various program analysis tasks. The paper introduces dynamic program embeddings learned from runtime execution traces to improve program representations. Dynamic analysis focuses on program executions modeled by atomic actions in a trace. Instrumenting a program's source code records the execution of statements, allowing for monitoring of variable values. Dynamic analysis involves automating the process by inserting \"write\" statements in a program's abstract syntax tree after statements that change variable values. By monitoring a specific variable, like A, in sorting algorithms, execution traces can be generated to identify program behavior. This method easily distinguishes between similar program syntaxes, such as bubble sort and insertion sort. Dynamic analysis uses execution traces to uncover distinct program semantics of sorting algorithms like bubble sort and insertion sort. This approach has been successful in program analysis, leading to tools like debuggers and profilers. Neural network models are employed to learn dynamic program embeddings from execution traces, which are then used to predict common error patterns in student submissions for an online programming course. State Trace Embedding is proposed to capture variable dependencies/interactions in program states. Each program point introduces a new state expressed by variable valuations. A single RNN encodes each program state to learn the state trace embedding. State Trace Embedding is utilized to capture dependencies among variables in program states. An RNN is used to encode each program state, and the resulting states are fed into another RNN. The order of variable values is consistent throughout all program states for a given trace. The final state of the second RNN is fed into a softmax regression layer. This method helps capture dependencies among variables in each program state and relationships among program states. However, it also faces challenges such as redundancy, especially in looping structures where new program states are created during iterations when variables are modified. To address challenges in capturing dependencies among variables in program states, a new approach called dependency enforcement embedding is proposed. This method combines variable trace embedding and state trace embedding by representing a program with separate variable traces handled by different RNNs. Hidden states from different RNNs are interleaved to simulate data and control dependencies, unlike variable trace embedding which uses average pooling on the final state. The proposed method, dependency enforcement embedding, combines variable trace embedding and state trace embedding by using separate RNNs for variable traces to capture dependencies in program states. The final program embedding is obtained through average pooling on the final states of all RNNs, and the program trace representation is computed by performing max pooling over the last hidden state of each variable trace embedding. The state trace model embeds program states as numerical vectors and uses RNNs to obtain the program embedding. Variable values are encoded as hidden states, and a sequence of all state embeddings is computed to generate the program embedding. The state trace model embeds program states as numerical vectors using RNNs. The model combines advantages of previous approaches by handling each variable trace with a different RNN to enforce dependency relationships. A potential issue addressed is variable matching/renaming to prevent memory issues and loss of precision. Our solution addresses the issue of variable matching/renaming to prevent memory issues and loss of precision by executing all programs to collect traces for variables, performing dynamic time wrapping on variable traces to identify top-n most used variables, and consistently renaming them across all programs. Dependency enforcement on the top variables is achieved by fusing hidden states of multiple RNNs based on variable dependencies. The RNN determines the previous state to produce the new max value, enforcing data and control dependencies in various programming statements. Equations (11 and 12) illustrate the workflow, with training done on programming submissions from Microsoft-DEV204.1X and CodeHunt platform. \"Introduction to C#\" course on edx and Microsoft CodeHunt platform. Three programming problems: Print Chessboard, Count Parentheses, Generate Binary Digits. Errors in student submissions categorized into low-level technical and high-level conceptual issues. Data used to train models for predicting error patterns by converting incorrect programs and mutating correct programs.\" Based on student submissions from the \"Introduction to C#\" course on edx and Microsoft CodeHunt platform, errors were categorized into low-level technical and high-level conceptual issues. To set up a dataset, incorrect programs were converted and correct programs were mutated to exhibit similar errors. Three syntax-based models were trained using RNNs on different program representations and compared with dynamic embeddings on an error prediction task. All models were implemented in TensorFlow with two stacked GRU layers in each encoder. The models in the trace model have two stacked GRU layers with 200 hidden units each, while the state encoder has one layer with 100 hidden units. Weight initialization is random, with a vocabulary of 5,568 unique tokens embedded into 100-dimensional vectors. Training uses the Adam optimizer with default values and a mini-batch size of 500. Padding is applied to ensure uniform length in trace and dependency models. During training, the dependency enforcement model faces optimization issues with complex dependencies, leading to gradient problems. To address this, traces are truncated into sub-sequences, with back-propagation only on the last sub-sequence. The baseline network and AST model both use a two-layer GRU with 200 hidden units and a 100-dimension embedding vector. The AST model learns embeddings for syntax nodes by propagating from leaf to root through production rules, using root embeddings to represent programs. The text discusses the comparison between dynamic program embeddings and syntax-based program embeddings in predicting common error patterns made by students. The embeddings trained on execution traces outperform those trained on program syntax, with over 92% accuracy compared to less than 27% for syntax-based embeddings. The difficulty for syntax models to generalize is attributed to minor syntactic discrepancies leading to major semantic differences, especially in programs with distinct labels that differ by only a few tokens or AST nodes. The size of the training dataset is relatively small for syntax-based models to learn precise patterns, while dynamic embeddings show better performance. Dynamic embeddings outperform syntax-based models in learning error patterns, as they can pinpoint semantic differences effectively even with smaller training data. These embeddings were incorporated into SARFGEN BID13, a program repair system, to prioritize corrections using a distribution learned from dynamic embeddings. The comparison between enumerative search and dynamic program embeddings in repairing incorrect programs shows that dynamic embeddings yield significant speedups, especially when multiple fixes are required. Dynamic program embeddings provide more than an order of magnitude speedups when there are four or more fixes needed. However, the performance gain decreases significantly when there are more than seven fixes due to poor prediction accuracy. The network views dynamic embeddings as capturing new execution traces, making predictions unreliable for programs with too many errors. Existing neural program repair techniques have not considered these dynamic embeddings proposed in this paper. Dynamic embeddings proposed in this paper can be a new feature dimension for existing neural program repair techniques. BID8 focuses on program representation using input-output pairs, but this approach may not be precise enough for different programs with the same input-output pairs. BID10 introduces using execution traces to induce algorithms from few examples, which differs from the approach discussed. The curr_chunk discusses the differences in approach from related works in synthesizing neural controllers for program execution and learning program representations from runtime execution traces. It also mentions related efforts in modeling semantics in sentence or symbolic expressions. The dynamic program embeddings outperform syntax-based embeddings in predicting error patterns in online programming submissions. They also lead to significant speedups in program repair compared to enumerative methods. These embeddings can be utilized for various program analysis tasks like program induction and synthesis."
}