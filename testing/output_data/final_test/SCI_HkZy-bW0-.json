{
    "title": "HkZy-bW0-",
    "content": "The text discusses how natural sensory data is temporally redundant, and how deep learning algorithms often do not take advantage of this redundancy, leading to wasted energy. A variant of backpropagation for neural networks is presented, where computation scales with the rate of change of the data. This is achieved through Predictive Coding and Sigma-Delta modulation, resulting in units resembling biologically-inspired neurons and a spike-timing-dependent weight-update similar to STDP. Our algorithm, based on Dependent Plasticity (STDP), performs comparably to standard deep networks on MNIST, temporal MNIST, and Youtube-BB datasets. Unlike traditional Machine Learning algorithms, our approach considers the temporally correlated nature of data, similar to how sensory data enters our brains. This temporal redundancy can be advantageous or disadvantageous in training models. Online Learning focuses on learning in domains where data is available in sequential order and given to the model only once. The temporal redundancy in data implies that not all computations are necessary, leading to a need for efficient learning systems. For example, CCTV feeds collect mostly-static data with low new information in each frame, highlighting the challenge of training models efficiently. The need for efficient learning systems is highlighted in scenarios like CCTV feeds, where data tends to be temporally redundant. This poses a challenge for training models, as the amount of computation scales with the number of frames rather than the new information in each frame. Robots, which learn online, face similar challenges with nonstationary temporal streams from multiple sensors running at different rates. The gyroscope can produce 1-byte readings at 1000 frames/s. Current deep learning methods struggle to integrate asynchronous sensory signals efficiently without recomputing the network function for each new signal. There is a need for a training method that scales computation with new information in the data, not just data dimensionality. While there is work on increasing neural network efficiency, little focus has been on reducing computation by exploiting data redundancies. BID12 aimed to exploit temporal redundancy in video by sending quantized activation changes between neurons. In order to efficiently approximate the function of the network, BID12 proposed a method of sending quantized activation changes between neurons and integrating these changes over time. However, this approach fails for training when weights are functions of time, leading to incorrect activation reconstruction for the next layer. To address this issue, a new encoding scheme was introduced that includes a proportional component and a derivative component of the layers' activation. This encoding scheme allows for a decoding scheme that involves taking an exponentially decaying temporal average of past inputs. In this work, a coding scheme is proposed where neurons represent activations as a temporally sparse series of impulses, encoding both value and rate of change. The scheme utilizes temporal redundancy in input data to reduce computation needed for training neural networks, demonstrated on MNIST and Youtube-BB datasets. This novel algorithm aims to use less computation as data becomes more temporally redundant, resembling biological spiking neurons. The algorithm efficiently approximates a function y t = f (x t ) using temporal redundancy between inputs x t\u22121 , x t to improve computation. Function composition is denoted as (f 3 \u2022 f 2 \u2022 f 1 )(x) = f 3 (f 2 (f 1 (x))), allowing functions to have internal states. Functions like \u2206 and \u03a3 are defined to calculate the difference and running sum of inputs, respectively. Equation 2 returns a running sum of inputs over calls. When a composition of functions (\u03a3 \u2022 \u2206) is called with input variables x 1 ..., it implies previous calls have been made in sequence. Variable definitions used later are highlighted in blue. Neurons with time-varying activation x 1 ..x t can be encoded using a combination of current activation and change in activation, inspired by PID controllers. The encoding scheme for neurons involves a combination of current activation and change in activation, inspired by PID controllers. Parameters determine the encoding representation and decoding formula, aiming for a sparse signal that can be quantized for reduced computation. The encoding scheme for neurons involves a sparse, integer signal representation using Sigma-Delta modulation. By applying temporal summation, rounding, and temporal difference, the original signal can be reconstructed using a decoder. Adjusting the coefficients results in a closer reconstruction to the original signal. The reconstruction function for neurons involves adjusting coefficients to improve signal reconstruction accuracy. Introducing a parameter k p helps align the reconstruction with the correct signal, as seen in Figure 1. This encoding scheme was previously used in O'Connor and Welling (2016b). The encoding-decoding process by O'Connor and Welling (2016b) aims to reduce computation by sparsifying communication in neural networks. The approach approximates matrix-product as additions, inversely proportional to input data sparsity. The activation of hidden layers is computed using quantization and weight changes over time. The effects of these approximations are explored in further detail in Appendix E.1. The cost of computing \u1e91 t depends on the sparsity of s t, which can be decomposed into one-hot vectors. The matrix product s t \u00b7 w can be simplified into row additions. Including encoding and decoding operations, the total cost involves multiplications and additions. This encoding scheme can be applied to every layer of a neural network. Our network function f pdnn can be written using linear and nonlinear operations. Gradients can be approximated for training by defining layer activations and backpropagating the approximate gradients. Implementation can involve executing forward and backward passes at each time-step or in an event-based manner. The internal state of components can change when computing these passes. The text discusses the efficiency of parameter updates in neural networks during training. It mentions the computation of weight gradients and the update formula for weight matrices. The process involves forward and backward passes, with a focus on optimizing the weight increment based on the learning rate. The text discusses optimizing weight updates in neural networks by taking advantage of sparsity in encoded signals. It introduces a more efficient way to compute the product of input and error signals using encoding-quantizing-decoding. This approach involves past and future update schemes to compute the sum over time. The text introduces efficient weight update schemes in neural networks by utilizing sparsity in encoded signals. It discusses past and future update rules for incrementing synaptic weights based on neuron spikes and geometric sequences. Additionally, it mentions adapting parameters to match the magnitude of the data gradient. The text discusses an efficient weight update scheme in neural networks using sparsity in encoded signals and adapting parameters to match data gradient magnitude. It also introduces a form of Spike-Timing Dependent Plasticity (STDP) for parameter updates based on timing differences between pre-synaptic and post-synaptic spikes. In neuroscience, a new form of STDP rule is introduced for neural networks, where presynaptic spikes come from the forward pass and postsynaptic spikes from the backward pass. Unlike classic STDP, weight changes are not dependent on the order of spikes. Experimental results show that different equations lead to equivalent updates over time. The network is trained on standard MNIST and a variant called \"Temporal MNIST\". Temporal MNIST is a reshuffling of the MNIST dataset, grouping similar inputs together. The Proportional-Derivative Net outperformed a conventional MLP on Temporal MNIST, achieving 98.36% accuracy compared to 98.25% for the MLP. The network required less computation to converge on Temporal MNIST than on standard MNIST. The Proportional-Derivative Net (PDNN) achieved slightly lower test scores compared to the MLP on Temporal MNIST (97.99% vs 98.28%). The MLP performed better due to correlated gradients across time-steps, affecting the PDNN's ability to track layer activations accurately. Additionally, hyperparameter results are detailed in Appendix F, and the study simulates CCTV camera settings with limited new information per frame due to a lack of large public video datasets. In the absence of large public CCTV video datasets, the study investigates frame-based object classification on YouTube videos from the Youtube-BB dataset. A subset of 358 Training Videos and 89 Test videos with 758,033 frames is used, each labeled with an object in one of 24 categories. A VGG19 network is utilized, with top layers replaced and trained as a spiking network or regular network with backpropagation. Training the entire spiking network end-to-end is compared to training only the top layers for efficiency. Results show that the spiking network performs comparably to the non-spiking network. The spiking network performs comparably to a non-spiking network, with computation decreasing as frame rate increases. Energy costs are compared using BID8 estimates. Noise-Shaping enhances signal reconstructions per bandwidth unit. Signal reconstructions are improved by noise-shaping in Sigma-Delta quantization, which pushes quantization noise to a higher frequency band for filtering during decoding. This encoding/decoding scheme is a form of Predictive Coding, where predictable parts of a signal are subtracted to transmit only unpredictable parts. Biological neurons may be utilizing predictive coding, as suggested by previous research. In a predictive-coding neuron, there is a distinction between the signal represented and transmitted. Neurons in this study implement a form of predictive coding where the neuron maintains a decayed form of its previous signal. Previous research suggests that biological neurons may utilize predictive coding. Some work has merged spiking neural networks and deep learning, efficiently mapping trained neural networks onto spiking networks. The method devised for training integrate-and-fire spiking neurons with backpropagation did not send a temporal difference of activations. Various approaches were taken to reduce computation on temporally redundant data, but some methods could not be used to train a neural network effectively. Additionally, a spiking network with adaptive quantization and Sigma-Delta modulation was developed, showing potential for training neural networks. The study focused on training neural networks with binary weights and activations, utilizing a simple rule to sparsify communication between layers by encoding spike information. This approach aimed to reduce computation in deep networks by leveraging temporal redundancy in data. The study demonstrates that neurons can act as leaky integrators when neural activations are quantized with Sigma-Delta modulation. Efficient weight update rules related to STDP are derived, leading to reduced computation in deep networks. The network performs well on temporal data and real video data, leveraging temporal redundancy for efficiency. Slow-Feature learning aims to discover latent objects that persist over time, reducing inter-layer communication in a spiking network. Code available at github.com/petered/pdnn. Supported by Qualcomm. Legend of notation provided for clarity in the paper.\u03c6: Internal state variable of quantizer Q. enc: An \"encoding\" operation. The internal state variable of the quantizer Q involves encoding and decoding operations, as well as rounding. The encoding operation encodes a signal into a combination of its current value and change since the last time step, while the decoding operation reconstructs the original signal. The rounding operation simply rounds an input to the nearest integer value. Positive scalar coefficients control the extent to which the encoding is proportional to the input versus the temporal difference. The encoding involves proportional representation of the input and temporal difference, while the decoding reconstructs the original signal. Positive scalar coefficients control the encoding process. The weight matrix and hidden layer activation values are key components in the network. The quantization step at the l'th layer involves encoding/decoding functions, activation approximation z l, and backpropagation through nonlinearity h l. The functions serve similar roles for the backward pass. \u2202L \u2202\u1e91 l approximates the loss derivative with respect to\u1e91 l. In the updates section, weight gradients in layer l are calculated using shorthand notations for input, backpropagated gradient, encoded, quantized, and reconstructed signals. The approximate gradient of the weight matrix w is calculated using the outer product of input and error reconstructions. Gradient approximations using STDP-type update converge to the same value as reconstructions over time. A reparametrization of k p is also discussed. Our encoding/decoding scheme is a form of predictive coding, where the predictable component of a signal is subtracted before transmission and reconstructed after. This reparametrization is used for automatic tuning of parameters to match data dynamics. Linear Predictive Coding is used for reconstruction. Linear Predictive Coding is a form of predictive coding used for reconstruction, defining optimal linear filter parameters to minimize average magnitude. The scheme is similar to feedforward predictive coding, with an additional constant determining quantization coarseness. The work could be extended to develop more efficient predictive coding schemes by learning the input signal's temporal characteristics. The encoding/decoding scheme involves subtracting the predictable component of a signal before transmission and reconstructing it after, allowing for automatic parameter tuning to match data dynamics. Linear Predictive Coding is a form of predictive coding used for reconstruction, with optimal linear filter parameters to minimize average magnitude. The scheme involves a constant for quantization coarseness. Q is described as a successive application of temporal summation, rounding, and temporal difference. Algorithms for \"Past\" and \"Future\" parameter updates are presented. Coefficients are re-parametrized in the update algorithms. Approximations are made for nonstationary weight and quantization. An experiment is conducted with time-varying scalar signal x t and weight w t for different coefficient values. The experiment with time-varying scalar signal x t and weight w t explores the effects of different coefficient values k p and k d on approximation error. Tuning hyperparameters is crucial to balance low reconstruction error and sparse signal representation. Blindly increasing k p and k d results in more spikes in the signal, emphasizing the need to find the optimal \"sweet spot\" for minimal error and computational costs. The effects of non-stationary weight approximation and dec(Q(enc(x t)) \u00b7 w) are shown in different approximations. The cosine distance between the true signal x w and the nonstationary w approximation, the quantization of x approximation, and the full approximation described in Equation 7 are compared. The number of spikes in the encoded signal corresponds to the number of weight-lookups required in a neural network. Scaling a signal by K has a similar effect on the quantized signal as scaling k p and k d by K. When training a network, adjusting k_p and k_d for signal magnitude is crucial. Instead of setting them directly, a ratio k_alpha is fixed, and k_beta is adapted based on signal magnitude. The update rule for k_beta includes a scale-adaptation learning rate, a rolling average of signal magnitude, and a parameter defining quantization coarseness relative to signal magnitude. When training a network, adjusting k_p and k_d for signal magnitude is crucial. The update rule for k_beta includes a scale-adaptation learning rate, a rolling average of signal magnitude, and a parameter defining quantization coarseness relative to signal magnitude. In experiments, k_alpha and k_beta values were set, and performance of PDNN compared to MLP was evaluated under different input orderings and hidden layer depths. No dropoff in performance of PDNN was observed with the accumulation of quantization noise over layers. The study compares the energy costs of additions for the PDNN and multiply-adds for the MLP, noting they are not directly comparable due to fixed-point implementation. Computation remains steady rather than approaching zero as frame-rate increases, possibly due to hidden layer activations not being smoother over time than input. This is demonstrated using video snippets with a non-spiking VGGNet architecture pre-trained on ImageNet, measuring the average relative change in layer activation as frame-rate increases. The study analyzes the average relative change in layer activation at different frame rates and layer depths. Deeper layers show less change in activation over frames compared to input layers, but this change approaches zero more slowly as frame rate increases. The study suggests that deeper layers show less change in activation over frames compared to input layers, but this change approaches zero more slowly as frame rate increases. Methods for learning slow feature detectors may be helpful in addressing this issue."
}