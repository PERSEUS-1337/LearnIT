{
    "title": "SyELrEeAb",
    "content": "Progress in probabilistic generative models has advanced with neural architectures and scalable algorithms for Bayesian inference. Limited progress has been made in capturing causal relationships, especially in understanding how genetic factors contribute to human diseases. This work focuses on building richer causal models to capture nonlinear relationships and adjusting for latent confounders. Ideas from causality and modern probabilistic modeling are synthesized to describe implicit causal models that leverage neural architectures and adjust for confounders. In experiments, Bayesian inference is scaled on up to a billion genetic measurements to identify causal factors with state-of-the-art accuracy, outperforming the second best result by 15-45.3%. Unlike models that learn statistical relationships, causal models allow manipulation of the generative process and making counterfactual statements. The curr_chunk discusses the challenges of combining modern probabilistic models and causality in genome-wide association studies (GWAS) to predict genetic predispositions to diseases and target individual SNPs. The focus is on developing richer causal models beyond additive noise models. The curr_chunk discusses the challenges of capturing interactions and addressing latent confounders in genome-wide association studies (GWAS) using nonlinear models. It aims to discover genetic predispositions to diseases beyond additive noise models. The curr_chunk discusses addressing confounders in genome-wide association studies using implicit causal models that leverage neural architectures to capture nonlinear interactions. It aims to overcome challenges in understanding principled causal models and accommodating complex latent structures. In experiments, Bayesian inference on implicit causal models is scaled up to a billion genetic measurements. Extensive simulation studies validate the model's accuracy in identifying causal factors, outperforming existing genetics methods by 15-45.3%. Real-world GWAS results show the model discovers real causal relationships and identifies similar SNPs as previous state-of-the-art methods. In growing work on causal models, Louizos et al. (2017) develop variational auto-encoders for causality, while Mooij et al. (2010) propose a Gaussian process over causal mechanisms. Other studies focus on post-nonlinear models and neural networks for causal discovery. Recent work in potential outcomes literature explores decision trees and neural networks for balancing covariates across treatments. This paper extends previous methods for causal inference in genome-wide association studies by exploring different approaches to adjust for confounders, such as using principal components or kinship matrices. It also introduces a new causal model with learnable gene-gene interactions. The paper introduces a new causal model with nonlinear gene-gene interactions and describes a Bayesian inference algorithm for estimation. Previous methods for capturing epistasis in genetic studies include linear model interactions, permutation tests, and neural networks for predicting protein bindings from DNA sequences. Probabilistic causal models focus on prediction rather than causality, representing variables as deterministic functions of noise and other variables. Implicit causal models extend this concept to encode complex, nonlinear causal relationships. The causal diagram illustrates a model with global and background variables, allowing estimation of causal mechanisms and probabilities of outcomes. The causal effect p(y | do(X = x), \u03b2) is different from the conditional p(y | x, \u03b2) as it accounts for the direct manipulation of X. The adjustment formula under the causal graph allows estimation of y from observational data, assuming the global structure \u03b2 is known. Different models like additive noise models can be used to estimate y, with induced density for y being normal with unit variance if s(\u00b7) is standard normal. Bayesian inference with prior over parameters allows for joint density calculation using standard algorithms like variational inference or MCMC. Additive noise models have limitations in applying simple nonlinearities and interactions with Gaussian noise. Richer causal models are described to relax these restrictions, addressing the issue of not observing \u03b2. Implicit models hypothesize about the generative process of an unknown distribution. Recent advances in observations have led to the definition of a function g that uses noise to output x with parameters \u03b8. Unlike models with additive noise, using a neural network for g allows for multilayer, nonlinear interactions. Implicit causal models separate randomness from the transformation, mimicking the structural invariance of causal models. These models extend implicit models by incorporating probabilistic causal relations among variables through structural equations. Implicit causal models are universal approximators, utilizing fully connected networks for representation. The Universal Approximation Theorem states that probabilistic causal models can be approximated using fully connected networks with a large number of hidden units. This allows for the approximation of each causal mechanism by a continuous function. Implicit causal models, defined by functions g and same noise distributions, universally approximate the true causal model. Fast algorithms for Bayesian inference can be used with variational methods, allowing for accurate posterior approximations. This approach shows good performance with finite-sized neural networks in experiments. The focus shifts to unobserved global structures, such as in genome-wide association studies (GWAS). In genome-wide association studies (GWAS), data points consist of input vectors of SNPs and outcomes. The causal effect of changes to each SNP on the trait is of interest, but standard inference methods are confounded by unobserved population structure and cryptic relatedness. Latent variables influence the data, but are not observed, complicating the analysis. In GWAS, the causal effect of SNPs on traits is confounded by unobserved variables. A model is built to capture these variables and mechanisms. Priors are placed on parameters, and the posterior of outcome parameters is estimated to account for unobserved confounders. Causal inference with latent confounders can be risky as it involves using the data twice, which may bias estimates. However, as more SNPs and data points are considered, the true confounders can be estimated, leading to a better understanding of the causal mechanism. This approach rigorizes previous methods in probabilistic causal models. Existing GWAS methods adjust for latent population structure using a two-stage Bayesian approach to estimate confounders and infer outcome parameters. The approach justifies using a point estimate of the posterior as a Bayesian approximation. The model specifies functions and priors for confounders, SNPs, and traits, visualized in FIG1. The generative process of confounders involves using a standard normal distribution. Appendix B provides an example implementation in the Edward probabilistic programming language. The generative process of confounders and SNPs involves using standard normal distributions. The dimension of confounders should be set to closely approximate the true population structure. SNPs are encoded as 0, 1, or 2 to denote genotypes in unphased data. The design for GWAS matrices is based on implicit modeling formulation of factor analysis. The generative process involves using a Binomial distribution on x nm, with variables w m acting as \"principal components\" for SNPs. Logistic factor analysis assumptions are relaxed using a neural network for representational capacity. The generative process involves using a neural network to relax assumptions on representational capacity for traits y n in a linear regression model. The neural network takes concatenated inputs of dimension 2K and outputs a scalar real value, enabling learning of nonlinear interactions between z n and w m. A standard normal prior is placed over the weights and biases \u03c6, preserving conditional independence assumptions and avoiding complexity. The model assumes linear dependence on SNPs, no gene-gene or gene-population interaction, and additive noise. The model uses a neural network to generalize a rich causal model for how SNPs cause traits, adjusting for latent population confounders. It aims to infer the posterior of outcome parameters by calculating the joint posterior of confounders, SNP parameters, and trait parameters. This allows for the use of typical inference algorithms on the joint posterior. LFVI is a likelihood-free variational inference method that scales to billions of genetic measurements. It posits a family of distributions over latent variables and optimizes to find the member closest to the posterior, using normal distributions with diagonal covariance for the SNP. The method addresses the intractability of evaluating density in implicit models by assuming sampling from the model's likelihood. LFVI uses normal distributions with diagonal covariance for SNP components and confounders. Stochastic optimization is employed for scaling to massive GWAS data. The algorithm cycles through two stages, inferring posterior distributions of confounders and SNP parameters. It can scale to millions of genetic factors and converges while scanning over the full set of SNPs. The algorithm converges while scanning over the full set of SNPs only once or twice. In the second stage, the posterior of outcome parameters \u03b8 is inferred given the confounders from the first stage. A simulation study on 100,000 SNPs and 940 to 5,000 individuals across 100 replications shows the model's robustness with a gain of 15-45.3% in accuracy. The model is also applied to a real-world GWAS of Northern Finland Birth Cohorts, capturing real causal factors. Our model, applied to a real-world GWAS of Northern Finland Birth Cohorts, captures real causal relationships and identifies similar SNPs as previous state-of-the-art methods. We compare it against PCA, LMM, and GCAT. Using Adam with an initial step-size of 0.005, we initialize neural network parameters uniformly with He variance scaling, and specify fully connected neural networks for traits and SNPs with two hidden layers, ReLU activation, and batch normalization. Including latent variables as input to the final output layer improves information flow in the trait model's neural network. The study analyzes 11 simulation configurations with varying SNP and individual numbers, totaling 4,400 fitted models across different methods. The setting a in PSD and Spatial affects sparsity in the population structure. ICM shows robustness to spurious associations, outperforming other methods by up to 45.3%. Only 10 out of 100,000 SNPs are causal, with Table 1 displaying precision in predicting causal factors. Our method achieves state of the art performance in predicting causal factors across various configurations, especially excelling in tasks with sparse, spatial, and mixed membership structures. It outperforms other methods by a significant margin, up to 45.3% in the Spatial configuration. In simpler configurations like the mixture model, our method shows comparable performance. The study also analyzes a real-world GWAS of Northern Finland Birth Cohorts BID11. The study analyzes a real-world GWAS of Northern Finland Birth Cohorts BID11, measuring metabolic traits and height with 324,160 SNPs and 5,027 individuals. Ten implicit causal models were fitted, each modeling the effect of SNPs on different traits. The models identify similar causal SNPs as previous methods, with comparable performance to other methods. Our model agrees with previous methods in identifying significant loci, with 15 loci compared to BID12's 16. The implicit causal model captures causal relationships similar to previous work, describing a rich class of models that can capture high-dimensional, nonlinear causal relationships in genome-wide association studies. The model achieves state-of-the-art accuracy, outperforming existing genetics methods by 15-45.3%. The limitations of learning true causal associations include linkage disequilibrium and data granularity issues. Better technology and accounting for sequencing mishaps can improve causal modeling. Implicit causal models have potential in various sciences beyond GWAS applications. In high energy physics and economics, modern probabilistic modeling and causality are used to drive new scientific understanding. The causal graph and global confounder are considered, with interventions leading to true structural values. Posterior probabilities concentrate on true functional mechanisms, improving our understanding of causal relationships. The posterior for \u03b8 given \u03b2 concentrates on the true functional mechanism f y, providing a consistent estimate of DISPLAYFORM1. This proof enhances our understanding of learning population structure in GWAS methods and extending them to complex latent variable models. An example of an implicit causal model in the Edward language is provided, where neural net weights and biases are written as model parameters. Local and global priors are included in the log-likelihood function for per-individual and per-SNP data. The text discusses the variational family following the posterior factorization, with local and global priors. It simplifies the objective by assuming independence of variational families for certain variables. The objective can be estimated with Monte Carlo, using reparameterization gradients for gradient-based optimization. Training parameters \u03bb wm and \u03bb zn are of interest. The text discusses training parameters \u03bb wm and \u03bb zn in the context of variational inference. The gradient estimation for \u03bb zn scales linearly with the number of SNPs M, which is undesirable for large M. To prevent this scaling, the text suggests that the information in x n,1:M will influence the posterior more than the single scalar y n. This simplifies the gradient with respect to \u03bb zn, leading to a more scalable solution. The text discusses training parameters \u03bb wm and \u03bb zn in the context of variational inference. The gradient estimation for \u03bb zn scales linearly with the number of SNPs M, which is undesirable for large M. To prevent this scaling, the text suggests that the information in x n,1:M will influence the posterior more than the single scalar y n. This simplifies the gradient with respect to \u03bb zn, leading to a more scalable solution. The gradients with respect to parameters \u03bb wm and \u03c6 are estimated unbiasedly. Inference is performed to approximate the posterior p(z 1:N , w 1:M , \u03c6 | x, y) and p(\u03b8 | z 1:N , w 1:M , x, y). Computational savings are significant within and across tasks. Stochastic gradient ascent is used to maximize the variational objective. Stochastic gradient ascent is used to optimize parameters for training \u03b8 in variational inference. Monte Carlo EM is employed with a single sample z n. The primary computation involves the backward pass of the neural network. Unlike the first stage, all SNPs are considered in the likelihood calculation. Intractable density of y n is addressed by exploiting tractable density for discrete y n and performing likelihood-free inference for real-valued traits. Empirical distribution q(y) is defined to subtract as a constant in the objective. The log-ratio is approximated using a ratio estimator parameterized by \u03bb r. Training involves minimizing a loss function. The ratio estimator scales with the number of SNPs, leading to computational challenges. Two extreme cases are studied for efficiency. One case assumes a tractable Gaussian density for y n. The optimal log-ratio estimator must relearn the neural network's forward pass, which can lead to unstable training. Parameterizing the ratio estimator reduces input dimensions but may result in a poor statistical approximation. Using the neural net's first hidden layer as input strikes a middle ground for efficiency. The ratio estimator uses the neural net's first hidden layer as input, reducing dimensions for efficiency. It preserves more relevant information for the trait y n and is statistically efficient. The gradient is estimated to train the ratio estimator, minimizing the auxiliary loss function. The gradient is estimated by maximizing likelihood with an adversarial objective, using stochastic gradient ascent. Implicit causal models can learn causal structures from data, demonstrated by simulating data from popular models in GWAS. Each data set consists of a M \u00d7 N matrix of genotypes X and vector of N traits y. The genotypes are simulated using allele frequencies encoded in a real-valued matrix F. Various models in GWAS can be described using the factorization F = \u0393S, where \u0393 and S represent structure across SNPs and individuals, respectively. The Balding-Nichols model is one such model that can be used in simulations. The Balding-Nichols Model (BN) + HapMap data set describes individuals based on ancestral subpopulations. The HapMap data set consists of three populations, allowing for allele frequency calculations for each subpopulation at each SNP. The simulated data includes 100,000 SNPs and individuals assigned to subpopulations based on proportions in the HapMap data set. The simulated data, with 100,000 SNPs and 5000 individuals, is based on subpopulation proportions from the HapMap data set. The TGP project catalogs human genetic variation by sequencing over 1000 individuals of diverse ancestries. The HGDP project genotyped DNA samples from individuals worldwide to assess global genetic diversity. The curr_chunk describes the analysis of DNA samples from individuals worldwide to assess genetic diversity using the PSD model with ancestral subpopulations. The rows of \u0393 are drawn from the Balding-Nichols model, and the columns of S are sampled from a Dirichlet distribution. The estimator of F ST value is computed based on allele frequencies and subpopulation assignments. The analysis involves simulating genetic data using the PSD model with ancestral subpopulations. The columns of S are sampled from a Dirichlet distribution, and spatial genotypes are simulated based on individual positions. Different hyperparameter settings are applied to assess the level of sparsity and population structure. The simulated data includes M = 100,000 SNPs and N = 5000 individuals. Traits y are simulated from a linear model. The p-value threshold is fixed at t = 0.0025. True positives are associated with causal SNPs, while false positives are linked to null SNPs. Spurious associations occur when p-values for null SNPs are artificially small. The study involved analyzing p-values for SNPs to identify false positives. The method accounted for structure by considering the average excess of false positives. The data used in the study was obtained from the Genotypes and Phenotypes database. Pregnant women were excluded from the analysis. The neural network used had 512 units in hidden layers for SNPs and 32/256 units for trait analysis. The study analyzed SNPs for false positives, excluding pregnant women from the analysis. SNPs were filtered for completeness and minor allele frequency, then tested for Hardy-Weinberg equilibrium. The genotype matrix had 324,160 SNPs and 5027 individuals. A Box-Cox transform was applied to traits, with adjustments for sex, oral contraception, and fasting status. Extreme outliers were removed for glucose analysis. Causal factors were predicted using a simulation study approach. In predicting causal factors, the p-value threshold was set to the genome-wide threshold of 7.2\u00d710 \u22128 following Kang et al. (2010)."
}