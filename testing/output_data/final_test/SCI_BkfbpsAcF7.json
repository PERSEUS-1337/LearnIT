{
    "title": "BkfbpsAcF7",
    "content": "Deep neural networks show impressive performance but also exhibit failures on out-of-distribution inputs. Adversarial example research aims to uncover errors in neural networks due to distribution shifts, attributing errors to sensitivity and invariance. Deep networks are overly sensitive to irrelevant changes and too invariant to relevant changes, leaving vast input space vulnerable to attacks. This excessive invariance is seen across tasks and architectures. Standard cross-entropy loss is insufficient, leading to failures, prompting an extension based on information-theoretic analysis. The text discusses the failures of deep neural networks due to excessive invariance, leading to vulnerabilities in machine learning models. An information-theoretic analysis is proposed to address this issue, encouraging models to consider all task-dependent features. Adversarial examples reveal that neural networks output the same probabilities for different images, highlighting the lack of perceptually relevant information in the model's decisions. The vulnerability of modern machine learning models is a prime example of their weakness in out-of-distribution generalization. Deep networks show superhuman performance under i.i.d. settings but can make mistakes with tiny shifts in input distribution. Research is ongoing to understand and mitigate these failures, including the need to generalize the concept of adversarial examples beyond small perturbations. The concept of adversarial examples needs to be generalized to the unrestricted case, focusing on invariance as a major cause for vulnerability. Invariance-based adversarial examples show that input content can be changed without affecting network activations, leading to new ways to analyze and control vulnerability. The invariance perspective suggests that adversarial vulnerability is due to narrow learning, where classifiers rely on few highly predictive features. Deep networks rely on spectral statistical regularities or stationary statistics for decisions, rather than abstract features. Excessive invariance can be understood from an information-theoretic viewpoint of crossentropy, which maximizes mutual information between labels and representation. To achieve general understanding, machine learning models must separate essence from nuisance and generalize under shifted input. The curr_chunk discusses the identification of excessive invariance in deep networks leading to adversarial examples. It proposes an invertible network architecture to manipulate images while maintaining classifier invariance. An alternative objective is suggested to reduce excessive invariance. The text also defines pre-images and their link to adversarial examples. The curr_chunk discusses pre-images and their connection to adversarial examples in deep networks. It introduces the concept of invariance-based perturbations and highlights the relationship between invariance-based and perturbation-based adversarial examples. The curr_chunk discusses the causes of adversarial vulnerability in deep networks, emphasizing the importance of both sensitivity to perturbations and insensitivity to transformations. It explains how non-trivial pre-images can occur due to non-injective functions, leading to accumulated invariance issues. The concept of perturbation-based adversarial examples is introduced, involving an oracle and bounded adversarial examples. The curr_chunk discusses invariance-based adversarial examples, focusing on perturbations that cause changes in classifier output while maintaining invariance in the pre-image. It highlights the sensitivity of classifiers to task-irrelevant changes and the importance of characterizing general invariances in deep networks. The curr_chunk discusses invariance-based adversarial examples, highlighting the insensitivity of classifiers to task-relevant changes and the failure modes of the learned classifier. It introduces the concept of invariance to nuisance and semantic variations. The curr_chunk introduces the concept of semantic/nuisance perturbation in inputs, discussing examples like translation or occlusion in image classification. It also mentions the Adversarial Spheres example where nuisance and semantics are formalized as rotation and norm scaling. The fully invertible RevNet is described as a hybrid of Glow and iRevNet with a simple readout structure. It emphasizes the need for a generic approach to access discarded nuisance variability in invariance-based adversarial examples. The curr_chunk introduces fully invertible RevNets, a bijective classifier with simplified readout structure, allowing visualization of hidden-space manipulations and analytic attack for decision-making analysis. It reveals invariance-based vulnerability in competitive classifiers by removing final linear mapping onto class probes. The curr_chunk discusses the architecture of the network, including semantic and nuisance variables, as well as components like actnorm, squeezing, and dimension splitting. The resulting invertible network allows for investigating task-specific content in variables and achieves performance comparable to baselines on MNIST and ImageNet. The curr_chunk discusses metameric sampling as an analytic tool to inspect dependencies between semantic and nuisance variables in trained models. It reveals adversarial subspaces and shows visual similarity to natural images on ImageNet, providing insights without costly optimization procedures. The curr_chunk discusses an analytic attack on synthetic spheres dataset using fully-connected fully invertible RevNet to achieve 100% accuracy in classifying samples. Decision-boundaries of the original classifier and a posthoc trained classifier are visualized, revealing excessive invariance in the classifier. The curr_chunk discusses the invariance of the classifier, highlighting a failure due to excessive invariance of the original classifier. The visualized failure is not due to a lack of data seen during training. The nuisance classifier on z n does not exhibit the same adversarial vulnerability in its subspace. The curr_chunk discusses the failure of classifiers to capture task-dependent variability, showcasing an attack on MNIST and ImageNet. Metameric sampling is applied to invertible RevNets, revealing that nuisance variables dominate the visual appearance of logit metamers. The attack on ResNet152 shows that the findings are not specific to bijective networks. The attack minimizes mean squared error between logits of images, highlighting failures in non-bijective models. The cross-entropy objective fails to explain all task-dependent variations, leading to a proposal to address this issue. Leveraging the invertible network's structure, a formal explanation framework using information theory is developed. The goal of a classifier is to maximize mutual information between semantic features and labels. Adversarial distribution shift is introduced to formalize modifications to input data. Two assumptions are made regarding the predictiveness of network variables. The goal of a classifier is to maximize mutual information between semantic features and labels. Adversarial distribution shift is introduced to formalize modifications to input data. Two assumptions are made regarding the predictiveness of network variables. In the context of bijective networks, the information in z s and z n can be redundant, but synergetic effects are excluded. Equation 5 offers two ways to increase mutual information: directly increasing I(y; z s) or indirectly via decreasing I(y; z n). Typically, in classification tasks, only I(y; z s) is actively increased through training a classifier, leading to high accuracies on training and test data. The text discusses the limitations of cross-entropy training in overcoming semantic invariance and introduces the concept of independence cross-entropy loss using a nuisance classifier. The goal is to minimize unused information by leveraging a bijective function. The main result is stated under assumed distribution shift and successful minimization. The text introduces the concept of independence cross-entropy loss using a nuisance classifier to overcome limitations of cross-entropy training in handling semantic invariance. It discusses the impact of distribution shift on information maximization and the effectiveness of iCE-loss in maintaining information under adversarial distribution shift. Incorporating the nuisance classifier aids in increasing information under distribution shift. Incorporating a maximum likelihood term to encourage factorization of variables, the independence cross-entropy objective is enhanced. The success of the objective is analyzed post-training using metameric sampling attack and evaluating a more powerful nuisance classifier. Our proposed independence cross-entropy loss effectively reduces invariance-based vulnerability compared to vanilla cross-entropy training. It is evaluated through various aspects such as error rates on train and test sets, distribution shift effects, accuracy on nuisance variables, and performance on shiftMNIST. The proposed loss function shows less overfitting and performs similarly to vanilla cross-entropy in terms of test error. The proposed loss function reduces overfitting and improves independence between z n and labels y. Interpolations in CE-trained networks allow for image transformation without changing logits, while iCE-trained networks only alter style, not semantic content. The loss function encourages the model to separate task-related variability from nuisances. Training with independence cross-entropy preserves semantic content during interpolations between nuisances. The classifier trained on nuisance variables of the cross-entropy model outperforms the logit classifier. However, the classifier on nuisances of the independence cross-entropy model performs poorly, indicating little class-specific information in the nuisances. The inability of the nuisance classifier to decode class-specific information is not due to difficulty in reading out from the nuisances. Textured shiftMNIST introduces textured backgrounds for each digit category, with textures underlayed during training but broken at test time. At test time, texture backgrounds are paired randomly with digits to minimize mutual information between background and label. Standard classifiers struggle with this task, showing high error rates on shifted test sets. However, using independence cross-entropy improves network errors by up to 38% on binary shiftMNIST and 28% on textured shiftMNIST. Our proposed loss function significantly reduces errors on binary shiftMNIST and textured shiftMNIST by up to 38% and 28% respectively. Adversarial examples, including -norm restrictions and -feature adversaries, challenge deep networks' decision-making processes. The relationship between standard and bijective networks is explored through reversible techniques. Recent advances in reversible BID21 and bijective networks BID28 BID4 BID31 show similarities between ResNets and iRevNets. ResNets have been proven to be bijective under mild conditions BID7, while non-residual architectures also exhibit excessive invariance BID20 BID6. RevNet-type networks are closely related to ResNets. RevNet-type networks are closely related to ResNets and provide a framework for studying issues like excessive invariance and adversarial robustness in deep learning. The information-theoretic view, including the information bottleneck and mutual information estimation, has gained interest in machine learning. Group-wise independence between latent variables and unbiased representations have also been explored. Additionally, extended cross-entropy losses and minimizing predictability of variables have been introduced for various applications. Our proposed loss shows similarity to the GAN loss BID22, focusing on exploring similarities in optimization for future work. Adversarial example research aims to address limitations in machine learning models, particularly in adversarial vulnerability due to excessive invariance to meaningful variations. This issue persists across tasks and is a key factor in out-of-distribution generalization challenges. The study demonstrates how a bijective network architecture can identify large adversarial subspaces on various datasets. It formalizes distribution shifts causing undesirable behavior and extends the loss function to capture all task-dependent variations in the input. The split between semantically meaningful and nuisance features helps remove unwanted invariances, as shown in targeted experiments. Performing targeted invariance-based distribution shift experiments, Example 7 discusses classifying inputs x from two classes with radii R1 or R2. Semantic perturbations are those where r* = r, while nuisance perturbations have r* \u2260 r. The max-margin classifier D(x) is invariant to nuisance perturbations but sensitive to semantic ones. Adversarial examples are attributed to perturbations that satisfy specific properties. The failure of classifier D is due to misalignment between its invariance and the semantics of the data. An example is given with a mis-aligned classifier on Adversarial Spheres, where perturbing the invariant dimension causes a drop in classification accuracy. The text discusses how classifiers can fail due to excessive invariance to semantic variations. It describes using a standard Imagenet pre-trained Resnet-154 to optimize images to be metameric to a seed image. The optimization involves minimizing a mean squared error loss in a 1000-dimensional semantic logit space. The attack targets all logit entries with no input distance restriction. The metameric sampling attack in bijective networks is the reverse equivalent of the attack discussed earlier. It provides an exact solution with one inverse pass instead of thousands of gradient steps. Quality of randomly sampled metamers from an ImageNet-trained fully invertible RevNet-48 is generally similar, with occasional colored artifacts. Computing mutual information is challenging due to intractability, but a variational lower bound can be used for approximation. The metameric sampling attack in bijective networks is the reverse equivalent of the attack discussed earlier, providing an exact solution with one inverse pass. Quality of randomly sampled metamers from an ImageNet-trained fully invertible RevNet-48 is generally similar. Computing mutual information is challenging, but a variational lower bound can be used for approximation. The lower bound removes the need for the computation of p(y|x), but estimating E Y |X still requires sampling. The nuisance classification loss tightens the bound on I D (y; z n ) under a perfect model of the conditional density. The nuisance classifier needs to model the conditional for I(y; z n ) = I \u03b8nc (y; z n ) to hold. The nuisance classifier must model the conditional density perfectly for I(y; z n ) = I \u03b8nc (y; z n ). Monte Carlo simulation requires sampling from p(y|z n ). The Markov property y \u2194 x \u2194 z n holds, with labels y interacting with inputs x and representation z n interacting with inputs x. Assuming F \u03b8 (x) = z n is deterministic, Lemma 11 shows that minimizing the MLE-term in equation 6 with cross-entropy on the semantics minimizes the mutual information I(z s ; z n ). The softmax function is shift-invariant, leading to independence between logits z s and z n. The log term and summation in L M LEn and L CE are for computational ease. The goal is to maximize mutual information I(y; z s) while minimizing I(y; z n). To maximize mutual information I(y; z s) while minimizing I(y; z n), it is important to ensure that the objective is well defined. The data processing inequality implies a fixed upper bound given by the data (x, y), eliminating the need for gradient clipping or a switch to the bounded Jensen-Shannon divergence. Experiments were conducted using a fully invertible RevNet model with different hyperparameters for each dataset, utilizing Pytorch for the spheres experiment and Tensorflow for MNIST and Imagenet. The network consists of 4 RevNet-type ReLU bottleneck blocks with additive couplings and no batchnorm, trained via cross-entropy with the Adam optimizer at a learning rate of 0.0001. The nuisance classifier is a 3 layer ReLU network with 1000 hidden units per layer. Spheres are 100-dimensional with R1 = 1 and R2 = 10, trained on 500k samples for 10 epochs and validated on 100k holdout set. Achieving 100% train and validation accuracy for logit and nuisance classifier. Using a convolutional fully invertible RevNet with additional actnorm and invertible 1x1 convolutions between each layer. Trained via Adamax with a base learning rate of 0.001. The network is trained using Adamax with a base learning rate of 0.001 for 100 epochs, with a batch size of 64 and l2 weight decay of 1e-4. Data augmentation includes random shifts of 3 pixels. A fully connected 3 layer ReLU network with 512 units is used as the nuisance classifier. Comparison between vanilla cross-entropy training and independence cross-entropy loss is done. Results show similar accuracy for logit classifiers in both training methods, but higher train error for cross-entropy training, indicating less overfitting. The network is trained using Adamax with a base learning rate of 0.001 for 100 epochs, with a batch size of 64 and l2 weight decay of 1e-4. Data augmentation includes random shifts of 3 pixels. A fully connected 3 layer ReLU network with 512 units is used as the nuisance classifier. Comparison between vanilla cross-entropy training and independence cross-entropy loss is done. Results show similar accuracy for logit classifiers in both training methods, but higher train error for cross-entropy training, indicating less overfitting. The final layer applies an orthogonal 2D DCT type-II to feature maps and reads out classes in the low-pass components, resembling ResNets' global average pooling. Training is done with momentum SGD for 128 epochs. The network is trained with momentum SGD for 128 epochs, using a batch size of 480 distributed to 6 GPUs, a base learning rate of 0.1, momentum of 0.9, and l2 weight decay of 1e-4."
}