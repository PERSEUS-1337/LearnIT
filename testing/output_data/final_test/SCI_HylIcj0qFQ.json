{
    "title": "HylIcj0qFQ",
    "content": "Most deep neural networks (DNNs) require complex models for high performance, with parameter quantization used to reduce implementation complexities. This research measures per-parameter capacity of DNN models to gain insights on optimal quantization. Tests are conducted using artificially generated data and various types of DNNs, assessing the effects of parameter number and precision on performance. The model capacities are evaluated by measuring mutual information between input and output. Results are extended to image classification and language modeling tasks. Deep neural networks (DNNs) have achieved impressive performance on various machine learning tasks, with architectures including fully connected DNNs (FCDNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs. Neural networks can be quantized to reduce parameter precision, but this may lead to performance degradation. Optimization of precision is typically done through extensive computer simulations using training data, which can be time-consuming and may not accurately predict real-world performance. In this study, the capacity of DNNs, including FCDNN, CNN, and RNN, is measured using a memorization and classification task with random binary input data. The per-parameter capacities of various models are estimated by measuring mutual information between input data and classification output. The relation between quantization sensitivity and per-parameter capacity is determined through fixed-point performance measurement. The analysis results are extended to real models for image classification and language modeling, comparing parameter quantization sensitivity between memorization and generalization tasks. The study measures the memorization capacity of DNNs, estimating the capacity per parameter to be between 2.3 to 3.7 bits. Quantized networks' performance is linked to capacity per parameter, with FCDNNs showing the most resilience. Severe quantization can be used without much performance loss in over-parameter regions. The suggested number of bits for representing weights in neural networks is approximately 6 bits for FCDNNs, 8 bits for CNNs, and 10 bits for RNNs. The study explores the capacity of neural networks, with estimates of 6 bits for FCDNNs, 8 bits for CNNs, and 10 bits for RNNs. Quantization performance is discussed, showing neural networks are more resilient to quantization during generalization tasks. The paper is structured with sections on previous works, capacity measurement methods, parameter capacity results, and quantization performances on DNNs. The capacity of neural networks has been extensively studied, with a focus on learnability and memorization of random samples. Single-layer perceptrons can memorize at least 2n random samples, while three-layer perceptrons' capacity is proportional to the number of parameters. Recent research has extended this study to quantization performance in various DNN models like FCDNN, CNN, and RNN, showing their generalization ability. In this paper, the effect of network quantization on generalization tasks is discussed. Retraining-based quantization has shown superior performance compared to directly quantized networks in CNN and RNN models. Various extreme quantization techniques, such as 2-bit ternary and 1-bit binary weight quantization, have been explored. Aggressive model compression techniques like vector quantization have also been employed. Our work focuses on the impact of network quantization on generalization tasks in CNN and RNN models. Different quantization techniques have varying effects on performance, with larger networks being more resilient to severe quantization. We introduce a general understanding of parameter quantization and assess network capacity using random data memorization and classification tasks. In task BID6, random binary vectors are generated and assigned to output labels in DNN models. The input size varies depending on the model type, with FCDNN using a one-dimensional vector and CNN requiring a 2-D or 3-D tensor. The DNN is trained to predict the label of the input data, and as the input size increases, classification accuracy decreases due to limited memorization capacity. Capacity is measured using mutual information, which quantifies the amount of information in a random variable. The mutual information of a trained network with N input samples is calculated to maximize accuracy through hyper-parameter tuning using grid search and Bayesian optimization. The optimization procedure involves finding the largest input data size with slightly lower accuracy than 1, followed by a grid search for hyper-parameter boundary values. The hyper-parameters for optimization include initialization, optimizer, learning rate, batch size, and optimizer variables. Hyper-parameter tuning is done using Scikit-learn library with the number of training samples N as a parameter. Quantization of model parameters can reduce model capacity, especially with very small bit sizes. This research observes the degradation in memorization capacity due to quantization in FCDNN, CNN, and RNN models. Uniform quantization is utilized in the process. Quantization in generic FCDNN, CNN, and RNN models involves using uniform quantization with varying precision levels. Bias values are not quantized due to their large dynamic range. Retraining is done after each quantization step, requiring only fine-tuning. Comparison of floating-point and fixed-point DNNs is done by visualizing the loss surface. Loss is measured by applying Gaussian random noise to the network parameters. The distribution of weights may vary depending on the model size and learning method. Two real networks are employed for image classification and language modeling with different datasets. Quantization with varying precision levels is applied to the trained models, and noise is added to the parameters for analysis. The capacities of FCDNNs, CNNs, and RNNs are measured through a memorization task. The capacities of FCDNNs, CNNs, and RNNs are measured through a memorization task using floating-point parameters. The input data dimension should be larger than log 2 N to avoid overlap. Experiments are conducted with hidden layer dimensions of 32, 64, 128, and 256, and depths of 1, 2, 3, and 4, using the 'He' initialization method and SGD with momentum for gradient updates. The FCDNN models' memorization capacities are evaluated based on training accuracy, using SGD with momentum for gradient updates. Hyperparameter tuning involves selecting an initial learning rate between 0.001 and 0.05 on a log scale, with decay factor and momentum values ranging from 0.1 to 0.5 and 0.6 to 0.99, respectively. Experimental results show the best accuracy achieved with different hyperparameters, with model capacity estimated based on training accuracy. Memorization capacities for FCDNN models with varying depths and widths are illustrated in FIG1. The memorization capacity of FCDNN models with different depths and widths is evaluated based on training accuracy. The amount of data that can be memorized quadruples when the dimension of the hidden layer is doubled. FCDNN models with 2, 3, or 4 hidden layers can memorize 2, 3, or 4 times the input data compared to a single layer DNN. The memorization accuracy and mutual information increase as the input data size grows, but eventually drop as the input size increases further. The model's capacity is divided into over-parameterized, maximum performance, and under-parameterized regions based on input data size. In over-parameterized regions, performance can be maintained even with reduced network capacity. FCDNNs have an average capacity of 2.3 bits per parameter, consistent with theoretical studies. The total capacity of the model is optimal for storing random binary samples. CNNs also undergo a similar memorization task to measure capacity. The capacity of CNNs is measured via a memorization task using binary samples. CNN structures vary based on channels, kernel size, and layers. The input dimensions are consistent at n height = n width = 32 and n channel = 1. Max-pooling is used to reduce parameters in the fully connected layer. Per-parameter capacity for convolution layers is calculated by subtracting fully connected layer capacity. Fully connected layer capacity is assumed to be 2.3 bits. The per-parameter capacity of convolution layers is between 2.86 and 3.09 bits, higher than that of FCDNNs. CNNs have an average capacity per parameter of 3.0 bits, even when memorizing uncorrelated data. The parameter-sharing nature of CNNs allows for increased information storage. RNNs also have a capacity per parameter of 3 to 6 bits, demonstrated by training with a dataset lacking sequence correlation. The training loss is calculated using cross-entropy, RNNs with a single LSTM layer of 32-D are used. Unrolling sequence of five-step saturates performance. 5 input random vectors are applied, with 160-D vector at the last time step. The network has 8,386 parameters, achieving 99.52% memorization accuracy with 32K samples. The per-parameter capacity of the RNN model is 3.7 bits, higher than FCDNNs and CNNs. Different network structures have varying per-parameter capacities. DNN performance under quantization depends on the parameter-data ratio. Experiments analyze performance degradation in maximum capacity and over-parameterized regions. Performance with parameter quantization is shown for FCDNN, CNN, and RNN models. The fixed-point performances of FCDNN, CNN, and RNN models with parameter quantization are compared. FCDNN shows no accuracy drop with 6-bit quantization, while CNNs and RNNs experience 5% and 18% drops, respectively. RNNs suffer the most due to their information amount. Performance decline occurs at 6-bit for FCDNNs, 8-bit for CNNs, and 10-bit for RNNs. Additionally, fixed-point performance of DNNs in the overparameterized region is examined, showing lower per-parameter capacity. The FCDNN model with 3 hidden layers and a hidden layer dimension of 128 has a capacity of about 2^17 bits. It is shown to memorize all samples even with 4-bit parameter quantization when using half of its capacity. Over-parameterized models are less sensitive to bit-precision on CNNs and RNNs. The performance of fixed-point DNNs is more robust when the networks are more over-parameterized. The required precision of networks for memorization tasks is assessed using training data. In this section, the effects of network quantization on real tasks are analyzed by training two different sized CNN models with CIFAR-10 data. The small model has 0.22M parameters while the large one has 3.5M parameters. Both models were trained with the same hyper-parameter setting. The impact of network quantization on test performance is evaluated by comparing the loss and accuracy surfaces of floating-point and quantized CNN models. The impact of network quantization on test performance is analyzed by training two different sized CNN models with CIFAR-10 data. The large model shows indifferent performance regardless of parameter precision, while the small 2-bit model has degraded performance but similar test accuracy. Quantized networks are more resilient in generalization tasks, suggesting that the required precision for memorization tasks may be a conservative estimate. The training and test data performance of fixed-point CNN and RNN models on real tasks show that large networks are more robust to quantization. The RNNs are trained for language modeling with Penn Treebank corpus, while FCDNN models are trained with MNIST dataset and ResNet models with ILSVRC-2012 dataset. The study experimented with 4-bit quantized FCDNNs and RNNs on PTB dataset, showing similar performance to floating-point networks. Resiliency to quantization increases with network size, making it a useful method for reducing DNN complexity in VLSI or neural processing engines. Simulation results indicate that per-parameter capacity is not sensitive to model size but depends on the network structure. The study experimented with 4-bit quantized FCDNNs and RNNs on PTB dataset, showing similar performance to floating-point networks. Per-parameter capacity is dependent on the network structure, with RNNs requiring more bits than FCDNNs. Quantized networks preserved capacity up to 6 bits for FCDNNs, 8 bits for CNNs, and 10 bits for RNNs. Precision is crucial for memorization tasks, indicating conservative estimates for real-world neural network implementation. The research provides insights on implementing neural networks for solving real problems, offering practical strategies for training and optimizing DNN models. In CNNs, the weights between layers are referred to as 'kernels' and the output as 'feature maps'. The convolution weights and feature map of the layer are denoted as specific mathematical expressions. In CNNs, weights are denoted as kernels and output as feature maps. The structure of CNNs can vary based on channels, kernel size, and layers. Experiments were conducted with models having twice the feature maps and half the height/width after pooling. The output of the last convolution layer is directly sent to the softmax layer to minimize side-effects from fully connected layers. Capacity per parameter is calculated for convolution layers only. RNNs have a feedback structure reflecting information flow. RNNs, specifically using LSTM, have a feedback structure that processes sequence data. The mutual information equation of a network is derived, considering random variables X, Y, and \u0176. The equation involves the network's average accuracy and loss surfaces of fixed-point CNNs. The loss surfaces of fixed-point CNNs and RNN LMs for CIFAR-10 and PTB datasets are shown in figures. Performance degradation on the test dataset is lower than on the training dataset in all experiments."
}