{
    "title": "Bya8fGWAZ",
    "content": "Value Propagation (VProp) is a parameter-efficient differentiable planning module based on Value Iteration, trained in reinforcement learning to solve new tasks, generalize to larger map sizes, and navigate dynamic environments. It can learn to plan in stochastic environments, offering a cost-efficient system for building size-invariant planners for various navigation problems. Classical planning algorithms have limitations in searching for optimal solutions in diverse tasks and complex environments. Learning to plan is a challenging task, especially in complex and diverse environments. Methods for learning to plan should be traces free and generalize to unseen instances and planning horizons. In a Reinforcement Learning setting, learning to plan involves finding a policy that maximizes expected rewards using algorithms like Value Iteration. Using deep convolutional neural networks, it is possible to approximate a learning algorithm for estimating the value of states in an environment represented as an occupancy map. This allows for end-to-end learning through planner steps without the need for target values, demonstrating capabilities in interactive and generalized settings. In this work, the formalization used in VIN is extended to accurately represent gridworld-like scenarios, enabling Value Iteration modules to be used in reinforcement learning. Hierarchical extensions of the model allow for multi-step planning, providing path-finding and planning capabilities in complex tasks. The models can navigate dynamic environments and generalize to larger maps for navigation tasks. The main contributions of this work include introducing VProp, a network module for pathfinding via reinforcement learning, demonstrating generalization to solve large unseen maps, and navigating environments with complex dynamics. The goal is to learn to plan through reinforcement learning in a grid world environment where entities interact with each other based on attributes. The text discusses learning a policy through reinforcement learning for navigation tasks in environments with various configurations. It involves using 2D images as input and a local embedding function to extract entity information. This formalization is commonly used in robotics for tasks like Simultaneous Localization and Mapping (SLAM) and physics simulators. Reinforcement learning involves developing models for real-world planning tasks using frameworks like SLAM and physics simulators. It aims to compute optimal policies for Markov Decision Problems defined by state, action, transition, reward, and discount factor tuples. The optimal policy maximizes the discounted sum of rewards by iteratively computing the value function. Learning to plan involves iteratively computing the value function to find the optimal policy \u03c0 : s \u2192 a. This is done through algorithms like Value Iteration, which approximates state-transition probabilities and immediate rewards. Different types of algorithms, such as policy gradient methods and actor-critic algorithms, can also find optimal policies by directly computing the policy function or combining policy gradient methods with value-based reinforcement learning for more accurate feedback. The Value Iteration module proposed by BID17 is a recurrent neural network that uses low-variance performance estimation in value-based RL to provide accurate feedback to the policy estimator. The control policy \u03c0 is defined based on the agent's position and current observation of the environment, with a discounted MDP defined by a transition probability function and discount factor \u03b3. The Value Iteration module proposed by BID17 is a recurrent neural network that uses low-variance performance estimation in value-based RL. It defines a transition probability function and immediate reward function for action in state. Value iteration defines a sequence of state and action-value functions, with a linear VI module capable of representing K iterations of value iteration in an MDP. In practice, fewer iterations are needed for tasks like path finding. The Value Iteration module proposed by BID17 is a recurrent neural network that uses low-variance performance estimation in value-based RL. It defines a transition probability function and immediate reward function for action in state. Value iteration defines a sequence of state and action-value functions, with a linear VI module capable of representing K iterations of value iteration in an MDP. In practice, fewer iterations are needed for tasks like path finding. However, (2) corresponds to only a very special case of linear VI modules, with sparse weights and a specific structure in the recurrence. This relationship motivates the development of VI modules for model-based planning with end-to-end architectures, often using Deep Reinforcement Learning. In the realm of navigation tasks, Deep Reinforcement Learning is commonly used as the algorithmic framework. Various methods have been explored, including the use of 3D navigation tasks and VIN-like architectures. Some approaches involve building on 2D occupancy maps for localization and feature grounding, while others focus on multi-agent planning in cooperative settings. Additionally, hierarchical planners have been proposed to address partially observable settings by utilizing VI modules. In the realm of navigation tasks, Deep Reinforcement Learning is commonly used. Various methods include 3D navigation tasks, VIN-like architectures, and hierarchical planners for partially observable settings. Novel alternatives to the Value Iteration module are proposed as drop-in replacements for VIN in graph-based tasks. A key limitation of the VI module is the translation-invariant transition probabilities encoded in the weights of the convolution, affecting the model's ability to learn complex tasks. The Value Iteration module in navigation tasks is limited by its translation-invariant transition probabilities, hindering complex task learning. A new module, Value Propagation (VProp), focuses on state reachability in the grid environment dynamics. The Value Propagation module (VProp) focuses on state reachability in grid environment dynamics. It takes three embeddings as input: a value r in (o), a cost r out (o), and a propagation p(o). The model computes the output based on these inputs, capturing the prior that state reachability does not depend on adjacent cells but on observed transition dynamics. The Value Propagation module (VProp) focuses on state reachability in grid environment dynamics by setting the probability of reaching unachievable states to 0. Goal states are represented with a propagation close to 0 and a positive reward value, while other cells have high propagation values bounded by the discount factor. The agent's policy is a function that takes input vectors and outputs actions or distributions over actions. The VI module and VProp module are purely convolutional, allowing them to generalize to larger environments by increasing recursion depth. The Value Propagation module (VProp) focuses on state reachability in grid environment dynamics by setting the probability of reaching unachievable states to 0. Despite convolutions hard-coding local planning algorithms, models may not generalize across all environment configurations. In pathfinding, different sets of parameters can be equally effective in a fixed environment size. For example, in a 20x20 grid-world with paths of length 40, a VProp module with specific values can be effective. The Value Propagation module (VProp) focuses on state reachability in grid environment dynamics by setting the probability of reaching unachievable states to 0. In larger environments where paths are longer than 50, the model's propagation stops after the 51st step due to the goal value becoming less than 0. To address this issue, the Max-Value Propagation module (MVProp) is introduced to constrain the network and improve pathfinding capabilities. The Max-Value Propagation module (MVProp) constrains the network to represent high base reward goals, propagates them multiplicatively through cells with lower values but high propagation, and learns reward and propagation maps based on cell content. It iteratively computes value maps and ensures values are propagated towards lower-value cells, dealing with individual cell costs using the propagation map. The optimal policy in the Max-Value Propagation module focuses on locally following the direction of maximum value. The training involves an actor-critic architecture with experience replay, collecting transition traces for reinforcement learning. The state is represented by agent coordinates and 2D environment observations, excluding the agent channel. Terminal states are handled differently. The architecture includes a policy \u03c0 \u03b8 and a value function V w, sharing weights until the end of the convolutional recurrence. Training involves sampling a minibatch of transitions and performing gradient ascent over importance-weighted rewards. Capped importance weights are used in off-policy policy gradient. The update rule for the parameters of the value function in the architecture involves a capping constant to control gradient variance and a regularizer to align current predictions with older model predictions. Keeping only the last 50000 transitions was found effective. Learning rates and relative weights are controlled for different objectives, with RMSProp used instead of plain SGD. In practice, RMSProp is used with relative weights \u03bb = \u03b7 = 100.0\u03b7 for training. The grid-world experimental setting involves an agent and a goal placed randomly in a 2d world with fixed dimensions. The agent can move in 8 directions, reaching a terminal state when it reaches the goal or hits a wall. The ratio of unwalkable blocks is fixed at 30% and blocks are uniformly sampled within the space. The grid-world experimental setting involves an agent navigating a maze with obstacles and narrow paths. The agent receives rewards for valid movements and reaching the goal, while hitting walls incurs a penalty. The goal is to navigate the maze quickly, with episodes ending if the agent makes an illegal move or reaches the maximum number of steps. The study uses MazeBase BID15 to create maze configurations for training and testing. Trained agents are also evaluated on maps from a 16x16 dataset for comparison with previous work. The curriculum involves gradually increasing the average length of the optimal path to help agents encounter the goal easier during training. The study compares VProp and MVProp to VIN in maze navigation tasks, showing superior performance. The models were tested on static grid-worlds with varying map sizes and training runs. VIN was primarily tested in a supervised setting, while VProp and MVProp showed better adaptability to goal features during training. In a comparison study, VProp and MVProp outperformed VIN in maze navigation tasks on static grid-worlds with varying map sizes and training runs. VIN achieved an 82.5% success rate in a RL setting, lower than the 99.3% in a supervised setting on a 16x16 map. Results for a larger 28x28 map dataset were not provided, but overall performance was consistent with the best results obtained. Average model performance was lower than expected, as shown in Figure 2 with training on 32x32 maps. Performance varied on VIN dataset, 64x64 generated maps, and evaluation maps with or without curriculum settings. The final average performances of VProp and MVProp in static-world experiments demonstrate their strength. MVProp quickly learns transition dynamics, leading to strong generalization and near-optimal policies within the first thousand training episodes. Dynamic adversarial entities are introduced in experiments, using custom policies like -noop and -direction to augment path-planning experiments. In path-planning experiments, VProp can successfully learn in dynamic environments with changing entities policies, requiring re-planning at each step. Training on 8x8 maps helps reduce computation time without the need for curriculum learning. Value Propagation (VProp) can successfully handle added stochasticity in environment dynamics and learn to tackle avalanche maps on larger sizes. While VIN agents can handle small sizes, they struggle on larger sizes without correct dynamics modeling. Planners learned from data can benefit navigation tasks, adapting to local environment dynamics for flexible planning without collecting new data. Reinforcement Learning shows that planners can be successfully learned if the problem is carefully formalized. Value Propagation (VProp) can handle added stochasticity in environment dynamics and tackle avalanche maps on larger sizes. Planners learned via Reinforcement Learning show great generalization capabilities when built on convnets for 2D path-planning tasks. The methods can even generalize in dynamic environments for complex, interactive tasks. Future plans include testing on graph-like structures and evaluating effects on architectures using VI modules. VProp architectures could be applied to mobile robotics and visual tracking algorithms. Value Propagation (VProp) architectures can be applied to mobile robotics and visual tracking algorithms, as they have the ability to learn and model various functions effectively."
}