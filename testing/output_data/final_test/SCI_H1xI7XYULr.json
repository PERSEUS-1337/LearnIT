{
    "title": "H1xI7XYULr",
    "content": "Animals excel at adapting to their environment, a property that intelligent machines lack. Cellular neuromodulation controls neuron responses in a context-dependent manner. A new deep neural network inspired by neuromodulation is designed for adaptive behaviors. Results show promise for improving artificial system adaptation. Highly efficient algorithms are emerging to improve adaptation of artificial systems, but learning models that can efficiently adapt to new problems remains challenging. Novel architectures are needed to enhance adaptation capabilities of deep neural networks (DNN). Cellular neuromodulation in biological nervous systems continuously tunes neuron behavior in response to external inputs using neuromodulators. Neuromodulation regulates critical nervous system properties beyond synaptic plasticity. The proposed NMN neural architecture for DNNs is inspired by cellular neuromodulation, allowing for adaptive control of continuous behaviors. It consists of a main network with a parametric activation function for neuromodulation and a neuromodulatory network that controls the main network's neuronal dynamics. This architecture enables adaptation to new problems efficiently. The NMN neural architecture for DNNs involves two networks with different inputs: the main network processes samples, while the neuromodulatory network handles feedback and contextual data. Inspired by Hebbian plasticity, the networks have plastic weights that can be dynamically tuned. The architecture mimics biological cellular neuromodulation by allowing the neuromodulatory network to adjust the main network's activation functions. In this work, the main network's activation functions are replaced with neuromodulatory capable versions controlled by a shared neuromodulatory signal z. The signal z is computed by a neuromodulatory network based on contextual inputs, depicted in Figure 1. The NMN architecture, depicted in Figure 1, introduces parameters that scale linearly with the number of neurons in the main network. This approach can be applied to large networks and is evaluated in the meta-RL setting, which divides an MDP into simpler MDPs. The NMN architecture introduces parameters that scale with the number of neurons in the main network. It is evaluated in the meta-RL setting, where an MDP is divided into simpler MDPs. The meta-learning agent aims to maximize the expected sum of rewards over all episodes and steps by using an advantage actor-critic algorithm. The actor and critic are modeled with NMN, compared to standard RNNs. The NMN architecture enhances standard RNN by modeling the actor and critic with neuromodulatory networks. The main network uses fully-connected neural networks with rectified linear units activation functions. Both models have the same number of recurrent layers/units and are trained using A2C algorithm with proximal policy updates. No parameters are shared between the models. The NMN architecture improves RNN by using neuromodulatory networks for actor and critic models. Parameters are not shared between them. Experiments were conducted on custom benchmarks, including a 1D state space problem with rewards based on distance, and a 2D navigation problem with noisy movements. Further details are available on Github. The second benchmark involves navigating towards a target in a 2D space with noisy movements. The agent observes its relative position to the target, outputs a move vector, and is perturbed before moving. If the target is reached, a high reward is given. The third benchmark also involves navigating in a 2D space with two targets, where the agent receives rewards based on reaching the targets. Results show that NMNs learn faster and converge towards better policies than RNNs on three benchmarks. NMNs exhibit stable results with small variances over different random seeds, unlike RNNs. The temporal evolution of the neuromodulatory signal and rewards obtained with respect to \u03b1 for 1000 episodes on benchmark 1 is shown. The agent's behavior initially shows low rewards and little dependence on \u03b1, indicating uncertainty in the current task. Over time, the agent learns and approaches a near-optimal policy, reflected in the convergence of the neuromodulatory signal with a clear dependency on \u03b1. The signal becomes almost state-independent for large time-steps, serving only for adaptation. The value of the signal varies continuously with \u03b1, converging towards similar values for similar tasks. This work utilizes cellular neuromodulation to enhance artificial neural networks. The study explores cellular neuromodulation to improve artificial neural networks' adaptive capabilities, outperforming classical RNN on meta-RL benchmark problems. Future research could focus on exploring different machine-learning problems requiring adaptation, enhancing the introduced NMN with new activation functions or spiking neurons, and analyzing the neuromodulatory signal's impact on activation functions for more complex tasks. Further research is needed to better characterize the performances of the NMN, despite good and robust results obtained with a large choice of parameters."
}