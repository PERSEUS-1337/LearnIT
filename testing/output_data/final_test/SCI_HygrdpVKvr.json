{
    "title": "HygrdpVKvr",
    "content": "Neural Architecture Search (NAS) is a new field with the potential to be as impactful as Convolutional Neural Networks. Despite advancements, comparing different methods remains a challenge due to the lack of a shared experimental protocol. A benchmark of 8 NAS methods on 5 datasets was conducted to address this issue. By measuring a method's relative improvement over randomly sampled architectures, the study aimed to remove biases from search spaces and training protocols. Many NAS techniques were found to struggle in significantly outperforming others. The study conducted experiments with the DARTS search space to understand the impact of different components in NAS. Findings suggest that evaluation protocol tricks greatly affect reported performance, cell-based search space has a narrow accuracy range, hand-designed macrostructure is more important than searched micro-structure, and depth-gap influences architecture rankings. Best practices are recommended to address current NAS challenges. Current NAS pitfalls include difficulties in reproducibility and comparison of search methods. Neural Architecture Search (NAS) aims to free us from hand-crafted architectures, crucial for Automated Machine Learning (AutoML). Various search strategies like Reinforcement Learning, Evolutionary Algorithms, and Gradient-based methods have been proposed, but the preferable approach remains unclear. Evaluation based solely on accuracy overlooks other influencing factors. NAS faces challenges in reproducibility and method comparison. The code for NAS benchmarking is available at https://github.com/antoyang/NAS-Benchmark. Neural Architecture Search (NAS) methods have three components: search space, search strategy, and model evaluation strategy. The focus is on finding the optimal architecture without human intervention, yet there is a debate on the need for expert knowledge infusion. Comparing NAS algorithms based solely on accuracy is challenging due to various influencing factors. The focus of Neural Architecture Search (NAS) is on finding optimal architectures without human intervention. There is a debate on the need for expert knowledge infusion, with concerns about the lack of ablation studies to pinpoint key components. Recent findings suggest that gains in NAS accuracy come more from manual improvements in training protocols rather than search algorithms. A study collected code for 8 NAS algorithms and benchmarked them on 5 CV datasets to understand their effectiveness. Most NAS methods perform similarly on 5 CV datasets, with training methods having a bigger impact than architecture choice. Different architectures from the same search space perform similarly, with hyperparameters and seed significantly affecting ranking. Operations have less impact on accuracy than network structure. The study found that architectures sampled from a search space had similar performance after training on CIFAR10. The paper also discusses the importance of reproducibility in NAS methods and highlights common mistakes to avoid, such as not comparing against proper baselines and lacking details for result reproducibility. In our paper, we argue that measuring the relative improvement over average architectures is a useful tool for evaluating proposed solutions. We conduct a detailed study on the DARTS search space and analyze the impact of training techniques. Sciuto et al. (2019) emphasize the importance of fair baselines and suggest random search with early stopping as a competitive benchmark. They find that expertly engineered search spaces lead to high performance even with randomly selected architectures. The study evaluates different search strategies on various datasets to determine their effectiveness, focusing on the impact of the search strategy rather than the specific architecture. The goal is to identify general trends and common features among the methods. The study compares 8 methods by sampling architectures from their search spaces and training them with the same protocol. The goal of NAS should be to find the optimal model for any dataset, but current practices focus mainly on CIFAR10, risking overfitting. NAS methods should be tested on a variety of tasks to avoid this issue. The study compares 8 NAS methods on 5 different datasets covering various image classification tasks. Criteria for dataset and method selection are outlined, with a focus on open-source code and reasonable running time. Selected methods include DARTS, StacNAS, PDARTS, MANAS, CNAS, NSGANET, and ENAS. The evaluation protocol for NAS algorithms involves two phases: search and augmentation. Methods like DARTS variants use weight sharing. Evaluation involves sampling architectures and reporting top-1 test accuracy. The NAS evaluation protocol includes a search phase where architectures are sampled and compared to a random baseline. The relative improvement (RI) is calculated based on top-1 accuracy, with a good NAS method expected to consistently yield RI > 0. Operations in the DARTS search space are selected with uniform probability. Hyperparameters are optimized on CIFAR10. The hyperparameters are optimized on CIFAR10, assumed to be generalizable to other tasks. Experiments on different datasets use the same hyperparameters. Evaluation results on 5 datasets show small improvements over random sampling, with some methods performing below random. The narrow search spaces result in a small range of accuracies. More experiments are detailed in Section 5. The text discusses the performance of various NAS methods on different datasets, noting that hyperparameters optimized on CIFAR10 may not generalize well. The computational cost of searching for architectures is highlighted as a limiting factor in NAS algorithms' applicability. In this section, the study focuses on the impact of training protocols on the final accuracy of network architectures in NAS algorithms. Sensitivity analysis was conducted using common performance-boosting training protocols on architectures from the DARTS search space on the CIFAR10 dataset. The process involved sampling random architectures, training them with different protocols, and reporting the top-1 test accuracy results. The study evaluates different training protocols on network architectures in NAS algorithms, reporting top-1 test accuracy results. Various protocols, ranging from Base to full with extended training and increased channels, were tested. Different components were selectively enabled/disabled, with specific parameters such as DropPath probability and cutout length. A significant difference of over 3 percentage points was observed in the results. The study evaluates different training protocols on network architectures in NAS algorithms, reporting top-1 test accuracy results. A large difference of over 3 percentage points exists between the simplest and most advanced training protocols. The training protocol is often more important than the architecture used, with expert knowledge applied to the evaluation protocol playing a significant role in achieving state-of-the-art results. The study analyzed various training protocols on network architectures in NAS algorithms, showing a significant difference in top-1 test accuracy results. Architecture sampled from a common search space performed similarly, with a mean accuracy of 97.03 \u00b1 0.23. The best architecture achieved 97.56 accuracy, while the worst had 96.18. Many methods using the same training protocol fall within this range. The study analyzed training protocols on network architectures in NAS algorithms, showing a difference in top-1 test accuracy results. The number of cells has a significant impact on accuracy. Correlation between test accuracies at different epochs grows linearly. A sub-optimal search space was developed to test results from previous sections. The study analyzed training protocols on network architectures in NAS algorithms, showing a difference in top-1 test accuracy results. A sub-optimal search space was proposed, which yielded similar results to the commonly used DARTS space. The specific operations used in the search space did not significantly impact performance, suggesting that the well-engineered cell structure plays a key role in the model's success. The impact of seed and depth-gap on ranking and test accuracy in training network architectures is examined. Restarting training from scratch with different seeds can affect ranking, with a correlation of 0.48 between sets. Test accuracy changes by 0.13% \u00b1 0.08 on average. The correlation between different depths is 0.54, with architectures shifting in rankings by up to 18 positions out of 32. Methods employing weight sharing (WS) can have a significant impact on architecture rankings, with architectures shifting up to 18 positions out of 32. The seed used in training has a large effect on ranking, suggesting that final accuracy should be averaged over multiple seeds. The lottery ticket hypothesis, along with these findings, may explain why methods searching on a different number of cells struggle to improve on randomly sampled architectures. Suggestions on mitigating these issues in NAS research are offered in this section. In this section, suggestions are provided to mitigate issues in NAS research. It is recommended to report results with and without training tricks to avoid hiding the search algorithm's contribution. Evaluating the effectiveness of proposed methods requires measuring the quality of randomly sampled architectures. Future works are encouraged to develop more robust search spaces by randomly sampling architectures and reporting mean and standard deviation. Future research should focus on developing more expressive search spaces to generate both good and bad network designs. The overall wiring of the network plays a significant role in its performance, suggesting the need for investigating optimal wiring at a global level. Using multiple datasets in AutoML research can help prevent algorithmic overfitting and reduce dependence on hyperparameter tuning. The importance of testing NAS algorithms on various datasets with different characteristics, investigating hidden components in the DARTS search space, and emphasizing reproducibility by releasing detailed training protocols and code. The code and training protocol, including hyperparameters, are crucial for assessing search strategies in NAS. NAS-Bench-101 dataset is valuable for evaluating search strategies independently. Hyperparameter tuning is costly in NAS, suggesting either general hyperparameters or including tuning costs in the search budget. AutoML and NAS have the potential to democratize machine learning and improve various tasks significantly. In this paper, notable improvements in various tasks are discussed, emphasizing the need for a principled approach focusing on fairness and reproducibility. The study reveals that many NAS methods have engineered search spaces where all architectures perform similarly well, with training protocols having a greater impact on final accuracy than the network itself. Suggestions are provided to enhance future research robustness in automated neural architecture design. The aim is to shift towards a more general approach in NAS to learn from generated architectures rather than being heavily influenced by current expert knowledge. The section details datasets, hyperparameters, and common settings for methods used in experiments on NVIDIA Tesla V100 GPUs. Models are trained and validated on subsets, with final evaluation on test subset. Common hyperparameters include momentum values and gradient clipping. DARTS, StacNAS, PDARTS, MANAS, CNAS share common settings inspired by DARTS. The datasets used in the experiments on NVIDIA Tesla V100 GPUs include CIFAR10 and CIFAR100, with details on their sizes and classes. The search process for methods like DARTS and PDARTS involves multiple stages with varying operations and dropout probabilities. The official implementation for PDARTS can be found at a specific GitHub link. The CIFAR100 dataset consists of 100 classes with 50,000 training images and 10,000 test images of size 32\u00d732. The dataset is split into training, validation, and testing subsets. Standard data pre-processing and augmentation techniques are applied. The SPORT8 dataset contains 8 sport event categories with 1579 images. The MIT67 dataset includes 67 classes representing indoor scenes with 15,620 images of different sizes. The MIT67 dataset includes 67 classes of indoor scenes with 15,620 images. The FLOWERS102 dataset consists of 102 classes of flowers with 8,189 images. Standard data pre-processing and augmentation techniques are applied to both datasets. After layer 2, Stocastic Depth was implemented as an alternative to DropPath for a similar effect."
}