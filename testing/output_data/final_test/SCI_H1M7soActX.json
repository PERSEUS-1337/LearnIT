{
    "title": "H1M7soActX",
    "content": "Understanding the behavior of stochastic gradient descent (SGD) in deep neural networks has recently become a topic of concern. A study on gradient-based optimization dynamics with unbiased noise, unifying SGD and Langevin dynamics, reveals insights into SGD's ability to escape minima and its regularization effects. A novel indicator is introduced to measure the efficiency of escaping minima by aligning noise covariance with the curvature of the loss function. It is shown that anisotropic noise in SGD helps escape sharp and poor minima effectively, leading to more stable and generalizable minima. Anisotropic noise in SGD helps escape sharp and poor minima effectively, leading to more stable and generalizable minima. Anisotropic diffusion with full gradient descent, Langevin dynamics, and position-dependent noise are used in learning algorithms like stochastic gradient descent (SGD) for training neural networks. SGD and its variants are widely used for deep learning due to their efficiency. Recent research focuses on analyzing SGD's optimization behaviors and their impact on generalization, including escaping stationary points like saddle points and local minima. Researchers have analyzed how SGD escapes from stationary points, such as saddle points and local minima. The noise introduced by SGD affects generalization in deep learning, with studies showing that training with a large batch can lead to a drop in test accuracy. Investigations have also looked at how the noise level, batch size, and learning rate impact generalization during SGD optimization. Additionally, some research interprets SGD from a Bayesian perspective, viewing it as performing variational inference to prevent overfitting. This aligns with the flat/sharp minima argument. In this work, the authors study the optimization behavior of stochastic gradient descent (SGD) and its regularization effects. They analyze a general form of gradient-based optimization dynamics with unbiased noise, which unifies SGD and standard Langevin dynamics. The research investigates how the noise structure of SGD influences the escaping behavior from minima and its regularization effects, deriving a key indicator to characterize the efficiency of escaping. The study analyzes the efficiency of escaping minima in stochastic gradient descent (SGD) by measuring noise covariance alignment and loss function curvature. It establishes conditions showing superior performance of anisotropic noise over isotropic noise in escaping sharp minima. SGD in deep neural networks meets these conditions, explaining its ability to escape sharp minima and converge to flat minima with higher generalization. Experiments confirm the importance of SGD's noise structure in optimization behavior. The text discusses the effectiveness of anisotropic diffusion in stochastic gradient descent (SGD) for training deep networks. It compares SGD with full gradient descent using different types of diffusion noise and explains how anisotropic diffusion leads to good generalization. The paper is organized into sections that introduce the background of SGD, study the behaviors of escaping minima, and show that the noise of SGD in deep learning meets conditions for efficient escaping from sharp minima. Various experiments support these findings. The paper discusses the optimization process in supervised learning, focusing on minimizing the loss function in deep neural networks using gradient descent. It explores the existence of multiple global minima and their impact on generalization performance. The text concludes with experiments verifying the understanding of the dynamics involved. In supervised learning optimization, gradient descent (GD) is commonly used to minimize the loss function. Other optimizers like gradient Langevin dynamics (GLD) and stochastic gradient descent (SGD) introduce noise for non-convex optimization. SGD involves selecting a minibatch of training samples in each iteration to evaluate a stochastic gradient, updating parameters with a learning rate. In supervised learning optimization, gradient descent (GD) is commonly used to minimize the loss function. Other optimizers like gradient Langevin dynamics (GLD) and stochastic gradient descent (SGD) introduce noise for non-convex optimization. Denote g(\u03b8) as the gradient for loss with a single data point, and assume g(\u03b8 t ) follows a Gaussian distribution. The update of SGD can be rewritten as gradient descent with unbiased noise, where the noise term is influenced by the magnitude of noise in SGD. In this work, the focus is on studying the benefits of anisotropic structure of noise covariance in SGD for escaping minima and its regularization effects on generalization in deep learning. The noise magnitude is constrained to be constant to eliminate its influence, and the energy is kept unchanged to maintain system stability.\u03b7\u03c3 2 t is absorbed into \u03a3 t for simplicity, denoted as \u03a3 t. The SDE associated with the gradient variant optimizer can be written as BID9 BID2 BID8. The escaping efficiency is measured using E[L t \u2212 L 0], where L t \u2212 L 0 \u2265 0. The escaping probability is given by DISPLAYFORM2, where H t denotes the Hessian of L(\u03b8 t) at \u03b8 t. The escaping efficiency for general processes is hard to analyze due to the intractableness of the integral in Eq. (8). However, considering the second-order approximation locally near the minima \u03b8 0, where H is a positive definite matrix and the diffusion covariance \u03a3 t = \u03a3 is constant for t, the SDE becomes an Ornstein-Uhlenbeck process. Proposition 2 discusses the escaping efficiency of this process, proposing Tr (H\u03a3) as an empirical indicator for measuring efficiency in escaping from minima. The noise covariance structure \u03a3 benefits escaping sharp minima, especially when the Hessian of the loss surface is anisotropic. For maximum efficiency, \u03a3 should be highly anisotropic, with Proposition 3 showing that anisotropic noise outperforms isotropic noise in ill-conditioned scenarios. Proposition 3 highlights the benefits of anisotropic noise over isotropic noise in terms of escaping efficiency when the noise covariance structure \u03a3 is aligned with the Hessian matrix H. The conditions for this advantage include the alignment of maximal eigenvalues and unit eigenvectors, ensuring a lower bound for efficiency. In modern deep neural networks, the gradient covariance is usually ill-conditioned and anisotropic near minima. This is further explored in the context of deep neural networks to understand its connection with the Hessian of the loss surface. In deep neural networks, the gradient covariance is often ill-conditioned and anisotropic near minima. The Hessian of the loss surface is closely related to the Fisher information matrix, especially when the parameter is close to the true value. The approximate equality between gradient covariance and Hessian is due to noise dominance over the mean gradient in later stages of optimization. This observation is supported by experimental evidence. Theoretical characterization of closeness between \u03a3 and H in one hidden layer neural networks, showing gradient covariance benefits in escaping minima. Results on bounding Fisher and Hessian in binary classification neural networks with one hidden layer. The text discusses the relationship between Fisher and Hessian matrices in neural networks with one hidden layer and piece-wise linear activation. It highlights the benefits of gradient covariance in escaping minima and provides results on bounding Fisher and Hessian in binary classification neural networks. The relationship between gradient covariance and Hessian in neural networks with one hidden layer and piece-wise linear activation is discussed. Proposition 5 shows the relationship between gradient covariance and Hessian, indicating loose conditions for population loss around true parameters. The ill-conditioning of Hessian due to over-parameterization in modern deep networks is highlighted. The noise structure of SGD helps escape sharp minima faster and converge to flatter solutions, leading to better generalization compared to other optimization dynamics. Experiments are conducted to verify these properties and understand the regularization effects of different noise structures. The noise structure of SGD helps escape sharp minima faster and converge to flatter solutions, leading to better generalization. A 2-D toy example with two basins, a sharp and flat minima, is used to illustrate this concept. In a 2-D toy example, dynamics escape from a sharp minimum towards a flat one. Experiments are conducted on various dynamics with controlled noise magnitude. GLD 1st eigvec(H) shows the highest success rate, indicating fast escaping speed. The experiments demonstrate that dynamics with anisotropic noise aligned with Hessian outperform those with isotropic noise, helping to escape sharp minima efficiently. Empirical results on neural networks show that anisotropic noise induced by SGD aids in faster escape from sharp minima. The number of hidden nodes affects the ratio significantly, as shown in FIG3. In real deep learning scenarios, experiments were conducted to demonstrate the effects of SGD noise and implicit regularization. A noisy training set was created using FashionMNIST dataset, with 1000 images having correct labels and 200 images with random labels. A small LeNet-like network was used for analysis, with two convolutional layers and two fully-connected layers. Standard gradient descent was run for 3000 iterations to reach parameters near the global minima with zero training loss and 100% accuracy. The study conducted experiments to analyze the effects of SGD noise and implicit regularization in deep learning. The global minima with near zero training loss and 100% accuracy were reached using standard gradient descent. The SGD optimizer was then initialized from these sharp minima found by GD, showing ill-conditioning as per Proposition 3. The study analyzed the effects of SGD noise and implicit regularization in deep learning. SGD was initialized from sharp minima found by GD, showing ill-conditioning as per Proposition 3. The projection coefficient decreases along the trajectory of SGD, satisfying conditions for faster escape from minima compared to GLD. Tr(H\u03a3) decreases significantly during SGD optimization, indicating convergence to minima. Tr(H\u03a3) can be used as an indicator for escaping efficiency. Different dynamics escaping from minima and their generalization effects were compared. The study compared different optimization methods in deep learning, including SGD, GLD 1st eigvec(H), GLD leading, and GLD Hessian. Results showed that SGD and some GLD variants successfully escaped sharp minima found by GD, while others remained trapped. The expected sharpness of minima was measured, and the results are depicted in FIG5. The study compared optimization methods in deep learning, showing that some methods escaped sharp minima while others remained trapped. Anisotropic noise aligned with loss curvature can find flatter minima. Theoretical investigation of optimization dynamics with unbiased noise unifies methods like SGD. Novel results on escaping minima and regularization effects were provided, with analysis on noise structure in SGD satisfying conditions for efficient escaping. The study analyzed the noise structure of SGD in deep learning, confirming its ability to escape sharp minima efficiently towards flat minima that generalize well. Experimental evidence supports the behavior of SGD and its effects on generalization. Isotropic noise has little impact on escaping sharp minima due to the highly anisotropic landscape. Further understanding of this behavior is needed. The multivariate Ornstein-Uhlenbeck process with constant \u03b8 follows a Gaussian distribution. Changing variables using Ito's lemma yields the covariance of \u03b8. The proof involves computing gradients and Hessians, and utilizing Gauss-Newton decomposition for piece-wise linear functions. The multivariate Ornstein-Uhlenbeck process with constant \u03b8 follows a Gaussian distribution. The gradient covariance and Fisher have a relationship. Additional experiments show the dominance of noise over gradient during training using SGD. In experiments using TensorFlow 1.5.0, SGD, GLD 1st eigvec(H), and GLD Hessian successfully escape sharp minima, while GLD diag, GLD dynamic, GLD const, and GD do not. The experiments were conducted on the FashionMNIST dataset with a VGG11 network. Training details include fixed learning rates and batch size for SGD. The experiments conducted on GD, GLD const, GLD dynamic, GLD diag, and SGD with noise std of GLD constant tuned to best. Sharpness estimated with M = 100 and \u03b4 = 0.01. Similar experiments as in main paper for CIFAR-10 and VGG11. Learning rates set to 0.005, dynamics tuned to have same expected square norm of 0.01. Training set consists of 1200 examples randomly sampled from FashionMNIST dataset. The training set consists of 1200 examples randomly sampled from the original FashionMNIST dataset, with 200 of them having randomly wrong labels. The test set remains the same as the original FashionMNIST test set. The model network architecture includes convolutional layers with 5x5 kernels and 10 channels, followed by fully connected layers with 50 hidden units. The total number of parameters in the network is 11,330. Different learning rates were tested for GD and GLD, but no improvement on generalization was observed. GLD dynamic, diagonal, leading, Hessian, and 1st eigvec(H) experiments were conducted with specific learning rates, number of leading eigenvalues, batch sizes, and update frequencies. Computational resource limitations led to Hessian matrix updates every 10 iterations with added noise. TensorFlow 1.5.0 was used for implementation."
}