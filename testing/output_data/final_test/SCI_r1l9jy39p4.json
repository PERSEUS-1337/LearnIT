{
    "title": "r1l9jy39p4",
    "content": "The simplicity bias in neural networks predicts that simple outputs are more likely to occur than complex outputs. This bias has been observed in various systems, from RNA sequences to models of plant growth. Deep neural networks map parameters to functions and show a strong bias towards functions with low complexity. The simplicity bias in neural networks predicts that simple outputs are more likely to occur than complex outputs. A recent paper derived an inequality using a simple procedure involving mapping inputs to outputs and ordering them by frequency. This procedure allows for describing inputs with a code of length -log2P(x) + O(1) using a Shannon-Fano code. The Shannon-Fano code allows for describing inputs with a code of length -log2P(x) + O(1), which upper bounds the Kolmogorov complexity. If the maps are simple, K(x|f, n) \u2248 K(x) + O(1). Kolmogorov complexity is uncomputable, but a pragmatic approach approximates P(x) using K(x). The complexity of the input-output map f needs to meet certain conditions, such as redundancy, large system size, nonlinearity, and being well-behaved to ensure good approximations of K(x). Random number generators may produce outputs that seem complex but have low Kolmogorov complexity due to simple algorithms. The simplicity bias bound (1) was empirically tested for a wide range of different maps, from sequences to stochastic financial models, and it worked remarkably well in each case. The simplicity bias bound was tested on various maps, showing that high probability outputs have low complexity and vice versa. A simple matrix map can directly test this condition, demonstrating that as the map becomes more complex, simplicity bias phenomena disappear. The method relies on the assumption of put x, with the upper bound being a poor approximation. Parameters a and b can be estimated with limited text information, and a small amount of information is needed to fix the chosen measure of complexity. Robustness is a useful property for different sized systems. The text discusses the simplicity-bias bound tested on various maps, showing that high probability outputs have low complexity. It illustrates an input-output map for RNA secondary structures and the efficiency of predicting its secondary structure based on specific nucleotide bonding. The mapping from sequences to RNA secondary structures fulfills simplicity conditions, with outputs having low complexity. Probability of finding a specific structure is bounded by physics laws, independent of sequence length. Simple compression algorithms can estimate structure complexity. The simplicity bias bound works well for a variety of maps, with predictions indicating that output probability decreases exponentially with output complexity. Meeting certain conditions is necessary but not sufficient for bias to occur in maps. The question remains whether deep learning can be viewed as input-output maps that also exhibit this bias. The parameter-function map in neural networks obeys conditions for simplicity bias. Random neural networks with distributions over parameters are used to explore the properties of this map. The parameter-function map in neural networks exhibits simplicity bias with Gaussian or uniform distributions over parameters. A Gaussian process approximation is used to estimate the probability of different labellings on CIFAR10 images. The complexity of functions is measured using the critical sample ratio, showing a strong correlation between log probability and complexity. In FIG7, log probability values for CNN and FC networks are shown on a sample of 10k images from CIFAR10, MNIST, and fashion-MNIST, illustrating the range of probabilities for different labels and the negative correlation with label corruption. Increasing label corruption likely corresponds to higher Kolmogorov complexity measures. Direct measurements of simplicity bias are computationally expensive, limiting results to datasets smaller than CIFAR10. Further research is needed to isolate simplicity bias in the parameter-function map. In the architecture analysis, Gaussian process approximation is used to compute the log probability of labelings on images with increasing label corruption. Different complexity measures are explored using MLPs with varying hidden layers and activation functions. Parameters are sampled from Gaussian or uniform distributions, and the probability of individual Boolean functions is estimated. The log probability is plotted against Lempel-Ziv complexity in FIG2. The simplicity bias in the parameter-function map of deep neural networks is demonstrated in FIG2, showing the probability versus Lempel-Ziv complexity. Similar results for other complexity measures are shown in Appendix C.3, highlighting that most of the probability mass is close to the upper bound when sampling parameters. Additionally, FIG9 in Appendix D displays the probability versus complexity plots for fully connected neural networks. The parameter-function map of deep neural networks exhibits a simplicity bias, with probabilities following Zipf's law as the number of layers increases. This behavior was also observed in networks of Boolean functions and general circuits. The parameter-function map of deep neural networks shows probabilities following Zipf's law for sufficiently overparametrized networks. Experiments confirm this behavior, with the probability-rank plot approaching Zipf's law for different parameter distributions. The architecture can fit almost all Boolean functions of 7 inputs, aligning well with Zipf's law. The Zipf's law curve in FIG3 shows excellent agreement with no free parameters, indicating a highly biased distribution with probabilities spanning a wide range of magnitudes. The mean probability over all functions is very small, and the range of magnitudes scales with the size of the input space. This aligns with Zipf's law, suggesting a consistent pattern in neural networks. Neural networks exhibit a simplicity bias encoded in the parameter-function map, leading to good generalization. This bias is shown to closely match true generalization error in PAC-Bayesian bounds. The number of parameters does not strongly affect generalization, aligning with Zipf's law and providing insights into deep network phenomena. The phenomena observed in neural networks show a bias towards simplicity, which is general across various architectures. Testing this claim on larger systems will require new sampling techniques and analytic arguments. The main experiments used fully connected networks with varying layers and ReLU nonlinearity. In recent work, it was shown that infinitely-wide neural networks are equivalent to Gaussian processes, implying that the outputs of the network are jointly distributed. The outputs of neural networks are jointly distributed with a Gaussian distribution. The kernel function depends on the network architecture and properties of the parameters. The main quantity in the PAC-Bayes theorem is the probability of a given set of output labels for the training instances. The distribution of neural network outputs is no longer Gaussian due to the output nonlinearity. Gaussian process approximation is used to calculate the probability of a training set, with expectation-propagation being a more accurate method than Laplacian approximation. The Laplacian approximation is compared to expectation-propagation (EP) in terms of accuracy. Identifying a suitable complexity measure is crucial for applying the simplicity bias framework. Different complexity measures yield similar simplicity bias behavior, demonstrating robustness. The same qualitative results are obtained for different complexity measures in convolutional networks. The Lempel-Ziv complexity measure, based on finding regularities in binary strings, is used to assess robustness in the simplicity bias framework. It is asymptotically optimal and equivalent to Kolmogorov complexity for an ergodic source. The variation of Lempel-Ziv complexity from FORMULA0 is utilized, which considers the number of words in the \"dictionary\" when compressing a binary string. The Lempel-Ziv complexity measure, based on finding regularities in binary strings, is used to assess robustness in the simplicity bias framework. The symmetrization makes the measure more fine-grained, and the log 2 (n) factor ensures that it scales as expected for Kolmogorov complexity. The binary string representation depends on the order of inputs, affecting LZ complexity, but for low-complexity input orderings, it has a negligible effect. Entropy is a weak measure of complexity based on the number of zeros and ones in a binary string. The Lempel-Ziv complexity measure assesses robustness in the simplicity bias framework by finding regularities in binary strings. Entropy is a weak measure based on the number of zeros and ones. Boolean functions can be compressed to simpler forms using the Quine-McCluskey algorithm. The complexity measure for Boolean functions introduced by L. Franco et al. captures the difficulty of learning and generalizing the function. It consists of terms measuring the average fraction of neighbors that change the output, with the first term known as average sensitivity. This measure is used to find that simple functions generalize better in a neural network. The complexity measure for Boolean functions introduced by L. Franco et al. captures the difficulty of learning and generalizing the function. It consists of terms measuring the average fraction of neighbors that change the output, with the first term known as average sensitivity. The measure is defined with respect to a sample of inputs as the fraction of all inputs that have another input at Hamming distance 1, producing a different output. Different complexity measures are compared in FIG1, showing that generally more functions are found with higher complexity. Probability versus complexity plots for other complexity measures are shown in FIG7. In FIG7, probability versus complexity plots for different complexity measures are shown, similar to the LZ complexity measure in FIG0. FIG3 displays probability versus LZ complexity plots for various parameter distributions, while FIG2 shows a histogram of functions in the log probability-complexity plane. Additionally, FIG9 illustrates the impact of the number of layers on bias in feedforward neural networks. The curr_chunk discusses the parameters in a network of shape BID6 40, 40, BID0, with points of frequency 10^-8 removed for clarity. The complexity measures are described in Appendix C. The text also mentions a histogram of functions in the probability versus Lempel-Ziv complexity plane. Histogram of functions in the probability versus Lempel-Ziv complexity plane, weighted by probability from a sample of 10^8 parameters for a network of shape BID6 40, 40, BID0. The distribution shows higher complexity functions with increasing expressivity up to a 2-layer network, then a shift towards lower complexity for 2 layers and above. Finite-size effects for sampling probability are discussed due to the minimum estimated probability of 1/N for a sample of size N. The finite-size sampling effect is illustrated in Figure 7, showing how it changes with sample size N. These points are usually removed from plots."
}