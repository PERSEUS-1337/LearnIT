{
    "title": "H1tSsb-AW",
    "content": "Policy gradient methods in deep reinforcement learning face high variance of gradient estimates, especially in long horizon or high-dimensional action space problems. To address this, a bias-free action-dependent baseline is derived for variance reduction, utilizing the stochastic policy's structural form without additional assumptions about the MDP. The benefits of this baseline are demonstrated through theoretical analysis and numerical results, showing improved learning efficiency in high-dimensional control tasks. Experimental results show that action-dependent baselines lead to faster learning in reinforcement learning benchmarks and high-dimensional tasks. This idea can also be applied to partially observed and multi-agent tasks. Deep reinforcement learning has been successful in various domains, with policy gradient methods playing a key role in optimizing stochastic policies through local gradient information. Policy gradient methods aim to improve performance by adjusting the log probability of actions based on future rewards. However, a major challenge is the high variance of the gradient estimator, especially in long horizon problems. To address this, a \"baseline\" is used to help with credit assignment and reduce variance by comparing actions to average performance from the same state. The paper discusses improving credit assignment in policy gradient methods by using a better baseline to reduce variance. It suggests incorporating information about factors that influence individual actions to further enhance credit assignment. This approach aims to remove the influence of other factors on rewards, leading to a more accurate evaluation of each factor's performance. Factorized policies are commonly used in various tasks such as continuous control, robotics, game domains, and multi-agent systems. Each agent deploys its own policy, and action-dependent baselines have shown to consistently improve performance compared to using only state information. The proposed method shows significant speed-up in learning process for various tasks, including high-dimensional door opening and target matching tasks, blind peg insertion, and multi-agent communication. The method aims to facilitate reinforcement learning in domains with high-dimensional actions. Videos and additional results can be found at https://sites.google.com/view/ad-baselines. Policy gradient methods offer an unbiased gradient but suffer from high variance compared to critic-based methods. Variance reduction techniques, such as using a baseline, have been studied extensively. However, fully exploiting the factorizability of the policy probability distribution to further reduce variance remains unexplored. Recently, methods like Q-Prop utilize off-policy data and action-dependent control variates to improve sample efficiency in reinforcement learning. While Q-Prop is computationally expensive, our formulation of action-dependent baselines offers a more efficient alternative with minimal overhead. This approach has shown promise in reducing variance and improving sample efficiency compared to on-policy methods with state-only baselines. This section establishes notations and basic results for policy gradient methods and variance reduction via baselines in a discrete-time Markov decision process (MDP). The MDP is defined by state space S, action space A, transition probability function P, reward function r, initial state distribution \u03c1 0, and discount factor \u03b3. The paper explores using the structure in policy parameterization to enhance learning speed, a novel approach not previously explored in multi-agent systems. The presented models focus on optimizing a stochastic policy parameterized by \u03b8 to find the optimal policy for a Markov decision process. Q-functions are used to describe cumulative discounted return, and additional components are required for partially observable processes. The Policy Gradient Theorem BID22 states that for policies over states, the state visitation frequency can be defined as \u03c1 \u03c0 (s) = \u221e t=0 \u03b3 t p(s t = s). This theorem allows for the estimation of the gradient \u2207 \u03b8 E x [f (x)] by using the score function estimator BID28. By subtracting a quantity dependent on s t from Q(s t , a t), the variance of the gradient estimator can be reduced without introducing bias. The Policy Gradient Theorem states that the state visitation frequency can be defined as \u03c1 \u03c0 (s) = \u221e t=0 \u03b3 t p(s t = s). By subtracting a quantity dependent on s t from Q(s t , a t), the variance of the gradient estimator can be reduced without introducing bias. Internal structure in policy parameterization can be exploited to further reduce variance of the gradient estimator without bias. The text discusses deriving optimal action-dependent baselines for a class of problems and analyzing the suboptimality of non-optimal baselines in terms of variance reduction. It also proposes practical baselines for implementation and presents the overall policy gradient algorithm with action-dependent baselines for factorized policies. Additionally, it explores action-dependent baselines for policies with conditionally independent factors, such as multivariate Gaussian policies with a diagonal covariance structure commonly used in continuous control tasks. In this section, the text discusses deriving optimal action-dependent baselines for policies with conditionally independent factors. It presents a gradient estimator compatible with the advantage function form of policy gradient, showing bias-free results with different baselines for general policy structures. In this section, the text discusses deriving optimal action-dependent baselines for policies with conditionally independent factors. It presents a gradient estimator compatible with the advantage function form of policy gradient, showing bias-free results with different baselines for general policy structures. The optimal action-dependent baseline is derived to minimize the variance of the policy gradient estimate, with the assumption that different subsets of parameters strongly influence different action dimensions. This assumption is primarily for theoretical analysis and not required for practical algorithm implementation. The text discusses deriving optimal action-dependent baselines for policies with conditionally independent factors. It presents a gradient estimator compatible with the advantage function form of policy gradient, showing bias-free results with different baselines for general policy structures. The optimal action-dependent baseline is derived to minimize the variance of the policy gradient estimate, with the assumption that different subsets of parameters strongly influence different action dimensions. The optimal action-dependent baseline is then derived, showing improvement over a traditional baseline that only depends on state. The text discusses deriving optimal action-dependent baselines for policies with conditionally independent factors, showing bias-free results with different baselines for general policy structures. It states that the difference in variance is large when the Q function is highly sensitive to actions, especially along directions that influence the gradient the most. Empirical results demonstrate the benefit of action-dependent over state-only baselines. The optimal state-only baseline is rarely used in practice due to computational and conceptual benefits. The text discusses deriving optimal action-dependent baselines for policies with conditionally independent factors. It highlights the benefits of using a baseline that is closely correlated with the action-value function. Monte Carlo estimates can be used to obtain the baselines, ensuring bias-free results. The text proposes a practical algorithm for fully factorized policies, reducing computational burden by using sample max for discrete action dimensions. It suggests a more computationally practical baseline and outlines steps for policy updates and action-value function estimation. The text presents a practical algorithm for fully factorized policies, suggesting a more computationally practical baseline for policy updates and action-value function estimation. It includes results on popular benchmark tasks in deep reinforcement learning. The proposed approach in high-dimensional tasks compares action-dependent baselines with state-only baselines, showing that action-dependent baselines perform better. A Random Fourier Feature representation is used for the baseline parameterization, approximating RKHS features under an RBF kernel. The baseline is parameterized using a Random Fourier Feature representation under an RBF kernel. The representation is chosen to have the same number of trainable parameters for all baseline architectures and to accurately estimate optimal parameters with a Newton step. For policy optimization, a variant of the natural policy gradient method is used. The influence of computing the baseline using empirical averages sampled from the Q-function is studied. In experiments comparing baseline computation methods, using empirical averages from the Q-function and mean-action of the action-coordinate showed comparable performance. The latter performed slightly better towards the end of the learning process, indicating that errors from function approximation may degrade estimates. Sub-sampling from the Q-function could yield better results if accurate approximations are obtained for a large fraction of the action space, especially in high-dimensional problems. The action-dependent baseline improves convergence more for higher dimensional problems than lower dimensional problems, as shown in a simple synthetic example called m-DimTargetMatching. Variants of the baseline using sampling from the Q-function or mean action perform comparably, with the latter being more computationally efficient. The action-dependent baseline improves convergence more for higher dimensional problems, providing notable and consistent variance reduction compared to a linear baseline. The algorithm scales well computationally to high-dimensional problems, as demonstrated in a synthetic high-dimensional target matching task. In the extension of the core idea, a blind peg-insertion task and a two-agent particle environment task are studied. The blind peg-insertion task involves the robot searching for a hole on the table to insert a peg, with faster learning observed when the location of the hole is known. In the two-agent task, each agent aims to reach their goal with continuous communication. Including information from other agents into the action-dependent baseline improves training performance in multi-agent reinforcement learning. An action-dependent baseline allows bias-free variance reduction by using additional signals beyond the state. Analysis of the variance is provided, showing success percentage on the blind peg insertion task. The baseline, with access to goal information, helps speed up learning compared to policies acting solely on observations. The curr_chunk discusses the proposal of practical action-dependent baselines for continuous control tasks and high-dimensional action problems, aiming to minimize variance in policy gradient estimates. It also mentions the generalization of using additional signals beyond local state in various problem settings. The future work includes investigating related methods in large-scale problems. The curr_chunk discusses minimizing the covariance trace of the policy gradient by deriving an optimal action-dependent baseline. It focuses on reducing variance in policy gradient estimates through the use of random variables and notations. The curr_chunk discusses deriving optimal action-dependent baselines to minimize variance in policy gradient estimates using random variables and notations. The optimal action-dependent baseline is sought to minimize variance in policy gradient estimates. The reduction in variance is quantified, comparing the optimal baseline to other suboptimal or action-independent baselines. Variance improvements are defined, and calculations are made to determine the difference in variance using specific notations and equations. The text discusses the derivation of baselines for general actions in a more general case where policy actions are not conditionally independent across dimensions. It introduces a factorization formula and explains how baselines can be set based on factors in a graphical model. The overall gradient estimator is provided in the most general case. The overall gradient estimator for general actions is derived without conditional independence assumptions. This analysis extends to various application domains where the assumption does not hold, such as language tasks and continuous control tasks. Action-dependent baselines are computed for general actions, emphasizing the importance of policies without a full conditional independence structure. Action-dependent baselines can be trained for general actions without the need for conditional independence assumptions. In the general case, m individual baselines can be fitted from data collected in previous iterations, resulting in a method described in Algorithm 2. Special cases like conditional independent actions or block diagonal covariance structures may have more efficient baseline constructions. Sparse covariance structures are also an interesting example to explore. Algorithm 2 presents policy gradient for general factorization policies using action-dependent baselines. Baselines help interpolate between high-bias, low-variance and low-bias, high-variance estimates of the policy gradient by predicting future returns. The Generalized Advantage Estimation (GAE) method reduces variance by averaging temporal difference terms, allowing control over bias-variance trade-off. Temporal difference error with action-dependent baselines provides an unbiased estimator for the advantage function, enabling the use of GAE. The study shows that action-dependent baselines are consistent with TD procedures, with their temporal differences estimating the advantage function. Results suggest that slightly biasing the gradient for reduced variance yields better outcomes, while high-bias estimates perform poorly. There is potential to improve upon prior work by studying the bias-variance trade-off. Training curves for high-dimensional action spaces indicate the effectiveness of the action-dependent baseline. The study found that for higher dimensional action spaces, the action-dependent baseline converges to the optimal solution 10% faster than the state-only baseline. The experiment showed that due to training instability in TensorFlow, both baselines underperformed compared to a revised experiment. Parameters used in the experiments included \u03b3 = 0.995 and \u03bb GAE = 0.97. In the experiments, parameters such as \u03b3 = 0.995, \u03bb GAE = 0.97, kl desired = 0.025 were used. Policies were implemented as 2-layer fully connected networks with specific hidden sizes. The policy initialization involved Xavier initialization with final layer weights scaled down. The baseline is linear with RBF features and estimated with a Newton step, making the initialization inconsequential. The experiment configurations in TAB5 apply to both state-only and action-dependent versions, with m-DimTargetMatching experiments using a linear feature baseline."
}