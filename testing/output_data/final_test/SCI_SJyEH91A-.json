{
    "title": "SJyEH91A-",
    "content": "The Wasserstein distance is gaining attention in machine learning for comparing distributions, but its heavy computational cost limits its use. An approximation mechanism using an embedding with a siamese architecture and decoder network can mimic the Wasserstein distance with Euclidean distance. This allows for fast computation of optimization problems in the Wasserstein space. The Wasserstein distance is a powerful tool in machine learning for comparing data distributions. It has various applications in image processing, computer vision, and deep learning. Its benefits include operating on empirical data distributions in a non-parametric way and leveraging the geometry of the underlying space for comparison. The Wasserstein distance is used to construct objects like barycenters and geodesics in data analysis. It is defined on the space of Borel probability measures with finite moments of order p. The p-Wasserstein distance between two measures is defined using probabilistic couplings. When p = 1, it is known as Earth Mover's distance. The Wasserstein distance, particularly W 2, is commonly used in computer vision, computer graphics, and machine learning applications. However, its widespread use is limited due to heavy computational requirements. Various strategies have been proposed to address this issue, such as slicing techniques, entropic regularization, and stochastic optimization. In response to the heavy computational requirements of computing pairwise Wasserstein distances, a proposal to learn an Euclidean embedding of distributions is made in this work. This embedding, expressed as a deep neural network, approximates Wasserstein distances with the Euclidean norm, allowing for faster computation and construction of objects like barycenters. The strategy used is similar to Siamese networks. In response to the computational challenges of computing pairwise Wasserstein distances, a proposal is made to learn an Euclidean embedding of distributions using a strategy similar to Siamese networks. This embedding allows for faster computation and construction of objects like barycenters. The approach involves simultaneously learning the inverse of the embedding function to reconstruct a probability distribution from the embedding. The concept of metric embedding is discussed, where a new representation of data preserves distances from the original space for computational ease or visualization of high-dimensional datasets. Metric embedding involves mapping two metric spaces with a distortion coefficient \u03b1, aiming for a low distortion value for better quality. Isometric embedding is not always guaranteed, but possible. Embeddability is feasible with constant distortion. Wasserstein space embedding remains a theoretical question, with most guarantees using W 1. Recent results have shown that there is no meaningful embedding for Wasserstein distance W2 over R3 with constant approximation. Embedding pointsets of size n into L1 incurs a distortion of O(\u221alog n). Embeddability for pointsets in regularly sampled grids in Rk remains a theoretical question, with distortions between O(k log n) and \u2126(k + \u221alog n). For locally concentrated measures, a good approximation can be achieved using a technique that considers linear approximations of the transport problem in Wasserstein space. By linearizing the space and projecting onto the tangent space, faster computation of pairwise Wasserstein distances is possible. This method also allows for statistical analysis of the data by deriving an embedding from linear transport plans with a template element. Our work proposes to learn a generic embedding for data analysis, different from previous methods that rely on explicit approximations or transformations. This approach is adaptable to any type of data and allows for fast handling of unseen data without the need for new optimal transport plans. Our method, DWE (Deep Wasserstein Embedding), learns a new data representation in a supervised manner without the need for new optimal transport plans. Using a Siamese neural network ensures symmetry in the Wasserstein space for a meaningful embedding. The DWE method aims to learn a new data representation using a Siamese neural network for metric and similarity learning. The architecture replicates a network to map two samples from the same set to a new space with a contrastive loss, mainly used in computer vision for face recognition and one-shot learning. The objective is to mimic a specific distance with computation challenges by learning an embedding network that projects histograms into a Euclidean space, mirroring the geometrical properties of the Wasserstein space and adding a reconstruction loss for regularization. The embedding computation includes a reconstruction loss using a decoding network \u03c8, improving learning and generalization performance. This regularization loss aids in capturing input data information for good reconstruction. The use of a decoder network allows result interpretation, crucial for data-mining tasks. The global objective function involves data fitting terms weighted by \u03bb and Kullbach-Leibler divergence. The Wasserstein metric operates on probability distributions, and once functions \u03c6 and \u03c8 are learned, various data mining tasks can be performed in the Wasserstein space. The computational scheme has a wide range of applications where the Wasserstein distance is important, and although not an exact estimator, it competes well with other strategies in numerical experiments. Wasserstein barycenters in the space are defined as minimizers of a weighted sum of squared Wasserstein distances, and can be obtained in the framework presented. In our framework, barycenters can be obtained as DISPLAYFORM0 where x i are the data samples and the weights \u03b1 i obey constraints: i \u03b1 i = 1 and \u03b1 i > 0. The barycenter corresponds to a Wasserstein interpolation between two distributions with \u03b1 = [1 \u2212 t, t] and 0 \u2264 t \u2264 1 BID33. When weights are uniform, the barycenter is the Wasserstein population mean, also known as Fr\u00e9chet mean BID4. Principal Geodesic Analysis (PGA) in Wasserstein space is a generalization of PCA on Riemannian manifolds, aiming to find geodesic directions that encode data variability. PGA was introduced by Fletcher et al. BID18. Principal Geodesic Analysis (PGA) is a generalization of PCA on Riemannian manifolds, aiming to find geodesic directions that encode data variability. PGA extends important concepts like variance and geodesic subspaces to analyze distributional data in a space equipped with the Wasserstein metric. However, a direct application of Fletcher's original algorithm is challenging due to the complexity of the space of probability distributions. The proposed novel PGA approximation involves finding the approximate Fr\u00e9chet mean of the data and building a subspace in the embedding space through a maximization problem, equivalent to PCA. Reconstruction from the subspace to the original space is done through \u03c8, with a detailed analytical study postponed for future works. The proposed method involves finding the approximate Fr\u00e9chet mean of the data and building a subspace in the embedding space through a maximization problem, similar to PCA. The method is evaluated on grayscale images normalized as histograms, showcasing its performance in computer vision tasks. The framework includes an encoder \u03c6 and a decoder \u03c8, with the encoder producing the representation of input images h = \u03c6(x). The architecture for the embedding \u03c6 involves 2 convolutional layers with ReLU activations: a layer of 20 filters (3x3 kernel) and a layer of 5 filters (5x5 kernel). This is followed by two linear dense layers of size 100 and a final layer of size 50. The reconstruction architecture \u03c8 includes a dense layer of output 100 with ReLU activation, followed by a dense layer of output 5*784. The decoder then outputs a reconstruction image of shape 28x28. The decoder outputs a reconstruction image of shape 28 by 28 for grayscale images normalized as histograms. A p pseudo-norm regularization with p = 1/2 is applied to promote sparse output in handwritten data. The first numerical experiment is on the MNIST digits dataset containing 28x28 images. The Wasserstein distance is computed on the MNIST digits dataset using a neural network model. Training data is created by randomly selecting one million pairs of indexes from 60,000 samples. The model achieves good precision with a test MSE of 0.4 and a relative MSE of 2e-3, with a correlation of 0.996. The approach shows a good approximation of W 2 2 with a test MSE of 0.4 and a relative MSE of 2e-3, correlation of 0.996, and small uncertainty. The ability to compute W 2 2 efficiently is investigated by measuring the average speed of computation on a test dataset. Two ways to compute W 2 2 are discussed: Indep and Pairwise, based on squared Euclidean norm in the embedding space. The second computation method efficiently computes pairwise Wasserstein distances between samples, allowing for faster retrieval in large datasets. The method shows impressive speedup on both CPU and GPU, making it suitable for mining large-scale datasets and online applications. Our method is efficient for mining large datasets and online applications. We evaluate it by computing Wasserstein Barycenters for each class of the MNIST dataset using 1000 samples per class. The resulting barycenters are sensible, sharp, and computationally efficient, taking only 20ms per barycenter. Additionally, Principal Geodesic Analysis for classes 0, 1, and 4 from the MNIST dataset is shown for squared Euclidean distance and Deep Wasserstein Embedding. The Principal Geodesic Analysis using Wasserstein encoding shows more detailed and nonlinear subspaces for classes in the MNIST dataset. The Google Doodle dataset, containing 50 million drawings, is freely available online. The Google Doodle dataset, available online, consists of 50 million hand-drawn images collected from users. The drawings are simplified into 28x28 grayscale images aligned to the center of the bounding box. In this study, classes like Cat, Crab, and Faces were downloaded to learn a Wasserstein embedding using the same architecture as MNIST. Training datasets were created by randomly selecting 1 million pairs of indexes from each category and computing the Wasserstein distance with squared Euclidean ground metric. The doodle dataset is more challenging than MNIST due to its uncurated nature and diverse unfinished doodles. Cross dataset comparisons show a loss in prediction accuracy, especially between doodle datasets with high variety. MNIST's structured nature requires a representative dataset for good generalization, unlike the doodle datasets. The method used finds a data-dependent embedding specific to each dataset. Our method finds a data-dependent embedding specific to the geometry of the learning set. Qualitative evaluation of the subspace learned by DWE includes comparing Wasserstein interpolation with other methods like regularized OT. LP solver is slow but noisy, regularized barycenter is smooth but may lose details, while our reconstruction is fast but may lose some details due to Auto-Encoder error. The computational approximation of the Wasserstein distance presented in this work allows for fast data mining tasks with a cheap computational price. While the method is practical, questions remain about its theoretical guarantees and approximation quality, particularly regarding the complexity of the network architecture and the data at hand. Theoretical existence results on Wasserstein embedding with constant distortion are lacking. Future works will explore applications of the approximation strategy on various ground loss and data mining tasks, as well as the transferability of databases to reduce computational burden. The role of the decoder in interpreting results and acting as a regularizer is discussed, with DWE achieving lower MSE with the decoder in training on MNIST. In FIG5, DWE achieves a lower MSE with the decoder, illustrating examples from the dataset in FIG6, including outlier images. Wasserstein interpolation is computed between samples in FIG7, showing a continuous displacement of mass. Surprising artefacts are observed, such as the eye of a face fusing with the border. Quantitative evaluation for DWE on three datasets is shown in FIG9. The quantitative evaluation for DWE on three datasets is presented in Table 1, showing MSE performances and the advantage of using W^2 for image retrieval. Nearest neighbor walk results in a smooth trajectory with DWE neighbors, contrasting with L2 distance in input space which is sensitive to outliers. This highlights the computational benefits of DWE for image retrieval."
}