{
    "title": "rJm7VfZA-",
    "content": "Multiagent systems can be formalized as stochastic games, with a subclass known as Markov potential games (MPGs) commonly used in economic and engineering applications. This study focuses on MPGs with continuous state-action variables, coupled constraints, and nonconvex rewards. Previous analysis was limited to simple cases, but this research presents a closed-loop analysis for MPGs with parametric policies that adapt to stochastic transitions. The study provides conditions for a stochastic game to be a Markov potential game (MPG) and shows that a closed-loop Nash equilibrium can be approximated by solving an optimal control problem. This approach is an improvement over previous methods and is illustrated with an example in a noncooperative communications engineering game. The game is then solved using a deep reinforcement learning algorithm. In a noncooperative stochastic dynamic game, agents compete in a time-varying environment with state-transition probabilities and reward functions. The game is solved using deep reinforcement learning to approximate a variational Nash equilibrium. The aim is for each agent to find the optimal policy over an infinite time horizon. In a noncooperative stochastic dynamic game, agents compete in a time-varying environment with state-transition probabilities and reward functions. The game is solved using deep reinforcement learning to approximate a variational Nash equilibrium. Agents aim to find the policy that maximizes expected long term return over an infinite time horizon. The game can be represented as a set of coupled optimal-control-problems (OCPs), analyzed for open-loop (OL) or closed-loop (CL) cases. In OL, actions are determined without feedback, while in CL, actions are based on feedback from the environment at each time step. In dynamic games, closed-loop (CL) solutions are preferred over open-loop (OL) solutions due to their ability to adapt to stochastic dynamics. CL equilibria involve agents considering opponents' reactions to deviations from the equilibrium trajectory, visualized as sets of state-action trajectories or trees. The sets of OL and CL equilibria differ even in deterministic dynamic games. The CL analysis of dynamic games with continuous variables is challenging, especially when considering coupled constraints. Generalized Nash equilibrium problems arise when agents interact strategically with both rewards and feasible sets. Markov potential games (MPGs) simplify finding Nash equilibria by solving a single optimal control problem (OCP), making them beneficial for economics and engineering applications with shared resources. In this work, the authors extend previous analysis on Markov potential games (MPGs) with coupled constraints. They propose a practical method for finding CL Nash equilibrium (CL-NE) for continuous MPGs, which has not been done before. The analysis considers agents' policies in a parametric set and simplifies derivations under potentiality conditions on reward functions. In this work, the authors extend previous analysis on Markov potential games (MPGs) with coupled constraints by proving that, under certain conditions on reward functions, a game is an MPG. They show that the Nash equilibrium for the approximate game can be found as an optimal policy of a related OCP, which is a practical approach for finding or approximating NE. This parametric solution can approximate an equilibrium of the original MPG well, and the same idea can be extended to MPGs in a principled manner. The text discusses extending the concept of Markov potential games (MPGs) to a more general setting using reinforcement learning techniques. It highlights the application of deep reinforcement learning to cooperative Markov games and provides conditions for a stochastic game to be considered an MPG. The text also mentions finding a closed-loop Nash equilibrium by solving an optimal control problem with a single-objective reward function. The proposed approach applies Deep Reinforcement Learning (DRL) to a noncooperative Markov game in a communications engineering application. It demonstrates the differences from the standard approach by solving a resource sharing game analytically. The set of agents is denoted by N, with actions represented by real vectors. The actions of all agents form a Cartesian product, denoted by A. The vector A contains actions of all agents at time i, denoted as ai \u2208 A. The set X represents the states of the game, with state transitions determined by a probability distribution. Constraint functions are included in the model. The general stochastic game involves a multiobjective variational problem with design space \u2126 and objective space R N. The policy spaces for all agents are denoted by \u2126, with policies \u03c0 k and \u03c0. The game includes constraints, state transitions, and instantaneous rewards for each agent. The multiobjective variational problem in the stochastic game involves finding a stationary policy that maximizes expected rewards while satisfying constraints. To address the challenge of finding a policy that maximizes every agent's objective, a Nash equilibrium solution concept is used. Instead of seeking a variational NE solution, a more manageable approach is proposed by restricting policies to a finite-dimensional parametric family. This introduces a set of parametric policies, \u2126 w, defined by parameter w \u2208 W, where each policy maps states to actions. The parametric closed-loop Nash equilibrium (PCL-NE) is a solution concept in a multiobjective optimization problem in a stochastic game. It involves finding a parametric policy where no agent has an incentive to deviate unilaterally. This concept is achieved by constraining the policy set to a finite-dimensional parametric family, \u2126 w, defined by parameter w \u2208 W. The PCL-NE approximates the performance of an exact variational equilibrium when the parametric family has high expressive capacity. Assumptions include nonempty and convex state and parameter sets, twice continuously differentiable reward functions, and regularity conditions for the state-transition function and constraints. Assumption 4 ensures proper reward functions and bounded level sets. These assumptions are common in engineering applications and are necessary for establishing optimality conditions. In this section, the standard approach for CL dynamic games is reviewed. The policy is expressed in reduced form, assuming the existence of a function h. The reward in reduced-form is obtained, and optimality conditions are derived from concave reward functions. The standard approach for analyzing CL dynamic games involves guessing parametric policies from a space of functions and checking if they satisfy optimality conditions. This method, illustrated with the \"great fish war\" game, has drawbacks such as difficulties in handling constraints and finding specific parametric forms that meet optimality conditions. In order to address the challenges of analyzing CL dynamic games, a new approach is proposed. By constraining the set of policies to a parametric family and deriving optimality conditions for this parametric problem, a more efficient method is introduced. This method, known as MPG with parametric policies, allows for the use of standard DRL techniques to find solutions for related OCPs. This extends the OL analysis of BID25 to the CL case and provides verifiable conditions for parametric approximate games. The main contribution of this paper is showing that in a parametric OCP, a parametric approximate game can be an MPG in the CL setting. When the game is an MPG, a PCL-NE can be found by solving the parametric OCP with a specific objective function. The potential function J shared by all agents plays a key role in this process. The generic form of a parametric OCP involves replacing multiple objectives with a single potential J, making it easier to solve. The relationship between G and P is formalized in Theorem 1, which shows how to obtain J. The potential J is the instantaneous reward for the OCP, given by a line integral formula. The potential function J is obtained through a line integral of a vector field with components as the partial derivatives of agents' rewards. Theorem 1 establishes the relationship between the potential function and the parametric OCP, allowing for the formulation and solution of the related OCP for any specific parametric policy family. This approach offers an improvement over the standard method by potentially yielding the same equilibrium or uncovering additional equilibria not found in the standard approach. Our approach provides an approximation of an exact variational equilibrium, with more accuracy for expressive parametric families like deep neural networks. In Appendix B, we solve \"the great fish war\" game, yielding the same solution as the standard approach. Evaluating J may be challenging if the parametric family is involved, but visual inspection can easily obtain J. Results show that J can be easily obtained by visual inspection. Cooperative games with a common reward are MPGs, where the potential equals the reward. Noncooperative games have a separable term common to all agents' reward functions. A game is an MPG in the CL setting if all agents' policies depend on disjoint subsets of components of the state vector. Theorem 2 states that a game is an MPG if the reward function of each agent can be expressed as a common term plus another term that does not depend on its own state-component vector or policy parameter. If a certain condition is met, the common term equals the potential function. The common term in the reward function equals the potential function if condition (26) holds. This condition applies in cases such as when agents have disjoint state-component subsets or when the policy is a predefined sequence of actions. Theorem 2 highlights that a dynamic game may not be potential in the CL parametric setting even if it is potential in the OL case. In this section, it is shown that there exists a deterministic policy achieving the optimal value of P1, which is also a Nash Equilibrium (NE) of G2 under certain conditions. The proposed MPGs framework is used to learn an equilibrium in a communications engineering application by extending a Medium Access Control (MAC) game to stochastic dynamics and rewards. The Trust Region Policy Optimization (TRPO) algorithm is employed to learn a policy that is a PCL-NE of the game. In a MAC uplink scenario with N = 4 agents, each user sets transmitter power to maximize data rate and battery lifespan. Interference occurs if multiple users transmit simultaneously, leading to inefficient battery use. The system state is defined by all user's battery levels, with each agent's battery depletion considered. The game in the MAC uplink scenario with 4 agents is formalized with a potential function and single objective. The optimal policy aims to deplete the battery in finite time, leading to a stationary episode. The game in the MAC uplink scenario with 4 agents aims to deplete the battery in finite time, leading to a stationary episode. The reward is chosen to be convex for benchmarking with a DRL algorithm. Random variables are included by generating independent sequences of samples. The optimal value of the stochastic OCP was estimated using two different approaches. The first approach yielded V cvx = 33.19, considered as an estimator, while the second approach resulted in V avg,cvx = 34.90, considered as an upper bound estimate. The batteries consistently depleted below x T < 10 \u22126, validating a time horizon of T = 100 steps. The proposed approach allows learning a PCL-NE using DRL methods without prior knowledge of the game dynamics and rewards. DRL methods like TRPO, DDPG, or A3C learn by interacting with a simulator without prior knowledge of reward functions. TRPO uses a neural network with 3 hidden layers and achieves an optimal value of V trpo = 32.34 after 400 iterations. The text discusses the application of TRPO, a DRL method, to find a PCL-NE in a resource-sharing game called \"the great fish war\". The approach involves solving an OCP to establish a relationship between a MPG and an NE, leading to near-optimal results. In the context of applying TRPO to find a PCL-NE in the \"great fish war\" game, the text discusses solving a game involving N countries aiming to maximize rewards from fish consumption. The standard method involves guessing parametric functions to replace policies and checking if they satisfy certain equations, leading to a solution with parameters. The text discusses the solution for a game involving N countries maximizing rewards from fish consumption. The proposed approach provides parameters for linear policies that constitute an equilibrium. An example using the \"great fish war\" game illustrates the application of this approach, yielding the same results as the standard method. In the \"great fish war\" example, the linear policy mapping is used to compare results with the standard approach. Conditions are verified to conclude that the game is an MPG. The OCP is solved with a potential function, and the Euler-Lagrange equation is applied for optimality. The optimality condition for the policy parameter is solved by obtaining the parameters through a set of equations. The approach allows for flexibility in choosing different parametric forms for the policy, ensuring a solution to the KKT system of the approximate game. The more expressive the parametric family, the better the approximation of the optimal policy in the game. The KKT systems for the game and the OCP with parametric policies are built, with stochastic Euler-Lagrange equations applied to each agent's Lagrangian. The KKT system of optimality conditions for the OCP is derived using stochastic Euler-Lagrange equations and the Lagrangian, with multipliers as random variables. The system is obtained for all agents and time steps, including the policy parameter. The KKT system of optimality conditions for the OCP is derived using stochastic Euler-Lagrange equations and the Lagrangian, with multipliers as random variables. By comparing different formulas, it is concluded that both KKT systems are equal under certain conditions. The existence of primal and dual variables is guaranteed, leading to a system of equations where user strategies are the only unknowns. The OCP primal solution satisfies the KKT necessary conditions of the game and is also a PCL-NE of the MPG. The primal solution of the OCP is a PCL-NE of the MPG. A vector field is introduced, and conditions are equivalent to certain formulas. The game and OCP are rewritten with explicit actions and dual variables. The KKT systems for both are equal under certain conditions. The reformulation of the OCP involves introducing new dual variables for equality constraints and ensuring first-order conditions hold. The gradient is taken with respect to specific components, leading to a sequence of conditions. The variable change is analyzed using the chain rule, and the proper function J is upper bounded. This results in level sets being bounded as well. From Assumption 2 and the fundamental theorem of calculus, we deduce that J is continuous, leading to compact level sets. This allows us to use Bertsekas (2007) to ensure the existence of an optimal policy."
}