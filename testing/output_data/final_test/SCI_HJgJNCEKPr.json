{
    "title": "HJgJNCEKPr",
    "content": "There is a growing interest in automated neural architecture search (NAS) using weight sharing methods. However, a model performing better with shared weights may not perform better when trained alone. This paper introduces Posterior Convergent NAS (PC-NAS) to address the posterior fading problem in weight sharing NAS approaches. PC-NAS achieves state-of-the-art performance under GPU latency constraints on ImageNet, with the model PC-NAS-S attaining 76.8% accuracy in a small search space. PC-NAS-S achieves 76.8% accuracy on ImageNet within GPU latency constraints. PC-NAS-L reaches 78.1% accuracy in a larger search space. The architecture transfers well to other computer vision tasks. Automated NAS solutions aim to streamline neural network design, reducing the need for extensive human experimentation. Initial NAS works faced limitations due to computational costs, but recent efforts have improved search efficiency. Automated NAS solutions aim to streamline neural network design by improving search efficiency. Two categories of approaches include continuous relaxation and one-shot methods, each with their own challenges and benefits. Automated NAS solutions streamline neural network design by improving search efficiency. One-shot methods, like the approach by Brock et al., divide NAS into training and searching stages. The fairness among models is ensured by sampling architectures or dropping out operators uniformly. However, the problem with one-shot methods is that the validation accuracy may not predict true performance. This paper formulates NAS as a Bayesian model selection problem, offering insights into one-shot approaches. This paper discusses the formulation of NAS as a Bayesian model selection problem, highlighting the issue of weight sharing and Posterior Fading in one-shot approaches. It proposes a practical approach to guide the convergence of the proxy distribution towards the true parameter posterior. The paper introduces a novel NAS algorithm that addresses the issue of Posterior Fading in one-shot approaches by guiding the proxy distribution towards the true parameter posterior. The newly proposed NAS algorithm addresses Posterior Fading by guiding the proxy distribution towards the true parameter posterior. Benchmarking on ImageNet shows PC-NAS-S outperforms EfficientNet-B0 with 76.8% accuracy and 20% faster speed. PC-NAS-L achieves 78.1% accuracy in a larger search space, showcasing the method's advantage over traditional NAS methods involving reinforcement learning or neuro-evolution. Recent works have aimed to reduce the computational cost of Neural Architecture Search (NAS) by treating it as a single training process of an over-parameterized network where weights are shared. ENAS and One-shot NAS are examples of methods that have significantly decreased computation costs, with ENAS focusing on small-scale datasets like CIFAR10. Recent works in Neural Architecture Search (NAS) aim to reduce computational costs by using over-parameterized networks. DARTS introduces real-valued architecture parameters for operators, while ProxylessNAS binarizes these parameters to save GPU resources. However, biases may arise as models performing well initially tend to be favored. Only one operator in each mixop is used per batch. In the pool, choices from mixop 1 and 2 are extended to form mixop 3. Candidate models are evaluated and ranked, with the top-2 models forming a new pool. The PC-NAS algorithm is introduced in a Bayesian manner to optimize architecture search, focusing on model rankings and operator evaluation on the validation set. This approach improves efficiency compared to previous methods and is combined with latency analysis. The Bayesian approach involves assigning prior probabilities to model parameters and models themselves, with a focus on maximizing test accuracy. The parameter prior is crucial for model selection, aiming to achieve the highest accuracy in a standalone training manner. The search space for network architecture in NAS literature is defined by mixed operators connected sequentially, with parameters shared among multiple architectures. The true parameter posterior refers to the distribution of parameters when trained alone on a dataset. The search space for network architecture in NAS literature involves mixed operators connected sequentially, with shared parameters among architectures. The one-shot method trains the supergraph by sampling different architectures and utilizing shared weights to evaluate single models. Training the supergraph involves sampling multiple models and optimizing them with a mini-batch of data, minimizing the objective function to make maximum likelihood estimates. Training multiple models with shared parameters corresponds to a stochastic approximation of the objective function. Maximizing the parameter posterior is equivalent to minimizing a specific loss function. Assuming independence of parameters at each layer, the KL-divergence between different architectures leads to Posterior Fading. The non-predictive problem arises from one-shot supergraph training due to a vast search space of architectures. The PC-NAS algorithm aims to reduce the number of architectures during training to decrease KL divergence. It adopts progressive search space shrinking by dividing training into intervals based on the number of mixed operators in the search space. This approach mitigates the posterior fading problem in NAS. The number of training epochs of a single interval is denoted as Ti. Partial model pool is a collection of partial models. At the l-th interval, each partial model in the pool will be extended by the N operators in l-th mixop, resulting in P \u00d7 N candidate extended partial models with length l. The top-P among these candidates are used as the partial model pool for the next interval. The potential of a partial model is defined as the expected validation accuracy of models containing the partial model, estimated by sampling valid models and computing their average validation accuracy using one mini-batch. The potential of a partial model is stable and discriminative among candidates when the evaluation number is large enough. The training iteration involves randomly sampling missing operators to complement the partial model to a full model and optimizing using mini-batch data. Initially, the supergraph is trained with uniformly sampled models before operators are added to the partial model pool. The training process involves evaluating operators in the first mixop, selecting top P operators for the partial model pool in the second stage, and resuming training with warm-up epochs. The number of models in the search space decreases at each interval, with the final pool having P-1 cross-entropy terms for each architecture. This helps the parameter posterior of PC-NAS move towards the true posterior. Operators in G are evaluated, and models are trained with warm-up epochs. The study focuses on high-accuracy models under GPU latency constraints using PC-NAS. Models with the highest potential are selected from a partial model pool and tested on ImageNet. Transferability is assessed on object detection and person re-identification tasks. 50,000 images are randomly sampled for dataset and latency measurement. Latency measurement is done by randomly sampling 50,000 images for dataset validation. PC-NAS is conducted on the remaining train set images. The model's performance is evaluated on the original validation set. Nvidia GTX 1080Ti is used for latency evaluation with a batch size of 16. Two search spaces are utilized, a small space similar to ProxylessNAS and FBNet, and a larger space with additional operators. PC-NAS is used to search in both spaces with S = 900 evaluations and P = 5 partial model pool size. The study conducted experiments with S = 900 and P = 5 in the model pool. They used mini-batch nesterov SGD optimizer with momentum 0.9, cosine learning rate decay, batch size 512, and L2 regularization. The supergraph training lasted 200 epochs with warmup epochs set at 100. The model performance on ImageNet was evaluated, achieving 76.8% accuracy under a latency constraint of 10ms. The search result in the small space outperformed EfficientNet-B0 by 0.5% in absolute accuracy improvement. Our search result from the large space, PC-NAS-L, achieves 78.1% top-1 accuracy, improving by 1.8% compared to EfficientNet-B0 and 2.3% compared to MixNet-S. PC-NAS-S and PC-NAS-L are faster than EfficientNet-B0 and MixNet-S. Transferability of PC-NAS is validated on object detection and person re-identification tasks using COCO and Market-1501 datasets. PC-NAS-L pretrained on ImageNet is used as a feature extractor, showing improved performance compared to other models. In this section, the performance of the PC-NAS model on COCO and Market-1501 datasets is discussed. The model surpasses MobileNetV2 and ResNet50 in mAP on COCO, with comparable quality to ResNet101 but with fewer parameters and faster speed. Hyperparameters like warm-up, training epochs, model pool size, and evaluation number are studied on ImageNet. Setting warm-up training epochs as 100 is chosen to save computation resources, with no significant difference in accuracy observed for different settings of P and S. The results in Fig. 2a show that the top-1 accuracy of models found by PC-NAS increases with both P and S. Choosing P = 5, S = 900 for better performance in experiments. No significant improvement observed with further increasing hyperparameters. Training the supergraph without space shrinking leads to 77.1% top-1 accuracy on ImageNet. Increasing P or S values did not significantly improve results. Search method alone already shows impressive performance, and space shrinking strategy improves model ranking. Our space shrinking training strategy improves model ranking and boosts accuracy by 0.7%-1%. Comparing PC-NAS and One-Shot methods, PC-NAS can rank models more accurately based on shared weights. The effectiveness of our search method is demonstrated by utilizing Evolution Algorithm (EA). The Evolution Algorithm (EA) is used to search for models in a large space trained with One-Shot. The top-1 accuracy of the discovered model drops to 75.9%, 2.2% lower than PC-NAS-L. EA is implemented with a population size of 5, aligned with the pool size in the method. The mutation operation randomly replaces operators in mixop operators. The total number of validation images in EA is constrained. The paper introduces a new architecture search approach called PC-NAS, addressing issues with shared weights from a Bayesian perspective. The search space for operators in the method includes Conv1x1-ConvNxM-Conv1x1 and Conv1x1-ConvNxM-ConvMxN-Conv1x1 structures. The small search space consists of MBConv operators with different kernel sizes and expand ratios, while the large search space adds NConv, DConv, and RConv operators with various kernel sizes and expand ratios. The large space contains 20 operators in total. The search space includes operators with different structures and sizes. The large search space contains 20 operators, with specific kernel sizes and expand ratios listed in Table 5. The small search space has 10^21 models, while the large one has 20^21 models. PC-NAS-S and PC-NAS-L have different specifications, with PC-NAS-S focusing on lightweight models and PC-NAS-L choosing powerful features. PC-NAS-L selects powerful bottlenecks in a large space to boost accuracy, using heavy operators when resolution decreases to prevent information loss. The model maintains low expansion rates for lightweight design."
}