{
    "title": "S1lTg3RcFm",
    "content": "In POMDPs, observations are typically from a fixed distribution, but in real-world scenarios, agents can actively choose their observations. A greedy strategy for observation selection is proposed to minimize state uncertainty and avoid computational complexity. The novel point-based value iteration algorithm incorporates a greedy strategy for near-optimal uncertainty reduction in belief points, separating perception from planning efficiently in POMDPs. In the era of information explosion, decision-making platforms must extract useful data for tasks like active perception and planning in robotics. Partially observable Markov decision processes (POMDPs) are crucial for modeling sequential decision-making with partial perception and stochastic outcomes in various applications. The use of POMDPs in modeling real-world problems has led to research on efficient algorithms for finding near-optimal policies. Previous work on POMDPs often focuses on perception or planning alone, but integrating both can be computationally challenging. By restricting perception to subset selection problems and using submodular optimization, it is possible to partially separate perception and planning while considering their impact on policy value. This work explores joint perception and planning in POMDPs, specifically involving decision-making on two sets of actions. In POMDPs, an agent makes decisions on perception and planning actions. Perception actions affect the agent's belief about the environment, while planning actions impact the environment's state transition. In subset selection problems, the agent selects information sources along planning actions due to constraints. This problem is common in control systems, signal processing, wireless sensor networks, and machine learning. Previous work assumed known planning strategies, but this work focuses on learning a selection strategy simultaneously. In POMDPs, the agent makes decisions on perception and planning actions. Previous work assumed known planning strategies, but this work focuses on learning a selection strategy simultaneously. Exact POMDP solvers optimize the value function over all reachable belief points, but finding exact solutions is computationally intractable. Sampling a finite set of belief points and applying value iteration is a common technique for near-optimal algorithms. SARSOP aims to minimize the gap between lower and upper bounds on the value function by guiding sampling towards belief points. The proposed greedy observation selection scheme in active perception leads to belief points close to optimal observations for uncertainty reduction. Previous work includes reinforcement learning and semi-definite programming for sensor selection, with some lacking theoretical guarantees. In the Kalman filtering setting, a greedy selection scheme with near-optimal guarantee has been developed to minimize error covariance matrix determinant. Active perception has been modeled as a POMDP in prior work. The main difference in our work compared to prior research is that we view active perception as a means to achieve the original task, while previous works treat active perception as the task itself. The problem of selecting the optimal set of sensors under a cardinality constraint is known to be NP-hard. The NP-hard problem of selecting a subset of sensors under a cardinality constraint has led to the development of greedy algorithms that achieve good approximation factors. Various studies have focused on maximizing different submodular information-theoretic objectives for sensor selection. One approach involves using the entropy of belief to reduce uncertainty in state by selecting sensors that lead to maximum expected entropy reduction. The proposed objective aims to minimize entropy by utilizing monotonicity and submodularity, achieving near-optimal approximation factor. Contributions include formulating the active perception problem for POMDPs and developing a perception-aware point-based value iteration algorithm for solving AP2-POMDP efficiently. The section introduces the concept of AP2-POMDP, a model that incorporates active perception actions alongside planning actions. It defines the finite sets of states and actions, emphasizing the role of perception actions in determining which subset of sensors the agent should receive. The AP2-POMDP model combines planning actions (Apl) and perception actions (Apr) in an n-dimensional lattice. Perception actions select sensors, with a maximum of k sensors to be chosen. The model includes a probabilistic transition function (DISPLAYFORM1), a set of observations (\u2126), a probabilistic observation function (O), and a reward function (R). The agent selects an action \u03b2 from Apl, transitions to a state s with probability Pr(s|s, \u03b2), selects k sensors by \u03b4 from Apr, receives an observation \u03c9 from \u2126, and obtains a reward R(s, \u03b2) at each time step. The agent's belief at each time step is represented by the posterior probability distribution of states given the history of actions and observations. The belief is updated using a well-known equation after taking an action and receiving an observation. The model assumes that observations from sensors are independent, and the belief is a sufficient statistic to represent the history of actions and observations. The goal is to learn a deterministic policy to maximize value function V, which converges to the optimal value function V* satisfying Bellman's optimality equation. The optimal policy can be derived once V* is learned, and it is noted that the value function is piecewise-linear and convex at any horizon. The text discusses the representation of value functions using hyperplanes and the use of approximate point-based solvers. It also introduces the concepts of submodular optimization and defines monotonicity and submodularity. The problem statement is then presented. ACTIVE PERCEPTION WITH GREEDY SCHEME: In a AP 2 -POMDP setting, the goal is to maximize the expected cumulative reward by learning a policy that includes perception actions. However, the complexity of selecting the optimal subset of sensors makes it challenging to apply existing POMDP solvers directly. To address this, a greedy strategy for selecting perception actions is proposed to efficiently pick sensors that enhance the belief and reward outcomes. The greedy strategy for selecting perception actions aims to minimize uncertainty about the state by picking sensors that do not affect the transition. This involves decomposing the belief update into two steps and quantifying uncertainty using Shannon entropy. The entropy is highest at the center of the belief simplex, indicating maximum uncertainty. The text discusses optimizing conditional entropy to minimize uncertainty about the state when selecting sensors. A greedy algorithm is proposed to find a near-optimal solution efficiently. The text introduces a greedy algorithm to efficiently find a near-optimal solution for minimizing uncertainty in sensor selection. It also discusses a theoretical guarantee for the algorithm's performance based on the properties of conditional entropy. The text discusses the properties of a function f(\u03b6) in relation to sensor selection, including its monotonicity and submodularity. It also presents a performance guarantee for a greedy algorithm in minimizing uncertainty by pushing belief towards higher value regions in belief space. The text discusses the properties of a function in relation to sensor selection and presents a performance guarantee for a greedy algorithm in minimizing uncertainty by pushing belief towards higher value regions in belief space. It shows that the value from the greedy scheme is close to the optimal observation selection value, with the distance between belief points and the difference in value functions being upper-bounded. In this section, a novel point-based value iteration algorithm is proposed for AP 2 -POMDPs, building on the performance guarantee of a greedy observation selection method. The proof of the algorithm's effectiveness is outlined in the appendix. The point-based solver operates by updating belief points and \u03b1 vectors through Bellman backup and pruning, sampling new belief points until convergence. Sampling depends on the reachability tree of belief space. State-of-the-art methods aim for good coverage without traversing the entire tree, considering the exponential growth in the reachability tree due to observation selection. The proposed method modifies the BackUp step of point-based value iteration to make the choice of \u03b4 dependent on \u03b2 and previous belief. This allows for the decoupling of perception action computation from planning policy learning.\u03b4 is computed based on greedy maximization, uniquely determined by b and \u03b2. This approach aims to improve efficiency in solving AP 2 -POMDPs. The BackUp step in Algorithm 3 computes new \u03b1 vectors from previous ones using Bellman backup operation, including perception actions. Greedy perception actions are computed for each belief point and action, affecting the computation of \u0393 b,\u03b2,\u03c9 t. This complexity is lower than concatenating perception and planning actions. The proposed algorithm for active perception and planning uses a point-based value iteration solver for AP 2 -POMDPs. The point-based value iteration solver for AP 2 -POMDPs was developed, with belief set initialized by uniform sampling. Alpha vectors were initialized using a specific formula, and a randomized backup step was employed for faster solving. The solver terminates when the difference between value functions falls below a threshold. A random perception policy was implemented for selecting information sources. The simulations were run on a laptop with specific hardware specifications. The robot, with probabilistic transitions and no sensors, navigates in a grid using cameras for localization. Each cell has a camera providing a probability distribution for the robot's position. The robot's goal is to reach the star-labeled goal state while avoiding obstacles. The robot uses cameras for localization in a grid, aiming to reach a specific cell labeled by a star while avoiding obstacles. The camera's accuracy is higher when the robot is closer to it, modeled by a binomial distribution. The robot selects navigation actions and cameras to reach its goal, with policy evaluation through Monte Carlo simulations. Initial state is at the map's origin with uniform belief. Results show discounted cumulative rewards for random and greedy selection of information sources. The robot uses cameras for localization in a grid to reach a specific cell while avoiding obstacles. Results show that the greedy perception policy outperforms random perception, with less uncertainty in planning actions. In a 2-D map setting, the robot must navigate using actions {up, right, down, left, stop} and avoid obstacles. The reward system includes +10 at the goal, -4 at obstacles, and -1 elsewhere. The proposed point-based solver was applied with both random and greedy perception in the 2-D example, running for 25 steps. In this paper, joint active perception and planning in POMDP models were studied. AP 2 -POMDPs were introduced to pick a subset of observations along with planning actions. A greedy scheme for observation selection was proposed to minimize uncertainty about the state. Theoretical analysis showed boundedness of value function difference for the greedy algorithm. Further results can be found in the appendix. Theoretical analysis of the greedy algorithm showed boundedness of value function difference for optimal entropy reduction. A point-based value iteration solver for AP 2 -POMDPs was developed based on this analysis. The solver's approach to active perception is general and can be applied to state-of-the-art solvers. Implementation and evaluation were done on various robotic navigation scenarios, with proofs provided for lemmas and theorems in the paper. Theoretical analysis of the greedy algorithm for optimal entropy reduction in AP 2 -POMDPs showed boundedness of value function difference. The algorithm's approach to active perception is general and applicable to state-of-the-art solvers. Implementation and evaluation were conducted on robotic navigation scenarios, with proofs for lemmas and theorems provided in the paper. The current chunk discusses the properties of the function f (\u03b6) in relation to conditional entropy and monotonicity. The conditional entropy decreases as the number of observations increases, showing diminishing returns. The greedy algorithm's performance is analyzed based on the properties of the objective function. Theorem 1 provides a performance guarantee for the optimal subset of observations and the output of the greedy algorithm. The greedy algorithm's performance guarantee is based on the properties of the objective function. Mutual information is a measure of dependence between random variables, inspiring subset selection algorithms. Minimizing conditional entropy is equivalent to selecting observations that decrease entropy. The mutual information of state and a set of observations can be maximized by minimizing conditional entropy, which is equivalent to selecting observations that decrease entropy. KL-divergence measures the difference between two distributions and is closely related to mutual information. The mutual information of state and a set of observations can be maximized by minimizing conditional entropy, which is equivalent to selecting observations that decrease entropy. KL-divergence measures the difference between two distributions and is closely related to mutual information. Proposition 2 states that the mutual information is the expected value of the KL-divergence from prior belief to posterior belief over all realizations of observations. This establishes a relation between minimizing conditional entropy and maximizing the expected KL-divergence. The I-projection of a center p with radius \u03c1 on \u039b is defined. Lemma 2 defines p0, pg, and p*. The inequality on expectation is proven by considering the set \u039bg containing distributions with entropy lower-bounded by pg. The projection \u039bg onto \u0394B is convex. It is shown that p0 \u2208 \u039bg and p* \u2208 \u0394B\\\u039bg. The figure demonstrates these concepts for an alphabet of size 3. pg is the I-projection of p* on \u039bg. By using the analogue of Pythagoras' theorem for KL-divergence, it is concluded that. The computational complexity of a point-based value iteration method is compared with a proposed method using a greedy approach. The KL-divergence between greedy and optimal selection strategies is upper-bounded, with the proof involving the gradient of the optimal value function and H\u00f6lder's inequality. The desired result is obtained by setting a constant C2. The proposed point-based method selects perception actions greedily, computing the computations for a single backup step with a concatenated action space. The complexity of the backup step is O(\u03a9^k \u00d7 |S|^2), while the greedy algorithm requires O(n \u00d7 k) calls to compute the objective function. The proposed point-based method selects perception actions greedily, leading to significant computational gain, especially for large n. The greedy selection results in smaller entropy and less uncertainty about the state, with better average discounted cumulative rewards compared to random selection. The proposed method demonstrates superiority by analyzing the effect of selected cameras on agent performance in a 1-D navigation scenario. The value function shows diminishing returns of entropy with the number of observations selected."
}