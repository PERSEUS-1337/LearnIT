{
    "title": "BkCV_W-AZ",
    "content": "In this work, a new deep reinforcement learning algorithm based on counterfactual regret minimization is proposed. It updates an approximation to a cumulative clipped advantage function and is robust to partially observed state. The algorithm outperforms strong baseline methods on partially observed reinforcement learning tasks such as Pong with single-frame observations, Doom, and Minecraft first-person navigation. Deep reinforcement learning methods often face challenges with partial observability in real-world scenarios. While value function-based methods like Q-learning assume a Markovian observation space, Monte Carlo policy gradient methods do not. However, practical policy gradient methods often introduce the Markov assumption to improve sample efficiency. One workaround for partial observation is to learn value functions on finite-length frame-history observations. Our contribution is a new model-free deep reinforcement learning algorithm based on regret minimization principles, designed to handle partial observability without requiring a Markovian state. The algorithm learns a policy by incorporating finite-length frame-history observations, offering a more robust approach to non-Markovian tasks. Our new algorithm, called \"advantage-based regret minimization\" (ARM), learns a policy by estimating a cumulative clipped advantage function without needing a Markovian state. It outperforms prior methods in partially observable environments like Doom and Minecraft, even with frame-history observations. Deep reinforcement learning algorithms have shown success in complex tasks like playing games. Prior methods include learning state or state-action value functions, policy gradients, and actor-critic architectures. Policy gradient methods do not require a Markovian state but suffer from poor sample complexity. Learning Q-functions with replay buffers can accelerate learning. Our method learns cumulative advantage functions that handle non-Markovian problems by depending only on the current state. This approach avoids increasing input space size and complexity, resembling positive temporal difference methods. Our method learns cumulative advantage functions for non-Markovian problems, avoiding complexity by focusing on the current state. The update rule for the modified cumulative Q-function is similar to the average Q-function used for variance reduction in Q-learning. The method is based on cumulative regret minimization, with connections to reinforcement learning and imitation learning. Regression regret matching is a related idea using linear regression, but limited compared to deep function approximation. The section provides background on CFR and CFR+, describes ARM, and reviews the algorithm of counterfactual regret minimization. The algorithm of counterfactual regret minimization (CFR) is closely followed with reinforcement learning notation in an extensive game setting with N players. Sequences of actions and information sets are defined, with terminal sequences in space Z. Information sets in extensive games represent partial observability. Strategies are defined as probability distributions over actions conditioned on information sets. Sequence probabilities determine the likelihood of reaching a sequence based on player strategies. Values are the terminal sequence values for each player. The expected value of a strategy profile is defined for each player in a learning scenario where players follow a strategy profile. Regret is calculated based on the player's optimal strategy, and counterfactual regret is defined for taking actions at information sets. Counterfactual regret is defined for each player based on their actions at information sets. It can be used in a learning algorithm where strategies are updated using regret matching. CFR+ is a modification that positively clips the counterfactual regret. The modification CFR+ positively clips the counterfactual regret, leading to a significant practical improvement in algorithm performance. CFR and CFR+ are designed for imperfect information extensive-form games, which can be extended to partially observed stochastic games. This extension involves mapping information sets to observations, allowing for a stationary observation-action value representation. Advantage-based regret minimization (ARM) is a reinforcement learning algorithm that updates the policy to regret match on the cumulative clipped advantage function. This leads to the outline of a batch-mode deep reinforcement learning algorithm, where data is collected by sampling trajectories using the current policy and processed to fit the advantage function. The ARM algorithm updates the policy to regret match on the cumulative clipped advantage function by fitting value functions using stochastic gradient descent and deep function approximation. The policy \u03c0 t+1 is set for the next iteration using Equation (10) after fitting the advantage function using Equation (9). The ARM algorithm updates the policy to regret match on the cumulative clipped advantage function by fitting value functions using stochastic gradient descent and deep function approximation. Each minibatch step of the optimization subproblem consists of three parameter updates in terms of regression targets. The algorithm learns a modified on-policy Q-function from transitions with an added reward bonus, incorporating the previous iteration's cumulative clipped advantage. The ARM algorithm updates the policy to regret match on the cumulative clipped advantage function by fitting value functions using stochastic gradient descent and deep function approximation. It incorporates a reward bonus as a form of \"optimism in the face of uncertainty.\" Recent work has connected policy gradient methods and Q-learning via entropy regularization, defining the policy gradient target policy as the softmax distribution on the entropy regularized advantage. The policy gradient target policy is parameterized by an explicit parameter \u03b8 and involves a baseline function b(o). Different target policies lead to different policy gradient updates, with ARM proposing a regret-matching distribution. Comparing equations shows the differences in policy gradients. The ARM-like policy gradient (Equation FORMULA2) has a logarithmic dependence on the advantage-like function \u0100 +, while the existing policy gradient (Equation FORMULA2) is linearly dependent on the advantage function A \u03b2-soft. This difference in dependence contributes to the distinction of ARM from other policy gradient methods, making it less sensitive to large positive advantages from overestimation. The existing policy gradient term (1/\u03b2) log(\u03c0(a|o; \u03b8)) becomes negligible for large \u03b2 values. In practice, the ARM-like policy gradient may perform entropy regularization by default, unlike existing policy gradient methods. Implementing ARM-like policy gradient may face obstacles due to positive clipping, but it is not an intrinsic barrier. The convergence results of CFR and CFR+ suggest that ARM could benefit from greater partial observability. The regret bounds of CFR/CFR+ benefit from partial observability. Regret is seen as \"area under the learning curve.\" CFR and CFR+ have a regret bound of O(|O| \u221a T). Policy gradient with a suitable baseline has a regret bound of O( \u221a T) with no explicit dependence on observation space size |O|. Regret proportional to observation space size is beneficial in partially observable domains. In highly partially observable domains, the regret bounds of different RL algorithms vary based on the observation space size. If the observation space size is above a certain threshold, algorithms like ARM can benefit from greater partial observability. For Q-learning, the convergence rate in the L \u221e -norm depends on a condition number C. The exploration strategy in Q-learning depends on the condition number C, which describes how balanced the strategy is. Partial observability can lead to imbalanced exploration, negatively affecting Q-learning. There are differences between ARM implementation and CFR theory, such as function approximation and sampling methods. Waugh et al. (2015) addressed CFR with function approximation using a noisy version of Blackwell's condition. Sampling is used in place of enumeration in CFR implementation. In this study, the focus is on comparing ARM with methods like double deep Q-learning for Markovian observations and TRPO for non-Markovian observations in reinforcement learning environments with partial observations. The experiments are conducted on visual domains using feedforward convnets with frame-history observations. The trade-off between unbiasedness and variance in RL algorithms is also discussed, with n-step returns being commonly used for lower variance. In this study, the focus is on comparing ARM with methods like double deep Q-learning for Markovian observations and TRPO for non-Markovian observations in reinforcement learning environments with partial observations. The experiments are conducted on visual domains using feedforward convnets with frame-history observations. Atari games consist of moving sprites with fixed shapes and palettes, and the motion can be highly deterministic. To increase partial observability, the frame-history length fed as input to the networks is varied. Agents were trained to play Pong via the Arcade Learning Environment with different frame-history lengths. In this study, ARM was evaluated on first-person navigation in the ViZDoom BID16 domain, a more complex domain than Atari, with partial observability. Evaluation was done on \"HealthGathering\" and \"MyWayHome\" benchmarks. The agent in ViZDoom BID16 domain faces challenges in \"HealthGathering\" and \"MyWayHome\" scenarios, where it must navigate toxic rooms to find healthkits and reach a target object in a maze before time runs out. Despite using only visual input, ARM quickly learns effective policies with minimal hyperparameter tuning, achieving high scores in under 1 million steps. On \"HealthGathering,\" ARM quickly learns a policy close to maximum return. Double deep Q-learning is more consistent but may be due to evaluating a less stochastic policy. ARM performs well on \"MyWayHome\" and utilizes off-policy replay memory in ViZDoom. Doom results are in FIG4. ARM is also evaluated on first-person navigation in the Malm\u00f6 domain based on Minecraft. The study evaluates first-person navigation in the Malm\u00f6 domain using Minecraft, adapting a curriculum learning protocol with 5 levels of increasing difficulty. Minecraft's diverse environments make it potentially more challenging than Doom. In a study using Minecraft, different algorithms were tested on a curriculum learning schedule with 5 levels. ARM and dueling double DQN learned quickly with 62500 simulator steps between levels, while TRPO required 93750 steps. ARM consistently performed well on all levels and achieved high scores on the final level. In this paper, a novel deep reinforcement learning algorithm called advantage-based regret minimization (ARM) is introduced. ARM is well suited for partially observed or non-Markovian environments, outperforming baseline methods like deep Q-learning and TRPO on tasks such as ViZDoom and Malm\u00f6 first-person navigation benchmarks. Future work will explore ARM's applications in more complex tasks, including continuous action spaces. The experimental details for the Pong game in the Arcade Learning Environment involved preprocessing and using a convolutional network model. The convnet had specific layers and parameters, and the learning algorithm used Adam with a constant learning rate. Results were averaged across multiple random seeds. The experimental setup for the Pong game in the Arcade Learning Environment included using ARM with specific hyperparameters and a convolutional network architecture similar to previous studies. The Doom screen was rendered at a resolution of 160 \u00d7 120 and downsized to 84 \u00d7 84, with input observations consisting of the last 4 rendered RGB frames. The convnet had 3 convolutions with 32 filters each, and only every 4th frame was rendered. The convolutional network architecture for the Pong game included 32 filters in 3 convolutions: 8x8 with stride 4, 4x4 with stride 2, and 3x3 with stride 1. The final convolution was followed by linear maps with 1024 filters. Hidden activations were gated by ReLUs. Rewards were scaled by a factor of 0.01 for \"HealthGathering\" only. Adam was used with a learning rate of \u03b1 = 10^-5 and minibatch size of 32. Different \u03b2 values were set for each scenario. Results were averaged across 3 random seeds. Double DQN and dueling double DQN used n = 5 step returns and specific update intervals and memory sizes. A3C utilized 16 workers and n = 20. The ARM algorithm utilized various hyperparameters including n = 5 step returns, sampling batch sizes, Adam minibatches, and target update step size. For \"HealthGathering\", the Adam learning rate was annealed to \u03b1 = 2.5 \u00d7 10 \u22126 after 500000 simulator steps. Off-policy ARM used a replay cache sample size of 25000 and 400 Adam minibatches. The ARM algorithm used n = 5 step returns, sampling batch size 1563, replay cache sample size 25000, 400 Adam minibatches per sampling iteration, target update step size \u03c4 = 0.01, and importance sampling weight clip c = 1. Minecraft tasks were similar to Matiisen et al. (2017) but with a discrete action space. Rewards were scaled by 0.001 and the screen resolution was downsized to 84 \u00d7 84. Only every 5th frame was rendered, and the input observation of the convnet is a concatenation of the last 4 rendered RGB frames. The convnet input observation consists of the last 4 RGB frames. Adam was used for training with specific parameters. Results were averaged across 5 random seeds. Different methods had specific settings such as update intervals and exploration rates. Our approach to running ARM with off-policy data involves applying an importance sampling correction to n-step returns. The corrected n-step return uses a truncated importance weight to reduce variance. We chose a weight clip of c = 1 based on experimentation, but other values may also work. This correction preserves most aspects of the ARM algorithm, except for transition sampling strategy and regression targets for learning value functions. ARM with off-policy data involves applying importance sampling correction to n-step returns, using a weight clip of c = 1. The regression targets for learning value functions are modified, and the target value function V (o k+n ; \u03d5) does not require an importance sampling correction. Additionally, the study includes testing ARM in Atari 2600 games, comparing it to double deep Q-learning and double deep fitted Q-iteration. Double deep Q-learning is a strong baseline for learning Atari games, while ARM successfully learns policies. Q-learning methods benefit from utilizing a large off-policy replay memory. Results on Atari games are shown in FIG3. Recurrent policy and value function estimation have a small positive effect on A2C convergence in the MyWayHome scenario of ViZDoom. Hyperparameters were similar to A3C, with a learning rate of 10^-4 and gradient norm clip of 0.5. LSTM was used for the recurrent policy and value function."
}