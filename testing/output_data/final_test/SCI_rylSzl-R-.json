{
    "title": "rylSzl-R-",
    "content": "Deep generative models like GANs and VAEs have been successful. This paper establishes connections between GANs and VAEs by interpreting sample generation in GANs as posterior inference. The unified view allows for analysis of existing model variants and transfer of techniques across research lines. For example, the importance weighting method from VAE literature can improve GAN learning. In this paper, the importance weighting method from VAE literature is applied to enhance GAN learning. The study explores deep generative models that define distributions over variables in multiple layers, with connections to hierarchical Bayesian models and neural network models like Helmholtz machines. The wake-sleep algorithm for training Helmholtz machines is of particular relevance, as it minimizes KL divergences in opposite directions of the posterior and its approximation. In recent years, there has been a resurgence of interest in deep generative modeling, with approaches like VAEs, GANs, and GMMNs leading to impressive results in image and text generation, disentangled representation learning, and semi-supervised learning. The deep generative model literature views these approaches as distinct training paradigms, with GANs aiming for equilibrium between a generator and discriminator, and VAEs maximizing a variational lower bound of data log-likelihood. In this paper, a new formulation of GANs and VAEs is presented that connects them under a unified view and links them back to the classic wake-sleep algorithm. The approach involves minimizing opposite KL divergences of respective posterior and inference distributions, extending the sleep and wake phases for generative model learning. The proposed reformulation of GANs interprets sample generation as posterior inference, resembling variational inference in VAEs. This approach offers a unified view of generative modeling algorithms, including InfoGAN, VAE/GAN joint models, and adversarial domain adaptation. The close parallelisms between GANs and VAEs allow for transferring techniques between the two classes of models. Examples include deriving importance weighted GAN (IWGAN) from IWAE and activating the discriminator in VAEs for improved performance. Research interest in deep generative models has increased recently, with techniques from one class benefiting the other. There has been a surge of research interest in deep generative models, with progress in understanding algorithms like the wake-sleep algorithm. This algorithm aims to maximize a variational lower bound of data log-likelihood by incorporating separate inference and generative models. Recent approaches like NVIL and VAEs work on maximizing the variational lower bound jointly for both models. VAEs improve stochastic gradient estimates by reparametrizing gradients. BID4 introduces importance weighted VAEs for a tighter lower bound. Hu et al. (2017) combine VAEs with a sleep procedure using generated samples for learning. GANs use a discriminator to distinguish between real and generated samples. The adversarial approach relates to approximate Bayesian computation and density ratio estimation. The generator aims to minimize the log probability of the discriminator recognizing a generated sample correctly. The objective of GANs is to minimize the log probability of the discriminator correctly recognizing fake samples, equivalent to minimizing the Jensen-Shannon divergence. In practice, another objective is used to maximize the log probability of the discriminator recognizing real samples. Various extensions of GANs have been developed, including combining with VAEs for improved generation and generalizing objectives to minimize other f-divergence criteria. The paper discusses the application of adversarial principles in various contexts beyond generation, such as domain adaptation and Bayesian inference using implicit variational distributions in VAEs. It provides a unified view of deep generative modeling by revealing connections between different approaches like GANs and VAEs. In this paper, the authors propose a new interpretation of GANs, suggesting that the generation of samples in GANs can be seen as performing inference, while discrimination acts as a generative process for real/fake labels. This new formulation reveals connections between GANs and traditional variational inference, as well as their correspondence to the wake-sleep algorithm. The authors also show that GANs are a special case of Adversarial Domain Adaptation (ADA), and analyze the links between GANs, VAEs, and their variants in this framework. ADA aims to transfer prediction knowledge from a source domain to a target domain by learning domain-invariant features through a feature extractor and discriminator. The feature extractor maps data examples to features, while the discriminator distinguishes between source and target domains. The feature extractor and discriminator in ADA aim to transfer prediction knowledge between domains by maximizing classification accuracy. The implicit distribution p\u03b8(x|y) is formed by data distribution p(z|y) and transformation G\u03b8, making likelihood evaluation difficult but sampling easy. The objectives of ADA involve conditional distributions q\u03c6(y|x) and q r \u03c6(y|x), with z encapsulated in the implicit distribution. The objectives of ADA involve conditional distributions q\u03c6(y|x) and q r \u03c6(y|x), with z encapsulated in the implicit distribution p\u03b8(x|y). The adversarial mechanism in GANs replaces q(y|x) with q r (y|x), involving respective conditional distributions q and its reverse q r. InfoGAN adds conditional generation of code z with distribution q\u03b7(z|x, y), while VAEs swap the generation and inference processes of InfoGAN. In GANs BID12, the generation and inference processes are swapped, making it a special case of ADA. The goal is to make generated images indistinguishable from real ones to the discriminator. Real examples and generated samples are defined by different distributions. In GANs, the conditional distribution over x is constructed with fixed distribution p(x|y = 1) representing real data distribution p data (x). The generative parameters \u03b8 are associated with p g \u03b8 (x), while discriminator D \u03c6 infers the probability that x is from the real data domain. The objectives of GANs are expressed as Eq.(3), with generative parameters \u03b8 translated into a classical form. The unsaturated objective for the generator BID12 focuses on reconstructing the real/fake indicator y using generative and inference distributions. This reformulation of objectives provides more insights into the problem. The unsaturated objective for the generator BID12 focuses on reconstructing the real/fake indicator y using generative and inference distributions. To reformulate the objectives and reveal more insights to the problem, each optimization step of p \u03b8 (x|y) at point (\u03b8 0 , \u03c6 0 ) in the parameter space involves minimizing the KLD to drive p \u03b8 (x|y = 0) towards the respective mixture q r (x|y = 0), resulting in a new state where p \u03b8 new (x|y = 0) gets closer to p data (x). The KL and Jensen-Shannon Divergences, KL(\u00b7 \u00b7) and JSD(\u00b7 \u00b7), are discussed in relation to GAN generator learning. The generator optimization is equivalent to minimizing the KL divergence between the inference distribution and the posterior, minus a JSD between the generative distribution and the data distribution. This reveals connections to VAEs and offers insights into training dynamics. The text discusses the optimization of GAN generators by minimizing the KL divergence between the inference distribution and the posterior, while also considering the Jensen-Shannon Divergence (JSD) between the generative distribution and the data distribution. The JSD term pushes the generative distribution away from the data distribution, acting oppositely from the KL divergence term. This helps drive the generative distribution closer to the data distribution during training. The JSD term in GAN optimization is upper bounded by the KLD term, meaning minimizing the KLD term reduces the magnitude of JSD. The missing mode issue in GANs is explained by the asymmetry of KLD, concentrating on large modes of the generative distribution. Previous works assume optimal discriminators, which may not hold true in practice. Our result is a generalization of the previous theorem in GAN optimization, not relying on optimality assumptions. It provides insights into training dynamics and the missing mode issue, even when the discriminator does not meet certain optimality criteria. This generalized result allows for a broader understanding of different scenarios, such as when the discriminator gives uniform guesses or when the generator and data distributions are indistinguishable. The InfoGAN model introduces an extra conditional parameter to recover the latent code z from a given sample x. This formulation can be applied to various GAN-related variants, such as Adversarial Autoencoder and cycleGAN. The model combines different conditionals to perform full reconstruction of both z and y, encapsulated in the implicit distribution p(x|y). The text discusses deep generative modeling, specifically comparing Variational Autoencoders (VAEs) to Generative Adversarial Networks (GANs). VAEs involve minimizing a KLD in an opposite direction with a degenerated adversarial discriminator, while GANs use adversarial mechanisms. The connection between VAEs and GANs is explored by assuming a perfect discriminator. The text discusses the equivalence between the VAE objective and a specific distribution in GANs, with a perfect discriminator assumed. The components in VAEs and GANs have similarities, with slight differences in the generation distribution. The proof of Lemma 2 is provided in the supplementary materials. The text provides the proof of Lemma 2 and discusses the graphical model of VAEs. It highlights the differences between VAEs and InfoGAN, emphasizing the role of distributions in GANs and VAEs. The loss on fake samples is degenerated to a constant in the presence of a perfect discriminator. Lemmas 1 and 2 reveal the minimization of KLD in GANs and VAEs. The text discusses the connections between GANs and VAEs, highlighting how the generator parameters in the two models lead to distinct behaviors. GANs can generate sharp images but may collapse to few modes, while VAEs aim to cover all modes of the data. This reveals new research directions and a unified statistical view of the two model classes. The KLD of VAEs drives the generator to cover all modes of the data distribution, leading to blurred samples. Combining KLD objectives can remedy this asymmetry. VAEs include an adversarial mechanism like GANs, with a perfect discriminator. GANs and VAEs have inverted latent-visible treatments, relating to the symmetry of the sleep and wake states. The wake-sleep algorithm, proposed for learning deep generative models like Helmholtz machines, consists of wake and sleep phases optimizing the generative and inference models. The wake phase updates generator parameters by fitting real data and hidden code, while the sleep phase updates parameters based on generated samples. The algorithm's relation to VAEs is discussed in previous sections. The wake-sleep algorithm involves optimizing generative and inference models in wake and sleep phases. The relation between Wake-Sleep and VAEs is that WS minimizes the variational lower bound like VAEs, with WS optimizing the generator and VAEs extending the wake phase. GANs resemble the sleep phase, with GANs optimizing the discriminator to reconstruct samples, while InfoGAN extends this concept further. The generative model p \u03b8 is optimized to reconstruct reversed y, with InfoGAN extending this to reconstruction of latents z. This new interpretation facilitates the exchange of ideas and techniques between VAEs and GANs, allowing for enhancements to be applied across both classes of algorithms. For example, IWAE proposed importance weighted autoencoder that maximizes a tighter lower bound on the marginal likelihood, which can be adapted to develop importance weighted GANs within this framework. The variational inference interpretation in Lemma.1 suggests GANs maximize a lower bound of the marginal likelihood on y. By deriving a tighter lower bound through k-sample importance weighting, the update rule for the generator learning involves an additional importance weight w i. This weight assigns higher weights to more realistic samples, consistent with IWAE's emphasis on better reconstructions. The importance weighting method in generator training assigns higher weights to more realistic samples, consistent with IWAE's emphasis on better reconstructions. The computational cost is minimal as it only involves evaluating the weight for each sample. The discriminator is trained in the same way as in standard GANs. In the semi-supervised VAE setting, remaining training data are used for unsupervised training, enabling adaptive incorporation of fake samples. The adaptive incorporation of fake samples in VAEs involves replacing the perfect discriminator with a parameterized one, enabling effective data selection based on sample resemblance to real data. Real examples are weighted based on their recognizability by the discriminator. AAVAE emphasizes harder examples by using importance weighting (IW) and adversarial activating (AA) techniques to improve standard GANs and VAEs. The IW method is applied to both vanilla GANs and class-conditional GANs (CGAN) without tuning hyperparameters. MNIST, SVHN, and CIFAR10 datasets are used for evaluation. The study evaluates the performance of GANs and VAEs using MNIST, SVHN, and CIFAR10 datasets. Inception scores are measured for GANs, while CGANs are evaluated for conditional generation accuracy. The IW strategy consistently improves base models, and the AA method is applied to VAEs on the MNIST dataset to generate fake samples. The study evaluates the performance of GANs and VAEs using MNIST, SVHN, and CIFAR10 datasets. Inception scores are measured for GANs, while CGANs are evaluated for conditional generation accuracy. The AA method is applied to VAEs on the MNIST dataset to generate fake samples, showing improved accuracy over the base semi-supervised VAE. The new interpretations of GANs and VAEs reveal strong connections between them, linking emerging approaches to the classic wake-sleep algorithm and offering a unified statistical insight into deep generative modeling. The formulation interprets sample generation in GANs as posterior inference, treating latent and visible variables symmetrically for better modeling and understanding. This approach connects GANs to adversarial domain adaptation and provides a variational inference interpretation of generation. The wake-sleep algorithm is another example where visibles are reconstructed conditioned on latents. The wake-sleep algorithm involves reconstructing visibles conditioned on latents during the wake phase and reconstructing latents conditioned on visibles during the sleep phase. Empirical data distributions are implicit and easy to sample from but intractable for likelihood evaluation, while priors are explicit and suitable for likelihood evaluation. The complexity of visible and latent spaces differs, with latent space typically simpler. Techniques like GANs and density ratio estimation help bridge this complexity gap. Generative models like GANs use prior distributions over latent variables for generating samples, while AAE leverages adversarial approach for implicit priors. Recent work extends VAEs with implicit variational distributions. Adversarial methods replace KL divergence minimization between variational distributions and priors. The difference in space complexity guides the choice of tools to minimize the distance between distributions and targets. VAEs and adversarial autoencoders both regularize models by minimizing distance between variational posterior and prior, using different loss functions. Conditional generative models create (data, label) pairs for classifier training. Adversarial Domain Adaptation (ADA) aims to transfer prediction knowledge from a labeled source domain to an unlabeled target domain by learning domain-invariant features. The conventional formulation involves a domain discriminator D\u03c6(x) = q\u03c6(y|x) and a classifier f\u03c9(t|x) for predicting labels in the source domain. This approach is similar to knowledge distillation and is used in classifier training. Adversarial domain adaptation, as seen in BID11 Purushotham et al., 2017, involves proving a lemma and deriving equations to show the upper bound of the JSD term by the KL term. The Adversarial Autoencoder (AAE) BID25 is obtained by swapping code variable z and data variable x of InfoGAN in the graphical model. The graphical model in FIG2 represents the objectives of the Adversarial Autoencoder (AAE) proposed in BID25. Detailed derivations show how to translate graphical model representations into mathematical formulations for various models like GANs, InfoGANs, and VAEs. Parameters \u03b8, \u03b7, and \u03c6 are associated with distributions over x, z, and y respectively. The inference process involves implicit distribution q\u03b7(z|y). The Adversarial Autoencoder (AAE) involves implicit distribution q\u03b7(z|y) for real and approximate distributions. The generative process includes p\u03b8(x|z, y)q, with fixed parameters for y=1 and \u03b8 associated with y=0. The model maps data x to code z using a deterministic transformation E\u03b7(x). The Adversarial Autoencoder (AAE) involves maximizing the log likelihood of generative distributions with fixed parameters for y=0. The objective is to minimize reconstruction loss of observed data x and generate code z that fools the discriminator, resulting in the conventional view of the AAE model. The AAE model aims to generate code z that deceives the discriminator, similar to Predictability Minimization (PM) and CycleGAN. AAE's objectives train the model to translate x into z, while InfoGAN handles the reversed translation. The AAE model aims to generate code z that deceives the discriminator, similar to Predictability Minimization (PM) and CycleGAN. For the reconstruction term, the VAE objective is recovered by combining equations. Previous works have explored combining VAEs and GANs to improve image sharpness and remedy mode covering behavior in VAEs. Incorporating GAN objectives in the model helps address mode missing problems and focuses on meaningful data modes. GANs can be viewed as maximizing a lower bound of the \"marginal log-likelihood\" on y. By applying importance weighting methods, a tighter bound can be derived to maximize the importance weighted lower bound. The importance weighting method is applied to extend vanilla GANs and class-conditional GANs. The base GAN model uses the DCGAN architecture without tuning hyperparameters. Evaluation is done on MNIST, SVHN, and CIFAR10 datasets using inception scores. Deep residual networks achieve scores of 9.09, 6.55, and 8.77 on the test sets. Accuracy of conditional generation is also evaluated for conditional GANs. The accuracy of conditional generation is evaluated for conditional GANs, achieving 0.990 and 0.902 on MNIST and SVHN test sets respectively. Adversary activating method is applied on various VAE models, with similar network architectures as GANs. In experiments with VAE models, the discriminator distributions are smoothed by adjusting the temperature of the output sigmoid function. The best temperature is selected from {1, 1.5, 3, 5} through cross-validation. The classification accuracy of semi-supervised VAEs and the adversary activated extension on the MNIST test set is evaluated with varying sizes of real labeled training examples."
}