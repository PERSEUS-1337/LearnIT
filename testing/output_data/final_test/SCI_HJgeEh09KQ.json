{
    "title": "HJgeEh09KQ",
    "content": "The novel approach combines scalable overapproximation methods with precise linear programming to certify neural networks against adversarial perturbations, achieving better precision than current verifiers. Neural networks are used in critical domains like autonomous driving, medical diagnosis, and speech recognition, but they can be vulnerable to adversarial attacks. Certification techniques are being explored to prove network robustness against adversarial examples. Current verifiers can be complete or incomplete. Complete verifiers use MILP or SMT solvers but are limited to small networks. Incomplete verifiers employ overapproximation methods like duality, abstract interpretation, and linear approximations for scalability, but may lack precision for deeper networks. This work addresses the challenge of designing a verifier that improves the precision of incomplete methods and the scalability of complete ones by combining overapproximation techniques and MILP solvers. The system called RefineZono is faster than existing methods. RefineZono is a system that combines overapproximation techniques and MILP solvers to improve precision and scalability of verifiers. It outperforms existing methods, such as BID27 and BID25, in terms of speed and accuracy for complete verification tasks. RefineZono is a system that combines overapproximation techniques and MILP solvers to improve precision and scalability of verifiers. It is more precise and faster than existing state-of-the-art incomplete verifiers on larger networks and faster than complete verifiers on smaller networks. The system is publicly available on GitHub at https://github.com/eth-sri/eran. The fully connected feedforward neural network with ReLU activations has two inputs in the range [0, 1] and consists of an input layer, two hidden layers, and one output layer, each with two neurons. The goal is to verify that the output at neuron x 13 is greater than the output at x 14 for any input in [0, 1] \u00d7 [0, 1]. The verifier operates on the network using a MILP formulation after the second affine transformation. The MILP formulation of the network is used for refining results after the third affine transformation. ReLU transformers compute affine forms with original and refined bounds. The selection of MILP or LP formulations for neurons is based on a heuristic explained in the next section. The analysis leverages Zonotope domain and transformers specialized to neural network activations in DeepZ, a verifier for neural network robustness. The Zonotope domain in DeepZ BID23 associates an affine form with each neuron in the network, making it more precise than non-relational domains. An abstract element in the analysis is an intersection between a Zonotope and a bounding box. The analysis starts by setting inputs and applying an affine transformation to produce an output. The Zonotope affine transformer is used for the transformation. The Zonotope ReLU transformer is applied to the inputs, resulting in the output DISPLAYFORM3. Neuron x3 takes only non-negative values, while neuron x4 can take both positive and negative values. An approximation is used for the output of x4, minimizing the area in the input-output plane and introducing noise symbol \u03b73. The Zonotope approximation for x6 permits negative values, accumulating overapproximation as the analysis progresses deeper into the network. The analysis progresses deeper into the network, accumulating imprecision due to overapproximation. MILP-based refinement is applied at the second layer to handle affine transformations. The bounds for x7 and x8 are imprecise, leading to further imprecision with the introduction of the DeepZ ReLU transformer. To reduce precision loss, the method refines the bounds for x7 and x8 by formulating the network as a MILP instance. The analysis delves deeper into the network, accumulating imprecision from overapproximation. MILP-based refinement is used at the second layer to handle affine transformations, improving bounds for x7 and x8. The ReLU transformer refines bounds for x7 and approximates x8, reducing approximation error. The final layer undergoes LP-based refinement to address approximations. The final affine transformation is processed, refining bounds using LP relaxation up to the third affine transformation. Proving robustness, the analysis shows the neural network provides the same label for all inputs in [0, 1] \u00d7 [0, 1], indicating its robustness. DeepZ fails to prove robustness due to imprecise bounds, but refinement-based approach can be extended to other abstractions like Polyhedra and DeepPoly for improved verification results. Our approach combines abstract interpretation with exact and inexact MILP formulations to compute more precise results for neuron bounds. It requires an abstract domain over variables like Interval, Zonotope, DeepPoly, or Polyhedra, with a concretization function that associates abstract elements with concrete points. The abstract domain over variables like Interval, Zonotope, DeepPoly, or Polyhedra requires an abstraction function that associates abstract elements with concrete points. It includes operations like meet, affine abstract transformer, and ReLU abstract transformer for robustness certification of neural networks with ReLU activations. The abstraction function \u03b1 m computes a symbolic overapproximation of possible inputs for a neural network. By propagating the abstract element through abstract transformers, we obtain a symbolic overapproximation of the network's outputs. This process is particularly useful for networks with ReLU activations, allowing for robustness certification. The abstract transformers produce a superset of concrete outputs, leading to imprecision in deep neural networks. This can be addressed by refining the bounds to combat spurious results. Refining the bounds using mixed integer linear programming (MILP) helps combat overapproximations in each layer. The abstract element is refined using the meet operator and linear constraints for input activations. A more refined abstract transformer for ReLU is obtained by leveraging tighter bounds, resulting in a refined ReLU transformer with improved bounds. The refined ReLU transformer benefits from improved bounds, with tighter constraints leading to smaller areas in the input-output plane. Constraints for refinement with MILP are obtained by capturing the neural network behavior up to the last executed layer. Each neuron is represented by a variable, and constraints are derived from the input layer. The refined ReLU transformer improves bounds by obtaining constraints from input, affine, and ReLU layers, encoded as MILP instances for optimization. However, current MILP solvers struggle with scalability for larger neural networks. The MILP-based formulation struggles with scalability for larger neural networks. To address this, an intermediate method is explored for deeper layers, balancing precision and scalability by using abstract interpretation to relax constraints and produce looser bounds. In practice, a fraction of neurons in each layer is chosen to compute bounds using MILP with a timeout. Tighter bounds reduce approximation, while looser bounds allow for scalability in deeper layers. MILP solvers provide lower and upper bounds on the objective function in a best-effort fashion. The MILP solver's time to solve instances is influenced by the parameter \u03b2 \u2208 [0, 1]. Neuron selection heuristic involves ranking neurons based on various criteria, such as activation bounds and output weights. If the next layer is ReLU, neurons with non-positive activations are ignored. The remaining neurons are ranked by width and output weights. To certify robustness of deep neural networks, an end-to-end approach combines MILP, LP relaxation, and abstract interpretation. Neurons are ranked and refined in fractions with different timeouts for each layer. The approach involves refining bounds using anytime MILP relaxation and neuron selection heuristic for the first k MILP layers, followed by refinement using LP relaxation for the next k LP layers. The approach to certify robustness of deep neural networks combines MILP, LP relaxation, and abstract interpretation. Neurons are ranked and refined in fractions with different timeouts for each layer. Bounds are refined using LP relaxation for the next k layers, followed by abstract interpretation for the remaining layers. Final property certification involves encoding the problem using the output abstract element obtained after applying the abstract transformer for the last layer. If needed, complete verification using MILP can be used. Our approach, implemented in RefineZono, uses MILP and LP instances with Gurobi for robustness verification of ReLU-based neural networks. It outperforms state-of-the-art verifiers like BID27 and BID25, and produces more precise results than incomplete verifiers like DeepZ BID23 and DeepPoly BID24. Evaluation datasets include MNIST, CIFAR10, and ACAS Xu. All code and data are publicly available on GitHub. In experiments, MNIST, CIFAR10, and ACAS Xu datasets were used with neural networks including feedforward and convolutional networks. Some networks were trained to be robust against adversarial attacks. The goal was to certify correct labels in adversarial regions for MNIST and CIFAR10, and verify specific properties for ACAS Xu. The largest network had over 88K neurons and the deepest had 9 layers. Experimental setup involved running experiments on different networks using specific CPUs and memory configurations. Benchmarks were set using MNIST and CIFAR10 datasets, filtering out incorrectly classified images. Certification methods varied for different networks, with RefineZono used for complete certification on some networks and incomplete certification on others. DeepZ analysis was run as part of the certification process. RefineZono is used for certification on networks like ACAS Xu 6\u00d750 and MNIST 3 \u00d7 50. For ACAS Xu, RefineZono certifies the network in 227 seconds, faster than other verifiers. For MNIST, RefineZono is compared against a state-of-the-art verifier using Interval analysis and LP for neuron bounds determination. The MILP solver is used if LP analysis fails. Verifiers certify robustness against perturbations on 85% of images. RefineZono, MILP with Interval analysis bounds, and MILP with LP analysis bounds have runtimes of 28, 123, and 35 seconds. Zonotope analysis offers a balance between speed and precision. RefineZono is compared against DeepZ and DeepPoly for incomplete certification. DeepZ is as precise as Fast-Lin, and DeepPoly is as precise as CROWN. The values for L \u221e -norm attack are larger for robust networks compared to non-robust networks. Verifiers report average runtime per image and precision in certifying network robustness. Interval analysis for initial bounds is imprecise for large networks, leading to reliance on LP per neuron for precise bounds. LP analysis takes over 20 minutes on a 9 \u00d7 200 network, while RefineZono runs in approximately 14 minutes. Different parameter values were tested for k MILP and k LP analysis. Parameter values were experimented with for the certification of neural networks, refining neuron bounds after affine transformations. Different values were chosen for MILP and LP based refinements, with specific parameters used for MNIST and CIFAR10 FNNs. Timeout, precision, and tradeoff values were also specified for the refinements. For the CIFAR10 FNN, \u03b8 = \u03c9 2 k\u22122 \u00b7p is used for MILP refinement with T = 6 seconds. Convolutional layers in CNNs are not refined due to a large number of candidates. Fully connected layers are refined with a larger timeout. MNIST ConvSmall, ConvBig, and CIFAR10 ConvSmall networks refine all candidate neurons using MILP with T = 10 seconds. MNIST ConvSuper network refines similarly using LP with T = 15 seconds. RefineZono improves or achieves precision compared to state-of-the-art verifiers on all neural networks, certifying more images than DeepZ. RefineZono certifies neural networks to be more robust on images compared to DeepPoly on 6 out of 10 networks. The runtime of RefineZono is not determined by the number of neurons in the network. It runs faster on networks trained to be robust, as they produce fewer candidate neurons for refinement. This makes them easier to certify. RefineZono's average runtime is significantly faster on the robustly trained ConvBig network compared to the non-robust ConvSmall network. Neuron selection heuristic is used to identify important neurons for refinement in FNNs, which helps speed up the analysis process. RefineZono, a novel refinement-based approach, combines overapproximation techniques with linear-programming-based methods for verification tasks on neural networks. The system improves precision compared to existing incomplete verifiers and can certify robustness properties beyond state-of-the-art complete verifiers. Combining overapproximation methods with mixed integer linear programming shows promise for advancing neural network verification beyond existing incomplete verifiers."
}