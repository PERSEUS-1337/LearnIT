{
    "title": "Hyl5V0EYvB",
    "content": "Existing defenses against adversarial attacks focus on robustness to L_p-bounded distortions. However, attackers can modify images in ways outside fixed distortion models, such as adversarial rotations. This work emphasizes measuring robustness against a broader range of unforeseen attacks, whose precise form is unknown during defense design. We propose new attacks and a methodology to evaluate defense against diverse distortions like JPEG, Fog, Gabor, and Snow. Introducing UAR as a metric for measuring robustness, we find that evaluating against a wider range of attacks is more beneficial than just L_p attacks. Adversarial training against specific distortions does not guarantee robustness against others, highlighting the importance of studying unforeseen distortions. Neural networks are vulnerable to adversarial attacks, even beyond L \u221e constraints. Existing defenses assume fixed adversaries, but in reality, attackers can adapt. This work introduces novel attacks to assess robustness against unforeseen distortions. The text introduces novel attacks to evaluate robustness against unforeseen distortions, including adversarial JPEG, Fog, Gabor, and Snow attacks. A methodology for evaluating defenses against diverse distortions is proposed, with code available for easy evaluation. Existing defenses based on adversarial training do not generalize to unforeseen adversaries, even when restricted to specific distortions. Evaluation results show that accuracy against different distortion types is highly correlated, indicating the need for robustness against a variety of attacks. The analysis suggests that evaluating models only against specific distortions can be misleading. Defending against multiple distortion types simultaneously is challenging, as joint adversarial training can lead to overfitting. A new metric, UAR, is proposed to assess defenses against unforeseen adversaries, with 4 novel attacks introduced for evaluation. In introducing 4 novel attacks, the study applies UAR to evaluate robustness against existing and new attacks. Results show that current defense methods do not generalize well to unforeseen attacks. Adversarial attacks aim to minimize the cross-entropy loss by applying constrained distortions to images. Adversarial training is a strong defense strategy against fixed attacks, updating using attacked images during training. The study introduces 8 attacks: L\u221e, L2, L1, Elastic, JPEG, Fog, Gabor, and Snow, with sample images and distortions shown. Novel attacks display different behavior from traditional attacks. Scaled pixel-level differences between original and attacked images are analyzed for each attack. Gabor and Snow attacks are new additions. The study introduces 8 attacks: L\u221e, L2, L1, Elastic, JPEG, Fog, Gabor, and Snow, with sample images and distortions shown. Gabor and Snow attacks are new to this paper. The attacks modify images with controlled distortion sizes using different algorithms. The Elastic attack warps images by allowing distortions based on a vector field. The study introduces novel attacks including Fog, Gabor, and Snow, which are adversarial versions of non-adversarial distortions. These attacks apply controlled distortions to images, such as partial occlusions resembling mist and snowflakes, and additive Gabor noise. The study introduces novel attacks including Fog, Gabor, and Snow, which apply controlled distortions to images. Adversarial attacks optimize parameters for distortion over L \u221e -bounded sets using gradient descent and projection techniques. The study introduces novel attacks like Fog, Gabor, and Snow, applying controlled distortions to images. A model trained against L2 at \u03b5 = 9600 on ImageNet-100 shows defense against Elastic at small distortion sizes but lacks transfer at larger sizes. A method is proposed to assess robustness against unforeseen distortions by evaluating defenses against diverse attacks not used during design. The method calibrates distortion sizes and defines a summary metric for assessing robustness. The study introduces novel attacks like Fog, Gabor, and Snow, applying controlled distortions to images. A method is proposed to assess robustness against unforeseen distortions by evaluating defenses against diverse attacks not used during design. Calibrate distortion sizes to define a summary metric for assessing robustness. The study introduces novel attacks like Fog, Gabor, and Snow, applying controlled distortions to images. A method is proposed to assess robustness against unforeseen distortions by evaluating defenses against diverse attacks not used during design. Calibrate distortion sizes to define a summary metric for assessing robustness. The maximum distortion size \u03b5 max is selected based on criteria to confuse humans or reduce accuracy of adversarially trained models. Evaluation against strong defenses is necessary to produce strong visual distortions. An example for Snow is shown to illustrate the constraint that humans recognize attacked images at \u03b5 max. UAR is used as an adversarial robustness metric. The study introduces novel attacks like Fog, Gabor, and Snow, applying controlled distortions to images. A method is proposed to assess robustness against unforeseen distortions by evaluating defenses against diverse attacks not used during design. Calibrate distortion sizes to define a summary metric for assessing robustness. The maximum distortion size \u03b5 max is selected based on criteria to confuse humans or reduce accuracy of adversarially trained models. Evaluation against strong defenses is necessary to produce strong visual distortions. UAR is used as an adversarial robustness metric, measuring a model's performance against specific distortion types compared to adversarially trained models. The UAR scores are expected to be lower than 100 against held-out distortion types to indicate that a defense is not outperforming an adversarially trained model. The normalizing factor in (1) ensures comparability between distortions, with \u03b5 values increasing geometrically from \u03b5 min to \u03b5 max by factors of 2. This approach helps in evaluating UAR roughly between distortions by minimizing the 1-distance to the ATA values of the L \u221e attack. When calibrating Elastic, \u03b5 values are selected based on ATA computations at 7 geometrically increasing values. Reference values for ATA(A, \u03b5) are provided for 8 distortion types on ImageNet-100 and CIFAR-10, reducing computational cost significantly. Evaluation against diverse distortion types is crucial to avoid overfitting when constructing a defense, such as with adversarial training. Choosing appropriate distortion types for evaluation requires careful consideration, as seemingly different distortions can have highly correlated scores against defenses. It is recommended to evaluate against a variety of attacks, including L\u221e, L1, Elastic, Fog, and Snow attacks. Results show that evaluating against commonly used Lp-attacks yields highly correlated outcomes. Our results emphasize the importance of evaluating against diverse attacks to assess robustness. We suggest using a set of 5 attacks with low pairwise robustness transfer as a starting point. We use CIFAR-10 and ImageNet-100 datasets with ResNet-56 and ResNet-50 models for training. Adversarial training is conducted by selecting a random target class for each image in each mini-batch. During training, random target classes are selected for images to apply targeted attacks with varying distortion sizes. The model is updated using only adversarial images, with different optimization steps for each attack. Evaluation is done on ImageNet-100 and CIFAR-10 validation sets against 200-step targeted attacks with random target classes. The text discusses the use of uniform random target classes for attacks during evaluation, highlighting the importance of adversarial training against unforeseen distortions. The results are analyzed using UAR scores, with a focus on robustness to random seed and number of attack steps. The text emphasizes the need for adversarial training against unforeseen distortions to achieve high UAR scores. Results show strong transfer between Lp-attacks, indicating progress in simultaneous robustness. Evaluations against diverse attacks like L\u221e, L1, Elastic, Fog, and Snow are necessary for generalization to unforeseen attacks. The text suggests evaluating the model against diverse attacks like L\u221e, L1, Elastic, Fog, and Snow to achieve high UAR scores and improve robustness against unforeseen adversaries through joint adversarial training. Joint training against different types of attacks such as (L\u221e, L2), (L\u221e, L1), and (L\u221e, Elastic) is conducted to improve model robustness. Results show that training against (L\u221e, L2) slightly enhances robustness against L1 without compromising robustness against other attacks. However, training against (L\u221e, L1) is less effective compared to training against L1 or L\u221e separately. Training against different attack pairs like (L\u221e, L1) and (L\u221e, Elastic) leads to overfitting, with high training accuracy but poor validation accuracy. Overfitting is more prominent when training against large distortions, but successful training is achieved for small distortion sizes with accuracies comparable to training against individual attacks. This behavior aligns with findings from Tram\u00e8r & Boneh (2019). The curr_chunk discusses the relationship between overfitting and model capacity in the context of robustness to different attacks. It suggests that robustness to one attack does not guarantee robustness to others, and that adversarial training may not provide sufficient protection against unforeseen attacks. The need to explore alternatives to adversarial training is highlighted, with joint adversarial training being one option, but it often leads to overfitting and may not confer robustness to all types of attacks. The curr_chunk discusses the challenges in evaluating robustness in adversarial training, emphasizing the importance of choosing and calibrating diverse unforeseen attacks. It recommends following existing guidelines while cautioning about interpreting specific numeric results. Previous implementations of adversarial training were prone to gradient masking, but recent advancements have shown success with moderately many PGD steps. The curr_chunk highlights the limitations of evaluating model robustness against a single known attack type, suggesting that it may provide a misleading picture. Existing alternatives like DeepFool and CLEVER estimate \"empirical robustness\" but have their own limitations. The results suggest that evaluating against diverse attacks is crucial for a more accurate assessment of model robustness. The curr_chunk emphasizes the importance of not solely evaluating model robustness against a single known attack type. It suggests building a diverse toolbox for understanding machine learning models, including visualization, feature disentanglement, and extrapolation measurement. This approach can provide a clearer picture of how to make models reliable in real-world scenarios. The study utilized machines with 8 NVIDIA V100 GPUs for training, following best practices for multi-GPU training. They used synchronized SGD for 90 epochs with batch size 32\u00d78 and a specific learning rate schedule. Additionally, they trained on a single NVIDIA V100 GPU for 200 epochs with batch size 32 for CIFAR-10. The Frank-Wolfe algorithm was chosen for optimizing the L1 attack. The study utilized machines with 8 NVIDIA V100 GPUs for training, following best practices for multi-GPU training. They used synchronized SGD for 90 epochs with batch size 32\u00d78 and a specific learning rate schedule. Additionally, they trained on a single NVIDIA V100 GPU for 200 epochs with batch size 32 for CIFAR-10. The Frank-Wolfe algorithm was chosen for optimizing the L1 attack onto a truncated L 1 ball, detailed in Algorithm 1. New Attacks JPEG were introduced with L 1 or L 2 constraints in JPEG-space instead of the L \u221e constraint. The study introduced novel attacks that do not fall under the Lp threat model and provided pseudocode for the Frank-Wolfe algorithm for the L1 attack. Results showed similar outcomes for L1-JPEG attacks, which were omitted for brevity. Calibration values for the attacks are presented in Table 2, and full results are shown in Figure 11. The Lp attacks and defenses offer correlated information on heldout defenses and attacks, recommending evaluation on a wide range. The study recommends evaluating on a wide range of distortion types for attacks and defenses. Full UAR scores are provided for ImageNet-100, with selected results shown in Figure 13. Adversarial accuracies of attacks on adversarially trained models for different distortion sizes on ImageNet-100 are also presented. The results of adversarial attacks and defenses for CIFAR-10 are shown in Figure 14. Difficulty was experienced training L2 and L1 attacks at larger distortion sizes. The study evaluated attacks and defenses on CIFAR-10 and ImageNet-100 datasets. The \u03b5 calibration procedure was similar for both datasets, starting with small \u03b5 min values and increasing geometrically. Results were replicated with different random seeds to assess variation, showing minor deviations. Additionally, results were replicated with fewer steps in the attack to observe changes. The study evaluated attacks and defenses on CIFAR-10 and ImageNet-100 datasets, replicating results with different random seeds to assess variation. Evaluation accuracies of jointly trained models and attacks against jointly adversarially trained defenses were analyzed. Joint adversarial training showed dependence on random seed, with inconsistent results for certain distortion pairs. Training accuracies for joint adversarial training at large distortion sizes were found to be dependent on seed selection. The study evaluated attacks and defenses on CIFAR-10 and ImageNet-100 datasets, replicating results with different random seeds to assess variation. Evaluation accuracies of jointly trained models and attacks against jointly adversarially trained defenses were analyzed. ResNet-101 models were trained to assess model capacity and overfitting, showing higher performance than ResNet-50. Common visual corruptions were tested for robustness, linking it to adversarial robustness. Corruption robustness was evaluated on the ImageNet-C benchmark. Adversarial training against small distortions increases corruption robustness on the ImageNet-C-100 dataset. Training against larger distortions can decrease average accuracy due to a decrease in clean accuracy. Adversarial distortions and common corruptions affect defenses differently, with Lp-JPEG and elastic attacks being adversarial versions of common corruptions. Training against adversarial JPEG at larger \u03b5 improves robustness against adversarial JPEG attacks but decreases robustness against common JPEG corruptions. Adversarial Elastic training at large \u03b5 also hurts robustness to its common counterpart. Common corruptions are easier than adversarial distortions, leading to decreased clean accuracy despite increased robustness. Sample images of attacks against undefended models are shown in Figure 22. The curr_chunk discusses different levels of L2 and L\u221e \u03b5 values in training for various types of attacks. It includes parameters for brightness, contrast, elastic pixelate, JPEG, speckle noise, Gaussian blur, spatter, and saturation. The L1 \u03b5 values are also mentioned."
}