{
    "title": "BJxoz1rKwr",
    "content": "Self-supervised learning (SlfSL) has made significant progress by learning feature representations without human annotation. It is now seen as a promising solution for semi-supervised learning (SemSL) by utilizing unlabeled data. A new framework is proposed to combine SlfSL with SemSL, where the prediction target in SemSL is modeled as the latent factor in the SlfSL predictor. This leads to a new approach called Conditional Rotation Angle Prediction (CRAP), which marries the prediction targets of both learning processes. The Conditional Rotation Angle Prediction (CRAP) module predicts image rotation angle based on the candidate image class. CRAP outperforms other methods combining SlfSL and SemSL, showing high extendability. By incorporating a simple SemSL technique, CRAP achieves state-of-the-art SemSL performance, leveraging unlabeled data effectively. Conditional Rotation Angle Prediction (CRAP) module utilizes SemSL techniques to predict image rotation angles based on image class, outperforming other methods. SemSL and Self-supervised Learning (SlfSL) leverage unlabeled data effectively to improve predictive models. State-of-the-art SemSL methods combine various techniques like MixMatch, unsupervised data augmentation, \u03a0-Model, Mean Teacher, and mixup. SlfSL aims to learn representations without human annotation, assuming a properly designed pretext. The SlfSL approach aims to learn feature representations from unlabeled data without human annotation. It proposes a new framework that integrates SlfSL with SemSL by using the prediction target in SemSL as a latent factor in predicting the pretext target in SlfSL. This connection between the two learning processes is established through marginalization. The proposed framework integrates SlfSL with SemSL by predicting pretext targets conditioned on SemSL targets. A module called Conditional Rotation Angle Prediction (CRAP) is implemented for this purpose, showing better performance than other SlfSL-based SemSL approaches. The paper proposes a new SemSL framework that combines SlfSL and SemSL, introducing a new approach (CRAP) that outperforms other SlfSL-based SemSL methods. By extending CRAP with SemSL techniques, the method achieves state-of-the-art performance in SemSL. In recent years, various principles and methods have been developed in the field of deep learning, such as transductive models, multi-view approaches, generative model-based methods, and consistency regularization based methods like \u03a0-Model, Mean Teacher, Virtual Adversarial Training, and mixup. These methods aim to improve model performance by enforcing consistency in predictions and handling perturbations in input data. Mixup has emerged as a powerful SemSL regularization method that requires the output of mixed data to be close to the output mixing of original images. State-of-the-art approaches combine various techniques like Interpolation Consistency Training, MixMatch, and Unsupervised data augmentation to achieve good performance. SlfSL is another paradigm that learns feature representations through training on pretext tasks with non-human annotated labels. Various pretext tasks, such as image inpainting, are designed in different approaches. Pretext tasks like image inpainting, colorization, and rotation angle prediction are used in Self-Supervised Learning (SlfSL) methods to train models. These pretrained models can then be fine-tuned for downstream tasks. While SlfSL may not yet match fully-supervised approaches in performance, it offers insights into using unlabeled data and has potential for impact in semi-supervised learning. SlfSL-based SemSL is an emerging approach that integrates SlfSL into semi-supervised learning. In SemSL, a novel architecture is proposed to link SlfSL and SemSL tasks, aiming to improve posterior probability estimation over discrete class labels using both labeled and unlabeled training samples. The approach involves jointly training downstream and pretext tasks in a multi-task fashion without breaking them into stages. The SlfSL method aims to learn feature representations through a pretext task, where a target z is defined, such as the rotation angle of an input image. Two existing schemes for leveraging SlfSL for SemSL involve either learning features from the whole training set and then fine-tuning on labeled data, or jointly optimizing tasks of predicting y and z. The S4L method constructs a network with two branches for these tasks. Our framework differs in that it explicitly separates the pretext target z predictor from the task of predicting y. Our framework explicitly incorporates y into the predictor for z by treating y as the latent factor in p(z|x; \u03b8) and factorizing p(z|x; \u03b8) through marginalization. This allows for the estimation of p(y|x; \u03b8) and p(z|x, y; \u03b8) for making predictions about z, with the loss optimization providing gradients for both networks. This approach contrasts with S4L, where gradients from unlabeled data do not flow through p(y|x; \u03b8). Theoretical models suggest p(z|x; \u03b8) and p(y|x; \u03b8) as separate networks, but in practice, they are modeled as two branches connected to a shared feature extractor. p(z|x; \u03b8) is a pretext target predictor with partial observations on its latent variable y. The benefits include p(y|x; \u03b8) acting as a soft selector for predicting z, leading to more accurate predictions when y matches the true class \u0177(x). This selective updating reinforces both p(y|x; \u03b8) and p(z|x, y; \u03b8) during training. The terms p(y|x; \u03b8) and p(z|x, y; \u03b8) reinforce each other during training. Even if p(y|x; \u03b8) is inaccurate initially, p(z|x, y; \u03b8) can still act as an unsupervised feature learner. Predicting z in Eq. 1 involves evaluating p(z|x, y; \u03b8) for each candidate y, similar to creating an ensemble of diversified pretext target predictors. Training features with Eq. 1 may benefit from ensemble learning, leading to better features for modeling p(y|x; \u03b8) and p(z|x, y; \u03b8). The framework guides the transformation of a Self-Supervised Learning method into a Semi-Supervised Learning algorithm by modifying predictors and optimizing predictions on the dataset. An implementation example involves upgrading rotation-angle prediction-based Self-Supervised Learning to its conditional version. Conditional rotation angle prediction (CRAP) is an upgraded version of rotation angle prediction for semi-supervised learning. It involves allocating a rotation angle prediction branch for each class to optimize predictions on the dataset. Conditional rotation angle prediction (CRAP) utilizes a network with multiple branches and a shared feature extractor. The method includes auxiliary branches for training purposes, containing rotation predictors and a semantic classifier. An additional semantic classifier is introduced for final classification, learned only through labeled data loss. The CRAP method utilizes auxiliary branches for training, with a focus on feature training rather than final classification. To address the increase in parameters with a large C, dimension reduction is proposed for the features feeding into the rotation predictor. The method is shown to be effective even with a significant reduction in dimensionality. The CRAP method is highly adaptable and can be extended to improve the prediction of latent variables. By introducing a network module with a special structure and utilizing SemSL techniques, additional loss can be incorporated for better modeling. This extension involves rotating images, averaging predicted distributions, and applying a sharpening operation to improve classification accuracy. The CRAP method can be extended by introducing a network module with a special structure and utilizing SemSL techniques to incorporate additional loss for better modeling. This extension involves a temperature hyper-parameter and cross entropy between predicted distributions as an additional loss. An improved version of the conditional rotation prediction task is also introduced, where the network predicts rotation angles for mixed versions of rotated images. The network is challenged with recognizing the rotation angle of a target object mixed with distractions from another image, known as denoising rotation prediction. This task relies on correct predictions from the semantic classifier to resolve ambiguities in mixed images, making it more dependent on accurate predictions from p(y|x; \u03b8). The task involves predicting rotations in mixed images, relying on semantic classifier accuracy. CRAP provides strong supervision signals to p(y|x; \u03b8). It differs from mixup by not requiring mixed image outputs. Experiments compare CRAP to other SemSL algorithms and state-of-the-art methods, evaluating component contributions. For more details on CRAP and CRAP+, refer to the Appendix A.1. Experimental protocols are adopted for different datasets in CRAP. Settings for CIFAR-10, CIFAR-100, and SVHN follow previous works, while ILSVRC-2012 settings include inception crop augmentation and horizontal mirroring. Baselines are compared under the same setting, with labeled sample sizes varying across datasets. Three independent trials are conducted for each experiment. In experiments with various datasets, CRAP is compared to other SlfSL-based SemSL algorithms. Baseline approaches include Fine-tune and S4L methods. Results show the effectiveness of the Fine-tune strategy. The results in Table 1 show that the \"Fine-tune\" strategy improves accuracy with 500 to 2000 labeled samples but not with 250 and 4000. S4L performs better with fewer samples due to its down-stream-task awareness design. CRAP method outperforms both Fine-tune and S4L, halving the test error of S4L in most cases, as shown in Table 2. The results in Table 2 show that Fine-tune and S4L do not always outperform the Labeled-only baseline, especially on SVHN. S4L shows benefits with small labeled samples, but CRAP consistently outperforms all methods. In CIFAR-100, all SlfSL-based SemSL methods have better accuracy than Labeled-only, with S4L showing marginal improvement over Fine-tune. The proposed CRAP consistently outperforms S4L in all settings, with significant improvements over baselines. By reducing feature dimensions, test performance remains stable, validating the effectiveness of the approach. The proposed CRAP method is validated by reducing feature dimensions from 2048 to 16. CRAP+ is compared to state-of-the-art methods in SemSL, showing comparable performance to Mixmatch in various datasets. CRAP+ demonstrates comparable performance to Mixmatch in SemSL datasets, showcasing the innovative framework of marrying SlfSL with SemSL. The study focuses on the impact of different components in CRAP and CRAP+, highlighting the potential for further extension with techniques like MixUp. The study explores different modifications to CRAP and CRAP+ models, including incorporating SemSL loss, removing certain branches, and conducting ablation studies on CIFAR-10 datasets with varying label counts. The results show that extensions in CRAP+ can lead to improvements in performance. Extension 1 in Section 4.1 suggests that a stronger p(y|x; \u03b8) modeling could lead to greater improvement. Using an additional semantic classifier slightly improves performance over directly utilizing p(y|x; \u03b8) in auxiliary branches. However, training a SemSL method alone does not yield good performance, indicating that the superior performance of CRAP+ comes from its incorporation with the CRAP framework. Applying rotation as data augmentation does not improve performance over the labeled-only baseline. The proposed CRAP method effectively couples SemSL with SlfSL. The CRAP method, an implementation of a framework coupling SemSL with SlfSL, outperforms other SemSL methods on benchmark datasets. Two extensions to CRAP further enhance its performance, matching state-of-the-art SemSL methods."
}