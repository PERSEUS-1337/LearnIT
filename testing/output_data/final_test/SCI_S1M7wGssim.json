{
    "title": "S1M7wGssim",
    "content": "Neural machine translation (NMT) models learn representations with linguistic information. Unsupervised methods are developed to discover important neurons in NMT models, based on the intuition that different models learn similar properties. Translation quality depends on these neurons, which capture common linguistic phenomena. Controlling NMT translations is possible by modifying activations of individual neurons. Our hypothesis is that different NMT models learn similar properties, leading to the emergence of similar important neurons. Our methods capture different levels of localization/distributivity, illustrated in Figure 1. Regression ranking involves linear regression to find neurons with distributed information in other models. SVCCA is a method for analyzing neural networks. Canonical correlation analysis (SVCCA) is a method for analyzing neural networks to capture information in fewer dimensions. The importance of neurons in NMT models is verified using quantitative and qualitative techniques, including erasing neurons to test their impact on translation performance. The study focuses on erasing neurons from the top and bottom of the encoder in NMT models to analyze their impact on translation performance. Different techniques are used, such as MaxCorr, MinCorr, and LinReg, to evaluate the importance of specific neurons. SVCCA is also used to project embeddings onto non-erased directions for comparison between models trained on the same or different language pairs. The results show that erasing neurons from the top affects translation performance more than erasing from the bottom. The study analyzed the impact of erasing neurons from the top and bottom of NMT encoder models using different ranking methods. Results showed that erasing from the top had a greater negative effect on performance. Different ranking methods revealed that some neurons capture information related to the position of words, while others are more influenced by the identity of the current token. The study analyzed the impact of erasing neurons from the NMT encoder models. Some neurons capture position information, while others are influenced by the identity of the current token. Neurons can represent tense and context, even detecting irregular past tense verbs. The results were obtained with a charCNN representation sensitive to common suffixes. The study found neurons in the NMT encoder models that activate on various linguistic properties such as numbers, dates, adjectives, and more. They also identified high-scoring neurons related to noun phrase segmentation. The study identified high-scoring neurons in NMT encoder models that activate on linguistic properties like numbers, dates, and adjectives. They also found neurons related to noun phrase segmentation. The research explores controlling translations by modifying neuron activations in the model. The study focused on controlling translations by modifying neuron activations in NMT encoder models. They found that controlling properties like gender and number was challenging, with varying success rates using top neurons. Future work may explore more sophisticated methods for controlling multiple neurons simultaneously. The study explored controlling translations by modifying neuron activations in NMT encoder models, showcasing examples of controlling number and gender translation. The results demonstrate the ability to control multiple properties and languages, with successful transitions from plural to singular and masculine to feminine translations. The study focused on controlling translations by modifying neuron activations in NMT encoder models. It demonstrated successful transitions in gender and tense translations across multiple languages. Additionally, unsupervised methods were developed to identify important neurons impacting translation quality. The study analyzed linguistic properties captured by individual neurons in NMT models and designed a protocol for controlling translations by modifying desired properties. The analysis can be extended to other NMT components and architectures, as well as different datasets and NLP tasks. More work is needed to analyze localized vs. distributed information in neural language representations and develop sophisticated ways to control translation output."
}