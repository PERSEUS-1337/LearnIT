{
    "title": "rkgqN1SYvr",
    "content": "The selection of initial parameter values for gradient-based optimization of deep neural networks is crucial for convergence times and model performance. This study analyzes the impact of initialization in deep linear networks, proving that drawing initial weights from the orthogonal group speeds up convergence compared to standard Gaussian initialization. The width needed for efficient convergence with orthogonal initializations is independent of depth, while it scales linearly with depth for Gaussian initializations. Our results show the importance of good initialization in deep neural networks, with benefits persisting throughout learning. Deep learning has achieved remarkable success in various applications, such as image recognition and speech recognition, but creating high-performing models requires significant engineering effort. The study focuses on the impact of initialization on the convergence rate of gradient descent in deep linear networks. It proves that drawing initial weights from the orthogonal group speeds up convergence compared to standard Gaussian initialization. This highlights the importance of good initialization in deep neural networks for achieving high performance. The study emphasizes the significance of initialization in deep neural networks for efficient convergence. It demonstrates that orthogonal weight initializations can expedite convergence in deep networks, independent of depth, compared to Gaussian networks where width scales linearly with depth. This finding builds on prior research showing the benefits of orthogonal weights in improving convergence speed in various network architectures. The paper focuses on the benefits of orthogonal weight initialization throughout the training process, proving its advantage for optimization. It includes a review of related work, preliminaries, a positive result on efficient convergence from orthogonal initialization, and experiments supporting theoretical results on deep linear networks. Deep linear networks have been extensively analyzed, with studies on landscape properties and gradient descent trajectories. While local minima are global under certain assumptions, global convergence and convergence rates are not guaranteed. Recent work by Du & Hu (2019) shows that wider hidden layers than depth lead to global convergence with Gaussian initialization. The width of hidden layers is larger than the depth for global convergence with Gaussian initialization. This linear dependence is necessary for Gaussian initialization, while orthogonal initialization allows for independent width. Orthogonal weight initializations have also shown success in non-linear networks, with spectral properties linked to convergence speed. The benefits of dynamical isometry and orthogonality in deep linear networks are established. Orthogonal matrices improve stability and conditioning, allowing for learning over long time horizons in recurrent neural networks. The benefits of orthogonality in deep linear networks are well-established, improving stability and conditioning for learning over long time horizons. Methods to constrain optimization to the orthogonal group have been developed, but it is unclear if these benefits persist during training. Further analysis could determine when orthogonality is beneficial in the recurrent setting. The Kronecker product between matrices A and B is defined, and a linear neural network with weight matrices is considered for training by minimizing the 2 loss. The algorithm for training a deep linear network involves gradient descent with random initialization of weight matrices, updating them iteratively. Orthogonal initialization leads to efficient convergence to a global minimum, especially when hidden layer widths are not too small. The algorithm for training a deep linear network involves gradient descent with random initialization of weight matrices. Orthogonal initialization leads to efficient convergence to a global minimum. The minimum value for the objective is denoted by *. Our main theorem states that with a sufficiently large constant C, the learning rate \u03b7 should be less than or equal to dy 2L X 2. The width m does not need to depend on the depth L, unlike Gaussian initialization which requires a near-linear dependence between m and L. The study highlights the importance of a near-linear dependence between the width m and depth L for efficient convergence with Gaussian initialization. The proof utilizes a framework tracking the network's output evolution, closely linked to a time-varying PSD matrix. This matrix's eigenvalues are carefully bounded during training to establish the benefit of orthogonal initialization in optimizing deep linear networks. The study emphasizes the importance of bounding the eigenvalues of a time-varying PSD matrix during training to ensure convergence. This is achieved by making simplifying assumptions and tracking the network's output evolution. The key idea is to establish the benefit of orthogonal initialization in optimizing deep linear networks. To establish convergence, it is crucial to show that the higher-order term E(t) is small and to prove bounds on P(t)'s eigenvalues. Under orthogonal initialization, matrices are isometric at the start and remain close to isometry during training, aiding efficient convergence. Lemma 4.2 outlines properties at initialization, with the loss satisfying a certain probability bound. The spectral property follows from a direct relation, while bounding the network's initial output involves studying vector projection onto a low-dimensional subspace, manageable through concentration inequalities. In this section, the text discusses the proof of Theorem 4.1, focusing on the exponential scaling of running time in gradient descent with Gaussian random initialization. The proof involves establishing properties A(t), B(t), and C(t) for all t, with a specific condition (9) to be satisfied. The dependence of width and running time is highlighted, showing that the width must become nearly linear in depth to avoid exponential scaling. The text discusses minimizing the objective F by gradient descent with a large depth L. It assumes Y = W * X and sets the scaling factor \u03b1 to prevent exponential blow-up. The main theorem states that for a universal constant 0 < \u03b3 \u2264 1, there exists a constant c > 0. The text discusses the limitations of efficient convergence from Gaussian initialization for large depth unless the width becomes nearly linear in depth. A phase transition from untrainable to trainable occurs when the width and depth have a nearly linear relation. Theorem 5.1 generalizes the result of Shamir (2018) and provides an upper bound on A j. The text discusses the limitations of efficient convergence from Gaussian initialization for large depth unless the width becomes nearly linear in depth. The proof of Lemma 5.2 in Appendix B.1 uses Markov inequality and union bound. A key property is that if j \u2212 i is large enough, A j:i (0) will become exponentially small. Lemma 5.3 provides a bound on A j:i (0) with probability at least 1 \u2212 e \u2212\u2126(L \u03b3 ). By using an -net argument, A j:i (0) can be bounded for any fixed unit vector v \u2208 R di\u22121. The text discusses the limitations of efficient convergence from Gaussian initialization for large depth unless the width becomes nearly linear in depth. For any u \u2208 R di\u22121 , it can be written as u = q l=1 a l u l where a l is a scalar and u l is a unit vector supported on S l. The inequality holds for any u \u2208 R di\u22121, allowing us to maximize A j:i (0)u. A union bound over all possible (i, j) gives a failure probability at most. Lemma 5.4 shows that properties in Lemmas 5.2 and 5.3 are preserved after small perturbations on weight matrices. Lemma 5.4 states that properties from Lemmas 5.2 and 5.3 are maintained after slight changes in weight matrices. Lemma 5.5 controls the objective value and gradient near random initialization. Theorem 5.1's proof relies on these lemmas, ensuring certain conditions are met. Weight matrices in the \"initial neighborhood\" have objective values between 0.4 Y 2 F and 0.6 Y 2 F. In the \"initial neighborhood,\" the objective value is constrained between 0.4 Y 2 F and 0.6 Y 2 F. To escape this interval, iterations are needed as per Lemma 5.5. The proof is completed by showing the necessity of iterations to leave the \"initial neighborhood.\" Empirical evidence is provided to support the convergence speed of gradient descent under different initialization schemes. The study compares different depth-width configurations and initialization schemes in neural networks. It shows a transition from untrainable to trainable models when increasing network width. Gaussian initialization has a linear relation between width and depth, while orthogonal initialization has a width independent of depth for the transition. The study examines the effect of initialization parameters on the convergence time of deep linear neural networks. It shows that Gaussian initialization leads to exponential growth in convergence time with depth unless the width is at least 4 times larger. Orthogonal initialization enables fast training at small width, independent of depth. The study compares Gaussian and orthogonal initialization in deep linear neural networks. Gaussian initialization leads to slow convergence with depth unless width is increased, while orthogonal initialization allows for fast training at small width, independent of depth. This proves that orthogonal initialization is superior in terms of convergence time. The proof of Claim 4.4 and Claim 4.5 demonstrates the convergence of deep linear neural networks using orthogonal initialization. The analysis shows that orthogonal initialization allows for fast training at small width, independent of depth, proving its superiority in terms of convergence time. The proof demonstrates the convergence of deep linear neural networks using orthogonal initialization, showing fast training at small width regardless of depth. The inequality \u03bb min (W i\u22121:1 (t)X) is crucial in bounding the high-order term E(t) to be at most 1/6 \u03b7\u03bb min (P t ) 2 (t)."
}