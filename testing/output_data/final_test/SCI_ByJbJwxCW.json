{
    "title": "ByJbJwxCW",
    "content": "Recent advances in computing technology and sensor design have enabled the collection of large amounts of longitudinal medical data. An issue arises with the lack of annotations in medical time series data, leading to the introduction of concept annotation for medical time series. A solution proposed is Relational Multi-Instance Learning (RMIL), a deep learning framework using recurrent neural networks, pooling functions, and attention mechanisms for concept annotation. Our proposed models using neural networks outperform various multi-instance learning models on medical datasets. Clinicians are overloaded with patient data from multiple sources in various formats, which should be summarized for faster diagnosis and treatment. Graphical visualizations are a popular approach to show patient data to doctors, but recent studies have shown limitations. Recent studies have shown that graphical visualisations may not always be helpful for clinicians' decision-making, while text summaries are widely embraced and adopted in practice. Existing systems use natural language processing techniques to generate summaries from doctor notes, but they only utilize one data source which may contain noisy entries. Electronic health records offer other sources of patient data such as vital signs and lab results in multivariate time-series, providing more accurate and rich information for patient conditions. Few existing patient summarization systems extract information directly from these time series. In this work, the concept annotation task involves predicting and localizing medical concepts from time series data, aiming to provide clinicians with easily comprehensible information. Previous studies have shown success in predicting clinical events and outcomes using medical codes or time series data, but directly obtaining medical concept annotations from time series remains a challenge. The goal is to predict time series concepts like intubation, extubation, and resuscitation using a deep learning framework called Relational Multi-Instance Learning (RMIL). RMIL utilizes Recurrent Neural Networks (RNNs) and attention mechanisms to make concept predictions from multivariate time series data. The model can learn to detect concepts and localize instances even with limited training data. In the medical domain, RMIL, a deep learning framework, predicts concepts from multivariate time series data using RNNs and attention mechanisms. It outperforms popular MIL approaches and shows promising results on real-world medical datasets. The paper discusses related works, the MIL framework, experimental results, conclusions, and anomaly detection as another application of RMIL. Concept annotation in the medical domain is a relatively new problem with limited prior work. In the medical domain, concept annotation is addressed in clinical narrative mining and biomedical text mining literature. Other domains like web-mining and computer vision use concept annotation for tasks such as semantic annotation, image annotation, object localization, and image captioning. Automated discovery of temporal relations from clinical narratives and doctor notes is crucial for uncovering disease progression patterns in clinical informatics. Recent efforts like SemEval competitions have focused on evaluating clinical information extraction systems. In the medical domain, concept annotation is addressed in clinical narrative mining and biomedical text mining literature. They focus on identifying and extracting temporal relations from clinical notes. Image annotation and captioning involve object recognition systems for images and videos, with deep learning models achieving excellent results. Multi-Instance Learning is a researched topic in machine learning for drug activity. Multi-Instance Learning (MIL) was first introduced by BID20 for drug activity prediction and has since been applied to various domains like image and text annotations. Different adaptations and extensions of MIL frameworks have been proposed, including miSVM, MISVM, MIMLSVM, miGraph, MIGraph, MIMM, DPMIL, and autoregressive hidden Markov model for activity recognition. Deep learning models have been successfully applied for Multi-Instance Learning (MIL) frameworks, particularly for image annotation, labeling, segmentation, and classification tasks. Recent works have focused on deep multi-instance learning models using convolutional neural networks or deep neural networks. However, there is a lack of research on extending deep MIL models for multivariate time series data. This paper aims to propose and study deep multi-instance learning models for multivariate time series data. In this section, we describe multi-instance learning for multivariate time series data. MIL is a form of weakly supervised learning where data is in bags with a label for the entire bag. The goal is to make bag-level and instance-level predictions without instance labels during training. We focus on classification in MIL and model the relationship between instance and bag labels. In multi-instance learning, the bag label is determined based on the labels of instances within the bag. The standard assumption is that a bag is labeled positive if at least one instance is positive, but this can be relaxed to require multiple positive instances for a positive bag label. The bag classifier is defined by an aggregator function, with a threshold parameter for collective assumption. In multi-instance learning, the bag label is determined based on the labels of instances within the bag. A bag classifier is defined by an aggregator function with a threshold parameter for collective assumption. The concept annotation task involves detecting and localizing concepts in medical time series data. Each patient is associated with a medical time series denoted by X, where D is the number of features and T is the length of time series observations. The set of all concepts associated with the patients is denoted by C, and Y represents the concepts associated with each patient. In multi-instance learning, each patient's medical time series is treated as a bag, with instances representing observations at each time step. The task involves predicting labels Y based on input X during training, with assumptions about the relationship between prediction and localization labels. The concept annotation task aims to detect and localize concepts in medical time series data, with \u03b7 as a constant depending on the multi-instance learning assumption. Inspired by the success of recurrent neural networks in sequence modeling and classification tasks, we adapt these models to the MIL framework for concept annotation tasks. Our Relational Multi-Instance Learning framework (RMIL) combines RNN models like LSTM and Sequence-to-Sequence models to model temporal dependencies in multivariate time series data for concept annotations. RMIL utilizes RNN models for concept annotations, with instance label predictions and bag label predictions obtained through aggregators like pooling layers. Different pooling functions and attention mechanisms can be incorporated to enhance concept annotations. The aggregator function in RMIL can be modeled using pooling layers, where the RNN model maps feature time series to concept time series for each concept. Instance probabilities for belonging to a concept are denoted as pjk. The bag level probability for a concept in RMIL is determined by combining instance probabilities using an aggregator function. Various pooling mechanisms and attention mechanisms can be used to improve concept annotations and focus on specific instances within each bag. To improve instance-level predictions, attention mechanisms can be used to leverage information from hidden states at all time steps in an RNN. One approach is feature-based attention, where an attention matrix is designed based on features and their time-stamps. This matrix can be modeled in various ways, such as using element-wise multiplication and division with a weight matrix that can be learned. Attention mechanisms in RNNs can be improved by using different types of attention mechanisms. Feature-based attention involves designing an attention matrix based on features and time-stamps, while time-based attention captures the relation between current and previous time steps. Interaction-based attention further enhances time-based attention by considering both previous and current hidden states. In this case, the focus is on learning v, W1, W2 with S = Q/2 for interaction-based attention. A simplified version can be achieved by using vector w1, w2 instead of matrices and setting v = 1. The attention mechanisms, Attention-I and Attention-IS, aid in prediction and localization tasks. The RMIL models are tested on concept annotation tasks using a health-care dataset, comparing performance with multi-instance learning approaches. The impact of pooling functions and attention mechanisms in the RMIL framework is also discussed. The RMIL framework utilizes pooling functions and attention mechanisms for concept annotation tasks on medical time series data from the MIMIC-III RESP dataset. The dataset includes 21 respiratory-based features collected during the first 3 days after admission, with additional concept annotations extracted from doctor notes using the NOTEEVENTS table of MIMIC-III. The study extracted concept time series from doctor notes in the MIMIC-III database, with 1.85% of notes having timestamps. They identified respiratory-related concepts from medical literature and extracted top 26 respiratory concepts for analysis. The study compared proposed LSTM, S2S, and Bi-LSTM models to popular MIL models like MISVM, DPMIL, and CNN with attention. All models were trained with RMSProp optimization and early stopping. Input variables were normalized, and all models used the same feature time series data. The study compared LSTM, S2S, and Bi-LSTM models to MISVM, DPMIL, and CNN with attention. RMIL models outperformed non-deep MIL models by 8-10% for concept localization and 10-15% for concept prediction. RMIL slightly outperformed CNN-based models. LSTM model showed slightly better results for localization task. Evaluation metrics used were AUROC and AUPRC from 5-fold cross validation on MIMIC-III RESP dataset. The study compared LSTM, S2S, and Bi-LSTM models to MISVM, DPMIL, and CNN with attention, showing RMIL models outperforming non-deep MIL models for concept localization and prediction. LSTM model slightly outperformed others for localization task. Choice of attention mechanisms and pooling functions had varying impacts on overall performance in the RMIL framework. Interpretability of concept localization was demonstrated through RMIL model results. Ground truth annotations for respiratory concepts were shown in FIG3. The RMIL attention-based LSTM models accurately predict intubation and extubation concepts, showing intubation usually precedes extubation for the same patient. The models indicate that intubation commonly occurs within the first 24 hours of admission, with higher probability on the first day. This demonstrates the model's ability to learn instance-level relationships in medical time. In this paper, Relational Multi-Instance Learning (RMIL) framework using recurrent neural networks is presented for concept annotation from medical time series data. The proposed models outperform state-of-the-art multi-instance learning approaches on medical datasets. Experiments show that certain pooling functions like ISR and Noisy-OR can negatively impact instance prediction results. Anomaly detection from medical time series data is demonstrated as a use case application of the RMIL framework. The lack of effective tools for ventilator weaning and extubation readiness assessment leads to unnecessary days on ventilators and premature discontinuation, causing potential harm to patients. Pressure-Rate Product (PRP) calculated from esophageal pressure shows promise for guiding ventilator weaning, but is hindered by sensor artifacts and breathing pattern anomalies. Our goal is to automatically detect and remove anomalies in ventilator time series data using Relational Multi-instance learning models, enabling real-time clinical decision making. Anomalies can be caused by patient factors (cough, movement) and instrument factors (probing, catheter drift) and should be automatically detected. There is a rich body of research on anomaly detection in time series data, with various surveys available on generic anomaly detection algorithms and techniques for time series anomaly detection. Anomalies in time series data can be detected using various domain-specific techniques such as minimum bounding rectangles, symbolic representation for faster distance approximation, self-learning methods based on ordinal symbolic approximation, expert incorporation inspired by the immune system, and extracting exemplars from Euclidean pairwise distance to speed up anomaly detection algorithms. Anomalies in time series data can be detected using various domain-specific techniques. One approach is to use pairwise distance to speed up anomaly detection algorithms. In a Multi-instance learning setting, anomaly detection is formulated as a concept annotation problem, where the prediction and localization tasks correspond to predicting the presence and location of anomalies in medical time series data. An experiment on a PICU dataset involving mechanically-ventilated patients in a pediatrics ICU ward was conducted. In a pediatrics ICU ward, medical time series data from four sensors are used to detect anomalies in breathing conditions. The data includes flow volume spirometry, esophageal pressure sensor, and respiratory inductance plethysmography. Each subject can be in one of four breathing conditions: CPAP ventilation, Pressure Support ventilation, 5 minutes after extubation, and 60 minutes after extubation. An algorithm generates binary anomaly labels for each sensor signal, which are used as ground truth. The dataset is processed in a MIL framework by splitting sensor recordings into 5-second windows, with each window treated as an instance and each recording as a bag. Features are extracted using 20 MFCCs from each sensor signal in each window. In anomaly detection for breathing conditions in a pediatrics ICU ward, 20 MFCCs are extracted from sensor data for each 5-second window. Anomaly labels are set based on the percentage of anomalies detected. RMIL models outperform non-deep MIL models, with LSTM and Bi-LSTM RMIL models showing better results. The attention mechanism does not significantly improve localization but may enhance prediction tasks. Cluster-MIL* utilizes both instance and bag labels for training, unlike other models. The study involved creating a new dataset called MIMIC-III RESP-II by sampling feature time series every 6 hours. Results from RMIL models on this dataset showed similar performance for prediction tasks, with Bi-LSTM RMIL outperforming other models in localization."
}