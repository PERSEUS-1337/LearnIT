{
    "title": "B1uvH_gC-",
    "content": "We propose a metric-learning framework using Siamese networks to compute distance-preserving maps for low-dimensional embeddings on certain manifolds. Our approach reduces training effort by computing geodesic distances with a farthest point sampling strategy. The use of a network for mapping leads to improved non-local generalization compared to non-parametric methods. We validate our technique on point-cloud and image manifolds, demonstrating the power of neural networks in modeling manifold data. The representational power of neural networks in modeling manifold data is crucial for nonlinear dimensionality reduction. Embedding high-dimensional data into a low-dimensional space while preserving the metric structure of the data manifold is essential for achieving a coherent global representation. Geodesic distance plays a key role in this process, making the resultant embedding meaningful for dimensionality reduction. The success of deep learning has shown that neural networks can effectively approximate complex attributes in visual and auditory phenomena. In this paper, neural networks are used to model maps that preserve geodesic distances on data manifolds. The study explores the behavior of neural networks under geometric constraints, optimizing over the function that generates points rather than individual coordinates. This approach reduces the complexity of the problem to the number of parameters of the network. The study uses neural networks to model maps preserving geodesic distances on data manifolds. It reduces complexity to network parameters rather than data points, enabling efficient memory and computational usage. Efficient sampling techniques select landmark points for training networks to generate accurate low-dimensional embeddings. Visualization in Figure 1 demonstrates the proposed approach's effectiveness in representing manifolds. Performing numerical analysis to measure the quality of embedding generated by neural networks and associating an order of accuracy to a given Figure 1: Learning to unfurl a ribbon. Demonstrating that parametric models offer better non-local generalization compared to extrapolation formulas of non-parametric counterparts. Advocating for a stronger connection between axiomatic computation and parametric learning methodologies. Existing MDS frameworks use a geometrically meaningful objective in a cumbersome non-parametric framework, while learning-based methods like DrLim BID18 use a computationally desirable infrastructure with a geometrically suboptimal objective. The proposed approach in manifold learning involves using a parametric neural network optimized by multidimensional scaling. Spectral methods dominate the literature on manifold learning, with techniques like Laplacian Eigenmaps, LLE, HLLE, and Diffusion Maps considered as local methods. Spectral techniques like Laplacian Eigenmaps, LLE, HLLE, and Diffusion Maps are commonly used in manifold learning. Local methods, such as Diffusion Maps, aim to minimize local distortion for embeddings that preserve locality. Global methods like Isomap enforce preserving all geodesic distances in the low dimensional embedding. Local methods are computationally advantageous due to sparse matrix eigenvalue problems, while global methods are more robust to noise and produce globally coherent embeddings. However, local methods can sometimes lead to excessively clustered results. Spectral techniques are non-parametric and do not characterize the generating map, leading to computational challenges with large data sets. BID3 and BID13 address this issue by providing formulas for out-of-sample extensions in spectral methods. Multidimensional scaling (MDS) is a classical problem in geometry processing and data science, aiming to find an embedding that preserves pairwise distances. Classical Scaling and Least Squares Scaling are two prominent versions of MDS, with Classical Scaling based on double centering of pairwise squared distances to obtain the desired embedding. Multidimensional scaling (MDS) involves finding an embedding that preserves pairwise distances. Classical Scaling minimizes Strain using eigen-decomposition, while Least Squares Scaling minimizes misfits between pairwise distances. MDS framework estimates geodesic distances and generates global embeddings by preserving metric properties. Least squares scaling technique is also discussed. The Laplace-Beltrami Operator of the manifold is discussed, along with a least squares scaling technique to overcome holes and non-convex boundaries. Algorithms for non-parametric manifold learning are explored, with some out-of-sample extensions suggested. Neural networks are examined for representing data manifolds, with a focus on unsupervised parametric manifold learning approaches using Siamese configuration and Stochastic Neighborhood Embedding. These techniques require extensive training efforts with a large number of training examples. Efforts in generating satisfactory embeddings require a large number of training examples. BID30 uses a parametric network to enforce a manifold criterion for similar representations of nearby points. BID1 argues that neural networks can efficiently represent manifolds as a chain of linear segments. BID17, BID22, and BID10 use neural networks for out-of-sample extension in manifold learning, but with some deficiencies in their approaches. Adopting a parametric approach to non-linear manifold learning is advantageous. In BID18, Siamese Networks are proposed for manifold learning using the hinge-embedding criterion as a loss function. A Siamese configuration consists of two identical networks processing separate data units to compare output pairs in a loss function. Contrastive training involves constructing pairs. Contrastive training in Siamese Networks involves constructing pairs with positive and negative labels to build a nearest neighbor graph from manifold data. Training with a specific loss function requires careful selection of negative examples to prevent excessive clustering of neighbors, leading to a hard-negative sampling problem. Incorporating Least Squares Scaling into the Siamese configuration can improve the quality of embedding. Scaling into the computational infrastructure of the Siamese configuration involves estimating geodesic distances for pairs and training the network to preserve these distances. The advantage of using a specific loss function is that it eliminates the negative sampling problem and allows for efficient manifold sampling techniques like Farthest Point Sampling. This strategy selects landmarks that uniformly cover the manifold, improving training with fewer pairs of examples. The proposed geometric manifold learning algorithm involves selecting landmarks that uniformly cover the manifold by maximizing geodesic distances. The algorithm consists of two steps: computing nearest-neighbor graph and obtaining landmark points with pairwise geodesic distances, then forming a dataset for training the network in Siamese configuration. This approach eliminates the need for negative sampling and allows for efficient manifold sampling techniques. In experiments, landmark pairs with geodesic distances are used to train a network in Siamese configuration with least-squares MDS loss. Point cloud data experiments involve using MLP with PReLU() activation function and ADAM optimizer. Results are shown for the Helical Ribbon and S-Curve. The results of the method on the Helical Ribbon and S-Curve with varying training samples show the impact of landmarks on the quality of the low-dimensional embedding. Stress function is used to measure the MDS fit quality, with Stress decreasing as the number of training points increases in a 2-Layer MLP setup. Questions about the number of landmarks, layers, and hidden nodes per layer arise in this numerical methods setup. In numerical methods, the accuracy of solutions depends on the resolution of the spatial grid. Methods are ranked by their order of accuracy, determined by the relationship between approximation error and grid resolution. This principle is extended to evaluate network architectures for isometric map quality using the Stress error function. The experiment results show that a single layer MLP can model functions up to the first order of accuracy, while adding an extra layer increases representational power to the second order. However, adding more layers does not significantly improve accuracy and may lead to overfitting. A two-layer MLP with 70 hidden nodes per layer is recommended for approximating the isometric map of the S-Curve with 200 landmarks. The parametric MDS framework is extended to image articulation manifolds with binary images modulated by a few parameters. The datasets provide a test-bed for evaluating metric preserving algorithms, with a horizon articulation manifold constructed using sinusoidal basis elements. Each sample has an intrinsic dimensionality of two articulation parameters. The proposed method generates 1000 images of the horizon articulation manifold using a network architecture with convolution layers and a fully connected layer. Training is done with the ADAM optimizer and specific parameters. The proposed method uses BID20 with a learning rate of 0.01 and parameters (\u03b2 1 , \u03b2 2 ) = (0.95, 0.99) to train the network with 50 Landmark points. Comparison with other methods shows high fidelity to the ground truth, indicating superior metric preservation. Training on the articulation manifold allows for a detailed understanding of neural network actions. The distance preserving loss helps in learning image filters that separate underlying frequencies on the non-linear manifold. The Landmark-Isomap method compares parametric multi-dimensional scaling to its non-parametric competitor, providing a visual validation of the parametric map. The approach uses landmarks to estimate embeddings of points, with a quantitative and visual comparison using the image articulation manifold dataset. Both methods are evaluated with the same set of landmarks, showing results from two independent horizon articulation datasets. The study compares Landmark-Isomap and a neural network on horizon articulation datasets. Landmark-Isomap shows better metric preservation on training data but poor generalizability. The neural network, despite higher stress values on training, exhibits better generalization. The experiment visualizes non-local generalization by training on manifolds with specific parameters. The neural network outperforms Landmark-Isomap in generalization on a realistic dataset with 1369 images. The integrated approach shows improved results in a shorter training time, using 600 landmarks and DrLim's architecture for embedding. In this paper, a scheme combining parametric modeling with neural networks and multidimensional scaling is demonstrated to improve training efficiency and generalization abilities. Future work aims to enhance numerical algorithms using learning methodologies."
}