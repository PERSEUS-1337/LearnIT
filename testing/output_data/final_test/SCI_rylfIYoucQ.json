{
    "title": "rylfIYoucQ",
    "content": "Deep neural networks face limitations on embedded devices like mobile phones due to high computing requirements. A novel encoding scheme using {-1,+1} decomposes quantized neural networks into multi-branch binary networks, enabling efficient implementation through bitwise operations for model compression and resource saving. The method offers up to ~59 speedup and ~32 memory saving compared to full-precision counterparts, allowing users to adjust encoding precision based on needs and hardware resources. This approach is well-suited for FPGA and ASIC use, providing a viable solution for smart chips. Validation on large-scale image classification tasks demonstrates the effectiveness of the proposed method. The effectiveness of various methods for computational acceleration in deep neural networks has been explored, including network sparsity, low-rank approximation, architecture design, model quantization, and weight constraints. While some approaches achieve limited acceleration, others suffer from significant performance degradation. Most models are designed for fixed precision, leading to slow convergence speed in training and difficulty in extending to other precision models. To bridge the gap between low-bit and full-precision models, a novel encoding scheme using {\u22121, +1} is proposed to decompose trained QNNs into multi-branch binary networks. This allows for efficient inference through bitwise operations, leading to model compression, computational acceleration, and resource saving. The encoding scheme enables encoding vectors to binary form using M bits, reducing the computational cost of matrix multiplication. The encoding scheme proposed uses {-1, +1} to encode vectors into binary form using M bits, allowing for efficient inference and model compression. This scheme decomposes the dot product into M \u00d7 K sub-operations, enabling the representation of both positive and negative integers. The novel encoding scheme proposed uses {-1, +1} as basic elements for vector multiplication operations in deep neural networks, suitable for various layers including convolution and deconvolution. Trigonometric functions are utilized for encoding, with a final hard division into -1 or +1 using the sign function. The mathematical expression is formulated accordingly. The network architecture chosen for CIFAR-10 and ImageNet is ResNet-18, known for its effectiveness in training on large-scale datasets like ImageNet. Parameter initialization is crucial for training on large-scale datasets like ImageNet. Well-trained full-precision model parameters can be used as initialization for quantized networks, leading to successful training after fine-tuning. The loss computed by quantized parameters is used to update full precision parameters."
}