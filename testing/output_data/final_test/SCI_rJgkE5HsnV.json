{
    "title": "rJgkE5HsnV",
    "content": "The paper proposes a method for incremental domain adaptation using a recurrent neural network (RNN) with a parameterized memory bank. This memory bank is updated with new slots when adapting to a new domain, increasing the model capacity. New memory slots are learned and existing parameters are fine-tuned through back-propagation. Experiments demonstrate that our method outperforms previous work on incremental domain adaptation (IDA), such as elastic weight consolidation and progressive neural networks. Our approach is more robust for old domains, supported by empirical and theoretical results. Incremental domain adaptation is crucial for transferring knowledge between different domains in machine learning systems, especially for data-hungry neural networks prone to overfitting. In scenarios where a company works with various partners over time, access to partner data is limited to current contracts. The company owns the machine learning model and aims to retain knowledge within it, independent of data availability. Additionally, rapid adaptation to new domains is essential. Traditional methods require retraining from scratch, while fine-tuning suffers from forgetting previous domains. This poses challenges in predicting data points from unknown domains. Progressive neural networks address the issue of forgetting previous domains during fine-tuning by progressively growing the network capacity. A new approach proposed in this paper is a progressive memory bank for incremental domain adaptation, augmenting a recurrent neural network with a memory bank to retain knowledge across domains. The model augments a recurrent neural network with a memory bank to capture domain knowledge, retrieved by an attention mechanism. It progressively increases memory slots when adapted to new domains, fine-tuning all parameters. The approach prevents forgetting previous knowledge and outperforms na\u00efve fine-tuning methods in Natural Language Inference and Dialogue Response Generation tasks. Our model, based on an RNN, incorporates an external memory bank to enhance model capacity. This memory augmented RNN is used for incremental domain adaptation, outperforming other techniques like EWC. The model incorporates a memory bank to increase capacity, using attention probabilities to retrieve memory content for RNN transitions. Memory slots contain key and value vectors, with attention weights computed for retrieval. Memory content is retrieved through a weighted sum of values, concatenated with the current word for processing. The memory bank in the model captures distributed knowledge and can be expanded to adapt to new domains by adding new memory slots learned from target data. The attention mechanism enables end-to-end training of memory content retrieval along with other parameters. The model's memory bank can be expanded to adapt to new domains by adding new memory slots learned from target data. During training, all parameters are updated by gradient descent. The approach is evaluated on natural language inference, achieving an accuracy of 68.37 on the official MultiNLI test set using a bi-directional LSTM. Our implementation and tuning of BiLSTM for IDA are fair, ready for study. Comparison with baselines and variants in Fic and Gov domains. Results in Table 1 show performance of RNN and memory-augmented RNN. Memory bank doesn't help RNN much in non-transfer setting. Performance improvement due to IDA technique confirmed. Application of two domain adaptation methods. In experiments, multi-task learning and fine-tuning are applied for domain adaptation. Both methods perform similarly on the target domain. Fine-tuning slightly outperforms training on the source domain only. Fine-tuning achieves the worst performance on the source compared to other methods. Our approach outperforms fine-tuning on both source and target domains, showing that increasing model capacity prevents catastrophic forgetting. Memory expansion combined with target-specific word learning yields the best performance. Comparing expanding hidden states to expanding memory capacity, the latter proves to be more robust, especially on the source domain. Previous work on IDA like EWC shows inconsistent results across different applications. The study compares different approaches in the context of incremental domain adaptation. Results show that the proposed approach outperforms others significantly on both the source and target domains. The study compares various approaches in incremental domain adaptation, showing that the proposed approach performs better on both source and target domains. Testing on multiple domains, including Fic, Gov, Slate, Tel, and Travel, demonstrates high performance on new and previous domains. The model achieves comparable results to multi-task learning and outperforms EWC and progressive neural network on all domains, indicating the effectiveness of the approach with more than two domains. In this paper, a progressive memory network is proposed for incremental domain adaptation (IDA). The network augments an RNN with an attention-based memory bank, adding new slots during IDA and tuning all parameters through back-propagation. The progressive memory network avoids catastrophic forgetting by increasing model capacity with new memory slots. Compared to expanding hidden states, the memory bank offers a more robust way to increase capacity, as shown empirically and theoretically. Memory bank provides a robust way to increase model capacity, outperforming previous work for IDA. Progressive neural network proposed by Rusu et al. gradually increases hidden states to avoid overriding existing information. Multiple predictors are used, requiring data samples to be labeled with their domain during test time. Yoon et al. (2018) propose an extension of the progressive network by identifying relevant hidden units for new tasks with a sparse penalty. Sparsity is uncommon in RNNs for NLP applications as it can be harmful. Our work is related to memory-based neural networks like the end-to-end memory network proposed by Sukhbaatar et al. (2015) for question answering tasks. Memory-based neural networks like the neural Turing machine (NTM) and slot-value memory are used for various tasks such as question answering and conversation systems. Our approach stores knowledge in a distributed fashion in a memory bank with each slot representing different information. Our memory bank stores knowledge in a distributed fashion, with each slot representing different information. The proposed IDA process provides a natural way of incremental domain adaptation, showing that expanding memory is more stable than expanding hidden states. The theoretical analysis suggests that this approach is suitable for IDA. Theoretical analysis focuses on the effect of model expansion on hidden states in RNNs. Assumptions include linear activation function, memory slots being d-dimensional, variables having zero mean and variance \u03c3^2, and learned memory slots following the same distribution. Theoretical analysis examines the impact of model expansion on RNN hidden states, assuming linear activation, d-dimensional memory slots with zero mean and variance \u03c3^2, and learned slots following the same distribution. The expanded hidden states are denoted as h i\u22121 and h i, with weights W connecting them. The effect of expanding memory slots on attention content vectors is also considered. Theoretical analysis explores the impact of model expansion on RNN hidden states and attention content vectors. The weight matrix W connects attention content to RNN states, and attention weights are computed before and after memory expansion. The attention probability is adjusted accordingly, as shown in Figure 7. Theoretical analysis examines the effect of model expansion on RNN hidden states and attention content vectors. Attention weights are adjusted based on memory expansion, ensuring total attention remains constant. The theorem requires more attention on existing memory slots than on newly added slots, aligning with training practices and domain considerations. The base model for the memory network includes 300D RNN hidden states and pretrained GloVe embeddings. The Adam optimizer is used for training with a batch size of 32. The initial learning rate is set to 0.0003. Progressive memory slots are tuned, with 500 slots chosen for each domain. Performance improves with more slots, peaking around 500. Table 3 illustrates the dynamics of IDA with the progressive memory network. Our progressive memory network shows quick adaptation to new domains without suffering from catastrophic forgetting. It achieves the best performance in most domains for dialogue response generation tasks, even with incremental training. A small dataset is manually constructed for target domain adaptation. A small dataset of 15k message-response pairs from the Ubuntu Dialog Corpus is used for quick adaptation to a new domain. Evaluation metrics include BLEU-2 and average Word2Vec embedding similarity. The results for dialogue response generation show that BLEU-2 and W2V similarity are not always consistent. Model variants achieve the best performance on most metrics and consistently outperform all other IDA approaches. Statistical tests confirm the superiority of the proposed method over other IDA methods. The experiment results show the effectiveness of the IDA model for classification and generation tasks, despite the noisy evaluation of dialogue systems."
}