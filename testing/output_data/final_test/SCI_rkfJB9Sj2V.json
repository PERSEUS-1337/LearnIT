{
    "title": "rkfJB9Sj2V",
    "content": "Hierarchical Reinforcement Learning is a promising approach for long-horizon decision-making with sparse rewards. A new algorithm is proposed to continuously adapt skills and the higher level when training on a new task. The method, Hierarchical Proximal Policy Optimization (HiPPO), efficiently trains all levels of the hierarchy simultaneously. Additionally, a method for training time-abstractions improves skill robustness to environment changes. Hierarchical Reinforcement Learning (HRL) is a technique that addresses the inefficiency of learning new behaviors from scratch in Reinforcement Learning (RL). HRL involves policies with multiple abstraction modules, making it easier to reuse subsets of these modules. This approach is particularly useful for tasks with sparse rewards or long horizons. The HiPPO algorithm efficiently trains all levels of the hierarchy simultaneously, improving skill robustness to environment changes. In this paper, a new framework is developed for adapting all levels of temporal hierarchies simultaneously in Hierarchical Reinforcement Learning (HRL). The approach involves deriving an efficient approximated hierarchical policy gradient, where the manager's decisions are considered part of the observation from the perspective of the sub-policies. This method aims to improve skill robustness to environment changes and optimize pre-trained sub-policies for new tasks. In this study, a new hierarchical policy gradient technique is introduced, decoupling the gradient with respect to the manager and sub-policies parameters. An unbiased sub-policy specific baseline is also proposed for faster convergence. Additionally, a more stable gradient usage method, Hierarchical Proximal Policy Optimization (HiPPO), is presented for conservative policy space exploration in hierarchies. The benefits of varying time-commitment to sub-policies are evaluated, showing improved performance and zero-shot adaptation to similar tasks in a discrete-time finite-horizon discounted Markov decision process (MDP). The discrete-time finite-horizon discounted Markov decision process (MDP) is defined by a tuple M = (S, A, P, r, \u03c1 0 , \u03b3, H), where S is a state set, A is an action set, and P is the transition probability distribution. The objective is to find a stochastic policy \u03c0 \u03b8 that maximizes the expected discounted reward within the MDP. The study focuses on learning a manager that combines provided sub-policies for improved performance and zero-shot adaptation to similar tasks. In this paper, a new Hierarchical Reinforcement Learning (HRL) method is introduced that focuses on continuously adapting sub-policies to achieve optimal performance in new tasks. The approach prevents sub-policy collapse behavior and improves performance by learning all levels of abstraction in the hierarchical policy. The manager learns to utilize low-level skills effectively, while the skills are adapted for maximum performance. In this section, a new method called Hierarchical Proximal Policy Optimization (HiPPO) is introduced for hierarchical policies in Reinforcement Learning. The method addresses the challenge of incorporating intermediate decisions into the Markovian framework and introduces concepts like information bottleneck and trajectory compression to improve learning reusable skills. HiPPO is an on-policy algorithm that monotonically improves the RL objective, preventing sub-policy collapse. Policy gradient algorithms estimate the gradient of returns with respect to policy parameters. In the context of Hierarchical Reinforcement Learning (HRL), a hierarchical policy with a manager selects sub-policies to execute. The probability of a trajectory can be written as a mixture action distribution, which can lead to numerical instabilities. In Hierarchical Reinforcement Learning, policy gradient algorithms can face instabilities due to the product of sub-policy probabilities. A Lemma provides an approximation of the policy gradient, showing that diverse skills can reduce errors. The manager's actions are influenced by the differentiation of skills, where the latent variable can be considered part of the observation for gradient computation. In Hierarchical Reinforcement Learning, policy gradient algorithms can face instabilities due to the product of sub-policy probabilities. The latent variable can be treated as part of the observation to compute the gradient of the trajectory probability. Algorithms promoting diversity among skills help reduce errors. A common approach to mitigate variance in the REINFORCE policy gradient estimate is to subtract a baseline from the returns. An unbiased latent dependent baseline can be formulated for the approximate gradient. In policy optimization, using a baseline like the Advantage function can reduce variance. Incorporating techniques from Proximal Policy Optimization can further enhance stability and sample efficiency. Adjusting the cost function in PPO prevents large policy changes, ensuring stable learning. In policy optimization, using a baseline like the Advantage function can reduce variance and incorporating techniques from Proximal Policy Optimization can enhance stability and sample efficiency. The new surrogate objective for the algorithm Hierarchical Proximal Policy Optimization (HiPPO) is DISPLAYFORM1. Two critical additions are introduced: switching time-commitment between skills and an information bottleneck at the lower-level. Time-commitment to skills is a random variable sampled from a fixed distribution Categorical(T min , T max) before the manager makes a decision, improving zero-shot adaptation to a new task. The HiPPO algorithm in policy optimization introduces two key additions: limiting the values of z and providing masked observations to skills. The masking function restricts task information, forcing skills to rely on latent codes for task completion. This setup acts as a lossy compression, encoding problem information into log n bits for the next p timesteps. The HiPPO algorithm introduces key additions like limiting z values and masked observations for skills, encoding problem information into log n bits for the next p timesteps. Experiments compare HiPPO against flat policy, showing faster learning and better performance. The HiPPO algorithm introduces new additions like limiting z values and masked observations for skills, encoding problem information into log n bits for the next p timesteps. Experiments show HiPPO outperforms flat policy in terms of faster learning and improved performance. Additionally, the presented baseline method demonstrates effectiveness in fine-tuning pre-trained subpolicies with a preference for cautious behavior, leading to higher final performance compared to traditional methods like PPO. In this paper, the HiPPO algorithm is introduced to adapt hierarchical policies effectively. The algorithm can train multiple layers of a hierarchy stably and optimize pretrained skills for downstream environments. It can also learn emergent skills without unsupervised pre-training and outperform non-hierarchical methods on zero-shot transfer tasks. Future work could involve using a variational autoencoder with an information bottleneck to further improve HiPPO's performance on various tasks. In this work, the HiPPO algorithm aims to enhance hierarchical policies and extend performance gains to different tasks. The focus is on building temporal abstractions by having higher levels make decisions at a slower frequency than lower levels. While there has been interest in Hierarchical Reinforcement Learning (HRL) for decades, recent applications to high-dimensional continuous domains show promise. Various methods are used to obtain lower level policies, such as access to demonstrations or policy sketches. Various methods are used to obtain lower level policies in Hierarchical Reinforcement Learning (HRL), such as access to demonstrations or policy sketches. Some methods use a different reward for the lower level, often constraining it to be a \"goal reacher\" policy. Our method does not require additional supervision and obtained skills are not constrained to be goal-reaching. Transferring skills to a new environment in HRL methods involves keeping them fixed and training a new higher-level on top. Our algorithm allows for seamless adaptation of skills in Hierarchical Reinforcement Learning, without the need for a hand-defined curriculum of tasks. Unlike other methods, our approach avoids overwriting fixed skills when adding new layers of hierarchy. The options framework is a general framework for defining temporally extended hierarchies, with a focus on the termination policy to prevent skill collapse. Our algorithm introduces random-length skills in Hierarchical Reinforcement Learning, avoiding skill collapse without the need for regularizers. This approach shows good performance in complex tasks, surpassing previous works that only used discrete action MDPs. Additionally, we propose two beneficial modifications: random time-commitment and an information bottleneck for better skill generalization. Our algorithm introduces random-length skills in Hierarchical Reinforcement Learning, avoiding skill collapse without regularizers. It shows good performance in complex tasks, surpassing previous works using discrete action MDPs. Two modifications are proposed: random time-commitment and an information bottleneck for better skill generalization. The algorithm is evaluated on various robotic navigation tasks, including the Gather environment where the agent collects apples while avoiding obstacles. In a challenging hierarchical task, agents collect apples and avoid bombs in the Gather environment using Snake and Ant robots. Snake has a 17-dimensional observation space and 4-dimensional action space, while Ant has a 27-dimensional observation space and 8-dimensional action space. Performance metrics for both robots are displayed in a table. Zero-shot transfer performance of flat PPO, HiPPO, and HiPPO with randomized period is compared in different modified environments using Snake and Ant robots. Changes in body mass, joint dampening, body inertia, and friction characteristics are tested. Results show that HiPPO with randomized period learns faster and handles dynamic changes better. The randomized period in HiPPO outperforms fixed period on 6 out of 8 tasks without gradient steps. The policy adapts to various scenarios and separates representations for planning and locomotion. Empirical evaluation shows the quality of the assumption, with cosine similarity between approximate and computed gradients. Sub-policies present should be considered. In this section, the assumption of diverse sub-policies is empirically tested using the HiPPO algorithm on Snake Gather and Ant Gather tasks. The average maximum probability under other sub-policies is reported to be around 0.1, indicating a negligible factor in the experiments. The cosine similarity between the full gradient and the approximated one is close to 1, confirming the quality of the approximation. For experiments with PPO and HiPPO, consistent results were obtained with two random seeds. Both used a learning rate of 3 \u00d7 10 \u22123, clipping parameter = 0.1, 10 gradient updates per iteration, batch size of 100,000, and discount \u03b3 = 0.999. HiPPO employed n = 6 sub-policies, with Ant Gather having a horizon of 5000 and Snake Gather 8000. HiPPO utilized a manager network with 2 hidden layers of 32 units and a skill network with 2 hidden layers of 64 units. Flat PPO had a network with 2 hidden layers of 256 and 64 units to match parameter count. HiPPO with randomized period resampled p \u223c Uniform{5, 15} for each manager network output, incorporating the timesteps until the next latent selection as input. The number of timesteps until the next latent selection is used as input for the manager and skill networks. Skill-dependent baselines receive the active latent code and time remaining until the next skill sampling. If skills are sufficiently differentiated, the latent variable can be treated as part of the observation to compute trajectory probability gradients. The policy is modeled as a discrete distribution with n outcomes. When taking the gradient of log P (\u03c4) with respect to the policy parameters, the dynamics terms disappear. To avoid numerical instabilities, the sum over possible values of z needs to be approximated by a single term, which can then be decomposed into a sum of logs. The policy is modeled as a discrete distribution with n outcomes. When taking the gradient of log P (\u03c4) with respect to the policy parameters, the dynamics terms disappear. To avoid numerical instabilities, the sum over possible values of z needs to be approximated by a single term, which can then be decomposed into a sum of logs. Under a specific latent, the probability of the sub-trajectory under a different latent is upper bounded. By applying the product rule and Lipschitz continuity of the policies, we can replace the summation over latents with a single term. The gradient of the probability of the trajectory includes the variables z as if they were observed. To prove the baseline is unbiased, we apply the law of iterated expectations and undo the gradient-log trick. The same strategy is used to prove the second equality by expressing the expectation as an integral."
}