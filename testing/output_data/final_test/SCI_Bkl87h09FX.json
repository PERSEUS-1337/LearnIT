{
    "title": "Bkl87h09FX",
    "content": "The paper discusses the progress in contextualized word representation through unsupervised pretraining tasks like language modeling with methods like ELMo. It presents a systematic study comparing different pretraining tasks, highlighting the effectiveness of language modeling. However, the study also reveals strong baselines and varied results across tasks, questioning the efficacy of pretraining and freezing sentence encoders for future work. During pretraining, the shared encoder and task-specific model are trained for each task, then the shared encoder is frozen and the task-specific model is retrained for each evaluation task. State-of-the-art NLP models include components for extracting sentence representations, typically trained directly for the task. Pretraining for sentence encoding is gaining interest to leverage external data and training signals. Pretraining sentence encoders is gaining interest to leverage external data and training signals. Recent papers have shown strong performance of pretrained sentence encoders on NLP tasks, utilizing various pretraining methods. This paper explores the effectiveness of training reusable sentence encoders on multiple pretraining tasks and their combinations. It evaluates 40 sentence encoders on nine language understanding tasks, finding that language modeling is the most effective pretraining task. In this experiment, language modeling is found to be the most effective pretraining task for sentence encoders. Multitask learning during pretraining can provide additional gains and achieve a new state-of-the-art. However, there are concerns about the brittleness of ELMo-style pretraining, as trivial baseline representations perform nearly as well as the best pretrained encoders. Different target tasks show varying benefits from pretraining, and multitask pretraining does not offer general-purpose pretrained encoders. The recent surge in progress in sentence encoder pretraining has been seen with models like CoVe, ULMFit, ELMo, and Transformer LM. However, there is a lack of understanding regarding the relative merits of these models. Zhang & Bowman (2018) found that language modeling is the most effective pretraining task for sentence encoders, uncovering the most syntactic structure. Multitask representation learning in NLP has been well studied, showing that pretrained encoders can effectively encode sentences with good morphological and syntactic properties. However, using these pretrained encoders for text classification tasks does not always yield state-of-the-art performance. The text discusses multitask representation learning in NLP, highlighting the benefits of multitask learning in sentence-to-vector encoding. It compares encoders pretrained on various tasks and task combinations, serving as baselines or based on promising prior work. The primary baseline is pretraining on a task with zero examples. The primary baseline for multitask representation learning in NLP involves pretraining a sentence encoder with zero examples. This baseline performs well, surpassing bag-of-words encoders, and is strengthened by a skip connection allowing direct access to word representations. Various tasks from GLUE are used for pretraining, including acceptability classification, binary sentiment classification, and semantic similarity tasks. The curr_chunk discusses tasks such as similarity with various corpora, textual entailment, and training language models on datasets like WikiText-103 and 1 Billion Word Language Model Benchmark. It also mentions training MT models on WMT14 English-German and WMT17 English-Russian datasets. The SkipThought model is described as a sequence-to-sequence model. The SkipThought model is a sequence-to-sequence model that reads a sentence from WikiText-103 and predicts the following sentence. The DisSent model reads two clauses connected by a discourse marker and predicts the marker's identity. These models reconstruct comment threads from reddit.com using a dataset of comment-response pairs. Two settings are considered: a classification task and a sequence-to-sequence task. The models are implemented using the AllenNLP toolkit. The model architecture includes a two-layer bidirectional LSTM with a pretrained character-level CNN word encoder from ELMo. This setup allows for effective handling of unknown words in transfer learning tasks. The model architecture includes a two-layer bidirectional LSTM with a pretrained character-level CNN word encoder from ELMo, enabling effective handling of unknown words in transfer learning tasks. The full pretrained ELMo model is used as an input handler for multitask learning, with lower layers pretrained on language modeling and higher layers pretrained on additional tasks. Comparisons are made between pretraining tasks to understand their complementarity with large-scale language model pretraining, and language models are trained to compare different pretraining methods. Scalar weights of ELMo's three layers are trained for input to the shared encoder and skip connections for target tasks. CoVe is not used as it was found to be less effective. The GLUE benchmark BID41 is used for evaluating sentence encoders, with nine tasks for classification or regression over sentences and sentence pairs. ELMo is preferred over CoVe due to its effectiveness on GLUE tasks. The pretrained encoder is frozen, and separate target-task models are trained on the encoder's representations for each of the nine tasks in the benchmark. The GLUE benchmark is used to evaluate sentence encoders on nine tasks. Target-task models are trained on the encoder's representations for each task, with different approaches used depending on the task type. The GLUE benchmark evaluates sentence encoders on nine tasks, with target-task models trained on encoder representations for each task using various approaches. Pretraining on GLUE tasks involves a 512D \u00d72 BiLSTM with max-pooling, heuristic matching, and a final MLP, excluding an attention mechanism for better cross-task transfer performance. Outside pretraining tasks include sentence pair classification with a non-attentive architecture. Language modeling uses separate forward and backward two-layer LSTM to prevent information leakage. ELMo uses a strategy of training separate forward and backward two-layer LSTM language models, concatenating their outputs during target task training. For multitask pretraining, different sets of tasks are investigated, with a sampling rate balancing small-data and large-data tasks. Language modeling is excluded from multitask runs using ELMo due to full context representation. In Appendix C, more extensive experiments with methods like early stopping based on validation metrics are shown. The models are trained with the AMSGrad optimizer and early stopping is performed at pretraining and target task training times. Typical experiments take 1-5 days on an NVIDIA P100 GPU. Hyperparameter tuning details are provided in Appendix B. The primary experiment required over 100 GPU-days on NVIDIA P100 GPUs, limiting hyperparameter tuning. Results on the GLUE dev set for pretrained encoders are shown, with and without ELMo BiLSTM layers. N/A baselines are untrained encoders, while Single-Task baselines use encoders pretrained on specific tasks. The given GLUE task uses a pretrained encoder specific to that task. Separate task-specific parameters are used for pretraining and target tasks, even if they share the same data. Only three pretrained encoders were evaluated on test data due to GLUE's test set access limits. The best models with and without ELMo encoder and GLUE data during pretraining were considered. Limited hyperparameter tuning was conducted, and baselines from prior work were omitted due to space constraints. The best test result for a model similar to the GLUE E multitask model is 68.9, while the best overall result is 72.8 for a model fine-tuned for each target task. Variance in GLUE scores was estimated by re-running setups with different random seeds, showing a substantial but not meaningless variation. Only one model reached the most frequent class performance for the WNLI dataset. The CoLA task benefits significantly from ELMo pretraining, with results more than double compared to no pretraining. STS benchmark shows good results with various pretraining methods but does not see a substantial improvement with ELMo. Language modeling performs best among pretraining tasks, followed by MNLI, while other tasks perform close to random baseline. Direct training on target tasks only slightly improves over the baseline, but adding ELMo enhances performance across all pretraining tasks. Adding ELMo improved performance across all pretraining tasks, with MNLI and English-German translation performing the best. Multitask models with ELMo outperformed those without it, but single-task models showed better generalization on new data. The correlations between tasks and pretrained encoders are shown in TAB1. The correlations between pairs of tasks over pretrained encoders show that different tasks benefit from different forms of pretraining. Models that perform best tend to overfit on certain tasks, leading to negative correlations. CoLA has a strong correlation with overall GLUE scores, but weak correlations with other tasks, which can be improved with ELMo or LM pretraining. The use of ELMo or LM pretraining significantly improves CoLA performance, while most other forms of pretraining have minimal impact. Different tasks benefit from different forms of pretraining, with some tasks showing slight improvements as the amount of pretraining data increases. Combining pretraining tasks with ELMo yields less interpretable results, with some of the best results achieved by models that combine different pretraining tasks. The paper compares the benefits of pretraining tasks like ELMo and multitask learning on various tasks, showing improvements with increasing data volumes. Results indicate that unsupervised pretraining aids in world knowledge and lexical-semantic tasks but less so in complex sentence structures. The study compares pretraining tasks for sentence-level BiLSTM encoders like ELMo and CoVe, finding that language modeling works well as a pretraining task. Multitask pretraining outperforms single tasks and sets a new state-of-the-art. However, the pretrain-and-freeze paradigm may not be ideal for future work. The study suggests that the current pretraining paradigm for sentence-level BiLSTM encoders like ELMo and CoVe may not be ideal for future work. Different tasks benefit from different forms of pretraining, and multitask pretraining does not reliably produce better models. Further research is needed to understand how neural networks target tasks for improved performance. The study highlights the need for a better understanding of how neural network target task models can benefit from external knowledge and data, as well as new methods for pretraining and transfer learning. Specific procedures are used to extract discourse model examples from the WikiText-103 corpus. The Reddit classification task involves selecting the correct reply from two candidate replies, with incorrect distractor replies included in the dataset. Large-scale comparisons like the one discussed in the paper are complex tasks. In large-scale comparisons, various NLP datasets were experimented on, but development-set performance did not surpass random encoder baseline. Tasks included image-caption matching, text understanding, common sense inference, POS tagging, and supertagging. Validation is done every 1,000 steps for the current training task, or every 9,000 steps during multitask learning. During training, the learning rate is adjusted based on the number of tasks and specific task types. AMSGrad BID36 optimizer is used. Learning rate decay occurs when validation performance does not improve for more than 4 checks. Early stopping is implemented after 20 validation checks without improvement. Regularization includes dropout with a 0.2 drop rate after specific layers in the neural network. For target tasks, we adjust MLP dropout to 0.4 and use different models and training regimes based on task size. We tokenize data using Moses tokenizer and set a max sequence length of 40 tokens. Word embeddings are trained on the decoder side for all sequence-to-sequence tasks. BPE tokenization with a vocabulary of 20,000 types is used for translation tasks. We used heuristic tuning to group GLUE tasks by data size, each with its own model specifications. Attention is disabled during pretraining. STS behaves similarly to larger tasks. Attention is beneficial for SkipThought and Reddit pretraining but not for machine translation. The decoder's hidden state is initialized with the encoder's max-pooled output. The decoder's output dimension is halved before the softmax layer. Our multitask learning experiments involve mixing tasks with varying amounts of training data, optimizing the shared encoder quality, and avoiding overfitting or underfitting. Little research has been done on this problem, so we conducted a small experiment using random task sampling and hyperparameters to control fitting. In multitask learning experiments, task sampling probabilities and loss scaling weights are adjusted based on the amount of data available for each task. Experiments are conducted on various subsets of tasks, showing results in TAB3. Different combinations of task sampling and loss scaling methods are also tested. In the multitask learning experiments, different combinations of task sampling and loss scaling methods were tested. Results showed that applying only one of non-uniform sampling or non-uniform loss scaling at a time is generally better than applying both simultaneously. Power 0.75 task sampling and uniform loss scaling were used in the experiments, with results shown in TAB0. Additionally, results on the four coarse-grained categories of the GLUE diagnostic set for all pretraining experiments are shown in TAB5, indicating that ELMo and other forms of unsupervised pretraining can be helpful in highlighting world knowledge. Unsupervised pretraining, including ELMo, is beneficial for tasks involving world knowledge and lexical-semantic knowledge but less effective for tasks requiring complex logical reasoning or sentence structure variations. This contrasts with previous findings that language model pretraining aids in tasks related to sentence structure."
}