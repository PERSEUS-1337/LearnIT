{
    "title": "B1suU-bAW",
    "content": "Word embedding is a useful approach to capture co-occurrence structures in text. A new tensor decomposition model for word embeddings with covariates is proposed in this paper. The model learns a base embedding for all words and a transformation to incorporate covariate information. This approach offers data efficiency and interpretability of the covariate transformation matrix. Our joint model improves embeddings by learning covariate-specific embeddings that are topic-aligned, enabling downstream analysis. Empirical evaluations show the algorithm's benefits in addressing questions about covariate effects. The use of factorizations in learning word representations has gained attention, exemplified by widespread algorithms like GloVe. In recent years, algorithms like GloVe, BID10, and Word2Vec BID8 have become widespread in downstream applications. The goal is to assign a vector to each word in a vocabulary to preserve semantic structure. Having covariates in a corpus can provide additional information, such as news articles from different publications. By learning covariate-specific embeddings, we can capture meaningful semantic relationships and address questions about covariate effects. Question 1: How can conditional co-occurrence statistics capture the effect of a covariate on word usage, like in the Shakespeare authorship question? Question 2: How can we break the rotational invariance of traditional embedding methods to give dimensions semantic meaning? There is interest in language differences appealing to different demographics, such as ideological signatures in voters. The study aims to infer topical differences between communities and trends in word usage. The goal is to develop a framework for learning embeddings of objects with co-occurrence structure and conditioning on covariates. Our model breaks rotational symmetry in embedding-learning algorithms, aligning word and covariate vectors by jointly learning on specific covariates. Previous methods like GloVe and Word2Vec focus on low-rank approximation of co-occurrence statistics. GloVe aims to minimize a loss function involving word vectors and biases, while Word2Vec uses nonlinear transforms of co-occurrence statistics. The loss was minimized in Word2Vec by learning vector representations through a neural-network based loss function. PMI-factorization methods aim to find vectors that approximate the co-occurrence matrix, capturing useful semantic structure. Previous studies on word embeddings' geometry informed the model formulation. A random-walk based framework was proposed to understand successful learning algorithms. In a proposed framework BID1, the corpus generation process involves a random walk of a discrete-time discourse vector. The study analyzes the effects of covariates on the random walk transition kernel and stationary distribution. Previous studies on \"multi-sense\" word embeddings BID11 BID9 are similar, where words can have different meanings in different contexts. However, in this framework, different meanings are imposed by conditioning on a covariate. Tensor methods have been used in collaborative filtering BID13 to learn the effects of conditioning on summary statistics. The main contributions of the joint learning model based on tensor factorization include a decomposition algorithm addressing sample complexity issues and methods for systematic analysis. Training an embedding model along covariates helps aggregate the co-occurrence structure, especially when dealing with large corpora and small conditional corpuses. Simply training a different embedding for each corpus may not be as effective. The proposed model addresses issues with comparing embeddings across different covariates by inducing a topic-aligned basis. The paper outlines the embedding algorithm, dataset details, validation process, and experiments for downstream analysis. The algorithm generalizes to higher-order tensor decompositions with multiple dimensions covariates. The co-occurrence tensor A represents word occurrences within a window in the corpus from a specific covariate. The algorithm results in sets of vectors and bias terms fitting into the objective. The algorithm generalizes to higher-order tensor decompositions with multiple dimensions covariates. It results in sets of vectors and bias terms fitting into the objective, including the element-wise product between two vectors. The objective function minimizes a partial non-negative tensor factorization objective for training word vectors and weight vectors representing the effect of covariates. The algorithm generalizes to higher-order tensor decompositions with multiple dimensions covariates, resulting in sets of vectors and bias terms fitting into the objective. The aim is to learn sets {v i \u2208 R d }, {c k \u2208 R d }, and {b ik \u2208 R} such that the vectors {c k v i } and biases {b ik } approximate those from running GloVe on the k th slice of the co-occurrence tensor. The algorithm aims to learn sets of vectors and bias terms to approximate those from running GloVe on a slice of the co-occurrence tensor. The rationale behind this objective is inspired by a geometric intuition related to a random walk model, where the context vector emits words based on a transition distribution with a stationary distribution that is uniform over an ellipse. This model is extended by conditioning on different covariates, equivalent to multiplying the transition matrix by a symmetric PSD matrix. The algorithm learns sets of vectors and bias terms to approximate GloVe results on a co-occurrence tensor slice. Conditioning on different covariates involves multiplying the transition matrix by a symmetric PSD matrix, affecting word embeddings. Each covariate is assigned its own symmetric PSD matrix. The algorithm learns sets of vectors and bias terms to approximate GloVe results on a co-occurrence tensor slice. Each covariate is assigned its own symmetric PSD matrix, which can be factorized as B k = R T k D k R k. The model considers the effect of covariates on a base embedding by applying the linear operator B k to each embedding vector, resulting in the new embedding B k M. The model proposes a restriction where embeddings are affected by covariates via multiplication by a diagonal matrix under a universal basis R. This leads to rotated versions of the original embeddings. The model learns parameters {v i \u2208 R d }, {c k \u2208 R d }, and {b ik } to approximate GloVe results on a co-occurrence tensor slice. It considers the rotation of the basis in which the original embedding was trained, implying the existence of independent \"topics\" that each covariate affects in terms of importance. The model learns parameters {v i \u2208 R d }, {c k \u2208 R d }, and {b ik } using the Adam BID6 algorithm for convergence. Specific hyperparameters for each dataset will be detailed later. Co-occurrence statistics are formed with size 8 windows and inverse-distance weighting. The \"book dataset\" includes text from 29 books by 4 authors like J.K. Rowling and C. S. Lewis. The co-occurrence tensor was formed by looking at all words that occurred in all of the series with multiple books, eliminating series-specific words. Series by the same author had very different themes, so there is no reason intrinsic to the vocabulary to believe the weight vectors would cluster by author. The vocabulary size was 5,020. The \"politics dataset\" consisted of comments from 6 different subreddits on Reddit in 2016. Subreddits included AskReddit, news, politics, SandersForPresident, The Donald, and WorldNews. A vocabulary size of 15,000 was considered after removing the 28 most common words. The politics discussion forum was noted to be left-leaning. The study involved a vocabulary of 15,000 words, with the 28 most common words removed for efficiency. The embedding had 200 dimensions and a learning rate of 10^-5. Tensor decomposition was performed on a book dataset to cluster weight vectors by series and author. Clear clustering was observed based on series and author, even when considering only co-occurrence statistics for words appearing in all series. The study involved clustering weight vectors by series and author based on co-occurrence statistics for words appearing in all series. Using covariate-specific embedding yielded better estimates of vectors compared to GloVe, especially for sparsely-occurring words. Individual books contained between 26,747 and 355,814 words. The study compared GloVe embeddings with a novel algorithm for individual book embeddings. The new method outperformed GloVe on all evaluation tasks, showing significant improvement. Weight sparsity in the new algorithm was experimentally verified and deemed desirable for the covariate weight vectors. The study compared GloVe embeddings with a novel algorithm for individual book embeddings, with the new method outperforming GloVe on all evaluation tasks. Weight sparsity in the new algorithm was experimentally verified and deemed desirable for the covariate weight vectors, with sparse coordinates becoming \"stuck\" at 0 in the objective function. The dimensions where covariates were sparse did not overlap much, suggesting specificity in the set of \"topics\" each conditional slice of the tensor cares about. Regularizing the word vectors forced the model into a smaller subspace, providing evidence for the natural isotropic distribution of word vectors. The study compared GloVe embeddings with a novel algorithm for individual book embeddings, with the new method outperforming GloVe on all evaluation tasks. Weight sparsity in the new algorithm was experimentally verified and deemed desirable for the covariate weight vectors. The sparsity pattern was confirmed to be a result of separation of covariate effects rather than an artifact of the algorithm. The weight vectors after 90 iterations were non-sparse, with specific dimensions corresponding to less relevant topics for that covariate. Methods to interpret covariate weights in terms of topics are developed in the next section. The study compared GloVe embeddings with a novel algorithm for individual book embeddings, with the new method outperforming GloVe on all evaluation tasks. Weight sparsity in the new algorithm was experimentally verified and deemed desirable for the covariate weight vectors. The sparsity pattern was confirmed to be a result of separation of covariate effects rather than an artifact of the algorithm. The task is to output words with the largest values in specific dimensions of sparse coordinates, revealing topic meanings in certain coordinates. The study compared GloVe embeddings with a novel algorithm for individual book embeddings, with the new method outperforming GloVe on all evaluation tasks. Weight sparsity in the new algorithm was experimentally verified and deemed desirable for the covariate weight vectors. The sparsity pattern was confirmed to be a result of separation of covariate effects rather than an artifact of the algorithm. The task is to output words with the largest values in specific dimensions of sparse coordinates, revealing topic meanings in certain coordinates. In analyzing the weights of covariates on different topics, it was observed that the consistency of resulting words varied. For example, coordinate 194 (related to military themes) had different weights assigned by different sources like AskReddit, news, worldnews, SandersForPresident, The Donald, and politics. By examining the words with large weights in specific coordinates, themes such as rights and protests were identified, indicating domestic issues rather than global ones. The study compared GloVe embeddings with a novel algorithm for individual book embeddings, with the new method outperforming GloVe on all evaluation tasks. Weight sparsity in the new algorithm was experimentally verified and deemed desirable for the covariate weight vectors. The task is to output words with the largest values in specific dimensions of sparse coordinates, revealing topic meanings in certain coordinates. In analyzing the weights of covariates on different topics, it was observed that the consistency of resulting words varied. For example, coordinate 194 (related to military themes) had different weights assigned by different sources like AskReddit, news, worldnews, SandersForPresident, The Donald, and politics. By examining the words with large weights in specific coordinates, themes such as rights and protests were identified, indicating domestic issues rather than global ones. The experiment aims to identify word pairs that start close in the baseline embedding but move far apart under certain covariate weights, revealing shifts in meaning within specific communities. Representative examples are presented for the covariate \"The Donald,\" showing how words like \"immigrant\" and \"parasite\" were significantly closer under this weight, highlighting the impact of reweighting on word relationships. The dimensions 89 and 139 in the weight vector were sparse and had a large contribution to the subspace zeroed out under reweighting. Words like {misunderstanding, populace, scapegoat, rebuilding} for 89 and {answers, barriers, applicants, backstory, selfless, indigenous} for 139 represented the zeroed out subspace meaning. This suggests emotional appeal and legal immigration as reasons for the drift in the embedding methods. The ability of recent embedding methods to capture analogies is highlighted, with a focus on covariate-specific analogies. The study focused on capturing covariate-specific analogies in word embeddings. Experiments were conducted to find words d related to fixed words a and b, with varying covariates k. The results showed strong analogies, such as \"woman\" to \"man\", with high variance in relative rankings across subreddits. Several representative examples were presented in TAB4. The joint tensor model presented learns embeddings for words and covariates, allowing for easy computation of covariate-specific embeddings. The model enables interpretation of covariate vectors by identifying dimensions with significant weights. These dimensions can represent coherent topics. The model can be applied to other settings, such as learning gene interactions based on cell types. Time-series data with conditional covariates is another potential application. The framework presented in the curr_chunk allows for learning embeddings with conditional covariates, particularly in time-series data. The optimization method used resulted in weight vectors with highly-overlapping sparse dimensions, indicating a suboptimal fit to the model."
}