{
    "title": "BJgqQ6NYvB",
    "content": "FasterSeg is an automatically designed semantic segmentation network that outperforms current methods in both speed and performance. It utilizes neural architecture search (NAS) to discover a model from a novel search space with multi-resolution branches. A decoupled latency regularization helps balance accuracy and latency goals, preventing the network from becoming low-latency but poor-accuracy models. FasterSeg can also be extended to a collaborative search framework, where a teacher-student distillation further improves the student model's accuracy. FasterSeg is a segmentation network that improves accuracy and speed. It can run over 30% faster than competitors while maintaining comparable accuracy. The demand for real-time semantic segmentation, like in autonomous driving, has led to the need for more efficient networks without sacrificing accuracy. Recent advancements in neural architecture search (NAS) algorithms have focused on designing low-latency segmentation networks without compromising accuracy. Auto-DeepLab introduced network-level search space optimization for segmentation tasks, while Li et al. searched for operators and decoders with latency constraints. Despite some successes, human domain expertise in segmentation model design has not been fully integrated into NAS frameworks yet. Human-designed architectures for real-time segmentation commonly utilize multi-resolution branches. Recent advancements in neural architecture search (NAS) algorithms have focused on designing low-latency segmentation networks without compromising accuracy. Auto-DeepLab introduced network-level search space optimization for segmentation tasks, while Li et al. searched for operators and decoders with latency constraints. Inheriting successful practices from hand-crafted efficient segmentation models, a novel NAS framework called FasterSeg aims to achieve fast inference speed and competitive accuracy by optimizing over multiple branches of different resolutions. The text discusses a novel NAS framework for real-time segmentation that utilizes multi-resolution branches for prediction. It introduces a decoupled latency regularization to balance accuracy and latency effectively. Additionally, the framework allows for collaborative search between a teacher network and a student network, enhancing the student's accuracy through feature distillation. The text introduces a novel NAS framework for real-time segmentation, incorporating decoupled latency regularization to balance accuracy and latency effectively. It also includes a collaborative search between a teacher network and a student network for accuracy enhancement through feature distillation. FasterSeg achieves faster speed and competitive accuracy compared to manually designed competitors on CityScapes. Existing resource-aware NAS efforts focus on classification tasks, while semantic segmentation requires preserving details and rich contexts. Handcrafted architectures like ENet, ICNet, and BiSeNet have successful design patterns for real-time segmentation. Recent studies have focused on optimizing segmentation efficiency by incorporating multi-resolution branching and aggregation designs. AutoDeepLab and Li et al. have introduced NAS algorithms for segmentation tasks, allowing for flexible control over spatial resolution changes. However, these search models still primarily follow a single-backbone design. The FasterSeg model integrates knowledge distillation to improve accuracy and speed up convergence in segmentation tasks. It is discovered from an efficient and multi-resolution search space, inspired by previous manual design successes. The model also introduces a fine-grained latency regularization. The FasterSeg model utilizes a multi-resolution search space inspired by manual design successes. A fine-grained latency regularization is introduced to prevent \"architecture collapse\". The framework automatically selects and aggregates branches of different resolutions for overall low latency. The framework enables searching for expansion ratios and spatial resolutions within cells, allowing for the selection of multiple branches of different resolutions in a multi-layer framework. This approach aims to improve real-time segmentation by decoding each branch via backtrace. Our NAS framework explores multiple branches with different resolutions for real-time segmentation, decoding each branch via backtrace. The model capacity is increased by downsampling the input image and setting searchable downsample rates. Operator selection is based on inference latency rather than FLOPs, ensuring efficient execution. See Appendix B for branch selection details. Group convolution and dilated convolution are analyzed for their efficiency in reducing FLOPs and parameters while maintaining a large receptive field. Group convolution can be 13% faster with the same receptive field as standard convolution, while dilated convolution offers an enlarged receptive field but may suffer from limitations. In engineering practice, dilated convolution with a dilation rate of 2 has higher latency compared to standard convolution. A new variant called \"zoomed convolution\" is designed with bilinear downsampling, standard convolution, and bilinear upsampling, resulting in 40% lower latency and 2 times larger receptive field. The search space includes skip connection, 3x3 convolutions, \"zoomed conv.\" with bilinear downsampling and upsampling, and \"zoomed conv. x2\" with bilinear downsampling and 3x3 convolutions. Network fragmentation can impact parallelism efficiency, so a sequential search space is chosen. In our network, convolutional layers are sequentially stacked for efficiency. Each cell is differentiable and contains only one operator. We allow each cell to be individually searchable across the search space and choose different channel expansion ratios. To address the challenge of searching for the width of connections between cells, we propose a differentiably searchable superkernel. This superkernel directly searches for the expansion ratio within a single convolutional kernel, supporting a set of ratios. During architecture search, a simplified and memory-efficient super network is created by incrementally increasing connections from slim to wide subsets of input/output dimensions. The expansion ratio for each superkernel is sampled and back-propagated using the \"Gumbel-Softmax\" trick. The width is determined by the resolution downsampling factor, and each pair of successive cells can have its own expansion ratio. Cells are connected with two possible options to facilitate the search for spatial resolutions. During architecture search, a simplified super network is created with incremental connections. The expansion ratio for each superkernel is sampled using the \"Gumbel-Softmax\" trick. Low latency optimization is challenging due to the risk of falling into bad \"local minimums\" with poor accuracy. Previous works have noted the tendency for searched networks to use more skip connections. To address the \"architecture collapse\" issue in network exploration, a fine-grained, decoupled latency regularization is proposed. This involves replacing operators with corresponding latencies and creating a latency lookup table for estimation. The correlation between real and estimated latencies is high at 0.993. The high correlation of 0.993 between real and estimated latencies is demonstrated in the context of addressing the \"architecture collapse\" issue in network exploration. The core reason behind this problem is the different sensitivities of the supernet to operator O, downsample rate s, and expansion ratio \u03c7. Operators like \"3\u00d73 conv. \u00d72\" and \"zoomed conv.\" show a significant gap in latency, while similar gaps exist between slim and wide expansion ratios. Downsample rates like \"8\" and \"32\" do not differ much due to resolution downsampling doubling the number of input and output channels. The influence of O, s, and \u03c7 on supernet latency is quantitatively compared by adjusting one aspect while fixing the others. The text discusses the sensitivity of a supernet's latency to different operators and expansion ratios, proposing a regularized latency optimization approach to prevent \"architecture collapse.\" By decoupling the calculation of latency into three granularities (operators, downsample rate, expansion ratio) and applying different regularization factors, the problem of biased architectures with low accuracy is addressed. In the NAS framework, knowledge distillation transfers knowledge from a large teacher network to a smaller student network. The teacher-student cosearching approach allows for collaborative searching for two networks simultaneously. The teacher and student share the same supernet weights during the search process, optimizing two sets of architectures iteratively. The extension does not increase memory usage or supernet size. Latency constraints are applied only on the student network. During the search process, a latency constraint is applied only to the student network, not the teacher. The teacher-student collaboration involves transferring knowledge through distillation loss. The discrete architecture is derived from \u03b1, \u03b2, and \u03b3 values, optimizing operators and expansion ratios. The architecture is adjusted for shallower design with fewer cells. Different probabilities for outputs from cells are considered for each cell. The design of multi-resolution branches in the network involves sharing cell weights and feature maps for faster processing. Equations are solved to achieve balanced sensitivities on different granularities. The Cityscapes dataset is used for testing the architecture search and ablation. The Cityscapes dataset is utilized for architecture search and ablation studies. Final accuracy and latency are reported on Cityscapes, CamVid, and BDD datasets using class mIoU and FPS as metrics. Nvidia Geforce GTX 1080Ti is used for benchmarking computing power, with TensorRT v5.1.5 for high-performance inference. PyTorch is employed for framework implementation. Our framework, implemented with PyTorch, utilizes CUDA 10.0 and CUDNN V7. The supernet has 16 layers with a downsample rate of 8, 16, or 32. We use 2 branches to avoid high latency and consider expansion ratios for different layers. The architecture search is conducted on the Cityscapes dataset, resulting in FasterSeg achieving multi-resolutions with proper depths. The search space is vast, with over 10^58 unique combinations. Our FasterSeg model achieved multi-resolutions with proper depths by utilizing two branches that share initial operators before diverging. Ablation studies on Cityscapes evaluated the impact of operators, downsample rate, expansion ratios, and distillation on accuracy and latency. Expanding to multi-branches improved mIoU despite a drop in FPS, and enabling the search for expansion ratios led to a faster network without sacrificing accuracy. The student network (S) achieved an FPS of 163.9 and an mIoU of 70.5% through a co-searching framework, surpassing pruning-based compression methods. Knowledge distillation from a well-trained cumbersome teacher further improved performance. In this section, the FasterSeg model is compared with other real-time semantic segmentation works on popular scene segmentation datasets. The FasterSeg model achieved an accuracy of 73.1% without using evaluation tricks like flipping or multi-scale. The evaluation was done on Cityscapes validation and test sets. FasterSeg achieves superior FPS of 163.9 on Cityscapes validation and test sets with an accuracy of 73.1% and 71.5% respectively. The model maintains competitive accuracy without using extra data and achieves an FPS of 398.1 on CamVid, over 47% faster than the closest competitor. Our FasterSeg model outperforms the best mIoU work, demonstrating high performance and transferability of our NAS framework. When applied to the BDD dataset, FasterSeg is 15 times faster than DRN-D-22 with slightly higher mIoU. It maintains fast speed and competitive accuracy compared to other models on BDD. The NAS framework FasterSeg achieves fast inference speed and competitive accuracy by addressing the \"architecture collapse\" problem and implementing regularized latency optimization. The STEM module quickly downsamples input images while increasing channel numbers, and the HEAD module reduces feature map channels for improved performance. The curr_chunk discusses the process of reducing channels in feature maps, upsampling, concatenating, and fusing feature maps using convolution layers. It also mentions the selection of branches based on resolution combinations and latency optimization criteria. The curr_chunk discusses the sampling of expansion ratios and probabilities for differentiable sampling using Gumbel noise. It also implements normalized scalars as probabilities for operators, predecessor's output, and expansion ratios. A latency lookup table is used to estimate relaxed latency, verifying continuous relaxation. The accurate estimation of network latency is achieved by sampling networks from the supernet M and measuring both real and estimated latency. A high correlation of 0.993 is observed between the two measurements. The optimization target during architecture search involves using cross-entropy with \"online-hard-element-mining\" as the segmentation loss. The architecture parameters can be optimized using gradient descent, similar to previous work. In architecture search implementations, a stride 2 convolution is used for connections, and bilinear upsampling is employed. The Cityscapes dataset is used for architecture search, with specific training image crops. Different optimizers are used for learning network weights and architecture parameters. The architecture search optimization process takes about 2 days on a 1080Ti GPU using Adam optimizer. FasterSeg selects operators and expansion ratios, with downsample rates matching in Table 7. The importance of low latency and large receptive field is highlighted by the heavy use of zoomed convolution."
}