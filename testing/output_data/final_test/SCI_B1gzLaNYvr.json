{
    "title": "B1gzLaNYvr",
    "content": "With the increasing use of deep learning in safety-critical situations, interpretability is crucial. Time-series data has been overlooked in terms of interpretability, but a new approach called TSInsight combines an auto-encoder with a sparsity-inducing norm to enhance interpretability by preserving important features for prediction and suppressing irrelevant ones. This method helps in attributing features that are correlated or causal for the prediction, boosting interpretability. TSInsight is a new approach that enhances interpretability of deep time-series models by attributing features useful for prediction. It can generate both instance-based and model-based explanations, achieving properties like adversarial robustness. Results show TSInsight's effectiveness in interpreting deep learning models across various domains. Automated feature extraction models lack transparency, limiting their use in safety-critical domains like finance, self-driving cars, and medicine. Efforts have been made to interpret these black-box models, focusing on making the network interpretable or explaining pretrained models. Efforts have been made to interpret black-box models by explaining pretrained models using attribution techniques such as saliency map and layer-wise relevance propagation. However, explaining time-series data is challenging, with only a few methods developed in the past. A novel approach involves attaching an auto-encoder on top of the classifier and fine-tuning it based on gradients to reconstruct specific parts of the input. The paper introduces a sparsity inducing norm on the output of an auto-encoder to reconstruct only relevant parts for the classifier. It presents a novel attribution method for interpreting deep learning models on time-series data and analyzes different attribution techniques' information capture abilities. The method leverages dataset-level insights and demonstrates resistance against adversarial noise. Since the resurgence of deep learning in 2012, deep learning has been integrated into various applications for significant improvement. Efforts have been made to enhance the interpretability of deep models, particularly for image modality, through two streams: explaining decisions of pretrained networks and making models more interpretable by sacrificing accuracy. Research has focused on explaining pretrained models using attribution techniques, with a common strategy being visualizing filters of deep models. Zeiler & Fergus introduced deconvnet layer to understand network representations and improve performance on ImageNet. Simonyan et al. proposed class-specific saliency maps, while Yosinski et al. developed a visualization framework for image-based deep learning models. Various methods have been introduced to explain pretrained models using attribution techniques, such as Layer-wise Relevance Propagation (LRP), SmoothGrad, Integrated gradients, and Bayesian non-parametric regression mixture model. These methods aim to identify relevant features in images and extract generalizable insights from trained models, but they are not directly applicable to time-series data or may lack intelligibility. Palacio et al. (2018) introduced a method using auto-encoders to understand deep models for time-series data. They attached the auto-encoder to the classifier and fine-tuned only the decoder, focusing on relevant features. Alvarez-Melis & Jaakkola (2018) proposed Self-Explaining Neural Networks (SENN) where two networks learn different concepts. The first network, the concept encoder, encodes different concepts while the second network learns the weightings of these concepts, transforming the system into a linear problem with interpretable features. SENN prioritizes interpretability over accuracy and includes a video-to-text network for generating natural language explanations using saliency information. Kumar et al. (2017) focused on understanding deep learning models for time-series analysis, specifically in financial data, by computing input saliency based on first-order gradients. TSInsight surpasses existing methods in identifying important input regions by combining local information for specific examples with general insights from the entire dataset. It utilizes auto-encoders, particularly sparse and contractive auto-encoders, to enhance its capabilities. In sparse and contractive auto-encoders, sparsity is induced on hidden representations to minimize KL-divergence. However, in our approach, sparsity is induced directly on the output, leading to a contraction on the input space of the classifier. This is achieved by using Manhattan norm on activations for real-valued outputs. Contractive auto-encoders introduce a contraction mapping by penalizing the Fobenius norm of the Jacobian of the encoder along with the reconstruction error. TSInsight introduces a sparsity-inducing norm on the output of the auto-encoder, along with a reconstruction and classification penalty. This forces the network to reproduce only relevant regions of the input for the classifier, optimizing the auto-encoder while keeping the classifier fixed. The auto-encoder is optimized using gradients from the classifier, with a reconstruction penalty in place to prevent sparsity that hampers input reconstruction. The optimization objective includes a classification loss function, pretrained weights for the classifier, encoder, and decoder, and introduces hyperparameters \u03b3 and \u03b2 to control focus on input reconstruction and sparsity. The auto-encoder focuses on input reconstruction, while \u03b2 controls output sparsity. Pretrained weights are obtained by training the auto-encoder and classifier separately. The output is sparse and aligned with the input. Selecting \u03b2 impacts model output significantly, with a trade-off between performance and interpretability. Automated hyperparameter selection via feature importance measures like saliency is tested. The importance measure for sparsity in the classifier is based on saliency. The values are scaled to serve as reconstruction weight (\u03b3) and sparsity weight (\u03b2). The final term imposing sparsity on the classifier is determined by these weights. The final term imposing sparsity on the classifier is determined by the average saliency value, scaled by a constant factor C. Despite being interesting, this approach results in inferior performance compared to manual hyperparameter fine-tuning and requires further investigation. TSInsight's efficacy was tested on various time-series datasets, including a synthetic anomaly detection dataset. The Siddiqui et al. (2019) synthetic dataset includes pressure, temperature, and torque values for anomaly detection in a production setting. The dataset contains point-anomalies, where the entire sequence is marked as anomalous if a point-anomaly is present. The electric devices dataset (Hills et al., 2014) is a subset of data from the UK government's study on reducing carbon footprint, collected from 251 households sampled in two-minute intervals over a month. The character trajectories dataset contains hand-written characters using a Wacom tablet with three dimensions kept. The FordA dataset is used for binary classification to identify automotive subsystem symptoms. The Forest Cover dataset is adapted for forest cover type classification from cartographic variables. The dataset includes hand-written characters with x, y, and pen-tip force dimensions. Sampling rate is 200 Hz, data is differentiated and smoothed with \u03c3 = 2 for character classification into 20 classes. The dataset was transformed into an anomaly detection dataset with 10 quantitative attributes out of 54. Instances from the second class were normal, while instances from the fourth class were considered anomalous. The anomalies to normal data points ratio is 0.90%. Other classes were discarded. The WESAD dataset is for affective state classification with three classes. The UWave Gesture Dataset contains accelerometer data for recognizing 8 gestures. TSInsight produced intelligible results for the datasets used in the study. TSInsight generated sparse representations focusing on salient regions, outperforming the base classifier in accuracy. While not optimized for performance, it prioritizes interpretability. Performance may decrease based on sparsity level. An anomalous example was visualized for qualitative assessment, along with attributions from various techniques. Relevant discriminative points were limited in forest cover and synthetic anomaly detection datasets. The auto-encoder in TSInsight suppresses most input to make decisions interpretable. Important input parts are preserved and passed to the classifier. If causal, the prediction stands; otherwise, it flips. High sparsity affects accuracy. TSInsight outperforms other saliency methods by preserving important features. The auto-encoder in TSInsight suppresses input to enhance interpretability. Important features are preserved and passed to the classifier. Various attribution techniques are evaluated, including input magnitude and occlusion sensitivity. The importance of features is determined using various methods such as occlusion sensitivity, auto-encoder output magnitude, and gradient input. These techniques help in evaluating the significance of input features for classification. Various methods are used to determine the importance of features, including gradient input, integrated gradients, smoothened gradient, and guided backpropagation. These techniques help assess the significance of input features for classification. In this case, ReLU layers were replaced with guided ReLU layers to filter out negative influences for a specific class. GradCAM was used to compute the importance of filters in the input, with Guided GradCAM being a variant that combines guided backpropagation and GradCAM. The results with different levels of suppression were visualized and averaged over 5 random runs. TSInsight produced the most plausible explanations and was the most competitive saliency estimator on average compared to other attribution techniques. It is compatible with any base model, including CNN and LSTM, and can extract salient regions of the input regardless of the underlying architecture. TSInsight is a competitive saliency estimator that can extract salient regions of input regardless of the underlying architecture. LSTM's saliency is focused on memory cells for past states, while CNN has equal distribution of saliency. The optimization problem in TSInsight defines the explanation scope, with the auto-encoder acting as a generic feature extractor for the complete dataset or discovering an instance's attribution for a particular input. TSInsight achieved a high level of immunity against adversarial noise, with a focus on interpretability rather than robustness. The study found that interpretability and robustness are complementary objectives in attribution techniques. TSInsight achieved high immunity against adversarial noise, emphasizing interpretability over robustness. The system overview pipeline is visualized in Fig. 7, analyzing the impact of stacking the auto-encoder on the original network's optimization problem. The loss landscape is assessed using filter normalization and random directions to identify the loss landscape. Further investigation is needed to validate these findings. The loss landscape for the combined model is visualized in Fig. 8, showing a nearly convex surface for the classifier and a kink for the auto-encoder. Optimizing the auto-encoder using gradients from the classifier is a challenging task, leading to convergence issues as observed in the study. The network failed to converge in many cases, similar to observations made by Palacio et al. (2018). They only fine-tuned the decoder of the auto-encoder to make the problem tractable. Results were consistent across different datasets. TSInsight was trained with a CNN and LSTM on a single V-100 (16 GB) using the DGX-1 system. The classifier training results showed no significant impact on classification performance with TSInsight. Accuracy increased after attaching the auto-encoder, but it was considered a coincidence. TSInsight was able to identify important input regions regardless of the classifier used, as shown in attribution plots in Figures 10, 11, 12, and 13."
}