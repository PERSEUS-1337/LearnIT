{
    "title": "HkljioCcFQ",
    "content": "In weakly-supervised temporal action localization, a marginalized average attentional network (MAAN) is proposed to address the issue of overestimating salient regions. The MAAN utilizes a marginalized average aggregation (MAA) module to reduce the difference in responses between salient and non-salient regions, improving class activation sequences and identifying dense and integral action regions. MAAN improves class activation sequences and identifies dense action regions in videos. A fast algorithm reduces complexity from $O(2^T)$ to $O(T^2). Extensive experiments show superior performance in weakly-supervised temporal action localization. Recent research in weakly-supervised temporal action localization focuses on finding ways to infer location information with only video-level class labels. Traditional methods based on instance proposals are becoming infeasible due to the varying lengths of actions and videos. Recent research has shifted towards acquiring location information by generating the class activation sequence (CAS) directly, inspired by CAM-based models in weakly-supervised object detection. CAS-based methods for action localization involve sampling non-overlapping snippets from a video, aggregating snippet-level features into a video-level feature, and generating a video-level class prediction during testing. The model generates a CAS for each class to localize action instances based on start and end times. The feature aggregator is crucial in weakly-supervised neural networks for capturing location information. Recent works have explored learning attentional weights for snippets to compute aggregated features. Several heuristic tricks have been proposed to reduce the domination of the most salient regions in models trained with video-level class labels. For example, BID35 BID44 attempt to erase the most salient regions predicted by the model and force the network to attend other salient regions by forwarding the model multiple times. However, this multiple-run model is not end-to-end trainable. The heuristic multiple-run model is not end-to-end trainable and relies on ensemble of mined regions. \"Hide-and-seek\" BID28 masks out input regions randomly during training to encourage localization of other salient regions. However, due to uniform prior, background is often masked out. Proposed MAAN addresses domination of most salient region in weakly-supervised action localization by suppressing action prediction response using marginalized average aggregation. Proposed MAAN utilizes marginalized average aggregation to address the domination of the most salient regions in weakly-supervised action localization. It learns latent discriminative probabilities to reduce response differences between salient and non-salient regions, resulting in more dense and integral identification. MAAN identifies dense and integral regions for each action by utilizing marginalized average aggregation. A fast iterative algorithm reduces complexity, and the network is easy to train end-to-end. Experimental results show MAAN outperforms baseline models in weakly-supervised temporal action localization. MAAN alleviates the issue of salient region domination by utilizing marginalized average aggregation. A fast iterative algorithm reduces computational complexity, leading to superior performance on benchmark video datasets for weakly-supervised temporal action localization. The proposed MAA module aggregates snippet-level features using binary indicators to sample snippets for average aggregation. It then computes the marginalized average aggregated feature by considering all possible subsets. This approach addresses salient region domination and improves performance on video datasets for weakly-supervised temporal action localization. The MAA module aggregates snippet-level features using binary indicators to sample snippets for average aggregation, addressing salient region domination in weakly-supervised action localization. The MAA module aggregates snippet-level features using binary indicators to sample snippets for average aggregation, addressing salient region domination in weakly-supervised action localization. The partial order preservation property ensures discriminative probabilities align with attention weights, while the dominant response suppression property reduces the impact of salient regions, promoting the identification of integral action regions. Proposition 1 demonstrates that latent discriminative probabilities {p i} maintain the order of attention weights {\u03bb i}. Eq. (3) factors \u03bb i into p i and c i, capturing snippet features and contextual information, introducing structural details into aggregation. The index set I represents the most salient features, with Proposition 2 showing that MAAN reduces the gap between the most salient regions and others by using latent discriminative probabilities instead of attention weights. This approach suppresses dominant responses and encourages denser identification. Our method utilizes latent discriminative probabilities to reduce the dominance of the most salient region in weakly-supervised temporal localization, compared to using attention weights. This helps in identifying dense and integral action regions by avoiding over responses to specific snippets. Our method proposes an iterative approach to calculate x with reduced complexity. The aggregated feature of {x1, x2, ..., xt} is denoted as ht, and the aggregated feature of {x1, x2, ..., xT} is x = hT. Zt is the summation of zi, with t + 1 distinct values. Expectation ht can be calculated accordingly. The expectation ht can be calculated as the summation of the t + 1 parts, denoted by mti in Eq. FORMULA11. The computation of q t+1 i = P (Z t+1 = i) involves selecting items from the first t items. Detailed proofs can be found in Appendix C. The recurrent formula in Proposition 3 provides a way to calculate mti. By iteratively calculating mti from i = 1 to t and t = 1 to T, we can obtain the aggregated feature of {x1. The iterative computation procedure is outlined in Algorithm 1 in Appendix E, with a time complexity of O(T^2). This fast iterative algorithm makes the MAA practical for end-to-end training in weakly-supervised temporal action localization using a network architecture that incorporates the MAA module. Our MAAN builds upon the STPN architecture by introducing a new attention module to generate discriminative probabilities and utilizing marginalized average aggregation instead of weighted sum aggregation for video-level representation. Training with video-level class labels is conducted to obtain class scores through an FC layer and a sigmoid layer. The model performs aggregation of snippet-level features and applies logistic regression for video-level classification prediction probability. The negative log-likelihood function is used for cross-entropy loss calculation. Temporal Action Localization is achieved through video-level action prediction score and probability. The model aggregates snippet-level features and uses logistic regression for video-level classification prediction probability. It computes the prediction score of snippets for action classes in a video, incorporating class-specific weights and relevance of snippets to classes. Temporal proposals are generated based on the prediction scores, with the class activation sequence (CAS) representing the context of other snippets. MAAN uses the class activation sequence (CAS) to extract temporal proposals for each class and employs the latent discriminative probability for prediction. It can suppress dominant responses compared to STPN, leading to better performance in weakly-supervised temporal action localization. The experiments focus on this problem, with an extension to weakly-supervised image object detection discussed in the appendix. In the experiments, MAAN is evaluated on THUMOS14 and ActivityNet1.3 datasets for action localization. THUMOS14 has 20 action classes with 200 untrimmed videos in the validation set and 212 in the test set. ActivityNet1.3 covers 200 activity classes with 10,024 training videos, 4,926 validation videos, and 5,044 test videos. In the experiments, models are trained on 10,024 training videos and tested on 4,926 validation videos. Evaluation is done using mean average precision (mAP) values at different IoU thresholds. Two-stream I3D networks are used to extract feature vectors from video snippets containing 16 frames. Videos are divided into non-overlapping segments for training. In training, videos are divided into segments and one snippet is sampled from each segment. T snippets are sampled for each video as input for the model. The attention module consists of FC layers and non-linear activations. Video-level representation is passed through FC layers to obtain class scores. ADAM optimizer is used for parameter optimization. At test time, classes with probabilities below 0.1 are rejected. Temporal proposals are generated by cutting the CAS with a threshold. Temporal proposals are generated by cutting the CAS with a threshold. The two-stream modalities combination ratio is set to 0.5. The MAAN model is compared with baseline models on the THUMOS14 dataset. The models use different feature aggregators, such as STPN and Dropout, with varying aggregation methods. Detection average precision is compared at different IoU levels. The comparison of detection average precision at different IoU levels and video-level classification mean average precision on the test set shows that achieving good video-level classification performance does not guarantee good snippet-level localization performance. MAAN consistently outperforms baseline models in weakly-supervised temporal localization tasks. The \"SoftmaxNorm\" aggregation method performs the worst among the normalized weighted average aggregation methods. The \"SoftmaxNorm\" aggregation method performs poorly in weakly-supervised temporal localization tasks as it over-amplifies the weight of salient snippets, resulting in sparse and non-integral localization. In contrast, MAAN outperforms \"Norm\" and \"Dropout\" by encouraging dense and integral action segment identification through learnable latent discriminative probability sampling. MAAN samples with learnable latent discriminative probability to stabilize aggregated features, outperforming STPN. It factorizes attention weight into ct and pt, where pt learns latent discriminative probability and ct captures contextual information. MAA properties prevent concentration on salient regions, leading to more accurate action segments. Temporal CAS from MAAN covers large, dense regions for precise action detection. In comparison, STPN misses segments and focuses on salient areas. Our MAAN model outperforms STPN by successfully localizing both easy and difficult action segments, unlike other methods. It can identify dense and integral action regions, not just the most discriminative ones. Comparison with state-of-the-art approaches on the THU-MOS14 dataset shows promising results. Our MAAN model outperforms STPN and existing weakly-supervised action localization approaches on the THU-MOS14 dataset. It also shows competitive results compared to recent fully-supervised approaches, even when trained with only video-level labels. The model is trained on the ActivityNet1.3 training set and achieves promising results when compared with state-of-the-art approaches on the validation set. The action segments in ActivityNet are longer and occupy a larger percentage of a video. Thresholds are used to generate the results. Our implemented STPN outperforms results reported in the original paper with a set of thresholds. The MAAN model surpasses the STPN approach on the large-scale ActivityNet1.3 dataset. Our proposed MAAN model outperforms the STPN approach on ActivityNet1.3 and THUMOS14 datasets for weakly-supervised temporal action localization. MAAN utilizes marginalized average aggregation to identify dense action segments and reduce the gap between discriminant regions in videos, resulting in better class activation sequences. Additionally, a fast algorithm is proposed to reduce computation complexity. The MAAN model surpasses the STPN approach on ActivityNet1.3 and THUMOS14 datasets for weakly-supervised temporal action localization. It uses marginalized average aggregation to detect dense action segments and minimize the gap between discriminant regions in videos, improving class activation sequences. Additionally, a fast algorithm is introduced to decrease computation complexity. Prof. Ivor W. Tsang was supported by ARC grants. Researchers have developed deep network models like two-stream networks and 3D convolutional neural networks for video action analysis. The I3D model combines these techniques and is used for various video analysis tasks. A few approaches based on weakly-supervised learning have been proposed for temporal action localization, such as the UntrimmedNet framework by Wang et al. Researchers have developed deep network models like two-stream networks and 3D convolutional neural networks for video action analysis. The I3D model combines these techniques and is used for various video analysis tasks. Singh et al. BID28 designed a Hide-and-Seek model to randomly hide regions in a video during training. Nguyen et al. BID17 proposed a sparse temporal pooling network (STPN) to identify key segments associated with actions through attention-based temporal pooling. The MAAN model is proposed to prevent the network from focusing only on salient regions by taking the expectation with respect to average aggregated features from sampled subsets in the video. Feature aggregation is essential for learning discriminative localization representations with video-level class labels. This mechanism is widely used in deep learning literature for various tasks, but most cases are fully supervised learning where the model attends relevant features based on direct supervision information. The feature aggregator in the MAAN model is trained with class labels only, unlike in fully supervised settings. It incorporates learnable latent discriminative probabilities through end-to-end differentiable marginalized average aggregation. The proposed model incorporates learnable latent discriminative probabilities through marginalized average aggregation. It is evaluated on weakly-supervised object localization, aiming to predict category labels and bounding boxes in images. Different feature aggregators are compared, including the weighted-CAM model and the proposed MAAN model with an attention module. The attention module in the proposed model generates attention weight \u03bb in STPN or latent discriminative probability p in MAAN. Evaluation on weakly-supervised localization accuracy is done on the CUB-200-2011 dataset. Comparison is made with weighted sum pooling and global average pooling. The proposed model includes global average pooling and two different feature aggregation sizes. Evaluation shows that MAAN outperforms GoogLeNet-GAP by 5.06% in Top-1 error. MAAN also achieves lower localization error compared to weighted-CAM. Both MAAN and weighted-CAM perform better with the 7x7 learning scheme. Heat maps and localization bounding boxes are visualized in FIG11. The proposed MAAN model outperforms GoogLeNet-GAP in Top-1 error by 5.06% and achieves more accurate bounding boxes with larger object coverage in heat maps."
}