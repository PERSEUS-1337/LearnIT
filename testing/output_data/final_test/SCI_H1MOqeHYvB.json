{
    "title": "H1MOqeHYvB",
    "content": "Automatic Piano Fingering can be learned by computers using data extracted from public videos and MIDI files through computer-vision techniques. This process resulted in the largest dataset for piano fingering with over 150K notes. By fine-tuning a model on this dataset and manually labeled data, state-of-the-art results were achieved. Additionally, a method for transferring deep-learning computer-vision models to out-of-domain data was introduced by fine-tuning on out-of-domain augmentation proposed by a Generative Adversarial Network (GAN). In this paper, a method for automatic prediction of piano fingering from piano playing performances captured on videos is proposed. Previous work used minimal training data for evaluation, making manual labeling of fingering an exhausting and expensive task. The process resulted in the largest dataset for piano fingering with over 150K notes, achieving state-of-the-art results by fine-tuning a model on this dataset and manually labeled data. Additionally, a method for transferring deep-learning computer-vision models to out-of-domain data was introduced. The method proposed in this paper introduces a low-cost approach for detecting piano fingering from video performances, utilizing deep learning methods to create an automatic labeled dataset. This dataset, named APFD, contains 90 finger-tagged pieces with over 155,000 notes, serving as a valuable resource for training models and generalizing to new pieces. The dataset APFD is valuable for training models to detect piano fingering from videos. The process involves extracting information from MIDI files to identify keyboard presses and fingers used to play each note. The text discusses the challenges in predicting piano fingering and proposes using object detection and CycleGAN to improve accuracy. Previous studies have explored piano fingering in music theory and computer animation. The task involves associating each note with a finger from a set of options. Predicting piano fingering involves assigning each note to a finger from a set of options {1, 2, 3, 4, 5} \u00d7 {L, R}, considering constraints like hand positions and anatomical plausibility. Fingering sequences have costs based on transitions between fingers. Early approaches treated fingering prediction as a search problem to find the optimal sequence, but exhaustive evaluation is not feasible. Transition matrices can define transition probabilities, allowing for a cost function to determine sequence likelihood. However, finding a globally optimal solution is impractical due to exponential complexity. Transition matrices can define transition probabilities for piano fingering prediction, but finding a globally optimal solution is impractical due to exponential complexity. Heuristics or Hidden Markov Models (HMM) are used to reduce space complexity, with practitioners manually fine-tuning the transition matrix. Dynamic programming algorithms or search algorithms like Tabu search or variable neighborhood search are suggested for solving the search space. In this work, the focus is on transitioning from search-based methods to learning methods for piano fingering prediction. These methods aim to imitate human behavior using large datasets, but require extensive training data. Fingering labeling is costly and challenging to obtain, hindering the use of advanced models like neural networks. Automatically gathering rich fingering data with full hand pose estimation can be done using motion capture gloves when playing the piano. Different methods have been explored, such as a rule-based and data-based hybrid approach using Directed Acyclic Graphs (DAG) and computer vision programs to detect fingering from video and MIDI files. These techniques aim to overcome the challenges of obtaining costly and hard-to-obtain fingering labeling data. In practice, the system was implemented with a camera capturing only 2 octaves out of 8, using simple rules for fingering. Previous methods for data collection were costly, but the use of YouTube videos in this work keeps costs minimal. A large dataset of manually labeled piano fingering has been released, providing a substantial amount of annotated data. The authors utilized a large dataset of annotated piano fingering notes and tags from multiple annotators. They proposed modeling methods including HMMs and neural networks, with the best performance achieved using an HMM-based model. The study aims to extract fingering information from online piano performance videos using machine learning techniques, creating a dataset of pieces and their fingering information. The authors utilized a dataset of annotated piano fingering notes and tags from multiple annotators to extract fingering information from online piano performance videos. They demonstrated the final output in Figure 1, colored fingers and notes based on pose estimation and predicted fingers. MIDI events provided information on played notes, extracted from videos on youtube.com. MIDI files are used for musical information interchange between instruments and computers. Musical Instrument Digital Interface (MIDI) is a standard format for exchanging musical information between electronic instruments. In piano recording setup, MIDI records note played, timing, duration, and pressure strength. Videos with MIDI files are used to source played notes and timestamps. Keyboard detection and key boundary identification are done using image processing techniques for correct fingering assignment. To handle interfering hands in piano playing videos, multiple frames are averaged to predict finger locations. Common pose estimation methods like OpenPose do not work well for piano playing as they assume wrist and elbow visibility. A pipeline approach is used instead, where hands are detected, cropped, and passed to a pose estimation model. The study focused on hand detection in piano playing videos, creating a dataset with 476 hands evenly split between left and right. A pre-trained object detection model was fine-tuned on this dataset, showing reasonable performance in detecting hands with distinction between left and right. The study fine-tuned a model for hand detection in piano playing videos, releasing a new dataset and trained model. They detect hands in each frame, choosing the 2 highest probability detections if more than two hands are found. They assign labels \"left\" and \"right\" based on hand position on the piano. To identify fingers pressing keys, they use pose estimation models. The study found that using off-the-shelf pose estimation models for hand detection in piano playing videos often failed due to visual effects like LED lights causing errors in finger position estimation. The models struggled with warm LED colors, mistaking them as extensions of fingers, and cool LED colors, leading to overestimation of hand pose length. The study highlighted errors in hand detection in piano playing videos caused by LED lights, leading to inaccurate finger position estimation. To address this, the pose estimation model needs augmentation for different lighting conditions, similar to sim2real works that transfer models from simulations to the real world using mapping functions like CycleGan. In the study, a mapping function G2 is sought to transform well-lit videos (source domain) into challenging lighted scenarios (target domain). The pose estimation model is then applied on the source domain using the transformation, resulting in improved generalization and performance on the target domain. Videos are manually categorized into well-lit or poorly-lit groups, with hands automatically detected and cropped for analysis. The study involved detecting and cropping hands from random frames, resulting in well-lit and poorly lit hands. A CycleGAN was trained on multiple epochs to create different lighting scenarios. Fine-tuning a pose-estimation model on original and transformed frames resulted in a model robust to various lighting conditions. Finger press estimation was modeled by calculating the probability of a specific finger being used, given the hand's pose and pressed note. The study involved detecting and cropping hands from random frames to create different lighting scenarios. Finger press estimation was modeled by calculating the probability of a specific finger being used, given the hand's pose and pressed note, using Gaussian distribution. Multiple frames were aggregated to estimate a more accurate prediction. The study focused on estimating finger presses by aggregating probabilities from multiple frames. A confidence score was defined based on the highest probability for each note, showing a trade-off between precision and recall based on the confidence threshold. MIDI and video files were considered for analysis. The study focused on estimating finger presses by aggregating probabilities from multiple frames, with a trade-off between precision and recall. MIDI and video files were used for analysis, with the video's audio track aligned with the MIDI file for synchronization. Some alignment mismatch was observed and addressed using a signal from the audio track. The study addressed alignment mismatch by using the system confidence score to maximize alignment precision between the audio-MIDI files. The process involved selecting the offset that resulted in the best confidence score for synchronization. In this section, the alignment offset with the best confidence score is chosen. The system is used to label 90 piano pieces with 155,107 notes in total. Evaluations of the overall system performance are presented, including training a PIANO-FINGERING model and fine-tuning it for better performance. The symmetry property of piano pieces played by two hands is utilized, and evaluation is based on the match rate between predictions. The text discusses evaluating a system for piano fingering prediction by comparing prediction accuracy with ground truth. Pose estimation is isolated to assess the impact on system performance. Manual annotation of dataset pieces is done to determine finger usage for each key. Confidence scores are used to filter key predictions, and precision, recall, and F1 scores are reported. Comparison is made between using CycleGAN for fine-tuning and not using it. The text evaluates a system for piano fingering prediction by comparing prediction accuracy with ground truth. Using confidence scores, the fine-tuned model achieves higher precision and recall, resulting in a 57% error reduction compared to the pre-trained model. The value of APFD is assessed in the context of Automatic Piano Fingering. The model for piano fingering prediction is fine-tuned on a manually labeled dataset, PIG, achieving superior results. It is treated as a sequence labeling task, using BiLSTM and MLP for prediction. The model, DNN (LSTM), is trained to minimize cross-entropy loss and a development set is created from the training set. The dataset, APFD, consists of 90 pieces split into 75/10. The dataset APFD is split into training and development sets, with 75 pieces for training and 10 for development. The results show improved accuracy using the model trained on silver data and fine-tuned on PIG. This approach outperforms the previous state-of-the-art model by 2.3%. The increase in performance is attributed to the larger number of training examples provided by the dataset. In this work, an automatic method for detecting PIANO-FINGERING from MIDI and video files of a piano performance is presented. A large scale PIANO-FINGERING dataset is created, containing 90 unique pieces with 155,107 notes. Training a neural network model on this dataset and fine-tuning on a gold dataset results in state-of-the-art accuracy. Future work aims to improve data collection by enhancing pose-estimation models and designing improved neural models. Future work aims to design improved neural models that consider previous fingering predictions to enhance global fingering transitions and address errors in pose estimation."
}