{
    "title": "H1z-PsR5KX",
    "content": "Neural machine translation (NMT) models learn representations with linguistic information. Unsupervised methods are developed to discover important neurons in NMT models, showing that translation quality depends on these neurons capturing common linguistic phenomena. Activations of individual neurons can control NMT translations in predictable ways. NMT systems achieve state-of-the-art results without additional linguistic information. Studies have shown that NMT models contain linguistic information on multiple levels: morphological, syntactic, and semantic. Previous work found meaningful individual neurons in computer vision and some NLP tasks. These methods require linguistic annotations, limiting their analysis. This work aims to address these limitations by developing unsupervised methods for analyzing individual contributions. In response to the limitations of previous methods, this work focuses on developing unsupervised techniques to analyze the importance of individual neurons in NMT models. The goal is to determine the significance of individual neurons in achieving high-quality translations, assess if they contain interpretable linguistic information, and explore the possibility of controlling MT output at the neuron level. Various unsupervised methods are employed to rank neurons based on their importance, with the hypothesis that similar important neurons should emerge in different NMT models. Mapping techniques such as correlation analysis, regression analysis, and SVCCA are used to compare neurons between different trained NMT models. The study focuses on analyzing the importance of individual neurons in NMT models using a method combining singular vectors and canonical correlation analysis. Highly-shared neurons impact translation quality more than unshared neurons, confirming the significance of shared information. Important neurons capture linguistic properties, including morphological and syntactic features. Intervening at the individual neuron level can help control NMT translations. The ability to control NMT translations on linguistic properties like tense, number, and gender is demonstrated. Individual neurons capture human-interpretable grammatical properties, and modifying their activations allows for controlling translation output. The methods developed are task-independent and contribute to the debate on neural machine translation. Recent work has focused on analyzing neural representations in NMT, including word embeddings, sentence embeddings, and representations at different linguistic levels. These studies evaluate learned representations through external supervision, but we propose intrinsic unsupervised methods for detecting important neurons based on correlations between independently trained models. This approach, previously used in computer vision, has not been applied to NMT or other NLP models before. Recent work has focused on analyzing neural representations in NMT, including word embeddings, sentence embeddings, and representations at different linguistic levels. This approach, previously used in computer vision, has not been applied to NMT or other NLP models before. Individual neurons in NLP have been studied, such as those that activate on brackets, sentiment, and length. However, not much work has been done on analyzing individual neurons in NMT. The aim is to address this gap by proposing intrinsic unsupervised methods for detecting important neurons based on correlations between independently trained models. In NMT, different correlations between neurons from various models are proposed as a measure of importance. Four methods for ranking neurons based on correlations between pairs of models are considered, aiming to identify similar important neurons across different models. The MaxCorr method looks for the highest correlation with any neuron in all other models. In NMT, different correlations between neurons from various models are proposed as a measure of importance. Four methods for ranking neurons based on correlations between pairs of models are considered. The MaxCorr method looks for neurons that capture properties strongly in two separate models, while the MinCorr method finds neurons well correlated with many models. Regression ranking involves linear regression to find neurons with distributed information in other models. SVCCA is used for analyzing neural networks through PCA on each model's representations. In NMT, importance of neurons is measured by correlations between models. PCA is used to capture variance in representations. Canonically correlated basis is obtained for each model pair. Neurons are ranked by CCA coefficients. Erasing neurons during translation tests their importance. Ranking of neurons is based on their impact on NMT models. In NMT, neurons are ranked by correlations between models using PCA and CCA coefficients. Neurons are then selectively erased from the top or bottom to test their impact on translation performance. Supervised verification is also used to validate the results. Neurons are analyzed using PCA and CCA coefficients in NMT. Supervised verification is used post-hoc to validate results. Simple metrics are reported for neuron activations, such as conditional variance and Gaussian mixture models. Linguistic annotations are obtained with Spacy for interpretability of machine learning models. Visualizing activations of neurons in machine learning models can provide valuable insights. Using the United Nations parallel corpus, models are trained from English to Arabic, Chinese, French, Russian, and Spanish. Different models are trained on various parts of the dataset to compare performance. LSTM encoder-decoder models with attention are utilized for training. The study focuses on 2-layer LSTM encoder-decoder models with attention, using a word representation based on a charCNN for language modeling and NMT. Erasure results on an English-Spanish model show that removing neurons from the top significantly impacts translation quality compared to removing from the bottom. Different rankings for erasure yield similar patterns, with top ranked neurons having a larger impact on BLEU score degradation. Erasing top neurons degrades BLEU significantly, while erasing SVCCA directions results in rapid degradation. Top SVCCA directions focus on identifying specific words. Erasing from the top hurts performance more than erasing from the bottom in language models. The study found specific neurons in NMT that capture word position in sentences. Different ranking methods reveal how neurons capture position or token identity information. LinReg and SVCCA focus on token identity, while MinCorr and MaxCorr tend to capture position information. This suggests that token information is distributed across multiple neurons. Neurons in NMT capture multiple linguistic information, with position represented in a few neurons. Top MinCorr neurons capture position, indicating a common way models capture this information. Specific token-activating neurons are important in some methods but not all, and less crucial for language information. Linguistic properties are explored through predictive capacity and neuron activations, with more details in the supplementary material. BID8 provides further analysis on interpretable neurons. Top neurons from each model are shown in parentheses TAB1. The BID8 study explores interpretable neurons in NMT models, focusing on detecting parentheses and predicting verb tense using GMM models. The study shows that neurons detecting parentheses are often unique and important in capturing patterns across multiple networks. Additionally, a top-scoring neuron in the English-Arabic model can distinguish between present and past tense verbs based on its activations. The study explores interpretable neurons in NMT models, focusing on detecting parentheses and predicting verb tense using GMM models. A charCNN representation captures past tense verbs, including irregular ones like \"held\", suggesting context understanding. The neuron also activates on nouns ending with \"s\" and \"Spreads\". Neurons correlated with tense prediction are highly predictive, with auto-encoder English models showing lower correlations. The study explores interpretable neurons in NMT models, focusing on detecting parentheses and predicting verb tense using GMM models. English models show lower correlations with tense neurons, suggesting tense emergence in \"real\" NMT models. Neurons also activate on numbers, dates, adjectives, plural nouns, and more, indicating various linguistic properties. The study explores interpretable neurons in NMT models, focusing on detecting parentheses and predicting verb tense using GMM models. Neurons in every network were found to have high accuracy, with many ranked highly by the MaxCorr method. Unsupervised methods can find important neurons without supervised annotations, such as a neuron that activates on numbers at the beginning of a sentence. This flexibility allows for controlling translation output based on linguistically meaningful properties. Neurons in NMT models can control translation output based on linguistically meaningful properties, such as gender biases in automatic translations. By intervening in neuron activations, desired translations can be induced, allowing for predictable control over the output. Neurons in NMT models can control translation output by modifying activations. Source and target sentences are tagged with properties like gender. Word alignments are obtained, and a GMM model predicts target properties. Source activations of top neurons are modified for words with desired properties to generate translations. In NMT models, translation output can be controlled by modifying activations of top neurons. The modification value is defined by mean activations of the property being modified. Results show success in changing tense with up to 67% success rate. Modifications may slightly degrade BLEU scores, but the impact is not significant. The success rate for controlling tense in NMT models is up to 67%, with a slight degradation in BLEU scores. However, controlling other properties like number and gender is more challenging, with success rates of 37% and 21% respectively. Future work can explore more sophisticated methods for controlling multiple neurons simultaneously. Examples show that translation control can work for multiple properties and languages. TAB3 displays translation control results for a neuron in an English-Spanish model, showing changes from plural to singular nouns as modification \u03b1 increases. High \u03b1 values result in nonsensical translations, but with correct number agreement. The translations demonstrate correct agreement between nouns and adjectives, such as Las partes interesadas vs. La parte interesada. Gender control examples are also shown for masculine and feminine nouns. When translating from English to various languages, tense and gender can be controlled by modifying activation levels of specific neurons. Changing tense from past to present is achievable in all language pairs, with some variations in phrasing. Gender control is more challenging, with a relatively low success rate. In Chinese, tense modification requires a large \u03b1 value due to the lack of tense markers in the language. Neural machine translation models learn linguistic information solely from example translations. Unsupervised methods were developed to identify important neurons in NMT and assess their impact on translation quality. Linguistic properties captured by individual neurons were analyzed through prediction tasks and visualizations. A protocol for controlling translations by modifying specific neurons was designed. This analysis can be extended to other NMT components and architectures, as well as other tasks. More research is needed to analyze localized vs. distributed properties. Further analysis is needed to examine the spectrum of localized vs. distributed information in neural language representations. Translation control experiments should be expanded to other components and architectures, with a focus on developing more sophisticated ways to control translation output. High-scoring neurons were identified in each network, particularly through the MaxCorr ranking method, indicating a correspondence with intrinsic unsupervised measures. The text discusses the correlation between high-ranking neurons based on intrinsic unsupervised measures and their ability to predict external annotations. Translation control experiments were conducted, showing successful tense modifications by activating specific neurons. The top scoring neuron in an English-Spanish model is visualized, demonstrating its effectiveness in translation tasks. The top scoring neuron in an English-Spanish model is visualized in FIG13, showing activation patterns for noun phrases and dates/numbers. Neurons capturing sub-word information like years and month names are highlighted. Additionally, a neuron sensitive to two properties simultaneously is discussed in FIG15. The neuron discussed in the curr_chunk is sensitive to the position of numbers at the beginning of a sentence and aims to capture patterns of opening list items. The methodology can be applied to models trained on different data and language pairs, showing highly correlated neurons across different checkpoints. The methodology discussed in the curr_chunk shows highly correlated neurons across different checkpoints, indicating its applicability to various models. Top neurons play a crucial role in translation performance, with similarities found in rankings across different models. The curr_chunk discusses a noun detector with a high F1-score for plural nouns and a Spanish noun gender detector. It also mentions positive and negative associations for certain nouns, prepositions, conjunctions, and determiners. The curr_chunk discusses the detection of conjunctions, tokens meaning \"in other words\", unknown tokens, tokens meaning \"regarding\", verbs suggesting equivalence, and various linguistic elements. The curr_chunk provides information on the detection of verbs, punctuation, forms of \"to be\", combined dates/prepositions/parentheses, plural noun detector, Spanish noun gender detector, singular noun detector, and possessives."
}