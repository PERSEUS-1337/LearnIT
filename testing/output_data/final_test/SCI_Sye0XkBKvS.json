{
    "title": "Sye0XkBKvS",
    "content": "This paper proposes using spectral element methods for efficient training of Neural Ordinary Differential Equations (ODE-Nets) for system identification. The dynamics are expressed as a series of Legendre polynomials, and the coefficients and network weights are optimized using coordinate descent. The approach is time-parallel, low memory, and outperforms standard methods in training surrogate models of dynamical systems. Neural Ordinary Differential Equations (ODE-Nets) can efficiently learn latent models from sparse time observations, improving generalization capabilities and performance in applications with sparse time information. This is crucial for complex control systems and model-based reinforcement learning that require long-term planning and high-frequency feedback for stability. In this paper, a novel alternative strategy for system identification is proposed, called SNODE, which is a compact representation of ODE-Nets using Legendre polynomials for full state information. A new optimization scheme is developed to find optimal polynomial coefficients and network parameters without solving an ODE at each iteration. This addresses the computational bottleneck of ODE-Nets, which are memory efficient but time inefficient. The proposed SNODE method uses Legendre polynomials for system identification, with an optimization scheme that avoids solving an ODE at each iteration. It outperforms explicit and adjoint methods in speed and convergence, reducing MSE significantly in numerical experiments. The backpropagation iterations through a solver are 50x faster than explicit schemes. The minimization of a scalar-valued loss function in an ODE-Net can be formulated as a constrained optimization problem. The problem can be solved using gradient-based optimization with time-stepping schemes for solving the ODE, such as the adjoint method proposed for neural networks. When f is a neural network, the adjoint method is used for solving optimization problems. However, limitations of backpropagation through ODE solvers include significant memory cost, the need to solve nonlinear equations, numerical errors affecting the solution, and unsuitable problem topology for optimization. The adjoint method, used for solving optimization problems with neural networks, has limitations. ODE-Nets solve using the adjoint method, involving simulating a dynamical system with an augmented Hamiltonian. The adjoint ODE is solved in the backward pass to provide gradients without storing intermediate states. Drawbacks include unstable dynamics, complex differential algebraic equations, and time-consuming boundary value problems. The ANODE method improves the robustness of solving complex differential algebraic equations by splitting the problem into time batches and using the adjoint method. SNODE algorithm discretizes the problem using spectral elements and relaxes the ODE constraint for efficient training through backpropagation. The ANODE method splits the problem into subproblems for optimization, using gradient descent and backpropagation. The initial trajectory is determined, and coordinate descent is applied to minimize residuals. The proposed algorithm uses SGD iterations with coordinate descent on residual and relaxation steps to find the optimal trajectory and parameters. The algorithm alternates between minimizing with respect to x(t) and \u03b8 to converge to a unique solution, with the choice of \u03b3 impacting the solution uniqueness. The algorithm uses SGD iterations with coordinate descent on residual and relaxation steps to find the optimal trajectory and parameters. The choice of \u03b3 impacts solution uniqueness by introducing a satisfaction tolerance. To numerically solve the problems, a discretization of x(t) is needed, achieved through the spectral element method using orthogonal Legendre polynomials and cosine Fourier basis. The algorithm utilizes a direct training scheme, \u03b4-SNODE, to solve the problem efficiently. It involves enforcing equations at collocation points and using Gauss-Lobatto nodes for initial condition enforcement. The integral is approximated as a sum of residual evaluations over quadrature points, with a new training scheme proposed for unique solution cases. The direct training scheme, \u03b4-SNODE, involves computing the integral using a least-squares approach and evaluating the loss function at quadrature points. An alternating coordinate descent scheme, \u03b1-SNODE, is also presented. The approach uses fixed numbers of updates for parameters \u03b8 and x(t) with standard routines like SGD. ADAM is used for optimization in experiments with an interpolation order of p = 14. Time parallelization is facilitated by enforcing R(t q ) = 0 at specific points. The proposed scheme enforces R(t q) = 0 at specific points, allowing for implicit time-stepping methods of order p. This increases parallelization capabilities across time, reducing complexity and memory requirements. The SNODE approximation error converges exponentially with p, leading to a compact representation of an ODE-Net with lower memory cost compared to explicit or implicit schemes. The proposed method greatly reduces complexity and memory requirements by using implicit time-stepping methods with a larger region of convergence. This results in more stable and robust training, supported by experiments. The proposed method reduces complexity and memory requirements by using implicit time-stepping methods with a larger region of convergence, leading to stable and robust training. Experiments show that reducing the time step can improve accuracy but may result in vanishing or exploding gradients. The method outlined in Appendix C does not suffer from this issue. The experiments had a common setup with specific hyperparameters, including a time horizon of 10s, batch size of 100, and learning rates of 10^-2 for ADAM and 10^-3 for SGD. The \u03b1-SNODE method used \u03b3 = 3 and 10 iterations for both algorithms at each epoch. The initial trajectory was perturbed to prevent exact convergence during initialization. The system is modeled using a neural network for Coriolis matrix, damping force, and coordinate transformation. The neural networks produce an ODE-Net surrogate model. Comparison of methods in high-data regime shows accuracy with fewer points needed for integration. The training performance of novel and traditional methods was compared using 100 equally-spaced time points. \u03b4-SNODE outperforms BKPR-DoPr5 by a factor of 50, with improved generalization. \u03b1-SNODE shows a speedup of 20 and reduces testing MSE by a factor of 10. \u03b1-SNODE maintains good testing MSE with fewer data points, but requires more iterations. The adaptive time step of DoPr5 significantly improves baseline performance but falls short of matching the accuracy of proposed methods. The adjoint method yields similar results to backpropagation. A multi-agent system with kinematic vehicles is described, with control laws detailed in Appendix B. Learning the kinematics matrix using a neural network is the goal. The task involves learning the kinematics matrix of a multi-agent system using a neural network. Different methods are compared, with \u03b4-SNODE being the fastest and \u03b1-SNODE performing the best. Results are summarized in Table 2, showing that ADJ-Euler is the slowest but falls between BKPR-Euler and the proposed methods in terms of test MSE. Random down-sampling of data affects ADJ-Euler the most. BKPR-DoPr5 had issues with time step tolerances. The use of a high order variable-step method (DoPr5) did not lead to good training results, as the loss function continued to increase over iterations. However, the fixed-step forward Euler solver was successfully used for learning the dynamics of a 30-state system. When training with increased gains, backpropagating through Euler fails, while our methods remain unaffected. Standard RNN dynamics can become unstable during training, causing gradients to explode and SGD to fail. Stability of SGD in RNNs with discretised ODEs is related to the solver's convergence region size. Higher-order and implicit solvers have larger convergence regions, potentially mitigating instabilities and improving learning efficiency. Unrolled architectures like RNNs with stopping criteria, highway and residual networks have been studied for iterative estimation. Residual networks have been treated as autonomous discrete-ODEs, with a focus on stability and convergence to input-dependent equilibrium. A discrete-time non-autonomous ODE based on residual networks has been explicitly stabilized for adaptive computation. In (Haber & Ruthotto, 2017; Ciccone et al., 2018), ODE stability conditions were used to train unrolled recurrent residual networks. Stability can be enforced by projecting state weight matrices into Hurwitz stable space. At test time, solver choice impacts overall stability. Physics priors on neural networks and differentiable physics frameworks have been explored for machine learning applications. In (Ruthotto & Haber, 2018), PDEs were used to analyze neural networks, while Gaussian Processes (GP) were utilized by Raissi et al., 2017 to model PDEs. Soleimani et al., 2017 used a linear ODE solution with a structured multi-output GP for patient outcome modeling. Pathak et al., 2017 predicted chaotic system divergence rate with RNNs. Test time requires an explicit integrator for unknown future outputs, while cross-validation involves evaluating loss on a different dataset by solving the ODE forward in time. Nonsmooth dynamics are also considered. The ODE-Net dynamics assume regularity r > p for exponential convergence of spectral methods. In cases where this assumption does not hold, a hp-spectral approach can be used near discontinuities. The set of functions generated by a fixed neural network topology may not have favorable topological properties for optimization, leading to theoretical open questions. The fixed neural network topology discussed in (Petersen et al., 2018) may not have favorable properties for optimization. The constraint relaxation proposed in this work aims to improve optimization space properties, similar to interior point methods, preventing local minima and gradient issues. The method resembles the MAC scheme with theoretical convergence results available (Carreira-Perpinan & Wang, 2014). It can be applied to a cascade of dynamical systems expressed as a single ODE, extending to cases like (Ciccone et al., 2018) where the final state feeds into the next block. The method discussed in (Ciccone et al., 2018) can be extended with smaller optimizations based on the number of ODEs. Latent states are not included in the loss function, making training and initializing polynomial coefficients challenging. A hybrid approach involves warm-starting the optimizer with backpropagation iterations. The model is formulated using a concentrated parameter form (Siciliano et al., 2008) and follows the notation of (Fossen, 2011). The system definition includes states \u03b7, v \u2208 R 3 representing x, y coordinates, vehicle orientation, and body-frame velocities, with input as torques in the body-frame. The Kinematic matrix is utilized with backpropagation. The Kinematic matrix is used with backpropagation for faster convergence in \u03b4-SNODE compared to Euler. A multi-agent simulation involves N a kinematic vehicles with states for position and orientation, and control signals for linear and angular velocities. The kinematics matrix is defined as J(\u03b7 i ) = cos(\u03c6 i ) 0 sin(\u03c6 i ) 0 0 1."
}