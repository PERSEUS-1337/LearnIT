{
    "title": "BygY4grYDr",
    "content": "Interpreting generative adversarial network (GAN) training as approximate divergence minimization has been theoretically insightful, spurring discussion and leading to interesting developments. Generative adversarial networks (GANs) have seen significant advancements with extensions like f-GANs and Wasserstein GANs. The non-saturating variant of training for GANs optimizes a reverse KL-like f-divergence, offering practical performance improvements. Theoretical tools have been developed to compare and classify f-divergences, aiming to clarify the theoretical aspects of GAN training. Recent advancements in GAN training have led to images with high fidelity and resolution. The key consideration in GAN training is the update scheme for the generator and critic, with developments focusing on divergence minimization. Different approaches like f-GANs and Wasserstein GANs aim to minimize specific divergences in a principled way. The non-saturating scheme in GAN training minimizes the f-divergence, known as the softened reverse KL divergence, putting it on par with Wasserstein GANs in terms of theoretical soundness and empirical results. This approach has been a topic of discussion since the original formulation of GAN training, with its impact on training dynamics being a point of confusion. The curr_chunk discusses the qualitative behavior of different divergences, such as softened reverse KL, and introduces tools to analyze f-divergences. It highlights the similarities and differences between divergences like reverse KL and Jensen-Shannon, showing the practical effects of the non-saturating training scheme in GANs. The discussion revolves around the non-saturating training scheme in GANs and its comparison to the saturating scheme based on Jensen-Shannon divergence. The original GAN paper suggests that the non-saturating scheme provides stronger gradients early in learning, leading to the same fixed point dynamics for the generator and discriminator. However, the final results are similar in the non-parametric case, where both gradients converge to q = p. The dynamics of training are also comparable when q \u2248 p, as all f-divergences agree in this scenario. The discussion focuses on the comparison between the non-saturating and saturating training schemes in GANs. The original GAN paper suggests that both schemes lead to the same fixed point dynamics for the generator and discriminator. However, the results differ in the general case of parametric q, where the gradients optimize different f-divergences. The argument that the original and non-saturating generator gradients have the same direction is found to be erroneous. The non-saturating generator gradient can have different behavior compared to the original gradient. It has been shown to optimize reverse KL instead of Jensen-Shannon divergence. This suggests that viewing GANs as optimizing Jensen-Shannon divergence may not be helpful. The divergence optimized by parametric critics is not the theoretically optimal divergence. The non-saturating GAN training optimizes a different divergence than Jensen-Shannon, focusing on reverse KL instead. This divergence optimization is crucial, especially when p and q have non-overlapping support. The success of non-saturating GAN training may be attributed to this different optimization approach. Arjovsky and Bottou acknowledge this by recognizing the unique gradient behavior and deriving a new objective function for classic GANs. The paper discusses a new divergence optimization approach for GAN training, different from Jensen-Shannon, focusing on reverse KL. Poole et al. (2016) also recognize the importance of optimizing different f-divergences for the generator and critic. They introduce an improved generator objectives for GANs (IGOG) divergence, which differs from previous work by a factor of u + 1. The paper discusses the definition and properties of f-divergences, which are used to measure the difference between probability distributions. These divergences are linear and non-negative, with equality only when the distributions are the same. The paper discusses f-divergences, which measure the difference between probability distributions. Divergences are determined by f and can be simplified in canonical form for easier comparison. The definition appears asymmetric but follows a specific symmetry. A and B represent left and right mismatches between distributions. The paper discusses f-divergences, which measure the difference between probability distributions. Divergences are determined by f and can be simplified in canonical form for easier comparison. A represents left mismatches and B represents right mismatches between distributions. f-GANs estimate f-divergence between distributions using samples. The paper introduces an elegant variational bound on the f-divergence D f (p, q) between two densities p and q, illustrated in Figure 4. The bound leads to variational divergence estimation, with the f-divergence between p and q being the focus. The paper introduces a variational bound on the f-divergence between two densities p and q, leading to variational divergence estimation. The f-divergence can be estimated by maximizing E f with respect to a neural net parameterized by \u03bd. This approximation may be close for sufficiently flexible neural nets. Three main f-divergences are briefly summarized. KL divergence KL(q p) is used to estimate a probabilistic model from data using an f-divergence. The goal is to minimize D f (p, q \u03bb ) with respect to \u03bb, where q \u03bb is the generator. Variational divergence bound E f has a gradient matching property. The optimal d given p matches gradients with respect to the generator parameters \u03bb. Adversarial optimization involves minimizing parameters \u03bb and \u03bd using gradient-based optimizers like stochastic gradient descent or ADAM. Hybrid (f, g) gradients can be used to train generators and critics separately. Non-saturating loss is often preferred for generator gradients in classic GAN training. In adversarial optimization, parameters \u03bb and \u03bd are minimized using gradient-based optimizers. Hybrid (f, g) gradients are used to train generators and critics separately. Non-saturating loss is preferred for generator gradients in classic GAN training to address optimization issues related to mismatched data distributions. The non-saturating generator gradient in f-GANs yields the traditional GAN scheme. It is the gradient of a globally coherent objective function, an f-divergence different from the saturating gradient. Non-saturating training is equivalent to a hybrid (f, g) scheme. The constant k in the f-divergence does not affect the gradients. The non-saturating gradient in f-GANs uses b instead of a in the generator gradient definition, resulting in a hybrid (f, g) scheme that approximately minimizes D f. Common choices of g lead to different tails in D f compared to D g, with left mismatches penalized more strongly. For the KL divergence, f (u) = u \u22122, making it a reverse KL divergence. This means \"non-saturating\" training based on the KL divergence is a hybrid (reverse KL, KL) scheme that approximately minimizes the reverse KL. The non-saturating training scheme in f-GANs uses a hybrid (f, g) approach that minimizes Df. The reverse KL divergence is utilized, resulting in a hybrid (1/2 \u03c7^2, reverse KL) scheme that minimizes the Pearson \u03c7^2 divergence. Goodfellow et al. (2014) described a hybrid (SRKL, JS) scheme that approximates the softened reverse KL divergence. The typical non-saturating GAN training scheme optimizes the softened reverse KL divergence. Analytic tools are developed to compare f-divergences visually, revealing mismatches between distributions p and q. The pushforward plot illustrates the distribution of p(x)/q(x) for x \u223c q(x). The f-divergence D f (p, q) involves an interaction between distributions p, q and the function f, nicely decomposing this interaction in terms of something that only depends on the one-dimensional distribution of a random variable. The distribution of the random variable can be described as the pushforward measure of q through the function u * (x) = p(x)/q(x), and can be rewritten in terms of d * (x) = log p(x) \u2212 log q(x). The f-divergence D f (p, q) involves an interaction between distributions p, q, and the function f, decomposing this interaction in terms of pushforwards and a one-dimensional integral. Plotting s f and pushforwards helps visualize different f-divergences and mismatches between p and q in x-space. Examples of pushforwards for multidimensional Gaussians with common covariance are shown. Examples of symmetry-preserving representations of f-divergences are shown in Figure 2, highlighting the penalization of mismatches by different divergence measures. The symmetry between KL and reverse KL is evident, with differences in how they penalize small versus large mismatches. Jensen-Shannon and Jeffreys divergence are also discussed in terms of their symmetric penalization of left and right mismatches. In this section, a classification scheme for f-divergences based on their behavior for large left and right mismatches is introduced. Softening the reverse KL divergence can make large right mismatches less severely penalized, leading to a more mode-seeking behavior. The Jensen-Shannon divergence exhibits extremely different behavior compared to reverse KL and softened reverse KL. The classification of f-divergences is based on their behavior for large left and right mismatches. Tail weights determine how strongly mismatches are penalized, with left tail weight R and right tail weight S playing key roles in determining the penalty for large mismatches. The right tail weight S determines the penalty for large right mismatches in f-divergences. Boundedness of f-divergences is characterized by a bound M such that D f (p, q) \u2264 M for all densities p and q. Left-boundedness and right-boundedness of f determine if D f is bounded. Tail weight plays a crucial role in determining boundedness. The tail weights of f-divergences determine boundedness, with left-boundedness if R < 2 and right-boundedness if S < 2. Divergences are bounded if R, S < 2. Boundedness properties are summarized in Table 1 and can be seen in Figure 2. Reverse KL is left unbounded but right bounded. Tail weights extend the classification of divergences as mode-seeking or covering. Models trained with reverse KL tend to have more compact distributions than the true distribution. The tail weights of f-divergences determine boundedness, with left-boundedness if R < 2 and right-boundedness if S < 2. Divergences are bounded if R, S < 2. Tail weights extend the classification of divergences as mode-seeking or covering, capturing distinctions in a precise way. The tail weights of f-divergences determine boundedness, with left-boundedness if R < 2 and right-boundedness if S < 2. Tail weights capture distinctions in a precise way, showing the qualitative effect of using different variants of GAN training. Jensen-Shannon divergence has tail weights (1, 1) and is bounded, while softened reverse KL divergence has tail weights (2, 0) and is unbounded. This affects the gradients and tolerance for mismatches in GAN training. The paper discusses discrepancies between different f-divergences in GAN training. Poole et al. (2016) define an approximation that matches non-saturating GAN gradients, but the gradients are not the same due to differences in partial derivatives. This impacts the optimization of f using gradient descent. The properties of f-divergences are discussed in more detail in this section. Adding an affine term to f(u) does not affect the divergence, as long as it respects the f(1) = 0 constraint. The second derivative of f determines the divergence completely, making f the essential quantity of interest. Working with f has advantages as it often has a simpler algebraic form than f. The f-divergence has a unique canonical form that can be achieved by scaling and adding an affine term. Different f-divergences behave differently for distant distributions but are essentially the same for close distributions. All f-divergences agree up to a constant factor on the divergence between nearby distributions. In this section, the text discusses how various f-divergences can be obtained from others through operations like reversing, symmetrizing, and softening. These operations provide a unified way to describe f-divergences based on KL divergence. Softening, for example, can reduce the penalty for large mismatches between distributions. The text discusses how f-divergences can be obtained through operations like reversing, symmetrizing, and softening based on KL divergence. Softening reduces penalties for large distribution mismatches. The main claim is that the non-saturating procedure for GAN training effectively minimizes the softened reverse KL divergence. In this section, details of f-divergences are discussed, obtained by plugging chosen f into specific expressions. The KL divergence and reverse KL divergence properties are outlined, along with the canonicalized Jensen-Shannon divergence. The canonicalized Jensen-Shannon divergence is both left-bounded and right-bounded. The Pearson \u03c72 divergence has specific tails and weights, while the canonicalized Pearson \u03c72 divergence has different tails and weights. There was a correction made to the expression for f in the original f-GAN paper. The non-saturating reverse KL generator gradient is equal to the conventional canonicalized Pearson \u03c72 generator gradient. The softened reverse KL divergence is discussed in the arxiv preprint, with tails and weights specified. The equality of the non-saturating canonicalized Jensen-Shannon generator gradient to the conventional softened reverse KL generator gradient is highlighted. The original f-GAN paper presents results in terms of the Legendre transform f*. The output activation in the original f-GAN paper adapts the neural net's output to the domain of f* by using f(exp(d)). The gradient matching property suggests that multiple critic updates followed by a single generator update is a viable learning strategy for optimizing the true divergence Df with respect to \u03bb. The approaches of updating generators and critics in GANs have correct fixed points in terms of Nash equilibria and optima of Df. Convergence properties are well-investigated elsewhere, and performing many critic updates for each generator update ensures the generator gradient is close to the gradient of Df. The fixed points of the gradients in GANs are correct in terms of Nash equilibria and optima of Df. The choice of f-divergence affects the generator gradient, with different factors allowing gradients to pass freely. Regions with large negative d have a significant contribution to the gradient, while regions with large positive d have a minimal impact. Based on the choice of f-divergence, regions with large negative d have a small impact on the gradient, while regions with large positive d are exponentially magnified. Left-unboundedness and right-unboundedness both play a role in allowing learning from random initialization. The overall gradient can serve as a learning signal, as shown in training models like multivariate Lines. The original scheme converges to the JS divergence minimum, while the non-saturating scheme converges to the SRKL divergence minimum. The gradient matching property shows that the saturation issue is fundamental to the Jensen-Shannon divergence. More critic updates lead to a more saturated d on samples from q, closely approximating the true divergence Df. The non-saturating generator gradient is used to address the saturation issue in the Jensen-Shannon divergence. A simple experiment was conducted to validate the mathematical conclusions, showing that the original and non-saturating gradients minimize different divergences."
}