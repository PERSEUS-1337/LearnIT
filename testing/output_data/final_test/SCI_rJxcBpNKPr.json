{
    "title": "rJxcBpNKPr",
    "content": "In the field of Continual Learning, the objective is to learn several tasks one after the other without access to the data from previous tasks. A new method, OvA-INN, is proposed to learn one class at a time without storing previous data. This method uses specific Invertible Neural Networks for each class and can predict the class of a sample by identifying the network with the smallest norm output. In the field of Continual Learning, a new method called OvA-INN is introduced to learn one class at a time without storing previous data. This method involves stacking an invertible network on top of a features extractor to outperform state-of-the-art approaches for Continual Learning on MNIST and CIFAR-100 datasets. The model achieves 72% accuracy on CIFAR-100 after training one class at a time. Recent works propose approaches in Continual Learning to address limitations in learning parameters on a sequence of datasets without reusing previous data. Performance is measured with multi-head or single-head protocols, where task identifiers determine the evaluation criteria. In Continual Learning, performance is evaluated using multi-head or single-head protocols, where task identifiers determine the criteria. Single-head learning is more aligned with the requirements of a Continual Learning system and is harder than multi-head learning. This scenario is the focus of the present work, aiming to update parameters with data from new datasets. The present work focuses on addressing catastrophic forgetting in Continual Learning by proposing various approaches such as storing samples from previous datasets, adding distillation regularization, updating parameters based on usefulness on previous datasets, and using a generative model. Despite efforts towards a more realistic setting, results often show better accuracy in a sequence of batches with multiple classes. In addressing catastrophic forgetting in Continual Learning, a new method called One-versus-All Invertible Neural Networks (OvA-INN) is proposed. This approach is designed for scenarios where each task consists of only one class, allowing for independent training of each class. The method is based on an invertible neural network architecture and utilizes a One-versus-All strategy for class prediction. The contributions of our work include a new approach for Continual Learning with one class per batch using a neural architecture based on Invertible Networks. We achieve state-of-the-art results on tasks like CIFAR-100 and MNIST. Inspired by biological mechanisms, our method aims to produce samples of previous data to aid in learning new tasks. The curr_chunk discusses different approaches in Continual Learning, such as using autoencoders, generative adversarial networks, and coreset-based models like iCaRL and End-to-end IL. These models store previous data samples to alleviate forgetting and improve training iterations. Gradient Episodic Memory is also mentioned as a method to ensure gradient stability. Distance-based models propose embedding data in a space to identify sample classes by computing distances. Popular methods include Matching Networks and Prototypical Networks, mainly applied to few-shot learning scenarios. Regularization-based approaches aim to mitigate catastrophic forgetting in Continual Learning. Approaches like Elastic Weight Consolidation, Synaptic Intelligence, and Memory Aware Synapses aim to prevent catastrophic forgetting by constraining the update of weights useful for discriminating previous classes. Learning without forgetting suggests using knowledge distillation to preserve past performances by dividing the network into shared and task-specific weights. In order to prevent catastrophic forgetting, various approaches like Elastic Weight Consolidation, Synaptic Intelligence, and Memory Aware Synapses constrain the update of weights for discriminating previous classes. Learning without forgetting involves using knowledge distillation to preserve past performances by dividing the network into shared and task-specific weights. Additionally, in the multi-head setting, Progressive Networks suggest using previously learned layers and adding new layers for each new task to reduce the growth in complexity. The authors of Dynamically Expandable Networks proposed a hybrid method to reduce memory growth in new layers. However, this approach is not suitable for single-head learning and is not included in benchmarks with OvA-INN. The problem of training multiple datasets sequentially with batches of one class at a time is investigated, where updating a feature extractor when data from a new class is available is inefficient without negative data samples. Our approach interprets Continual Learning as out-of-distribution (OOD) detection problems, where each class is trained to predict if an input is from the same distribution as the training data. This method addresses the lack of negative samples in deep learning, which can lead to overfitting new classes. Generative models have been proposed to generate samples of old classes, but updating a network with sampled data is less efficient and can degrade the generative quality over time. Our approach treats Continual Learning as out-of-distribution (OOD) detection, training each class to predict if an input belongs to its distribution. This method overcomes the lack of negative samples in deep learning, preventing overfitting of new classes. Generative models have been suggested for generating samples of old classes, but updating a network with sampled data is less efficient and can degrade generative quality over time. The neural network architecture proposed by NICE operates a change of variables between two density functions, assuming invertibility and respecting certain constraints. The log-likelihood computation involves identifying the network with the smallest output norm, with fully-connected feedforward layers used in the experiments. The proposed neural network architecture by NICE involves using invertible blocks to create a bijection between input and output. Each Invertible Network is specialized to a specific class by training it to output a vector with small norm for data samples from that class. The weights of the network are not updated when new classes are added, and at inference time, the network can handle multiple classes. The proposed neural network architecture by NICE involves using invertible blocks to create a bijection between input and output. At inference time, the predicted class for a sample is obtained by running each network and identifying the one with the smallest output. In experiments, fully connected layers are used to reduce memory footprint by compressing parameters. The method is compared against state-of-the-art baselines for single-head learning on MNIST and CIFAR-100 datasets. The proposed neural network architecture by NICE involves using invertible blocks to create a bijection between input and output. In experiments, fully connected layers are used to reduce memory footprint by compressing parameters. The method is compared against state-of-the-art baselines for single-head learning on MNIST and CIFAR-100 datasets. The memory cost and regularization techniques are discussed, along with rescaling images to match the size of images from Imagenet. Generative models such as Parameter Generation and Model Adaptation (PGMA) and Deep Generative Replay (DGR) are also mentioned. The proposed neural network architecture by NICE involves using invertible blocks to create a bijection between input and output. For Parameter Generation and Model Adaptation (PGMA) and Deep Generative Replay (DGR), results from original papers are reported. SupportNet, iCaRL, and DGR models are evaluated with specific architectures and coreset sizes. The average accuracy over all classes is computed after training on all batches without using pretrained features extractors. Our approach utilizes an Invertible Network with two stacked invertible blocks, outperforming other methods with lower memory cost and single-class batch training. Unlike baselines using convolutional layers, our architecture employs simple fully-connected layers within invertible layers. For a more complex image dataset, we compare the effectiveness of using a pretrained features extractor for Continual Learning. Our Distance-based model includes -Nearest prototype method, computing mean vectors for each class from a pretrained ResNet32. Inference is based on finding the closest prototype to the ResNet output. Additionally, our Generative model, FearNet, is implemented. OvA-INN uses the weights of a ResNet32 pretrained on CIFAR-100 classes. FearNet, iCaRL, and End-to-end IL are core-set based models that retrain ResNet architectures on new data with different loss functions. FearNet uses pretrained weights from a ResNet and trains an autoencoder for feature extraction. Despite training on all available data, FearNet's accuracy is lower than OvA-INN. The Nearest prototype baseline shows the benefits of using pretrained feature extractors on this dataset. The Invertible Network (INN) is compared to ResNet features using t-SNE projection in 2D. Classes well represented by ResNet features are separated from INN clusters, while ambiguous classes are better clustered in the INN space. OvA-INN requires adding a new network for each new class, increasing memory and computational costs linearly with the number of classes. OvA-INN achieves superior accuracy on CIFAR-100 class-by-class training with fewer parameters. Invertible Networks face challenges in maintaining output size equal to input size, impacting memory consumption. Dependency on quality of pretrained features extractor is crucial. Rescaling input may be necessary for compatibility. Recent research shows promising results in training efficient features extractors, eliminating the need to retrain them. Future research could explore incorporating this method into Reinforcement Learning scenarios for improved policy learning. Additionally, in a regression setting, this approach can be beneficial. In a regression setting, a fully connected layer can be added after an intermediate layer of an Invertible Network to predict the output for the trained class. A new approach for single-head Continual Learning without storing previous data is proposed, where Invertible Networks are trained for each class on top of a fixed pretrained neural network. This allows for predicting the class of a sample by running each Invertible Network and identifying the one with the highest log-likelihood, resulting in good performances on class-by-class training of CIFAR-100. The models achieve good performances on class-by-class training of CIFAR-100. The memory consumption for different methods is calculated based on the parameters used. For example, OvA-INN uses 2 blocks with 2 layers for 100 classes, resulting in a memory requirement of 6656000. The implementation includes a default coreset size of 2000 for iCaRL and End-to-End IL. Our implementation of S iCaRL on CIFAR-100 uses Pytorch with Adam optimizer and a scheduler. We conduct multi-head learning with 10 tasks of 10 classes each, achieving state-of-the-art accuracy but with higher memory and time consumption compared to other methods. S iCaRL implementation on CIFAR-100 achieves state-of-the-art accuracy but is more memory and time consuming compared to other methods. EWC 81.34, Progressive Networks 88.19, DEN 92."
}