{
    "title": "rkltE0VKwH",
    "content": "In reinforcement learning, addressing sparse rewards is a key challenge. In the multi-agent setting, coordinating exploration among agents can accelerate learning. A proposed approach involves dynamically selecting intrinsic rewards based on what all agents have explored, allowing for coordinated exploration and maximizing returns. In reinforcement learning, addressing sparse rewards is a challenge. A policy is proposed where a high-level controller selects among sets of policies trained on different intrinsic rewards. The low-level controllers learn action policies of all agents under these rewards. The approach is effective in a multi-agent gridworld domain with sparse rewards and scales up to more complex settings on the VizDoom platform. Recent work in deep reinforcement learning tackles challenging problems like Go, Atari video games, and robotic control, but often relies on dense rewards for feedback. Dense rewards can lead to locally optimal but globally sub-optimal behavior, making it desirable to specify only a. Sparse rewards in reinforcement learning pose challenges such as the credit-assignment problem and the difficulty in determining which actions led to a reward. Researchers have developed methods to provide intrinsic rewards to encourage meaningful behavior and exploration of the state space. Exploring the state space is crucial for solving sparse reward problems in reinforcement learning. Novelty-based intrinsic motivation rewards agents for reaching novel states, but applying this to cooperative multi-agent settings can lead to inefficient independent exploration. In cooperative multi-agent reinforcement learning, exploring the state space efficiently is essential. Agents should avoid redundant exploration and instead focus on dividing tasks to reach multiple landmarks. Sharing information about exploration can benefit the agents, but the strategy depends on the specific task at hand. In cooperative multi-agent reinforcement learning, efficient exploration of the state space is crucial. Identifying useful inductive biases and designing intrinsic reward functions are key for improving exploration. Introducing a set of intrinsic rewards with varying exploration properties and a hierarchical method for learning policies can enhance the agents' performance. Sharing a replay buffer among all policies significantly boosts sample efficiency and effectiveness. In cooperative multi-agent reinforcement learning, efficient exploration is crucial. Prior works propose reward bonuses for agents to reach novel states. Some use count-based approaches while others focus on intrinsic rewards inspired by psychology. These methods aim to improve sample efficiency and learning effectiveness. In cooperative multi-agent reinforcement learning, exploration is crucial. Prior works propose reward bonuses for agents to reach novel states, using intrinsic rewards inspired by psychology. Challenges include multi-agent credit assignment, non-stationarity of the environment, and learning communication protocols between agents. Exploration in multi-agent reinforcement learning (MARL) is a relatively unexplored area at the intersection of RL and MARL. Previous works have focused on opponent strategies in competitive games and exploring large joint action spaces. Some works have defined intrinsic reward functions to encourage agents to influence other agents' behavior. However, there is a lack of focus on exploring large state spaces and improving exploration in multi-agent systems. In this work, the focus is on decentralized POMDPs (Dec-POMDPs) for cooperative multi-agent tasks. Dec-POMDPs are defined by a tuple and involve learning diverse policies with a shared replay buffer. The approach differs from previous work focused on single-agent tasks and lacks intrinsic rewards, making it unsuitable for sparse reward problems in MARL. The focus is on decentralized POMDPs (Dec-POMDPs) for cooperative multi-agent tasks. A joint action is denoted as a = {a 1 , . . . , a n } \u2208 A and a joint observation is o = {o 1 , . . . , o n } \u2208 O. T is the state transition function, O is the observation function, R is the reward function, and \u03b3 is the discount factor. The approach uses Soft Actor-Critic (SAC) as its underlying algorithm, which incorporates an entropy term to encourage exploration and prevent premature convergence. The policy gradient with an entropy term is computed using a replay buffer, critic parameters, state-dependent baseline, and reward scale parameter. The critic is learned with a loss function and updated using a target critic with a hyperparameter controlling the update rate. Centralized training with decentralized execution is a common approach in deep multi-agent reinforcement learning. In deep multi-agent reinforcement learning, agents can train by sharing information without communication at execution time. Intrinsic reward functions for exploration incorporate information about other agents' exploration, using a novelty function to determine observation novelty based on past experience. Recent approaches for developing novelty-based intrinsic rewards in complex domains include random network distillation. All agents share the same observation space, allowing each agent's novelty function to operate on all other agents' observations. Different intrinsic reward functions are defined in Table 1, with INDEPENDENT rewards focusing on an agent's own observation novelty, while MINIMUM rewards consider the novelty of all agents to reward exploration in uncharted areas. The method of providing intrinsic rewards for exploration includes COVERING and BURROWING, which reward agents for exploring novel areas compared to their teammates. COVERING encourages agents to explore more novel regions, while BURROWING incentivizes exploring less novel areas in the hope of discovering new regions. These rewards lead agents to continue exploring until all possible intrinsic rewards are exhausted. The method of providing intrinsic rewards for exploration includes COVERING and BURROWING, which incentivize agents to explore novel areas compared to their teammates. These rewards result in agents continuing to explore until they exhaust all possible intrinsic rewards from a given region. LEADER-FOLLOWER uses burrowing rewards for the first agent and covering rewards for the rest, leading to thorough exploration by one agent and the others following along to cover the space. This approach can incorporate other reward types as long as they can be computed off-policy. In this section, the approach for learning policies with different intrinsic rewards simultaneously is presented. Policies are trained in parallel using a shared replay buffer and off-policy learning to maximize sample efficiency. Novelty functions are computed off-policy for each agent, allowing for dynamic selection of the best intrinsic reward type during training. In learning policies with different intrinsic rewards simultaneously, a shared network is used for each agent's set of policies, branching out into different heads for each reward type. A single network is learned for critics across all agents, branching out into separate heads for each agent and reward type. Separate heads are learned for intrinsic and extrinsic rewards. Agents are indexed by i and intrinsic reward types by j, with a shared base/input and a head/output specific to each reward type in a neural network. In a neural network, agents are indexed by i and intrinsic reward types by j, with separate heads for each reward type. The critics are trained with a loss function adapted from soft actor-critic, using target Q-functions and policies. Intrinsic rewards are calculated based on observations resulting from actions taken, allowing for the calculation of loss functions for expected intrinsic and extrinsic returns for all policies. The method involves learning multiple policies for each agent in parallel, using intrinsic and extrinsic rewards. A scalar \u03b2 determines the weight of intrinsic rewards, and a multi-agent advantage function aids in credit assignment. Dynamic policy selection is crucial for maximizing extrinsic returns while considering unknown regions. A meta-policy is learned to select policies for environment rollouts. The method involves learning multiple policies for each agent in parallel, using intrinsic and extrinsic rewards. A meta-policy is learned to select policies for environment rollouts based on maximizing extrinsic returns without collapsing to a single set of policies too early. The selector policy \u03a0 is parameterized with a vector \u03c6, and the probability of sampling head j is proportional to exp(\u03c6[j]). Policy gradients are used to train the policy selector to maximize expected extrinsic returns. The method involves learning multiple policies for each agent in parallel, using intrinsic and extrinsic rewards. A meta-policy is learned to select policies for environment rollouts based on maximizing extrinsic returns without collapsing to a single set of policies too early. The selector policy \u03a0 is parameterized with a vector \u03c6, and the probability of sampling head j is proportional to exp(\u03c6[j]). Policy gradients are used to train the policy selector to maximize expected extrinsic returns. The returns received during rollouts are used to train the policy selector, promoting entropy to prevent it from collapsing onto a single exploration type. This allows for learning a diverse set of behaviors based on various multi-agent intrinsic reward functions. Experimental results demonstrate the effectiveness of this approach in maximizing performance on tasks. Additional details are provided in the appendix. In the appendix, additional details are provided on the approach of using multiple policies for each agent in parallel. The tasks involve cooperative treasure collection with negative time penalties to motivate quick completion. Different versions of tasks are assigned to agents and treasures based on the number of agents involved. The method aims to maximize extrinsic returns by selecting policies for environment rollouts using a learned meta-policy. In a multi-agent gridworld domain, agents are motivated to complete tasks quickly by collecting treasures. Rewards are shared, and the optimal strategy varies for different tasks. The environment includes random transitions, black holes, and stochastic elements. In a multi-agent gridworld domain, agents collect treasures to complete tasks quickly. The environment includes random transitions and black holes that open with a probability at each time step. If an agent steps into an open black hole, they are sent back to their starting position. The rate of black holes dropping out is higher in TASK 1 to balance difficulty. The novelty function for each agent is defined as 1/N\u03b6, where N is the number of times the agent has visited its current cell and \u03b6 is a decay rate. In VizDoom, tasks similar to the gridworld environment are implemented for testing scalability. The \"My Way Home\" map is used for multi-agent tasks, with agents starting closer to rewards to increase exploration difficulty. Action repeat is lowered to 2 to maintain challenge. Count-based intrinsic rewards are used in VizDoom. Our approach in VizDoom involves separating agents' positions into discrete bins and using counts for these bins. We found that \u03b6 = 0.7 works well in our experiments. The shaded region represents a 68% confidence interval across 6 runs. Our MULTI-EXPLORATION approach is competitive with the best individual intrinsic reward function. Ablations of our model show that both the meta-policy selector and diverse intrinsic reward functions are crucial for successful exploration tasks. Figure 3a demonstrates the results of our approach on the 2 agent version of TASK 1 in gridworld. Our approach in VizDoom involves using counts for discrete bins of agents' positions. The MULTI-EXPLORATION approach is competitive with the best individual intrinsic reward function. Ablations show that the meta-policy selector and diverse intrinsic reward functions are crucial for successful exploration tasks. The policy selector learns to select the best performing reward functions on each task, such as BURROWING and MINIMUM rewards on TASK 1 with 2 agents. Our approach in VizDoom involves using counts for discrete bins of agents' positions. The MULTI-EXPLORATION approach is competitive with the best individual intrinsic reward function. Ablations show that the meta-policy selector and diverse intrinsic reward functions are crucial for successful exploration tasks. The policy selector learns to select the best performing reward functions on each task, such as BURROWING and MINIMUM rewards on TASK 1 with 2 agents. Our results on the VizDoom domain mirror those in the gridworld, showing that our methods are not limited to discrete domains. Our approach can sometimes surpass the performance of the best individual reward function on TASK 3, with exploration types like BURROWING and MINIMUM potentially speeding up the exploration process. Our method surpasses individual reward types but struggles to match the best method in some settings. This limitation may be due to the need for a specific exploration strategy early in training. Visualizations show that INDEPENDENT rewards lead to agents exploring the state space without considering other agents. On TASK 1, INDEPENDENT rewards struggle due to lack of coordination between agents. However, on TASK 3, independent exploration performs better. TASK 2 has a lower rate of black holes, making exploration easier, where INDEPENDENT rewards and LEADER-FOLLOWER perform well. Minimum rewards prevent redundant exploration but can lead to one agent exploring all regions first. COVERING rewards lead to agents constantly switching up regions they explore, not useful in tasks tested but may be effective in spreading out scenarios. BURROWING rewards cause agents to explore different subregions until exhausted, effective in TASK 1 for mutually exclusive exploration. Comparing to a baseline meta-policy selecting actions randomly. Our approach outperforms a baseline meta-policy that selects actions randomly. Testing different reward functions and a meta-policy selector, we find that both aspects are crucial for success in multi-agent exploration tasks. Further ablations/comparisons show the effectiveness of our method in various scenarios. Results on tasks 1, 2, and 3 with 2 agents in GridWorld are shown in Figure 3b and the Appendix (A.5). The first approach involves computing intrinsic rewards by treating all agents as one entity, leading to less efficient learning compared to our method. The second approach removes the entropy term from the loss function, resulting in lower performance, highlighting the importance of entropy in preventing early convergence. Our method focuses on the importance of entropy in preventing early convergence to suboptimal policies. We propose various multi-agent intrinsic reward functions and compare them qualitatively and quantitatively on exploration tasks. By dynamically selecting the most effective intrinsic reward types, our method matches or surpasses performance while using the same number of environment samples. In future work, methods for directly learning multi-agent intrinsic reward functions will be introduced. Black holes in the environment send agents back to their starting positions, adding difficulty to exploration. The probability of a black hole opening at each step evolves based on specific parameters for different tasks. Agents observe global and local information to navigate the environment effectively. The global state in the episode is represented by agents' (x, y) coordinates and local information on black holes, walls, and treasures. Each agent's action space includes cardinal directions and a stationary option. Agents receive a 48x48 grayscale egocentric view as observations. In VizDoom, count-based intrinsic rewards are used, and agents' positions are separated into vectors. In VizDoom, agents' (x, y) positions are discretized into bins for counting. Each agent has 3 actions: turn left, turn right, or go forward. Training details are outlined in Algorithm 1 with hyperparameters listed in Tables 3 and 4. Parameters were fine-tuned on task 1 with 2 agents before applying to other settings. The curr_chunk discusses the update of target parameters for Multi-Explore with Soft Actor-Critic. It includes details on convolutional layers and ablations/comparisons to the model across different tasks. In the 2 agent version of gridworld, two ablations/comparisons were made to the model across all tasks. The first (Centralized) involved computing intrinsic rewards by treating all agents as one, leading to less efficient learning. The second (Multi (No Entropy)) removed the entropy term from the loss function, resulting in suboptimal performance compared to the full method. In Figure 19, the meta-policy behavior is analyzed in two separate runs on Task 3, surpassing individual reward functions. Independent exploration is effective but our method outperforms it by selecting burrowing rewards. However, deploying burrowing rewards alone is not successful as agents commit to exploring a specific region. Using our methods, the meta-policy can wait for burrowing exploration regions to align with assigned rewards before selecting policies trained on these rewards, which is more efficient than independent exploration."
}