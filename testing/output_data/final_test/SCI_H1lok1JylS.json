{
    "title": "H1lok1JylS",
    "content": "State-of-the-art sentence representation models can perform tasks requiring knowledge of grammar, but evaluating their grammatical knowledge remains an open question. Five experimental methods inspired by prior work are explored to evaluate pretrained sentence representation models using negative polarity item (NPI) licensing as a case study. NPIs like 'any' are grammatical only in specific environments like negation. An artificially generated dataset manipulates key features of NPI licensing for the experiments, revealing that BERT has significant knowledge of these features but its success varies across different methods. Various methods are necessary to reveal all aspects of a model's grammatical knowledge in a specific domain. Recent work has evaluated the grammatical knowledge of sentence encoders like BERT through various methods, including probing tasks, comparing language models' probabilities for different sentences, and using Boolean acceptability judgments. However, there has been no direct comparison between these methods to determine if they yield similar conclusions about a model's knowledge. The aim is to understand the trade-offs in task choice by comparing different methods inspired by previous research. The study aims to understand trade-offs in task choice by comparing methods to evaluate sentence understanding models. Negative polarity item (NPI) licensing is chosen as a case study, where NPIs are words that can only appear in negative environments. The syntactic scope of a licensor determines the acceptability of a sentence with an NPI. The study compares five experimental methods to test BERT's knowledge of NPI licensing, finding that BERT understands NPI licensing environments. BERT's knowledge of NPI licensing environments was tested using five experimental methods, showing varying results. The gradient minimal pair experiment suggests systematic knowledge, while the absolute minimal pair and probing experiments reveal uneven knowledge across domains. No single method accurately depicts the model's grammatical knowledge, recommending the use of multiple methods for evaluation. Significant interest in understanding linguistic knowledge in models. Related work focuses on Boolean classification tasks to evaluate grammatical knowledge. Various methods like acceptability classification and probing classifiers are used to assess the models' ability to identify syntactic dependencies and encode grammatical information. The curr_chunk discusses the encoding of syntactic and surface features in sentence embeddings, including tense, voice, sentence length, and word content. It also mentions the use of diagnostic classifiers and automatic data generation to evaluate compositional reasoning in RNN-based language models. Additionally, it touches on the study of contextualized sentence encoders and negative polarity items in NPI licensing environments. The curr_chunk discusses the importance of structure in learning non-local dependencies like NPI licensing in computational linguistics. It highlights the use of neural networks for acceptability judgments using the Corpus of Linguistic Acceptability (CoLA) dataset. The curr_chunk discusses the evaluation of grammatical knowledge of sentence representation models like BERT using a generated NPI acceptability judgment dataset. The dataset includes sentences with Boolean labels indicating grammatical acceptability and meta-data variables. The evaluation of sentence representation models like BERT involves testing their ability to judge the grammatical acceptability of sentences using Boolean meta-data variables. Performance is measured using the Matthews Correlation Coefficient, and a minimal pair experiment is conducted to analyze classifiers on minimally different sentences. In evaluating sentence representation models like BERT, performance is measured using the Matthews Correlation Coefficient. A minimal pair experiment is conducted to analyze classifiers on sentences that differ in only one NPI-related Boolean meta-data variable. The models are tested on acceptability judgments with minimal pairs, using absolute and gradient minimal pair evaluations, as well as a cloze test with masked language modeling in BERT. The experiment involves testing the probability of a masked token in a sentence using BERT's native MLM functionality. Probing classifiers are used to identify grammatical variables by training lightweight classifiers on frozen sentence encoders. Each meta-data label alone does not determine acceptability. In order to probe BERT's performance on sentences involving NPIs, a set of sentences and acceptability labels were generated for experiments. A controlled set of 136,000 English sentences was created using an automated procedure inspired by previous work. The study involved creating a set of sentences with NPIs to test BERT's performance. The sentences were part of a controlled set of 136,000 English sentences. The NPI/NPI replacement and licensor/licensor replacement were analyzed within different licensing environments. The study involved creating a set of sentences with NPIs to test BERT's performance. NPI presence indicates whether an NPI is in the sentence or replaced by a non-NPI with similar distribution. Scope manipulation determines if an NPI falls within the licensor's scope. Simple Questions lack scope manipulation as the question takes scope over the entire clause. The study generated sentence templates for NPIs, specifying part-of-speech and instance number. Templates included verb tense and noun number details. Training set had 10K sentences per environment, with dev and test sets having 1K each. BERT was also tested on 104 handcrafted sentences from a similar paradigm. The NPI sub-experiment in BID34 overlaps partially with our paradigm but includes an additional condition where the NPI linearly follows its licensor. This extra test set was used to evaluate acceptability classifiers. Sentences were validated using Amazon Mechanical Turk, with 500 sentences randomly sampled and rated by 20 participants on a Likert scale. Results showed that acceptable sentences were more often rated as acceptable compared to unacceptable sentences. In the study, sentence understanding models like GloVe BoW and BERT were used, with BERT showing promising results in pilot experiments. Additionally, intermediate training on related tasks was found to impact BERT's performance positively. In addition to exploring BERT's performance on downstream tasks, two additional BERT-based models were investigated: BERT\u2192MNLI and BERT\u2192CCG. BERT\u2192MNLI was fine-tuned on the Multi-Genre Natural Language Inference corpus, while BERT\u2192CCG was fine-tuned on the Combinatory Categorial Grammar Bank corpus. These models were motivated by prior work on pretraining sentence encoders and showed significant improvements on semantic tasks. Results from the minimal pair show high scores ranging from 0.65 to 1.00. Licensor, Scope, and NPI-Present values are also provided in Figure 2. Results from the minimal pair test in Figure 2 display high accuracy scores for NPI detection, licensor detection, and minimal pair contrasts. The study evaluates GloVe BoW and BERT models in various training-evaluation configurations to assess their ability to generalize NPI licensing across different environments. BERT outperforms the BoW baseline on all test data with different training-evaluation configurations, showing high accuracy scores for NPI detection. However, performance drops when using the All-but-1 NPI configuration, especially in the simple question environment. On NPI environments, CoLA fine-tuning results in lower BERT performance compared to NPI data finetuning. Plain BERT outperforms BERT\u2192MNLI and BERT\u2192CCG on most NPI environments. MNLI and CCG fine-tuning do not significantly improve acceptability judgments. BERT shows near-perfect accuracy on minimal pairs differing by the presence of an NPI. BERT achieves near perfect performance on the gradient measure for licensor and scope, while BoW does not. Absolute judgment is more challenging for licensor, involving a larger pool of lexical items and syntactic configurations. Intermediate fine-tuning on MNLI and CCG does not improve performance. BERT MLM can distinguish between acceptable and unacceptable sentences in the NPI domain without supervision on NPI data. Accuracy for NPI-detection ranges from 0.76 to 0.93, while licensor-detection accuracy varies. Accuracy for licensor-detection varies, with BERT MLM performing well in certain environments but poorly in others. Plain BERT outperforms BoW in detecting scope. Licensor detection is slightly more challenging for models fine-tuned with CoLA or NPI data. CoLA fine-tuning improves BERT's performance, especially for NPI presence, while fine-tuning on NPI data enhances scope detection. Models struggle with superlatives and quantifiers in environment-specific results. BERT and BERT\u2192MNLI show comparable performance in various settings and tasks, outperforming BERT\u2192CCG in scope detection. BERT represents NPI licensing features well, with near-perfect performance in gradient minimal pairs task. However, results vary across evaluation methods, showing a nuanced understanding of NPI expressions. BERT's knowledge of NPIs and NPI licensors is stronger than its knowledge of licensors' scope, as seen in the probing results. BERT performs well in licensor-detection but struggles with scope-detection. The BoW baseline also excels in licensor-detection, suggesting a common property among NPI-licensors. Identifying if an NPI is in the scope of a licensor requires word order information, where BERT's performance varies in different evaluation tasks. BERT's knowledge of NPIs and NPI licensors is stronger than its knowledge of licensors' scope, as seen in the probing results. The gradient task is easier than the absolute task, revealing different aspects of BERT's knowledge. While BERT shows systematic knowledge of acceptability contrasts, it varies across environments and is not categorical. Human performance in natural language understanding may also not be categorical, as shown in an MTurk study on acceptability judgments. The generated dataset does not align with our judgments. Additional pretraining on CCG and MNLI does not enhance BERT's performance and may even decrease it. While intermediate pretraining was thought to be beneficial, our data does not support this. This contrasts with BID34's findings that syntactic pretraining improves NPI domain performance. Differences in models and training procedures likely explain these discrepancies. Future studies should use diverse methodologies to assess model performance within specific domains. Evaluation of sentence encoders using different tasks can reveal different aspects of the encoder's knowledge within a specific domain of English grammar. The reduced paradigm for Simple questions is shown in Table 3, highlighting the licensor, licensor replacement, NPI, and NPI replacement. Minimal pairs demonstrate the shift from unacceptable to acceptable sentences. The results from MTurk validation show the percentage of acceptable sentences for different conditions. The 'Diff' score indicates the difference between acceptable and unacceptable ratings. The results from MTurk validation show the percentage of acceptable sentences for different conditions, with 'Diff' score indicating the difference between acceptable and unacceptable ratings. GloVe BoW (Gradient Preference) CoLA All NPI All-but-1 NPI Avg Other NPI 1 NPI Trained on achieved high scores across various metrics. The results from MTurk validation show high scores for different conditions, with a 'Diff' score indicating the difference between acceptable and unacceptable ratings. The MTurk validation results show high scores for various conditions, with a 'Diff' score indicating the difference between acceptable and unacceptable ratings. The scores range from 0.52 to 1.00."
}