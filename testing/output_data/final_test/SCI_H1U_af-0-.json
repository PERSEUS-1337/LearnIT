{
    "title": "H1U_af-0-",
    "content": "We propose improving kernel approximation using more efficient numerical integration techniques to enhance approximation and facilitate fast computations. Our approach allows for better estimates of integrals compared to current methods, supported by convergence behavior and empirical studies. Kernel methods are efficient in real-world problems, utilizing the kernel trick to compute inner products in high-dimensional feature spaces. The feature map transforms input space X into feature space F, but kernel methods have high complexity for large datasets. A technique introduces a low-dimensional randomized approximation to reduce complexity, allowing the use of standard linear methods. This approach improves kernel approximation using efficient numerical integration techniques for faster computations. Recent research aims to improve the convergence of kernel approximation for faster computations by considering kernels with integral representations. These kernels cover shift-invariant and Pointwise Nonlinear Gaussian kernels, widely used in practice with connections to neural networks. The main challenge in constructing low-dimensional feature maps is approximating the expectation in (3) with a d-dimensional integral using Gaussian weight. While standard Monte Carlo rule is easy to implement, better quadrature rules exist for such integrals. For example, BID22 use quasi-Monte Carlo rules for better quality kernel matrix approximations. Unlike other studies, we propose using specific quadrature rules to improve kernel approximation accuracy and provide an analytical error estimate. Additionally, for kernels with specific integrands, properties can be improved. In constructing low-dimensional feature maps, the challenge lies in approximating the expectation with a d-dimensional integral using Gaussian weight. Specific quadrature rules can improve kernel approximation accuracy and provide an analytical error estimate. For kernels with even function f xy (w), a reduced quadrature rule can be derived, resulting in a smaller embedded dimension D with the same accuracy. Structured orthogonal matrices are used to speed up the approximation of the kernel function and reduce memory requirements. Empirical results demonstrate better quality of approximation and classification/regression outcomes with different kernels. Integration can be performed using quadrature rules, which involve interpolating functions for easy integration. Sampling points from the domain and calculating the rule values at these points allows for approximation of the integral. The sampled quadrature rules developed by BID12 provide unbiased estimates of I(f xy ). A change of coordinates is necessary for stochastic spherical-radial rules, where a combination of radial R and spherical S rules is used. Stochastic radial rule R(h) of degree 2l + 1 involves weighted symmetric sums for integrands in infinite range integrals. The text discusses the use of weighted symmetric sums in stochastic spherical rules for integration. Points are sampled from specific distributions to obtain unbiased estimates for integrals, with weights derived to ensure exactness for certain integrands. Higher degrees require sampling from more complex distributions, making the process challenging. Spherical rule S(s) approximates integrals over unit d-spheres, with weights and sums calculated based on specific formulas. The text discusses the use of stochastic spherical-radial rules for integration, where randomization is achieved through orthogonal matrices. The aim is to generate unbiased estimates for spherical surface integrals by combining various designs of such matrices. The stochastic spherical-radial rule of degree three is applied to approximate integrals by averaging samples. The new feature dimension D in quadrature based approximation is 2n(d + 1) where n is the number of sampled SR rules. Even a small number of rule samples n provide accurate approximations. Better versions of SR rules can be derived for specific functions. Variance estimation for stochastic spherical-radial rules applied to kernel functions is contributed. The quadrature rule (6) provides an unbiased estimate of the integral of any integrable function f, under certain conditions. The error of approximation is proportional to the scale of inputs, with constants M1 and M2 bounding the derivatives of function g. The new feature dimension D is 2n(d + 1) in quadrature-based approximation, where n is the number of sampled SR rules. Variance estimation for stochastic spherical-radial rules applied to kernel functions is also discussed. The error of approximation in the quadrature rule is proportional to the scale of inputs, with wider RBF kernels being approximated better than narrow ones. The rate of convergence is O(1/nd). An experiment was conducted with different input dimensions to show the relative error of approximation of the kernel matrix depending on the scaling factor. The experiment was conducted with input dimensions d = 10, 100, 500 using the quadrature rule. Random orthogonal matrix Q is discussed for fast matrix by vector multiplication in the SR 3,3 rule implementation. Haar distribution on orthogonal group O(d) ensures unbiased estimates for integrals over unit sphere using spherical rules S 3 Qi (s). Sampling techniques for such matrices vary in complexity. The complexity of generating random orthogonal matrices varies, with some methods having cubic complexity in d, while others have quadratic complexity but result in unstructured matrices. Butterfly matrices are proposed as a solution, offering low computational complexity and Haar distributed orthogonal matrices. The method of generating Haar distributed random orthogonal matrices using butterfly structured factors offers fast multiplication and low computational complexity. The butterfly orthogonal matrix B(d) can be defined recursively and is efficient even for cases when d is not a power of two. The randomization is based on sampling angles \u03b8, and the method is denoted as B in experiments, demonstrating its application to quadrature rules and popular kernels like Gaussian radial basis functions. The Gaussian kernel is a popular radial basis function used in kernel methods. Arc-cosine kernels, introduced in connection with deep learning and kernel methods, have a closed form expression based on the angle between vectors x and y. The text discusses the integral representation of arc-cosine kernels of order 0 and 1, with explicit mappings for different kernels like Gaussian. It also mentions studying the method on benchmark datasets like Powerplant, MNIST, and CIFAR100, evaluating kernel approximation error and SVM model quality. The comparison of SVM models with approximate kernels is presented, including methods like simple Monte Carlo and quasi-Monte Carlo. Different ways of generating samples are discussed, along with the use of stochastic spherical-radial rules and orthogonal matrices. See Appendix D for more details. The comparison of SVM models with approximate kernels includes methods like simple Monte Carlo and quasi-Monte Carlo. Experiments were conducted for kernel approximation using orthogonal matrices and random subsets of datasets. Results for kernel approximation error on various datasets are shown in FIG1. The comparison of SVM models with approximate kernels includes methods like simple Monte Carlo and quasi-Monte Carlo. Experiments were conducted for kernel approximation using orthogonal matrices and random subsets of datasets. Results for kernel approximation error on various datasets are shown in FIG1. Three kernels (arc-cosine 0, arccosine 1, Gaussian) on three datasets (Powerplant, LETTER, USPS) are analyzed. The proposed methods (B, H) show better results than the baselines in terms of accuracy and R2 scores for classification and regression tasks. The study compares SVM models with approximate kernels using various methods for kernel approximation error. The results show that their method delivers comparable and often the best quality for final tasks, with better performance using fewer features in some cases. Popular methods for scaling up kernel methods involve low-rank approximation of the kernel with data-dependent or independent basis functions. The construction of basis functions in kernel approximation techniques utilizes the training set, making them more attractive for certain problems compared to Random Fourier Features approach. Data-dependent approaches generally outperform data-independent approaches when there is a gap in the eigen-spectrum of the kernel matrix. Most data-independent techniques, including Random Fourier Features, approximate the kernel function directly using a weight matrix that can be generated in various ways. The authors propose methods to improve the quality of approximation using structured matrices and orthogonal weight matrices. They introduce special matrices for fast computations and provide analytical estimates for mean squared error. Quasi-Monte Carlo sampling is suggested for better convergence in Random Fourier Features approximation. The authors aim to enhance the quality of Random Fourier Features approximation by optimizing sequences based on a given dataset. They propose advanced integration rules to achieve higher quality kernel approximation, with a focus on optimizing weights for quadrature points to improve performance. The authors optimized Random Fourier Features approximation by improving kernel approximation quality using advanced integration rules and optimizing weights for quadrature points. They explored the variance of error in relation to data scale and kernel width, speculating on the impact of approximation quality on accuracy. Butterfly orthogonal matrices were used to speed up computations, with a complexity of O(d log d). Despite claims of producing uniformly random orthogonal matrices, not all matrices were found to be so. Comparison between methods using properly distributed orthogonal matrices and those that sometimes fail to do so did not show any differences. The experimental study confirmed that the proposed approach delivers better kernel approximation on most datasets, leading to higher quality in final task performance compared to state-of-the-art baselines. The choice of degree for the SR rule was discussed, with the use of degree three SR 3,3 in experiments. The function f xy in quadrature rule FORMULA12 was considered as a function of two variables. The function f xy in quadrature rule FORMULA12 is expressed as \u03c6(w x)\u03c6(w y) = g(z 1 , z 2 ), where z 1 = w x, z 2 = w y. Random variables w i = Qv i are uniformly distributed on a unit-sphere. A 4-th order Taylor expansion of function g is derived around 0, with odd terms canceling out. The random orthogonal matrix Q is uniformly distributed on a set of orthogonal matrices O(n), leading to uniformity of vector w j on a unit n-sphere. The variance of the estimate is calculated using the distribution of random variable \u03c1, which is \u03c7(d + 2) for d > 2.1. The variance of A i is estimated along with the covariance Cov(A i , B i ). Symmetry is used to simplify calculations, resulting in terms equal to 0. The text discusses deriving inequalities and correcting bias in integral estimates by using randomization with permutation matrices. To achieve a uniformly random orthogonal butterfly matrix, a sequence of angles is crucial, and a specific algorithm is followed for distribution. The BID8 algorithm computes a random point u from U d, calculates angles by taking ratios of u coordinates, and computes cosines and sines of the angles. The text discusses datasets not in the main body, displays experiment settings in TAB2, and shows results for kernel approximations. For QMC, the weight matrix M is generated from quasi-random sequences. The best estimators are for arc-cosine kernels, while Gaussian kernel error varies by dataset. The study compares different kernel approximations on various datasets, showing the superiority of their approach in most cases. The Powerplant dataset performs best with Halton and Orthogonal Random Features, while ROM's performance stagnates due to the small input feature space. The comparison includes the subsampled dense grid method from BID6, implemented based on the paper's code. Performance on the LETTER dataset is illustrated in FIG6, showing similar results between the method (GQ) and the baseline RFF (G). The proposed method (denoted GQ) shows similar performance to the baseline RFF method on various datasets, with nearly matching relative error of kernel approximation. The structured explicit mapping of the proposed method allows for favorable scalability in higher dimensional data."
}