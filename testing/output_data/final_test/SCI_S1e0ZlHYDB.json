{
    "title": "S1e0ZlHYDB",
    "content": "Deep learning training faces challenges with data retrieval speed over networks and storage. Progressive Compressed Records (PCRs) reduce overhead by splitting training examples into higher fidelity versions without increasing data size. Grouping similar fidelity examples reduces system overhead and data bandwidth. PCRs enable faster training with high accuracy, showing a 2x speedup over baseline formats using JPEG compression across various datasets and deep learning architectures. Distributed deep learning optimizes training time by utilizing parallelism with three key components: data pipeline, forward/backward computation, and variable synchronization. While much research has focused on scaling deep learning from a compute or network perspective, little attention has been given to scaling the storage layer. Hardware trends indicate a growing gap between compute and storage bandwidth, impacting data transportation for machine learning. The transportation of data for machine learning in modern data centers is crucial, with a growing gap between compute and storage bandwidth. The storage pipeline is a key area for improving training times, and a novel on-disk format called Progressive Compressed Records (PCRs) is proposed to reduce bandwidth costs for training over massive datasets. Progressive Compressed Records (PCRs) reduce bandwidth costs for training over massive datasets by utilizing a compression technique that decomposes data into deltas to increase fidelity. Applications can control dataset size and fidelity trade-offs, resulting in a 2\u00d7 reduction in bandwidth and training time without affecting model accuracy. Neural network training is robust to data compression, but the tolerance level varies across learning tasks. Progressive Compressed Records (PCRs) offer a novel on-disk format for training data, enabling dynamic fidelity selection to reduce data bandwidth. By using PCRs, training speed can improve by 2\u00d7 on average over standard formats with JPEG compression, without significantly impacting model performance. The layout and representation of data on storage media are crucial for maximizing bandwidth utilization. Data representation, such as compression, can reduce data transfer requirements, but may affect image quality. Different storage media have similar considerations for space, bandwidth, and locality. Progressive Compressed Records (PCRs) offer a new on-disk format for training data, allowing dynamic fidelity selection to enhance training speed without compromising model performance. Progressive Compressed Records (PCRs) maintain sequential behavior of record formats at multiple fidelities without space overheads. Record layouts like TFRecords and ImageRecord batch data points together to increase access locality, reducing delays in access time. Serialization is key to record layouts, impacting performance properties when written to disk. Image compression is commonly used to represent data in compressed forms. Image compression, such as JPEG, is widely used for training data representation. Most compression formats only allow setting the compression level at encoding time, leading to potential issues of over-compression or under-compression. This can impact application convergence quality and storage system performance. Additionally, deep learning pipelines may involve post-processing steps that further distort images and affect the relationship between image fidelity and model accuracy. Dynamic compression in image encoding can help avoid bandwidth expenses by terminating decompression once sufficient fidelity is achieved. The JPEG algorithm splits an image into 8x8 blocks, with low frequencies storing the most relevant content. The JPEG algorithm splits an image into 8x8 blocks, prioritizing the most relevant content. Quantization discards information from the block for compression, resulting in a serialized format. Decoding the data involves inverting the process. Progressive compression writes out important coefficients before revisiting the block, with diminishing fidelity returns in higher scans. Progressive image compression allows data to be read at varying degrees of compression without duplication. It ensures all blocks receive some information before revising them, improving fidelity. Progressive formats serialize the image matrix in disjoint groups of deltas called scans, ordered by importance. Progressive Compressed Records (PCRs) are introduced as a novel record format for machine learning training. They combine layout and data representation to ensure efficient hardware utilization and reduce the total amount of work required. Scan groups are introduced to leverage layout and progressive compression for dynamic compression, allowing high performance reads while reducing the amount of data read. Progressive Compressed Records (PCRs) break images into deltas for efficient data access. PCRs are easy to implement, lossless, and offer fast processing. They result in significant speedups and can be applied beyond images and JPEG. Scan groups combine layout and compression for high hardware efficiency in reading subsets of data. Progressive Compressed Records (PCRs) use scan groups to rearrange data for efficient decoding. Scan groups are sorted by importance and consist of scans, with metadata at the beginning. This layout enables sequential reading of data deltas, creating dynamicity in the decoding process. The encoding process of Progressive Compressed Records (PCRs) transforms JPEG files into a directory, allowing users to read data at a certain scan fidelity by reading the on-disk byte stream from the start of the PCR to the corresponding scan group offset. This process enables bandwidth savings without re-encoding the data. The encoding process transforms JPEG files into a directory containing PCR metadata and .pcr files. The decoding process efficiently reverses a subset of the encoding. The dataset is split into PCRs, with the training process reading multiple .pcr files per epoch. The data loader interfaces the PCR decoding library with deep learning libraries. The PCR encoder groups scans by fidelity, serializes them with offsets, and writes the metadata to disk. JPEG images are decomposed into progressive JPEG using JPEGTRAN. Each JPEG is divided into 10 scans, with markers indicating the end of a scan group. The encoder can access all 10 offsets within the JPEG files for decoding. The encoder groups scan regions by fidelity, using Protobuf to serialize the groups and labels. Each group is serialized as a separate message to maintain order. The contents are concatenated and written out as one file, reducing the need for multiple record format conversions. Decoding a PCR file involves looking up scan group offsets in the database and using them to read only the necessary bytes for JPEG decoding. The process includes inverting the scan-group grouping and terminating the byte stream with an EOI JPEG token. The inverse conversion is achieved with 150 lines of C++ code. PCR loaders are implemented using PyTorch's dataloader and DALI's ExternalSource operator to return image batches with labels at different fidelity levels. Our implementation of PCR loaders simplifies loader design by efficiently fetching record formats off disk, achieving competitive image rates with minimal engineering effort. The evaluation focuses on large-scale image datasets, highlighting the efficiency of PCRs in handling taxing network and storage systems. Our evaluation focuses on datasets with high-resolution images, showing that reducing data bandwidth per image results in comparable accuracy with half the training time. We analyze the relationship between image fidelity and time-to-accuracy, tracing training time speedups to data loading times. The datasets used include ImageNet, HAM10000, Stanford Cars, and CelebA-HQ. The dataset evaluation includes ImageNet, HAM10000, and Stanford Cars, focusing on high-resolution images and reducing data bandwidth for faster training times. ImageNet is fine-grained with 100 classes, HAM10000 is split 80%/20% for train/test, and Stanford Cars has 196 classes. The datasets are converted into PCRs for analysis. The CelebAHQ-Smile dataset consists of high-resolution celebrity faces with annotations for smiling or not smiling. The dataset is split into 80%/20% train/test sets and converted into 93 PCRs. Pretrained ImageNet weights are used for HAM10000 and Cars due to limited training data, with standard training procedures applied. After augmentations, inputs are resized to 224 \u00d7 224. Pretrained experiments start at a learning rate of 0.01. ResNet18 and ShuffleNetv2 architectures are used with a batch size of 128 per worker. Experiments are run at least 3 times for confidence intervals. PyTorch with NVIDIA Apex and NVIDIA DALI are utilized for training. The study utilizes PyTorch with NVIDIA Apex and NVIDIA DALI for training in a distributed setting. Results for ResNet18 and ShuffleNetv2 training are shown in Figures 4 and 6 respectively, with varying epochs for different datasets. Test accuracy is sampled every 15 epochs for non-ImageNet datasets to reduce interference with training measurements. The evaluation focuses on the impact of reading different amounts of scan groups on training measurements. Lower scan groups provide a speedup in time-to-accuracy due to smaller sizes, leading to potential bandwidth savings. However, there is a trade-off between convergence quality and speedup when using less storage resources. The evaluation shows that lower fidelity scan groups offer a speedup in training time and potential bandwidth savings. However, there is a trade-off between model convergence and efficiency. Scan group 1 performs poorly on tasks like Cars due to lack of fine-grained details. Scan group 5 is a good default option for most datasets, costing half the bandwidth while achieving the same test accuracy as the baseline. The study compares different scan groups based on image fidelity using MSSIM as a measure of similarity. Results show a strong correlation between MSSIM and final test accuracy, with similar MSSIM groups performing similarly. Groups tend to cluster together, with group 5 introducing a difference from others. The study found that MSSIM can predict relative test accuracy within tasks and can be used to choose scans. Data loading can slow down training, and using all scan groups can result in high loading stalls. The study presents loading time data for ResNet18 ImageNet-100, showing improvements with PCRs by not using all scan groups. Training with massive parallelism stresses system bandwidth, causing stalls from pre-fetching data and servicing multiple minibatches. Using fewer scan groups reduces data read, lowering stalls observed with DALI and PyTorch loaders. Efficient data pipelines are crucial for training models over ImageNet in record time, with recent interest in training at massive scale with thousands of GPUs. Our work focuses on reducing storage bottlenecks for training models at massive scale with thousands of GPUs. We aim to improve data representation independently of the storage system, while techniques like custom Protobuf parsers are used to speed up simple models in production systems. Data reduction techniques like sketching, coresets, clustering, and sampling are used to reduce training set size. Another approach is to reduce the active training set size to lower bandwidth requirements. Compression methods are also commonly used to reduce data size in computer systems, including compressing neural network models to avoid costly storage. Our work focuses on compressing data for training ML models to reduce bandwidth requirements, utilizing compression techniques commonly found in models. This approach differs from compressing neural network models themselves. Prior work has explored optimizing training systems by compressing network traffic, not limited to machine learning, but also seen in databases, computer memories, and web systems. Our work introduces Progressive Compressed Records (PCRs) as a novel record format for efficiently compressing data during training, focusing on the effects of compression on training rather than model computation. This approach aims to optimize storage and networking bandwidth utilization for larger datasets in machine learning. Progressive Compressed Records (PCRs) is a novel record format that balances data fidelity with storage and network demands, allowing for training models with 2\u00d7 less storage bandwidth while maintaining accuracy. PCRs use progressive compression to split training examples into higher fidelity versions without duplication, making them easy to implement and applicable to various tasks. The format is demonstrated with images using JPEG compression, but can be extended to other data modalities and compression techniques. Future work will explore applications beyond visual classification, such as audio generation or video segmentation. Additional experiment plots for ResNet-18 and ShuffleNetv2 are provided in Figures 9 and 10, showing loss over time. Figure 9 and Figure 10 display loss trends over time for ResNet-18 and ShuffleNetv2 experiments. Figure 11 shows scan sizes for all datasets, with Top-5 accuracies reflecting Top-1 trends. Compression does not consistently improve accuracy, with gains in time-to-accuracy mainly from faster image rates. Scan sizes vary, with certain scans adding more data due to different techniques. The text discusses the impact of using multiple scans on bandwidth and decoding speed. It mentions that using all 10 scans can significantly increase bandwidth and that optimizations like multi-core parallelism can improve decoding rates. The text also suggests that reducing computational expense by optimizing implementation or using fewer scans is a potential area for future work. Additionally, it highlights the possibility of offloading some decoding tasks to accelerators like GPUs. Image loading rates during training are provided in Table 2, showing that using more scans slows down training significantly. ShuffleNetv2 has a higher maximum training rate than ResNet-18. The ImageNet-100 dataset was created by subsampling 100 classes from the ImageNet ILSVRC dataset. The CelebAHQ dataset was also mentioned. The CelebAHQ dataset is a high-quality version of CelebA, containing attributes like smiling faces. CelebAHQ-Smile dataset has 30k faces with binary smiling variable. Image quality details are provided using JPEG quality setting. Higher quality JPEG images allow for more aggressive PCR compression rates, especially for ImageNet and HAM10000 datasets. CelebAHQ has lower quality images downscaled to 256 \u00d7 256 for training, increasing information density. Cars dataset is not high quality or large resolution. Bandwidth-optimized record baselines re-encode images with 50% and 90% JPEG quality to reduce dataset size while maintaining fidelity. The baseline method and PCR method are compared for dataset bandwidth reduction. Re-encoding images with lower JPEG quality can result in decreased fidelity. PCRs offer a progressive format without the need for multiple fidelity levels, saving storage space and encoding time. The encoding time for both methods scales with dataset size, with PCRs being competitive in terms of time efficiency. Converting ImageNet into record format takes significantly longer than the baseline method. Converting the full ImageNet into record format takes significantly longer than the baseline method, with the PCR conversion taking 96 minutes. System caches are less likely to see a hit due to the larger working set size. Compression needs vary within the same dataset for different tasks, as shown in experiments validating accuracy and loss. Lower scan groups can be used for easier tasks. Compared to the original task, coarser tasks reduce the gap between scan groups, with the binary task closing the gap even more. Re-assigning class labels to a coarser class simplifies the task and improves accuracy across scan groups. Fixed PCR record encoding supports multiple tasks efficiently, while static approaches may require one encoding per task."
}