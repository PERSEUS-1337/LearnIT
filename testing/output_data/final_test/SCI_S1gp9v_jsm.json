{
    "title": "S1gp9v_jsm",
    "content": "Sequence-to-sequence attention-based models are a promising approach for end-to-end speech recognition. Various analyses were conducted to better understand training and model properties, including investigating pretraining variants and studying the attention process. Experiments were performed on different datasets, showing over 8% relative improvement in word error rate. The encoder-decoder framework with attention has been successfully applied to automatic speech recognition and is a promising end-to-end approach. The encoder-decoder model in speech recognition does not explicitly model alignment like the hybrid HMM/NN approach. Instead, it uses an attention process for implicit soft alignment, making it harder to enforce constraints such as monotonicity. The attention process in the encoder-decoder model makes it difficult to enforce constraints like monotonicity and interpret the attention weights as soft alignment. The model can shift and reorder evidence, leading to challenges in studying temporal alignment behavior. Other end-to-end models like connectionist temporal classification have been used in ASR. In the context of encoder-decoder models, various variants like the recurrent transducer and recurrent neural aligner have been developed. Some models use hard attention, where a hard decision is made instead of soft attention. Recurrent NN encoder-decoder models, including LSTM units, are studied, along with the transformer model that uses feed-forward and self-attention layers. Positional encoding is added to the transformer model to incorporate temporal information. In the study of attention models, the focus is on understanding when, why, and how they fail, analyzing search errors, relative error positions, implicit alignment behavior, attention weights, and encoder output representation. Previous works have analyzed neuron activations in RNN language models and correlations in DeepSpeech 2 CTC end-to-end systems. The curr_chunk discusses various studies analyzing the encoder state, attention weights, alignment behavior, and effects of different factors on speech recognition systems. Saliency methods are used for interpreting model decisions, and the RETURNN framework is utilized for neural network training. The focus is on understanding the inner workings of the system, comparing LSTM and GRU systems, and using attention models. The curr_chunk discusses the use of custom CUDA kernels in BID54, RETURNN for decoding attention models, and single GPU experiments. RASR BID59 is used for feature extraction and decoding in some cases. The Switchboard corpus BID19 is utilized with a 300h train dataset for training and cross-validation. The hybrid HMM/NN model employs a deep bidirectional LSTM with 6 layers and 500 nodes in each direction. WER is reported on Hub5'00 and Hub5'01, with the best model selected from Hub5'00. The attention model in the curr_chunk uses dropout, gradient noise, Nadam optimizer, Newbob learning rate scheduling, and focal loss. Byte pair encoding is used as subword units with a vocabulary of 10k BPE units. The LibriSpeech dataset consists of about 1000h of speech, with a subset used for cross-validation and model selection. The end-to-end attention model also employs BPE units and is compared to other models in the literature. The attention model in the curr_chunk is based on an end-to-end approach using BPE subword units with a vocabulary size of about 1000. The model's performance is evaluated on the WSJ dataset, with training, validation, and evaluation splits. Preliminary results are presented, showing minimal search errors, indicating potential issues within the model itself. The results in Fig. 2 show that a low beam size may be sufficient for the model. Pretraining is crucial for good performance and model convergence. Comparison between pretraining and non-pretraining cases is provided for attention-based models. The pretraining variant of the Switchboard baseline involves gradually increasing the number of layers. In experiments on Switchboard, varying the number of encoder layers and LSTM units was conducted, with and without pretraining. The best model was found to be with 4 layers without pretraining, showing good results starting directly with 4 layers and time reduction 8. Surprisingly, starting directly with 6 layers with a reduced learning rate also yielded positive results, which was not possible in earlier experiments. This improvement may be attributed to a reduced and enhanced BPE. Starting with 6 layers and pretraining showed better stability and results compared to no pretraining. Pretraining allows for training deeper models, but excessive pretraining can be detrimental. Beginning with a deeper network improves performance, but starting too deep may not work well. Directly starting with time reduction 8 also enhances final performance. Starting with 6 layers and pretraining showed better stability and results compared to no pretraining. Directly starting with 4 layers and time reduction 8 further improves performance, but may make training slightly more unstable. The optimal number of repetitions for each pretrain step was found to be 5. Lower time reduction factors performed better, with the lowest being 8 in experiments. Using a pool size of 3 in the first time max pooling layer resulted in a better-performing model with a time reduction factor of 6. Keeping the top layer during pretraining was found to be beneficial for maintaining consistent time reduction factors. After experimenting with different time reduction factors and layer configurations, it was found that adding layers on top with a simpler scheme performed better. Additionally, increasing the encoder width during pretraining also improved performance. The final number of LSTM units in each direction of the bidirectional deep LSTM encoder was 1024, with a gradual increase from an initial 512 units. This width growing scheme, along with adjusting the dropout rate accordingly, led to improved results. Our current best model shows that pretraining is more stable for deep models, with the pretraining scheme being crucial. Less pretraining can enhance performance but may lead to instability. Improved pretraining and time reduction techniques were applied to WSJ and LibriSpeech datasets, resulting in similar enhancements. Training attention models requires careful tuning of initial learning rate, warm-up, and pretraining to address high training variance. TensorFlow operations may introduce non-deterministic behavior, impacting final performance. The training variance in TensorFlow operations such as tf.reduce_sum is influenced by random seeds and other hyperparameters. Comparisons with hybrid HMM/LSTM models show lower variance. Pretraining is crucial for stability in deep models, with less pretraining potentially leading to instability. The encoder in the model creates a high-level representation of the input, providing information for the decoder to attend to. By analyzing the encoder output and attention weights, important functions like silence detection can be identified. Dimensionality reduction techniques like PCA help in identifying distinct information such as silence detection and encoder time position. The encoder in the model creates a high-level representation of the input, providing information for the decoder to attend to. By analyzing the encoder output and attention weights, important functions like silence detection can be identified. Dimensionality reduction techniques like PCA help in identifying distinct information such as silence detection and encoder time position. We further identify individual cells in the LSTM encoding positional information through neuron activations. Attention weights are observed to be very local in the encoder frames, often focusing on a single frame. The information about the label needs to be well-localized in the encoder output, as shown in experiments restricting attention to a fixed-size window around the arg max of attention energies. The results in TAB9 confirm the hypothesis that information is localized in the encoder. Global attention helps the decoder gather information from multiple frames at once, with attention weight sometimes on the first and/or last frame. Improved pretraining scheme led to over 8% relative improvement in Switchboard baseline WER. High training variance of attention models compared to hybrid HMM/NN models was noted. Encoder output analysis revealed representation of relative input position visible in PCA. The encoder output analysis showed the representation of relative input position, visible in PCA reduction, and individual neurons marking frames that can be skipped by the decoder, correlating to silence."
}