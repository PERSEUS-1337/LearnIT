{
    "title": "Byg3y3C9Km",
    "content": "The Boltzmann distribution is a natural model for various systems, but Monte Carlo algorithms struggle to simulate it efficiently. This challenge is evident in the protein folding problem, where energy landscapes play a crucial role. To address this gap, a neural energy function combined with a novel simulator based on Langevin dynamics is used to model atomic protein structure from amino acid sequence information. The Boltzmann distribution is a foundational model for understanding how macroscopic structures arise from simple interactions in natural systems like cells and proteins. Techniques for stabilizing backpropagation under long roll-outs have been introduced to make multimodal predictions and generalize to unobserved protein fold types when trained on a large corpus of protein structures. Energy-based models with simple interactions can encode complex system behaviors, but learning model parameters and generating samples from the Boltzmann distribution is challenging. This has led to a shift towards easier-to-learn generative models like directed latent variable models and autoregressive models. The protein folding problem exemplifies the power and difficulty of energy-based models in describing complex relationships in data. NEMO is a model for protein structure that combines a neural energy function, a stochastic simulator, and an atomic imputation network. It is trained end-to-end by backpropagating through the unrolled folding simulation to generate low-energy protein structures. Contemporary simulations aim to create low-energy structures efficiently. A potential solution involves training an unrolled simulator of an energy function directly. This approach eliminates the need to determine when the simulator has converged and focuses on producing useful results within a set timeframe. NEMO is a neural energy simulator model for protein structure that can generate 3D protein structures from sequence information. It combines a deep energy function with unrolled Langevin dynamics. The model for protein structure includes a deep energy function, unrolled Langevin dynamics, and an atomic imputation network for an end-to-end differentiable model. It also features an efficient sampling algorithm based on a transform integrator and stabilization techniques for long roll-outs of simulators. The systematic analysis of combinatorial generalization is done with a new dataset of protein sequence and structure. The Markov Random Field model utilizes structure-based features and sequence-based weights computed by neural networks for sampling low-energy configurations efficiently. The model incorporates rotationally and translationally invariant internal coordinates, pairwise distances, and orientations as base features. Related work includes Structured Prediction Energy Networks with unrolled optimization and approaches to learn energy functions and samplers simultaneously. NEMO is an end-to-end differentiable model of protein structure conditioned on sequence information, consisting of a neural energy function, an unrolled simulator, and an imputation network. Proteins are linear polymers of amino acids that fold into defined 3D structures. Proteins are linear polymers of amino acids that fold into defined 3D structures based on interactions between side-chains, backbone, and solvent. Protein structure X is predicted in terms of 5 positions per amino acid, including the backbone atoms and the center of mass of the side chain R group. The differentiable simulator generates an initial coarse-grained structure with the loss function targeted to the midpoint of the C \u21b5 carbon and the side chain center of mass. Two modes for conditioning the model on sequence information are considered: 1-seq and Profile. Internal coordinates parameterize structure in terms of relative distances and angles. The internal coordinates z parameterize structure using distances and angles between points in a spherical coordinate system. The transformation from internal coordinates to Cartesian is defined by a unit vector normal to each bond plane. The inverse transformation involves local calculations of distances and angles. Deep Markov Random Field models the distribution of a structure x conditioned on a sequence s using the Boltzmann distribution. Internal coordinates z, except 6, are invariant to rotation and translation. Distances between points are processed using radial basis functions with Gaussian kernels. Orientation vectors encode relative positions in a local coordinate system. Langevin dynamics samples from the Boltzmann distribution, typically simulated by a first-order discretization. The efficiency of Langevin dynamics in exploring conformational space depends on the energy landscape geometry. Cartesian dynamics are good for local rearrangements, while internal coordinate dynamics sample global changes more efficiently. A combination of Cartesian and internal coordinate dynamics is used in simulations, with each time step consisting of one Cartesian step followed by one internal coordinate step. Simulating internal coordinate dynamics is computationally intensive due to the sequential process of rebuilding Cartesian geometry from internal coordinates. The imputation network bypasses recomputing coordinate transformations by integrating the Jacobian on-the-fly. Atomic coordinates are placed in a local reference frame using a 1D convolutional neural network. The model is trained on a dataset of 67,000 protein structures and evaluated hierarchically and temporally. The model for protein structure prediction involves hierarchically and temporally split domains. It is trained using gradient descent with a composite loss combining likelihood-based and empirical-risk minimization terms. A transform integrator simulates Langevin dynamics in a more favorable coordinate system, exchanging inner-loop transformation steps for extra Jacobian evaluations. Structural stratification in prediction ranges from small sequence differences to predicting 3D fold topologies not in the training set. The dataset for protein structure prediction involves hierarchically split protein domains from the CATH classification. The dataset is divided into training, validation, and test sets, with the test set further subdivided based on similarity to the training set. The model aims to predict 3D fold topologies not present in the training data. The gradient of the data-averaged log likelihood of the Boltzmann distribution is minimized to reduce the average energy of samples from the data compared to the model. A Monte Carlo estimator is used to calculate this gradient in an automatic differentiation setting. In addition to the likelihood loss, an empirical risk loss is developed to measure protein model quality, with no weighting between the two losses. To stabilize training in machine learning, two techniques were developed to regularize against chaotic dynamics in simulators: Lyapunov regularization and Damped backpropagation through time. Lyapunov regularization focuses on the simulator time-step function to prevent chaotic dynamics, while Damped backpropagation through time decays gradient accumulation to stabilize learning. In machine learning, techniques like Lyapunov regularization and Damped backpropagation through time are used to stabilize training. Damped backpropagation involves multiplying each backwards iteration by a damping factor to cancel out exploding gradients. It is a continuous relaxation and alternative to truncated backpropagation through time. The text also discusses examples of fold generalization at the topology and architecture level, showing a range of prediction accuracy for structural generalization tasks. The text discusses the evaluation of model performance based on structural similarity using the TM-Score BID27 measure. It highlights that confident models are accurate, even with low sequence identity and high generalization difficulty. Successful predictions are illustrated at different levels, showcasing the model's ability to predict 3D structures. Figure 5 shows successful predictions at C and A levels, including 4ykaC00, 5c3uA02, and beta sheet formation in 2oy8A03. The predictive distribution is multimodal with differences between clusters representing alternate chain packing. Some models exhibit uneven uncertainty distribution along the chain, corresponding to loosely packed regions. A baseline model was constructed using a two-layer bidirectional LSTM, showing that NEMO generalized more effectively than RNNs for difficult tasks. The study compares the performance of NEMO and RNNs in protein structure prediction. NEMO shows better generalization and incorporates evolutionary information for improved prediction quality. The approach combines simulators' bias with directed models' speed, resulting in faster model sampling times. However, it has higher computational costs compared to angle-predicting RNNs. The study introduces a model for protein structure prediction that combines a neural energy function with an unrolled simulation. Challenges include backpropagation instability and slow learning, addressed with Lipschitz regularization and gradient damping. Efficient N-body simulations and network stabilization methods are suggested for future improvements. The study introduces an efficient simulator for Langevin dynamics in transformed coordinate systems to predict protein structures with hundreds of atoms and generalize to distant fold classifications effectively. The model combines a neural energy function with an unrolled simulation, addressing challenges with Lipschitz regularization and gradient damping. The composite loss function is optimized via backpropagation using atomic coordinates, simulator trajectory, and secondary structure predictions. The inverse transformation involves local computations of bond lengths and angles, with the Jacobian defining the response of Cartesian coordinates to perturbations of internal coordinates. This is crucial for converting forces into torques and bond forces, as well as for the transform integrator development. The model utilizes unconstrained representations for bond lengths and angles, enforced by transformations using fully unconstrained variables. CNN primitives in the model follow a common structure of residual blocks. The energy function components are depicted in FIG2. The model schematic (FIG2) consists of stacks of residual blocks with channel mixing layers and Batch Renormalization. Sampling is preferred over optimization due to challenges in optimizing globally and considering entropy. Sampling is preferred over optimization for faster global exploration, uncertainty quantification, and training with an approximate Maximum Likelihood objective using Langevin dynamics, which simulate continuous-time stochastic dynamics. The dynamics design aims to converge quickly to an approximate sample in the canonical ensemble. Corrector step crucial for eliminating second-order errors in curvilinear motions caused by angle changes. Hybrid integrator formed to balance local and global structural rearrangements in the folding process. In the folding process, a hybrid integrator (Algorithm 3) is formed to balance local and global structural rearrangements. Translational and rotational detrending are used to isolate and remove components of motion, treating the system as a set of particles with unit mass. Effective structural translational and rotational velocities are computed to explain per time-step changes. In the folding process, a hybrid integrator is used to balance local and global structural rearrangements. Translational and rotational detrending are applied to isolate motion components. Speed clipping is enforced to stabilize the model and prevent extreme dynamics. This involves limiting the fastest motion to 2 Angstroms per iteration. The folding process utilizes a hybrid integrator to balance local and global structural rearrangements. Speed clipping is enforced to stabilize the model by limiting the fastest motion to 2 Angstroms per iteration. Modified predictor and corrector steps are computed with a potentially slower time step, breaking the asymptotics of Langevin dynamics. Regularizing the dynamics away from situations requiring clipping can avoid this issue. Non-Gaussian perturbations with kinetic energies similar to Relativistic Monte Carlo may offer a more principled approach. The final integrator combining these ideas is presented in Figure 3. Training and validation data sets were obtained from protein domains of specific lengths and categories in CATH release 4.1. The test set for the study included new folds from CATH release 4.2, totaling 10,381 test domains. The test set was further categorized into C, A, T, and H based on their nearest CATH classification in the training set. Validation statistics were monitored during training, but the validation set was not used to tune hyperparameters due to the high training cost. Models were optimized for 200,000 iterations. During training, validation statistics were tracked, and models were optimized for 200,000 iterations using Adam. The model was optimized with a composite loss including distance and angle losses, with details on contact-focused distance loss and angle loss functions. The angular losses were based on the inner product of feature vectors rather than Euclidean distance. The Euclidean loss has a cusp at z a while the Von-Mises Fisher loss is smooth around z a, similar to L1 and L2 losses. Trajectory loss involves an intermediate loss function for criticizing transient states. The TM-score measures superposition quality between protein structures. The text discusses optimizing superpositions of protein structures using iterative optimization and backpropagation. It also explains the process of determining hydrogen bonds within protein structures using a specific model. The text discusses predicting hydrogen bonds and secondary structure in protein structures using logistic regression and cross-entropy loss. The goal is to optimize the process through gradient descent, refining the atomic level model of protein structure from sequence. Challenges arise when computing gradients for unrolled simulations like Recurrent Neural Networks due to exploding or vanishing gradients. Despite no obvious changes in behavior, models can suddenly exhibit diverging gradients after thousands of iterations, making optimization difficult. In unrolled simulations like Recurrent Neural Networks, challenges can arise with exploding or vanishing gradients. Even with no apparent changes in behavior, models can suddenly show diverging gradients, making optimization challenging. Chaos in learning can lead to irrevocable loss, as illustrated in a simple example of a particle-in-a-well system with certain parameter choices causing chaotic dynamics and explosive gradients. Diagnosing chaos may not be straightforward, as seen in the divergence of sensitivities despite the particle's confined position. In dynamical systems, exploding gradients and chaotic dynamics involve a multiplicative accumulation of sensitivities, leading to exponentially diverging sensitivity to initial conditions. Recent work focuses on stabilizing RNNs to keep norms of per time-step Jacobians well-behaved. A general-purpose regularizer is proposed to enforce this goal for any differentiable computational iteration with continuous state. The dynamics of a computational iteration with continuous state converge to a periodic orbit over 2 k values. After a critical step size, a period-doubling bifurcation occurs, leading to chaotic dynamics where gradients diverge. Non-expansive conditions ensure that trajectories iterated through a mapping remain close. Close trajectories iterated through a non-expansive mapping must remain close for arbitrary time. Online sensitivity analysis enforces non-expansivity in simulations by rolling back time steps and adding regularization to the loss. Being 'almost' non-expansive is helpful for learning, while Lipschitz conditions encourage stable backpropagation. In chaotic phase transitions, a fall-back plan is needed to continue learning. To reduce reliance on alignments and generate profiles for new sequences, diverse sequences were dynamically added to the training set. Sampling sequences based on normalized Hamming distance increased available sequence-structure pairs significantly. This strategy was used for both profile and 1-seq based training, resulting in a substantial increase in data for learning. In the results section, structures were generated from different models and TM-scores were calculated. Clusters were determined using hierarchical clustering. Sampling speed for NEMO RNN was compared with an RNN baseline. In the results section, structures were generated from different models and TM-scores were calculated. Clusters were determined using hierarchical clustering. Sampling speed for NEMO RNN was compared with an RNN baseline. For each protein, 100 models were sampled and consensus clustering was done with 3D-jury, with an additional factor of 10 cost between NEMO and the RNN."
}