{
    "title": "H1xTup4KPr",
    "content": "In computer vision, CNNs for image classification may struggle with tiny object classification due to biased datasets. A testbed with MNIST digits and histopathology images showed that CNNs have a limit to signal-to-noise ratio for generalization, affected by dataset size. More data improves performance, but training data needed for generalization scales rapidly with object-to-image ratio. Convolutional Neural Networks (CNNs) are the leading method for image classification. Higher capacity models show better generalization, and adapting receptive fields based on object sizes is beneficial. Global pooling operations impact optimization for low signal-to-noise ratios, while for higher ratios, all tested operations perform similarly. The goal of image classification is to assign labels to images where the object is clearly visible and occupies a significant portion of the image. In real-life applications like medical or hyperspectral image analysis, the Object to Image (O2I) ratio can be low, leading to a distribution shift between standard classification benchmarks and domain-specific datasets. For example, in ImageNet, objects fill at least 1% of the image, while in histopathology slices, cancer cells can occupy as little as 10^-6% of the image. Recent studies have explored CNNs under different noise scenarios, either through random input-to-label experiments or working with noisy annotations. In this study, CNNs are examined in the context of tiny object classification and the relationship between input signal-to-noise ratio and model generalization. Two synthetic datasets are created to specifically analyze CNN performance in scenarios with low Object to Image (O2I) ratios. The study examines CNN performance in scenarios with low Object to Image (O2I) ratios using two synthetic datasets inspired by Where's Wally? book. One dataset is derived from MNIST digits and medical imaging datasets, while the second is from histopathology imaging. These datasets allow for explicit control of the O2I ratio, with examples of images displayed alongside object area A object. The study explores CNN performance in low Object to Image (O2I) ratios using synthetic datasets. It develops a classification framework based on CNNs and analyzes factors affecting model optimization and generalization. Higher capacity models show better generalization, learning input noise structure. The amount of training data needed for generalization scales rapidly with the inverse of the O2I ratio. The study investigates CNN performance in low Object to Image (O2I) ratios using synthetic datasets. It emphasizes the importance of model inductive bias, particularly the model's receptive field size. Different pooling operations show similar performance for larger O2I ratios, but for very small ratios, max-pooling leads to faster convergence. The code for the testbed will be publicly available for reproducibility, aiming to facilitate further research on low signal-to-noise classification scenarios. The dataset used for evaluating CNN-based binary classifiers alters data-related factors like dataset size, object size, image resolution, and class balance. A cluttered MNIST dataset was created by randomly placing MNIST digits on a large canvas with varying resolutions to achieve different Object to Image ratios. The dataset includes images with resolutions ranging from 64x64 to 1024x1024 pixels. The object of interest is the digit 3, randomly placed within the image canvas. Distractors (clutter digits) from the MNIST dataset are included to maintain a constant clutter density. Different Object to Image ratios are achieved by varying the number of clutter objects. Training, validation, and test images are generated for each O2I ratio. Refer to supplementary material for more details on image generation and dataset visualizations. The CAMELYON dataset contains gigapixel histopathology images with pixel-level lesion annotations from 5 different acquisition sites. Datasets are generated for various Object to Image ratios and image resolutions, resulting in training sets with unique lesions per dataset configuration. Positive examples are created by cropping lesion annotations, while negative images are randomly cropped healthy images. See supplementary material for more details on the dataset sizes. The dataset for the nCAMELYON experiments includes gigapixel histopathology images with lesion annotations. Positive examples are lesion annotations, while negative images are randomly cropped healthy images. Class balance is maintained by sampling an equal amount of positive and negative crops. No pixel-wise information is used during training, and BagNets backbone is followed for classification pipelines. The approach involves topological embedding extractor, global pooling operation, and network receptive field control. The study involves testing 48 different architectures by varying the embedding extractor and pooling operation. The topological embedding extractor converts an image into a topological embedding, while different pooling operations such as max, logsumexp, average, and soft attention are experimented with. Instance Normalization is used due to small batch sizes during training. In experiments, different pooling operations like max, logsumexp, average, and soft attention are tested. The study investigates how CNNs' optimization and generalization scale with low O2I ratios. Key questions include the impact of image-level annotations, O2I ratio limit, dataset size, model capacity, and receptive field adjustments on generalization. In experiments, the study explores the impact of receptive field adjustments and global pooling operations on model generalization. The models were trained using RMSProp with a specific learning rate and decay schedule. Validation accuracy was monitored throughout training, with test set results reported for the best-performing model. The study analyzed the impact of receptive field adjustments and global pooling operations on model generalization. It reported the best validation accuracy and test set results for the top-performing model. The minimum training set size needed to achieve specific validation accuracy was also examined. Additional results and analysis can be found in the supplementary material. The experiment varied the O2I ratio on nMNIST and nCAMELYON to assess its effect on network generalization. The study analyzed the impact of receptive field adjustments and global pooling operations on model generalization, reporting the best validation accuracy and test set results. Results showed that CNNs achieved reasonable test accuracies for specific O2I ratios on nMNIST and nCAMELYON datasets. Testing the influence of training set size on model generalization for nMNIST data revealed insights into CNNs' generalization problems for small O2I ratios. The results show that larger datasets lead to better generalization, especially for small O2I ratios. A heatmap is used to visualize the mean validation results for different O2Is and training set sizes. The study focuses on nMNIST and nCAME-LYON datasets, analyzing the number of training epochs needed and successful runs for model fitting. The model's fit to the training data varied with different random seeds. Training set size increases with decreasing O2I ratio for good classification generalization. Networks with varying capacities were trained, showing improved test set performance with increased capacity, especially for smaller O2Is. This improvement is attributed to the model's ability to ignore input data noise. For the nCAMELYON dataset, limited dataset size results in less pronounced trend of higher network capacity requirement. Larger histopathology dataset may enable training CNN models with image level annotations. Test accuracy in relation to O2I ratio and receptive field size is shown for nMNIST and nCAMELYON datasets. Bigger receptive fields outperform smaller ones for nMNIST, while for nCAMELYON dataset, results vary. In the experiment, different pooling approaches were compared for nMNIST and nCAMELYON datasets with varying receptive field sizes. For nMNIST, max-pooling performed best for smaller O2I ratios, while all pooling operations showed similar performance for larger ratios. The global max pooling operation was suggested to be effective in removing structured input noise. For nMNIST datasets, max-pooling is effective in removing structured input noise, while soft attention pooling performs best for smallest O2I mean. In large scale experiments, some configurations struggle to fit the training data, leading to training accuracy close to random. Substituting nMNIST datapoints with samples from an isotropic Gaussian distribution shows that most setups can memorize Gaussian samples but struggle with nMNIST samples. The nMNIST dataset poses a challenge for CNNs compared to Gaussian noise. Optimization becomes harder with decreasing O2I ratio, with max pooling being the most robust. Results are consistent across different random seeds, either succeeding or failing to converge. Reasoning about tiny objects is crucial in various computer vision applications. Most approaches in computer vision rely on manual dataset curation and additional pixel-level annotations to improve signal-to-noise ratio. Collecting pixel-level annotations is costly and requires expert knowledge, posing a bottleneck in the data collection process. Other approaches leverage the fact that task-relevant information is not uniformly distributed across input data. Recent research has focused on attention mechanisms for processing high-dimensional inputs in various contexts, such as multi-instance learning and histopathology image classification. However, the exact O2I ratio used in experiments is often not reported. This subsection discusses the optimization and generalization dimensions of CNNs in low O2I classification scenarios, highlighting the benefits of over-parametrized models with higher capacity for improved generalization. Higher capacity models perform better with proper regularization and fixed size datasets. CNN performance improves logarithmically with dataset size, and model capacity should scale with dataset size. Incorporating inductive biases, like convolutions in CNNs, improves model performance. Different CNN architectures also enhance model performance. In this paper, the authors identified a new machine learning problem of image classification in low signal-to-noise ratios. They conducted experiments manipulating signal-to-noise ratios and found that CNNs struggle to generalize well in these scenarios, even on basic datasets like MNIST. The study focused on exploring CNN architectures and training data scale for low signal-to-noise classification. They found that properly designed CNNs can generalize in low signal-to-noise scenarios with sufficient training data, but the amount needed scales rapidly with the inverse of the signal-to-noise ratio. The experiments used a synthetic nMNIST dataset due to the lack of large-scale datasets with controlled signal-to-noise ratios. The study utilized the nMNIST dataset for analysis, but due to limited unique lesions in the histopathology dataset, scaling experiments was challenging. Large-scale datasets like MS COCO show object-background correlations, with \"sports ball\" having the smallest object-to-image ratios. Future research could explore setups with negative images containing \"person\" and \"baseball bat\" objects, and positive images with \"sports ball\". All tested models showed improvement. In this section, the study discusses the need for computationally-scalable, data-efficient inductive biases to handle low signal-to-noise ratios with limited dataset sizes. Further research is suggested to explore the knowledge of sparse signal as an inductive bias. The study also highlights the focus on binary classification scenarios and suggests future investigation into multiclass problems. The hope is that this study will inspire research in image classification for low signal-to-noise input scenarios. Additional details about the datasets used in the experiments, such as the needle MNIST (nMNIST) dataset designed for binary classification, are provided. The nMNIST dataset is created for binary classification, specifically to identify if an image contains the digit 3. Different subsets are generated by varying the object-to-image ratio, with positive images having the digit 3 and negative images without it. The resolution of nMNIST images ranges from 64 \u00d7 64 to 1024 \u00d7 1024 pixels by adjusting the canvas size. MNIST digits are split into digit-3 and clutter subsets for assignment to the canvas. The nMNIST dataset is designed for binary classification to detect the digit 3 in images. Positive images contain digit 3, while negative images do not. The object-to-image ratio is adjusted by sampling digits and clutter subsets. Different canvas resolutions are used, ranging from 64 \u00d7 64 to 1024 \u00d7 1024 pixels. The nCAMELYON dataset focuses on binary classification for breast cancer metastases detection using pixel-level annotations. The nCAMELYON dataset uses pixel-level annotations to extract samples for breast cancer metastases detection. Positive examples are identified within annotations, and random crops are taken around each region. Negative crops are selected from healthy images based on a heuristic. Training, validation, and test sets are split center-wise to prevent data contamination. In the nCAMELYON dataset, crops from center 3 are in the validation set, while crops from center 4 are in the test set. Images are generated at different resolutions and O2I ratios. The pipeline used in the experiments includes global pooling operations and different architectures. Four global pooling functions are being tested. In experiments testing global pooling functions, four different functions are being evaluated: max-pooling, mean-pooling, logsumexp, and soft attention. Max pooling returns the maximum value per channel in the embedding, affecting gradient backpropagation. Logsumexp pooling is a soft approximation to max pooling. Mean pooling computes the mean value for each channel in the embedding. Attention-based pooling involves additional operations. The operation involves attention-based pooling with additional weighting tensors to rescale topological embeddings before averaging them. Soft-attention mechanisms are parametrized using fully connected layers with tanh-activation and 128 hidden units. The BagNet architecture is adapted for the experiments, with different receptive field sizes tested. Downsampling is performed using convolutions with stride 2 within the first residual block. In this section, additional experimental results and visualizations are provided, focusing on testing CNNs' generalization under class-imbalanced scenarios using the nMNIST dataset. The receptive field is adjusted by replacing 3 \u00d7 3 convolutions with 1 \u00d7 1 convolutions and increasing the number of convolution filters by a factor of 2.5. Different network capacities are tested by scaling the number of convolutional filters with a constant factor of s \u2208 {1/4, 1/2, 1, 2}. The experiment focuses on testing CNNs' generalization under class-imbalanced scenarios using the nMNIST dataset. The training set class balance is altered by changing the proportion of positive images, with various balance values tested. Different receptive field sizes and pooling operations are used, with model performance observed to drop as training data becomes more unbalanced. The experiment tested the effect of model capacity increase on a small dataset of 3k class-balanced images compared to a larger dataset of 11k training images using global max pooling operations with different receptive field sizes. Results showed that increasing model capacity did not improve generalization for small datasets. Additional results for all tested global pooling operations on O2I limit vs. dataset size were reported, including heatmaps representing validation set results for different O2I and training set sizes. Figure 13 shows the impact of network capacity on generalization performance for nMNIST at O2I ratio = 1.2%. The improvement from increased capacity decreases with smaller training sets. Figure 14 displays the validation set accuracy heatmap for different pooling methods. Object localization capabilities are tested using saliency maps. Figure 15 illustrates the minimum training set sizes needed to achieve specific validation accuracies. In this study, the authors tested different training set sizes and reported the minimum number of examples needed to achieve specific validation performance. Saliency maps were generated using max-pooling and a ResNet-50 capacity to detect objects of interest in nMNIST images. The average precision for object detection using saliency maps was calculated, with the confidence of detection based on the magnitude of the saliency. The study tested different training set sizes to achieve specific validation performance. Saliency maps were generated using max-pooling and ResNet-50 to detect objects in nMNIST images. The average precision for object detection was calculated based on the magnitude of saliency, with objects of interest correctly localized based on maximum saliency. The study examined object detection performance using saliency maps generated by different methods. Detection scores were best with max-pooling for smaller object-to-image ratios and with logsumexp for larger ratios."
}