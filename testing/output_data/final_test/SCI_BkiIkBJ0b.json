{
    "title": "BkiIkBJ0b",
    "content": "Deep reinforcement learning (DRL) algorithms have shown progress in learning to navigate challenging environments, but they do not use mapping or path-planning strategies. The algorithms memorize maze maps during training, not testing. The NavA3C-D1-D2-L algorithm can choose shorter paths to the goal when trained and tested on the same maps. However, on unseen maps, it uses a wall-following strategy without mapping or path planning. Navigation in mobile robotics and artificial intelligence involves exploration and exploitation stages for path planning based on optimality criteria. Recently, end-to-end navigation methods utilizing Deep Reinforcement Learning (DRL) have gained traction in solving navigation challenges in unstructured environments. These methods, such as BID10, BID6, BID7, and BID12, bypass the need for mapping and path-planning, potentially leading to simpler yet more effective navigation solutions. One algorithm by BID7 has shown promise in efficiently exploring and reaching goals in complex environments. The algorithm by BID7 shows promise in efficiently finding goals in complex environments using monocular first-person views. However, DRL-based navigation is still relatively unexplored with limitations due to the black-box nature of these methods. Recent work has shown that deep learning-based object detection methods can be easily fooled, highlighting the importance of analyzing DRL methods to understand their strengths and limitations. In this work, the authors develop a better understanding of recent DRL-based methods by thoroughly exploring and analyzing state-of-the-art BID7 methods across hundreds of maps with increasing difficulty levels. The environment is set up as a randomly generated map with an agent and a goal. The agent is provided with only the first-person view and is tasked to find the goal as many times as possible within a fixed amount of time, respawning its location each time it reaches the goal. The algorithms are trained and evaluated with increasing difficulty levels. The authors evaluate algorithms with increasing difficulty levels by gradually randomizing spawn locations, goal locations, and map structures. While BID7's algorithm shows success in exploiting goal knowledge for reward maximization, it raises concerns about repeatability and generalization to unseen maps. The authors evaluate DRL-based navigation methods on maps with unseen structures, questioning the effectiveness of algorithms like NavA3C+D 1 D 2 L in mapping and path planning. Experiments reveal a lack of mapping and optimal path planning on unseen maps, with attention-maps showing models focus on specific image areas. The authors present experimental results on robot localization and mapping for navigation, using multiple maps for training and testing. They highlight the evolution of algorithms in the field over the past three decades, emphasizing the importance of capturing detail in maps. The code and data will be made available after the blind review process. End-to-end navigation algorithms optimize map storage based on the task at hand, eliminating the need for manual tuning. Deep reinforcement learning has been successful in training agents for Atari games and end-to-end navigation tasks. Agents are typically trained and tested on the same maps with variations in spawn points and goal locations. In contrast to previous studies on end-to-end navigation, BID1, BID7, BID12, and BID18 test their algorithms on random unseen maps with different approaches to goal selection and map information storage. BID1 focuses on texture invariance, while BID12's agents choose between multiple potential goal locations without storing map information. This study aims to determine if DRL-based agents require mapping algorithms or can rely solely on learned information for navigation tasks. The problem of navigation is formulated as an interaction between an environment and an agent in a Partially Observable Markov Decision Process (POMDP). The future state of the environment is conditionally independent of past states given the current state. A POMDP is defined by a six-tuple (O, C, S, A, T, R) consisting of an observation space, state space, action space, transition function, and reward function. The problem of navigation is formulated as an interaction between an environment and an agent in a Partially Observable Markov Decision Process (POMDP), defined by a six-tuple (O, C, S, A, T, R) with observation, state, action, transition, and reward components. The observation space O is an encoded feature vector from input image, action space A includes rotate left, rotate right, move forward, move backward, and reward function R incentivizes reaching the goal location. State space S is represented as a vector of floats, and a combined transition function Tc estimates the next state. The function Tc models the estimation of the next state by considering previous observations and rewards. Policy-based DRL involves functions \u03c0t and Vt sharing parameters to maximize expected future rewards. The DRL objective is to find weights that maximize future rewards. Asynchronous Advantage Actor-Critic (A3C) is a policy-based method that allows asynchronous weight updates in a multi-threaded environment. The architecture in DISPLAYFORM1 Figure 2 updates a shared network with accumulated gradients every few iterations. It includes inputs for the current image, previous action, and previous reward. The architecture improves upon vanilla A3C by using auxiliary outputs for loop-closure signal and predicted depth. Gradients are applied to a local copy of weights, which are periodically synced from the shared target weights. Weight updates are based on advantage and characteristic eligibility. The NavA3C+D 1 D 2 L architecture, proposed by BID7, enhances the A3C algorithm by incorporating two LSTMs and auxiliary outputs for depth and loop-closure predictions. Inputs include the current image, previous action, and reward. This architecture optimizes predictions for auxiliary outputs, improving upon vanilla A3C. The agent in a simulated environment navigates a maze with rewards and penalties, respawning after reaching the goal. The game setup encourages exploration with apple rewards and includes a wall penalty to push the agent away from walls. The agent in a simulated maze environment faces a small wall penalty (-0.2) to prevent hugging walls and discard vision input for exploration. A discrete 4-action space is used for faster training, with 10 randomly chosen mazes out of 1100 generated for evaluation. The NavA3C+D 1 D 2 L algorithm is evaluated on maps with 5 stages of difficulty, showing smooth performance on easier stages but not outperforming wall-following methods on the hardest stage. These experiments serve as a 5-stage benchmark for end-to-end navigation algorithms, including scenarios with static goals and maps, as well as random spawn points. In a partially observable environment, the agent must find an optimal policy from each starting point in the maze. Different setups include fixed spawn and map with a random goal, as well as random spawn and goal locations. The agent needs to remember and exploit goal information to perform well in these experiments. BID7 addresses a problem with limited success by evaluating it on two maps, finding Latency 1 :> 1 to be greater than 1 in one map. They propose algorithms for end-to-end navigation problems and test them on unseen maps, a first in deep reinforcement learning based navigation methods. Agents are trained on multiple maps and tested on unseen ones, with results shown in Fig 5. Evaluation metrics include rewards, Latency 1 :> 1, and Distanceinefficiency. The NavA3C+D1D2L algorithm is evaluated on ten maps with varying difficulty levels. When the goal is static, rewards are higher and Distance-inefficiency is close to 1, indicating efficient pathfinding. However, with random goals, agents struggle to find the shortest path. The algorithm performs well on trained maps but struggles to generalize to new maps. The Latency 1 :> 1 metric measures map exploitation ability, with higher values indicating better performance. Distance-inefficiency is the ratio of total distance traveled versus shortest distances to the goal from each spawn point. This metric is meaningful when the goal location is unknown at evaluation time. The algorithm's performance is evaluated on different map scenarios. For static goal, static spawn, and static maze, the path chosen is consistently the shortest. The Distance-inefficiency metric is close to 1, indicating efficiency. The agent's performance in various map scenarios shows that it can learn the optimal policy for the shortest path to the goal at training time. However, there is inconsistency in exploiting map information and performance from episode to episode. Inconsistent performance is observed in agents trained on 1000 maps and tested on a subset of 10 maps. The Distance-inefficiency is larger than 1, indicating non-optimal path traversal. Results show map-exploitation and random exploration rather than shortest path planning. Training on more maps leads to improved performance, but no significant increase beyond 100 maps. The algorithm's performance increased from 10 to 100 maps but did not significantly improve when trained on 500 to 1000 maps. Training on additional maps did not enhance the learned wall-following strategy. The algorithm was found to be robust to variations in textures and presence of apples during evaluation. Simple maps with only two paths were used to evaluate the algorithm's strategies to reach the goal. The algorithm's performance on simple maps with two paths to the goal was evaluated. Results showed that the agent often moved in the direction of initialization, possibly due to initial learning from small rewards. The agent only took the shortest path 50.4% of the time, similar to random behavior. To address this, the algorithm was evaluated on a Wrench map to eliminate orientation dependency. The agent's decision-making process was evaluated on different maps, showing that it often moved in the direction of initialization. It only took the shortest path around 32.9% of the time, similar to random behavior. The goal map penalized wrong decisions more than the wrench map, with a success rate of 42.6%. The presence of apples or textures had a negligible effect on the agent's performance. The experiments show that the NavA3C+D 1 D 2 L algorithm, trained on 1000 maps, does not generalize well to simple maps. Even with only two possible paths to the goal, the agent struggles to choose the shorter path. Models trained on 1000 maps tend to follow a wall-following strategy rather than planning based on goal location. The experiments demonstrate that the NavA3C+D 1 D 2 L algorithm, trained on 1000 maps, struggles to generalize to simple maps, often opting for a wall-following strategy instead of choosing the shortest path to the goal. The attention in the image is represented by the normalized sum of absolute gradient of the loss with respect to the input image, acting as a soft mask to visualize the agent's focus shifting from a uniform distribution to a concentrated area in the center while navigating. The attention in the image shifts from a uniform distribution to a concentrated area in the center as the agent navigates through corridors, turns, and junctions, focusing on important objects like the goal and apples. The NavA3C+D 1 D 2 L algorithm is evaluated through experiments on navigation tasks. The NavA3C+D 1 D 2 L algorithm, a DRL-based navigation model, is tested on various maps. Results show that while it can navigate and map paths on familiar maps, it fails to do so on new environments. This raises doubts on whether DRL-based algorithms truly \"learn to navigate\" beyond specific environments. In this work, a systematic approach to experiments using the Deepmind Lab environment is presented for future DRL-based navigation methods. The experiments involve navigating mazes with apple and goal rewards, wall penalties, fixed time episodes, and A3C implementation. The mazes are 900units\u00d7900units in size, with the ability to generate mazes of arbitrary dimensions. Our A3C implementation, based on OpenAI's universe-starter-agent, uses RGB images of 84x84x3 dimensions. 16 threaded agents are utilized with a learning rate of 10^-4 and AdamOptimizer. Models train for a maximum of 10^8 iterations, released with corresponding reward curves and videos online for comprehensive evaluation. Trained models are tested on various environments including original conditions, absence of apples/textures, 100 unseen maps, and planning maps. The planning maps, square, wrench, and goal map, are used in our work to create better generalized DRL navigation methods. Our work will be available on github after the blind-review process."
}