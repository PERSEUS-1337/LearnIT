{
    "title": "BJepraEFPr",
    "content": "Dialogue systems require expertise in various domains to assist, inform, and entertain humans. This paper proposes a dialogue system that independently parameterizes different dialogue skills and combines them through Attention over Parameters (AoP). Experimental results show competitive performance on datasets like MultiWOZ, In-Car Assistant, and Persona-Chat. Each dialogue skill is effectively learned and can be combined to produce selective responses. The text discusses the challenge of training chat interfaces to handle both casual talk and tasks like reservations. Existing systems focus on either chit-chat or goal-oriented dialogues due to modularization constraints. The proposed solution involves learning different conversational skills across multiple domains to create a unified chat interface. The text proposes using the Mixture of Experts (MoE) model to handle multiple conversational skills in a unified chat interface, combining both chit-chat and goal-oriented dialogues. This approach aims to overcome the limitations of a fixed shared-parameter framework, providing better controllability and interpretability in response generation. The text proposes using the Mixture of Experts (MoE) model to model multiple conversational skills in dialogues, addressing limitations in expert selection and information extraction from a Knowledge Base (KB) in end-to-end task-oriented dialogues. The text proposes a novel Transformer-based architecture called Attention over Parameters (AoP) to model conversational skills in end-to-end task-oriented dialogues. The model predicts Sys turns by generating responses from a dynamically updated Memory content. The skills required for responses are based on prior knowledge, with Persona referring to chit-chat. The text suggests using advanced multi-hop models like the Transformer to address certain challenges. The text introduces a novel Transformer-based architecture called Attention over Parameters (AoP) for end-to-end task-oriented dialogues. It demonstrates the efficiency of AoP compared to traditional methods and shows the effectiveness of using specialized parameters in a combined dataset of Multi-WOZ, In-Car Assistant, and Persona-Chat. The model is highly interpretable and can combine different learned skills to generate responses. In this paper, the focus is on end-to-end dialogue models that train a single model directly on text transcripts of dialogues. These models can select predefined utterances or generate a sequence of tokens. Copy-augmented models are effective for extracting entities from a knowledge base. On the other hand, end-to-end open domain chit-chat models have also been widely studied. Several works have improved on domain chit-chat models with various methodologies, including the first attempt at an end-to-end system for task-oriented models and chit-chat. Specialized parameters or \"experts\" have been widely studied topics, utilizing different architectures and methodologies such as SVM and Gaussian Processes. Several models have been proposed in the field of machine learning, including Gaussian Processes, Dirichlet Processes, Hierarchical Experts, Infinity Number of Experts, and sequential expert addition. The Mixture Of Expert model added a large number of experts between two LSTMs. Previous works did not apply the results of the gating function to the parameters themselves. Conditional Computational models dynamically select their computation graph using methods such as reinforcement learning, halting function, pruning, and routing/controller function. The curr_chunk discusses multi-task learning in Natural Language Processing, where models learn multiple tasks simultaneously. It mentions applications such as parsing, machine translation, image captioning, and DecaNLP. The focus is on optimizing model performance for specific tasks. In this work, the focus is on conversational data and building a generic conversation model for both chit-chat and task-oriented dialogues using a Transformer for both encoder and decoder. The dialogue history and dynamic memory content are concatenated to obtain the final input. The final input is obtained by concatenating dialogue history D and memory M. The model is expected to produce a sequence of tokens denoted by Y, which can be plain text or SQL-like queries. A binary skill vector V specifies the skills required to generate Y, serving as a prior for learning to select the correct expert during training. Multiple skills in V can be enforced to achieve semantic compositionality. To achieve semantic compositionality, a standard Transformer is used to map input sequences to output sequences. The encoder and decoder are denoted as TRS enc and TRS dec, respectively. The input is encoded using a word embedding matrix E, and the decoder generates the output sequence using teacher-forcing. A distribution over the vocabulary is generated for each token. The model uses an affine transformation followed by a Softmax function to generate a distribution over the vocabulary for each token. It incorporates encoder-decoder attention distribution to enable token copying from the input sequence. The training loss is minimized using a standard cross entropy loss function, and inference generates tokens in an auto-regressive manner. Parameters for decoder TRS dec are produced by a weighted sum of independently parameterized decoders, similar to attention mechanisms. Each decoder is represented by its parameters, and key vectors are assigned accordingly. The model assigns key vectors to each \u03b8 and uses a key matrix K and a GRU to produce the query vector. Attention weights for each decoder's parameters are computed, and a new set of parameters \u03b8* is initialized for a new TRS dec. Equation 6 resembles a gating function, and the scoring vector \u03b1 is applied based on semantic skills assigned to each index in V. The model introduces a Transformer-based implementation of MoE called Attention over Representation (AoR), where a scoring vector \u03b1 is applied to each output representation Oi generated by the decoder. An additional loss term supervises the attention vector \u03b1 using a prior knowledge vector V. Multiple decoder parameters can be selected simultaneously, trained using binary cross-entropy, resulting in a final loss that is the summation of two terms: LP(Y|X) and LV. In the MoE framework, a second loss is defined as the summation of LP(Y|X) and LV. The Universal Transformer is a workaround for models with a large number of parameters, showing similar or better performance than a multi-layer Transformer. An AoP version using this architecture does not add any additional parameters. The model's performance is evaluated on different conversational skills using three datasets: MultiWOZ, Stanford Multi-domain Dialogue, and Persona-Chat. MultiWOZ (MWOZ) is a dataset with seven domains and two APIs interfaces: SQL and BOOK. The dataset includes SQL/BOOK queries and responses, allowing for training end-to-end models to produce SQL queries, retrieve knowledge, and generate plain text responses. The Stanford Multi-domain Dialogue (SMD) dataset contains three domains: Point-of-Interest, Weather, and Calendar. Unlike MWOZ, SMD has fixed memory with records relevant to dialogues. Persona-Chat is a multi-turn conversational dataset where speakers are assigned random persona descriptions. Training models using persona descriptions like \"I am an old man\" and \"I like to play football\" from the dataset results in more consistent and fluent conversations. The dataset has become a standard benchmark for chit-chat systems and includes real entities instead of placeholders, making the task more challenging and interesting. Evaluation for MWOZ and SMD datasets is done using BLEU 4 score and Entity F1-Score. The study evaluates response fluency and entity generation ability using BLEU 4 score and Entity F1-Score. It also measures exact match accuracy and BLEU score for SQL and BOOK queries. Additionally, comparisons are made with human-generated predictions using perplexity, BLEU score, F1-score, and Consistency score. The Consistency score is computed using a Natural Language Inference (NLI) model trained on dialogue NLI data. The study compares different models for dialogue consistency, including Sequence-to-Sequence, Transformer, Mixture of Expert, and Attention over Representation, with a proposed Attention over Parameters model. The authors aim to achieve a high consistency score similar to previous work by re-ranking beam search hypotheses. The study evaluates various models for dialogue consistency, such as Sequence-to-Sequence, Transformer, Mixture of Expert, and Attention over Representation, alongside a proposed Attention over Parameters model. Different experiments are conducted, including AoP with gold attention vector, AoP without L V optimization, Universal Transformer for AoP, and standard Transformer with 6 hops. Results are presented in Table 2 and Table 3 for MWOZ+SMD and Persona-Chat datasets, showing that AoP and AoR consistently outperform other baselines by effectively combining parameters using the correct prior V. The study compares different models for dialogue consistency, with AoP performing slightly better than AoR due to faster inference. Using Oracle attention gives the best performance, highlighting the importance of attention accuracy. Removing L V decreases model performance, emphasizing the need for good inductive bias. Results for SQL, BOOK, and sentences are reported in Table 3, showing AoP's high Consistency and BLUE score. MoE has the lowest perplexity and F1-score. The study compares different models for dialogue consistency, with AoP having the highest Consistency and BLUE score. Adding layer recursion improves performance for both AoP and the Transformer. AoP outperforms Oracle in SQL accuracy and Persona-Chat evaluation. The study demonstrates the effectiveness of the model in learning independent skills and composing them together by modifying \u03b1 to generate different responses. The model accurately captures the meaning of each skill and learns to compose them properly. The model effectively learns to compose multiple domain-specific skills, generating coherent responses such as SQL-syntax and relevant attributes for different domains like restaurants. This novel training approach enables the model to produce contextually appropriate dialogue responses. In this paper, a novel approach called Attention over Parameters is proposed to train a single end-to-end dialogue model with multiple composable and interpretable skills. The model learns to combine independent sets of specialized parameters into a single set, achieving compositionality, interpretability, and faster inference speed. The effectiveness of the model is verified by organizing multi-domain task-oriented datasets and a conversational dataset (Persona-Chat) into end-to-end trainable formats. The computational cost of Attention over Parameters (AoP) is shown to be lower than Mixture Of Experts (MoE) for sequences longer than 1. The cost of operations for MoE is O(rtdn + rtn), while for AoP it is O((r + t)dn). The computational cost in terms of operations for the model is O((r + t)dn), with the cost of summing parameters being O(rdn) and the cost of f \u03b8 * being O(tdn). Using a simple affine transformation W is considered optimal, as the number of operations increases with attention but parameters remain constant. An additional embedding matrix P is defined for encoding token types such as speaker information and segment types. The attention vector \u03b1 in the model focuses on correct skills like SQL and BOOK when API-calls are needed. There are instances of false positives where the model puts too much weight on BOOK instead of plain text responses. The attention vector in the model focuses on correct skills like SQL and BOOK for API-calls. Instances of false positives occur when the model emphasizes BOOK over plain text responses. The model sometimes focuses on multiple skills not directly relevant to the task, especially when other skill-related entities are mentioned in the context. This can lead to non-negligible activations for unrelated skills like restaurant and hotel. The curr_chunk discusses the conversion of MultiWOZ into an end-to-end trainable dataset by adding SQL-syntax queries using state-tracker and speech acts annotations. These annotations help generate well-formed queries and decide when to include them. The SQL query syntax and the booking API syntax are explained, with speech acts like \"INFORM-DOMAIN\" and \"RECOMMEND-DOMAIN\" used to determine when to issue APIs. The curr_chunk discusses the effectiveness of using speech acts like \"INFORM-DOMAIN\" and \"RECOMMEND-DOMAIN\" to trigger APIs based on state-tracker changes and query history. Despite some noise in the speech act annotations, a strategy is employed to handle different scenarios, such as using a special token <TM> for large record results and filtering records based on speech acts. The curr_chunk discusses using speech acts like INFORM or RECOMMEND to filter records for correct answers in a booking call. A Transformer architecture with pre-trained Glove embedding is used, along with Adam optimizer. Additional transformer layers are added on top of the model output. High level designs for MoE, AoR, and AoP are shown in Figures 6, 7, and 8. A batch size of 16 is used for all models. The experiments were conducted using a single Nvidia 1080ti with a batch size of 16. Hyper-parameters were tuned using a small grid-search. The model consists of feed-forward neural networks embedded between LSTM layers and a trainable gating network to select experts. Attention over Representation (AoR) involves a transformer encoder for encoding source input and computing attention over skills. Attention over Parameters (AoP) also uses a transformer encoder for specialized representation and output response generation. The AoP model consists of a transformer encoder for encoding source input and computing attention over skills, followed by specialized transformer decoder layers and a dummy transformer decoder layer parameterized by the weighted sum of the specialized transformer decoder layers parameters. The output layer is omitted in the figure."
}