{
    "title": "BygXY34FDr",
    "content": "In this paper, novel transfer reinforcement learning techniques are applied to target-driven navigation using the photorealistic AI2THOR simulator. The approach introduces the architectural contribution of a Successor Feature Dependent Policy (SFDP) and utilizes Variational Information Bottlenecks for improved performance. The final architecture, VUSFA, outperformed recent approaches in transfer learning ability and showed greater stability in training. The approach aims to mimic human navigation abilities in unknown spaces, relying on visual perception, previous experience, and training. Visual navigation algorithms in robotics have evolved, with map-based and map-building approaches being favored in the past. However, these methods require accurate mapping and human-guided training, limiting their generalizability. Recent advancements in Deep Reinforcement Learning (DRL) have led to significant progress in map-less navigation, demonstrating more human-like navigation capabilities. Recent advancements in Deep Reinforcement Learning (DRL) have enabled more human-like navigation in target-driven visual navigation tasks. The agent must navigate to dynamic goals represented as visual information, requiring the ability to learn new tasks through transfer learning. Transfer learning in Deep Reinforcement Learning (DRL) can be achieved through General Value Functions (GVF) or Successor Feature Approximation (SFA). While GVF improves transfer learning in target-driven visual navigation, it lacks transparency in learning dynamics and struggles in complex environments. On the other hand, SFA captures environment dynamics by learning future state visitations but faces limitations with multiple tasks. Universal Successor Features Approximators (USFA) extend SFA to consider multiple tasks and enhance transfer learning ability. Our research introduces Universal Successor Feature Approximators (USFA) for target-driven visual navigation, enhancing transfer learning in complex environments. We also propose Successor Feature Dependant Policy (SFDP) to improve the DRL agent's transfer learning ability by utilizing information from USFA. Additionally, we contribute Variational Universal Successor Feature Approximators (VUSFA) using Variational Information Bottlenecks for stable performance in complex tasks. In this paper, we address transfer reinforcement learning in complex tasks like target-driven visual navigation. We focus on Universal Successor Feature RL (USF-RL) and its extension, Successor Feature Reinforcement Learning (SF-RL). The goal-directed navigation task is formalized as a Markov Decision Process (MDP). In target-driven visual navigation tasks, the goal-directed navigation is formalized as a Markov Decision Process (MDP). The transition probability defines the probability of reaching the next state when an action is taken in the current state. A goal-dependent reward function and discount function are defined for each goal. A Generalized Value Function (GVF) can be defined for any policy, aiming to find the optimal policy that maximizes future rewards. The agent learns multiple optimal policies and value functions to navigate to different goals, treating each goal as a new task. In target-driven visual navigation tasks, the agent learns multiple optimal policies and value functions to navigate to different goals. Universal Successor Features (USF) extend the idea of Successor Features (SF) by defining the immediate reward as a linear combination of state representations and a goal-dependent reward prediction vector. The agent relies on state representations of the new state to recover the scalar reward rather than capturing physical features. The value function in target-driven visual navigation tasks is described as a cumulative sum of discounted features, with USF incorporating shared dynamics between tasks. Learning USFA is achieved through TD error using neural networks. The USFA model extends SF for multiple tasks with A3C agent, but faces challenges in complex tasks like target-driven visual navigation. The state representation \u03c6 vector decouples reward and learns USFA \u03c8 \u03c0 g (s t ) with TD-error loss function.\u03c6 is learned using an autoencoder to capture state information. Training a convolutional autoencoder to define an optimal representation for complex visual states can lead to overfitting. Training \u03c9 g with a regression loss based on scalar rewards may not provide enough information, leading to difficulties in training. Ma et al. (2018b) suggest training \u03c9 g with a separate neural network using goal features as input to avoid overfitting on limited goal locations. Our first contribution is the application of USFA for target-driven visual navigation, creating a stable architecture that allows the agent to learn task-dependent state representation features. By using a siamese network to generate \u03c6 and \u03c9 g, the agent can capture only salient features relevant to navigation. Training a stable USFA-based DRL agent can be challenging due to the risk of overfitting on limited goal locations. Training a stable USFA-based DRL agent can be challenging due to the imbalance in reward structure, leading to an unstable \u03c9 g. To address this, the A3C agent's critic update is proposed to overcome the problem of divergence in the network. The A3C algorithm requires each agent to optimize critic and policy functions with N-step Return. The critic's loss function is used to learn \u03c9 g, which is a linear combination of \u03c8 g (s t) and \u03c9 g. The method is more informative as it depends on discounted scalar rewards. \u03c8 is updated with Loss \u03c8 T D and Loss V T D to train \u03c9 effectively. Embeddings from the Siamese network are utilized to counter the problem of having few training goals for \u03c9. Our second contribution includes adding a Successor Feature Dependant Policy (SFDP) to the USFA implementation. By utilizing embeddings from the Siamese network as goal information, we trained \u03c9 as another branch of the USFA-A3C agent. The abstract map of future states, represented by \u03c8 g (s t), can aid in determining the next action, contrary to traditional methods. USF represents the cumulative sum of discounted future states visited following an optimal policy, facilitating transfer learning. The property of cumulative discounted future states visited, represented by \u03c8 g, aids in transfer learning by allowing the agent to learn from similar paths. The proposed network architecture \"VUSFA\" includes a shared Siamese encoder E(z|s t) and a policy conditioned on the USF vector for training the \u03c9 vector. The policy is conditioned on the USF vector \u03c8 for training the \u03c9 vector. The USFA \u03c8 is trained with temporal difference error using \u03c6 for expected future state occupancies. Gradient flow from policy to USFA head is stopped to preserve true USF representation and transfer learning capabilities. Introduction of Variational Siamese Bottleneck (VSB) improves USFA and reward prediction vector quality. Siamese layers' embeddings play a key role in enhancing performance by generating \u03c8 and \u03c9 g. In order to enhance performance and improve transfer learning capabilities, the introduction of the Deep Variational Information Bottleneck was implemented. This addition guided the Siamese layers to extract more informative features, resulting in better generalization and improved network performance. The theory of the Information Bottleneck, introduced by Tishby & Zaslavsky (2015), explains how a deep neural network balances compressing input information into a latent representation while preserving relevant information for output prediction. This is achieved by minimizing the mutual information between input features and the latent representation, encouraging the network to compress relevant information for accurate predictions. Deep Variational Information Bottlenecks by Alemi et al. (2016) is a parameterized approach to the Information Bottleneck theory, enabling easy use with deep neural networks. It introduces a regularized objective function to encourage the generation of an informative compressed embedding Z from input X. The encoder function E(Z|X) maps input features X to latent vector Z, while the decoder function q(Y |Z) maps Z to output labels Y. The bottleneck layer Z, generated under an information constraint, can be applied as an intermediate layer in the neural network. The Variational Siamese Bottleneck (VSB) enforces an upper-bound on the mutual information term I(Z, X) to focus on discriminative features. A modified version with a variational lower bound and lagrangian multiplier \u03b2 is used for neural network training. The final deep variational information bottleneck loss includes updating the Lagrangian multiplier \u03b2 adaptively. The encoder outputs the mean and variance of Z for KL divergence calculation. The agent's loss function combines L total and the KL divergence term, with hyperparameters \u03bb \u03c0 ,\u03bb \u03c8 and \u03bb V. The network takes recent states and goal states as inputs for training. The training procedure for the model involves using the A3C algorithm and a reparameterized embedding. The mean vectors of the state representation for both goal and state are concatenated and fed through the layers for predicting the policy and USFA. The final part of the network predicts the policy and USFA for each scene. The model used reparameterized embeddings to predict the policy and USFA, focusing on the \u03c9 vector for decoupling the value function. Reparameterized values were used for goal and state encoding during inference to improve generalizability and exploration. Evaluation of the agent in target-driven visual navigation included zero-shot learning ability and adaptation to new unknown goals during fine-tuning. The evaluation criteria for the agent in target-driven visual navigation included zero-shot learning ability and adaptation to new unknown goals during fine-tuning. The models were trained on four scenes for 20 different goals before evaluation. The deep Siamese A3C model was used as the baseline, and variations of the proposed model were evaluated against the state-of-the-art. The agent's zero-shot learning capability was tested by evaluating its success in reaching new goal locations. The agent was trained on a limited subset of goals and tested on its ability to reach goals in less than 500 time-steps. Despite training on only 20 goal states, even the worst performing model was able to generalize to over 16% of all states. The study evaluated the transfer learning ability of four models by testing their performance on 20 new goals. The models were tested on states that were 1, 2, and 4 steps away from trained goals, as well as completely random goals. Five random states were sampled from each environment for the new goals, excluding the trained goals. The study evaluated transfer learning ability of models on 20 new goals using 5 trials with random states. USFA-based policies consistently decreased time-steps to reach goals faster than baseline. SFDP with USFA further reduced time-steps, with VUSFA showing improved performance in target-driven visual navigation tasks. Our approach, VUSFA, applies Deep Variational Information Bottleneck theory with Universal Successor Features in Deep Reinforcement Learning, improving transfer learning ability compared to previous state-of-the-art methods. The approach is generalizable beyond navigation tasks and the source code is available on our github repository. Future research could explore the semantical impacts of \u03c6, \u03c9, and \u03c8. The A3C framework was used to train the agent with 100 threads assigned to 20 goals. The agent performs actions based on the policy, collects rewards, and updates parameters asynchronously until a maximum threshold is reached. The A3C framework was utilized to train the agent with 100 threads assigned to 20 goals. Training involved a high-performance computer with specific hardware specifications and limited hyperparameter tuning. The SFDP-A3C model architecture included a shared siamese layer for generating embeddings and conditioning the policy on the USF vector. The USFA \u03c8 is trained with the temporal difference error using \u03c6 to give the expected future state occupancies. Designing a value function based on General Value Functions (GVF) and Universal Value Function Approximators (UVFAs) is challenging but theoretically sound. Successor Representation (SR) is modeled on the brain's predictive map creation ability. Successor Representation (SR) is a model based on the brain's predictive map creation ability. It has been combined with Deep Learning to create Deep Successor Reinforcement Learning. Transfer in RL has been evaluated on multiple tasks using Successor Features (SF) and adapted for the continuous domain. SR-based DRL architecture has been used for visual navigation tasks. Zhang et al. (2017) used a similar approach to Kulkarni et al. (2016) but showcased their solution in maze-like environments with DQN. In contrast, we use actor-critic methods in a photorealistic simulation environment. USFA (Ma et al., 2018b) is a recent extension to SR that learns a policy directly with actor-critic methods, aiming to optimize an agent for multiple tasks simultaneously. Unlike DQN-based approaches, USFA can directly optimize an agent to learn multiple tasks simultaneously. This paper demonstrates the adaptation of USFA to complex target-driven visual navigation tasks in a photorealistic environment. The authors extended the concept of SF-RL in USFA by incorporating GVF and the generalized policy improvement theorem. They found that the \u03b5-greedy Q learning method used in Deepmind-Lab simulator was not suitable for their problem domain, as it relied on a linear combination of tasks for state representation. The algorithm was effective for simple tasks like object collection but struggled with new goal locations. Additionally, the authors did not explore policy gradient methods. The authors extended the concept of SF-RL in USFA by incorporating GVF and the generalized policy improvement theorem. They proposed a goal-dependant SF approximated with Universal Successor Feature Approximators (USFA) and a fundamental architecture to learn USF consisting of three sub-networks. The final reward prediction network in USF-RL generates \u03c9 for different goals to train the agent. Unlike SF-RL, \u03c6 is trained separately with an autoencoder in USF-RL. The \u03c9 vector is predicted with a network taking goal information as input, allowing quick adaptation to novel goals. Equation 15 states that a neural network should predict the label while reducing mutual information between input features X and the encoding Z. The trade-off hyperparameter \u03b2 controls the quality of Z. Equation 16 illustrates the mutual information term I(X, Z) between input features and encoder embeddings. It involves the joint distribution of p(x, z) with the encoder E(z|x) and input distribution p(x). Equation 16 introduces the intractable distribution of the latent variable p(z), which is replaced with a known prior distribution r(z) to bound the mutual information term I(X, Z). This modification simplifies I(X, Z) into a KL divergence between the distribution of Z generated by the encoder and the approximated prior r(z), facilitating neural network training with the loss function in Equation 9. The interpretation of I(X, Z) with KL divergence allows for adaptive updates to the conditional loss function \u03b2 during training. Incorporating the concept of Deep VIB with the USF, this paper demonstrates significant enhancement in the transfer learning capability of a navigation agent. Alemi et al. (2016) found that models trained with a VIB are less prone to overfitting and more resilient to adversarial examples, emphasizing the importance of adaptive updates to the conditional loss function \u03b2 during neural network training."
}