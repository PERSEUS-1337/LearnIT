{
    "title": "rJFOptp6Z",
    "content": "Knowledge distillation is a method for model compression where a small student network imitates a larger teacher network. Previous studies have mainly focused on distillation in classification tasks, but this paper proposes knowledge transfer from face classification to alignment and verification tasks. By selecting appropriate initializations and targets, the student network can compete with the teacher network in non-classification tasks like alignment. Experiments on CelebA and CASIA-WebFace datasets show promising results. The study demonstrates that the student network can rival the teacher network in alignment and verification tasks, surpassing it at specific compression rates. A common initialization trick is used to enhance knowledge transfer and distillation performance in classification. Evaluations on CASIA-Webface and MS-Celeb-1M datasets validate the effectiveness of this approach. Model compression is essential for deploying large and deep networks on mobile devices. Various compression methods, including knowledge distillation, weight quantization, pruning, and decomposition, have been proposed. This paper focuses on knowledge distillation for model compression. In this paper, the focus is on knowledge distillation for model compression, specifically in face recognition tasks such as classification, alignment, and verification. The objective is to make the student network competitive with the teacher network by learning specific targets. Previous studies have mainly focused on distillation in classification tasks, but this paper expands to include regression and retrieval tasks as well. The focus is on knowledge distillation for model compression in face recognition tasks like classification, alignment, and verification. Previous studies have mainly focused on distillation in classification tasks, but this paper expands to include regression and retrieval tasks as well. In face alignment, the objective is to locate key-point locations in each image, while in face verification, the goal is to determine if two images belong to the same identity. Transfer learning from a pretrained classification model on ImageNet has shown success in tasks like object detection, object segmentation, and image retrieval, indicating the importance of domain similarity for distillation performance. In face recognition tasks, knowledge distillation is used for model compression in classification, alignment, and verification. While classification tasks have been the focus of previous studies, this paper explores distillation in regression and retrieval tasks as well. The challenge lies in selecting the appropriate targets for distillation, with face alignment and verification having additional task-specific targets beyond soft-predictions used in classification. The relevance of objectives between non-classification and classification tasks is crucial in determining the targets for distillation. In this paper, the model distillation in face alignment and verification is proposed by transferring knowledge from face classification. The distillation performance on CelebA and CASIA-WebFace BID28 datasets can be significantly improved, with the student network even surpassing the teacher network at specific compression rates. Knowledge transfer through classification distillation is the main contribution of this method. The distillation performance of classification can be boosted using a common initialization trick. Evaluations on CASIA-WebFace and MS-Celeb-1M BID5 datasets show that this simple trick yields the best results. Previous studies on knowledge distillation for classification tasks are discussed, with different approaches such as generating synthetic data, regressing logits, and using deep convolutional networks for better learning. The distillation performance of classification can be improved by using a common initialization trick. BID25 suggests that the student network should be deep and convolutional to match the teacher network's performance in CIFAR BID11. While most methods require multiple teacher networks for better distillation, BID20 introduces noise-based regularization to simulate the logits of multiple teachers. However, BID15 notes that the unconstrained values of these logits can lead to fitting issues due to high dimensionality. To address this, they use hidden layers to capture information more compactly. BID8 proposes a multi-task approach that combines identity labels and teacher network targets to improve training, using post-softmax activation with temperature. In this paper, the authors propose a method to improve knowledge distillation for non-classification tasks, specifically focusing on face recognition. They suggest initializing the shallow layers of the student network by regressing the mid-level target of the teacher network. This approach extends knowledge distillation beyond classification tasks, with a focus on improving classification performance through a simple initialization trick. The authors propose a method to enhance knowledge distillation for non-classification tasks, particularly in face recognition. They suggest initializing the shallow layers of the student network by regressing the mid-level target of the teacher network. This extends knowledge distillation beyond classification tasks, focusing on improving performance through a simple initialization trick. The distillation framework involves using a temperature parameter to smooth predictions and optimize a loss function to train the student network. The multi-task training approach in knowledge distillation involves balancing softmax loss and cross-entropy between teacher and student networks. The student network is initialized by regressing mid-level targets of the teacher network before distillation. This initialization is crucial for learning high-level features effectively. In knowledge distillation, initializing the student network with deep layers is essential for high-level feature learning. A simple trick involves training the student network with softmax loss to achieve full initialization. This process improves network transferability, especially when tasks are similar. The transfer of distilled knowledge from face classification to face alignment and verification involves two steps: transfer initialization and target selection. The motivation for transfer initialization is based on the use of pretrained classification models to boost performance in detection, segmentation, and retrieval tasks. This idea stems from the similarity in domain, allowing for easy transfer from low-level to high-level representation. The transfer of distilled knowledge from face classification to face alignment and verification involves two steps: transfer initialization and target selection. For non-classification tasks, a general distillation approach is proposed with task-specific network parameters and loss functions. The targets selected in teacher and student networks are used in the distillation process. The transfer of distilled knowledge from face classification to face alignment and verification involves setting balancing terms \u03b1 and \u03b2 for non-classification tasks. Face alignment is treated as a regression problem, optimizing the Euclidean loss between regression prediction and label. Distillation includes soft predictions and task-specific targets like hidden layer KT. In face alignment, the hidden layer KT is preferred for distillation with \u03b1 < \u03b2. Face verification uses triplet loss for model distillation to determine if two images belong to the same identity. The metric learning method BID21 is used for model distillation in face verification. The teacher network is trained with hidden layers for anchor, positive, and negative samples, with a margin controlled by \u03bb. Soft prediction is preferred for distillation in face verification to boost performance. The distillation process in face verification involves using soft prediction with a specific loss function. The choice of \u03b1 and \u03b2 values depends on the task's relation to classification. Experimental evaluation of the method is then conducted to validate its effectiveness. The experimental setup includes knowledge distillation for face classification, alignment, and verification tasks using datasets like CASIA-WebFace, CelebA, and MS-Celeb-1M. Evaluation involves splitting datasets for training and testing, assessing classification accuracy, and verifying results on the LFW database. The experimental setup involves knowledge distillation for face classification, alignment, and verification tasks using datasets like CASIA-WebFace, CelebA, and MS-Celeb-1M. Evaluation includes splitting datasets for training and testing, assessing classification accuracy, and reporting results on the LFW database. The correct identity label BID12 and results on the LFW BID13 database are reported by computing the percentage of correctly verified pairs. NRMSE is used to evaluate alignment BID27, while verification involves computing the Euclidean distance between testing sample pairs. The top1 accuracy is based on whether a test sample and its nearest sample belong to the same identity. ResNet-50 is used as the teacher network to learn a large number of identities, while student networks are modified by reducing the number of convolution kernels in each layer. In training, ResNet-50/2, ResNet-50/4, and ResNet-50/8 are used with different numbers of convolution kernels. Batch sizes vary for classification, alignment, and verification tasks. Nesterov Accelerated Gradient is used for faster convergence. Distillation involves training student networks with teacher network targets. Temperature and margin values are set by cross-validation. The temperature and margin values are set to 3 and 0.4 by cross-validation. Balancing terms have various combinations, and their setting is explained later. Different initialization methods for student networks are compared, with the full initialization achieving the highest accuracy of 75.06%. The full initialization method achieves the best accuracy of 75.06%, significantly outperforming other methods like Scratch and Fitnet BID19. This demonstrates the effectiveness of full initialization in improving transferability in classification tasks. Additionally, student networks trained with full initialization show significant improvements over those trained from scratch, with some even surpassing the teacher network by a large margin. For example, in the CASIA-WebFace database, ResNet-50/4 can compete with the teacher network, while ResNet-50/2 achieves a 3% higher accuracy. In face alignment evaluation, ResNet-50/8 distillation results with different initializations and targets on CelebA are shown in Table 3. The selection of ResNet-50/8 is due to the need for a large compression rate for deep networks in this relatively easy problem. The setting of \u03b1 and \u03b2 in Eqn.(7) is crucial, with a simple trick used to measure their individual influence and discard negative impacts by setting \u03b1 = 0 or \u03b2 = 0. In face alignment evaluation, ResNet-50/8 distillation results with different initializations and targets on CelebA are shown in Table 3. The setting of \u03b1 and \u03b2 in Eqn.(7) is crucial, with a simple trick used to measure their individual influence and discard negative impacts by setting \u03b1 = 0 or \u03b2 = 0. When the initializations of P retrain and Distill are used, \u03b1 = 1, \u03b2 = 0(soft prediction) always decreases performance, while \u03b1 = 0, \u03b2 = 1(hidden layer) gets consistent improvements, implying that the hidden layer is preferred in the distillation of face alignment. Distill has a lower error rate than P retrain, showing that W cls S has higher transferability on high-level representation. The highest distillation performance 3.21% is obtained with Distill and \u03b1 = 0, \u03b2 = 1, competitive to the teacher network(3.02%). In face verification evaluation, similar selection of \u03b1 and \u03b2 is made. In face verification, different initializations and targets were evaluated on CASIA-WebFace. Results showed that using \u03b1 = 0, \u03b2 = 1 always decreased performance, while \u03b1 = 1, \u03b2 = 0 remained consistent. The hidden layer was discarded in favor of soft prediction, as \u03b1 = 0, \u03b2 = 0 consistently achieved the best performance. Additional softmax loss was added to improve classification ability, resulting in improved accuracy for ResNet-50/2 and ResNet-50/4. Results in Table 5 show significant improvements in accuracy for ResNet-50/2 and ResNet-50/4 with the addition of softmax loss. The use of \u03b1 = 1, \u03b2 = 0 resulted in better performance, surpassing the teacher network. This study focuses on knowledge distillation in face recognition, extending the framework to non-classification tasks like face alignment and verification. Guidelines for target selection are provided to facilitate distillation on these tasks. Experiments on datasets like CASIA-WebFace, CelebA, and MS-Celeb-1M show the effectiveness of the proposed method in producing student networks that can outperform the teacher network with appropriate compression rates. A common initialization trick is used to enhance distillation performance in classification tasks, which also benefits non-classification tasks like face alignment. Simple tricks like this have been proven effective in experiments on CASIA-WebFace."
}