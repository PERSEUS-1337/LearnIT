{
    "title": "BJubPWZRW",
    "content": "Cross-View Training (CVT) is a method for deep semi-supervised learning that uses a teacher-student framework with multiple auxiliary student prediction layers. The students learn from the teacher by receiving soft targets and improve the quality of representations. When combined with Virtual Adversarial Training, CVT outperforms current state-of-the-art on semi-supervised CIFAR-10. CVT improves semi-supervised learning on CIFAR-10 and SVHN. It is also effective for training models on natural language processing tasks with unlabeled data, outperforming supervised learning and competing with the state-of-the-art. Deep learning classifiers benefit from large labeled datasets, but acquiring labels can be expensive. Semi-supervised learning techniques leverage unlabeled data during training, often using self-labeling methods where the model acts as both a \"teacher\" and a \"student\". Adding noise to the student's input has been successful in training the model to give consistent predictions to nearby data points. Cross-View Training (CVT) is a new method introduced to address issues with adding noise to the student's input in deep learning classifiers. Instead of only training the full model as a student, CVT adds auxiliary softmax layers to the model and trains them as students as well. Each student layer sees a restricted view of the input example, similar to cotraining, while the full model still serves as the teacher. The Cross-View Training (CVT) method adds auxiliary softmax layers to the model, allowing student layers to learn from the teacher's unrestricted view of the input. This approach improves model representations for CNNs, BiLSTM sequence taggers, and dependency parsers. The Cross-View Training (CVT) method enhances model representations for various tasks such as CNNs, BiLSTM sequence taggers, and dependency parsers by adding auxiliary layers. CVT, when combined with Virtual Adversarial Training, outperforms current state-of-the-art models on semi-supervised CIFAR-10 and SVHN datasets. Additionally, CVT shows significant improvements in semi-supervised models for tasks like English dependency parsing, named entity recognition, and part-of-speech tagging using unlabeled data from the 1 billion word language modeling benchmark. Semi-supervised learning methods, including CVT, are effective for handling discrete inputs like language. Early approaches involve pre-training neural models on unlabeled data for applications in computer vision and natural language processing. Recent work incorporates generative models like autoencoders and Generative Adversarial Networks. Self-training is a common approach where a classifier is initially trained on labeled data only, then labels some of the data in subsequent rounds. In each subsequent round of training, the classifier labels some unlabeled data and adds it to the training set. The model learns from noisy \"pseudo labels\" and is trained with soft targets for knowledge distillation. Consistency Training and Distributional Smoothing techniques add noise to the input to encourage distributional smoothness. Adversarial methods are used to select perturbations for input, improving the targets provided by the teacher. Our approach involves training a single unified model with auxiliary prediction layers that see different views of the input, leveraging unlabeled data through self-supervised losses. This method can be combined effectively with Co-Training and other approaches for improving model performance. Our approach involves training a single unified model with auxiliary prediction layers that see different views of the input, leveraging unlabeled data through self-supervised losses. This method can be combined effectively with Co-Training and other approaches for improving model performance. Unlike traditional approaches, our auxiliary losses are based on self-labeling rather than deterministically constructed labels from the input. Data augmentation, such as random translations or crops of input images, is similar to our method in exposing the model to different views of input examples. Our approach involves using a labeled dataset and an unlabeled dataset. We train the model with a cross-entropy loss on labeled data and add a consistency cost on unlabeled data to encourage distributional smoothness. The model imitates a teacher's prediction without back-propagating through it. Our approach involves using labeled and unlabeled datasets. We train the model with cross-entropy loss on labeled data and add a consistency cost on unlabeled data to encourage distributional smoothness. The model imitates a teacher's prediction without back-propagating through it. Our dependency parsing models use auxiliary layers similar to sequence tagging ones. Cross-View Training adds additional prediction layers produced by the model, with a distribution over labels outputted at test time. Different variants are proposed for CNN image classifiers, BiLSTM sequence taggers, and graph-based dependency parsers. The distances between the output distributions of the teacher and auxiliary students are added to the consistency loss, resulting in a cross-view consistency (CVC) loss. The supervised and CVC losses are combined into the total loss, which is minimized. We combine supervised and CVC losses in the total loss, minimizing it with stochastic gradient descent. \u03bb 1 and \u03bb 2 control the strength of auxiliary prediction layers and unsupervised loss. Noise or adversarial perturbation can enhance results, but L CVC can be trained without it. L consistency requires a nonzero \u03b7 to make student and teacher outputs different distributions. Our method adds little overhead to training time over consistency training as the auxiliary layers are only used during training. Image recognition models are based on Convolutional Neural Networks, producing feature vectors from images. The size of the feature vectors is denoted by d, with the first two dimensions indexing spatial coordinates. Feature vectors correspond to regions of the input image, with deeper CNNs using a \"region\" of the representations. The CNNs in the experiment use a \"region\" of representations from an earlier layer. The primary prediction layer takes the mean of H over the first two dimensions, resulting in a d-dimensional vector fed into a softmax layer. Auxiliary softmax layers are added to the top of the CNN, with each layer taking a single feature vector as input. Earlier layers in the CNN did not improve performance. In sequence tagging, an L-layer bidirectional RNN model is used, with each layer running an RNN like an LSTM in the forward direction. The bidirectional RNN model in sequence tagging uses multiple layers, each running an LSTM in both forward and backward directions. The model includes auxiliary softmax layers for predictions based on current and previous outputs of the RNN. The forward and future layers make predictions without the right context, resembling a neural language model. The neural graph-based dependency parser predicts the class of tokens in a sentence by forming a tree structure describing the syntactic relationships between words. It uses a BiRNN encoder and multilayer perceptrons to generate representations for head and dependent words, which are then fed into a bilinear classifier for classification. The neural graph-based dependency parser uses a bilinear classifier to produce probabilities for candidate edges based on representations generated by a BiRNN encoder and multilayer perceptrons. Cross-View Training is validated on two semi-supervised learning benchmarks. Cross-View Training is evaluated on semi-supervised learning benchmarks in image recognition and Natural Language Processing tasks using unlabeled data. The approach is tested on CIFAR-10 and Street View House Numbers datasets by utilizing provided labels for a subset of examples. The model incorporates a convolutional neural network with auxiliary softmax layers to improve performance. Cross-View Training is evaluated on semi-supervised learning benchmarks in image recognition and Natural Language Processing tasks using unlabeled data. The approach utilizes a convolutional neural network with auxiliary layers that optimize L with \u03bb 1 = 1 and each minibatch consisting of 32 labeled and 128 unlabeled examples. Results show that CVT works well as a semi-supervised learning method without any noise being added to the student, and performs close to VAT when random noise is added. Cross-View Training (CVT) is a method that requires only one backwards pass for each training minibatch, making it faster than Virtual Adversarial Training (VAT). When combined with VAT, CVT shows further improvements and state-of-the-art results. The benefits of CVT are reduced when data augmentation is applied, as random translations of the input provide similar advantages. However, gains on SVHN are smaller than CIFAR-10 due to the positioning of digits in the images. Incorporating auxiliary softmax layers into the supervised loss does not enhance results, indicating that CVT's effectiveness lies in its self-training mechanism rather than additional losses. The CVT model outperforms the VAT model on CIFAR-10 by learning meaningful representations for edge regions, as shown in Figure 5. The implementation by Miyato et al. produces slightly different results than reported in their paper. Error rates on semi-supervised learning benchmarks are shown in Table 1, with data augmentation applied for some datasets. The CVT model requires meaningful representations for edge regions to train auxiliary softmax layers effectively. The CVT model outperforms the VAT model by learning meaningful representations for edge regions, improving accuracies significantly. This indicates that CVT is enhancing the model's representations, particularly for the outside parts of images. In this section, CVT is successful in training semi-supervised models on NLP tasks such as CCG supertagging, text chunking, and named entity recognition using datasets like CCGBank BID19 and CoNLL-2000. The curr_chunk discusses the use of various NLP tasks such as Named Entity Recognition, Part-of-Speech Tagging, and Dependency Parsing. It mentions the datasets used for evaluation and the models employed, including a CNN-BiLSTM sequence tagging model. The model represents each word using a word embedding and character-level CNN output. The dependency parser in the curr_chunk utilizes a CNN-BiLSTM encoder and a MLP-Bilinear classifier for predictions. Despite experimenting with Virtual Adversarial Training, it was found ineffective for word-level tasks due to the discrete nature of words. The curr_chunk discusses the use of dropout in training a neural language model, along with the results of CVT on various tasks. It reports improved accuracies over the supervised baseline and achieving state-of-the-art results in CCG-supertagging and dependency parsing. The model does not include part-of-speech tags as input for dependency parsing. The model discussed in the curr_chunk does not include part-of-speech tags as input, unlike other works that have shown improved performance with them. Results for sequence tagging tasks are reported, with different training methods and the addition of language modeling as an auxiliary task. The model achieves high scores in dependency parsing tasks, but it is noted that other systems with access to more information during training also produce higher scores. The large TagLM model is competitive with the model discussed in the previous paragraph for Chunking and NER tasks, but reducing its size causes it to perform worse. Consistency-cost-based learning does not provide significant gains for NLP tasks, with consistency loss not improving sequence tagging and only slightly improving dependency parsing. CVT works well as a semi-supervised learning method for NLP. Training larger NLP models, especially using CVT, shows improved accuracy and scalability with model size. This suggests that semi-supervised learning methods can help develop more sophisticated models for natural language processing tasks with limited labeled data. The model for Chunking and Named Entity Recognition uses a BIOES tagging scheme and is trained with SGD with momentum. Word embeddings are initialized with GloVe vectors. The model architecture includes a 2-layer CNN-BiLSTM encoder and MLPs with hidden layers of size 512 and output layer of size 256. Dropout is applied to the hidden layer output. Punctuation is omitted from evaluation. Initial experiments explored the use of cross-view losses for purely supervised classifiers. Adding auxiliary softmax layers did not provide significant benefits as a regularizer on the model. Results slightly improved for sequence tagging on CCG and POS tasks but degraded on NER and Chunking. Image recognition with augmented models showed a slight decrease in performance on CIFAR-10 and CIFAR-100 datasets. Experimenting with L sup-cv instead of L sup on semi-supervised CIFAR-10 and CIFAR-10+ also resulted in decreased performance across various methods. These negative results suggest limited gains from the approaches tested. The gains from CVT in the semi-supervised setting are not due to additional prediction layers but from the improved self-training mechanism."
}