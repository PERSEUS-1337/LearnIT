{
    "title": "Syl5mRNtvr",
    "content": "Differentiable adversarial grammar model proposed for future prediction, modeling formal grammar with differentiable functions and latent representations for learning through backpropagation. Captures sequential structures with multiple possibilities from data. The adversarial grammar model can learn stochastic production rules from data distribution, enabling accurate future prediction in tasks like 3D human pose and activity prediction. It outperforms state-of-the-art approaches by efficiently modeling multiple plausible futures, crucial for tasks like robot action planning. The adversarial grammar model proposes a method for future prediction by learning sequential dependencies and predicting multiple possible outcomes through adversarial sampling. This model can output real-valued predictions like 3D human pose or semantic predictions such as activity classes. The use of grammar rules and adversarial sampling is crucial for capturing temporal structural dependencies and relationships between states. Adversarial sampling in learning grammar rules is crucial for generating multiple candidate future sequences efficiently. Unlike traditional grammar methods, this approach avoids enumerating all possible rules and can handle long sequences and multi-label settings. The model is driven by learning grammar rules and relationships to data symbols, along with adversarial losses to model data distribution over long sequences. The approach introduces adversarial grammar learning for generating multiple feasible long-term future predictions for high dimensional outputs. It outperforms previous methods like RNN/LSTM and memory-based approaches, predicting further into the future. The method is general and applied to tasks like 3D human pose prediction and activity forecasting on challenging datasets. Grammar models are used for visual data, drawing from Chomsky's concept of grammars in computational science. In visual data, grammars are utilized for parsing scenes and activity recognition. Various approaches have been proposed, including a grammar-based language for image description and MCMC-based inference. While some methods lack explicit grammar representation, others have used traditional stochastic grammar for activity prediction within a limited timeframe. Generative Adversarial Networks (GANs) are highlighted as a powerful tool for data generation through adversarial learning of data distribution. Generative Adversarial Networks (GANs) have been popular for image generation tasks and sequences generation. Differentiable rule learning and human pose prediction have also been explored in previous works. The use of recurrent models like RNNs or GRUs/LSTMs for long-term predictions in video sequences has been explored. Some models focus on learning spatio-temporal relationships of pose joints. Existing models lack the ability to handle multi-modality and ambiguity in predictions, producing only short-term results. Our approach in video prediction is related to adversarial formulations commonly seen in the literature. Our approach focuses on learning production rules of a grammar to predict transitions between continuous events in time, such as 3D human pose or activity. By using an adversarial mechanism, we can learn multiple candidate future sequences, enabling robust and realistic future prediction. This approach differs from recurrent models like RNNs or GRUs/LSTMs, as it can handle multi-modality and ambiguity in predictions for long-term results. The adversarial grammar model involves a set of non-terminals, terminals, and production rules to generate sequences. The objective is to learn latent representations from training data and model the production rules as a generative process. The proposed method involves learning a nonlinear function that maps non-terminals to sets of (non-terminal, terminal) pairs, enabling the modeling of multiple sequences. This function is applied recursively to generate grammar production rules for latent non-terminals, similar to recurrent methods like RNNs and LSTMs. The proposed method involves learning explicit production rules for latent non-terminals, enabling the modeling of multiple sequences. By learning these rules, more salient and distinct predictions can be made for long-term, complex output tasks with multiple possibilities. For example, rules like A \u2192 walkingA can generate continual 'walking' behavior, while other rules allow for activity switches. The model involves learning production rules for latent non-terminals to enable modeling of multiple sequences. Non-terminals act as memory capturing the current state with the Markov property. The model has a special structure with non-terminals and terminals learned. The input A is mapped to a vector r through nonlinear transformations, specifying a set of rules. The model learns production rules for latent non-terminals to model multiple sequences. Rules are shared globally but only a subset is selected for each non-terminal. Rule vectors are used to build grammar-like structures for modeling temporal dependencies. Gumbel-Softmax function allows stochastic selection of a rule based on learned probabilities, enabling generation of multiple outputs. The model learns production rules for latent non-terminals to model multiple sequences using learned probabilities. Nonlinear functions f T and f N are used to output terminal and non-terminal sequences. The schematic of G is visualized in Figure 1, with non-terminals and terminals modeled as high-dimensional vectors. The grammar model allows for multiple potential next non-terminals to be generated, providing more control over sequences compared to stochastic RNNs. It learns to generate the starting non-terminal from an initial input data sequence, enabling the generation of terminal symbols. The grammar model generates sequences of terminal symbols by learning to predict the starting non-terminal from an input sequence. It recursively applies a function G to generate possible sequences by generating sets of (non-terminal, terminal) pairs. Sampling G multiple times results in a variety of sequences, requiring an exponential number of sequences to be generated during training to cover all possibilities. The grammar model uses stochastic adversarial rule sampling to address the computational limitations of enumerating all possible sequences beyond a certain branching factor. This approach allows the model to generate realistic sequences by learning to sample the most likely rules for a given input, similar to GAN approaches. The model uses a function G as the generator and a discriminator function D in standard GAN training to generate realistic sequences. The discriminator predicts if a sequence is from the data or generated, ultimately producing the final output sequence. D is conditioned on both terminal and non-terminal sequences to learn the distribution of non-terminals implicitly. The discriminator function D is implemented with 1D convolutional layers for terminals and non-terminals, followed by a fully-connected layer for binary prediction. The discriminator and generator functions are trained jointly in GANs training to create an adversarial grammar. The optimization objective involves comparing generated sequences to real data distribution for loss computation during training. The model uses adversarial training to generate sequences matching the dataset distribution, enabling learning of longer sequences. Architectures include networks with fully-connected layers, a two-layer GRU module for pose, and sequential temporal for activity prediction. The model utilizes two sequential temporal convolutional layers for activity prediction, trained for 5000 iterations with gradient descent. Experiments show strong performance in future activity and 3D human pose prediction, handling multi-label datasets and predicting further into the future than previous methods. The method is tested for video activity anticipation, predicting activities up to 45 seconds ahead. The model predicts future activities up to 45 seconds ahead using video sequences as input, outperforming prior approaches. Evaluation is done using mean average precision (mAP) for activities occurring T seconds in the future. Multiple future sequences are generated, and the maximum mAP over 10 predictions is reported. Comparisons are made with predictions at different time intervals. The model predicts future activities up to 45 seconds ahead using video sequences as input, outperforming prior approaches on the MultiTHUMOS dataset. The evaluation is based on mean average precision (mAP) for activities occurring at different time intervals. Our approach, utilizing adversarial grammar, outperforms alternative methods like LSTMs in predicting future activities up to 45 seconds ahead. The gap in accuracy widens over time, with our model achieving 11.2 mean accuracy compared to 3.9 for LSTMs. By generating multiple outcomes and selecting the best one, our grammar model produces better long-term predictions on the Charades dataset. The Charades dataset consists of 9858 videos over 157 activity classes, demonstrating the ability to handle complex data. Adversarial grammar model provides more accurate future prediction than previous work, slightly outperforming grammar only. Charades is challenging for recognition and prediction tasks, with grammar only not feasible for high dimensional tasks. Multiple futures generated by the adversarial grammar are beneficial due to the dataset's diverse sequences. The approach is further evaluated on forecasting 3D human pose, a high dimensional structured output problem. The study focuses on forecasting 3D human pose, using the Adversarial Grammar model on the Human 3.6M dataset. The goal is to predict future 3D locations of 32 joints in the human body, using quaternions for joint representation. Predicting differences instead of absolute positions leads to more stable learning, allowing for longer horizon future sequences up to 4 seconds. The study uses the Adversarial Grammar model to forecast 3D human pose on the Human 3.6M dataset, predicting future sequences up to 4 seconds. Results outperform state-of-the-art methods, with confirmed accuracy in predicted poses for various activities. The study introduces a novel differentiable adversarial grammar for future prediction tasks, outperforming previous methods by generating sequences further into the future. The model is applied to activity prediction tasks with promising results, setting parameters for non-terminals and terminals based on the dataset. The code will be released for further research. The study introduces a novel differentiable adversarial grammar for future prediction tasks, outperforming previous methods by generating sequences further into the future. Parameters for non-terminals and terminals were set based on the dataset. The model was applied to activity prediction tasks with promising results. The dataset used had 65 instances in MultiTHUMOS and 157 in Charades. The 3D pose estimation involved 1024 non-terminals, 1024 terminals, and 2048 rules. Results for future 3D human pose prediction on the Human3.6M dataset are provided in Table 5. The study presents a novel differentiable adversarial grammar for future prediction tasks, surpassing previous methods by generating sequences further into the future. Results for future 3D human pose prediction on the Human3.6M dataset are provided, with examples of predicted 3D poses at different timesteps shown in Figure 5."
}