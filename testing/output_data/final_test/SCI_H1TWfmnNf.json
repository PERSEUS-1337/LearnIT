{
    "title": "H1TWfmnNf",
    "content": "We present a simple approach based on pixel-wise nearest neighbors to understand and interpret the functioning of state-of-the-art neural networks for pixel-level tasks. The main hypothesis is that convolutional neural networks for pixel-level tasks learn a fast compositional nearest neighbor synthesis/prediction function, supported by experiments on semantic segmentation and image-to-image translation. CNNs have revolutionized computer vision, showing impressive results for tasks like image classification and semantic segmentation. The development of machine learning systems aims to make feed-forward networks more interpretable and explanatory, especially in cases of failure or unseen data. One approach is explanation-by-correspondence, which involves interpreting results through case-based reasoning or historical examples. Our work demonstrates that deep networks can generate exponentially more explanations through composition, termed \"explanation-by-correspondence\". This approach provides detailed correspondence of parts of a query image to a set of exemplars, allowing for spatial prediction in computer vision tasks such as semantic segmentation and image translation. Our work proposes a non-parametric method to explain and modify the behavior of convolutional networks, including tasks like semantic segmentation and image translation. By visualizing image patches from training data, we can understand why networks generate certain artifacts in their output. The text discusses the use of an \"explanation-by-example\" approach to interpret convolutional neural network outputs. It includes visualizations of training exemplar images and a compositional nearest-neighbor operation to explain network behaviors in tasks like semantic segmentation and image translation. The text discusses using local convolutional neighborhoods to capture global semantics and local structure in image patches for non-parametric matching. It explains how networks generalize to new data through composition by synthesizing outputs from different training images. The text proposes a non-parametric method to explain and modify the behavior of convolutional networks using global and compositional nearest-neighbors. It aims to provide explanations for network behaviors by showing closest-matching training exemplar images. The text discusses a non-parametric method using global and compositional nearest-neighbors to explain and modify convolutional network behavior. It focuses on matching input patches to training images to understand errors and biases in the network. The text explains how networks generalize using composition and nearest-neighbors. It proposes a non-parametric method to modify convolutional network behavior, focusing on understanding errors and biases through matching input patches to training images. The curr_chunk discusses the process of creating output through nearest-neighbor operations by matching input patches to training images. It emphasizes the importance of using embeddings to ensure consistency in patch matching and how convolutional neighborhoods of feature activations play a role in this process. This method allows for explaining errors and adjusting biases in a network by changing the set of patches used for matching. The curr_chunk explains a non-parametric method for modifying convolutional networks to improve output quality, focusing on compositional nearest-neighbors to generate a larger set of outputs. It addresses issues like strange artifacts in generated images and the ability to match input patches to training images. The curr_chunk discusses using a compositional nearest-neighbor approach to improve output quality in convolutional networks by matching input patches to training images. It addresses issues like strange artifacts in generated images and the ability to segment objects accurately. The curr_chunk explains how local convolutional neighborhoods of feature activations can produce rich embeddings, allowing for error explanation and bias modification in networks. It introduces compositional nearest-neighbors as a method to improve network generalization by synthesizing output from different training images, resulting in a larger set of possible outputs. The curr_chunk discusses how image patches can be matched using embeddings from image classification networks to ensure global consistency in synthesizing outputs. These embeddings capture both local and global semantic knowledge, enhancing the accuracy of nearest-neighbor predictions in spatial prediction networks. The curr_chunk explores compositional NN matching as a mechanism for interpretation and demonstrates its potential accuracy compared to CNNs. It highlights the ability to manipulate implicit biases by changing the image set used for matching, allowing for targeted predictions in image generation networks. The Compositional Nearest Neighbors pipeline interprets and modifies Convolutional Neural Networks by demonstrating how CNNs memorize image patches and compose them into new configurations. Local embedding of image patches is crucial for efficient processing of potential outputs. This approach can predict activations of internal layers and generate spatial correspondences in state-of-the-art networks for image translation and semantic image segmentation. Spatial prediction in neural networks can be achieved through discriminative models like fully convolutional networks, used for tasks such as semantic segmentation and depth estimation. These models have shown improvement with deeper architectures. In the quest for better performance, advancements have been made in neural network architectures by employing deeper models, increasing capacity, and utilizing skip connections. However, the interpretability of these models for pixel-level prediction has been overlooked. This work focuses on interpreting encoder-decoder architectures for spatial classification and image generation using a Generative Adversarial Network (GAN) formulation. In this work, the Pix2Pix network is examined for its conditional image synthesis capabilities. Interpretability of GANs has been relatively unexplored compared to CNNs. Previous work has focused on interpreting CNNs, with approaches to understand and visualize intermediate layers. The recent work of PixelNN focuses on high-quality image synthesis using a two-stage matching process. In contrast, our focus is on interpretability rather than image synthesis, examining networks for discriminative features. Our focus is on interpretability rather than image synthesis, examining networks for discriminative features and a simpler single-stage matching process that does not require feed-forward processing. The design of part-based models, pictorial structures, and recent works using CNNs share a common theme of compositionality, with suggestions that CNNs also take a compositional approach. Compositional embeddings are seen as different from compositional objects/parts, with past work arguing that distributed representations are central. \"Past work argues that distributed representations are crucial for deep network success. CNNs outperform classic hierarchical part models by implementing sparse activations. An embedding perspective suggests linear combinations of activations are as informative as original activations. High-level activations may represent objects in the network.\" Our central thesis is that activations in convolutional networks do not correspond to individual objects/parts, but rather represent points in a local embedding space. The method introduced interprets fully convolutional networks for pixel-level tasks by treating classification networks as linear classifiers on nonlinear features extracted from the penultimate layer. The linear classifier parameters and feature encoder are typically trained on large-scale supervised datasets. The approach involves extracting embeddings for each pixel, finding closest matching patches in the training set using nearest neighbor search, and generating final output for label-to-image synthesis and image-to-label prediction. A nonparametric nearest-neighbor predictor is used for complex tasks like image captioning, performing well even with feature encoders. The NN classifier performs well even when feature encoders are trained for classification. Deep nets implicitly learn embeddings for linear classifiers to operate on. The predicted classification can be interpreted as a visual \"explanation\" of the prediction. Pixel-prediction networks extend this observation to predict labels for each pixel in an image. In pixel-level regression, a Pix2Pix network is trained to regress RGB values at each pixel location using filters of size 4 \u00d7 4 \u00d7 128. Nearest-neighbor regression is then performed to output pixel values, revealing spatial correspondences for each output pixel. In pixel-level regression, a Pix2Pix network is trained to regress RGB values at each pixel location using filters of size 4 \u00d7 4 \u00d7 128. Nearest-neighbor regression is then performed to output pixel values, revealing spatial correspondences for each output pixel. The explanation of errors is provided using distance functions like cosine distance, which consistently performed better in experiments. Visualizing a non-parametric approach to computing activations from internal layers involves matching to a training database of decoder 4 features from Pix2Pix to compute activations for the next layer with nearest-neighbors. In pixel-level regression, a Pix2Pix network is trained to regress RGB values at each pixel location using filters of size 4 \u00d7 4 \u00d7 128. Nearest-neighbor regression is then performed to output pixel values, revealing spatial correspondences for each output pixel. Global nearest-neighbors produce poor matches, but compositional-pasting matches from different exemplars produce nearly identical activations to those computed by the CNN. The compositional nearest-neighbor formulation is extended to internal convolutional layers, where activations at a spatial position can be computed using linear functions of features from the previous layer. In pixel-level regression, a Pix2Pix network is trained to regress RGB values at each pixel location using filters of size 4 \u00d7 4 \u00d7 128. Nearest-neighbor regression is then performed to output pixel values, revealing spatial correspondences for each output pixel. Global nearest-neighbors produce poor matches, but compositional-pasting matches from different exemplars produce nearly identical activations to those computed by the CNN. The compositional nearest-neighbor formulation is extended to internal convolutional layers, where activations at a spatial position can be computed using linear functions of features from the previous layer. Linearly combined activations with linear filters produce activation maps without explicit access to the filters. In the previous paragraph, a Pix2Pix network was trained for pixel-level regression using filters. The current paragraph explores the idea of using image patches to explain the behavior of subsequent layers in the network. By regressing final-layer pixel values from stored patches, it is possible to mimic the behavior of the entire network. Matching features at their native resolution before interpolation results in significant speed ups. This approach does not require explicit access to filters, as responses are encoded in the training dataset. The method produces reasonable activations without direct filter access. The current paragraph discusses generating final outputs using a compositional nearest-neighbor formulation for internal layers. It explores creating an \"output\" affecting field of neurons in spatial positions and layers. The approach aims to mimic the behavior of the entire network by regressing final-layer pixel values from stored patches. The paragraph discusses generating final outputs using a compositional nearest-neighbor formulation for internal layers. It focuses on creating an \"output\" affecting field of neurons in spatial positions and layers, aiming to regress final-layer pixel values from stored patches. The process involves selecting a central subset of pixels for linear projection and mapping feature positions to image patches on the final output. Training patches with features as data and corresponding labels are used to generate output results. The curr_chunk discusses the use of global descriptors for nearest-neighbor search and bias modification in the context of a network's associative memory. It highlights the benefits of using bottleneck features as a global descriptor to speed up runtime performance and the ability to modify the memory by changing training images or labels. The curr_chunk discusses refining training labels for smoother visual results and using self-supervised labels in associative networks. It also explores adding composition by matching features from different layers. In the curr_chunk, the study compares Pix2Pix results using original labels and \"self-supervised\" labels. The bottleneck layer serves as a fast global descriptor for matching images, but lacks compositionality. Matching convolutional embeddings from later layers allows for more compositional matches. In the curr_chunk, patch embeddings are used to prune the NN pixel search, improving run-time performance. The matching database serves as an \"associative memory\" that can be modified by changing training images or labels. Various modifications were experimented with, including refining training labels predicted by a network. Refining training labels predicted by a network, known as \"self-supervised\" learning, produces smoother visual results. This approach captures a more accurate representation of the network's internal memory. Using self-supervised labels, reconstructions can be generated by replacing the target label with the input image. Pixel embeddings contain sufficient local information for reconstruction. This method is helpful for image generation and discrete label prediction. The introduced nearest-neighbor embedding \u03c6 i (\u00b7) produces sufficient statistics for various pixel level tasks, such as predicting global class labels from images. The embedding reconstructs input queries and contains enough local information for pixel reconstruction. This method showcases the ability of the learned embedding and compositional matching framework. The introduced nearest-neighbor embedding \u03c6 i (\u00b7) provides sufficient statistics for pixel-level tasks like predicting global class labels from images. It reconstructs input queries and contains local information for pixel reconstruction, showcasing the learned embedding's ability and compositional matching framework. Standard loss functions in deep learning search for an embedding that minimizes label entropy given the representation \u03c6(x), indicating that \u03c6(x) serves as a sufficient representation rich in information to predict the label y. If the learned embedding satisfies the Markov assumption, the prediction y won't improve even with access to raw data x. Pixel-wise embeddings in spatial networks predict pixel-wise labels {y i} assuming conditional independence given {\u03c6 i (x)} and x. The pixel-wise features \u03c6 i (x) serve as a rich characterization of the image, capturing both global and local properties. The conditional independence assumption in Eq. (11) must be conditioned on the entire image x for extracting global properties. The factorization in Eq. FORMULA1 allows image synthesis by predicting labels based on the sufficient statistics \u03c6 i (x). The factorization in Eq. FORMULA1 allows image synthesis by predicting pixel values independently, suggesting a simple nearest-neighbor regression approach. Skip connections in neural networks break the Markov independence assumption, indicating that higher layer features may not be sufficient representations. However, Eq.(12) still holds if lower-level features are included in the concatenated representation. In Pix2Pix, \u03c6 i (x) \u2208 R N where N = 2048. In the previous section, image synthesis was discussed using a factorization approach. The current section focuses on the analysis of embeddings in CNN layers, comparing it to prior work and highlighting the use of off-the-shelf CNN layers for feature extraction. Theoretical analysis is presented to support the discussion. The current section analyzes embeddings in CNN layers, showing that optimal local representation is achieved through convolutional neighborhoods. Experimental results demonstrate the effectiveness of local embeddings in semantic segmentation and image synthesis tasks using SegNet and Pix2Pix networks. BID27 is a state-of-the-art network for conditional image synthesis and translation, evaluated on multiple datasets and tasks. It includes tasks like synthesizing facades from architectural labels, predicting segmentation class labels from urban RGB images, and synthesizing Google maps from aerial or satellite views. The CityScape BID13 and CamVid datasets are used for semantic segmentation tasks, with SegNet for CamVid sequences and Pix2Pix for the CityScape dataset. Results show that the compositional NN produces similar results to the networks, indicating the method's effectiveness. Our method enables a good interpretation of discriminative deep networks on pixel-level classification, as shown in the results of semantic segmentation on cityscape and CamVid datasets. The difference between images generated by generative networks and NN embedding is small, with some noise edges indicating the difficulty of image segmentation tasks. Our approach can explain the results from Pix2Pix, reproducing color and structure effectively. Our approach effectively reproduces color and structure from Pix2Pix outputs, including some artifacts. For architectural labels-to-facades, we used the BID27 dataset with 400 training images and 100 validation images. The Pix2Pix BID27 models were trained on 400 images for labels-to-facades and vice versa. Qualitative examples in FIG4 show the synthesis of real-world images using pixel-wise nearest neighbor embedding. The NN-embedding explains the generation of deformed edges and how the generative architecture memorizes patches from the training set. Satellite-to-maps dataset contains 1096 training images. The dataset contains training and testing images from Google Maps. Bias modification is demonstrated by altering the database to generate different facades. Nonparametric matching can control the output by modifying the database. The experiment demonstrates that image properties can be controlled by specifying exemplars in the database. The synthesized images maintain structure but change textural components. Additionally, the learned embeddings of a CNN enable image reconstruction in a compositional fashion. The experiment shows that image properties can be controlled by specifying exemplars in the database, with synthesized images maintaining structure but changing textural components. The reconstructions using the proposed compositional nearest-neighbors approach accurately reproduce the input scene, suggesting that CNNs understand input images by finding patches from training images to compose an image. This is the first approach to reconstruct an input image using a learned pixel-embedding. The correspondence map in FIG0 illustrates how patches from training images are cut-and-pasted to synthesize an output image, ensuring global consistency and local structure. Quantitative analysis in Table 1 shows that the pixel-wise nearest neighbor approach approaches the accuracy of the baseline CNN and outperforms global matching by up to 2X in semantic segmentation tasks. The self-supervised labels perform similarly to original labels but help with compositional matching and hurt global matching. Reconstruction using compositional nearest-neighbor approach shows better results than global nearest-neighbor approach. The learned embedding enables reconstruction of input and output images, containing relevant and semantic information. CNNs understand input images by finding patches from training images. Implementation details include using U-net for Pix2Pix, Tensorflow code for Pix2Pix and SegNet, and Eigen Library for cosine distance computation. Shortlisting global neighborhoods for Cityscape and CamVid datasets leads to faster computation. In this paper, a simple approach based on pixel-wise nearest neighbors is presented to interpret the functioning of convolutional neural networks for spatial prediction tasks. The analysis suggests that CNNs behave as compositional nearest neighbor operators and can generalize to novel data by composing local patches from different training instances. The average compute time per image is 22 minutes for Cityscape and 13 minutes for CamVid dataset. The study compares Compositional Nearest Neighbors (CompNN) to baseline CNN and global nearest neighbor approaches, showing CompNN performs slightly worse on average but outperforms in some cases. Compositional matching significantly outperforms global matching, with the last feature layer (Decode2) performing better than the intermediate Bottleneck layer. Self-supervised labels (SS) show overall performance. The study compares Compositional Nearest Neighbors (CompNN) to baseline CNN and global nearest neighbor approaches, showing CompNN performs slightly worse on average but outperforms in some cases. Self-supervised labels (SS) overall perform similarly to original labels (O), but help for compositional matching and hurt for global matching due to being overly-smoothed. The framework enables further analysis of convolutional networks from a non-parametric perspective. The study compares Compositional Nearest Neighbors (CompNN) to baseline CNN and global nearest neighbor approaches, showing CompNN performs slightly worse on average but outperforms in some cases. Using Decode2 features can generate more similar structures. Global NN approaches overall resemble global properties of the output of the Convolutional Neural Network (CNN) and of the CompNN approach. The global NN captures the organization of the scene because many labels overlap considerably with the output of the CNN and the ground truth. In this section, results of the Compositional Nearest Neighbors (CompNN) approach using self-supervised and original labels are compared with those of the Convolutional Neural Network. CompNN produces similar results to CNN, with smoother outcomes when using self-supervised labels. The self-supervised CompNN method yields results closer to CNN compared to using original labels. CompNN with self-supervised labels produces smoother image results compared to using original labels. The computational complexity of CompNN is high, but experimental details are provided to speed up the process. Several approaches are introduced to speed up the searching process in prediction tasks using a Convolutional Neural Network and Compositional Nearest Neighbors with self-supervised and original labels. The implementation includes using bottleneck features to narrow down the training set, utilizing multiple threads for faster processing, and synthesizing pixels efficiently. The synthesis of facades with the Facades dataset takes about 2 hours with 20-30 threads on the CPU, serving as a reference for experiments on other datasets."
}