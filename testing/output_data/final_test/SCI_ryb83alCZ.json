{
    "title": "ryb83alCZ",
    "content": "Deep generative models have improved semi-supervised classification, but their ability to derive discriminative features in an unsupervised manner for challenging real-world datasets has not been fully explored. A new deep hierarchical generative model using a mixture of discrete and continuous distributions is proposed to effectively separate data manifolds and is trainable end-to-end. The model's discriminative performance is tested on CLL diagnosis against computational FC baselines. The performance of Variational Autoencoders (VAEs) in unsupervised generative modeling and semi-supervised classification has been remarkable. However, there is limited investigation into the discriminative capabilities of purely unsupervised frameworks, with most works focusing on unsupervised clustering. Existing evaluations have been on benchmark datasets, lacking real-world data challenges. Deep generative models have not been tested on data sets with class imbalance. Can deep generative models be effective as unsupervised classifiers? A deep hierarchical generative model is proposed and evaluated on a challenging real-world data set. The model is trained unsupervised but performance is measured using labeled data, outperforming state-of-the-art baselines in the field. The contributions include a framework utilizing a hierarchy of continuous representations leading to a discrete variable for better classification results in imbalanced datasets. The model also allows for a controllable representation structure tailored to the task at hand, building upon techniques like Variational Autoencoders. In deep generative models, generation is achieved through top-down stochastic passes through multiple layers of latent variables. The model uses multi-layered perceptrons for nonlinear dependencies between layers. Inference is done through bottom-up stochastic passes. Optimization involves applying the reparameterization trick to each layer of stochastic latent variables. Models typically use Gaussian latent stochastic variables and optimize indirectly through discrete variables. The Gumbel-Softmax distribution and Concrete distribution were developed as continuous relaxations of discrete random variables to enable backpropagation. Sampling from a discrete distribution can be done using the Gumbel-Max trick, where samples are drawn from the Gumbel distribution. The Concrete distribution is a continuous relaxation of discrete random variables, allowing for backpropagation. It uses the softmax function to produce samples, with the reparameterization trick similar to Gaussian samples. As the temperature parameter approaches 0, samples from the Concrete distribution approximate one-hot samples from a Categorical distribution. Chronic lymphocytic leukemia (CLL) is the focus, being the most common form of leukemia in adults in Western countries. Flow cytometry (FC) is a key technique for diagnosing chronic lymphocytic leukemia (CLL) in adults. Minimal Residual Disease (MRD) is used to detect small amounts of leukemic cells post-treatment. The challenge lies in accurately separating data manifolds for CLL diagnosis. The challenge in diagnosing chronic lymphocytic leukemia (CLL) lies in separating the data manifolds of healthy and leukemic cells due to manifold overlap and other factors. Traditional clustering algorithms used in computational FC struggle to separate these manifolds effectively, leading to sensitivity to noise and cluster merging. Traditional clustering algorithms used in computational FC struggle to separate data manifolds of healthy and leukemic cells effectively due to sensitivity to noise and cluster merging. Significant hyperparameter tuning is required for interpretable clustering results, making the overall solution impractical and computationally expensive. An alternative approach could be learning a low-dimensional feature mapping with deterministic autoencoders and then performing clustering in feature space. This strategy may alleviate some problems but could lack expressiveness depending on the problem domain. A practical solution in computational FC involves learning low-dimensional feature mappings with models like VAEs and deep generative models. These models can effectively separate data manifolds and classify cells in an unsupervised manner, without the need for separate clustering algorithms. VAEs and deep generative models are suitable for generating new samples and learning global data structure. However, recognition models may only capture local information near training examples, hindering generalization. Using powerful generative models can further complicate the balance between generation and representation learning. The text discusses the balance between generation and representation learning in deep generative models like VAEs. It highlights the challenge of recognition models capturing only local information and proposes a framework with a hierarchy of continuous latent variables to address these issues and model latent factors affecting cell measurements for diagnosis. The text introduces a hierarchy of continuous latent variables to model latent factors affecting cell measurements for diagnosis. The generative model assumes non-linear functions computed by neural networks, with a prior on the discrete variable set to a uniform probability. The observation model is also defined. The inference model involves PCA plots of latent representations for different classes of the discrete variable y, encouraging the model to learn and separate different cell populations. Parameters are computed by neural networks, with a temperature parameter governing the relaxation of the discrete variable. The discrete variable y is tightly coupled with the latent factors of variation z, allowing for control over the structure of latent variables in the feature hierarchy. This relationship is illustrated in Figure 1. The model architecture involves latent variables for manifold learning, with covariances assuming a diagonal structure. A relaxed discrete variable is used to avoid marginalization, and a continuous surrogate loss is employed. Deterministic warm-up is adopted to induce more expressive latent codes and prevent collapse onto the prior. A linear annealing process is used for a lambda term during training. During training, a \u03bb term is introduced and linearly annealed from 0 to 1. Gradient estimations of the lower bound are biased towards the original discrete loss but unbiased and low-variance towards the surrogate loss. The reparameterization trick is used for both discrete and continuous variables concurrently to obtain low variance updates. Batch normalization is utilized to improve convergence time and training of upper layers in deep generative models. In experiments with deidentified patient data, the model's ability to learn useful representations is tested as an \"unsupervised classifier\". Comparison is made with popular baselines and generative models using confusion matrix-based metrics due to class imbalance. The study evaluates the model's performance using confusion matrix-based metrics like TPR, TNR, and MCC, with a focus on training difficulties and early stopping to retain optimal representations. MCC is considered a good measure for classifier performance on imbalanced datasets. In experiments on imbalanced datasets, the study uses MCC as a measure of classifier performance. The model's predictive capacity is evaluated based on correctly classifying cell measurements using a threshold of 0.5 for probabilities. The study compares the model's performance with three best-performing algorithms from previous research. The study evaluates clustering algorithms for rare population detection in computational FC. Baselines include X-shift based on kNN, Rclusterpp for hierarchical clustering, and flowMEANS for k-means with concave cell population identification. The model aims to separate healthy and pathological cells using more expressive representations during training. The proposed model for rare population detection in computational FC must have a mixture of continuous and discrete distributions. Two methods from the VAE literature are included for comparison, VAE+SVM and a version within the \u03b2-VAE framework. The \u03b2-VAE framework introduces a hyperparameter \u03b2 to force the model to learn more compressed and disentangled latent representations. The HCDVAE model focuses on learning efficient and disentangled representations for separating multiple data manifolds. Unsupervised classification is used as a proxy for manifold separation, with generative modeling showing merit in separating overlapping data manifolds. Results are presented in tables 1 and 2, with HCDVAE as the model. Baselines in the field of FC are clustering algorithms, which face traditional issues. Clustering algorithms face issues such as sensitivity to parameters and dependence on initialization schemes, leading to high variance across runs. Baselines perform well for healthy cell populations but struggle with pathological cells. Most algorithms prioritize either sensitivity or specificity, with Rclusterpp standing out for good overall performance. The supervised MLP classifier struggles to separate the two data manifolds. The supervised MLP classifier struggles to separate healthy and pathological cell populations, indicating overfitting. Generative modeling outperforms supervised classifiers for binary problems with imbalanced classes. VAE+SVM performance suggests limitations in vanilla VAE's ability to distinguish between healthy and pathological manifolds due to collapsed posterior and uninformative representations. Deep VAE architectures are also affected by this issue. The \u03b2-VAE and HCDVAE approaches show similar predictive performance, but HCDVAE's denser representation captures more explanatory factors in the data set compared to \u03b2-VAE's excessive compression. Both approaches visually collapse healthy cells into a cluster and separate pathological cells, remaining invariant to other data factors. The HCDVAE model achieves near human-level performance in both data sets, especially in the second case, by utilizing a mixture of discrete and continuous variables. This approach captures more explanatory factors in the data compared to \u03b2-VAE's excessive compression. The discrete variable representing cell state is largely invariant to perturbations in the lower latent layers. Previous literature has discussed models using mixtures of discrete and continuous variables for semi-supervised classification. The curr_chunk discusses a model that utilizes discrete and continuous variables for optimization, similar to the HCDVAE model. It emphasizes learning discriminative features for easier classification and enforces dependence between the variables. Another method presented in BID24 also uses discrete and continuous variables for semi-supervised learning to model natural clusters and class information. The curr_chunk discusses the use of discrete and continuous variables in clustering schemes, with various methods such as introducing Gaussian Mixture priors and tree-based priors in VAE models for unsupervised clustering and learning representations in videos. The model presented combines a discrete bipartite Boltzmann machine with binary units and a continuous component with multiple continuous layers. The \"discrete VAE\" learns object classes in images but is complex in architecture and relies on marginalization of discrete variables. Two data sets consist of 500,000 blood cell measurements, each represented by a 12-dimensional vector with markers, time stamp, and health label. The data sets used in the experiments contain pathological cells with protein markers for diagnosis. Patient identifiers are removed and the data is anonymized. The data is shuffled and split into training and test sets with a 75/25 split. Training is sped up by ensuring at least one pathological cell in every minibatch and reshuffling every 20 iterations. Early stopping with a threshold score of 0.5 for MCC is used to stabilize training. The model includes 3 layers of latent variables with 128, 64, and 32 units respectively. The inference and generative models use neural networks with hidden layers of 256 units each. The mean and variance parameters are computed using linear and softplus layers. Optimization is done with minibatch gradient descent using the Adam optimizer with an initial learning rate of 10^-4. The model architecture includes hidden layers of 256 units for both inference and generative models. Optimization is done with minibatch gradient descent using the Adam optimizer with a learning rate of 10^-4. The relaxation parameter \u03c4 is set at 0.66, and the \u03bb term is linearly annealed from 0 to 1. The architecture is shared between the \u03b2-VAE and HCDVAE models, with the VAE part having a Gaussian layer architecture."
}