{
    "title": "r1uOhfb0W",
    "content": "In this work, a two-stage method is proposed to learn Sparse Structured Ensembles (SSEs) for neural networks. The first stage involves running SG-MCMC with group sparse priors to draw an ensemble of samples from the posterior distribution of network parameters. In the second stage, weight-pruning is applied to each sampled network followed by retraining over the remaining connections. This approach not only improves prediction accuracy by enhancing exploration of the model-parameter space but also significantly reduces memory and computation costs in training and testing of NN ensembles. The effectiveness of learning SSE ensembles is demonstrated through experiments with FNNs and LSTMs. In experiments with FNNs and LSTMs, learning Sparse Structured Ensembles (SSEs) resulted in a 21% reduction in LM perplexity for LSTM based language modeling. This SSE consisted of 4 large LSTM models with 30% fewer parameters and 70% less computations compared to the baseline model. This work introduces a novel methodology for integrating SG-MCMC, group sparse prior, and network pruning for learning NN ensembles. Ensembles of Deep Neural Networks (DNNs) are gaining interest due to their robustness and accuracy compared to individual networks. The diversity in models obtained from stochastic optimization leads to improved prediction accuracy when predictions are averaged across multiple neural networks. The improved prediction accuracy of model averaging in neural networks can be explained through Bayesian inference. Bayesian model averaging is more accurate and robust compared to point estimates of model parameters. However, a practical issue hindering the use of DNN ensembles in real-world tasks is the requirement for an ensemble. In real-world tasks, the problem with using DNN ensembles is the high computation required for training and testing. Recent studies propose learning ensembles with multiple snapshot models using cyclic learning rates. This paper suggests using Stochastic Gradient Markov Chain Monte Carlo algorithms to reduce testing costs, along with group sparse priors for network connections and model pruning for compression. The paper proposes a two-stage method to learn Sparse Structured Ensembles (SSEs) for DNNs, using SG-MCMC and weight-pruning to reduce memory and computation costs while maintaining high prediction accuracy. Experimental results validate the effectiveness of the approach on FNNs and LSTMs for image classification tasks. The study conducts experiments on image classification using Feed-forward Neural Networks (FNNs) and language modeling with Long Short-term Memory (LSTM) on different datasets. The proposed method shows significant improvements in reducing model parameters and computations while maintaining performance across both tasks. This work integrates SG-MCMC for Bayesian posterior sampling, group sparse priors for network sparsity, and network pruning to improve ensemble learning. SG-MCMC algorithms like SGLD and SGHMC enhance exploration of model-parameter space for learning ensembles from large datasets. The SG-MCMC algorithm, such as SGLD, enhances exploration of model-parameter space for ensemble learning. Group Lasso penalty is used for network sparsity in Bayesian learning, but previous works focus on point estimates and not ensembles. Applying group sparse priors with SG-MCMC for ensemble learning has not been explored. Model compression techniques like model pruning and retraining have been studied for CNNs and LSTM models. Model averaging enables ensembles with heavily-pruned networks to be more robust in prediction. Efforts have been made to reduce training and testing costs for ensembles, such as using a cyclic learning rate schedule to visit multiple local minima during optimization. In contrast to empirical learning rate settings, SG-MCMC methods establish theoretical consistency in posterior sampling. Bayesian inference framework is applied to the classification problem, viewing a neural network as a probabilistic model. The posterior distribution over model parameters is computed, allowing for model averaging in testing. The posterior distribution over model parameters is computed using SG-MCMC methods like Stochastic Gradient Langevin Dynamics (SGLD) for model averaging in testing. This method works on stochastic gradients over small mini-batches, alleviating efficiency and scalability issues with traditional MCMC methods. SGLD is a method that calculates stochastic gradients over small mini-batches to efficiently sample the posterior distribution of model parameters. It uses annealing learning rates and Gaussian noise to update the parameters, overcoming issues in traditional MCMC methods. In practice, constant learning rates are suggested for SGLD as they improve mixing rate and exploration of parameter space. Constant learning rates outperform annealing rates in experiments. Sampling \u03b8 from the parameter updating sequence involves a burn-in process and a thinned collection of samples. The SGLD learning method outperforms other strategies like backward collection, with lower correlations between samples. Thinned collection is used as the default setting. Longer running allows better exploration of parameter space. Averaging more models improves performance, as shown in empirical results. Network pruning and retraining are done by removing connections with weights below a certain threshold. The network connections are pruned by removing weights below a threshold, leading to a reduced posterior. Retraining is then done for each pruned network to obtain an ensemble of networks. Pruning reduces model size and computation cost, and retraining improves ensemble performance. Retraining after pruning compensates for limited sample size in model averaging. The MAP estimate is more likely under the reduced posterior. Retraining increases posterior probabilities and improves prediction performance of the ensemble. Matrix calculations in DNN training/testing are commonly accelerated using GPU hardware. In Sparse Structured Ensembles (SSEs), structures are learned to reduce FLOPs in matrix calculations. The group Lasso regularization BID33 focuses on feature selection at a group level for structured sparsity.\u03b8 g represents a group of weights in \u03b8, with G being the number of groups. The group Lasso regularization focuses on feature selection at a group level for structured sparsity. It involves a hyperparameter \u03bb to balance group sparsity and loss function minimization. In Bayesian inference, the regularization term corresponds to a negative log prior term in the potential energy U(\u03b8). In this paper, the authors discuss learning sparse structured networks for Feed-forward Neural Networks (FNN) and Long Short-Term Memory (LSTM) models. They propose grouping strategies for FNN, where neurons with all zero outputs are pruned to reduce weight matrix size and lower FLOPs. The authors propose grouping strategies for FNN to reduce weight matrix size and lower FLOPs. For LSTM, removing an input or hidden unit is difficult due to their impact on all updating steps, but reducing the matrix size by removing a row or column is still beneficial. The authors propose grouping strategies for FNN to reduce weight matrix size and lower FLOPs. Specifically, they keep two index lists during pruning to record the remained rows and columns for each weight matrix, allowing for flexible updating of gate dimensions. The grouping strategy involves grouping each row and column for the four weight matrices separately. The LSTM updating formulas can be written as a matrix of dimension 2n by 4n, which is the concatenation of the four weight matrices. The authors propose grouping strategies for FNN to reduce weight matrix size and lower FLOPs. They introduce a tied W strategy for simplification and efficiency. Another strategy called Intrinsic Sparse Structures (ISS) is suggested for structurally sparse LSTMs. This approach reduces hidden size by grouping weights associated with a hidden unit. LSTMs learned by ISS can be easily reconstructed with a smaller hidden size. However, the embedding size is not reduced, leading to high computing costs. Two schemes are proposed to overcome this issue. The authors propose grouping strategies for FNN to reduce weight matrix size and lower FLOPs. They introduce a tied W strategy for simplification and efficiency. Two schemes are proposed to reduce the input size of the 1st LSTM layer and share weights between the embedding and softmax layers. Experimental results are presented for image classification and word-level language modeling tasks. The sparsity of a network is defined as the percentage of pruned weights, and FLOPs are calculated for matrices and networks. The authors propose grouping strategies for FNN to reduce weight matrix size and lower FLOPs. They introduce a tied W strategy for simplification and efficiency. Experimental results for image classification and word-level language modeling tasks are presented. The parameters and FLOPs in the tables are total sizes for all models in an ensemble. The baseline FNN-784-300-100-10 is used for classification on the MNIST dataset without additional tricks like dropout or batch normalization. Averaged results from 10 independent runs are reported in table 1. The baseline FNN-784-300-100-10 model is trained using Stochastic Gradient Descent (SGD) for 100 epochs with a learning rate that decays by a factor of 2 every 10 epochs. An ensemble of 18 independently trained networks reduces the test error rate from 1.66% to 1.49%. Using SGLD learning with Laplace priors results in a test error rate of 1.53%, slightly worse than the independently trained ensemble due to less accurate sampling. Adding L1 regularization could enforce sparse structure learning and allow for network pruning. After training the baseline FNN-784-300-100-10 model with SGD, an ensemble of 18 networks was created, reducing test error from 1.66% to 1.49%. Adding L1 regularization enforced sparse structure learning and allowed for network pruning. Two groups of experiments were conducted: one with SGLD+L1+PR achieving 90% sparsity without accuracy loss, and another with SGLD+GSP+PR achieving up to 96% sparsity and FLOP reduction without accuracy loss. The new method decreased test error from 1.66% to 1.29% compared to the baseline FNN. The new method decreases test error from 1.66% to 1.29% with 70% of parameters and 2.2\u00d7 computational cost for 18 networks. LSTM-based language modeling is tested on the Penn TreeBank corpus with a vocabulary of 10K words. The LSTM models in BID34 use 2 layers with 650 hidden units for medium and 1500 hidden units for large models. Word embedding size matches hidden unit size. Models are trained with dropout technique. Experiments show higher dropout keep ratios are needed when applying GSP regularization. GSP strength coefficients vary for untied and tied LSTM weight matrices, and for ISS case. The GSP strength coefficients for medium and large models are \u03bb = 3.0 \u00d7 10 \u22125 and \u03bb = 1.5 \u00d7 10 \u22125 respectively. Learning rates are fixed at 1.5 and 1.0 for SGLD training, and decay by a factor of 1.25 for retraining by SGD. Hyperparameter settings were found empirically via grid search. Results are organized into four parts: ablation analysis, effects of model samples and sampling strategies, sparse structures, and main results comparison. Model ablation results of SGLD training with different combinations of pruning, retraining, and GSP are shown in TAB2. Applying pruning or GSP alone leads to worse performance than vanilla SGLD for learning LSTM ensembles. However, when applied together, GSP forces the network to learn group sparse structures, leading to a better result. With GSP, applying pruning only results in a negligible loss of performance but greatly reduces the model size. The new method SGLD+GSP+PR reduces the PPL from 72.0 to 69.5 with only 10% parameters and 40% FLOPs in total, compared to a medium LSTM ensemble. SGLD provides a good approach to finding diverse sample models for ensembles. The performance gains of model averaging and PR in LSTM ensemble learning are shown in FIG3. Different sampling strategies and the relationship between ensemble performance and the number of models are examined. Testing on the PTB development set with ensembles of 2 to 35 medium LSTMs, results show that sampling with larger intervals and a relatively small number of models is better. The traditional ensemble learning method by SGD training of multiple models is inferior to the SGLD learning method. Sparse structured patterns of weight matrices from a single LSTM model sample in the ensemble trained by applying SGLD with GSP are shown in FIG4. Comparison of various models based on LSTMs on PTB dataset is summarized in TAB4, highlighting the trade-off between cost and performance with a model that requires fewer parameters and computational cost while decreasing perplexity. The study compared different training methods for LSTM models, showing that ensemble learning with SGLD outperformed traditional ensemble learning with SGD. The results indicated that SGLD reduced perplexity more effectively and with lower memory and computing costs. Additionally, comparisons between different ensemble training methods showed that SGLD and SGD achieved similar perplexity levels, but SGLD required less training and retraining epochs. The study compared different training methods for LSTM models, showing that ensemble learning with SGLD outperformed traditional ensemble learning with SGD. SGLD ensemble learning reduces training time by 30% compared to SGD. Applying SGLD+GSP+PR to models with shared embeddings further reduces perplexity to 62.1. Without shared embeddings, the lowest perplexity achieved is 66.4. The study proposes a novel method, SGLD+GSP+PR, for learning NN ensembles efficiently. The method integrates SG-MCMC sampling, group sparse prior, and network pruning techniques. It is easy to implement and effective in learning Sparse Structured Ensembles (SSEs) of FNNs and LSTMs. The SSEs learned using this method show better prediction performance with reduced training and test costs compared to traditional methods. Additionally, the method can produce SSEs that outperform baseline NN significantly without increasing model size and computation cost. Future works include interleaving model sampling and pruning, and exploring new applications of the method. The study introduces a new method, SGLD+GSP+PR, for efficiently learning NN ensembles through sampling and model pruning. This method is a powerful tool for learning ensembles and can be applied to various tasks."
}