{
    "title": "Hyp-JJJRW",
    "content": "Deep networks excel in classification tasks but often lose stylistic information. A new network combines classification and reconstruction by adding a \"style memory\" to the output layer. By training as a deep autoencoder, it minimizes both classification and reconstruction losses. The network's generative capacity produces good reconstructions when classification is accurate. Style memory is explored in relation to composing digits and letters. Deep neural networks now match human performance in complex tasks like image recognition. Deep neural networks excel in classification tasks but lack the feedback connections found in the mammalian cortex. Humans are less susceptible to being fooled by adversarial inputs compared to neural networks. In pursuit of research to enhance classification networks, the goal is to create networks capable of both recognition and reconstruction. Simply adding feedback connections will not suffice to generate specific examples of input. By combining features of a classifier network and an autoencoder network with a \"style memory\" added to the top layer, a network can have both classification and generative capabilities. Adding a style memory to the top layer of a deep autoencoder allows for classification and reconstruction capabilities. The network aims to minimize classification and reconstruction losses to perform both tasks effectively. Experiments with MNIST and EMNIST explore the properties of this style memory, which differs from other neural architectures that encode class and style for digit reconstruction. BID8 introduced bidirectional backpropagation for generative networks with feed-back connections. The network can choose a digit class at the top layer and render a corresponding image at the bottom layer. However, it only generates a generic sample of the class and does not reconstruct specific samples from the data. Previous work by BID4 and BID12 showed the importance of learning to generate images for image recognition. Restricted Boltzmann Machines (RBMs) have been effective for pretraining classifier networks. Autoencoders pre-trained in a greedy manner lead to better classifier networks. BID6 and BID0 use tied weights, which is not biologically feasible. Successful models like stacked denoising autoencoders BID16 reconstruct the original input image from a noise-corrupted input. BID13 maps inputs to a lower dimensional space by pre-training a network as RBMs and forming a deep autoencoder. Fine-tuning is done with nonlinear neighbourhood component analysis (NCA) to separate inputs with the same class. Our method involves a bidirectional network with a top layer containing a classification vector and class-agnostic style memory. The network includes input, convolutional, fully connected layers, and an augmented output layer with classifier and style-memory neurons. The network aims to minimize within-class distance in a lower-dimensional space while maintaining good reconstruction, similar to nonlinear NCA. Inference is performed using K-nearest neighbor in the lower-dimensional space. The augmented network includes a classification vector and style memory in the output layer. The style memory encodes specific input details, allowing for diverse renderings of a class. Training follows a standard deep autoencoder procedure. The network undergoes standard training for deep autoencoders, with small Gaussian noise added to the input. The top layer aims to minimize two loss functions: classifier loss (categorical cross-entropy) and reconstruction loss (Euclidean distance between input and its reconstruction). The objective is to find connection weights that minimize both loss functions in the last layer. The experiments in this paper used MNIST and EMNIST datasets with 28x28 pixel input dimensions. The network architecture includes two convolutional layers, two fully connected layers, and specific neuron configurations. The reconstruction loss weight was set to 0.05, and the network was trained using the Adam optimization method for 250 epochs. The network trained with Adam achieved high classification accuracy on MNIST and EMNIST test sets. Reconstructions show the network's ability to mimic original character styles. Examples in FIG2 display different styles of digits and letters. Softmax classification nodes and style memory interaction in misclassification shown in FIG4. The network trained with Adam achieved high classification accuracy on MNIST and EMNIST test sets, showcasing its ability to mimic original character styles. In a specific example, a digit '3' was misclassified as a '5' with 71% confidence, but correcting the softmax neurons to the correct label improved the reconstruction. This ability to identify misclassified inputs forms the basis of how these networks defend against adversarial or ambiguous inputs. The generative abilities of the classifier networks enable them to recognize misclassifications. The distance between digits in style-memory space differs from image space. Proximity in style memory shows various classes, while proximity in image space shares common pixels. For example, Fig. 7a has 18 '5' digits dominated by '0', while Fig. 7b has 13 '5' digits with fewer '0' digits. The image distance between different figures in style-memory space differs from image space. For example, in Fig. 7a there are 54 '0' digits, while in Fig. 7b there are only 25. Similarly, in Fig. 7c there are 76 '3' digits, while in Fig. 7d there are only 46. The style distance also varies between the figures. Additionally, in FIG6 there are differences in the number of 'S' and 'P' letters, with corresponding changes in image and style distances. These results demonstrate the successful separation of style memory. In an experiment, style memory successfully separates class information from data, allowing for gradual transformations between different styles of the same character class. Interpolation of style memories generates smooth transitions between images, as shown in FIG0. This within-class interpolation demonstrates that style memory effectively captures variations within classes. The results show that style memory captures information about how a digit was drawn, with examples of interpolations between incongruous letter forms. Transferring style memory between digit classes is possible, but reconstructions may not resemble characters. Classification networks focus on mapping inputs to classes and may not have enough information for sample generation. In this paper, the addition of \"style memory\" to the top layer of a classification network is proposed for generating samples. The top layer is trained to minimize classification error and reconstruction loss simultaneously. Experiments show that style memory encodes information different from the classification vector, allowing for unique image generation based on style interpolation. The study explores the generation of images in style-memory space, highlighting the potential of interpolating digits and letters within the same class. It suggests that incorporating perception as a two-way process can effectively guard against adversarial inputs, a novel approach not typically seen in existing defense mechanisms. Future work will delve deeper into this concept. The network's reconstruction is influenced by classification neurons and style memory. The study suggests creating a classifier network that uses its generative capability to detect misclassifications. Implementing predictive estimator networks for feedback loops is also considered. The experiments are ongoing."
}