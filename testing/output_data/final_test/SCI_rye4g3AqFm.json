{
    "title": "rye4g3AqFm",
    "content": "Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime. Various proposals have been made to explain this success, but there is no consensus on the fundamental reason why DNNs do not overfit. This paper offers a new explanation using a probability-complexity bound derived from algorithmic information theory (AIT), suggesting that the parameter-function map of many DNNs is biased towards simple functions. Evidence for this simplicity bias is provided in DNN models for Boolean functions, as well as in larger networks trained on CIFAR10 and MNIST. The intrinsic simplicity bias of deep networks helps explain their generalization on real-world problems. Deep learning is a machine learning paradigm based on large, expressive models that require large datasets to train. A novel PAC-Bayes approach is used with DNNs to guarantee good generalization for target functions. By estimating the marginal likelihood, tight generalization PAC-Bayes error bounds are produced, correlating well with true error on datasets like MNIST and CIFAR10. Deep learning models, known as deep neural networks (DNNs), have been successful in various domains like image recognition and natural language processing. Despite being overparametrized, they show good generalization performance, contrary to classical learning theory. However, a full theoretical understanding of their properties is still lacking. Deep learning models, such as deep neural networks (DNNs), have shown good generalization performance despite being overparametrized. Classical learning theory provides bounds on generalization error, but neural networks have high complexity, leading to trivial bounds. Various regularization methods have been proposed to lower complexity, but recent research has shown that they are not necessary for good generalization. Recent research has shown that regularization methods are not necessary for good generalization in deep learning models like deep neural networks (DNNs). DNNs can memorize a dataset quickly, raising questions about why they generalize well with uncorrupted training data. Various studies have explored this issue, but there is no consensus on the reason behind DNNs' strong generalization capabilities. Many argue that stochastic gradient descent (SGD) can exploit loss-function features for good generalization in deep neural networks (DNNs). However, there is no consensus on the extent to which SGD contributes to DNNs' generalization performance. Recent studies show that DNNs can generalize well even with other optimization methods like Adam or gradient-free methods, with only minor differences compared to SGD. The paper addresses the question of why deep neural networks (DNNs) generalize well despite being highly expressive and overparameterized. While stochastic gradient descent (SGD) is important for optimization, it is not the main source of generalization in DNNs. Arguments focus on the local curvature of the loss function's stationary points, with flatter minima associated with better generalization performance. Recent work has shown that flat minima can be transformed into sharp minima through parameter re-scaling, impacting generalization performance in deep neural networks. A study using an attack dataset demonstrated a correlation between the flatness of minima and generalization error rates. It was conjectured that differences in the flatness of minima could explain variations in the basin of attraction volumes for well-generalizing and poorly-generalizing solutions. In this paper, the authors suggest that differences in the flatness of minima in deep neural networks can be correlated with measures of descriptional complexity. They demonstrate that the probability of obtaining a function in DNNs varies significantly based on random parameter choices, which can impact the generalization performance of the network. The authors argue that the bias in random parameter choices impacts the generalization performance of deep neural networks. They show that the parameter-function map is biased towards simpler functions, leading to regularization and improved generalization. Gaussian processes are used to approximate the prior over functions, showing a strong match with DNN marginal likelihoods. The authors demonstrate that Gaussian processes can accurately model DNN marginal likelihoods for finite width networks. By shifting from priors over parameters to priors over functions, tight generalization error bounds can be obtained. The parameter-function map is defined for supervised learning models, highlighting the relationship between parameters and functions implemented by the model. The parameter-function map is crucial for algorithms like stochastic gradient descent, determining behavior in function space for generalization. Inspired by previous work, an upper bound for the probability of output x from input-output map g is derived, considering simplicity criteria and Kolmogorov complexity. The upper bound for the probability of output x from input-output map g can be derived based on simplicity criteria and Kolmogorov complexity. Conditions must be met for this bound to hold, including redundancy in the map. Statistical lower bounds can also be determined for P(x), showing that outputs x will typically be close to the upper bound upon random sampling of inputs. If there is bias, it will follow the derived equation. Empirical evidence supports the predictive behavior of this equation for various maps. The success of Eq. (1) in predicting various maps suggests that the parameter-function map of different DNNs may exhibit simplicity bias. To explore this, random neural networks are considered by putting a probability distribution over parameters \u0398. The distribution over functions induced by this distribution is estimated by counting the number of samples producing individual functions. The distribution over functions induced by a probability distribution over parameters is estimated by counting the number of samples producing individual functions using a neural network with specific architecture. The resulting probabilities are plotted to show a range of probabilities spanning orders of magnitude based on different parameter distributions. The rank plot of P(f) can be accurately fit with a normalized Zipf law, suggesting Zipf behavior over the whole rank plot. The distribution of functions is estimated by counting samples using a neural network, showing a range of probabilities spanning orders of magnitude. In a study on complexity measures, high-probability functions exhibit low descriptional complexity. Various complexity measures, including Lempel-Ziv complexity, show correlations with probability. Probability also correlates with the size of the smallest Boolean expression and sensitivity of output to input changes. Different measures show correlations and differences in their abilities. Uniform distribution with variance of 1/ \u221a n was used for analysis. Real-world functions are expected to have low descriptional complexity, leading to a simplicity bias in deep neural networks. However, these networks do not generalize well for complex functions. The number of high complexity functions is exponentially larger than low complexity ones, but they may be less common in real-world applications. The exponential probability-complexity bias is supported by arguments from Dingle et al. (2018). The study by Dingle et al. (2018) suggests an exponential probability-complexity bias for larger neural networks. They tested this bias by sampling networks parameters for a 4 layer CNN on a random sample of 1000 images from CIFAR10. The results showed a correlation between network complexity and probability, indicating a simplicity bias in more realistic DNNs. Additionally, they explored the link between bias and generalization using the PAC-Bayes theorem. Langford & Seeger (2001) tightened the bound in Theorem 1 for any distribution P on any concept space and any distribution D on a space of instances. With probability at least 1 \u2212 \u03b4 over the choice of sample S of m instances, all distributions Q over the concept space satisfy the generalization error and empirical error conditions. In the realizable case, an algorithm can achieve zero training error and sample functions with a weight proportional to the prior. The posterior distribution Q(c) minimizes the general PAC-Bayes bound, simplifying to the marginal likelihood of the data. Corollary 1 tightens the original bound for Bayesian binary classifiers, especially in the realizable case where DNNs often reach 100% training accuracy. In the context of Bayesian binary classifiers, the posterior distribution Q(c) minimizes the general PAC-Bayes bound, simplifying to the marginal likelihood of the data. Under the assumption that D is realizable, additional concepts in H consistent with the sample S are considered. Stochastic gradient descent is assumed to sample the zero-error region close to uniformly, with further discussion on the training algorithm validity in Section 7. The bias observed in the parameter space affects which functions SGD finds. The large variation in function probability correlates with basin volume, explaining similar results with GD and SGD. A Gaussian distribution with a large variance is used, and the choice of variance is further discussed. The PAC-Bayes approach requires a method to calculate P(U). The PAC-Bayes approach requires a method to calculate P(U) for large systems. Infinitely-wide neural networks are equivalent to Gaussian processes when parameters are distributed i.i.d. The real-valued outputs of the neural network for any finite set of inputs are jointly distributed with a Gaussian distribution. The kernel function depends on the architecture and properties of the weight and bias variances. The kernel for fully connected ReLU networks has a known analytical form called the arccosine kernel, while for convolutional and residual networks it can be efficiently computed. The main quantity in the PAC-Bayes theorem, P(U), is the probability of a given set of output labels for the training set instances, known as marginal likelihood. For binary classification, these labels are related to the network outputs via a nonlinear function. The distribution of the output labels no longer follows a Gaussian form due to the output nonlinearity. The text discusses the approximation of P(U) for neural networks using Gaussian processes. By sampling from a fully connected neural network and a Gaussian process, the agreement between the probabilities is shown to be good even for small input sets. The expectation-propagation (EP) approximation in GPy is used to calculate P(U) with GPs, which is more accurate than the Laplacian approximation. The text discusses the comparison of EP and Laplacian approximations in GPy for neural networks. Results show good correlation with empirical frequencies, with a less than 10% difference in log-likelihoods. Generalization error bounds were tested on various networks, showing an increase with randomized labels in CIFAR10. The text discusses generalization error bounds on different datasets, showing an increase with randomized labels in CIFAR10. Results indicate that MNIST and fashion-MNIST are similarly hard, while CIFAR10 is considerably harder. The PAC-Bayes bound closely follows the trends in mean generalization error. The text discusses the generalization error bounds on different datasets, showing an increase with randomized labels in CIFAR10. Results indicate that MNIST and fashion-MNIST are similarly hard, while CIFAR10 is considerably harder. The PAC-Bayes bound closely follows the trends in mean generalization error. The assumption that SGD samples parameters close to uniformly within the zero-error region is tested, with evidence suggesting that Bayesian sampling of neural network parameters produces similar generalization performance to SGD-trained networks. The text discusses the probability of finding functions consistent with the training set using two methods: training the neural network with variants of SGD (advSGD and Adam) and Bayesian inference using Gaussian processes. The probability is estimated for learning a target Boolean function of LZ complexity 84.0. The text discusses the probability of finding functions consistent with the training set using SGD-like algorithms and approximate Bayesian inference. Results show close agreement between the two methods, indicating that SGD may behave similarly to uniform sampling of parameters. Further research is needed to explore potential divergence of SGD from Bayesian parameter sampling. The paper presents an argument on generalization in highly overparameterized DNNs based on PAC-Bayes theory. The text discusses the bias towards true distribution in highly expressive models like neural networks, regardless of the number of parameters. Real-world problems are expected to have a biased prior towards the right class of solutions. Average probability of finding a function using SGD and Gaussian process approximation is compared for a fixed target Boolean function. The text discusses Gaussian process parameters and the bias in the parameter-function map. It demonstrates how to make the approach quantitative by approximating neural networks as Gaussian processes. The approach is not yet able to explain the effects of different tricks used in practice on generalization. The paper aims to explain the generalization in highly overparametrized regimes, which classical learning theory predicts to be poor. It is still uncertain if the approach can extend to explain practical tricks and methods beyond neural networks. The main potential sources of error for the bounds are discussed, including the probability of finding a specific function during training and the modeling of neural networks with Gaussian processes. The paper discusses generalization in overparametrized regimes, showing that Gaussian processes model neural networks well. Expectation-propagation approximates the Gaussian process marginal likelihood effectively. The bounds predict true error behavior accurately, supporting the approach despite the need for further validation. The agreement of the bounds provides evidence for the claim that bias in the parameter-function map drives generalization. Further research can enhance the results significantly. In the main experiments, two classes of architectures were used: Fully connected networks (FCs) with varying layers and Convolutional neural networks (CNNs) with 200 filters. Training was done with SGD using a learning rate of 0.01 and early stopping at 100% accuracy on the training set. In the experiments, a variation of SGD called adversarial SGD (advSGD) was used to train a smaller neural network for learning Boolean functions. This method, proposed by Ian Goodfellow, was successful in finding solutions with 0 training error for most cases, unlike traditional SGD. In advSGD, classification error for each training example in a mini-batch is computed after every training step. In the experiments, adversarial SGD (advSGD) was used to train a smaller neural network for learning Boolean functions. A moving average of classification errors is updated for each training example in the mini-batch. The scores are passed through a softmax to determine the probability of each example being included in the mini-batch. Adam could learn Boolean functions with a smaller neural network using mean squared error loss function. Bernoulli likelihood with a probit link function was used to approximate the true likelihood. In the experiments, adversarial SGD was used to train a smaller neural network for learning Boolean functions. The network has 2 fully connected hidden layers of 784 ReLU neurons each. The empirical frequency of labellings for a sample of 10 random MNIST images was compared using different methods to approximate the marginal likelihood of the Gaussian process corresponding to the neural network architecture. The Laplacian and expectation-propagation approximations were compared. The variance hyperparameters have a significant impact on the bound for neural networks. Different choices of variance affect the bound for MNIST and fashion-MNIST. The weight variance plays a crucial role, especially for fully connected networks. Results align with previous studies showing the transition between phases at \u03c3 w \u2248 1.0. The PAC-Bayes bound behavior is best near the phase transition for CIFAR10. Convolutional networks exhibit sharper transitions with weight variance. In neural networks, weight variance has a significant impact on the bound. Sharper transitions are observed with weight variance in convolutional networks. The best choice of variance corresponds to the Gaussian distribution that approximates the behavior of SGD. Variance values above the phase transition were chosen for experiments, leading to significantly worse bounds when using smaller variances. This suggests that SGD may be biased towards better solutions in parameter space. SGD bias towards better solutions in parameter space may be a source of error. Understanding the relation between SGD and Bayesian parameter sampling is crucial for progress in this area. The PAC-Bayes bound's dependence on variance hyperparameters is illustrated in Figure 6. In Section 7, experiments comparing training with SGD and approximate Bayesian inference (ABI) were conducted. The average probability of finding a function f was analyzed over multiple training sets. For SGD-like algorithms, P(f|S_i) was approximated by the fraction of times f was obtained from M random runs. For ABI, the probability was calculated using a prior probability P(f). In Section 7, experiments compared training with SGD and approximate Bayesian inference (ABI). The probability of finding a function f was analyzed using different methods. Lower variance hyperparameters for the Gaussian process resulted in worse correlation. The ABI method approximates uniform sampling on the zero-error region. The main text discusses the simplicity bias phenomenon in neural networks, referencing Dingle et al. (2018). It outlines criteria for a map to exhibit simplicity bias, including limited complexity and more inputs than outputs. The map f should have more inputs than outputs to allow for significant variation in output probabilities. It must be nonlinear to avoid bias, and should not primarily produce pseudorandom outputs. In the context of deep learning systems, the inputs are parameters that fix weights for a neural network, while the outputs are the functions produced. The map f described in the context of deep learning systems grows in complexity with the input layer size, but can be represented with low complexity by reading parameters, setting up a neural network, and evaluating it. The information needed to describe the map grows logarithmically with input dimension n, making it simpler than a typical function. The Kolmogorov complexity of this map is smaller than the output complexity, meeting the map simplicity condition. The redundancy condition depends on network architecture and discretization, typically satisfied for overparameterized networks using floating point numbers for parameters. Neural networks satisfy conditions related to simplicity bias, including using floating point numbers for parameters and Boolean functions for outputs. The non-linear parameter-function maps of neural networks also meet the simplicity bias criteria. The lack of high probability and high complexity functions, as well as the inability to predict outputs of a good pseudorandom number generator, provide empirical validation for simplicity bias. The simplicity bias framework relies on well-chosen complexity measures to approximate Kolmogorov complexity for most outputs. Identifying a suitable complexity measure is crucial for practical application. Different complexity measures have shown consistent simplicity bias behavior, indicating robustness in the framework. Further research is needed to determine the conditions under which these assumptions hold. The Lempel-Ziv complexity measure, based on finding regularities in binary strings, is used to extend experiments in the main text. It is asymptotically optimal and equivalent to Kolmogorov complexity for an ergodic source. The variation of Lempel-Ziv complexity from Dingle et al. is based on the 1976 Lempel Ziv algorithm, using a dictionary to compress output. The Lempel-Ziv complexity measure, based on finding regularities in binary strings, is used to extend experiments in the main text. N w (x 1 ...x n ) is the number of words in the LempelZiv \"dictionary\" when it compresses output x. The symmetrization makes the measure more finegrained, and the log 2 (n) factor as well as the value for the simplest strings ensures that they scale as expected for Kolmogorov complexity. This complexity measure is the primary one used in the main text. The binary string representation depends on the order in which inputs are listed to construct it, which may affect the LZ complexity. The entropy is a fundamental measure of complexity defined as DISPLAYFORM1 N , where n 0 is the number of zeros, n 1 is the number of ones, and N = n 0 + n 1 . The complexity measure for Boolean functions involves minimizing them into a simpler form using the Quine-McCluskey algorithm. The number of operations in the resulting expression is defined as a Boolean complexity measure. This measure is compared to entropy and K LZ, showing a correlation between low entropy and low K LZ. The goal is to compress Boolean functions by finding simpler representations. The complexity measure for Boolean functions introduced by L. Franco et al. captures the difficulty of learning and generalizing functions. Simple functions were found to generalize better in a neural network. The measure includes terms that measure the average fraction of neighbors changing the output at different Hamming distances. The first two terms are used in the measure, known as average sensitivity and generalized robustness. The Hamming distance is a critical sample ratio used to measure the complexity of a function. It is defined as the fraction of inputs that have another input at Hamming distance 1, producing a different output. Different complexity measures are compared in Fig. 9, showing that generally more functions are found with higher complexity. Probability versus complexity plots are shown in FIG1 for other complexity measures, with similar behavior to the LZ complexity measure. In FIG1, probability versus LZ complexity plots are shown for various parameter distributions. The complexity of the target function affects learning, compared to random guessing. Neural network learning is compared to an \"unbiased learner\" with the same hypothesis class. Functions in the experiments were randomly sampled parameters of the neural network, not fully random. When training neural networks on truly random functions, generalization errors are equal or higher than those of an unbiased learner. This is in line with the No Free Lunch theorem, which states that no algorithm can outperform others uniformly over all functions. The correlation between LZ complexity and generalization is not solely due to function entropy. Non-random strings can have maximum LZ complexity, as LZ complexity is less powerful than Kolmogorov complexity. Neural networks perform well for non-random functions, even with maximum LZ complexity, suggesting a bias towards simplicity capturing a stronger notion of complexity. Results show that simpler functions are better captured by more powerful complexity measures than entropy, with generalization errors varying even when target function entropy is fixed. The generalization error of learned functions shows considerable variation and a positive correlation with complexity. Entropy is shown to be a less accurate complexity measure compared to LZ or generalization complexity for predicting generalization performance. Despite fixed entropy, there is still variation in generalization error, which correlates with function complexity. The complexity of learned functions has a positive correlation with generalization error. Entropy is not as accurate as LZ or generalization complexity in predicting performance. The number of layers in neural networks affects the complexity of functions, with higher layers showing increased complexity due to network expressivity. The expressivity of neural networks does not significantly change with increasing complexity, but rather shifts towards lower complexity. A neural network of shape (7, 40, 40, 1) with sample sizes N = 10^6, 10^7, 10^8 shows finite-size effects causing higher frequency functions to have a bias. Deep neural networks generalize well due to their heavily biased implicit prior over functions, as supported by PAC-Bayes theory. Models that perform poorly relative to deep learning exhibit little bias, connecting to the curse of dimensionality. In complex machine learning tasks, models like Gaussian processes and kernel methods were used before deep learning. These models, while infinitely expressible, showed poor generalization in high-dimensional input spaces due to little bias. The Gaussian kernel in machine learning ensures high correlation within a radius \u03bb, dividing space into regions of length scale \u03bb. In binary classification, the kernel prefers functions with 2^d possibilities, biased towards functions with probability close to 2^(-a*d). The Gaussian kernel in machine learning is biased towards locally continuous functions, but in high dimensions, it is not biased enough. As dimensionality increases, PAC-Bayes-like bounds grow exponentially, leading to vacuous results. Empirical studies show that deep neural networks can generalize even with randomly labeled data, with generalization error linked to label randomization. In experiments with smaller neural networks, Franco (2006) introduced a complexity measure for Boolean functions called generalization complexity, which correlates well with generalization error. Arpit et al. propose that neural networks prioritize learning simple patterns first, supported by experimental evidence. They suggest that SGD may be the reason for this implicit regularization, as it converges to minimum norm solutions for linear models. However, the question of its impact on nonlinear models remains open. Full-batch gradient descent also generalizes well, indicating that SGD may not be the main cause of generalization. The bias towards simplicity in neural networks is a strong factor in implicit regularization, supported by historical principles like Occam's razor. Various learning theories, such as MDL and Blumer algorithms, emphasize simplicity in hypotheses. Non-uniform learnability, like MDL, predicts data-dependent generalization by minimizing structural risk. The generalization error in MDL is linked to target function complexity, with a bias towards simplicity. Neural networks may approximate universal induction due to their parameter-function map. Nonvacuous bounds have not yet been achieved in these approaches. The idea of flat minima in neural networks is linked to generalization, as discussed by Hochreiter and Schmidhuber. While flatness correlates with generalization, it is not the only factor, as sharp minima can also generalize. Simple functions have larger regions of parameter space producing them, leading to flat minima. Other studies have also explored the parameter-function map in neural networks. Montufar et al. (2014) suggested studying the size of parameter space producing functions of certain complexity for neural networks. Poole et al. examined sensitivity to small perturbations of the parameter-function map in BID6. There is still much to explore in understanding the properties of the parameter-function map for neural networks. Other works applying PAC-Bayes theory to deep neural networks include Dziugaite & Roy (2017) and Neyshabur et al. Dziugaite and Roy's work computes non-vacuous bounds but their networks differ from practical ones. In contrast to previous PAC-Bayes work, this study focuses on the prior over functions rather than parameters, resulting in tighter bounds. The relationship between KL divergence in parameter and function space is explored, showing that bounds using distributions in function space can be more precise. The less-commonly used PAC-Bayes bound from McAllester (1998) is utilized in this analysis. Our work utilizes the less-commonly used PAC-Bayes bound from McAllester (1998), which assumes the Bayesian posterior, known for providing tight bounds. Additionally, we contribute to the exploration of random neural networks as a means to understand fundamental properties of neural networks."
}