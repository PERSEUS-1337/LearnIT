{
    "title": "H1l0O6EYDH",
    "content": "Some conventional transforms like DWHT and DCT are used in image processing but not in neural networks. These transforms can capture cross-channel correlations without learnable parameters. Applying these transforms on pointwise convolution reduces computational complexity without sacrificing accuracy. DWHT, in particular, requires only additions and subtractions, reducing computation overheads significantly. Its fast algorithm further decreases complexity from O(n^2) to O(nlog n), making networks more efficient in terms of parameters and computation. The DWHT-based model showed a 1.49% accuracy increase with significantly reduced parameters and FLOPs compared to the baseline model on the CIFAR 100 dataset. Large CNNs and NAS-based networks have achieved high accuracy but are limited in mobile applications due to their high computational cost and memory constraints. Depthwise separable convolution, proposed by Howard et al. (2017) and others, offers parameter and computation efficiency for mobile devices with low memory space and real-time computation needs. This approach breaks down standard convolution into depthwise and pointwise convolutions, reducing parameters and FLOPs while maintaining accuracy. This technique is widely used in modern CNN models. The existing Pointwise Convolution (PC) layer in modern CNN architectures is computationally expensive and occupies a significant proportion of weight parameters. This paper proposes a new PC layer using non-parametric and fast conventional transforms like Discrete Walsh-Hadamard Transform (DWHT) and Discrete Cosine Transform (DCT), which have been widely used in image processing but not in CNNs. These transforms do not require any learnable parameters but show the ability to capture features effectively. The proposed CNN models utilize non-parametric properties of transforms like DWHT to reduce computational complexity and parameter count, leading to efficient distributed training and less communication. DWHT is a good replacement for PC layers as it requires no floating point multiplications, only additions and subtractions, reducing computation overheads. Its fast version further reduces computation complexity from O(n^2) to O(n log n), making neural networks extremely efficient in terms of parameters and computation. The proposed CNN models utilize transforms like DWHT to reduce computational complexity and parameter count, making neural networks efficient. A new PC layer with conventional transforms is introduced to decrease floating point operations and computation complexity. ReLU after conventional transforms is found to reduce accuracy, leading to the proposal of an optimal computation block. Conventional transforms are effective for extracting high-level features in neural networks. The proposed method utilizes DWHT to reduce parameters and FLOPs in neural networks, showing an accuracy gain compared to MobileNet-V1 on CIFAR 100 dataset. Various approaches have been suggested to reduce computation complexity in convolution methods, such as factorizing large kernels into smaller ones and introducing learnable spatial displacement parameters for flexibility. The active shift layer proposed by Jeon & Kim (2018) replaces spatial convolutions with shift operations, reducing computation complexity in lightweight CNN models like MobileNet-V1. Previous attempts include ShuffleNet-V1 by Zhang et al. (2017b) which decomposes features into groups over channels for PC operation, aiming to reduce computational cost. Our work aims to reduce computation complexity and weight parameters in convolution layers by finding mathematically efficient algorithms. Quantization in neural networks, such as 8-bit and 16-bit representations, has been shown to speed up inference with minimal accuracy loss. In the quest for efficiency, researchers have explored various methods such as 16-bit fixed point representation, weight pruning, quantization, and binarization of neural networks. These techniques aim to reduce computation complexity and weight parameters while maintaining accuracy in inference tasks. Our work utilizes binary fixed weight values similar to Local Binary CNN (Juefei-Xu et al., 2016), but overcomes limitations of Local Binary Patterns in CNN applications. Discrete Cosine Transform (DCT) is highlighted as a powerful feature extractor for image processing. The Discrete Walsh Hadamard Transform (DWHT) is a fast and efficient transform using binary elements for feature extraction in various applications like texture image segmentation, face recognition, and video shot boundary detection. DWHT operates without multiplication, making it suitable for fast processing. The Discrete Walsh Hadamard Transform (DWHT) utilizes a structured-wiring-based fast algorithm for efficient encoding of spatial information. A new PC layer is proposed using conventional transforms, with a focus on reducing computation complexity in neural networks. The basis kernel matrix of DWHT is defined recursively, resulting in an extremely fast and efficient neural network. The proposed method replaces learnable parameters with bases from conventional transforms to create new PC layers, utilizing DWHT and DCT basis kernels. Normalization factors are not applied in the proposed PC layer as Batch Normalization handles this. The benefit lies in utilizing fast algorithms of existing transforms for reduced computation complexity. By utilizing fast algorithms, the computational complexity of the PC layer can be reduced from O(N^2) to O(N logN) without affecting the results. The proposed fast PC layer uses DWHT in Algorithm 1, where even-indexed and odd-indexed channels are manipulated in each iteration. This results in a complexity of O(N logN) for additions or subtractions, compared to the existing O(N^2) complexity for multiplication. Our method offers a cheaper and more efficient alternative to the conventional PC layer, reducing computation costs and power consumption. DCT, like DWHT, can be computed quickly by breaking down the input sequence into subproblems. DCT utilizes cosine basis kernels for better feature extraction, but requires multiplications and a look-up table. In contrast, DWHT kernels consist of only +1 and -1, allowing for a multiplication-free module. The structured-wiring-based fast DWHT algorithm offers a more efficient alternative to DCT in the PC layer, with kernels consisting only of +1 and -1. Memory access towards kernel bases is not needed with this algorithm, and padding zeros along the channel axis ensures a general formulation of the PC layer. The architecture of the fast DWHT algorithm ensures a receptive field of N for each output channel. The structured-wiring-based fast DWHT algorithm offers an efficient alternative to DCT in the PC layer, with kernels of +1 and -1. The architecture ensures a receptive field of N for each output channel, reflecting input channel correlations through log 2 N iterations. To integrate the new PC layer into neural networks, optimal block search and insertion strategies were explored hierarchically. Experiments evaluated the effectiveness of these non-learnable transforms in neural networks. The effectiveness of networks is evaluated based on accuracy fluctuation with changes in learnable weight parameters or FLOPs. Experimental settings include 128 batch size, 200 training epochs, 0.1 initial learning rate, and 0.9 momentum with 5e-4 weight decay. Model accuracy is averaged from three training results. Input channels are represented by a black circle, with black and red lines indicating additions and subtractions. The number of input channels ranges from 2^0 to 2^n. The proposed PC layer blocks are tested in ShuffleNet-V2 to find the optimal block structure. Results show that the ReLU activation function significantly impacts the accuracy of neural networks. The ReLU activation function significantly affects the accuracy of neural networks. Empirical analysis shows that PC layers outperform randomly initialized non-learnable layers in extracting cross-channel correlations. The proposed DWHT and DCT kernels extract more meaningful information. Blocks (c) and (d) show a trade-off between accuracy and computation costs, with a 2.3% accuracy drop when reducing learnable parameters and FLOPs. This prompts further exploration for more efficient neural network designs. In the next subsection, the problem of finding an optimal neural network architecture is addressed by applying conventional transforms on optimal hierarchy level features. Through comprehensive experiments, a default proposed block structure (d) is set for all following experiments. The search for an optimal hierarchy level where the proposed PC layer-based optimal block can be effectively applied in the network architecture is conducted. This optimal hierarchy level minimizes the number of learnable weight parameters and FLOPs without sacrificing accuracy, achieved through non-parametric and fast conventional transforms. Applying the proposed block on high-level blocks in the network results in a significantly reduced number of parameters. Our proposed block on high-level blocks in the network reduces parameters and FLOPs compared to low-level blocks due to exponential channel depth increase. Experiments on CIFAR100 dataset using ShuffleNet-V2 models with customized width hyper-parameter. Replaced 13 stride 1 basic blocks with our optimal block. Evaluation based on number of learnable weight parameters and FLOPs. Baseline models tested with different width hyper-parameters. Our models experimented with 1.1x setting. In Figure 3, the optimal block is applied on high-, middle- and low-level blocks. Performance evaluation is based on the number of blocks where the optimal block is used. Model notation includes transform type, number of proposed blocks, and hierarchy level (Low, Middle, High). For example, DWHT-3-L represents a neural network model with the first three blocks in ShuffleNet-V2 consisting of the optimal block. The neural network model in ShuffleNet-V2 includes proposed blocks in the first three blocks, while the rest are original blocks. The experiment compares performance based on transform types {DCT, DWHT}, hierarchy levels {L, M, H}, and the number of proposed blocks {3, 6, 10}. Middle-level experiments were conducted for DCT/DWHT-3-M and -7-M models. The results show the performance of different configurations in terms of Top-1 accuracy and learnable weight parameters. In Figure 3, the performance of different models with varying numbers of learnable weight parameters and FLOPs is compared. Applying the optimal block on high-level blocks showed a better trade-off between parameters, FLOPs, and accuracy. However, applying it on middle and low-level features resulted in inefficiencies. DWHT-based models generally showed slightly higher accuracy with fewer FLOPs compared to DCT-based models across all hierarchy levels. The fast version of DWHT requires fewer operations than DCT, showing higher accuracy with fewer FLOPs. The proposed method was applied to MobileNet-V1, replacing high-level blocks to verify effectiveness. The DWHT-6-H model yielded a 1.49% increase in Top-1 accuracy. The MobileNet-V1 model showed a 1.49% increase in Top-1 accuracy by reducing parameters and FLOPs. Depthwise separable convolutions were key in achieving this improvement, with PC layers dominating computation costs and memory space. The performance results for different hierarchy levels and block numbers are detailed in Appendix A, showing better efficiency at high levels with the proposed PC block. The MobileNet-V1 model achieved a 1.49% increase in Top-1 accuracy by reducing parameters and FLOPs, with PC layers playing a significant role. The performance results of applying an optimal block on the CIFAR100 dataset showed a decrease in performance gain when exceeding a certain capacity of transform-based PC blocks. The accuracy was significantly degraded by applying ReLU after the proposed PC layer, and using 3x3 depthwise convolution weight kernel values showed active utilization. Applying ReLU after conventional transforms significantly harmed accuracy due to the properties of conventional transform basis kernels. The distributions of positive and negative parameters in the kernels are almost identical, leading to important cross-channel information residing in the value range under zero. The negative values in the activations of PC layers contain important cross-channel correlation information, which is discarded when applying ReLU. This leads to a significant drop in accuracy. It is shown that fully reflecting the negative values results in the best accuracy. As a result, non-linear activation functions are not used after the PC layer. The study compared the near-zero values in depthwise convolution weights of different models, showing that DCT-3-H and DWHT-3-H models had more values apart from near-zero compared to the baseline model. These learnable weights were actively fitted to the optimal domain favored by conventional transforms, leading to increased accuracy. The study compared depthwise convolution weights in different models, showing DCT-3-H and DWHT-3-H had more non-zero values than the baseline. Higher weight decay values hinder active utilization of these weights, leading to accuracy drop. New PC layers enable efficient computation and learnable weight parameters, especially the DWHT-based PC layer. The study empirically found optimal block unit structures and hierarchy levels in neural networks for conventional transforms, improving accuracy and cross-channel correlations. ReLU was found to hinder cross-channel representability, while depthwise convolution weights were active in the last blocks of the proposed neural network. Performance curves of applying the optimal block on CIFAR100 were shown, comparing learnable weight parameters and FLOPs with baseline models using MobileNet-V1 architecture. Our proposed models were experimented with different settings and configurations, showing that applying the optimal block within the first 6 blocks in the network may lead to a mild or significant decrease in accuracy. The study focused on finding the hierarchy levels of blocks favored by the proposed PC layers, with performance curves indicating the impact on Top-1 accuracy. The study compared the performance of DCT/DWHT based PC layers with RCPC layer in the network's hierarchical levels. DCT/DWHT layers consistently outperformed RCPC in accuracy, computational costs, and number of learnable parameters. The study demonstrated the superior performance of DCT/DWHT-based PC layers over RCPC layers in various hierarchical levels. Experiments were conducted on the FDDB dataset, showing improved true positive rates at 1,000 false positives. The proposed method was also applied to object detection, specifically in face detection tasks, aiming for real-time algorithms with high accuracy. The study compared DCT/DWHT-based PC layers with RCPC layers in face detection tasks using MobileNet-V1 0.25x as the backbone model. Results on the WIDER FACE dataset showed that DWHT-3-H and DWHT-6-H models achieved comparable or higher mAP values than the baseline model. The DWHT-3-H and DWHT-6-H models outperformed the baseline model on all subsets with fewer parameters and FLOPs. The DWHT-3-H model achieved a 0.27% higher mAP on the Hard subset with significant reductions in parameters and FLOPs. DCT-3-H and DCT-6-H models showed improvements on Easy and Medium subsets. The effectiveness of the proposed method was verified on the FDDB dataset, with DWHT-6-H and DWHT-3-H models showing higher AP values. Our experiments on WIDER FACE and FDDB datasets demonstrate the effectiveness of our lightweight neural network method, which shows only a slight degradation in performance compared to the baseline model despite having fewer parameters and FLOPs. This highlights the potential for reducing computational overhead while maintaining generality."
}