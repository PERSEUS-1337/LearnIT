{
    "title": "rJevYoA9Fm",
    "content": "We study the singular values of linear transformations in 2D convolutional layers, providing an efficient computation method and a regularization algorithm. This regularization improves test error in deep networks, showing its effectiveness in practice. Convolutional layers are crucial in deep learning, and understanding their singular values helps prevent gradient issues. In this paper, the authors focus on studying the singular values of convolutional layers in deep learning. They provide an efficient method to compute these values exactly, which can lead to various regularization techniques. Convolutional layers are essential for image analysis tasks, where the input consists of a feature map with multiple channels. The output is also a field with multiple channels per position, obtained through a linear combination of values in a local neighborhood. The coefficients in the kernel of a convolution determine the output channels based on values in the input channels and their positions in the neighborhood. Practical convolutional layers handle edge cases by not computing outputs for inputs off the feature maps. Practical convolutional layers approximate behavior by treating input as a torus, wrapping around edges. Singular values of a convolutional layer can be computed exactly using kernel tensor K, in O(n^2 m^2 (m + log n)) time. The singular values of a convolutional layer can be computed exactly using kernel tensor K in O(n^2 m^2 (m + log n)) time, which is significantly faster than brute force methods. Timing tests confirm the efficiency of this approach, making it practical for use in neural networks. The implementation runs faster than NumPy, possibly due to parallelism. Singular values of convolutional layers were computed using code on the ResNet-v2 model. Regularizing operator norms improved test error on CIFAR-10. Operator-norm regularization complements batch normalization, with both being beneficial. Prior to our work, authors have responded to the difficulty of computing the singular values of convolutional layers in various ways. BID6 constrained the matrix to have orthogonal rows and scale the output of each layer by a factor of (2k + 1). BID9 proposed regularizing using a per-mini-batch approximation to the operator norm. BID23 used an approximation of the operator norm of a reshaping of K in place of the operator norm for the linear transformation associated with K in their experiments. The largest singular value of the reshaped matrix is often different from the operator norm of the linear transform associated with K. To regularize using projection onto an operator-norm ball, the whole spectrum of the linear transformation is needed. The reshaped K has m singular values, while the linear transformation has mn/2 singular values, with mn/2 being distinct in most cases. Projection of reshaped K onto an operator-norm ball involves taking its SVD and clipping its singular values. The linear transformation associated with a convolution is encoded by a circulant matrix for 1D signals with a single input and output channel. For 2D signals with a single input and output channel, the linear transformation is doubly block circulant. When there are m input and output channels, a m \u00d7 m matrix of blocks is used. Bibi et al. independently discovered a related result. The analysis extends tools from the literature for circulant and doubly circulant matrices to analyze matrices with a third level in the hierarchy arising from convolutional layers in deep learning. The eigenvectors of a circulant matrix are Fourier basis vectors, and in the multi-channel case, singular values can be computed by finding eigenvalues of doubly circulant matrices using a 2D Fourier transform. The singular values of matrices in deep learning can be computed by finding eigenvalues of doubly circulant matrices using a 2D Fourier transform. Notation and definitions for matrices and tensors are provided, including the operator norm denoted by ||M || 2 and the mapping \u03c3(\u00b7) from a matrix to its singular values. The paper introduces the notation \u03c9 = exp(2\u03c0i/n) for use in indexing and calculations. The paper introduces notation for matrices and tensors, including the use of i as \u221a-1 and \u03c9 = exp(2\u03c0i/n) for indexing. It defines the discrete Fourier transform matrix F, identity matrix In, basis vector ei, and Kronecker product \u2297. The focus is on filter coefficients represented by an n \u00d7 n matrix K, with a warmup on the case of m = 1 input/output channels. The concept of doubly block circulant matrices is discussed. A doubly block circulant matrix A is characterized by circulant blocks within an n \u00d7 n matrix. The singular values of A can be determined using eigenvalues and eigenvectors, with the eigenvectors being the columns of Q. The singular values are a simple function of the filter coefficients represented by matrix K. The eigenvalues of a doubly block circulant matrix A are characterized by circulant blocks within an n \u00d7 n matrix. The singular values of A can be determined using eigenvalues and eigenvectors, with the eigenvectors being the columns of Q. The singular values are a simple function of the filter coefficients represented by matrix K. BID4 provided a technical characterization of the eigenvalues, but a proof from first principles is presented here. The singular values of any normal matrix are the magnitudes of its eigenvalues. Lemma 4 completes the proof by considering the 2D Fourier transform of K and the largest singular value of A. For a 4D kernel tensor K, the connection strength between input and output units is defined by elements of K. The input and output matrices X and Y are defined accordingly. The main result states that for any K, M encodes the linear transformation of a convolutional layer. The singular values of a normal matrix are the magnitudes of its eigenvalues. The analysis in this section focuses on proving Theorem 6 through a series of lemmas related to characterizing the singular values of a block matrix. Reshaping the nonzero elements of L into a tensor G helps in expressing this characterization. The singular values of G are discussed in terms of left and right singular vectors, with specific properties outlined for these vectors. The proof of Theorem 6 involves characterizing the singular values of a block matrix by reshaping its nonzero elements into a tensor G. By taking the Kronecker product of singular vectors of G with specific properties, a singular value decomposition of L is formed. This allows for projecting a convolution onto convolutions with bounded operator norm by clipping the singular values of the associated linear transformation. After characterizing the singular values of a block matrix in Theorem 6, a convolution can be projected onto convolutions with bounded operator norm by clipping the singular values of the linear transformation. The resulting convolution neighborhood may expand, but it can be projected onto convolutions with specific neighborhoods by zeroing out irrelevant coefficients. Repeatedly alternating these projections can lead to a k \u00d7 k convolution with a bounded operator norm. However, for controlling the operator norm during iterative optimization, the process of alternating projections may not be necessary. After characterizing the singular values of a block matrix in Theorem 6, a convolution can be projected onto convolutions with bounded operator norm by clipping the singular values of the linear transformation. The alternating projections process may not be necessary as the first two projections often produce a convolutional layer with an operator norm close to the desired value. The projections can provide a warm start for the next pair, and in practice, they are run once every few steps. The validation of Theorem 6 was done through unit tests comparing different methods of computing singular values on 4D tensors with random values. The NumPy code outperformed TensorFlow for small tensors, but TensorFlow was faster for larger tensors due to GPU parallelism. Regularizing convolutional layers by clipping their operator norms was explored, with little impact on performance when clipping to 2.5 or 3.0. Regularizing convolutional layers by clipping their operator norms to values between 2.5 and 2.8 had little impact on performance. Clipping to 0.1 resulted in a surprising 6.7% test error, while clipping to 0.5 and 1.0 yielded test errors of 5.3% and 5.5% respectively. The projections did not significantly slow down training, as shown in Figure 4 in Appendix B. The baseline algorithm used batch normalization, which makes the network less sensitive to linear transformations with large operator norms. However, the existence of trainable scaling parameters in batch normalization complicates its interaction with methods that control the norm of linear transformations before batch normalization. In experiments, operator-norm regularization was studied to see if it made training more stable and robust to hyperparameters. Baseline with no batch normalization and a learning rate of 0.1 was used. Different hyperparameter combinations were tested to improve results. Operator-norm regularization improved training stability and robustness to hyperparameters, even in the presence of batch normalization. Clipping the singular values of the reshaped K every 100 steps was found to be effective in the experiment. Clipping singular values of reshaped K every 100 steps improved accuracy. Different constants were tried, with 0.2 yielding the best results. Clipping norms using a GPU was 25% faster than clipping singular values, with our method being more efficient. Our method for parallelization takes O(m^2 FFTs followed by n^2m x m SVDs, completing in O(n^2 log n + m^3) time. Results may not generalize, but suggest finding the full spectrum of the convolutional layer is comparable to heuristic approximations in accuracy and speed. Test error vs. training time plots in CIFAR-10 experiment are shown in Figure 4, while Figure 5 displays singular values of convolutional layers from \"Resnet V2\" pre-trained model BID12. The singular values of convolutional layers from the \"Resnet V2\" pre-trained model BID12 are plotted in Figure 5, showing that the number of non-negligible singular values increases as we proceed through the layers. The effective rank of the convolutional layers is larger closer to the inputs, with different layers having significantly different numbers of non-negligible singular values. The number of non-negligible singular values in convolutional layers varies significantly, possibly due to differences in layer sizes. To investigate this, singular value ratios were normalized by dividing by the total number of singular values, as shown in FIG10."
}