{
    "title": "SJgvl6EFwH",
    "content": "Continuous Normalizing Flows (CNFs) are effective deep generative models for various tasks due to their invertibility and exact likelihood estimation. InfoCNF is a proposed efficient conditional CNF that partitions the latent space for conditional image generation. It utilizes gating networks to optimize ODE solvers for improved speed and performance, showing enhanced test accuracy compared to the baseline. InfoCNF improves test accuracy over the baseline, reduces NFEs on CIFAR10, and enhances extrapolation performance on time-series data. Invertible models, like flow-based generative models, enable exact latent-variable inference and likelihood estimation for tasks such as anomaly detection and model-based reinforcement learning. These models transform simple distributions into more complex ones while maintaining invertibility and exact likelihood estimation. Continuous Normalizing Flow (CNF) uses ODEs for transforming between latent variables and data, reducing computation cost. This allows scaling up invertible models for complex tasks on larger datasets. CNF has mainly been trained with unlabeled data but can benefit from labeled data for improved performance. Conditioning methods for CNF are needed to utilize labeled data effectively. Existing approaches for conditioning flow-based models do not work well on CNF due to overparametrization, hindering learning efficiency. The dimension of the latent code in CNF needs to be the same as the input data, resulting in unnecessary parameters that increase model complexity. The overparametrization in CNF negatively impacts efficiency and the performance of flow-based generative models. The complexity of CNF models affects the ODE solvers, leading to increased NFEs and slower training. This challenge hinders scaling CNF for real-world tasks. The proposed InfoCNF conditioning approach aims to address these issues by partitioning the latent code effectively. InfoCNF partitions the latent code into class-specific supervised and unsupervised parts. The supervised code conditions the model on given signals and is used for classification, reducing the classifier size. This separation allows the model to learn task-relevant features and other variations in the data. The cross-entropy loss in InfoCNF corresponds to mutual information, encouraging disentangled representations. InfoCNF encourages disentangled representations by tuning error tolerances of ODE solvers. Increasing tolerance enhances precision but leads to longer training time. Some noise in activations can improve generalization and robustness. Carefully selected tolerances in InfoCNF result in higher speed and better performance. The proposed method in InfoCNF uses learnable gating networks to compute error tolerances for ODE solvers, improving training efficiency. Experiments on CIFAR10 show that InfoCNF with gating networks outperforms CCNF in test error and NFEs in both small-batch and large-batch training, with reductions in test error and NFEs. InfoCNF with gating networks improves training efficiency by reducing NFEs and test error compared to CCNF. Gating networks help achieve reductions in test error and NFEs in both small-batch and large-batch training. The gating approach also benefits unconditional CNF on CIFAR10 by reducing NFEs while maintaining performance. In experiments on CIFAR10, error tolerances reduce NFEs by 15% while maintaining NLL. Partitioning the latent code in LatentODE improves curve extrapolation performance. Bijective flow-based generative models like RealNVP and Glow have gained interest for their invertible structures. Flow-based generative models use bijective transforms to generate data and enable exact inference and density evaluation. Stacking these transforms forms a normalizing flow, with the latent code typically chosen as a standard Gaussian. Parameters are learned by maximizing log-likelihood on the training set. However, the requirements for invertibility and tractable computation limit the model's expressive power. Continuous Normalizing Flows (CNFs) bypass restrictive requirements in flow-based generative models, allowing for more expressive tasks. CNF defines invertible transforms via continuous-time dynamics, reducing computation costs from O(d^3) to O(d^2). Conditional CNF further enhances model capabilities. Conditional CNF, a method not specifically designed for CNF but applicable due to its flow-based model nature, can be enhanced with conditioning methods like Gaussian Mixture Models and auxiliary classifiers. This results in Conditional Continuous Normalizing Flow (CCNF), where the latent code distribution follows a GMM dependent on the conditional signal y. Additionally, a predictive model is used on z to model the distribution p(y|z) through an auxiliary predictive task. InfoCNF uses a portion of the latent code z for conditioning by splitting it into supervised latent code zy and unsupervised latent code zu. This approach reduces the number of parameters introduced by the conditioning network, making it more efficient compared to traditional methods. The latent code is split into supervised code zy and unsupervised code zu. The supervised code captures structured semantic features, while the unsupervised code is used for predictive tasks. The distribution of zy is modeled by a GMM, with centers and scales determined by a neural network. The posterior is approximated by another neural network applied on zy. InfoCNF is learned by optimizing the supervised loss from q \u03b8 (z y) and the conditional log-likelihood log p(x|y) of the model. The objective includes a weighting factor \u03b2 between cross-entropy loss L Xent (\u0177, y) and conditional log-likelihood loss L NLL (x|y), calculated using Gaussian distribution formulas. The mutual information between the latent codes zy and zu is also considered. The mutual information in InfoCNF is approximated using a neural network as an \"auxiliary\" distribution. By leveraging the invertibility of InfoCNF, the need for an additional network is eliminated, resulting in fewer parameters compared to CCNF. For instance, in experiments, InfoCNF requires 4% fewer parameters due to the smaller size of the supervised code zy. InfoCNF requires fewer parameters than CCNF, leading to improved learning. Experiments on CIFAR10 show InfoCNF needs less NFEs from ODE solvers, indicating the partition strategy helps training. Tuning error tolerances of ODE solvers further enhances InfoCNF by reducing NFEs and providing regularization for improved model training. In InfoCNF, error tolerances of ODE solvers are tuned to improve model training. InfoCNF with learned tolerances uses gating networks to compute error tolerances for each solver, optimizing accuracy and NFEs. CNNs are used for the gates in experiments, and reinforcement learning is employed to handle the non-differentiable optimization challenge. In InfoCNF, the gating networks learn error tolerances through reinforcement learning. The policy decides which tolerance to use based on input x, with CNN estimating the distribution parameters. Rewards are based on the number of function evaluations at each layer. The policy in InfoCNF learns error tolerances through reinforcement learning, deciding on tolerance based on input x. Rewards are based on function evaluations at each layer, aiming for good performance with fewer computations. The model combines supervised, unsupervised, and reinforcement learning to improve classification and density estimation while reducing the number of function evaluations by ODE solvers. Empirical results show InfoCNF's advantage over CCNF when trained on CIFAR10. InfoCNF, equipped with gating networks to learn error tolerances, outperforms CCNF on CIFAR10 with better test errors, smaller NFEs, and improved NLL. Learning error tolerances enhances InfoCNF in all aspects except for a slightly worse NLL in small-batch training. Evaluation methods ensure results are unbiased by numerical errors. The CIFAR10 dataset validates the model's advantages with added noise and random horizontal flips during training using the FFJORD multiscale architecture. In experiments, the FFJORD multiscale architecture is used with multiple flows. The network details are in Appendix B. Separate linear networks are used for conditioning the model. Parameters are initialized to zeros. A dropout rate of 0.5 is applied on the linear classifier. Training is done with the Adam optimizer for 400 epochs with different batch sizes and learning rates. Adaptive ODE solvers introduce errors during numerical integration. When evaluating models, it is important to consider numerical errors that may arise from integration. To determine error tolerances for evaluation, InfoCNF is trained on 1-D synthetic data sampled from a mixture of three Gaussians. The area under the curve is computed using Riemann sum at different error tolerance values, with results showing negligible errors when tolerance is set to 10^-5. Therefore, in experiments, error tolerances are set to 10^-5 at test time. In experiments, error tolerances are set to 10^-5 at test time. Trained models are evaluated using tolerances of 10^-6, 10^-7, and 10^-8, yielding the same results as 10^-5. NFEs of ODE solvers are averaged over training epochs. Learned error tolerances from InfoCNF on CIFAR10 are validated to not overfit training data. InfoCNF improves test classification error on CIFAR10 by 12% with small batches and 10% with large batches compared to CCNF. It achieves similar NLLs in small-batch training and better NLLs in large-batch training. InfoCNF is more efficient than CCNF in the first 240 epochs but then NFEs increase and exceed CCNF. Overall, InfoCNF requires 16% less NFEs during training and 11% less with large batches. InfoCNF with learned error tolerances outperforms InfoCNF with fixed error tolerances, achieving a 4% lower test error and slightly better NLLs. In small-batch training, both models perform similarly, but InfoCNF with learned tolerances reduces NFEs by 21% compared to fixed tolerances. Large-batch training shows similar NFEs for both models. Overall, InfoCNF with learned tolerances shows notable improvements over fixed tolerances. In comparison to InfoCNF with fixed error tolerances, InfoCNF with learned tolerances shows significant improvements in test error and NLLs. The automatic approach via reinforcement learning outperforms manually-tuned tolerances in both classification and density estimation, requiring less time and computational resources. Training neural networks using large batches is faster but suffers from poor generalization. To improve performance, experiments were conducted to tune the error tolerance of ODE solvers and use CNN gates to learn error tolerances in small-batch settings. This approach significantly reduced NFEs by 15% compared to baseline CNF. Our experiments show that adjusting error tolerance of ODE solvers and using larger learning rates, as suggested in previous studies, improves the performance of InfoCNF and CCNF. This results in better test error, lower NLLs, and reduced NFEs. We explore the benefits of applying the conditioning method in InfoCNF on time-series data using the LatentODE as a baseline model. The experiment is conducted on a synthetic bi-directional spiral dataset with spirals of different parameters. The LatentODE with conditioning strategy outperforms the baseline Latent ODE on trajectory fitting and extrapolation tasks, especially in unseen domains. Conditional generative models use a generalized linear model on the latent code of a flow-based generative model to compute p(x) and p(y|x) exactly. Their method is complimentary to our partitioning approach. InfoCNF is a method that splits the latent code without penalizing mismatches between distributions or minimizing MMD distance. It maximizes the likelihood of the input image given the label, unlike other approaches that use encoders trained with adversarial loss. Our InfoCNF splits latent codes without using complex architectures like GANs, avoiding instability in ODE solvers. Adaptive computation techniques, such as gating networks and reinforcement learning, have been explored to enhance neural network efficiency. Previous works have used gating networks to skip blocks in residual networks and developed reinforcement learning frameworks to select compression techniques for DNNs. Our InfoCNF, with learned tolerances, is the first to determine error tolerances for ODEs. Our InfoCNF with learned tolerances is a novel approach that determines error tolerances for ODE solvers in CNF. Large-batch training methods have been proposed to improve neural network generalization, but our approach focuses on tuning error tolerances. We developed InfoCNF, a framework for conditioning CNF by partitioning the latent code. By tuning error tolerances, we aim to enhance the performance and speed of InfoCNF. Additionally, we introduce InfoCNF with gating networks to learn error tolerances from data, showing advantages over the baseline CCNF. InfoCNF introduces learned error tolerances for ODE solvers, improving large-batch training with faster speeds. Results on MNIST show similar performance to baseline CCNF but with faster processing. Experiments on CIFAR10 utilize FFJORD multiscale architecture for improved performance. The network architecture in the experiment uses 4 scale blocks with 2 flows each, a \"squeeze\" operator, and softplus nonlinearity. It aims to reduce latent representation dimensionality while maintaining invertibility. Separate linear networks parameterize q \u03b8 and q \u03c6. A synthetic 1-D dataset is used to estimate error tolerances of ODE solvers at test time. To estimate error tolerances of ODE solvers with negligible numerical errors, InfoCNF is trained on a 1-D synthetic dataset. The area under the curve is calculated at different error tolerance values, starting from machine precision of 10^-8. The dataset consists of samples from a mixture of Gaussians. The experiment explores using error tolerances from input data batches to evaluate the trained InfoCNF. Using learned error tolerances, the trained InfoCNF model achieves low numerical errors on 1-D synthetic data and CIFAR10 with small batches. Test error and NLL results are comparable to fixed error tolerances, indicating the effectiveness of the approach. In experiments on synthetic time-series data, large batch sizes lead to worse metrics due to error tolerances being computed for each batch. A bi-directional spiral dataset with 5,000 2-dimensional spirals is used, with 2,500 clockwise and counter-clockwise curves having different parameters. The study involves generating synthetic spiral data with Gaussian noise for training and testing. The model used includes a LatentODE baseline with a fully connected RNN encoder and 5-dimensional latent states. The initial latent state includes supervised and unsupervised codes. The model computes future states by solving an equation with a dynamic function parameterized with one-hidden-layer networks. Conditioning and supervised functions are linear networks, trained with Adam optimizer for 20,000 epochs on CIFAR10 data. In Figure 10, error tolerances for small-batch training on CIFAR10 are compared with results from CCNF, InfoCNF, and InfoCNF with learned tolerances. InfoCNF with learned tolerances performs similarly to manually-tuned error tolerances, with slightly worse NFEs. The NLLs discussed are conditional negative log-likelihoods. Figure 11 shows that InfoCNF with learned tolerances has better marginal NLLs in large-batch training but slightly worse NLLs in small-batch training compared to InfoCNF and CCNF. Figure 12 confirms that using large learning rates and tuned error tolerances in InfoCNF and CCNF models improves marginal NLLs in large-batch training compared to small learning rates and constant error tolerances. The models trained with large batches show better test classification errors and negative log-likelihoods. In our study, models trained with large batches show better test classification errors and negative log-likelihoods on CIFAR10 compared to small batches. We aim to investigate if models trained with large batches can achieve certain metrics faster than those trained with small batches. InfoCNF with learned tolerances, which performs well in large-batch training, is compared with small-batch training. Results show that InfoCNF with learned tolerances trained with large batches outperforms CCNF trained with small batches in test error but lags behind in NLLs. Future work includes improving large-batch training with CNF-based models. InfoCNF with learned tolerances reduces NFEs while enhancing test error and NLL. Exploring the use of InfoCNF with learned tolerances for training larger models is suggested for better performance in classification and density estimation. Training a modified InfoCNF with 4 flows per scale block on CIFAR10 using large batches shows promising results compared to the original model and baseline CCNF. The 2x-InfoCNF with learned tolerances outperforms InfoCNF and CCNF in test errors and NLLs while increasing NFEs."
}