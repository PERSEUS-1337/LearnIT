{
    "title": "Bkeqb1BFvB",
    "content": "Graphs are complex structures with variable size and no natural ordering of nodes, making analysis challenging. To address this, graph feature representation is necessary, balancing expressiveness, consistency, and efficiency. While current methods use powerful graph neural networks, we propose utilizing the graph Laplacian spectrum for a simpler approach that maintains isomorphism-invariance, expressiveness, and deformation-consistency. This theoretical analysis focuses on comparing graphs using graph perturbation. The analysis focuses on comparing graphs using graph perturbation to understand the comparison between GLS. It derives bounds for the distance between GLS related to the divergence to isomorphism. GLS is experimented as a strong graph feature representation baseline for consistency tests and classification tasks. Graphs are present in various systems and extracting information from them is crucial for downstream machine learning tasks. In this paper, the importance of satisfying criteria for graph feature representation is analyzed. The criteria include consistency under deformation and invariance under isomorphism. The trade-off between expressiveness and efficiency in embedding methods is discussed, with a focus on the need for equal dimensionality and dimension-wise alignment in graph feature representation. The Laplacian matrix of a graph, known for its spectral learning properties, such as identifying communities and graph structure, is considered a suitable candidate for graph representation due to its interpretability and efficient algorithms for large datasets. The Laplacian matrix is analyzed for its spectral properties, including the spectral gap and eigenvalues, which provide information about graph connectivity and energy levels. The Laplacian matrix eigenvalues correspond to energy levels and frequencies in graph analysis. Truncating the Fourier decomposition acts as a signal filter. Characterizing a graph by its Laplacian eigenvalues is akin to characterizing a melody by fundamental frequencies, containing structural information. GLS is analyzed for its capacity to represent graph structure, considering two weighted graphs. The Laplacian matrix eigenvalues correspond to energy levels and frequencies in graph analysis, akin to characterizing a melody by fundamental frequencies. GLS is analyzed for its capacity to represent graph structure using two weighted graphs, G1 and G2, with Laplacian matrices defined as L1 = D1 - W1 and L2 = D2 - W2. The goal is to build a fixed-dimensional representation to compare graphs that are not aligned or equally sized. Adding isolated nodes and perturbing edges on graph G can be achieved through edge and node perturbations. Withdrawal of a node is equivalent to removing all its connected edges. In unweighted graphs, edge perturbations are limited to {-1, 0, 1}. Perturbations involving node indexing can be annulled by a permutation matrix. A perturbed version of G, denoted as G P *, is the sparsest possible without permutations. The sparsest perturbation P * for graph G is denoted as G P *. It includes isolated nodes and perturbed edges, with P * being a block matrix representing perturbations between G and additional nodes. The eigenvalue of a square matrix X is denoted as \u03bb(X) in ascending order. The deformation of graphs involves perturbations between graph G and additional nodes, characterized by unknown deformation P*. The Laplacian spectrum is analyzed for graph representation and its link to graph isomorphism under Hypothesis 1. A good embedding of graphs should be close when deformation is low and far otherwise, quantified by global and node-wise entries of P*. This idea is used to propose an analysis using the Laplacian of P*, denoted L P*. The Laplacian of a hypothetical perturbation P* is used to analyze the distance between two graphs G1 and G2 for graph isomorphism. The divergence between the graphs is measured by the minimal Frobenius norm of the difference between their Laplacians. This divergence is linked to the norm of LP*. Graph isomorphism is a challenging problem to solve efficiently, especially for large graphs and datasets. The distance between GLS relaxes the isomorphism-based graph divergence. Proposition 1 states that the higher the difference between GLS, the larger the hypothetical perturbation P*, indicating higher structural dissimilarity. The problem of non-isomorphic L-cospectrality is addressed, where two graphs can have equal eigenvalues but different Laplacian matrices. Equal spectrum only implies equal graphs when eigenvectors are also equal, otherwise, L-cospectrality for non-isomorphic graphs exists, showing that some graph families are not fully determined by their spectrum. Families of graphs are not fully determined by their Laplacian spectrum, as some share structural properties but not adjacency. Most graphs are determined by their spectrum, which indicates common properties regardless of adjacency. However, for datasets with L-cospectral non-isomorphic graphs requiring unique representation, Laplacian spectrum may not be suitable. GLS is suitable for characterizing graphs in most situations, but for unique representation properties, more advanced embedding methods should be considered. Proposition 3 suggests that the closer the GLS, the closer the Laplacian matrices are to unitary-similarity. This concept is a relaxed version of graph-isomorphism divergence. In this section, the divergence between graphs can be approximated by the divergence between their GLS. A fixed embedding dimension d is chosen for all graphs in dataset D, typically set to the maximum number of nodes in a graph. Padding with isolated nodes is equivalent to adding zeros in the GLS, but for larger graphs, excessive padding may be necessary. We propose truncating the GLS of graphs to keep only the highest d eigenvalues, saving computation time. However, this may lead to information loss for graphs with more than d nodes. The impact of truncation is assessed in experiments, with potential solutions like embedding the lowest eigenvalues with simple statistics. The experimental section provides further details and reproducibility information. In Section 4, deformation-based results are illustrated using ErdosRnyi random graphs with parameter p = 0.05. Three simple experiments are focused on, showing the relationship between Laplacian spectrum distance and perturbations. The use of truncated GLS (t-GLS) for larger graphs is also mentioned. The truncated GLS (t-GLS) method is used for larger graphs to save computation time and avoid adding isolated nodes. Experimental results show that t-GLS is consistent with node addition, with higher connectivity leading to greater GLS divergence. Spectral feature embedding is evaluated through a classification task on molecular and social network graphs. The experimental setup for classification tasks on molecular graphs involves using five datasets: Mutag, Enzymes, Proteins Full, Dobson and Doig, and National Cancer Institute. Molecular graphs consist of nodes representing atoms or molecules and edges representing chemical or electrostatic bindings. Some models in Table 1 utilize node attributes from these graphs. Table 1 presents the accuracy of classification models using different graph representations on molecular graphs. Models are divided into two groups: feature + SVC and end-to-end deep learning, with some utilizing node attributes. The graphs in these datasets are social networks without node attributes, allowing for a more appropriate comparison of GLS + SVC to deep learning based classification. The classification accuracy of different deep learning models, including GLS + SVC, on standard social networks datasets is presented in Table 2. GLS is shown to capture graph structural information effectively, while graph neural networks are more expressive due to their ability to leverage specific information for classification. GLS is effective for graph classification, especially with node labels. Truncating GLS can reduce computational cost and dimensionality without significantly impacting classification results, except for challenging tasks like ENZYME multiclass classification. Graph kernel methods create high-dimensional feature representations of data by performing pairwise comparisons between atomic substructures of graphs. These substructures, such as graphlets and subtree patterns, are used to build a representative dictionary for graph embedding. The kernel trick avoids explicit computation of coordinates in feature space, making it an implicit embedding method. Graph kernel methods involve creating high-dimensional feature representations of data by comparing atomic substructures of graphs, such as graphlets, subtree patterns, random walks, or paths. The challenge lies in selecting the right algorithm and kernel for graphs of varying sizes. Techniques like the Nystrm algorithm can reduce computational complexity. Feature-based representation methods represent graphs by concatenating features like node or edge counts and node degree histograms, offering interpretability and isomorphism-invariance. Graph-level features provide isomorphism-invariance but lack expressiveness. Various algorithms use features based on random walks or graphlets. Permutation-invariant features are created by mapping the adjacency matrix. Graph spectral distances are used to build features. Experimental work shows promising results using normalized Laplacian spectrum with random forest for graph classification. Analyzing the cospectrality of graph matrices and their spectra is also explored. These works are related to the current research, but lack theoretical analysis and comparative experiments with benchmarks. In response to the lack of theoretical analysis and comparative experiments with benchmarks in current research on graph features, this paper proposes a solution. Deep learning methods like GNNs learn node representations in graphs by considering attributes, neighboring nodes, and connecting edges. Node embeddings are pooled to create graph-level representations using permutation invariant readout functions. Capsule networks and convolution-like operations are also utilized for better information preservation. Formulation of GNNs can be in spectral or spatial domain, with the former limited to single graph structures due to fixed Laplacian spectrum. Spatial domain formulations, on the other hand, can handle multiple graph structures and infer information from unseen ones. Alternative approaches like random walk embedding and RNN-based graph transformations also exist, with models learning isomorphism-invariance through repeated exposure to the same graph. In this paper, the authors propose using the graph Laplacian spectrum (GLS) as a whole graph representation, showing that comparing two GLS can measure the structural divergence between graphs. They highlight the advantages of GLS, such as isomorphism invariance, simplicity of implementation, and computational efficiency. The proof of Lemma 1 is provided, demonstrating the isomorphism invariance of eigenvalues. The eigenvalues of a matrix M are isomorphism invariant. By selecting an eigen couple (\u03bb, x) of M, we can construct x such that |x i | = 1 and |x j =i | < 1. The Frobenius norm is unitarily invariant, and for any orthogonal matrix P \u2208 O(|V 2 |), we have certain properties. The orthogonal matrices Q 1 and Q 2 are eigenvector matrices of symmetric matrices L 1 and L 2. Classification is done using a standard 10-folds cross validation setup to preserve class proportions in each fold of the dataset. Results are averaged over all testing sets using a 10-fold cross-validation setup to preserve class proportions. The dimension d is chosen based on the 95th percentile of graph sizes in each dataset for truncating eigenvalues. Larger datasets with large graphs can be truncated more severely for computational efficiency, especially when using the support vector classifier (SVC) as a baseline for graph representation. The support vector classifier (SVC) from scikit-learn is used with a Radial Basis Function kernel for graph representation. Hyperparameters C and \u03b3 are tuned for different datasets. A nested hyperparameter search cross-validation is used to avoid overfitting. The study uses cross-validation to prevent overfitting in model selection. Results are shown using the k-nearest neighbor algorithm with L2 norm to demonstrate proximity in graphs. Five molecular and social network datasets are used for experiments. Molecular datasets include Mutag, Enzymes, Proteins Full, Dobson and Doig, and National Cancer Institute. The study uses cross-validation to prevent overfitting in model selection, demonstrating proximity in graphs using the k-nearest neighbor algorithm with L2 norm. Molecular datasets Mutag, Enzymes, Proteins Full, Dobson and Doig, and National Cancer Institute are utilized. Social network datasets include IMDB-Binary, IMDB-Multi, REDDIT-Binary, REDDIT-5K-Multi, and COLLAB, each with unique graph structures and node labels leveraged by graph neural networks. The COLLAB dataset represents scientific collaborations in Physics, with graphs showing co-authorship relationships. The graphs have no node attributes for fair comparison with deep learning methods. Figure 4 demonstrates using the highest eigenvalues of the Laplacian spectrum as whole-graph features. The study computes L2 distance between t-GLS with varying dimensions to confirm the importance of the dimensionality of GLS-embedding. The importance of the first largest eigenvalues of the Laplacian in discriminating between structurally different graphs is highlighted. For Erdos-Reyni graphs, adding nodes with stochastic connections makes it difficult to distinguish the augmented graph from the original one based solely on structural information."
}