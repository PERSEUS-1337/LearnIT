{
    "title": "H1cT3NTBM",
    "content": "In this paper, the use of neural networks for music information retrieval tasks is explored. The study focuses on improving convolutional neural networks (CNNs) on spectral audio features for singer classification and singing performance embedding. Three aspects of CNN design are investigated: network depth, residual blocks with grouped convolution, and global time aggregation. Results indicate that global time aggregation significantly enhances CNN performance. Additionally, a singing recording dataset is released for training and evaluation purposes. Advancements in computer vision and natural language processing have improved retrieval problems. Recent experiments explore using ResNet and ResNeXt variants to enhance deep neural networks for time-frequency representations in audio analysis. These variants allow for deeper convolutional layers in neural networks. In this paper, a deeper architecture with more than 5 convolution layers is proposed for music information retrieval using convolutional neural networks. The convolution layers learn local patterns in input matrices, and recurrent neural networks are used to capture temporal relations in time-frequency representations. The paper explores the use of attention mechanisms in modeling temporal dependencies and relations in music information retrieval. It compares global aggregation operations like average and max with the attention mechanism experimentally, focusing on singer classification and singing performance embedding tasks. The paper discusses using attention mechanisms for modeling temporal dependencies in music retrieval, specifically in singer classification and performance embedding tasks. It aims to create an embedding space where singers with similar styles are closer together. The challenge lies in isolating the \"singer effect\" from the \"song effect\" in audio analysis tasks. The paper discusses the challenge of isolating the \"singer effect\" from the \"song effect\" in audio analysis tasks, aiming to create an embedding space where singers with similar styles are closer together. This is analogous to learning an embedded space for face verification in computer vision. The paper aims to create an embedding space for singing voice audio recordings using a siamese neural network to group recordings of the same identity together and separate those of different identities. This allows for the identification of \"singing style\" or \"singing characteristics\" by examining clusters formed from the embeddings. The architecture employs CNNs to extract features and dense layers for classification and performance embedding. The architecture uses fully connected dense layers for singer identity classification and a linear layer for singing performance embedding. The model creates fixed length vectors for singing recordings, enabling efficient similarity comparison through Euclidean distance calculation. This allows for quick querying of a large database of singing recordings. The paper introduces a new set of \"balanced\" singing recordings for unbiased evaluation of a singing performance embedding model. The dataset is an addition to the existing DAMP data set of vocal music performances. The neural network architecture used in the experiments involves feeding input time-frequency features as 2-D images into convolutional layers and then to a global time-wise aggregation layer. The neural network architecture involves feeding input time-frequency features as 2-D images into convolutional layers, followed by a global time-wise aggregation layer. The construction blocks include a vanilla convolution layer and a ResNet design with a bottleneck block. ResNeXt is an extension of ResNet using grouped convolutional blocks. The ResNet bottleneck block is y = x + f(g(h(x))), while the ResNeXt bottleneck block is y = x + \u0393(x) with \u0393(\u00b7) being grouped convolution. A max pooling layer is placed between convolutional blocks with a pool size of (2, 2) and stride of (2, 2). The distinction between convolution layer and block is important. The neural network architecture in this paper includes convolutional layers with batch normalization, global time-wise aggregation, dense layers, and an output layer. The 3-D feature map is reshaped before feeding it to the global aggregation. The attention mechanism was originally introduced for sequence-to-sequence learning in an RNN architecture. The paper introduces a feed-forward attention mechanism in an RNN architecture for learning BID0, allowing predictions to access information from every input hidden sequence step in a weighted manner. The attention operation calculates weight vector \u03c3 over time-steps using a non-linear function, producing an outputX as a weighted average of input matrix X with weights \u03c3. This mechanism is used instead of the original attention for experiments not requiring sequence-to-sequence prediction. The attention operation in the RNN architecture is determined by learnable parameters w and b, acting as an aggregation over the time-axis. It is part of a family of operations including feed-forward attention, max, and average, with the latter two having no learnable parameters. This family differs from standard pooling in convolution layers by globally aggregating over the input sample, reducing the dimension of the aggregation axis to 1. Singer identity classification and singing performance embedding are the tasks explored in the paper, with experimentation on hyperparameters and network architectures for the singer classification problem. The singer classification problem provides clear evaluation criteria for model performance with different hyperparameters and network architectures. The embedding task explores spatial relationships between samples. Numerical evaluation metrics and plots of embedded samples from singing performance are provided for both tasks using the DAMP dataset with 34620 solo singing recordings by 3462 singers. The DAMP dataset contains solo singing recordings by multiple singers, leading to an unbalanced dataset. To address this, the DAMP-balanced dataset was created with 24874 recordings by 5429 singers, featuring 14 songs. The dataset is structured with the last 4 songs for testing and the first 10 for training/validation splits based on a 6/4 ratio. The DAMP-balanced dataset is suitable for singing performance embedding tasks, while the original DAMP dataset can train singer identity classification algorithms. Time-frequency representations extracted from raw audio signals are used as input for neural networks, taking the form of 2-D matrices with time and frequency axes. The Mel-scaled magnitude spectrogram (Mel-spectrogram) is used as input for neural networks, while the constant-Q transformed spectrogram (CQT) is less effective due to its inability to preserve octave relationships between frequency bins. The Mel-scaled magnitude spectrogram is obtained from audio recordings with a single singing voice, using a Fast Fourier Transform with specific parameters. The spectrogram is squared, transformed into decibels, and values below -60dB are clipped. An offset is added to ensure values between 0 and 60. The Mel-spectrogram of each singing performance audio recording is chopped into overlapping matrices with a duration of 6 seconds and 20% hop size. Gradient descent is optimized with ADAM, a learning rate of 0.0001, and a batch size of 32. Dropout of 10% is applied at the last fully connected dense layers. L2 weight regularizations with a weight of 1e \u2212 6 are applied. Hyperparameters are chosen using Bayesian optimization. Early stopping tests are applied every 50 epochs for singer identity classification and every 1000 epochs for singing performance embedding. Non-linear activation functions are used in all convolution layers and fully connected layers. The singer classification task involves using a neural network with specific configurations and a subset of 46 singers from the DAMP dataset. A 10-fold cross-validation is used to evaluate test accuracies for different models. Different combinations of neural network configurations are explored for the classification task. Different combinations of neural network configurations are explored for singer classification, including vanilla CNN or ResNeXt building blocks, different layers, and types of aggregation. A baseline SVM classifier is included, achieving 27% accuracy, while neural network models exceed this by at least 35%. The number of convolution filters is adjusted for parameter consistency across configurations. The neural network models significantly outperformed the baseline by 35% or more. Global aggregation methods improved performance by 5% to 10%. The experiment focused on creating an embedding space that groups recordings by the same singer together and separates recordings by different singers. A siamese neural network architecture was used for this purpose, with an embedding dimension of 16. The embedding dimension for the linear fully connected output layer is chosen to be 16 by SPEARMINT. A siamese network arranges pairs of samples from the dataset and learns the embedding by adjusting the distance between them based on their label. The optimization goal is the contrastive loss, defined as the squared euclidean distance between embedded vectors with a target margin of 1. Training involves pairs of samples from the same or different singers. The siamese networks are trained using pairs of samples from the same or different singers. Different network configurations show contrastive losses in TAB1. Training and validation errors are plotted in FIG1, indicating that feed-forward attention and average aggregation tend to overfit the data. Shallow architectures perform slightly better than deeper ones. Figure 4 shows qualitative characteristics of the embeddings. The embeddings of 40 performances by 10 singers are compared using shallow ResNeXt architecture with/without feed-forward attention and handcrafted features for singer classification. The handcrafted features captured the \"song\" effect, while the learned embeddings grouped performances by the same singers regardless of the \"song\" effect. The t-SNE projections of 6-second clips before summarizing into songs are shown in Figure 4. The t-SNE projections of 6-second clips before being summarized into songs are analyzed for singer classification using leave-one-out k-nearest neighbor classifications. Results show classification accuracies for different network configurations and k values. Additionally, k-nearest neighbor classifications on performed songs are conducted to demonstrate the \"song effect\". The study explores the \"song effect\" in singer and song classification using k-nearest neighbor results. Singing performance embedding learning dilutes the \"song effect\" and enhances singer style. Global aggregation improves performance significantly. The balanced dataset enables k-nearest neighbor classification on performed songs. The study shows that global aggregation significantly improves performance in singer and song classification. Using feedforward attention accelerates the learning process by creating \"frequency templates\" for each convolutional channel. Training deep neural networks with more than 15 convolutional layers on time-frequency input is feasible with global time aggregation. The authors released a dataset of over 20000 single singing voice recordings for music information retrieval research. They plan to experiment with replacing max-pooling with striding in convolutional layers and considering temporal order during global-aggregation. The neural network configurations will also be tested in other music tasks like music structure segmentation. The DAMP-balanced dataset is separate from the original DAMP but has the same metadata format. The DAMP-balanced dataset, separate from the original DAMP, has the same metadata format. It is collected by querying the Sing! Karaoke app database and specifically asks for users who sang a specific collection of songs at least once. The dataset was collected by querying the Sing! Karaoke app database for users who sang a specific collection of 14 popular songs. Queries were created to retrieve audio recordings and metadata for different combinations of splitting the songs into 6/4 song collections. The train/validation sets used the first 6 songs for training and the next 4 songs for validation, resulting in 276 performances for training and 88 performances for validation. The dataset allows for different splits of songs into sets for validation, resulting in varying numbers of singers. The \"balanced\" structure of the dataset enables train/validation rotations within the first 10 songs, leaving the last 4 songs as a test set."
}