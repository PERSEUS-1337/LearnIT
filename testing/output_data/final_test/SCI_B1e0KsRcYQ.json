{
    "title": "B1e0KsRcYQ",
    "content": "Learning rich and compact representations is a key challenge in various fields like word embedding, visual question-answering, object recognition, and image retrieval. While deep neural networks have made significant progress in providing hierarchical and semantic representations, they still lack the necessary richness and compactness. This paper introduces factorization schemes extended to codebook strategies to achieve compact representations with second order performances, maintaining the same dimensionality as first order models. This paper introduces a framework for compact second-order models with few additional parameters, achieving state-of-the-art results. It extends factorization schemes and codebook strategies to reduce parameters and computation costs. The standard approach involves extracting features from input data and building representations for tasks like classification and retrieval using deep neural networks. Recently, second-order models have outperformed first-order models by computing higher order statistics like bilinear models. These models generate richer representations and have been successful in various tasks such as word embedding, VQA, and fine-grained classification, achieving state-of-the-art results. However, second-order models have drawbacks such as increased dimensionality and computational costs. Second-order models, like bilinear models, have drawbacks such as high dimensionality and computational costs. These models lack an efficient pooling scheme, leading to sub-optimal representations. Factorization schemes have been studied to address the dimensionality issue, but the intermediate representation is still too large for easy training. In this paper, the drawbacks of second-order models, such as high dimensionality and computational costs, are addressed by exploring joint factorization and codebook strategies to improve pooling schemes and reduce model size. The paper addresses the drawbacks of second-order models by exploring joint factorization and codebook strategies to improve pooling schemes and reduce model size. The proposed joint codebook and factorization scheme achieves competitive results at a reduced cost, focusing on representation learning in a retrieval context on image datasets. The paper is organized into sections discussing related work, the factorization with the codebook strategy, and an ablation study. In section 4, an ablation study is conducted on the Stanford Online Products dataset to compare the proposed approach with state-of-the-art methods on image retrieval datasets. The focus is on second-order methods utilizing bilinear pooling, factorization schemes, and codebook strategies for representation learning. The end-to-end trainable Bilinear pooling method is briefly reviewed, which outperforms other second-order representations. Recent works on bilinear pooling focus on computing the covariance of CNN features, using factorization schemes to reduce computation. The high-dimensional representation is improved by new factorization schemes with the objective of reducing computation. Compact Bilinear Pooling reduces high dimensionality of second-order features by analyzing low-rank approximations of the polynomial kernel. It keeps less than 4% of components with minimal loss in performance. The output representation is generated by concatenating these scalars after projecting features using a hyperplane and computing a rank-deficient quadratic form. In Grassmann Bilinear Pooling, a new factorization scheme is presented by Wei et al. that utilizes Singular Value Decomposition (SVD) to take advantage of rank deficient covariance matrices. This approach allows for efficient computation of classifiers over Grassmann manifolds, replacing the number of classes with representation dimension. However, SVD complexity necessitates significant reduction in input feature dimension. The introduction of a codebook strategy improves performances by pooling only related features. Fisher Vectors extend the Bag of Words framework by using a Gaussian Mixture Model and diagonal covariance matrices, resulting in representations of size N(2d+1). FisherNet integrates Fisher Vectors into an architecture. FisherNet integrates Fisher Vectors as a differentiable layer, outperforming non-trainable FVs with high output dimension. MFA-FV network extends MFA-FV by producing second order information embedding trainable end-to-end, combining efficient representation with small latent space. Main drawback is direct computation of second-order information. Our proposed method uses a codebook N = 32 and a projection set of size R = 8. The representation dimension reaches 500k, twice the dimension of Bilinear Pooling. Efficient factorization combined with codebook strategy has not been proposed to exploit the richer representation with a small increase in parameters. Our proposed method combines a codebook and factorization optimization scheme with a similar number of parameters and computation cost to methods without codebook strategies. Sections 3.1 to 3.3 detail the factorization scheme, Kronecker Product, dot product, codebook strategy, and joint codebook and factorization optimization. The advantages and limitations of this approach are discussed. The factorization scheme presented in this paper efficiently reduces the number of parameters needed for projection matrices, utilizing a rank one decomposition to achieve this reduction. This method combines codebook and factorization optimization strategies, offering advantages in terms of parameter efficiency. The factorization scheme in this paper reduces the parameters needed for projection matrices by using a rank one decomposition. Extending second-order pooling to a codebook strategy allows for dimension reduction while preserving rich representation. The codebook pooling assigns similar features to the same codeword, resulting in a more efficient encoding with fewer dimensions. In this approach, a rank one decomposition is used to reduce parameters for projection matrices. By extending second-order pooling to a codebook strategy, dimension reduction is achieved while maintaining a rich representation. The codebook pooling assigns similar features to the same codeword, leading to more efficient encoding with fewer dimensions. The C-CBP approach extends CBP to a Codebook strategy, learning a projection matrix for each codebook entry. It computes second order features for better performance and efficient parameter reduction. The codebook strategy pools related features for encoding with fewer dimensions. The C-CBP approach extends the Codebook strategy by sharing projectors across codebook entries to enhance feature decompositions and reduce parameters. This allows for smaller models with improved performance. The proposed factorization extends the codebook strategy by generating projection matrices U i and V i from sets { U i } i\u2208{1,...,R} and { V i } i\u2208{1,...,R}. This leads to a more efficient equation for computing the fully factorized z transform. The new equation requires fewer parameters and is more efficient than the previous one. The JCF approach introduces a shared projection method, reducing parameters and computation. An ablation study compares different equations, showing efficient learning recombination. Experiments use image retrieval datasets with pre-trained networks like VGG16 or ResNet50, reducing features to 256 dimensions and normalizing them. The assignment function uses softmax over cosine similarity with the codebook. The network is trained in 3 steps using a standard triplet loss function. In the first step, the ResNet50 is frozen and only the added layers are trained with 100 images per batch. The negative is sampled within the batch and a learning rate of 10^-4 is used for 40 epochs. In the second step, ResNet50 is unfrozen and the whole architecture is fine-tuned for 40 epochs with a learning rate of 10^-5 and a batch of 64 images. In the last step, the network is fine-tuned with a batch size of 64. In the last step, the network is fine-tuned with a batch size of 64 images sampled by hard mining the training set with a learning rate of 10^-5 and a margin of 0.1 for the triplet loss. Images are resized to 224x224 pixels for both train and test sets. Different configurations are tested for retrieval tasks, including Baseline, Bilinear Pooling (BP), Compact Bilinear Pooling (CBP), and their codebook extensions (C-BP and C-CBP). The experiment demonstrates the effectiveness of bilinear pooling in image retrieval, showing a 2% improvement over the baseline with a 512 dimension representation. Using a codebook strategy with few codewords further enhances bilinear pooling by 1%, but the number of parameters becomes intractable for codebooks larger than 4. Factorization from equation FORMULA0 reduces the parameter requirements, allowing for exploration of larger codebooks. In this section, the impact of sharing projection on factorization without a codebook is studied. Results show that sharing projectors lead to smaller models with minimal loss in performance. Using richer codebooks allows for more compression with superior results on fine-grained visual classification datasets. The effectiveness of a codebook based factorization scheme is demonstrated on fine-grained visual classification datasets. Comparison with related formulations on FGVC tasks shows the efficiency of dimensionality reduction. Sharing projectors in factorization without a codebook leads to smaller models with minimal performance loss. Richer codebooks enable superior compression and results on FGVC datasets. Our method outperforms multi-rank variants in classification accuracy on FGVC datasets. The importance of grouping features by similarity before projection and aggregation is highlighted, as multi-rank variants lack a selection mechanism for projectors. In this section, the method is compared to the state-of-the-art on 3 retrieval datasets: Stanford Online Products, CUB-200-2011, and Cars-196. The method utilizes similarity-driven aggregation, making it easier to train and explaining the results. In this study, the method is evaluated on three retrieval datasets: Stanford Online Products, CUB-200-2011, and Cars-196. The results show that the codebook factorization outperforms traditional models with fewer parameters. Additionally, a model with shared projections over the codebook has significantly fewer parameters with minimal loss on the CUB-200-2011 dataset. In this paper, a new pooling scheme is proposed for improved performance on Stanford Online Products and Cars-196 datasets. The method achieves state-of-the-art results with more than 10% improvement over the Baseline by utilizing second-order information and a codebook strategy. The pooling scheme is extended with a factorization to reduce parameters and computational cost while maintaining performance. Our method achieves state-of-the-art results on image retrieval datasets like Stanford Online Products and Cars-196, surpassing global average pooling."
}