{
    "title": "rk3mjYRp-",
    "content": "Policy gradients methods often achieve better performance when the change in policy is limited to a small Wasserstein distance. In the small steps limit, policy dynamics are governed by the heat equation, concentrating near actions with high reward. This helps elucidate convergence in probability matching setups and justifies empirical practices in deep reinforcement learning algorithms. In this work, policy gradients iteration converges towards a function of the rewards field using optimal transport metrized by the second Wasserstein distance. Continuous control policy transport is shown to be smooth through the heat flow following the Fokker-Planck equation, shedding light on qualitative convergence. The connection between variational optimal transport and reinforcement learning is explored for the first time, focusing on policy gradients with entropy regularization in bandits and continuous action space settings. The study aims to maximize expected rewards under a policy \u03c0, where \u03c0(a|s) represents the probability of taking action a in state s. The research skips over regularity and integrability questions to simplify formal derivations. Entropic regularization of policy gradients leads to a limit energy-based policy that matches rewards distribution. The dynamics of convergence are investigated, interpreting gradient flow as continuous-time policy iteration. Choosing a Wasserstein-2 trust region solves the Fokker-Planck equation in policy space, leading to policy transport via diffusion and advection. Shannon entropy is added for exploration and to prevent early convergence to suboptimal policies. Entropy-regularised reward is a key concept in policy iteration towards optimal policies. The implicit Euler method is preferred over the explicit Euler method for policy iteration. The method involves finding a solution to the proximal problem and goes beyond just L2 distance regularization. In the context of policy iteration, a gradient flow is obtained in the small step limit. The Wasserstein distance W2 is defined for pairs of measures, and the optimal coupling is called the optimal transport plan. The Monge-Kantorovich problem is reformulated as an equivalent linear problem in L2. Gradient flows are continuous-time analogues of discrete gradient descent steps, while the Euler continuity equation is the continuous-time analogue of discrete Euler methods. The Euler continuity equation in Wasserstein space analysis ensures conservation of mass for flows of measures, with a suitable vector velocity field. The first variation density for entropy regularized solutions is computed to obtain a partial differential equation. The first variation density for entropy regularized policy gradients is computed, leading to a partial differential equation associated with steepest descent within W 2 for entropy-regularized rewards. The entropy-regularized rewards J is convex in the policy \u03c0, ensuring a single optimal policy. The optimal policy is determined by setting the derivative of the policy to zero. The Gibbs measure of rewards density per action is the optimal policy, resembling an energy-based policy. The entropy-regularized reward includes a transport term and the Wasserstein gradient flow transforms entropy into the Laplacian operator. The entropy-regularized reward J(\u03c0) generates an extra transport term in the Fokker-Planck equation. Policy gradient ascent in W 2 is equivalent to solving the Fokker-Planck equation for policy \u03c0 with a potential field equal to the gradient of rewards. The policy undergoes diffusion and advection, concentrating around actions with high rewards. The stochastic diffusion version of the Fokker-Planck equation involves a finite dimensional discretization of policy \u03c0 and a Brownian motion. The text discusses the link between entropy-regularization and noisy gradients in stochastic differential equations for on-policy rewards maximization. It suggests a technique for implementing noisy stochastic action-gradients ascent on the rewards field, emphasizing the need to generate gradient noise equivalent to isotropic Gaussian policy noise. The Kullback-Leibler and Wasserstein-2 problems are related, ensuring convergence under certain conditions. The text discusses the relationship between entropy-regularization and noisy gradients in stochastic differential equations for on-policy rewards maximization. It introduces a technique for implementing noisy stochastic action-gradients ascent on the rewards field, emphasizing the need for generating gradient noise equivalent to isotropic Gaussian policy noise. The Kullback-Leibler and Wasserstein-2 problems are connected, ensuring convergence under specific conditions. The text also delves into the Kantorovich potential function for transport cost and its role in the optimal transport plan. The distribution is crucial in the stochastic differential equation. The W 2 gradient flow solution with discrete steps is explicitly known. Deriving the W 2 optimal transport and its cost at each gradient flow step is computationally expensive. Numerical estimators for Wasserstein distances gradients are biased, with alternatives like the Cramer distance performing better in practice. The gradient flow of the Cramer distance is unknown, and there are no results linking it to entropy and Fisher information functionals. Fast approximate algorithms are used in the small parameter regime. In the small parameter regime, a Gibbs measure related to optimal transport and the Sinkhorn algorithm is discussed. The study of Wasserstein distances is an active research field with foundational developments in Villani's work. Efficient algorithms for regularized optimal transport have been explored, with references to related literature provided. In the context of optimal transport, various researchers have explored regularized optimal transport, the equivalence to steepest descent of KL, and convex formulations. Neural networks, partial differential equations, and convex analysis methods are covered. The Monge-Kantorovich duality and Wasserstein representation gradients are applied to generative adversarial networks. Minimum Kantorovich Estimators are defined for machine learning problems in a Wasserstein framework. Tools of quadratic optimal transport are used to provide a theoretical framework for entropy-regularized reinforcement learning. Our contribution highlights the connection between reinforcement learning and mathematical methods inspired by statistical thermodynamics, specifically the Jordan-Kinderlehrer-Otto result. It explains the advection and diffusion of policies towards the optimal policy, which is the Gibbs measure of rewards. This sheds light on the success of noisy gradient methods and the emergence of Gaussian distributions in distributional reinforcement learning. Our paper explores the connection between reinforcement learning and statistical thermodynamics, focusing on the Jordan-Kinderlehrer-Otto result. We aim to simplify proofs and suggest further research on n-step returns. Additionally, we investigate efficient numerical methods for heat equation flows compatible with function approximation. The study shows that optimal transport can be achieved by entropic regularization of the Wasserstein distance, with a second layer of regularization introduced. The coupling space \u0393(\u00b5, \u03bd) is transformed into a convex problem, improving numerical conditioning. The solution no longer needs to be on a vertex of a convex polytope, making it more robust to initial conditions. Some smoothing is acceptable due to uncertainty in reinforcement learning. Moving the inner product inside the H part simplifies the expression into a single KL divergence, as discussed in Peyr\u00e9 (2015). The link between earlier sections and the reference measure in Peyr\u00e9 (2015) is clear, connecting evolution gradient flows to the heat equation. By performing JKO stepping from DISPLAYFORM4, the problem DISPLAYFORM5 is solved in 2-d coupling space. Entropic smoothing allows the optimal transport problem to be recast as a Kullback-Leibler problem, using iterative convex projection algorithms for computation. This method generalizes the projection on the intersection of convex sets using a convex function \u03a8 and the associated Bregman divergence D \u03a8 Amari. The Bregman algorithm involves minimizing the Bregman divergence D \u03a8 on the intersection of convex sets C = \u2229 C i. By iteratively performing projection on each set C i in a cyclical manner, the algorithm builds a sequence with cyclicality ensured by the modulo operator. This approach allows problems cast under the convex Bregman form to be solved through steepest descent steps. The algorithm involves minimizing the Bregman divergence D \u03a8 on convex sets C by performing steepest descent steps. The optimization problem aims to minimize a convex form subject to marginal constraints, leading to the optimal coupling being a diagonal scaling of the ground cost's Gibbs kernel. The Sinkhorn algorithm is a fast iterative algorithm for optimal transport, involving matrix multiplications and vector operations. It scales well on GPU platforms and can be used to numerically approximate optimal couplings. The algorithm minimizes the Bregman divergence on convex sets by performing steepest descent steps, leading to the optimal coupling being a diagonal scaling of the ground cost's Gibbs kernel. The Sinkhorn algorithm is used for optimal transport, involving matrix multiplications and vector operations. It minimizes Bregman divergence on convex sets by performing steepest descent steps, leading to the optimal coupling being a diagonal scaling of the ground cost's Gibbs kernel. The algorithm involves solving a matrix balancing problem using linear algebra, iterating through the Sinkhorn algorithm till convergence to a fixed point. The Sinkhorn algorithm is a method for optimal transport, involving matrix operations and iterative steps to minimize Bregman divergence on convex sets. It converges linearly and can be viewed as iterative convex projections. The algorithm allows for differentiation of the Wasserstein distance in policies, enabling gradient descent in neural networks. The gradient descent in Sinkhorn layers embedded in neural network systems allows for differentiation of the Wasserstein distance in policies, enabling policy improvement and reducing 'unnaturalness' of mistakes. This compatibility with function approximation and continuous action state settings can help agents pick actions closer to the optimum action. The Wasserstein loss can speed up convergence and training of reinforcement learning agents by implying relevant semantic directions in action space. However, limitations exist due to the assumption that the MDP and statewise rewards density are known. Further exploration includes possibilities like bootstrapping the rewards density distribution."
}