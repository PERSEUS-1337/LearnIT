{
    "title": "SkMwpiR9Y7",
    "content": "To optimize a neural network, it is important to focus on optimizing the function that maps inputs to outputs rather than just the parameters. Calculating distances between functions in a $L^2$ Hilbert space is computationally feasible. The $L^2/\\ell^2$ ratio decreases during optimization, reaching a steady value when test error plateaus. This suggests that the $L^2$ distance could be directly applied to optimization, especially in multitask learning scenarios. In multitask learning, catastrophic forgetting can be avoided by limiting input/output function changes between tasks. A new learning rule constrains network movement in $L^2$-space, allowing minimal interference with previous learning. This approach measures and regularizes function distances directly, without relying on parameters or local approximations. The goal is to converge upon a good input/output function by considering how it changes relative to the space of possible functions. Most techniques and analyses focus on neural network parameters, but it is important to develop metrics directly applicable in function space. This work shows that measuring the distance between networks in function space, specifically in L2-space, is feasible by calculating the expected 2 distance between outputs given the same inputs. The text discusses measuring the distance between networks in function space using the expected 2 distance between outputs. It explores how networks move in function space during optimization and the relationship between trajectories in function space and parameter space. Additionally, it demonstrates how a function space metric could assist in optimization, particularly in multitask learning and preventing catastrophic forgetting. The text discusses a method to prevent forgetting by regularizing changes in the input/output function of early tasks, which is more data-efficient than retraining on old task examples. It also introduces a learning rule called Hilbert-constrained gradient descent (HCGD) that penalizes each step of SGD to reduce the magnitude of the resulting step in L2-space, aiming to track a shorter path in function space. HCGD is related to the natural gradient and aims to track a shorter path in function space by penalizing each step of SGD to reduce step magnitude in L2-space. The norm used is over an empirical distribution of data, not all possible inputs, and can be applied to vector-valued functions in a Hilbert space. The text discusses the use of a normed vector space, specifically a Banach space, to measure distance between functions. It also explores approximating expectations through empirical examples and investigating the quality of estimators for different sample sizes. Additionally, it compares trajectories of networks through parameter and function space using a low-dimensional embedding approach. By saving network parameters and outputs on a validation batch, the text visualizes trajectories of network initializations in function and parameter space. The distance between snapshots at each epoch is computed and visualized in a two-dimensional embedding, showing networks diverging from their initial points. The trajectories of network initializations diverge from their starting points, yet all learn the training data perfectly and generalize with around 50% accuracy. Random initializations lead to similar learning paths in function space, despite starting at different points. During late-stage optimization, random initializations diverge in L2 space, showing that L2 distances behave differently than 2 distances. Functional regularization can aid training and reduce overfitting. The relationship between parameter distances and function distances is crucial for theoretical analyses and optimization methods. Empirical evaluation is needed to assess how well parameter distances correspond to function distances in typical situations. During optimization of a CNN on CIFAR-10, the relationship between parameter and function space changes significantly. Early epochs show larger changes in L2 space for a given change in parameters, while late optimization sees a convergence to a constant ratio between parameter and function distances after test error saturates. This shift in the loss landscape results in a consistent ratio between the two distances. The loss landscape shifts during optimization, leading to a constant ratio between parameter and function distances. The correlation between parameter and function distances varies at different stages of optimization, influenced by Batch Normalization and weight decay. The L2/2 ratio changes during optimization, affected by Batch Normalization and weight decay. The correlation between parameter and function distances varies, with trends differing based on training data. Function distance estimation requires relatively few examples, as shown in FIG4. After training on a task, neural networks often experience 'catastrophic forgetting' when retrained on a new task. This challenge is a central issue in multitask training and online learning, especially in non-IID situations. Efforts to address this problem involve restricting parameter changes between tasks, such as Elastic Weight Consolidation (EWC). The L2 estimator variance is small enough to be accurately estimated with few examples. The L2 estimator variance can be accurately estimated with few examples, as shown in panels A and D. Increasing the number of validation examples improves accuracy. Confidence bounds for the estimation are shown in panels B and E, obtained from the standard deviation of the L2 distance. The standard deviation scales linearly with the L2 distance between updates, but not from initialization, which requires more examples for the same uncertainty. The convergence of the L2 distance estimator between epochs on task A is displayed in the Appendix, multiplied by the diagonal of the Fisher information matrix F. This approach aims to prevent catastrophic forgetting by restricting parameter movement between tasks using a precision matrix calculated on previous tasks. Other similar methods include Bayesian online learning and Synaptic Intelligence (SI), which also utilize diagonal matrices to control parameter changes. The loss for a new task B is modified to include a regularization term that measures the L2 distance between the current function and the function after training on task A. A working memory approach is used to store previously seen examples and calculate the distance between the current iteration and the snapshot after training. This method is novel and memory-efficient for large networks, although it violates strict online learning rules. The working memory approach is memory-efficient for large networks, storing parameters comparable to thousands of MNIST images. Performance was compared on the permuted MNIST task using an MLP with 2 hidden layers. In a study comparing different methods on the permuted MNIST task using an MLP with 2 hidden layers, the working memory approach was found to be memory-efficient for large networks. The method involved keeping 1024 examples from previous tasks and remembering predictions on those examples. The L2 distance was calculated by re-inferring on the examples in working memory and regularizing the distance from current outputs to remembered outputs using a regularizing hyperparameter of \u03bb = 1.3. This method was compared to four other methods, including ADAM and ADAM+retrain. Regularizing the L2 distance on a working memory cache of 1024 examples proved more successful than retraining on the same cache. It outperformed EWC but not SI in preventing catastrophic forgetting. The method involves storing diagonal matrices and old parameters, which were larger in memory than the memory cache. Test performance on the first task improved over 7 subsequent tasks compared to retraining on the same cache. Our method, which uses L2 distance for regularization in a single supervised task, outperforms retraining on the same cache. By constraining the path length of the optimization trajectory in L2 space, we discourage sampling a large volume of parameter space during optimization. This approach helps in preventing catastrophic forgetting and improving test performance on subsequent tasks. The method utilizes L2 distance for regularization in a supervised task, minimizing a new cost at each step to constrain the path length in optimization trajectory. The cost function penalizes the output difference between the current and proposed networks, with an update rule called Hilbert-constrained gradient descent (HCGD). The approach aims to prevent catastrophic forgetting and enhance test performance on subsequent tasks. The rule Hilbert-constrained gradient descent (HCGD) minimizes C in Equation 1 via an inner loop of gradient descent. It optimizes C by replacing C 0 with its first order approximation J T \u2206\u03b8, where J is the Jacobian. The algorithm decreases the distance traveled in function space and is shown in Algorithm 1. The algorithm presented in Algorithm 1 for Hilbert-constrained gradient descent (HCGD) includes a velocity term that is updated with the final Hilbert-constrained update \u2206\u03b8. This modification of momentum has been found to quicken optimization and reduce generalization error. The natural gradient also regularizes the change in space. The natural gradient, similar to HCGD, regularizes the change in functions' output distributions using the Kullbeck-Leibler divergence. Optimization involves minimizing the change in network's output distribution P \u03b8 with a regularization term controlled by hyperparameter \u03bb. The natural gradient optimizes the change in output distribution by minimizing the Kullbeck-Leibler divergence with a regularization term controlled by hyperparameter \u03bb. The optimization process involves evaluating the KL divergence directly and expanding it around \u03b8 to second order with respect to \u03b8. The Hessian of the KL divergence is the Fisher information metric F, and the regularized cost function is optimized via gradient descent by replacing C 0 with its first order approximation. The natural gradient emerges as the optimal update when \u03bb = 1. The natural gradient optimizes output distribution changes by minimizing KL divergence with a regularization term. HCGD approximates natural gradient with inner optimization loop, requiring fewer computations. Variants of natural gradient offer benefits like data efficiency and faster optimization. The natural gradient optimizes output distribution changes by minimizing KL divergence with a regularization term. It uses the Fisher information matrix to scale updates based on parameters' informativeness. Whitening gradients through techniques like HCGD ensures steps are made in a parameter space that is whitened by the covariance of the gradients. Many methods have been proposed in the literature to normalize and whiten activations or gradients, aiming to make parameter space a better proxy for function space. Comparisons between HCGD and SGD on different architectures suggest that HCGD may improve upon SGD by limiting changes in function space. Tuned learning rates were used for both algorithms, with values of \u03bb = 0.5 and \u03b7 = 0.02 typically being much lower than the principal learning rate. We tested HCGD on the CIFAR-10 image classification problem using Squeezenet v1.1 model. HCGD does not outperform SGD with the same learning rate, but performs better in the early stage with a higher learning rate. HCGD shows improved performance compared to SGD in early stages with a higher learning rate, but requires a different learning rate schedule. It outperformed SGD on a sequential MNIST task with permuted pixels, but underperformed compared to ADAM. HCGD proposes an update by SGD and then corrects it, showing that Adam can also be improved by penalizing the change in function space. This combination of Adam and L2 functional regularization could achieve state-of-the-art performance on recurrent tasks. It is crucial to discuss the relationship between function space and parameter space in neural networks. The L2 Hilbert space over an input distribution is a useful space for analysis in neural networks. Networks traverse this function space differently than parameter space, with the distance of parameters not always representing a proportional distance between functions. Two possibilities for using L2 distance directly in applications were proposed, including addressing multitask learning. The proposed approach involves using SGD+HC or Adam+HC steps, with the former showing improved performance with more iterations. A regularization term is used to maintain the function across tasks without changing the network architecture. This method outperforms retraining on stored examples and is more efficient for large networks compared to methods like EWC and SI. Additionally, a learning rule called Hilbert-constrained gradient descent (HCGD) limits movement in function space during optimization, similar to how gradient descent limits parameter movement. HCGD can improve test performance in image classification by limiting function movement during optimization, similar to how gradient descent limits parameter movement. However, it may not always lead to better results, suggesting that other principles may be more important for generalization. Using an L2 norm for optimization is not the only possibility, as it could also be used to regularize the confidence of the output function. Further exploration is needed to see if architectural methods like normalization layers could be designed with the L2 norm in mind. Architectural methods, like normalization layers, could be designed with the L2 norm in mind. It is interesting to consider if there is support in neuroscience for learning rules that reduce the size of changes with a large effect on other tasks. Behavioral learning rates in motor tasks are dependent on error direction but independent of error magnitude, which is unexpected by most gradient descent models. Regularization on behavioral change rather than synaptic change could result in slow learning rates for neurons central to many actions, such as motor neurons in the spinal cord. Weight decay has a strong effect on reducing the distance traveled during optimization at all scales, especially late in the process. It also increases the L2 distance from the last epoch during optimization. Regularization of behavioral and perceptual change may play a role in neural learning processes. The L2 distance from the last epoch increases in scale during optimization without weight decay, but decreases with weight decay. Weight decay has a strong effect on the L2/2 ratio, which changes considerably throughout training. Movement in function space mostly occurs early in optimization, with a higher standard deviation of the L2 estimator compared to CIFAR-10. The relationship between parameter distance traveled and function distance is similar to a CNN on CIFAR-10. The HCGD algorithm aims to reduce motion in L2-space during optimization for a simple MLP trained on MNIST. It shows that while SGD continues to drift in L2-space even after test error saturates, HCGD plateaus, indicating convergence to a single location. The HCGD algorithm allows parameters to drift even after convergence, includes momentum and multiple corrections. A natural gradient algorithm is proposed for comparison, aiming to minimize Equation 2 efficiently. The HCGD algorithm includes momentum and multiple corrections, allowing parameters to drift even after convergence. A natural gradient algorithm is proposed for efficient minimization of Equation 2, using matrix-vector products instead of inverting large matrices. The algorithm corrects update steps towards the natural gradient in an inner loop during each update step. Starting with a fast diagonal approximation like Adagrad or RMSprop, additional corrections require just one matrix-vector product after calculating gradients, leading to improved updates with a small number of iterations. The Fisher matrix F can be calculated from the covariance of gradients, allowing for efficient computation of the array of per-example gradients on the minibatch. This method, known as the 'empirical Fisher', is crucial for proper optimization in deep learning frameworks. To efficiently compute per-example gradients on a minibatch, the Fisher matrix F can be calculated from the covariance of gradients. This is crucial for proper optimization in deep learning frameworks. Additionally, one can calculate G from the predictive distribution of the network, P \u03b8 (y|x), by sampling randomly from the output distribution and re-running backpropagation on fictitious targets. Alternatively, G can also be calculated using unlabeled or validation data on each batch."
}