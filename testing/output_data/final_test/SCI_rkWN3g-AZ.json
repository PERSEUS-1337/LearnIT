{
    "title": "rkWN3g-AZ",
    "content": "Style transfer, specifically semantic style transfer, involves applying color and texture information from one image collection to another while preserving shared semantic content. XGAN (\"Cross-GAN\") is introduced as a dual adversarial autoencoder that learns a mapping between the style of two image collections and preserves semantic content. A semantic consistency loss is utilized to maintain semantics in the learned embedding space. Promising results are reported for face-to-cartoon translation using a cartoon dataset collected for this purpose. The curr_chunk discusses the use of image-to-image translation for tasks like style transfer, colorization, super-resolution, and semantic segmentation. It highlights the importance of supervision in tasks like colorization and introduces a new benchmark dataset for face-to-cartoon translation. The proposed XGAN for semantic style transfer aims to adapt images to the visual appearance of another domain while preserving important face semantics like hair style and face shape. The goal is to map images from one domain to another's style without changing the semantic content, focusing on retaining higher-level semantic content in the feature space rather than pixel-level structure. The experiment involves translating faces to cartoons while maintaining various facial attributes like hair and eye color. The paper focuses on semantic style transfer, a many-to-many mapping problem, where images can be transformed between different domains without changing their key features. It specifically looks at converting faces to cartoons while preserving facial attributes like hair and eye color. The approach relies on two unlabeled training image collections for each domain, facing challenges of global domain appearance and content distribution shifts. XGAN (\"Cross-GAN\") is proposed for semantic style transfer, focusing on transformations between different image domains while preserving key features. Unlike previous models, XGAN introduces a constraint in semantic space in addition to pixel-level constraints for more general transformations. XGAN (\"Cross-GAN\") is a dual adversarial autoencoder that learns a shared semantic representation of two input domains in an unsupervised way, enabling domain-to-domain translations while preserving semantic feature-level information. In Section 4, CARTOONSET dataset of cartoon faces for semantic style transfer research is introduced. Section 5 discusses experimental results of XGAN on face-to-cartoon task and ablation experiments. Recent literature suggests traditional style transfer and pixel-level domain adaptation for semantic style transfer task, highlighting their limitations. Inspired by domain adaptation and image-to-image translation, style transfer involves transferring texture while preserving structure. XGAN focuses on learning a shared semantic representation for unsupervised domain adaptation, aiming to overcome limitations of traditional style transfer methods. Recent domain adaptation work focuses on semantic style transfer by mapping synthetic images to natural images. Unlike previous works, they consider pixel-level transformations and use a cycle-consistency loss to ensure good results for image-to-image translation tasks. The proposed semantic consistency loss in image-to-image translation focuses on feature-level transformations, unlike UNIT which relies on pixel-level cycle-consistency. XGAN introduces a semantic consistency component explicitly, while DTN uses self-supervised semantic-consistency feedback for domain transfer. The DTN encoder is pretrained and fixed for feature-level image-to-image translation, specifically for the face-to-cartoon problem. However, using a fixed encoder does not generalize well with large domain shifts between input domains. The XGAN architecture is a dual autoencoder on domains D1 and D2, aiming to learn a joint domain-invariant embedding that captures semantics without supervision. The training objective includes components like reconstruction loss and domain-adversarial loss to ensure meaningful knowledge encoding and alignment of embeddings from both domains in the same subspace. The XGAN architecture utilizes domain-adversarial loss, semantic consistency loss, GAN objective, and optional teacher loss to bridge the domain gap at a semantic level, preserve input semantics, generate realistic samples, and distill prior knowledge. The total loss function is controlled by hyper-parameters, with a focus on reconstruction loss to encode domain information for input reconstruction. The XGAN architecture uses domain-adversarial loss to ensure embeddings from different domains lie in the same subspace. A binary classifier is trained to categorize encoded images, encouraging cross-domain transformations. Additionally, a semantic consistency loss is introduced as a key contribution to the model. The XGAN architecture introduces a semantic consistency feedback loop to reinforce domain translations and preserve learned embeddings, in addition to using domain-adversarial loss for cross-domain transformations. XGAN aims to learn a joint meaningful and semantically consistent embedding by generating realistic image transformations. The transformed distribution is made close to the original domain to improve sample quality. The models are trained jointly in an adversarial scheme with state-of-the-art GAN objectives. XGAN introduces an optional component, L teach, to incorporate prior knowledge in a semi-supervised framework. L teach encourages embeddings to align with a teacher network's output, serving as a form of regularization. It should not be used for both domains simultaneously to avoid conflicting embeddings. The XGAN model introduces L teach as a regularization component to align embeddings with a teacher network's output. It should not be used for both domains simultaneously to prevent conflicting embeddings. The model uses a mirrored convolutional architecture for the autoencoder, encouraging shared representations at different levels. The teacher network utilizes the highest convolutional layer of FaceNet, a state-of-the-art model pretrained for face recognition on natural images. The XGAN model utilizes adversarial training with GAN and dann losses to optimize the generators and discriminator. The training process involves updating generator parameters, fixing them, then updating discriminator parameters in an alternating fashion. The adversarial training for dann involves connecting the classifier and embedding layer with a gradient reversal layer for implementation. The XGAN model uses adversarial training with GAN and dann losses to optimize generators and discriminator. A gradient reversal layer is used to update encoder parameters in the same step as the generator. A new dataset, CartoonSet, is introduced for transforming frontal faces to a specific cartoon style, with 16 components including facial and color attributes. The model is trained using ADAM optimization with an initial learning rate of 0.0001. The XGAN model utilizes adversarial training with GAN and dann losses to optimize generators and discriminator. A new dataset called CartoonSet is introduced for transforming frontal faces into a specific cartoon style, consisting of 16 components with various attributes. The artwork components in CartoonSet are divided into 8 layers that define a Z-ordering for rendering, ensuring correct placement and visual appeal of the cartoon. The CartoonSet dataset is created by randomly sampling attributes for cartoon faces and filtering out unrealistic combinations, resulting in approximately 9,000 realistic cartoons. The XGAN model is evaluated for semantic style transfer from frontal faces to cartoon avatars using unpaired samples. The source domain consists of real-world frontal-face images from the VGG-Face dataset, while the target domain is cartoon avatars. The faces are aligned based on eyes and mouth location, and the background is removed. The cartoon images are center-aligned by localizing the center of the irises, mouth, and nose tip. 20% of images are randomly selected for testing, and the rest are used for training. All images are resized to 64x64. The two domains vary significantly in appearance, with cartoon faces being simplistic compared to real faces. The cartoon faces differ significantly from real faces in appearance, with limited variety in features like noses and eyebrows. There is a shift in content distribution between the two domains, with certain hair colors and eyeglasses being more prevalent in the cartoon dataset. The Domain Transfer Network (DTN) and XGAN are compared for semantic style transfer, with DTN considered the current state of the art for this task. The Domain Transfer Network (DTN) and XGAN were compared for semantic style transfer, focusing on transferring face pictures to cartoons. Evaluation metrics for style transfer are still being researched, with optimal hyperparameters chosen manually for accurate transfer of semantic attributes. DTN failed to capture the transformation function for stylizing frontal faces to cartoons, while XGAN produced sensible results. XGAN outperformed DTN in transferring frontal faces to cartoons by producing sensible results in terms of style and semantic similarity. Despite some failure cases like hair or skin color mismatch, XGAN demonstrated better semantic consistency abilities. The failure of DTN was attributed to its fixed joint encoder assumption, leading to poor preservation of semantics across domains. Additional samples can be found in FIG3 and Appendix 7.3. The decoder in FaceNet struggles with domain transfer due to poor quality samples. Bitmojis in a different study are visually closer to real faces, explaining their good performance with a fixed encoder. Using a fixed encoder is restrictive and does not adapt well to new scenarios. Fine-tuning the DTN yields better quality samples but is sensitive to training hyperparameters and prone to mode collapse. Ablation study on XGAN reveals insights on training hyperparameters and mode collapse. Experimenting with different losses shows improvements in sample consistency and quality. GAN loss is crucial for producing good quality samples. The model in this setting is robust to hyperparameter choice and does not require much tuning to converge to a good regime with low reconstruction error and around 50% accuracy for the domain-adversarial classifier. Applying each decoder to the output of the other domain's encoder yields reasonable cross-domain translations, preserving simple semantics like skin tone or gender. Failure modes occur in extreme cases, such as when the model capacity is too small or when \u03c9 dann is too low. Adding semantic consistency in XGAN through sharing high-level layers in the autoencoder helps capture common semantics earlier in the architecture. However, sharing only the middle layer does not capture relevant shared domain semantics. Using semantic consistency loss as self-supervision for the learned embedding ensures better results. The semantic consistency loss is used as self-supervision for the learned embedding in XGAN, reinforcing the domain-adversarial loss and optional teacher loss. The teacher loss guides the source embedding to align with FaceNet's representation layer, improving the learned embedding's quality. In FIG4, test samples show the impact of removing the teacher loss and semantic consistency loss on domain-to-domain translations. The teacher network has a positive regularization effect on the learned embedding. Training the model without the teacher loss leads to more distorted samples, especially with outliers like people wearing hats or cartoons with unusual hairstyles. The semantic consistency loss improves sample variety and preserves rare attributes better. XGAN aims to address semantic style transfer in an unsupervised manner by combining domain adaptation and image-to-image translation techniques. Adding the GAN loss and semantic consistency loss to XGAN improves sample quality and captures fine-grained attributes better. However, there are still failure cases, especially with nonrepresentative input samples. The teacher loss helps regularize the learned embedding but requires additional supervision. Future work will focus on evaluating XGAN on more tasks beyond semantic style transfer. In this work, XGAN is introduced as a model for unsupervised domain translation for semantically-consistent style transfer. The model focuses on learning image-to-image translation between structurally different domains by utilizing a high-level joint semantic representation and a semantic consistency loss for self-supervision. Experimental results show improved performance in mapping face images to cartoon avatars, with the option to incorporate weak supervision through a pretrained feature representation. The curr_chunk discusses the addition of teacher knowledge to a model as a regularizer for learned embeddings and generated samples. It also describes the architecture of an autoencoder for image translation using convolutional and deconvolutional blocks. The embedding vector is normalized and cosine distance is used for comparisons. The architecture of the model includes a sequence of deconvolutions to generate a 64x64 color image. Shared convolutional blocks are used to encourage shared representations across domains. The discriminator has a similar architecture to the encoder but outputs a single logit for binary classification. XGAN architecture details are also provided. The model architecture includes deconvolutions for image generation, batch normalization, and ReLU activation. Experimenting with the DTN showed issues with fixed pretrained encoders. Finetuning the FaceNet encoder improved results but raised domain adaptation concerns. XGAN uses a teacher network for knowledge distillation. Finetuned DTN is prone to mode collapse. The finetuned DTN is prone to mode collapse due to training only on semantic consistency loss, leading to the same cartoon being generated for all domains. XGAN uses source domain reconstruction loss for regularization, allowing for joint domain embedding learning from scratch. Fine-tuning the FaceNet encoder yields better results but is unstable for training in practice. The DTN baseline fails to capture a shared embedding for input domains. Three models are experimented with: Finetuned DTN, XGAN with L rec and L dann active, and the full XGAN model. The full XGAN model produces the best visual samples. The study focused on the components and failure modes of the XGAN model, specifically for transforming faces to cartoons. The model is robust to hyperparameters, with the main failure being mismatched hair color. The generated cartoon samples closely resemble the original dataset, preserving key identity characteristics. The XGAN model for transforming faces to cartoons is robust to hyperparameters but often fails in mapping hair color accurately. Mistakes in the generated samples are mainly due to content distribution shifts between domains rather than style distribution. Examples include bald faces being mapped to cartoons with light hair and eyeglasses on cartoon faces disappearing when mapped to real faces. In image generation applications, the importance of the GAN objective is highlighted, as it leads to more realistic samples compared to using simpler regularization techniques. The failure cases when the domain-adversarial loss is too high show the impact on cross-domain translation, emphasizing the need for the GAN loss term for source \u2192 target translation."
}