{
    "title": "B1VPIA7iM",
    "content": "In this paper, Sliced-Wasserstein Autoencoders (SWAE) are introduced as generative models that utilize optimal transport and Wasserstein distances to shape the latent space distribution without the need for adversarial training. The formulation offers similar capabilities to Wasserstein Autoencoders (WAE) and Variational Autoencoders (VAE) with a simple implementation. Scalable generative models like GANs and VAEs play a key role in various machine learning applications, enabling unsupervised modeling of high-dimensional data distribution. The approach to generative modeling is influenced by optimal transport theory, aiming to minimize dissimilarity between data and model output distributions. The optimal transport problem provides a way to measure distances between probability distributions by morphing one distribution into another. Wasserstein distances, arising from this problem, are true distances that metrize a weak convergence of probability measures. Recently, there has been interest in Wasserstein distances in the learning community due to their geometric characteristics. A new type of autoencoder for generative modeling, called Sliced-Wasserstein, is introduced in this paper. Sliced-Wasserstein Autoencoders (SWAE) are a new type of autoencoders for generative modeling that minimize the sliced-Wasserstein distance between encoded samples and a predefined distribution. This approach avoids costly adversarial training and is not limited to closed-form distributions, benefiting from a Wasserstein-like distance measure for a simple numerical solution. The paper provides a review of preliminary concepts, formulates the method, presents a numerical scheme for solving the problem, summarizes experiments, and concludes the work. The paper introduces Sliced-Wasserstein Autoencoders (SWAE) for generative modeling, focusing on minimizing the sliced-Wasserstein distance between encoded samples and a predefined distribution. It discusses the use of Random Variable Transformation (RVT) in encoding input data points into latent codes using Variational Auto-Encoders (VAEs). The paper introduces Sliced-Wasserstein Autoencoders (SWAE) for generative modeling, emphasizing the minimization of the sliced-Wasserstein distance between encoded samples and a predefined distribution. It discusses the use of Random Variable Transformation (RVT) in encoding input data points into latent codes using Variational Auto-Encoders (VAEs), focusing on learning \u03c6 and \u03c8 to minimize dissimilarities between distributions p Y and p X, and p Z and q Z. Defining and implementing the dissimilarity measure is a key design decision in VAEs. The VAE work often assumes stochastic encoders and decoders, but we focus on deterministic mappings. Various dissimilarity measures have been used between distributions p X and p Y, including f-divergences and Wasserstein distances. Nowozin et al. showed that minimizing these divergences leads to a min-max problem equivalent to adversarial training. Others have utilized the OT problem and Wasserstein distances in generative modeling. The problem of finding the Wasserstein distance between distributions p X and p Y was reformulated as a min-max optimization. BID4 simplified the distance calculation using an autoencoding approach, showing that it can be implemented easily for i.i.d samples. This method takes advantage of available pairs of x n and y n, making the transport distance calculation straightforward. The total transport distance is defined as the sum of Euclidean distances between all pairs of points. W c (p X , p Y ) is used to measure the discrepancy between p X and p Y. Methods for measuring the discrepancy between p Z and q Z are reviewed. The log-likelihood of z = \u03c6(x) with respect to q Z is used to minimize the KL-divergence between p Z and q Z. Various alternatives exist in the literature to address limitations in measuring the discrepancy between distributions supported on low-dimensional manifolds. Tolstikhin et al. proposed GAN-based and maximum mean approaches for this purpose. The GAN-based and MMD-based approaches are used to measure the discrepancy between distributions. The GAN approach defines a discriminator network to classify samples, while the MMD approach uses a reproducing kernel. An alternative is the Wasserstein distance, known for measuring distances between distributions on low-dimensional manifolds. The sliced-Wasserstein metric is proposed as a method to measure the discrepancy between distributions on low-dimensional manifolds. It eliminates the need for training an adversary network and offers a simple numerical implementation, contrasting with GAN-based approaches. Autoencoders simplify the problem of matching high-dimensional point clouds compared to GANs by establishing correspondences between the input and output data. The Sliced Wasserstein Autoencoders (SWAE) method introduces a nonlinear dimensionality reduction to solve a simpler problem. The approach involves understanding the Wasserstein and sliced-Wasserstein distances between probability measures. The Wasserstein distance between probability measures \u03c1 X and \u03c1 Y is defined as an optimization problem involving transportation plans and costs. It can be defined based on diffeomorphic mappings and the Monge formulation of the problem. The Wasserstein distance involves solving an optimization problem and has efficient techniques for calculation. For one-dimensional probability densities, the closed-form solution of the Wasserstein distance can be obtained. This motivates the definition of sliced-Wasserstein distances, which are easier to compute and have similar properties to the Wasserstein distance. The sliced-Wasserstein distance is used for barycenter calculations in various studies and for learning Gaussian mixture models. It involves slicing higher-dimensional probability densities into one-dimensional distributions and comparing them using Wasserstein distance. The slicing process is related to Integral Geometry and the Radon transform, allowing for unique representation of d-dimensional probability densities. The sliced-Wasserstein distance involves representing d-dimensional probability densities as one-dimensional marginal distributions using the Radon transform. This allows for comparing distributions using Wasserstein distance, with a closed-form solution for the integrand. The sliced Wasserstein distance involves comparing distributions using a natural transportation cost function. The proposed formulation for the Sliced Wasserstein Autoencoder (SWAE) includes encoder, decoder, data distributions, and encoded data distribution. The sliced Wasserstein distance is used to measure the difference between distributions in the Sliced Wasserstein Autoencoder (SWAE). It involves comparing distributions using a transportation cost function, with a hyperparameter \u03bb determining the importance of loss functions. The Wasserstein distance is used for pX and pY, while the sliced-Wasserstein distance is used for pZ and qZ due to the lack of correspondences between the distributions. The Wasserstein distance is approximated by sorting samples and calculating a sorting problem efficiently. In scenarios with only samples from a d-dimensional distribution, the empirical distribution can be estimated, and marginal distributions can be obtained. The sliced-Wasserstein distance is minimized by integrating over the unit sphere in Rd, but can be approximated using random samples of Sd\u22121 at each step. This approach allows for a good approximation of SWc(pZ, qZ) without the need for expensive fine sampling. The sliced-Wasserstein distance is minimized by integrating over the unit sphere in Rd, approximated using random samples of Sd\u22121. A stochastic gradient descent scheme is used to optimize the SWAE objective function, involving random sampling of input data and projection angles from Sd\u22121. The loss function in Eq. FORMULA1 can be rewritten using numerical approximations. Sorting is an optimization problem, and the gradient descent update on \u03c6 and \u03c8 is a min-max problem solved alternately. SWAE results for MNIST and CelebA datasets are shown using deep convolutional neural networks. Implementation details are in the Supplementary material. The SWAE model is used to map handwritten digits into a two-dimensional embedding space for visualization. Four different distributions were chosen for the experiment, and results on the MNIST dataset show that the embedding space closely matches the predefined distributions. Implementation details can be found in the supplementary material. In experiments with the CelebA dataset, a higher-dimensional embedding space (K = 128) was found necessary as the SWAE loss function improved but the decoder struggled to match certain distributions. The SWAE was trained on CelebA with a (K = 128)-dimensional uniform distribution for q Z, and experiments showed that linear combinations of encoded faces resulted in new faces in the embedding space. In the embedding space, two experiments were conducted to verify convexity. Linear interpolations of face pairs were visualized, showing faithful results. Principle Component Analysis revealed various modes like hair color and gender. Sliced Wasserstein Autoencoders were introduced to shape encoded sample distributions. Utilizing the sliced Wasserstein distance as a dissimilarity measure between encoded sample distributions and a predefined distribution eliminates the need for adversarial network training in the embedding space. A numerical scheme for this problem, requiring few inner products and sorting operations, was provided. The method was tested on MNIST and CelebA datasets, yielding results comparable to techniques with adversarial training. Public implementation available. Supported by NSF (CCF 1421502). Acknowledgments to Drs. Dejan Slep\u0107ev and Heiko Hoffmann. The Jensen-Shannon divergence and Wasserstein distance are compared in a simple example. Jensen-Shannon divergence fails when distributions are on non-overlapping domains. The text discusses maximizing or minimizing similarity between distributions on non-overlapping domains using a specific formula. It also includes a demonstration of SWAE results in a manifold learning experiment."
}