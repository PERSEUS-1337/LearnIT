{
    "title": "rJeQoCNYDS",
    "content": "Transfer and adaptation to new unknown environmental dynamics is a key challenge for reinforcement learning. A proposed algorithm optimizes a probe and an inference model to estimate latent variables of test dynamics for single episode transfer in related environments. This approach integrates state-of-the-art algorithms for variational inference or RL without requiring access to rewards at test time. Our method excels in single episode transfer tasks, outperforming existing adaptive approaches and showing strong performance against baselines. The challenge lies in achieving success in one attempt, similar to human intelligence in recognizing critical characteristics of a task instance. This is crucial in precision medicine where each patient's response to therapeutics is unique. Adaptive therapy in precision medicine involves formulating treatment strategies as sequential decision-making problems. Variability in patient responses requires early detection of dynamics, such as through blood measurements. A proposed algorithm aims for near-optimal test-time performance by training an inference model and probe to address single episode transfer tasks efficiently. The approach involves training an inference model and probe to achieve rapid inference of latent variables in dynamical systems using a small fraction of the test episode. A universal policy is then deployed for near-optimal control on new instances, adapting to the new environment by estimating new latent dynamics. This method optimizes performance in one test episode without accessing the reward function at test time. Our method enables rapid inference of latent variables and immediate execution of a universal policy for single episode transfer in varying environments, achieving higher cumulative rewards with significantly faster computation time during test. Our method outperforms state-of-the-art model-based approaches on high-dimensional domains with discontinuous and continuous dynamics, showing superior performance in meta-learning and robust transfer. The goal is to train a model that performs optimally within a single episode of a test instance with unknown dynamics, formalized as a Markov decision process. Each instance has hidden parameters sampled from a distribution, determining transition dynamics. In 2014, each T z has a hidden parameter z \u2208 Z sampled from a distribution P Z. For stochastic systems, Z is induced by a latent variable model associating T z to a learned latent variable z from observed data. Latent variables are never observed to apply to cases without prior knowledge. Latent space Z has physical meaning in systems with continuous functions of physical parameters. Latent variables can have a significant impact on dynamics, such as in a 2D navigation task. Training approach is compatible with RL for episodic environments, focusing on known instances with controllable changes. Rare cases of unknown demarcation can be addressed differently. SEPT is introduced as a high-level algorithm for single episode transfer between MDPs with different dynamics, emphasizing the need for near-optimal performance in critical applications like precision medicine. The algorithm imposes the constraint that the trained model is evaluated on only one episode of a new test instance, reflecting the urgency of adapting quickly to new scenarios. The approach contrasts with prior methods that rely on multiple experience rollouts for adaptation during test time. SEPT is designed for single episode transfer between MDPs with different dynamics. It focuses on near-optimal performance in critical applications and involves learning latent variables, training a universal policy, and deploying both models for quick adaptation in a new test episode. This approach contrasts with traditional methods that require multiple experience rollouts for adaptation. SEPT focuses on single episode transfer between MDPs with different dynamics, utilizing expressive variational inference models for uncertainty quantification. A universal policy is trained based on latent variables, allowing for immediate deployment on new instances without the need for further fine-tuning, crucial for applications with safety constraints. In the training phase, a variational auto-encoder is used to generate optimized short trajectories for a universal policy. The model estimates a latent representation of dynamics and uses domain knowledge to choose the dimensionality reduction. The dynamics of natural systems are determined by independent parameters, allowing for a flexible approach in modeling. In the context of modeling natural systems with independent parameters, a \u03b2-VAE is used to create a disentangled latent representation. The latent units capture the effects of generative parameters independently, allowing for training by maximizing the variational lower bound. The meaning of latent variables is not affected by isometries, and only the partition of latent space is crucial for training. The use of variational inference in training a universal policy is more economical compared to earlier methods using Bayesian neural network approximations. The encoder produces low-variance encodings by minimizing entropy, while the decoder plays a crucial role in optimized probing. The encoder q \u03c6 (z|\u03c4 ) is regularized by adding \u2212H(q \u03c6 (z|\u03c4 )) to equation 1. To capture higher-order dynamics, a bidirectional LSTM is used to parameterize the encoder, while an LSTM decoder is used for trajectory encoding. This approach differs from embedding trajectories from a single MDP for hierarchical learning. Our goal is to encode trajectories from various transition dynamics for optimal control using a universal policy \u03c0(a|s, z). Previous methods trained multiple optimal policies on training instances with hidden parameters and used behavioral cloning or off-policy Q-learning. However, this supervised training scheme may not be robust. To improve generalization and computational efficiency in training a policy model, we interleave training of a VAE and a single policy \u03c0(a|s, z). This approach allows for significant computation savings and higher robustness by increasing the effective sample count. Rapid computation of initial experience trajectories is used to optimize inference in the initial phase, similar to preliminary medical treatment guiding subsequent prescriptions. During the initial phase of training, optimizing the trajectory for inference by the policy model is crucial. The VAE should be trained on short trajectories from a dataset to ensure efficient inference. Failure to probe specific dimensions can lead to adversarial cases where certain dynamics are not exercised, impacting the effectiveness of the model. Our algorithm focuses on learning to distinguish dynamics by generating and using short initial trajectories for control. We train a probe policy to optimize latent variable inference and ensure VAE performance on short trajectories. This approach is different from training a meta-policy for exploration in standard RL, as it aims to perform well on new test environments. SEPT method allows off-policy batch data usage. Training episodes start with probe phase using \u03c0 \u03d5 for T p steps, recording probe trajectory \u03c4 p into D. VAE is trained using minibatches from D, then \u03c4 p is used with the encoder to generate \u1e91 for \u03c0 \u03b8. At test time, SEPT only requires lines 5, 8, and 9 in Algorithm 1. Reward function for \u03c0 \u03d5 is VAE objective, approximated by variational lower bound. To train the VAE, use minibatches from dataset D for gradient ascent and descent. Estimate latent variables \u1e91 from probe trajectory \u03c4p with encoder q\u03c6(z|\u03c4). Execute policy \u03c0\u03b8(a|s, z) for remaining steps and train with RL algorithm. Use probe to aid VAE's inference of latent variables distinguishing dynamics. Gradient of entropy H(p\u03d5(\u03c4)) is crucial for training VAE. Optimal probe trajectories are generated by descending the gradient for maximum effectiveness. The text discusses the importance of optimal probing for training a VAE and how it differs from diverse exploration. It also highlights the defining equation of a simple policy gradient algorithm for training the policy. The reward function R_p(\u03c4) is justified as being more effective than other choices. The text discusses using a second VAE to provide a more stable reward signal for the policy \u03c0 \u03d5 in training. Transfer learning in a family of MDPs with different dynamics is also explored. The text discusses the theoretical grounding of RL algorithms in MDPs and HiP-MDPs, which implement transfer learning through latent variable inference models. Unlike POMDPs, HiP-MDPs do not assume hidden parameters but induce them through latent variables. Our approach in transfer learning through latent variable inference models in HiP-MDPs differs from prior methods that rely on multiple optimal policies or surrogate transition models. We use a variational model-free approach that encodes trajectories to infer latent representations of varying dynamics. This approach avoids high computational costs associated with other methods. Our method in transfer learning through latent variable inference models in HiP-MDPs differs from prior approaches by encoding trajectories to infer latent representations of dynamics. This contrasts with existing methods that rely on optimal policies or transition models, avoiding high computational costs. Our method, SEPT, differs from prior approaches by not requiring a transition function and can be deployed without optimization during test. It outperformed five baselines in single test episodes across three benchmark domains, showing improved performance, speed of reward attainment, and computational efficiency. SEPT was evaluated with four ablation variants to study algorithmic design choices. Each method underwent 20 independent training runs and evaluation on 20M test instances per method per domain. Hyperparameters were adjusted using a coarse coordinate search. DDQN with prioritized replay was used as the base RL component. Domains consisted of continuous state discrete action HiP-MDPs proposed by Killian et al. (2017). Each isolated instance from each domain is solvable. In various domains, isolated instances are solvable by RL, but optimal performance is challenging due to different dynamics requiring different policies. For example, in 2D navigation, dynamics include barrier locations, flipped action effects, and wind direction. In Acrobot, torques are applied to swing a pendulum with dynamics determined by masses and lengths. HIV dynamics are modeled by differential equations with high sensitivity to hidden factors. The patient's state dynamics are modeled by differential equations with high sensitivity to hidden variables, requiring unique treatment policies. Different actions determine the activation of two drugs. Different domains have different dynamics that require specific policies for optimal performance in reinforcement learning. In single-episode test evaluation, various model-based and model-free methods are compared, including BNN, EPOpt-adv, and MAML. These methods use different strategies for training policies and optimizing performance in reinforcement learning tasks. The study also introduces an optimized probing method called SEPT-NP to accelerate inference in these tasks. SEPT-NP is a method for accelerated inference in reinforcement learning tasks. It involves using trajectories generated by the control policy to train the VAE. Different reward functions such as TotalVar and MaxEnt are tested for probe trajectory generation. DynaSEPT, an extension that dynamically decides to probe or execute control, is also evaluated. SEPT outperforms other methods in solving 2D navigation and Acrobot tasks. SEPT outperforms baselines in 2D navigation, requiring fewer steps to solve. MAML struggles despite rewards at test time, while BNN takes significantly longer and more steps than SEPT. SEPT is statistically faster than BNN and Avg, competitive in solving instances, robust to out-of-training dynamics, and outperforms other methods in cumulative rewards on HIV. SEPT outperforms BNN and Avg in cumulative rewards, showing significant improvement in settings with large discontinuous effects on dynamics like 2D navigation. SEPT outperforms BNN and Avg in cumulative rewards, showing significant improvement in settings with large discontinuous effects on dynamics like 2D navigation. Interpolation-based methods like SEPT work better than BNN in cases such as Acrobot and HIV, as SEPT explicitly distinguishes dynamics and does not require learning a full transition model. SEPT does not need rewards at test time, making it useful for a broader range of problems compared to optimization-based meta-learning approaches like MAML. TotalVar gives slight improvement in Acrobot but SEPT significantly outperforms it in 2D navigation and HIV, indicating that using VAE performance directly as the reward for probing can be more effective in certain environments. SEPT outperforms DynaSEPT in stationary dynamics problems, while DynaSEPT is better for non-stationary dynamics. SEPT shows robustness in varying probe length and dimensionality, outperforming baselines in 2D navigation and performing comparably in Acrobot. The use of VAE performance as a probe reward proves more effective in certain environments compared to traditional reward structures. SEPT proposes a general algorithm for single episode transfer among MDPs with different dynamics, achieving success in stationary dynamics. The method, Single Episode Policy Transfer (SEPT), involves training a probe policy and an inference model to discover a latent representation of dynamics using few initial steps in a single test episode. The approach outperforms DynaSEPT in stationary dynamics problems and shows robustness in varying probe length and dimensionality. Additionally, the use of VAE performance as a probe reward proves more effective in certain environments. SEPT proposes a method for single episode transfer in MDPs with different dynamics, showing success in stationary settings. The approach involves training a probe policy and an inference model to discover latent dynamics in a single test episode. SEPT outperforms DynaSEPT in stationary dynamics and is robust to varying probe length and dimensionality. Further research on dynamic probing and control, as outlined in DynaSEPT, is suggested to address challenges in domains with undetectable dynamics or nonstationary dynamics. SEPT proposes a method for single episode transfer in MDPs with different dynamics, involving training a probe policy and an inference model to discover latent dynamics in a test episode. The gradient of the entropy for trajectories induced by the probe policy is derived. The procedure involves running the probe policy for a set time, estimating latent variables, and using them with a control policy for the test episode. DynaSEPT is an alternative algorithm for non-stationary dynamics in MDPs, where a single policy dynamically decides to probe for better inference or maximize the MDP reward based on uncertainty in posterior inference. In posterior inference, the total reward is a combination of probe actions and environmental rewards. The history-dependent term R p (\u03c4) is a delayed reward for probe actions. DynaSEPT is considered only for rare nonstationary dynamics, while SEPT is the primary focus for stationary dynamics. DynaSEPT does not offer clear advantages over SEPT in stationary MDP instances. DynaSEPT and SEPT are compared in stationary MDP instances. DynaSEPT requires \u03b7 to start at 1.0 and the choice of hyperparameter T p. SEPT outperforms DynaSEPT if a trajectory of length T p is sufficient for estimating latent variables. Table 1 reports the number of steps in a test episode, while Table 2 shows the mean cumulative reward over test episodes on Acrobot. Figure 6 displays training curves for various methods across different domains. Baselines, except for Oracle, struggle to converge in 2D navigation due to the challenge of interpolating between optimal policies. MAML faces difficulties adapting without informative rewards. In contrast, HIV shows learning progress similar to previous studies. Figure 7 illustrates two-dimensional encodings for Acrobot instances, highlighting the tradeoff between reconstruction and disentanglement with increasing \u03b2 values. Increasing \u03b2 in Acrobot leads to a tradeoff between reconstruction and disentanglement, with evidence showing lower quality of separation in latent space. Experimental details include the total number of training episodes allowed for 2D navigation, Acrobot, and HIV, as well as the unique training and validation instances. Transfer learning methods require modification for single episode testing. In the setting of single episode testing, modifications are needed for transfer learning methods. A pre-trained BNN model is fine-tuned within the first test episode to generate fictional episodes for policy training. Fine-tuning occurs every 10 steps, with the same number of fictional episodes allowed as in previous studies. The cumulative reward attained by the policy during training in the single real test episode is measured. EPOpt trains on the lowest percentile rollouts from sampled instances for training. EPOpt trains on the lowest percentile rollouts from sampled instances for training and adapts the source distribution using observations from the target instance. The adversarial part of EPOpt was implemented without allowing observation from the test instance. For MAML, adaptation during test is limited to a partial episode, similar to meta-training. The implementation of SEPT-NP involves using a trajectory of the same length as the probe trajectory, with a first-order approximation for computing meta-gradients. The encoder used is a bidirectional LSTM with 300 hidden units and tanh activation, followed by mean-pooling and two linear output layers. The decoder and Q network in the experiments consist of fully-connected neural networks with specific hidden layers and activation functions. The input for SEPT and Oracle includes the concatenation of state and estimated or ground truth z, while other methods only use the state as input. The probe policy network is also a fully-connected neural network. The probe policy network used in the experiments is a fully-connected neural network with specific hyperparameters. The VAE learning rate was set to 1e-4, dataset size limited to 1000 trajectories, and DDQN parameters included a minibatch size of 32 and a target network update rate of 5e-3. Exploration decayed according to a specific formula, and prioritized replay used parameters from a previous study."
}