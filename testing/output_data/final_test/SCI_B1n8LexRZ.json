{
    "title": "B1n8LexRZ",
    "content": "We present a method to train Markov chain Monte Carlo kernels using deep neural networks to improve convergence and mixing speed. Empirical results show significant improvements in sample size and mixing compared to standard methods. The approach is also applied to real-world tasks like latent-variable generative modeling. Python source code will be made available. Protein folding, physics simulations, and machine learning are critical tasks for learning and inference. Markov Chain Monte Carlo methods offer a solution by generating correlated samples that converge to the target distribution. Detailed balance ensures convergence, often achieved through a Metropolis-Hastings accept/reject step. Choosing a suitable proposal distribution is crucial for convergence and mixing speed. Determining when an MCMC chain has converged is more of an art than a science. Hamiltonian Monte Carlo (HMC) introduces momentum variables and computes a new state by integrating Hamiltonian dynamics, allowing for efficient sampling in many domains. However, HMC struggles with mixing across energy levels and multi-modal distributions. The proposed method improves upon Hamiltonian Monte Carlo (HMC) by automatically returning an exact sampler with good convergence and mixing properties for complex distributions. It utilizes deep neural networks to transform the HMC trajectory and includes a tractable Metropolis-Hastings accept/reject step. The proposed method enhances Hamiltonian Monte Carlo (HMC) by utilizing deep neural networks to transform the trajectory and incorporating a tractable Metropolis-Hastings accept/reject step. It introduces a generic training procedure to generate a fast-mixing MCMC kernel for distributions where HMC struggles, showing empirical gains and improved performance in training latent-variable generative models. The text discusses improving Hamiltonian Monte Carlo (HMC) by adaptively modifying proposal distributions to enhance convergence and mixing. Previous work has explored tuning parameters like step size and leapfrog steps, while a simpler approach involves choosing hyperparameters to maximize expected squared jumped distance. Additionally, machine learning models have been applied to MCMC tasks, such as using kernel methods for learning proposal distributions and approximating energy gradients. Restricted and semi-Restricted Boltzmann machines have also been utilized in physics for building approximations. Recent work has explored using adversarial training of a volume-preserving transformation for improving proposal distributions in Hamiltonian Monte Carlo. However, this technique has limitations such as not utilizing gradient information and instability in measuring sample quality. Recent work introduces RNVP, an invertible transformation that modifies variables at each layer based on the remaining variables, making it exactly invertible with an efficiently computable Jacobian. This parameterization motivates an extension of the leapfrog integrator in HMC for constructing a Markov Chain to provide samples from a target distribution. MCMC methods aim to provide samples from a target distribution p by simulating a Markov Chain. To achieve this, certain conditions must be met, such as irreducibility, aperiodicity, and detailed balance. Metropolis-Hastings accept/reject rules can be used to construct a transition kernel that respects detailed balance. Hamiltonian Monte Carlo (HMC) addresses slow mixing in MCMC algorithms by proposing updates that move far in state space while staying on iso-probability contours of the target distribution. It extends the state space with a momentum vector and samples from a distribution defined by an energy function. Hamiltonian Monte Carlo (HMC) uses a momentum vector v independently distributed from x to propose states by integrating Hamiltonian dynamics on x and v. The dynamics move along iso-probability contours of the joint distribution p(x, v). The leapfrog integrator is typically used for simulation, involving operators L and F. In this section, we introduce our method L2HMC ('Learning To Hamiltonian Monte Carlo'). L2HMC learns a parametric leapfrog operator L \u03b8 over an augmented state space using only an energy function U. We discuss the desired properties for L \u03b8, how we parameterize our sampler, and our training procedure. In this section, we introduce our method L2HMC ('Learning To Hamiltonian Monte Carlo'). L2HMC learns a parametric leapfrog operator over an augmented state space using only an energy function. HMC can struggle on simple problems due to slow mixing and inability to traverse low-density zones. Our goal is to create a learned MCMC kernel with fast mixing, fast burn-in, and the ability to mix between modes. The proposal operator in L2HMC must retain key features of the leapfrog operator in HMC: it must be invertible and have a tractable Jacobian determinant. The leapfrog operator achieves this by updating subsets of variables linearly. Generalizing the leapfrog operator is allowed as long as these properties are preserved. The current state is augmented with momentum and a direction variable, following the principles of HMC. The augmented leapfrog integrator L \u03b8 updates momenta v based on a subset \u03b6 1 of the full state, excluding v. It involves functions T v, Q v, and S v, with a translation T v and a rescaling exp(Q v). The update is determined by a fixed random binary mask m t, drawn uniformly, and a direction variable d = 1. The augmented leapfrog integrator L \u03b8 introduces three new functions: T v for translation, exp(Q v) for rescaling the gradient, and exp(2S v) for rescaling the momentum. The determinant of the Jacobian of this transformation is exp(2S v(\u03b6 1)). The update of x involves two subsets of coordinates, with the first update depending on \u03b6 2(xmt, v, t) and the second update depending on \u03b6 3(xmt, v, t). The second update only affects xmt. T x is for translation, exp(Q x) rescales momenta, and exp(S x) rescales positions x. The determinant of the Jacobian of the first transformation is exp(mt\u00b7S x(\u03b6 2)). The augmented leapfrog integrator introduces new functions for translation, rescaling the gradient, and rescaling the momentum. The scaling applied to momentum can enable acceleration in low-density zones, while the scaling term applied to the gradient of energy may allow better conditioning of the energy landscape. The integrator essentially just inverts the updates in equations 4, 5, and 6. Functions Q, S, T are implemented using multi-layer perceptrons with shared weights. The functions Q, S, T are implemented using multi-layer perceptrons with shared weights. The leapfrog operator L \u03b8 corresponds to running M steps of a modified leapfrog. The proposal and acceptance probability are identical to standard HMC, but this parameterization allows for learning non-volume-preserving transformations. Sampling involves alternating application of FL \u03b8 and R in two steps that satisfy detailed balance. The parameterization of the functions Q, S, T in the extended state space allows for non-volume preserving transformations in sampling. Training the parameters \u03b8 aims to minimize lag-one autocorrelation by maximizing expected squared jumped distance. To optimize for both typical and worst-case behavior, a reciprocal term is included in the loss function. The training procedure includes a reciprocal term in the loss function with a scale parameter \u03bb to capture the characteristic length scale. The loss is minimized over both the target and initialization distributions, encouraging mixing and fast burn-in. Each training iteration can be completed with one pass through the network. The training procedure involves a scale parameter \u03bb in the loss function to capture the characteristic length scale. Each training iteration can efficiently batch through the network. This framework can be applied to any learnable operator with a tractable Jacobian determinant, making it suitable for training MCMC proposals. An empirical evaluation of the trained sampler shows promising results on various energy functions, including toy distributions and deep generative models. The L2HMC sampler is evaluated on a diverse set of energy functions, showcasing its ability to adapt to different challenges. Examples include Ill-Conditioned Gaussian and Mixture of Gaussians, demonstrating significant improvements in mixing time compared to HMC. The L2HMC sampler shows improved autocorrelation and effective sample size compared to HMC on various tasks, with over 106\u00d7 improved ESS on the SCG task. It can easily mix between modes in MoG, while standard HMC struggles to traverse low density zones. The L2HMC sampler can effectively mix between modes, unlike standard HMC which gets stuck in low density zones. A-NICE-MC also struggles to traverse between modes due to volume mapping issues. The L2HMC sampler can traverse low-density regions between modes and map different volumes effectively. It is applied to training and sampling from a latent-variable generative model with intractable log-likelihood. BID27 proposed training an approximate posterior to address this issue. BID27 proposed jointly training an approximate posterior q \u03c8 that maximizes a tractable lower-bound on the log-likelihood. Hoffman introduced HMC-DLGM to improve upon VAE pitfalls. L2HMC-DLGM replaces HMC by training an L2HMC sampler for more efficient posterior sampling. In Appendix D, the formal description of the training procedure for the model is presented. The L2HMC sampler is compared to the standard VAE model and HMC-DGLM. Training with L2HMC results in better generative models and enables a more expressive, non-Gaussian posterior. Implementation details include a neural network decoder and encoder with specific architectures. The model was trained for 300 epochs using Adam with a learning rate of 10^-3 on the MNIST dataset. Samples from decoders trained with L2HMC were compared to HMC and ELBO, showing sharper samples. The log-likelihood of the data was compared to HMC using annealed importance sampling. In FIG2, the log-likelihood of data is plotted against the number of gradient computation steps for HMC-DGLM and L2HMC-DGLM. L2HMC-DGLM achieves higher likelihood for both training and held-out data, indicating better posterior samples. L2HMC enables learning of a richer posterior landscape compared to the standard VAE framework. Block Gibbs Sampling is used to in-paint an image's top using the approximate posterior or L2HMC. After training a decoder with L2HMC, 512 parallel chains run for 20,000 Metropolis-Hastings steps. The histogram in FIG3 shows non-Gaussianity in the latent space for the posterior, enabling the decoder to be more expressive. Our improved sampler allows the decoder to utilize a more expressive posterior and the encoder to sample from a non-Gaussian distribution. The loss function targets lag-one autocorrelation and can potentially be extended to target higher autocorrelations and other statistics like sample energy and position. Combining our approach with other research on Hamiltonian Monte Carlo (HMC) could lead to interesting developments. Our work introduces a general method to train MCMC kernels using deep neural networks. This approach is complementary to other methods in the field of Hamiltonian Monte Carlo (HMC) and can be extended to target higher autocorrelations and other statistics. Additionally, incorporating ideas from Multiple Try Metropolis schemes and adaptive literature could further enhance the performance of the sampler. Our method utilizes deep neural networks to train MCMC kernels for fast-mixing sampling in domains like protein folding and physics simulations. The L2HMC-DGLM model is illustrated in Figure 4, with nodes representing functions and auto-correlation defined for effective sample size calculation. Comparison with LAHMC is shown in Table 1, demonstrating the effectiveness of our trained sampler. In this section, we present our training algorithm for L2HMC-DGLM. The model utilizes deep neural networks to train MCMC kernels for fast-mixing sampling. The input, an image from MNIST, is processed through a 2-layer neural network with softplus non-linearities and 512 hidden units. The weights are shared across Q, S, and T, and the distribution over the next state is determined by the Metropolis-Hastings step."
}