{
    "title": "B1xIj3VYvr",
    "content": "A weakly supervised learning framework is proposed in this paper, introducing a novel multiple instance learning task based on unique class count (ucc). The framework does not require annotations on individual instances during training and achieves comparable clustering performance to fully supervised models. The framework's applicability to semantic segmentation tasks has also been tested. In machine learning, there are two main learning tasks: unsupervised learning and supervised learning. Supervised models generally outperform unsupervised models due to the explicit mapping between data and labels. However, supervised learning requires a lot of labeled data, which can be expensive. Weakly supervised learning falls between these two tasks and includes incomplete, inexact, and inaccurate supervision. Multiple instance learning is an example of weakly supervised learning. In weakly supervised learning, Multiple Instance Learning (MIL) involves bags of instances with corresponding bag level labels. The goal is to predict labels of unseen bags without explicit instance labeling. This approach is explored for semantic segmentation of breast cancer metastases in histological lymph node sections. In weakly supervised learning, a clustering framework is proposed to label each pixel in pathology images of lymph node sections as either metastases or normal without exhaustive annotation. This approach is crucial for staging breast cancer and can provide fine-level information from coarse-level measurements. The clustering framework introduces a novel Multiple Instance Learning (MIL) task based on unique class count (ucc) labels for unsupervised clustering of instances in a dataset organized into bags. The MIL task aims to learn the mapping between bags and their ucc labels to predict labels for unseen bags, enabling the ucc classifier to perform unsupervised clustering on individual instances. Our weakly supervised clustering framework utilizes a neural network-based Unique Class Count (UCC) model for learning discriminant features in bags of instances. The UCC model is used for supervised training and then as a feature extractor for unsupervised clustering on individual instances within the bags. An application of this framework is in the semantic segmentation of breast cancer metastases in lymph node sections. The input consists of images labeled as ucc1 (fully normal or fully metastases) or ucc2 (mixture of normal and metastases). The goal is to segment pixels into normal and metastases using a UCC model for supervised training and feature extraction. Unsupervised clustering then generates semantic segmentation masks. This weakly supervised clustering framework outperforms unsupervised models. The study demonstrates that the performance of their UCC classifiers surpasses unsupervised models and matches fully supervised learning models. Their model was tested on the task of segmenting breast cancer metastases in lymph node sections, showing comparable results to the popular U net architecture. The paper's main contributions include defining unique class count as a bag level label in MIL setup, proving the effectiveness of a perfect UCC classifier, and incorporating kernel density estimation into their neural network based UCC classifier. The paper introduces a weakly supervised clustering framework with kernel density estimation (KDE) as a layer in the model architecture. Experimental results show better clustering performance compared to unsupervised models and similar performance to fully supervised learning models. The paper is organized with related work in Section 2, details of the framework in Section 3, experiment results on various datasets in Section 4, and results on semantic segmentation of breast cancer metastases in Section 5. The work is related to Multiple Instance Learning (MIL) and different types of MIL are reviewed. The curr_chunk discusses the use of different types of pooling layers in Multiple Instance Learning (MIL) for various applications such as image annotation, text categorization, spam detection, medical diagnosis, and object tracking. In contrast, the U CC model utilizes a KDE layer to estimate the distribution of extracted features, embedding instance-level features into distribution space. The curr_chunk discusses the embedding of instance-level features into distribution space using KDE in contrast to summarizing them. It also compares clustering accuracies of models with unsupervised baseline and state-of-the-art models. The curr_chunk introduces machine learning objectives and defines a novel weakly supervised clustering framework. It outlines the dataset structure, class labeling assumptions, and the ultimate goal of predicting class labels for instances. The curr_chunk introduces a novel weakly supervised clustering framework where unique class counts are used as bag level labels. The goal is to predict the unique class counts of unseen bags by learning the mapping between bags and their associated labels. The eventual objective is to assign consistent labels to instances based on their underlying unknown classes. The deep learning model in the curr_chunk is designed to achieve intermediate objectives while being trained on a weakly supervised clustering framework. It aims to predict unique class counts of unseen sets, label instances in sets as belonging to different classes, and assign labels to individual instances based on their underlying unknown classes. The deep learning model described in the curr_chunk is focused on achieving unsupervised learning objectives by designing a Unique Class Count (U CC) model with three neural network modules. The model extracts features from instances, predicts unique class counts, and improves feature extraction by ensuring semantic information for reconstruction. The U CC model utilizes a feature extractor module to extract features from instances in a set \u03c3 \u03b6, followed by a kernel density estimation module to accumulate feature distributions. Simultaneously, a decoder module reconstructs input images from the extracted features in an unsupervised manner. The model optimizes two losses concurrently: 'ucc loss' and 'autoencoder loss', with the former being a cross-entropy loss and the latter a mean square error loss. The U CC model utilizes a feature extractor module to extract features from instances in a set \u03c3 \u03b6. The KDE module provides permutation-invariant property using Gaussian kernel, enabling end-to-end training. It also estimates probability distribution of features, allowing \u03b8 drn to utilize distribution shape for better information utilization. The model guarantees achieving intermediate objectives stated in Section 3.1. The UCC model achieves stated intermediate objectives by utilizing a feature extractor and KDE module for end-to-end training. Propositions in Appendix B show how a perfect unique class count classifier can correctly predict class counts and cluster datasets into subsets. The classifier can accurately label instances from different classes, allowing for unsupervised clustering. The perfect UCC classifier enables unsupervised clustering on distributions of sets without knowing truth classes, achieving \"Labels on sets\" and \"Labels on instances\" objectives. Adding an unsupervised clustering branch allows for clustering all instances in dataset X. Given a perfect UCC classifier, perfect clustering for individual instances is theoretically possible. In practice, approximating a perfect UCC classifier is necessary. The trained model is used to extract features and cluster them unsupervisedly using k-means and spectral clustering methods. A strong feature extractor is crucial for successful clustering, achieved through an autoencoder branch. To achieve high clustering performance in unsupervised instance clustering, an autoencoder branch is employed to ensure semantic information in the extracted features. Different unique class count models were trained and optimized jointly over autoencoder and ucc loss for MNIST, CIFAR10, and CIFAR100 datasets. The models were optimized over both autoencoder and ucc loss, with some models trained without the autoencoder branch. Different models were trained on bags with labels ranging from ucc1 to ucc4, showing the impact of the autoencoder branch on clustering performance. Training was done up to ucc4 for faster performance, as perfect clustering only requires a perfect ucc classifier to distinguish between different bags. The aim was to experimentally check if U CC 2+ and U CC 2+ \u03b1=1 models perform as well as U CC and U CC \u03b1=1 models without pure subsets during training. Fully supervised models (FullySupervised) used individual instances with ground truths as labels, while unsupervised autoencoder models (Autoencoder) were trained without labels. All models shared the same architecture for feature extraction and were fine-tuned for optimal performance and training time. The models were trained and tested on MNIST, CIFAR10, and CIFAR100 datasets using different subsets for training, validation, and testing. Results were obtained on hold-out test sets. FullySupervised models used individual instances with ground truths, while Unique class count models used sets of instances sampled from power sets and trained on unique class count labels. The models evaluated in the study were trained in a supervised setup on ucc labels or instance level ground truths. They were used for unsupervised clustering of individual instances, with the objective of predicting ucc labels of unseen subsets accurately. The study evaluated models trained on ucc labels for predicting unseen subset labels accurately. Results showed that as tasks became harder, it was more challenging to approximate a perfect ucc classifier. U CC and U CC \u03b1=1 models generally outperformed U CC 2+ and U CC 2+ \u03b1=1 models, as the absence of pure sets made the prediction task easier. Additionally, U CC 2+ models had higher accuracies than U CC 2+ \u03b1=1 models due to the autoencoder branch. The study compared U CC and U CC \u03b1=1 models with U CC 2+ and U CC 2+ \u03b1=1 models, showing that U CC 2+ had higher accuracies due to the autoencoder branch. Inter-class JS divergence was used to compare feature distributions of pure sets for assigning labels. The study compared U CC and U CC \u03b1=1 models with k-means and spectral clustering accuracy baselines on MNIST, CIFAR10, and CIFAR100 datasets. Inter-class JS divergence values were calculated for pairwise classes, showing a drop in values as tasks became more challenging. Clustering accuracy was used as a comparison metric for the eventual objective of 'Labels on instances'. Trained models were used to extract features for unsupervised clustering using k-means and spectral clustering on test set instances. The study compared clustering accuracies of unique class count models with baseline and state of the art models in the literature using k-means and spectral clustering. The unique class count models outperformed unsupervised models on all datasets and showed comparable performance to fully supervised learning. Our unique class count models outperform unsupervised models on all datasets and show comparable performance to fully supervised learning models in MNIST and CIFAR10 datasets. The performance gap widens in the CIFAR100 dataset. Semi-supervised methods like AAE and CatGAN perform on par with our models in MNIST, while LN and ADGM models reach the performance of fully supervised models by utilizing 'exact' labeled data. On CIFAR10, LN and CatGAN models slightly outperform our unique class count models, but they use a small portion of instances with 'exact' labels. Overall, our UCC and UCC \u03b1=1 models perform similarly and better than UCC 2+ and UCC 2+ \u03b1=1 models due to the absence of pure sets during training. In the task of semantic segmentation of breast cancer metastases, U CC 2+ models outperform U CC 2+ \u03b1=1 models due to the autoencoder branch. The autoencoder branch helps extract better features during the initial phases of training, leading to higher clustering accuracy values. During the initial phases of training, the U CC classification accuracy is low, especially in the MNIST dataset. The autoencoder branch improves clustering accuracy in CIFAR10 and CIFAR100 datasets, but its impact is limited. The complexity of CIFAR10 and CIFAR100 datasets makes the autoencoder less effective in extracting discriminant features. Semantic segmentation of breast cancer metastases in histological lymph node sections is crucial for staging breast cancer. The task involves detecting and locating metastases regions in images using a novel MIL framework. This segmentation can be achieved using weakly supervised clustering without requiring ground truth metastases region masks, which is a tedious annotation process. The study utilized image crops from the CAMELYON dataset to train a unique class count model for semantic segmentation of breast cancer metastases. Unsupervised clustering was conducted on image patches to obtain segmentation masks. A fully supervised U net model was also trained for benchmarking purposes. Our model, U CC segment, and the fully supervised model, U net, outperform the unsupervised baseline method of K-means clustering for semantic segmentation of metastases regions in lymph node sections. Both models show high TPR and TNR scores, low FPR and FNR scores, and lower FPR compared to the baseline method. The U CC segment model performs well in semantic segmentation of metastases regions, with lower FPR values than FNR values. It is trained on ucc labels, which are easier to obtain compared to exhaustive mask annotations, but may sometimes be noisy. The U CC segment model is robust to noise in cancer regions and normal cells, giving good results similar to the U net model. A weakly supervised learning framework with a novel MIL task was proposed, defining ucc as a bag level label. A neural network ucc classifier showed better clustering performance than unsupervised models and comparable to fully supervised models. The U CC segment model can be used for semantic segmentation of breast cancer metastases. The U CC segment model, used for semantic segmentation of breast cancer metastases, approximates the performance of a fully supervised U net model. Future plans include testing the model with other medical image datasets and discovering new morphological patterns in cancer. Kernel density estimation is a statistical method for estimating unknown probability distributions in data. Training the unique class count model end-to-end requires demonstrating its effectiveness. The unique class count model requires the KDE module to be differentiable for end-to-end training. The weight updates for the feature extractor module are shared between the autoencoder and UCC branches. Gradients from both branches are used during back-propagation, with the UCC branch also back-propagating through a custom KDE layer. The decomposability property of kernel density estimation allows for partitioning sets into pure subsets, which is crucial for the proofs of propositions. Unique class count is defined as the number of unique classes in a subset, with pure sets having a unique class count of one. The unique class count of a set does not depend on the number of instances in the set. The unique class count of a set does not depend on the number of instances in the set. Proposition B.2 states that \u03b8 drn is non-linear. Proposition B.3 discusses disjoint subsets with predicted unique class counts. A perfect unique class count classifier can cluster a dataset into subsets with single instances, merging them until no subsets can be merged while maintaining a unique class count of one. The dataset X is decomposed into K subsets \u03c3, with a perfect unique class count classifier. The feature extractor module \u03b8 features convolutional blocks similar to wide residual blocks. Parameters such as number of layers and filters were decided based on performance and training times, avoiding heavy computation burden. During model development, the architecture of \u03b8 drn was optimized by testing different numbers of fully connected layers. Increasing the layers improved classification performance, but the focus was on making \u03b8 feature powerful without increasing complexity. Parameters for the KDE module were tested, with the best results obtained using 11 bins and \u03c3 = 0.1. The number of features at the output of \u03b8 feature module was also optimized based on clustering performance and computation burden. Training included early stopping criteria based on validation loss not dropping for a certain number of iterations. We trained and tested models on MNIST, CIFAR10, and CIFAR100 datasets. MNIST and CIFAR10 have 10 classes each, while CIFAR100 has 20 classes. For MNIST, 10,000 images were randomly split from the training set as a validation set. CIFAR10 has 50,000 and 10,000 images in training and testing sets, with 10,000 images split for validation. In CIFAR100 dataset, there are 50,000 and 10,000 images with equal instances in training and testing sets, respectively. 10,000 images were randomly split from the training set for validation. The training, validation, and testing sets for CIFAR100 had 40,000, 10,000, and 10,000 images, respectively. FullySupervised models were trained on individual instances with ground truths from MNIST, CIFAR10, and CIFAR100 datasets. Unique class count models were trained on sets of instances with ucc labels sampled from the datasets. The power sets of MNIST, CIFAR10, and CIFAR100 datasets are used in experiments with subsets containing 32 instances for MNIST and CIFAR10, and 128 instances for CIFAR100. Models are trained on ucc labels up to ucc4 instead of ucc10 for faster training, as larger ucc labels require more instances in a bag. Perfect clustering of instances only requires a perfect ucc classifier. For perfect clustering, a perfect ucc classifier is needed to discriminate ucc1 and ucc2. Results were obtained on hold-out test sets from MNIST, CIFAR10, and CIFAR100 datasets. Ucc labels were predicted using trained models and accuracies calculated. Confusion matrices of U CC and U CC 2+ models are shown for the datasets. Features of instances in a class are extracted and inter-class JS divergence values are calculated. The inter-class JS divergence values are calculated for each pair of classes based on extracted features. Clustering accuracies for the FullySupervised and U CC models are presented in Table 4. The KDE layer is chosen as the MIL pooling layer in the U CC model for its essential properties. The KDE layer in the U CC model is important for stability and enables end-to-end training. It has decomposability property for theoretical analysis and allows full utilization of distribution shape information. Averaging layer can be an alternative to KDE layer. Additional experiments compared clustering accuracy values with both layers. The U CC segment model has the same architecture as the U CC model in CIFAR10 dataset but with 16 features. The U net model was constructed with the same blocks as the U CC segment model for a fair comparison. 512x512 image crops from the publicly available CAMELYON dataset were used, which provides annotations for metastases regions. UCC labels were assigned based on whether the image was fully metastases/normal or a mixture. Obtaining UCC labels is easier when no annotations are provided. In cases where no annotations are provided, obtaining UCC labels is more cost-effective and efficient than exhaustive metastases region annotations. UCC1 label is assigned to images with less than 20% or more than 80% metastases, while UCC2 label is for images with 30-70% metastases. This labeling scheme mimics potential user-introduced noise. The segmentation dataset was created using this method for training, validation, and testing sets. The segmentation dataset for training, validation, and testing sets consists of cropped images from the CAMELYON dataset. Bags for training the UCC segment model are constructed using 32x32 patches from these images. Each bag contains 32 instances, with details provided in Table 6. The dataset is available in the \"./data/camelyon/\" folder. Citation of the paper is required for benchmarking. The segmentation dataset includes training, validation, and testing sets with different numbers of images and label distributions. Confusion matrix for UCC predictions and loss curves for U net model are shown. Best model weights were saved at iteration 58000, with training stopping early at iteration 60000 due to overfitting. Pixel level evaluation metrics were defined for performance comparison of weakly supervised UCC segment model. The text discusses metrics for comparing the performance of weakly supervised UCC segment model, fully supervised U net model, and unsupervised baseline K-means model. Key metrics include TPR, TNR, FNR, and PA."
}