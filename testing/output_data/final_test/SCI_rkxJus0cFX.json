{
    "title": "rkxJus0cFX",
    "content": "Data parallelism is a popular method for scaling Deep Neural Network (DNN) training across multiple nodes. Compressing communication traffic to alleviate synchronization bottlenecks in large-scale distributed training has gained significant attention. Residual Gradient Compression (RGC) is a successful approach that significantly compresses transmitting message size while preserving accuracy. A new RGC method called RedSync improves training time in real-world multi-GPU systems by optimizing communication bandwidth with limited overhead. Performance of RedSync is tested on different GPU platforms for tasks like image classification and language modeling. RedSync improves performance for training large-scale deep neural networks on multiple computing nodes by optimizing communication bandwidth. The increasing size of DNN models and the bottleneck of communication bandwidth pose challenges for data parallelism. The development of DNN training accelerators has shifted the bottleneck of training towards communication across models. The bottleneck of training has shifted towards communication across models due to the evolution of network bandwidth not keeping pace with computing hardware. Recent studies focus on reducing communication costs by quantizing gradients or sparsifying communication gradients. Residual Gradient Compression (RGC) is a promising method for achieving good compression ratio. Residual Gradient Compression (RGC) is a promising method for achieving good compression ratio in training without loss of accuracy. BID14 is the most effective pruning method, transmitting only a small subset of gradients while maintaining the rest locally as residuals. Previous implementations like BID0 and BID4 had limitations in choosing appropriate thresholds for communication, leading to accuracy loss in different DNNs. Newer RGC variants like BID14, BID4, and BID9 have improved compression ratios on local gradients with minimal model loss. The challenges of applying Residual Gradient Compression (RGC) to distributed GPU systems include the lack of efficient compression algorithms and difficulties in synchronizing sparse data structures with existing communication libraries. Top-k algorithms for selecting elements are costly, outweighing the benefits of bandwidth reduction. RedSync is a highly-efficient RGC implementation for multi-GPU systems. It combines pruning and quantization techniques to compress transmitting gradients and uses a parallel-friendly top-0.1% selection method for pruning operations. Allgather operation using MPI is applied for sparse synchronization, with a cost model to analyze communication and calculation overhead. RedSync ensures minimal performance bottlenecks. RedSync ensures minimal performance bottlenecks and provides significant performance improvements for communication-intensive networks like VGG, AlexNet, and LSTMs. It evaluates the performance of RGC method on 128 GPUs and integrates with the latest algorithm improvements to train DNNs with almost no accuracy loss. RedSync utilizes synchronous SGD method with node k computing gradients using local data. Residuals are accumulated and compressed into sparse data structures, with important elements selected for synchronization among all nodes using allreduce operations. The efficiency of communication-set selection method is crucial for the overall performance of the RedSync algorithm. Recent work has shown that determining a predefined threshold is challenging, so our contribution focuses on implementing select, Allreduce, and decompress to improve the workflow efficiency in practice. The efficiency of communication-set selection is crucial for RedSync algorithm performance. Recent work suggests selecting top 0.1% elements from residuals of each layer, but this is challenging on GPU. A more efficient method is proposed using trimmed top-k selection and threshold binary search selection algorithms for GPUs. The efficiency of communication-set selection is crucial for RedSync algorithm performance. Recent work suggests selecting top 0.1% elements from residuals of each layer, but this is challenging on GPU. A more efficient method is proposed using trimmed top-k selection and threshold binary search selection algorithms for GPUs. This involves calculating mean and maximum of residuals' absolute values, setting a threshold based on these values, and trimming elements below the threshold before performing a top-k selection operation using radixSelect. The efficiency of communication-set selection is crucial for RedSync algorithm performance. A method is proposed to select approximate top-0.1% elements as communication-set by setting a threshold between the kth to 2kth largest element and selecting elements larger than the threshold. This avoids using radixSelect operation on GPU and ensures at least 0.1% largest elements are included in the communication-set. The RedSync algorithm proposes a method to select the top 0.1% elements for the communication-set by setting a threshold between the kth to 2kth largest element. A binary search algorithm is used to find this threshold efficiently. The process ensures that at least 0.1% of the largest elements are included in the communication-set. The algorithm efficiently selects the top 0.1% elements for the communication-set by reusing threshold elements and reducing count nonzero operations. This approach significantly reduces selection time for large sizes compared to radixSelect. In practice, compression strategies are dynamically chosen based on the size of the parameters. Trimmed top-k selection is suitable for middle size layers like convolutional layers, ensuring a compression ratio of 0.1%. Threshold binary search selection is more suitable for large layers like hidden layers in LSTMs, optimizing compression costs. Compressed residuals include k indices and values. Compressed residuals include k indices and values. To reduce communication bandwidth, quantizing values by setting elements of the same sign to their mean can be almost eliminated. The select method is modified to ensure all elements in the communication-set have the same sign by choosing the largest and smallest k elements alternately. Threshold binary search selection is not compatible with quantization. In distributed DNN systems, sparse allreduce is challenging due to different non-zero indices in compressed residuals. Training VGG16 on Cifar10 with 16 GPUs at 0.1% compression ratio per node results in 1.55% average compression ratio. Allgather operation is used for data gathering in this scenario. The allgather operation is used to implement sparse allreduce by gathering data contributed by each node at all nodes. The message representing compressed residuals includes indices and values of elements in the communication-set. Instead of separate allgather operations for indices and values, they are packaged into a single message to reduce latency. Each node collects N compressed residuals from all other nodes, adding them to corresponding weights in the local model after scaling with the learning rate. This operation effectively adds a sparse array to a dense array. RedSync implements algorithm improvement techniques for sparse synchronization, including momentum correction, momentum factor masking, warmup training modification, and local gradient clipping. Performance gain is analyzed using a communication cost model based on latency and bandwidth estimation. The network interface is assumed to be single ported for message transfer. The communication cost model for sparse synchronization in RedSync includes compression ratio, message size, and computational costs for reduction operations. Different compression ratios for nodes are considered, with specific algorithms for communication. The cost of quantized sparse and dense synchronization is calculated. The performance model shows that the compression rate varies for different aspects of the model. The communication cost model for sparse synchronization in RedSync includes compression ratio, message size, and computational costs for reduction operations. The bandwidth term for sparse synchronization is proportional to the number of nodes. The overhead of reduction may become a bottleneck at larger scales. Testing was done on two multi-GPU systems to evaluate accuracy and performance. Server Muradin has eight GPUs in the same node with an Intel Xeon CPU E5-2640 v4 and 8 TITAN Vs connected through PCI-E 3.0. Piz Daint is a GPU supercomputer with nodes containing two Intel Xeon E5-2690v3 CPUs and one NVIDIA Tesla P100 GPU. Pytorch v4.0 and Horovod MPI wrapper were used for DNN training and collective communication operations. Performance was tested on mainstream deep learning applications like ResNet-44, VGG16, AlexNet, and ResNet-50 using Nesterov's momentum SGD optimizer. For Language Modeling tasks, two datasets were evaluated: the Penn Treebank corpus with 923,000 training words and the WikiText dataset with over 100 million tokens. A 2-layer LSTM model with 1500 hidden units per layer was used for evaluation. RedSycn convergence was examined on these datasets. RedSync convergence was examined on various datasets using different models such as ResNet44, VGG16, AlexNet, and LSTM. Results showed improved validation error and scalability on multiple GPUs. Increasing batch size did not affect accuracy, and performance on Piz Daint was illustrated in figures. RedSync scalability was tested on Piz Daint with four test cases and on Muradin with six test cases. Comparison was made between RedSync, Quantized-RedSync, and a baseline data parallel implementation by horovod. Data compression methods were used for CNNs and LSTM layers. Cost analysis of scaling RedSync to 128 GPUs on Piz Daint was shown in Fig. 6. The importance of parallel-friendly selection methods for compression was highlighted, with pure RGC implementation showing slower performance than proposed methods. RedSync is effective for accelerating data parallel training on DNNs with high communication to computation ratio. While it may not perform as well as the baseline version on a single GPU due to compression and decompression overhead, significant speedup can be achieved with more than 2 GPUs. However, ResNet50 shows no performance gain with RedSync due to high computation to communication ratio. Training with RedSync on ResNet50 wastes time on decompression, negating the benefits of communication bandwidth reduction. The scalability curve of RedSync on Piz Daint exhibits a concave shape. The scalability curve of RedSync on Piz Daint shows a concave shape, with better speedup on 32 GPUs compared to 128 GPUs for AlexNet due to communication bandwidth and decompression overhead. Quantized-RedSync outperforms RedSync for CNNs, but performs worse for LSTM training on a small scale. CNN uses trimmed top-k for communication-set selection, with quantization reducing communication costs and improving overall performance. RedSync accelerates DNN training by utilizing Residual Gradient Compression. It outperforms Quantized-RedSync on small-scale LSTM training due to faster sampled selection. RedSync's performance improves on more than 16 GPUs by reducing communication overhead. The implementation addresses compression overhead on GPUs and lack of support for collective communication on sparse data structures. RedSync accelerates DNN training on two GPU platforms, showing significant speedup for large-scale DNN training. Sparse allgather works through recursive doubling method with a compression ratio of D. Threshold binary search is used for communication-set selection, with nodes exchanging compressed residuals. The Rabenseifners algorithm is used for allreduce operation on messages, performing reduce-scatter followed by allgather. Reduce-scatter scatters the result among all nodes using a recursive halving algorithm. The time for message transfer in this algorithm is calculated based on the amount of data exchanged by each node in each step. The Rabenseifners algorithm utilizes a recursive halving algorithm for reduce-scatter, followed by allgather. Each node exchanges data with nodes at varying distances in multiple steps, reducing data communicated at each step. The total time is the sum of reduce-scatter, allgather, and reduction operation times. Improving data parallel efficiency involves overlapping communication. Improving data parallel efficiency involves overlapping communication by pipelining communication and gradient calculation. Gradient clipping is used to avoid gradient explosion by rescaling gradients when their norms exceed a threshold. The RGC algorithm adopts local clipping before communication, unlike traditional methods that do clipping after communication. This approach requires waiting for back-propagation completion to clip gradients and perform compression for communication. The RGC algorithm integrates local gradient clipping before communication to avoid gradient explosion and improve data parallel efficiency. This approach synchronizes computing and communication, eliminating the possibility of communication hiding. Gradients are clipped after backpropagation of all time steps for RNNs, allowing for overlap between communication and compression calculations. RedSync integrates momentum masking and momentum correction schemes from BID9 for momentum SGD and Nesterov momentum SGD optimizers. A warm-up training with decreasing compression ratios of residuals is used to accelerate convergence in the first few iterations. However, high-compression-ratio warm-up training may be inefficient for large-scale models. Instead of this approach, RedSync uses the original SGD method to avoid excessive bandwidth usage during synchronization. Instead of using high-compression-ratio warm-up training, RedSync utilizes the original SGD optimizer synchronized by allreduce in the first few epochs if needed."
}