{
    "title": "rygo9iR9F7",
    "content": "This work proposes a progressive weight pruning approach based on ADMM to compress DNN models for edge computing devices. It achieves up to 34\u00d7 pruning rate for ImageNet dataset and 167\u00d7 pruning rate for MNIST dataset, surpassing previous literature work. The proposed method achieves a 167\u00d7 pruning rate for the MNIST dataset, surpassing previous literature work. It also shows faster convergence and higher compression rates under the same number of epochs. The released codes and pruned DNN models can be found at bit.ly/2zxdlss. Extensive research efforts have focused on DNN model compression, with weight pruning being a key technique. BID7 introduced DNN weight pruning by removing small magnitude weights and retraining iteratively. More advanced methods like incorporating weight pruning with growing, L1 regularization, and genetic algorithms have been proposed. Other approaches include energy-aware pruning, channel pruning, and structured sparsity learning. Weight quantization is another method to reduce redundancy in DNN models. Weight pruning is a major DNN model compression technique that focuses on reducing the number of weights while maintaining model expressiveness. Prior work has used heuristic approaches to achieve sparsity without sacrificing accuracy. BID32 proposed an optimization-based approach to push for maximum compression rates through weight pruning. In response to BID32's optimization-based approach using ADMM for weight pruning, this paper introduces a progressive weight pruning method combining ADMM and masked retraining. The goal is to achieve extremely high compression rates with minimal accuracy loss, targeting 150\u00d7 for LeNet-5 or 30\u00d7 for AlexNet. The paper introduces a progressive weight pruning method to achieve high compression rates without significant accuracy loss. By using multiple partial prunings with progressive rates, the approach reaches up to 34\u00d7 pruning rate for the ImageNet dataset. This method outperforms other state-of-the-art weight pruning techniques in terms of compression rates while maintaining accuracy. The proposed progressive weight pruning method achieves high compression rates with minimal accuracy loss. It outperforms other pruning techniques with up to 34\u00d7 pruning rate for ImageNet and 167\u00d7 pruning rate for MNIST datasets. The method shows better convergence and compression rates compared to prior methods, and codes and pruned DNN models are available for download. The proposed progressive weight pruning framework utilizes masked ADMM regularization to prune a pre-trained network model by forcing weights smaller than thresholds to zero. This process is repeated for several rounds to achieve high compression rates with minimal accuracy loss. The proposed progressive weight pruning framework uses ADMM regularization to prune pre-trained network weights, preventing them from recovering to nonzero values and accelerating convergence. The algorithm outputs a pruned network model satisfying sparsity constraints for an N-layer DNN. The weight pruning problem is formulated as minimizing weights and biases without nonconvex constraints. The ADMM technique is used to solve the weight pruning problem by decomposing it into two subproblems and iteratively solving them. The first subproblem involves DNN training with an L2 regularization term, while the second subproblem seeks the globally optimal solution. Global optimality cannot be proven for the solutions to these subproblems. The ADMM technique is utilized for weight pruning in DNNs by decomposing it into two subproblems. The globally optimal solution involves the Euclidean projection onto a nonconvex set, which is analytically expressed for the set S i. Progressive pruning is introduced by defining set P i for zero-weight elements. The dual variable U i is updated to conclude one iteration of the ADMM algorithm. The ADMM-based algorithm for DNN weight pruning is a smart regularization technique that dynamically updates the regularization target in each iteration. It outperforms heuristic methods and other techniques but has limitations for high compression rates due to non-convexity. Pruned DNN models may not be exactly sparse, leading to accuracy degradation at high compression rates. The ADMM-based algorithm for DNN weight pruning outperforms heuristic methods but has limitations for high compression rates. A masked retraining step is proposed to terminate ADMM iterations early, keeping the largest weights and setting others to zero, leading to good compression rates with reasonable training time. The ADMM-based algorithm for DNN weight pruning achieves high compression rates but faces limitations at extremely high pruning rates, such as 150\u00d7 for LeNet-5 or 30\u00d7 for AlexNet. The algorithm can take a long time to choose which weights to prune at very high pruning rates. The progressive weight pruning method overcomes difficulties faced by the ADMM-based algorithm at high pruning rates. By starting from partial pruning models with moderate rates, such as 15\u00d7, 18\u00d7, and 21\u00d7, the method achieves a 30\u00d7 weight pruning rate on AlexNet without accuracy loss. This approach allows for faster convergence and efficient selection of the most accurate model for further pruning. The progressive weight pruning method achieves a 30\u00d7 weight pruning rate on AlexNet without accuracy loss by starting from partial pruning models with moderate rates. The algorithm uses \"masked\" training to prevent already pruned weights from recovering, focusing on pruning nonzero weights. Pruning becomes harder at higher rates, with methods like Projected Gradient descents and Iterative Pruning BID7 showing significant accuracy loss. The proposed ADMM-based progressive weight pruning framework mitigates performance degradation caused by high pruning rates. Evaluation is done on ImageNet ILSVRC-2012 and MNIST datasets using various DNN models like AlexNet, VGG-16, and ResNet-50. Accuracies of uncompressed DNN models are provided for reference. The study implements ADMM-based weight pruning with key parameters set for optimal performance. Experiments are conducted on Nvidia GPUs, adjusting the penalty parameter for different pruning rates. Initial learning rates are specified for the masked ADMM-based algorithm and masked retraining. Codes and pruned DNN models are available for ImageNet and MNIST datasets. The study compares weight pruning results on the AlexNet model, showing our method achieves a 31\u00d7 weight reduction rate without loss of accuracy. Progressive weight pruning outperforms direct ADMM pruning, with higher accuracy for the CaffeNet model. Results also indicate that direct ADMM pruning suffers from accuracy drop at high compression rates. Our progressive weight pruning technique achieves higher accuracy compared to the original DNN model at compression rates of 18\u00d7 and 21\u00d7. Results on VGG-16, ResNet-50, and LeNet-5 models show superior performance in achieving the highest sparsities. Notable achievements include 30\u00d7 weight pruning on VGG-16 with comparable accuracy to prior works, and 34\u00d7 weight pruning with minor accuracy loss. ResNet-50 model testing confirms minor accuracy loss at a 17.43\u00d7 weight pruning rate. Limited prior work on ResNet weight pruning for ImageNet dataset is attributed to the difficulty in weight pruning due to convolutional layers and slow training speed. Our method achieves high training speed for weight pruning testing on large-scale DNN models. For LeNet-5 model compression, we achieve 167\u00d7 weight reduction with minimal accuracy loss, surpassing prior work. We also demonstrate over 10\u00d7 weight pruning rate with minor accuracy loss on facial recognition DNN models BID16 and BID12. Our framework consistently outperforms prior work in weight pruning for DNN models, achieving over 10\u00d7 weight reduction with minimal accuracy loss. The results show promise for energy-efficient implementation of DNNs on various hardware platforms. Additionally, weight pruning and quantization can be unified under the ADMM framework, as demonstrated with the LeNet-5 model achieving 167\u00d7 weight reduction. The table demonstrates achieving 167\u00d7 weight reduction using 2-bit for fully-connected layer weight quantization and 3-bit for convolutional layer weight quantization, with an overall accuracy of 99.0%. The compression rate is 1,910\u00d7 compared to the original DNN model with floating point representation, and 623\u00d7 when accounting for indices. BID7's work shows effective DNN weight pruning with 9\u00d7 pruning in AlexNet and 13\u00d7 pruning in VGG-16. Extensions of the initial work on DNN weight pruning methods include BID6 and BID3, which achieve high pruning rates in models like AlexNet. Other techniques like Optimal Brain Surgeon and L1 regularization also show promising results in reducing weights in neural networks. Additionally, the direct ADMM weight pruning algorithm offers a systematic framework for weight pruning. The algorithm BID32 is a systematic weight pruning framework that achieves state-of-the-art performance in multiple DNN models. Recent work has focused on incorporating regularity and structure in weight pruning methods to reduce overheads in GPU, embedded systems, and hardware implementations. While these methods can improve acceleration in these platforms, they typically cannot achieve higher pruning ratios than unrestricted pruning. Future work will explore the application of progressive weight pruning to regular/structured pruning. The proposed progressive weight pruning approach based on ADMM achieves extremely high pruning rates by using partial prunings, resolving accuracy degradation and convergence time issues. It achieves up to 34\u00d7 pruning rate for ImageNet and 167\u00d7 pruning rate for MNIST, surpassing existing literature. The method also shows better convergence and higher compression rates under the same number of epochs."
}