{
    "title": "H1g4k309F7",
    "content": "In this paper, model ensembling in multiclass or multilabel learning using Wasserstein barycenters is proposed. Wasserstein distance allows incorporating semantic side information like word embeddings. W. barycenters help find consensus between models, balancing confidence and semantics. Applications include attribute-based classification, multilabel learning, and image captioning generation, showing W. ensembling as a viable alternative to basic mean ensembling. Model ensembling combines models for a stronger, more accurate model, common in machine learning for improved accuracies in tasks like multi-class or multi-label classification. In deep learning, models' predictions can be represented as a distribution in a label space defined by word embeddings. Current approaches to model ensembling lack the ability to incorporate side information like class relationships represented by a graph or embedding space. The text proposes achieving consensus between models' predictions by combining them via Wasserstein barycenters, balancing model confidence and semantic information encoded in a cost matrix. This approach utilizes the semantic information in the label space to find a consensus among models. The paper discusses the effectiveness of Wasserstein barycenters in model ensembling and finding a semantic consensus across different label sets in machine learning applications. The paper discusses the advantages of Wasserstein barycenter ensembling in machine learning, highlighting its benefits in terms of semantic smoothness and diversity. It also explores applications of Wasserstein ensembling in attribute-based classification, multi-label learning, and image captioning. In a multi-label setting, model ensembling with unnormalized positive scores from logistic units has a long history in deep learning and machine learning. Two prominent methods for ensembling are majority vote using arithmetic mean or consensus based using geometric mean. Weighted arithmetic and geometric means can be interpreted as Frechet means. In model ensembling for multi-label settings, using Frechet means with Wasserstein distance is proposed for incorporating semantics of the target space. Wasserstein-2 metrics leverage the underlying geometry of the label space through a cost matrix, making them suitable for comparing positive measures. For model ensembling, Wasserstein barycenters are used to find a distribution close to all base distributions in the Wasserstein sense. The cost is defined by the distance in the word embedding space, with extensions to deal with unnormalized measures known as unbalanced OT. This is motivated by multi-class and multi-label ensembling applications. The optimal transport metric is defined using a coupling matrix and cost function. Wasserstein-2 distance is used when the cost function is x - y^2. Unbalanced optimal transport metrics handle unnormalized measures with different total masses using extended KL divergence. Generalized Wasserstein distance is defined for unnormalized measures. The paper discusses Wasserstein distance between unnormalized measures and the concept of balanced and unbalanced Wasserstein barycenters for normalized and unnormalized predictions. The goal is to find a consensus prediction by minimizing the Wasserstein distance. The unbalanced barycenter problem involves solving for coupling matrices. The computation of the Wasserstein distance is improved by entropic regularization, making it strongly convex. The solution can be found using scaling algorithms like the Sinkhorn algorithm. Entropic regularized OT distances converge to the original OT distance as the hyperparameter \u03b5 approaches 0. For higher \u03b5 values, the Sinkhorn divergence allows for more diffuse transport between points. Balanced and unbalanced Wasserstein barycenters can be defined naturally. The entropic regularized OT distance allows for simple iterative algorithms to compute balanced and unbalanced Wasserstein barycenters. Algorithms 1 and 2 provide the geometric mean of K with Gaussian kernel bandwidth \u03b5. The barycenter can be expressed as a matrix product of individual models' probabilities and convergence values v*. The matrix product K multiplies individual models' probabilities \u00b5 and Lagrange multipliers v* to transfer probability mass between semantically related classes. As K approaches identity (I) in Algorithm 1, the BID2 projection converges to the geometric mean \u03bcg. Wasserstein Ensembling benefits various machine learning tasks by utilizing different kernel matrices K. In multi-class and multi-label ensemble learning, W. barycenters balance semantics and confidence in finding consensus between source and target domains. An application example in attribute-based classification involves predicting attributes and labels using a semantic consensus to increase prediction entropy. This approach enhances diversity and smoothness in sequence generation beyond basic language models. The W. barycenter aims to increase prediction entropy by finding a semantic consensus that is diverse and smooth on semantically coherent concepts without compromising accuracy. It provides semantic accuracy, diversity based on Wasserstein distance, and smoothness in the embedding space. The W. barycenter is smoother in the embedding space than individual models, preserving accuracy while increasing entropy. The diversity of the barycenter depends on Wasserstein pairwise distance between models, with less diversity if models have similar semantics. Propositions in Appendix F provide similar results for geometric and arithmetic mean, with guarantees given in terms of KL and Wasserstein distance. The W. barycenter, computed between softmax outputs of image captioners, shows higher entropy and smoothness along semantics compared to individual models. It outputs clusters according to semantics and is more diverse, illustrated by top 15 words in the barycenter, arithmetic, and geometric means. The W. barycenter outputs clusters based on semantics and maps high probability words back to individual models using couplings. It has higher entropy, spreading probability over synonyms related to the top word \"car\" and downweights irrelevant objects. Controllable entropy is achieved through regularization. The W. barycenter outputs clusters based on semantics and maps high probability words back to individual models using couplings. Controllable entropy is achieved through regularization, where the entropy of the entropic regularized W. Barycenter is controllable via the entropic regularization parameter \u03b5. Increasing \u03b5 increases the distance of the kernel K from identity and the entropy of the optimal couplings \u03b3. The entropy of the W. barycenter increases as the distance of the kernel K to identity increases, maintaining smoothness within semantically coherent clusters. The entropic regularization in Wasserstein Barycenter outputs clusters based on semantics and maps high probability words back to individual models using couplings. The regularization parameter \u03b5 controls the distance of the kernel K from identity, with decreasing \u03b5 leading to K approaching the identity matrix. This regularization fosters applications in machine learning, including Wasserstein loss in multilabel settings and Wasserstein discriminant analysis for representation learning. The output from Algorithm 1 decreases in entropy as \u03b5 decreases, converging towards the geometric mean for very small entropic regularization. Recently, discriminant analysis in BID12 was followed by a new approach using generative adversarial networks with Wasserstein distance (Arjovsky et al., 2017; BID15 BID31). Applications in NLP included pioneering work on Word Mover Distance (WMD) on word embeddings of BID23. Algorithmic developments have led to the application of W. barycenters in various fields such as graphics, clustering, dictionary learning, topic modeling, bayesian averaging, and learning word and sentence embeddings. These applications focus on learning balanced barycenters in the embedding space, while in ensembling, embeddings are assumed and the barycenter is computed at the predictions level. Incorporating side information like knowledge is also considered. Incorporating side information like knowledge graphs or word embeddings in classification is not new and has been utilized in various ways. In this section, the evaluation of W. barycenter ensembling is done in attribute-based classification, multi-label prediction, and natural language generation in image captioning. Object classification based on attribute predictions is studied using Animals with Attributes dataset, with 85 attributes and 50 classes. Two attribute classifiers are used to predict the absence/presence of each attribute independently, based on resnet18 and resnet34 input features. In attribute-based classification, the W. barycenter ensembling method is used to predict 50 animal categories based on 85 attributes. The data is split into train, validation, and test sets, with hyperparameters selected on the validation split. The ensembling approach achieves higher accuracy by utilizing a cross-domain similarity matrix compared to a simple linear transformation. The W. barycenter outperforms arithmetic and geometric mean in attribute-based classification using a cross-domain similarity matrix. It shows potential in multi-label prediction tasks using MS-COCO dataset with 80 object categories. The study built 8 models using 'resnet18' and 'resnet50' architectures pretrained on ImageNet or Places365. The last fully-connected layer was replaced with a linear layer for 80 output categories. Different fine-tuning variations were applied, with training using ADAM and stopping at 40 epochs. Only the center crop of 224 * 224 of an input image was used. The study built 8 models using 'resnet18' and 'resnet50' architectures pretrained on ImageNet or Places365. The models were evaluated using mean Average Precision (mAP) and showed reasonable performances overall. Arithmetic and geometric means improved mAP over individual models. The transport of probability mass for unbalanced W. barycenter was defined by a matrix K in Algorithm 2, with different candidates investigated. The study explored different methods to improve model performance, including using pairwise visual word2vec embeddings distance and co-occurrence counts. A diagonal matrix K was created for each test sample based on top-N scoring categories from each model, enhancing the contributions of likely categories while suppressing others. This approach yielded the best results when considering the top-2 scoring categories per model. In improving model performance, a diagonal matrix K is utilized with top-2 scoring categories per model, outperforming arithmetic and geometric means. W. barycenters parameters are tuned on a validation set, showing a solid alternative for ensembling 5 image captioner models. The objective is to find a semantic consensus by ensembling models trained on COCO dataset. The training of GAN-trained models was done on the COCO dataset using data splits for training, validation, and test sets. The vocabulary size was reduced to 10096 after pruning. The matrix K in Algorithm 1 was constructed using word similarities from GloVe embeddings and synonym relationships. Model predictions were based on the softmax output of the captioner's LSTM. The barycenter was computed and fed into a beam search process. The text discusses the use of W. barycenter in generating diverse captions through randomized Beam search. Different regularization parameters were tested to show the impact on caption diversity while maintaining semantic relevance. The barycenter ensembling was compared using arithmetic and geometric means. The results indicate that increasing the regularization parameter leads to more diverse captions with higher entropy. The comparison of ensembling methods on the validation set using topK and randomized beam search is shown in FIG0. Different regularization parameters were tested to impact caption diversity while maintaining semantic relevance. Increasing the regularization parameter leads to more diverse captions with higher entropy. The results show that while n-grams matching metrics deteriorate, WMD similarity remains stable, indicating that generated sentences are semantically close to the ground truth. Randomized beam search increases entropy, leading to paraphrasing effects up to a certain point before a significant deterioration occurs. The diffusion effect is smaller for synonyms-based methods due to limited neighborhood size. The performance of GloVe-based W. barycenter on COCO test split using topK beam search is compared to Geometric and Arithmetic ensembling. W. barycenter generated sentences are semantically close to the ground truth but more diverse, showing robustness to semantic perturbations. The barycenter is able to recover from random shuffling of \u00b5 values within semantically coherent clusters, unlike arithmetic and geometric means. The study compared ensembling methods using shuffled predictions and found that W. Barycenter ensembling outperformed simple averaging methods. Human evaluation on challenging images showed that W. Barycenter, using a similarity matrix defined by visual word2vec, produced better captions. Randomized beam search was used for all models, and MTurkers rated captions on a scale of 1-5 based on correctness and detailedness. The study demonstrated the effectiveness of W. barycenters in model ensembling in machine learning, particularly in attribute-based and multi-label classification. They also showed improvements in natural language generation by incorporating knowledge of synonyms or word embeddings. Table 8 displays sample outputs of barycenter for different similarity matrices based on GloVe, highlighting the impact of entropic regularization \u03b5. The effect of entropic regularization \u03b5 on the distribution of words in W. barycenter using GloVe embedding matrix is shown in Table 8. As K approaches the identity matrix, the entropy decreases, resulting in outputs similar to the geometric mean. Conversely, a large entropic regularization causes K to become uninformative, leading to a uniform distribution across all words. This can be visualized in Figure 5 with histograms showing uniform distributions, indicating failure cases where gibberish captions are generated. The text discusses the influence of entropic regularization on the word distribution in the barycenter output using GloVe embedding matrix. It shows how different values of K affect the output, with low entropy leading to outputs similar to the geometric mean and high entropy resulting in uniform distributions. Failure cases are illustrated with histograms showing gibberish captions. Additionally, captioning examples demonstrate qualitative differences between ensembling techniques. The text discusses the impact of entropic regularization on word distribution in the barycenter output using GloVe embedding matrix. Different values of K affect the output, with low entropy resembling the geometric mean and high entropy leading to uniform distributions. Histograms illustrate failure cases with gibberish captions, while captioning examples show qualitative differences between ensembling techniques. The W. Barycenter produces high entropy distributions, spreading probability mass over synonyms of the word \"car\" based on a similarity matrix. The barycenter output, based on a similarity matrix using synonyms, shows the contribution of words for each input model. Examples include a television on the curb, a car parked at a station, people sitting on a sidewalk, and a sheep in a car. The text discusses evaluating models using precision, recall, and F1-measure metrics with a threshold of 0.5 for predicting positive labels. Macro precision is the average of per-class precisions, while micro precision is computed by the ratio of true positives across all samples over the number of positive classes in a dataset. The text discusses evaluating models using precision, recall, and F1-measure metrics with a threshold of 0.5 for predicting positive labels. Mean Average Precision (mAP) is used to assess the performance of multi-label predictors. Results for 8 models and ensembling techniques are reported in the paper. Weighted ensembles of models are optimized on a validation set before being used on a test set. The text discusses evaluating models using precision, recall, and F1-measure metrics with a threshold of 0.5 for predicting positive labels. Mean Average Precision (mAP) is used to assess the performance of multi-label predictors. Weighted ensembles of models are optimized on a validation set before being used on a test set. Adaboost BID13 is a well-known approach where weights are dynamically defined based on the accuracy of base models. In this study, a simpler approach is followed by defining the performance of each model as mAP on the validation set. The weights are then applied to the models' scores for ensemblings using arithmetic, geometric mean, and W.Barycenter techniques. The weights are based on the final metric evaluation, mAP in this case, but can be adapted for other tasks using different metrics. Tab. 11 shows the mAP for each ensembling technique over the MS-COCO test set. Performance-based weighting helps both arithmetic and W.Barycenter ensembling, with the latter retaining its performance advantage. Table 11 displays the mAP for multi-label models ensembling on the MS-COCO test set. Weighting models by their performance benefits arithmetic and W.Barycenter ensembling, reinforcing the contributions of better performing models. Geometric means ensembling is not significantly impacted by non-uniform \u03bb as it relies more on consensus than confidence. The Sinkhorn divergence interpolates between MMD distance and Wasserstein Distance. Algorithm 1 provides a solution that is an interpolation between unregularized Wasserstein Barycenter and MMD Barycenter. As \u03bb approaches infinity, the unbalanced cost converges to the Hellinger distance. Improving computational complexity involves using low rank approximation of the kernel matrix K and parallelization on m machines. Dependency on Maxiter: Maxiter = 5 is enough for convergence, with computational complexity dependent on m and N. The main complexity comes from matrix vector multiply K u of O(N^2). Using low rank approximation of K reduces complexity to O(Nk). The algorithm is fully parallelizable with complexity O(Nk) by using m. The algorithm for Wasserstein Barycenters is fully parallelizable with a computational complexity of O(N k) using m machines. The GPU implementation can be further accelerated by computing Sinkhorn divergences in batches. Results show that using Maxiter = 5 on a GPU-V100 can achieve below 4ms/image for Wasserstein ensembling, with potential for improvement through parallelization, batching, and low rank approximation. In TAB14, timings for ensembling 8 models were reported over two GPU architectures, NVIDIA Tesla K80 and V100. W.Barycenters showed significantly higher wall clock times compared to Arithmetic and Geometric means, with varying Maxiter values impacting the results. The GPU was leveraged for W.Barycenters but not for Arithmetic and Geometric means in the computations. The geometric mean is the Frechet mean of KL divergence, with properties such as convexity and higher entropy in convex combinations. This was demonstrated in a conference paper at ICLR 2019."
}