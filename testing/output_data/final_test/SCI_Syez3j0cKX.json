{
    "title": "Syez3j0cKX",
    "content": "Recent advances in Generative Adversarial Networks have led to extensions in various domains. IRGAN aims to apply this framework to Information-Retrieval by modeling the correct conditional probability distribution over documents given a query. However, there are inaccuracies in their formulation, and a co-training setup may lead to better performance. Two models are trained cooperatively for Information-Retrieval tasks like web-search, content-recommendation, and Question-Answering. GANs are proposed as alternatives to generative models for modeling data well in high dimensional settings. IRGAN is a framework proposed for Information-Retrieval tasks, utilizing a discriminator and a generator to model the training data's distribution. The discriminator learns to distinguish between real and generated documents, while the generator aims to bring its distribution closer to the real one to confuse the discriminator. The loss function in the work has issues with the baseline term, leading to problems in the training process. Pre-training the discriminator and generator results in decreasing generator performance, contrary to expectations. IRGAN struggles to choose between generator and discriminator based on performance, instead of reaching equilibrium where the generator is preferred. The paper critiques the performance of IRGAN and proposes two models that outperform IRGAN on two tasks and match its performance on the third. It suggests that the generator may not play a crucial role during training or testing. The contributions include a Co-training motivated model, pointing out flaws in IRGAN's loss function, and supporting it with insights from loss curves. Generative Adversarial Networks (GANs) were introduced as an alternative to generative models using Markov Chains. GANs consist of a generator modeling real data distribution and a discriminator distinguishing between real and generated data. Despite being challenging to train, advancements like DCGANs and Wasserstein GAN have addressed some issues. GANs have diverse applications in image and text generation, cross-modal retrieval, Interactive Image Generation, Text to Image, Image to Image style transfer, and robotics. Conditional GANs partition the sample variable into z and y, with y indicating the part of the probability distribution to generate. Conditional GANs use y to represent the query or its embedding, allowing the model to generate the required document. Sketch-GANs were used for retrieving similar merchant seals based on an input image. DCGANs are employed to generate images, with the discriminator's last layer used as an encoder for image responses. IRGAN is a model that combines generative and discriminative retrieval approaches to train a discriminator and generator alternately. The discriminator minimizes the likelihood of fake data and maximizes it for real data, while the generator aims to produce data that the discriminator perceives as real. This approach results in a generator capable of generating data resembling real data. The generative retrieval model p \u03b8 (d|q, r) samples relevant documents to clone the true distribution, while the discriminative model f \u03c6 (q, d) classifies real vs. generated pairs. Two loss functions IRGAN-Pointwise and IRGAN-Pairwise are proposed for training. The generator G can also be expressed as p \u03b8 (d|q n , r), and the discriminator's score is D(d|q) = \u03c3(f \u03c6 (d, q)). In some IR problems, training data consists of ordered document pairs where one document is more relevant for a query than the other. The generator samples documents based on relevance scores, leading to the use of policy gradients for training. Documents are likened to arms in a contextual multi-arm bandit, with picking an arm analogous to selecting a relevant document. The policy gradient (REINFORCE) is used in training the generator for Information Retrieval problems. To reduce variance, the advantage function is used instead of just the reward. Another baseline term suggested is f \u03c6 (d + , q), where d + is the positive document, to improve generator scores. This baseline term is used in two out of three tasks and affects the adversarial formulation. The generator can be optimized using REINFORCE for Information Retrieval problems. The baseline term exacerbates training by affecting the adversarial formulation. The discriminator's updates are in the direction of \u2207J D, while the generator's updates are in the direction of \u2207J G. The equivalent loss functions can be written to allow easier flow. The discriminator and generator in IRGAN optimize opposite loss functions, affecting model performance. Experimental proof shows the discriminator's role in maximizing real data likelihood. Two models are proposed to analyze performance gains in IRGAN, focusing on increasing training data likelihood and decreasing irrelevant documents likelihood. The proposed model uses two discriminators in a cooperative setup influenced by Co-training BID3 to improve upon IRGAN. Training involves using the same views for both discriminators but letting them influence each other in a feedback loop. This model achieves better performance by decreasing the likelihood of documents relevant to the other discriminator. The proposed model uses two discriminators in a cooperative setup influenced by Co-training to improve upon IRGAN. Experiments are conducted on three tasks: Web Search, Item Recommendation, and Question Answering, using the same datasets as IRGAN. The tasks involve retrieving relevant documents, recommending movies based on user ratings, and answering queries. The hyperparameters for the model are mostly the same, except for the absence of G Epochs. The proposed model uses two discriminators in a cooperative setup influenced by Co-training to improve upon IRGAN. Experiments are conducted on three tasks: Web Search, Item Recommendation, and Question Answering. The performance of various models is reported, with the Single Discriminator and Co-training models outperforming IRGAN models. The proposed models show good performance in sparse reward settings, with each query associated with approximately 5 positive documents. The single discriminator model achieves a slightly lower score, making just 7 more mistakes due to the small dataset size. The proposed model, influenced by Co-training, outperforms IRGAN in various tasks. Despite a slightly lower score due to the small dataset size, the model shows promising results. However, attempts to replicate results for Question-Answering tasks were unsuccessful, and only the best results from multiple random seeds were reported. The co-training model surpasses IRGAN-Pairwise, with observed discrepancies in the generator's performance compared to actual adversarial training. In the minimax setting, the generator's performance in GANs and DCGANs improves as it captures the real data distribution. A deteriorating generator suggests the discriminator's improvement is solely due to the first term of J D, indicating potential for better performance than IRGAN. The reason for a worse generator being attributed to document distribution sparsity is questioned, as DCGANs have successfully modeled high-dimensional data. The discriminator's performance increase is consistently accompanied by a deteriorating generator. The discriminator and generator in IRGAN optimize opposite loss functions. In the item-recommendation task, the generator's performance improves while the discriminator's loss remains high. This suggests IRGAN's success is due to maximizing the likelihood of real data. Conditional GANs are directly connected to Information Retrieval, and the problem can be viewed as a contextual multi-armed bandit problem. In previous works, the action-value function has been modeled as linear or deep neural networks. A parallel is drawn between Actor-Critic algorithms and GANs, showing equivalence in certain scenarios. However, this equivalence only holds when a baseline term is not used, making the formulation in IRGAN not exactly equivalent to a GAN framework. Another study connects Inverse Reinforcement Learning and GANs. The study draws a parallel between Inverse Reinforcement Learning (IRL) and GANs as both methods aim to learn the cost function. However, IRGAN's performance is not state-of-the-art on various datasets, with long training times and shaky mathematical formulation. The generator becomes useless after training, and the poor performance on certain tasks is attributed to optimizing opposite loss functions. The study discusses the limitations of Content-Recommendation in sparse reward scenarios and suggests using techniques like reward shaping and Hindsight Experience Replay for better performance. It also mentions exploring adversarial frameworks for tasks like Image Retrieval and Question Answering. Hyperparameters are detailed in tables 6-10 for hyperparameter tuning. The study discusses limitations of Content-Recommendation in sparse reward scenarios and suggests techniques like reward shaping and Hindsight Experience Replay for better performance. Hyperparameters for hyperparameter tuning are detailed in tables 6-10. Gradient Descent Optimizer was used for fair comparison with IRGAN. Different architectures were used for tasks like Web-retrieval, content-recommendation, and Question-Answering. For the Question-Answering task, words are initialized to a 100 dimensional vector. A Convolutional Neural Network with a convolutional kernel window size of (1, 2, 3, 5) is used, followed by max-pooling-over-time to produce a 100 dimensional output vector. This architecture is similar to the one in the IRGAN paper. Refer to the IRGAN paper for more details."
}