{
    "title": "BkDB51WR-",
    "content": "We propose using a Recurrent Neural Network (RNN) to compute the temporal evolution of a probability density function for time series regression. A softmax layer is used for numerical discretization, and regularization strategies are implemented for smoothness. A Monte Carlo procedure is presented for multiple-step forecasting. The algorithm outperforms baselines on synthetic and real datasets, showing promise for deep learning in manufacturing processes. Manufacturing processes in Industry 4.0 require system identification from sensor observations due to system complexity. Limited methods exist for reconstructing noisy nonlinear dynamics. The study focuses on systems driven by complex dynamics, with noisy measurements observed at each time step. The study focuses on computing the temporal evolution of the probability density function of noisy measurements in manufacturing processes in Industry 4.0. The observations are driven by complex dynamics with noisy measurements observed at each time step. In Section 3, the study discusses problems where simple regression for forecasting future values is not sufficient, especially in chaotic systems. The computation of time evolution of a probability density function (PDF) has been a long-standing topic in statistical physics. Extensive research has been done on modeling systems under linearity assumptions and certain noise models. Approaches like auto-regressive processes and Kalman filter estimate predictive probability distributions and forecast uncertainty. The Gaussian Process State Space Model (GPSSM) is a nonlinear model based on Gaussian processes, useful for identifying nonlinear systems with small datasets. However, its joint Gaussian assumption may limit its representation of complex non-Gaussian noise. Deep learning, particularly RNNs, has shown success in modeling complex spatiotemporal relationships in time series data, outperforming traditional methods. The recent works of BID authors introduced RNN-based algorithms for time series predictions, showing advantages over traditional methods. However, RNN lacks the ability to estimate probability distributions due to its deterministic nature. To address this, BID2 incorporated a latent random variable in RNN's hidden state and used variational inference for modeling stochastic properties. Additionally, BID0 and BID14 extended the Kalman filter to handle nonlinear dynamics by formulating a variational lower bound. The recent works by BID authors introduced RNN-based algorithms for time series predictions, addressing the deterministic nature of RNN by incorporating a latent random variable and using variational inference. BID6 and BID27 proposed methods to estimate model parameter uncertainty and forecast uncertainty in RNN, including model mis-specification error and data noise. The text proposes an RNN-model to compute the temporal evolution of a PDF, using a numerical discretization technique for classification. It introduces regularizations for cross-entropy loss to account for class ordering and suggests estimating one-step-ahead prediction from the output softmax layer. For multiple-step forecasts, a sequential Monte Carlo method is proposed. The text introduces the DE-RNN model for computing the temporal evolution of a PDF, proposing a method for multiple-step forecasts using a sequential Monte Carlo approach. It formulates the regression problem as a predictive density-estimation task and discusses the ability to handle multiplicative data noises. The proposed algorithm uses a Long Short-Term Memory (LSTM) network to compute the time evolution of probability distribution. It includes two regularizations for CE loss to capture class ordering in the discretized PDF. The algorithm was evaluated on synthetic and real datasets, showing advantages over baselines. The DE-RNN model is relevant for uncertainty quantification and propagation in physics and engineering. The LSTM network utilizes nonlinear transformations of input variables to approximate a complex dynamical system. It can be interpreted as a series expansion where simpler dynamical systems are combined. The LSTM network is often complemented by feed-forward neural networks for input features. The LSTM network, complemented by feed-forward neural networks, models the conditional PDF of a random variable y given input x. This simplifies the problem by treating the conditional PDF as a Markov process, allowing for direct application to estimating p(\u0177 t+1 |s t). The discrete probability p(k|x) is modeled by the softmax layer as an output of \u03a8 d, transforming the problem into a classification task. The bin size affects the approximation fidelity, and the discretization technique is similar to the finite volume method. This leads to conventional cross-entropy minimization in numerical simulations of partial differential equations. The softmax layer in the data set provides training data for cross-entropy minimization, aiming to ensure smoothness in the estimated distribution. The CE loss function penalizes incorrect labels uniformly, but in this study, proximity between classes is important. Two approaches are proposed to address this issue. In Sections 2.2.1 and 2.2.2, two regularization methods are proposed to impose class proximity structure in CE loss. The Regularized Crossentropy (RCE) minimization aims to smooth out the distribution by penalizing local minima or maxima. This can be achieved by enforcing spatial correlation in the network output using a one-dimensional convolution layer. In addition to Regularized Crossentropy (RCE) minimization, a convolution layer is used in DE-RNN to enforce smoothness in the distribution. This convolution layer, denoted as o \u2208 R K, is added on top of the last layer of DE-RNN before the softmax layer. The parameter h controls the smoothness of the estimated distribution, and the model can be trained using standard Crossentropy (CE) with this convolution layer. This approach, called convolution CE (CCE), acts as implicit regularization similar to kernel density estimation. The proposed method involves using a set of independently trained DE-RNNs to compute the joint PDF of a multivariate time series. By training l DE-RNNs to represent conditional PDFs, the joint PDF can be computed as a product of their Softmax outputs. This approach allows for parallel training of the DE-RNNs, reducing overall training time. The multivariate DE-RNN is used for forecasting by computing the probability distribution of the output. Multiple-step forecasts involve evaluating the temporal evolution of the distribution. The distribution of future time steps can be computed based on the previous steps and input variables. The method allows for efficient training and forecasting of multivariate time series data. The multivariate DE-RNN is used for forecasting by computing the probability distribution of the output. Multiple-step forecasts involve evaluating the temporal evolution of the distribution. The distribution of future time steps can be computed based on the previous steps and input variables. The method allows for efficient training and forecasting of multivariate time series data. The predictive distribution is computed using a sequential Monte Carlo method outlined in Algorithm 1, which approximates the high dimensional integration for intractable calculations. The DE-RNN method is tested on synthetic and real datasets using a consistent LSTM architecture. Two feed-forward networks are utilized with softplus and softmax functions. The LSTM is trained with ADAM BID13, a minibatch size of 20, and a learning rate of 10^-3. The first dataset considered is a modified Cox-Ingersoll-Ross (CIR) process. The DE-RNN method utilizes a modified Cox-Ingersoll-Ross (CIR) process with a stochastic differential equation solved using the forward Euler method. The simulation generates training data for two different bin sizes and is tested for errors in expectation and standard deviation. The normalized root mean-square errors (NRMSE) are calculated for the testing data. The DE-RNN method uses a modified CIR process with a stochastic differential equation. Prediction results improve with regularization for smoothness. RCE provides better predictions compared to CCE. NRMSEs are compared with predictions by AR(1) and KF. The DE-RNN method outperforms AR(1) and KF in modeling complex noise processes. DE-RNN is compared with a Monte-Carlo solution in a 200-step forecast, showing promising results. The DE-RNN method is used for a 200-step forecast with 20,000 samples, showing good agreement with the MC solution for the CIR process. The noise process in the forecast increases rapidly, then decreases before plateauing, accurately captured by the SMC forecast. DE-RNN is then applied to a time series from the Mackey-Glass equation, using specific parameters and solving methods. The DE-RNN model is trained and validated using noisy observations with white noise added. It consists of 128 LSTM cells and can accurately filter out noise to reconstruct the original dynamics of the chaotic system. The model shows strong de-noising capabilities and accurately represents the original attractor. The estimated probability distribution is shown in FIG2, demonstrating the effectiveness of the DE-RNN model. The DE-RNN model effectively filters out noise in chaotic systems, with CCE producing a smooth Gaussian distribution compared to the noisy standard CE results. The prediction errors are not sensitive to regularization parameters, with CCE achieving the best results. NRMSEs from KF and ARIMA are larger due to the underlying nonlinear dynamical system. DE-RNN accurately estimates noise components with only a 2% \u223c 5% error. The GP model outperforms KF and ARIMA in forecasting the Mackey-Glass time series, with a multiple-step forecast showing good initial approximation by a standard regression LSTM but eventual divergence. DE-RNN forecast bounds y(t) by a 95%-confidence interval due to the chaotic nature of the time series. In DE-RNN forecast, y(t) is bounded by a 95%-confidence interval even for a 500-step forecast, with uncertainty growing at a mild rate in time. The 95%-CI may vary following system dynamics, unlike conventional models like ARIMA and KF. DE-RNN is tested against atmospheric CO2 observations at Mauna Loa Observatory, using data from Mar-29-1958 to Sep-23-2017 for training and a 17-year forecast. DE-RNN with 64 LSTM cells and \u03b4y = 0.1sd[dy t ] is used for a 17-year CO2 forecast. The forecast captures the growing trend and oscillatory pattern of the data. While initially accurate, it eventually underestimates CO2 concentration. The upper bound of the 95%-CI grows rapidly, providing a good approximation of the observation. A regression LSTM forecast is also compared. The regression LSTM makes good short-term predictions but eventually diverges from observations. GP provides sharper uncertainty estimates for short-term forecasts but may overestimate for mid-term forecasts. It is challenging to determine which method, DE-RNN or GP, offers better forecasts. GP uses handcrafted kernels tailored to the problem, while DE-RNN does not require such tuning. In an experiment using IBM Power System S822LC and NAS Parallel Benchmark, CPU temperature is controlled by CPU frequency, utilization, and fan speed. Randomized CPU frequencies and job arrival times mimic real workload behavior. Forecasting CPU temperature accurately is crucial for energy-efficient thermal management in a cloud system. RCE and regression LSTM provide multiple-step forecasts of CPU temperature. The bin size is smaller than the sensor resolution, and future control parameters are considered in the forecast. In the forecast, DE-RNN predicts the probability distribution of future temperature with respect to control scenarios using 5,000 Monte Carlo samples. A 1800-step forecast is made, showing results only for t \u2208 (50, 1800) sec. RCE provides a more stable forecast compared to regression LSTM near the peak temperature at t 500. Maximum errors are observed near the peak temperature. Other forecasting methods like ARIMA, KF, and GP perform worse than LSTMs. Step changes in control parameters are associated with temperature changes. In the experiment, the multivariate DE-RNN is evaluated using a noisy time series generated by the Lorenz equations. The system of equations is solved using a third-order Adams-Bashforth method with specific coefficients. The RCE prediction is more robust to abrupt changes compared to regression LSTM, which shows large oscillations. The RCE prediction reflects uncertainty in measurements, while regression LSTM closely follows discrete levels. The Adams-Bashforth method with a time step size of 0.001 generates a time series data set by downsampling. A noisy time series is created by adding a multivariate joint normal distribution to the ground truth. The DE-RNN model accurately predicts expectations and covariances, with errors in covariance less than 4% except for \u03a312. DE-RNN also correctly predicts the signs of the covariance, outperforming a vector autoregressive model (VAR) and a GP model. The GP model outperforms VAR in forecasting the MackeyGlass time series, but errors are larger than DE-RNN. The error in covariance is about 10 times larger than DE-RNN, while the error in mean is about 3 times larger. DE-RNN accurately predicts expectations and covariances for complex time series data using LSTM to learn nonlinear dynamics. DE-RNN utilizes LSTM to learn nonlinear dynamics from time series data, supplemented by a softmax layer for probability density function approximation. Regularization strategies are proposed to impose a geometric structure in the distribution. The algorithm is validated on synthetic and real datasets, showing promising results. The extension of DE-RNN to multivariate time series is straightforward. The extension of DE-RNN to multivariate time series involves defining discretization grid points for variables, computing marginal PDFs, and training conditional PDFs using LSTM. The DE-RNN for multivariate time series involves training conditional PDFs using LSTM, where each DE-RNN is independent during training. This allows for parallel training, improving computational efficiency linearly with the number of dimensions. The joint PDF is computed by multiplying the DE-RNN outputs, enabling the computation of covariance for bivariate time series. The DE-RNN algorithm for multivariate time series involves training conditional PDFs using LSTM. During training, each DE-RNN is independent, allowing for parallel training. The joint PDF is computed by multiplying DE-RNN outputs to compute covariance for bivariate time series. In the prediction phase, there is a hierarchical dependency between all DE-RNNs. Sparse grid or Monte Carlo methods can be used for numerical integration in high-dimensional systems. The DE-RNN algorithm involves training conditional PDFs using LSTM for multivariate time series. During training, each DE-RNN is independent for parallel training, and the joint PDF is computed by multiplying DE-RNN outputs for covariance in bivariate time series. In the prediction phase, there is a hierarchical dependency between all DE-RNNs, and sparse grid or Monte Carlo methods can be used for numerical integration in high-dimensional systems. The architecture of DE-RNN and the process of computing one-step-ahead predictions of univariate time series are detailed, including the estimation of the predictive probability distribution and multi-step forecast using the sequential Monte Carlo method. The model computes a discrete approximation of the expectation for the next step observation using N s samples."
}