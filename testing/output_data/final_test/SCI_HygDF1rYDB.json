{
    "title": "HygDF1rYDB",
    "content": "We propose a method to automatically compute the importance of features in time series by simulating counterfactual trajectories. Our approach generates more precise explanations compared to existing methods and is less sensitive to noise. This is crucial in high-stakes applications like healthcare and finance where explaining model outcomes builds trust among users. In time series models, explaining feature importance is challenging due to complex dependencies and cross-correlations. Current work focuses on globally relevant features, but we aim to identify individualized feature importance and relevant time instances. Our proposed method uses counterfactuals to learn sample-specific feature importance at the observation level, providing more precise explanations in high-stakes applications like healthcare and finance. In this work, a counterfactual based method is proposed to learn the importance of each observation in a multivariate time series model by evaluating the expected change in model prediction. Plausible counterfactual observations are generated based on signal history to assess temporal changes in dynamics, ensuring more reliable explanations compared to other methods. The Feed Forward Counterfactual (FFC) method is described for generating explanations for time series models, considering the changing dynamics of features over time that may impact model outcomes. In time series, dynamics of features change over time, impacting model outcomes. A counterfactual based method is proposed to identify important observations for high-dimensional time series models. The FFC procedure generates CounterfactualXt using signal history to evaluate the effect of replacing observations on model output. Importance of a feature is determined by replacing an observation with a counterfactual, as shown in Figure 1. Multi-variate time series data is represented as X(n) \u2208 Rd\u00d7T for n \u2208 [N]. The FFC approach aims to explain the black-box model F by assigning importance scores to observations x_i,t in high-dimensional time series data X(n) \u2208 Rd\u00d7T. Feature importance is determined by the impact of replacing observations on model output, using CounterfactualXt generated from signal history. Feature Importance: The importance of an observation for a specific feature at a given time is defined as the change in model output when the observation is replaced by a counterfactual sample. The quality of the counterfactual sample directly impacts the explanation quality, and it is generated based on the signal history up to that time. Conditioning on the history ensures that the counterfactuals are not only sampled within the domain but also likely under the individual sample. Having a generator that models data across population allows for learning the dynamics of the signal and generating plausible counterfactuals. The counterfactuals are sampled from the marginal distribution obtained from the model output. Feature importance is estimated by comparing the model output with and without the generated counterfactual. The proposed method highlights relevant time events for different features, crucial for actionable explanations in various settings such as clinical scenarios. The importance of feature importance in generating actionable explanations is highlighted. By ranking time instances based on feature importance, a subset of important features can be reported at each time point, reflecting the correlation between various features. A recurrent latent variable generator model is used to approximate the conditional distribution of the time series data. The architecture in Chung et al. (2015) uses a conditional generator to model the history of a time series. The generator only uses past information to reflect temporal dependencies and can handle varying lengths of observations. Counterfactuals are based on past dynamics, not future values. Feature importance is limited by the quality of imputation used by the risk predictor. The proposed procedure involves training a generator (G) using a trained black box model F and training data. The generator samples counterfactuals (x i,t) to evaluate their effect on the black-box outcome. Model performance in time-series deep learning can be explained through visualization of latent layers and sensitivity analysis. Understanding latent representations and sensitivity in model behavior is crucial for model debugging. Attention models are commonly used for explaining sequential data, but attributing attention weights to individual observations in recurrent models is challenging. An attention model for mortality prediction in ICU patients has been proposed, but attention weights may not always provide consistent explanations. In vision, explainability has been approached from a counterfactual perspective to identify influential regions in images for model predictions. Counterfactual explainability focuses on identifying image regions that impact model predictions the most. Methods like conditional generative models address generating realistic counterfactuals for images. Evaluating feature importance for time series models is understudied, with most efforts focusing on population-level importance. Existing methods for image classification cannot be directly applied to time series models due to their complex dynamics. Existing methods for evaluating feature importance in time series models are limited, with a focus on population-level importance. Suresh et al. (2017) propose a method called \"feature occlusion\" to attribute feature importance by replacing time series observations with noisy samples. However, the selection of counterfactuals is crucial for deriving reliable importances, as using out-of-domain samples can lead to arbitrary changes in model output. It is important to model the data distribution to generate accurate feature importance assessments. The text discusses the importance of modeling the data distribution to generate reliable counterfactuals for assessing feature importance in time series models. It compares the method of Feature Occlusion (FO) introduced by Suresh et al. (2017) with a new explainability method on simulated and real datasets. The choice of generator for counterfactuals impacts the quality of explanations. The text discusses the importance of explainability methods in time series models, including Feature Occlusion (FO) and LIME. Evaluating the quality of explanations is challenging due to the lack of a gold standard, and explanations are reflective of model behavior. A simulated environment was created to test the reliability of the model. In an experiment, a simulated environment was created to test a method for time series data. The task was kept simple to focus on evaluating the quality of explanations without worrying about the classifier's quality. The goal was to have a gold standard for explanations by knowing the exact important event predictive of the outcome. The experiment involved generating non-linear time series data with spikes and linear trends. A black-box model was trained on this data, achieving an AUC of 0.99 on the test set. Different explanation methods were compared, with augmented feature occlusion showing improved performance in identifying important features. The proposed method (FFC) assigns importance to the first feature at the time of spike, generating fewer false relevance scores compared to baseline methods. Evaluation using ground truth shows higher log-probabilities of counterfactuals under the true distribution. The first simulation does not assess feature importance under complex state dynamics, creating a dataset with known ground truth for evaluation. In a simulation, a dataset with complex dynamics and known ground truth explanations is created. It consists of multivariate time series signals with 3 features, modeled using a Hidden Markov Model with 2 latent states. The outcome y is influenced by different features in each state, and non-stationarity is introduced by modeling state transition probabilities as a function of time. The ground truth explanation for the output at time T includes the most important feature variable and the time point of importance. Figure 4 illustrates the importance assigned to a time series sample by different methods. AFO, FO, and FFC can learn state dynamics and identify important features, but only FFC's top important observations correspond to state changes. Table 2 compares performance to ground-truth explanations, emphasizing the importance of quality counterfactuals in explaining latent dynamics. The conditional distribution models individual sample characteristics, while the marginal represents population averages. Counterfactuals inconsistent with a patient's dynamics can lead to inaccurate importance assignments. Log probability of counterfactuals under the true generator distribution is evaluated. Conditional generators are trained for non-static time series features using an HMM. Importance scores over time are compared across existing methods. Visualizing importance scores over time for patient trajectories, different methods mostly agree on important features but not on exact timing. Counterfactuals are based on patient history, with FFC assigning importance at signal change times. Evaluation of accordance scores and average scores across test samples are provided in the Appendix A.3 and Figure 10. The FFC counterfactual method conditions on patient history to represent an observation assuming no change in state. Evaluation of explanations includes intervention information from patient records to assess clinical applicability. Clinicians intervene when there is an adverse change in patient trajectory, making it important to identify relevant features before intervention onset. The experiment evaluates the method's utility in attributing importance to GHG tracers across different locations. The study evaluates the utility of a method for attributing importance to GHG tracers in California using an RNN model. The method assesses feature importance for weather modeling over time and is tested on real data by selecting globally relevant methods. Results show that FFC generates instance-wise feature importance over time. FFC method generates instance-wise feature importance over time and reasonable global relevance of features. Evaluation includes randomization tests to test sensitivity of explanations to black-box model parameters and data labels. Test conducted on Simulation Data II to assess explanation quality when model trained on permuted labels. The drop in explanation quality is evaluated by comparing the AUROC and AUPRC of generated explanations to ground truth when model parameters are shuffled. Results show significant differences in explanation performance, indicating sensitivity to black-box model parameters. Randomization tests on saliency map methods for images also demonstrate drops in explanation performance. The model's AUROC drops to 0.52 from 0.95 when trained on permuted labels. The explanation method's performance significantly drops when model parameters are randomized, indicating sensitivity to perturbations. A new method for sample-based feature importance in high-dimensional time series data is proposed, focusing on identifying important observations causing significant changes in model output. A generative model is proposed to sample counterfactual observations, evaluating its effectiveness in localizing important observations over time. The method provides individual feature importance over time and will be extended to analyze real datasets with feature importance explanations and evaluate changes in risk based on relevant subsets of observations. Simulated data is used to demonstrate the method's application. The generative model samples counterfactual observations with individual feature importance over time. It includes linear trends, spikes, and a logit output assignment. The true conditional distribution is derived using the forward algorithm. The true conditional distribution is derived using the forward algorithm. The generator G i is trained using an RNN (GRU) to model the latent state z t with a multivariate Gaussian. Counterfactual observations can be sampled by marginalizing over other features at time t. Adult ICU admission data from the MIMIC dataset is used for analysis to predict 48-hour mortality based on clinical data. Samples with less than 48 hours of data are removed. The recurrent network learns a hidden latent vector h t with specific parameter settings. The recurrent network with specific parameters learns a hidden latent vector representing the history, which is then used to model the conditional distribution. Accordance testing compares how different baselines agree on important feature assignment, with counterfactual methods mostly agreeing on key features. The accordance score measures the similarity between methods in identifying important signals. Inference runtime comparison across multiple baselines is also provided. The inference runtime for counterfactual approaches (FFC, FO, and AFO) depends on the time series length, while for FFC, it also depends on the number of features. FFC performs well compared to ad-hoc approaches due to the efficiency of the RNN conditional generator. The RNN generator model is used to approximate the conditional distribution. Figure 11 displays the training loss of the black-box used for feature importance results in Section 4.4."
}