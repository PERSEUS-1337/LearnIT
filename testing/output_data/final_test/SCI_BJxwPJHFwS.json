{
    "title": "BJxwPJHFwS",
    "content": "Robustness verification is crucial for understanding neural network behavior and ensuring safety. Previous methods are limited to simple networks, but this paper introduces a verification algorithm for Transformers. The algorithm addresses challenges like cross-nonlinearity and cross-position dependency, providing tighter robustness bounds than Interval Bound Propagation. These bounds also help interpret Transformers in sentiment analysis. Neural network verification is essential for analyzing and understanding the behavior of deep neural networks, especially in unseen situations. It helps ensure reliability and stability in predictions, with applications in safety-critical scenarios, model explanation, and robustness analysis. Neural network verification algorithms aim to characterize network predictions within a specified input space. For example, in robustness verification, algorithms verify if the output of a specific class is always greater than another class within a defined input space. This involves solving a nonconvex optimization problem, with recent algorithms focusing on finding lower bounds for efficiency and safety guarantees. In this paper, a robustness verification algorithm for Transformers with self-attention layers is developed. Transformers are widely used in natural language processing and various domains. The algorithm aims to compute a lower bound for frames under perturbation in the input sequence. The algorithm computes a lower bound for frames under perturbation in the input sequence by adopting a linear-relaxation framework. It resolves challenges in verifying Transformers with self-attention layers due to their complex architecture and the involvement of multiplication or division in self-attention layers. The algorithm computes linear bounds for frames under perturbation in input sequences, addressing challenges in verifying Transformers with self-attention layers. Unlike feed-forward networks, self-attention involves cross-nonlinearity and cross-position dependency, which previous methods struggle to track efficiently. Our approach derives closed-form linear bounds with O(1) complexity, offering a more efficient solution. The algorithm introduces an efficient bound propagating process for self-attention layers in Transformers, reducing complexity by O(n) for input length n. It addresses challenges like cross-nonlinearity and cross-position dependency, providing tighter bounds for verification. Our bounds are tighter than those obtained using Interval Bound Propagation (IBP) and reflect the importance of input words in sentiment analysis. Robustness verification for neural networks involves verifying if the prediction remains unchanged within a small region around the input. This problem can be mathematically formulated and solved optimally to derive the minimum adversarial perturbation. Several works focus on solving this problem exactly and optimally. Several works focus on solving the problem of verifying if the prediction remains unchanged around the input using various methods like MILP, branch and bound, and SMT. Due to the nonconvexity of the model, computing a lower bound efficiently is challenging, leading to the use of convex relaxations for non-linear activation functions. Existing methods for robustness verification in neural networks are limited to simple architectures like feed-forward networks and RNNs, unable to handle Transformers and Self-Attentive Models. Transformers, based on self-attention mechanism, have achieved state-of-the-art performance in NLP tasks. Self-attentive models are also used in tasks beyond NLP, such as VisualBERT for text and image features extraction. In this paper, the authors propose the first verification method for Transformers to verify their robustness. They aim to verify the robustness of a Transformer model using a sequence of frames as input, focusing on binary text classification as an example. The method is based on Interval Bound Propagation (IBP) for certified robustness training of Transformers. The method for verifying Transformers is general and can be applied in various applications. It aims to compute a lower bound \u03b4 L (X) for the output score difference between correct and incorrect classes within a perturbed input set S. This lower bound is obtained by computing the bounds of each neuron when the input is perturbed. The method verifies Transformers by computing lower bounds for neuron bounds when input is perturbed. Transformer layers consist of sub-layers with neurons after operations categorized into linear transformations, nonlinear functions, and self-attention. Each sub-layer has n positions with groups of neurons. The goal is to compute global lower bounds from the first to the last sub-layer, representing neuron bounds as linear functions of previous layers' neurons. The method efficiently computes global lower bounds for neuron bounds in Transformers by using linear functions of previous layers' neurons. It combines backward and forward processes to reduce computational complexity, supporting verification of self-attentive Transformers. The method efficiently computes global lower bounds for neuron bounds in Transformers by combining backward and forward processes to reduce computational complexity. Linear transformations and unary nonlinear functions are used to propagate bounds between sub-layers in neural networks. The method efficiently computes global lower bounds for neuron bounds in Transformers by combining backward and forward processes to reduce computational complexity. Detailed bounds for functions involved in Transformers are provided in Appendix B. Back propagation is used to retain positive and negative elements in vectors while setting other elements to 0. Self-attention layers pose the most challenges for verifying Transformers. Linear bounds for different heads of the multi-head attention in Transformers can be easily concatenated. Linear bounds for the self-attention mechanism in Transformers are represented as linear functions of x (r) for queries, keys, and values. The output is bounded by linear functions for multiplications and divisions. The method uses a forward process to compute bounds, reducing computational complexity. Attention scores are computed from q (l,i) (X) and bounded accordingly. Linear bounds for q (l,i) (X) and k (l,i) (X) are propagated forward to S (l). The self-attention mechanism in Transformers uses linear bounds for queries, keys, and values. Attention scores are normalized into probabilities with a softmax function. The output bounds of self-attention are obtained through a forward propagation process. When computing bounds for a later sub-layer, the bounds from the closest previous self-attention layer are directly propagated. The forward process in Transformers reduces complexity by computing linear bounds for the l-th layer, improving efficiency compared to the backward process. This approach represents bounds as linear functions of perturbed positions only, reducing the need to compute all positions and making it more efficient for long input sequences. The forward process in Transformers reduces complexity by computing linear bounds for the l-th layer, improving efficiency compared to the backward process. The number of perturbed positions is assumed to be small, reducing the number of matrices \u039b and \u2126 computed to O(m 2 n). This approach tightens bounds and is more efficient for long input sequences. In experiments on sentiment analysis datasets like Yelp and SST-2, Transformers are trained from scratch with specific model configurations. The study focuses on N-layer models with 4 attention heads, hidden sizes of 256 and 512, and ReLU activations. Variance-related terms in layer normalization are removed to tighten verification bounds without compromising clean accuracies. This method can be applied to Transformers with any architecture. Our method focuses on verifying Transformers with any number of layers, excluding large pre-trained models like BERT due to verification challenges. Current verification methods for feed-forward networks struggle with Transformers' increased complexity. The study presents clean accuracies and bounds for perturbations, comparing upper bounds obtained through enumeration with certified lower bounds by IBP and our method. Our method computes certified lower bounds for different models on various datasets, including 1-position perturbation constrained by different norms and 2-position perturbation constrained by 2-norm. We compare our lower bounds with those from Interval Bound Propagation (IBP) and upper bounds obtained through word enumeration, which has exponential complexity and is not feasible for perturbations on multiple positions. Our method computes certified lower bounds for perturbations on 1 or 2 positions, with results reported for 10 test examples. The lower bounds are tighter than those obtained through Interval Bound Propagation (IBP), showing reasonable gaps compared to previous work. Our proposed method computes robustness bounds for Transformers, demonstrating similar quality to simpler neural networks. A comparison of certified lower bounds and computation time shows the effectiveness of combining backward and forward processes. The method outperforms variations like FullyForward and Fully-Backward in tightness of bounds and computation time. Experiments were conducted on smaller models with reduced hidden sizes and 1-position perturbation to accommodate Fully-Backward with large computational cost. Average importance scores of important words were analyzed on SST and Yelp datasets. Certified lower bounds can identify important words based on model sensitivity to input token perturbation. In an experiment, a 1-layer Transformer classifier was used to verify the identification of important words using certified lower bounds. The method was compared with two baselines - Upper bounds and Gradient. Sentiment labels on SST were analyzed, with importance scores based on the distance to the neutral label. The evaluation was done on 100 random test input sentences. Our method using certified lower bounds accurately identifies the most and least important words in sentences compared to baseline methods. The evaluation was done on 100 random test input sentences, showing that our method outperforms the baselines. Additionally, a qualitative analysis on a larger dataset, Yelp, further supports the effectiveness of our approach. Our certified lower bounds accurately identify word importance in sentences, especially sentiment polarities. Baseline methods struggle with this task, showing the superiority of our approach. Our method provides valid lower bounds within a large neighborhood, offering more accurate results and proposing a robustness verification method for Transformers. The method proposed provides certified lower bounds for Transformers, addressing challenges like cross-nonlinearity and cross-position dependency. It outperforms baseline methods in identifying word importance in sentiment analysis. The illustration in Figure 1 shows different bounding processes for a 2-layer Transformer model. The method proposed provides certified lower bounds for Transformers, addressing challenges like cross-nonlinearity and cross-position dependency. It outperforms baseline methods in identifying word importance in sentiment analysis. The illustration in Figure 1 shows different bounding processes for a 2-layer Transformer model. The curr_chunk discusses the operations and processes involved in the algorithm, including linear transformations and nonlinear functions, with a focus on forward and backward bound propagation. The curr_chunk discusses deriving linear bounds for unary nonlinear functions in Transformers, such as ReLU and tanh, to ensure validity and tightness. Parameters are determined based on input bounds, aiming for tight linear lower and upper bounds for each neuron. Additionally, bounds for functions like e^x, 1/x, x^2, and \u221ax are derived. The curr_chunk by Zhang et al. (2018) discusses deriving linear bounds for ReLU and tanh activation functions in Transformers. Parameters are determined based on input bounds to ensure tight linear lower and upper bounds for each neuron. Additionally, bounds for functions like e^x, 1/x, x^2, and \u221ax are derived. The curr_chunk discusses deriving linear bounds for tanh activation functions in Transformers. It explains the approach for determining upper and lower bounds based on the concavity or convexity of the function. Binary search is used to find the bounds, with specific considerations for the exp function used in softmax calculations. The curr_chunk discusses constraints for the reciprocal and square functions used in softmax and layer normalization. It explains how to ensure the lower bounds are always positive and how to determine upper and lower bounds based on the convexity of the functions. The curr_chunk explains the constraints imposed on the square root function in layer normalization to ensure non-negativity. The function is limited to positive inputs by lower bounds and a smoothing constant. The curr_chunk discusses the mathematical proof of optimal parameters for linear bounds of multiplications and how linear bounds of division can be indirectly obtained. It aims to determine optimal parameters of bounding planes to make the bounds as tight as possible. The goal is to minimize the integral of the difference function between the original function and the lower bound. The text discusses minimizing the integral of the difference function to tighten the lower bound for optimal bounding planes. It focuses on ensuring non-negativity of F L (x, y) by checking corner points for minimum values. The text discusses minimizing the integral of the difference function to tighten the lower bound for optimal bounding planes by checking corner points for minimum values. The minimum function value F L (x, y) within the concerned area can only appear at the border, with optimal parameters derived for the lower bounding plane. The text derives optimal parameters for the upper bounding plane by minimizing VU1 and VU2. Linear bounds of multiplications can be derived, but directly bounding z = xy is challenging. The partial derivatives of FL are discussed for finding minimum values at corner points."
}