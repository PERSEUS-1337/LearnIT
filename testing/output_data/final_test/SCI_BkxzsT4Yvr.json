{
    "title": "BkxzsT4Yvr",
    "content": "Stochastic gradient descent (SGD) is the primary optimization method for training deep neural networks, known for its desirable properties and relatively good generalization. Deep gradient boosting (DGB) is introduced as an extension of SGD for neural networks, where back-propagated gradients are treated as pseudo-residual targets. The weight update at each layer is calculated by solving a boosting problem using a linear base learner. Implementing DGB as a separate input normalization layer (INN) improves performance on image recognition tasks compared to architectures without normalization layers like batch normalization (BN). Boosting is a successful machine learning technique that combines simple predictors to improve performance. AdaBoost was one of the first successful implementations, later recognized as performing gradient descent in functional space. Friedman defined a statistical framework for training boosting-like classifiers. Boosting minimizes a loss function by iteratively choosing a weak learner that points in the negative gradient direction of a functional space. Neural networks, especially deep neural nets, are trained using stochastic gradient descent (SGD) for good generalization error and scalability with large datasets. Training neural networks involves updating network parameters using SGD and comparing the output to a target value with a loss function. The backpropagation algorithm efficiently calculates parameter gradients for updating network parameters in the direction of the negative gradient of the loss function. Despite the availability of accelerated optimization methods like Adam, Adagrad, and RMSprop, stochastic gradient descent (SGD) still shows very good performance on validation data sets compared to other methods. Explicit regularization methods like dropout or batch normalization can further improve performance. Recent work by Dinh et al. (2017) challenges the prevailing explanation that SGD performs well due to its preference for \"flat\" minima. They introduce deep gradient boosting (DGB), a method that combines backpropagation with gradient boosting. DGB reinterprets each backpropagation iteration as solving a regularized linear regression problem at each layer, replacing decision trees with linear regressors. SGD is seen as an extreme case under this model. Under the deep gradient boosting (DGB) model, decision trees are replaced by linear regressors, with SGD emerging as an extreme case due to highly regularized network weights. INN, a parameter-free layer normalization method, is introduced for learning problems with less strict regularization criteria. It achieves competitive results in image recognition compared to batch normalization (BN). The goal of gradient boosting is to approximate a function F(x) that minimizes a specified loss function in training examples generated from a probability distribution. Gradient boosting seeks weak learners from a differentiable class to minimize a loss function. Neural networks update weights using steepest descent and backpropagation. The algorithm for gradient boosting involves choosing a weak learner and fitting it to pseudo-residuals using regularized linear regression. The update rule for a given input batch involves weight matrices, input matrices, and pseudo-residual matrices. In practice, the dual form formulation is often used for neural networks with a large number of neurons per layer. The dual form formulation of ridge regression is preferred for neural networks with many neurons per layer. Weight update formulas can lead to differences in scale between weights from layers of different sizes. A comparison of DGB and SGD performance on various datasets is conducted using simple dense layer neural networks. The first dataset used is the MNIST database of handwritten digits. The second dataset used is the Higgs data set from high energy physics, with 1 million training samples and 500 thousand testing samples. The third dataset is Reuters-21578, consisting of 11,228 news articles labeled over 46 topics. The fourth dataset is the Human Resource (HR) data set by IBM Inc., predicting employee attrition with 47 features. The fifth dataset is a regression problem for air quality prediction based on chemical sensor outputs, air temperature, and humidity. The experiments involved using an array of chemical sensors, air temperature, and humidity. The network architecture and activation functions were fixed, with weights initialized using a uniform distribution. Cross-entropy loss was used for classification tasks and mean squared loss for regression tasks. Performance metrics included accuracy for classification and RMSE for regression. Results were consistent across different network complexities. The study used a dense neural network with three hidden layers and 500 ReLU nodes. Models were trained with a batch size of 100 for 25 epochs and achieved 100% accuracy on the training set. Two DGB variants, DGB(l) and DGB(r), were compared on the MNIST dataset, with DGB(r) achieving the highest accuracy of 98.59% with \u03b1 = 1.0. This outperformed SGD with 98.41% accuracy. Increasing the regularization parameter improved test set performance for both variants. The study compared two DGB variants, DGB(l) and DGB(r), on an image classification problem. DGB(r) showed similar performance to DGB(l) but was faster due to smaller matrix calculations. Training on a single Nvidia GPU, SGD took 0.0166 sec, DGB(r) took 0.0188 sec, and DGB(l) took 0.1390 sec per iteration. The study decided to use DGB(r) for the rest of the experiments. The Higgs dataset used Monte Carlo simulations with 11 million samples and 28 features for binary classification. The Higgs dataset is a classification problem distinguishing between signal and background processes. 1 mil samples were used for training and 500k for testing, with 28 normalized features. A neural network with two hidden layers of 500 ReLU activations each was trained with a batch size of 30 over 50 epochs. The learning rate was adjusted based on \u03b1 values. Performance on the training set was relatively poor, with the best test accuracy achieved for smaller \u03b1 values. The Reuters text categorization dataset consists of articles on 46 news topics, divided into training and test sets. The curr_chunk discusses the processing of a dataset containing news articles on 46 topics, divided into training and test samples. A dense neural network with two hidden layers was trained on this data, yielding results close to 78.05% mean accuracy. The dataset also underwent min-max normalization on all 47 features for a binary classification task related to predicting employee attrition. The curr_chunk discusses a binary classification task to predict employee attrition using a dense neural network with one hidden layer. The dataset used had 1029 training samples and 441 test samples. Another dataset on air quality with 9358 instances was divided into training and test samples for a regression problem to predict concentrations of benzene. The curr_chunk focuses on predicting benzene concentrations using a neural network with two hidden layers and ReLU activations. Deep gradient boosting (DGB) with larger \u03b1 values performed closer to stochastic gradient descent (SGD), but relaxing the regularization parameter improved test set performance significantly. However, DGB's computational cost is high due to matrix inversion at each step and layer. The curr_chunk discusses speeding up matrix inversion in neural networks by using diagonal approximations for gradient updates. It introduces fast DGB(r) and fast DGB(l) for convolutional neural networks. Convolutional neural networks (CNN) are specialized for visual imagery analysis. Input at each CNN layer is a tensor of size (B \u00d7 c \u00d7 w \u00d7 h), where B is batch size, c is feature maps, and w and h are map dimensions. CNNs divide the feature map into sections using a small kernel (usually 3x3) to increase input samples and reduce weights. This regularization technique outperforms dense networks on image recognition tasks. The equivalent DGB formulation for CNN layers involves resizing the input tensor and normalizing it to optimize computational efficiency. Small kernels with a small step size allow for calculating the raw second moment of each feature map only once. The new normalized input can be viewed as input normalization defined as N(X) := X(X^T X + \u03b1I)^-1 or N(X) := (XX^T + \u03b1I)^-1 X followed by a regular SGD update, similar to other normalization methods like batch norm (BN), layer norm (LN), and instance norm (IN). These normalization methods accelerate descent, enable higher learning rates, and make training more resilient to parameter or input variations. Batch norm, layer norm, and instance norm normalize features/inputs by standardizing them with z-score, reducing the risk of gradient explosion in deep networks and improving generalization. They also act as regularization methods, eliminating the need for dropout. The normalization procedure described here introduces new parameters to scale and shift the output of a layer, improving the expressive power of the network. Unlike other normalization methods, this procedure is applied to the layer input and has only one predefined parameter \u03b1. The text discusses the effect of input normalization on the convergence rate of gradient descent in multiple linear regression models. It introduces a new normalization method using singular value decomposition to improve the condition number of the Hessian matrix. Input normalization in multiple linear regression models leads to a smaller condition number of the Hessian matrix, improving convergence rate. The new method involves using singular value decomposition and mean centering the input data. Input normalization in linear regression models improves convergence rate by bringing the input data near the critical point of activation functions and removing bias towards certain neurons. It has a regularization effect by shifting data in a nondeterministic way during training. INN(l) and INN(r) are defined as left and right input normalization, respectively. Exponential averages are calculated during training for batch statistics, and the new values are used at test time. Implicit and explicit input normalization models were tested on the CI-FAR10 dataset and compared to Batch Normalization. The best performing models were validated on the ImageNet object recognition dataset. The CIFAR10 dataset contains 60000 images in 10 classes, with 50000 for training and 10000 for testing. Performance on CIFAR10 is measured in accuracy. The ImageNet dataset has 1,281,167 training and 50,000 validation images in 1,000 categories. VGG19 and ResNet101 with batch normalization were compared to versions with input normalization layers. This work introduces Deep Gradient Boosting (DGB), an extension of Stochastic Gradient Descent (SGD) for better control over generalization properties. DGB outperforms SGD in certain cases for classification and regression tasks. A faster approximation of DGB (FDGB) is proposed and extended to convolutional layers. DGB is reinterpreted as a normalization layer (INN) and tested on image classification tasks, showing performance comparable to batch normalization without extra parameters. For this experiment, a modified version of the VGG11 network was used with 8 convolutional layers, a linear layer with 512 ReLU nodes, and a final softmax layer. The architecture was further modified by removing batch normalization and dropout layers, replacing them with a fast version of DGB for the FDGB(l). The modified VGG11 network used convolutional and linear layers with a fast version of DGB for the FDGB(l) architecture or added INN(l) layers. Input normalization was based on the left pseudo-inverse for regularization. Weights were initialized according to Simonyan & Zisserman (2014) and trained using stochastic gradient descent. Training started with a learning rate of 0.1, reduced to 0.01 after 250 epochs, and continued for 350 epochs. Experiments were repeated 10 times with different random seeds, and performance was reported on the validation set as mean accuracy \u00b1 standard deviation. The VGG19 architecture by Simonyan & Zisserman (2014) consists of 16 convolutional layers with max-pool operations, followed by two linear layers with 4096 ReLU nodes and a final softmax layer with 1000 outputs. A version with batch normalization (VGG19 BN) was tested with and without affine parameters. An INN(l) version was created by removing dropout and batch normalization layers and adding input normalizations. The performance of INN on network architectures with residual connections was explored using ResNet101 proposed by He et al. (2016b). The ResNet101 network was modified to remove batch normalization layers and add INN operations in front of convolutional and dense layers. Stochastic gradient descent was used for training over 120 epochs with specific parameters. Evaluation was done by computing top-1 and top-5 validation errors on a held-out dataset."
}