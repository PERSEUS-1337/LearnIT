{
    "title": "SJxbps09K7",
    "content": "Unsupervised bilingual dictionary induction (UBDI) using Generative adversarial networks (GANs) with linear generators can align word vector spaces in different languages, but may be unstable for some language pairs. The instability depends on the shape and density of vector sets and can be mitigated through best-of-N model selection. The alignment of word vector spaces in different languages using GAN-based UBDI can be stabilized through best-of-N model selection. Learning a projection of word vector spaces into another is crucial for machine translation, especially for low-resource languages. Joint embeddings of words across languages can aid in bilingual extraction. Projections between word vector spaces can be learned from as little as 50 words, or even just identical words across languages, removing the need for an actual dictionary. Recent approaches use Generative Adversarial Networks (GANs) for unsupervised alignment of word vector spaces. The possibility of unsupervised bilingual dictionary induction (UBDI) using a linear transformation to minimize divergence between target and source distributions has shown promise in machine translation. However, challenges arise with language pairs having different properties and when monolingual corpora vary. Identifying easy, hard, and impossible instances of GAN-based UBDI is crucial, with hard cases exhibiting instability. The experiments investigated the instability of hard cases in alignment problems between word vector spaces. A distinction was made between easy, hard, and impossible cases, with a linearity test used to differentiate them. The impossible cases were attributed to properties of corpora and embedding algorithms, while the likelihood of being trapped in local minima in hard cases depended on vector set characteristics. Model selection over different initializations led to a significant error reduction in alignment. The paper presents MUSE BID6, a GAN-based approach to unsupervised best-of-N model selection for stabilizing UBDI. It discusses theoretical results from GAN literature, tests for discriminating between easy, hard, and impossible cases, and analyzes the instability of GAN-based UBDI through experiments. The dynamics of GAN-based UBDI training behavior are explored to understand limitations in alignment problems between word vector spaces. The curr_chunk discusses GAN-based UBDI, focusing on MUSE BID6. It explains how a GAN consists of a generator and discriminator, with the generator trained to fool the discriminator. The goal is to choose a linear transform \u2126 to make the distribution of \u2126E close to F. The discriminator's objective is to discriminate between vector spaces F and \u2126E. Training involves optimizing model parameters using stochastic gradient descent. During training, model parameters \u2126 and w are optimized using stochastic gradient descent by updating the discriminator and generator based on their respective losses. The loss function used is cross-entropy. The optimal parameters aim to solve a min-max problem where the generator competes against an ideal discriminator. If the generator wins consistently, the model learns the true data distribution. During training, the generator in MUSE alternates between optimizing the discriminator and itself. The discriminator loss can drop to zero if there is no overlap between the generated and true data distributions. However, in this case, the discriminator is initially presented with data for which there is no trivial solution. The discriminator and generator loss are not ideal model selection criteria in MUSE. Instead, a criterion based on cosine similarities between nearest neighbors in the learned alignment is proposed. A bilingual dictionary can be extracted from \u2126E and F using nearest neighbor queries. The method CSLS is used to expand high-density areas and condense low-density ones, improving cross-domain similarity. MUSE uses cosine similarity and CSLS for unsupervised model validation and bilingual dictionary extraction. The Procrustes algorithm is employed for refinement, while the idea of minimizing nearest neighbor similarity is also utilized in point set registration. The iterative closest point (ICP) optimization minimizes the distance between nearest neighbor pairs by computing the new relative transformation. ICP can converge to local optima in polynomial time but may get trapped in local optima. Exact algorithms for ICP exist only for two-and three-dimensional point set registration and are slow. The optimal solution for the GAN min-max problem is also optimal for ICP, as the L2 loss in ICP approaches 0 with increasing sample size. The ICP loss approaches 0 when an optimal solution to the UBDI min-max problem is found. PCA initialization is crucial for ICP according to BID16. In contrast, PCA initialization leads to performance degradation in MUSE. Linear transformations and isomorphisms are defined between vector spaces E and F. Most alignment work on word vector spaces assumes approximate isomorphism. Word vector spaces are not necessarily approximately isomorphic, leading to easy and hard cases in unsupervised alignment. Hard cases, like two smoothly populated vector spaces on unit spheres, have multiple equally good linear transformations. This poses challenges when using GANs for alignment. In unsupervised alignment, aligning two vector spaces can be challenging due to the non-convex loss landscape and small basin of convergence. Even in easy cases, minimizing Jensen-Shannon divergence may not guarantee alignment between translation equivalents. Linearly alignable vector spaces may not always be learnable using certain methods, requiring testing for approximate isomorphism and analysis for successful alignments. In experiments, embeddings are pretrained using FastText on various datasets. Procrustes fit is a linearity test that captures GAN-based UBDI dynamics well. It tests the linear alignability between two embedding spaces by applying Procrustes analysis on a dictionary seed. The Procrustes fit measure correlates with UBDI performance, serving as a sanity check and highlighting limitations. A strong correlation is shown in FIG0, indicating that poor UBDI performance in certain languages is due to a lack of good linear transforms from English. Comparisons are made to similarity measures for nearest neighbor graphs in vector spaces. The nearest neighbor graphs in vector spaces are used to determine isomorphism between two vector spaces. Algorithms like VF2 BID7 can check for graph isomorphism, but struggle with large graphs. A spectral metric based on eigenvalues is introduced as an alternative approach. The spectral metric introduced is based on eigenvalues of Laplacian of nearest neighbor graphs, quantifying the extent of isospectrality. Isospectral graphs may not be isomorphic, but isomorphic graphs are always isospectral. Adjacency matrices A1 and A2, Laplacians L1 and L2, and eigensimilarity are computed to find the smallest k where the sum of k largest Laplacian eigenvalues is <90% of the Laplacian. The smallest k is determined by summing the k largest Laplacian eigenvalues to be <90% of the Laplacian eigenvalues. Isospectrality varies with Procrustes fit, where a drop in fit leads to a drop in eigenvector similarity. Adding an edge to a graph changes its spectrum monotonically. In practice, Procrustes fit is more discriminative and computationally efficient compared to k-subgraph isomorphism and k-subgraph isospectrality tests. It correlates much better with UBDI performance, with a correlation coefficient of 96%. The Procrustes fit test was used to analyze the alignment between English-Bengali and English-Cebuano word vector spaces. The results showed poor alignment, which could be due to the quality of Bengali embeddings. Further tests with Bengali-Hindi also showed low alignment, contrary to expectations based on their relatedness. Observation 2 discusses the impact of different word embedding algorithms on alignment tasks. It highlights that while MUSE's performance may degrade with alternative algorithms, alignment is still achievable. However, using different monolingual embedding algorithms, such as FastText for English and Hyperwords for Spanish, can hinder alignment. Observation 2 discusses the impact of word embedding algorithms on alignment tasks, noting that while MUSE's performance may degrade with alternative algorithms, alignment is still possible. However, aligning FastText for English and Hyperwords for Spanish can be challenging. GAN-based UBDI struggles with aligning such cases, as seen in BID14. To understand GAN-based UBDI in difficult scenarios, three ablation transformations are introduced: unit length normalization, PCA-based pruning, and noising. Unit length normalization leads to instability and decreased performance in GAN-based UBDI. The text discusses the use of a transform to project word vectors onto a sphere in supervised bilingual dictionary induction. It highlights the sensitivity of UBDI to normalization and the impact of PCA pruning on GAN-based UBDI performance. The text discusses the impact of PCA-based pruning on GAN-based UBDI performance, highlighting the importance of density information for stability. Removing density clusters and applying UBDI results in reduced performance, emphasizing the significance of shape information. The text discusses the impact of noise injection on GAN-based UBDI performance, showing that M-noise uses random vectors to evaluate noise impact. M-cosine shows a 7% error reduction over MUSE for HARD languages and 4% across all language pairs. Pruning with PCA initialization leads to significant performance drops across different dimensions. GAN-based UBDI is minimally affected by noise injection due to injected vectors rarely ending up in seed dictionaries. In hard cases, GAN-based UBDI gets stuck in local optima, especially in cases where linear alignment is possible but UBDI is unstable. Increasing batch size or decreasing learning rate does not help. The discriminator loss does not increase in the region around the current discriminator model, leading to the model getting stuck in a local optimum. This is observed in English-Greek and English-Hungarian alignment cases. The comparison between the model induced by GAN-based UBDI and the (oracle) model obtained using supervised Procrustes analysis is shown in FIG2. The discriminator's loss curves indicate an initial drop in loss, suggesting no learning signal in the GAN-based UBDI direction. Sampling methods include random direction vectors and normal distributions orthogonal to the line segment. Observation 7: Over-parameterization does not consistently help in difficult cases. Recent work suggests that over-parameterization leads to smoother loss landscapes, making optimization easier. Experimenting with widening discriminators to smoothen the loss landscape yielded inconsistent results. Observation 8: Changing batch size or learning rate to hurt the discriminator also does not help. Previous research indicates that large learning rates and small batch sizes lead to flatter minima, but in experiments, the focus is on preventing the discriminator from ending up in flat regions. In an attempt to improve training stability, smaller learning rates and larger batch sizes were experimented with to reduce random fluctuations in SGD dynamics. The default hyperparameters of MUSE were found to be close to optimal. Two unsupervised model selection criteria were compared, with the proposal to use mean cosine similarity between translations predicted by the CSLS method. The CSLS method was used as a stopping criterion by BID6 for model selection in GAN-based UBDI. Using cosine similarity led to stable alignments, while discriminator loss increased instability. Random restarts and oracle model selection improved alignment probability. Some word vector spaces are not alignable based on distributional information alone, but GANs can induce alignment with varying degrees of instability. The degree of instability in word vector alignment is influenced by the shape and density of the vector spaces, not noise. Standard techniques like over-parameterization do not solve this issue. An unsupervised model selection criterion proposed leads to stable learning and reduces errors by 7% compared to MUSE. Further observations on word vector alignability are presented."
}