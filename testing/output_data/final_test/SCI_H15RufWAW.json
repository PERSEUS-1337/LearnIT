{
    "title": "H15RufWAW",
    "content": "GraphGAN is the first implicit generative model for graphs, aiming to mimic real-world networks by learning the distribution of biased random walks over a single input graph. Our model, GraphGAN, is a stochastic neural network trained using the Wasserstein GAN objective to generate sibling graphs with similar properties but not exact replicas. It learns a semantic mapping from the latent input space to the generated graph's properties, showcasing strong generalization in link prediction and node classification tasks. Generative models for graphs have various applications such as data augmentation, anomaly detection, and recommendation. Explicit probabilistic models like Barab\u00e1si-Albert or stochastic blockmodels are standard in the field, but they may not capture all the essential properties of real graphs. Recent works highlight surprising characteristics of real-world networks not accounted for by existing models, leading to the question of how to define a model that captures all these properties. Transitioning from explicit to implicit models is becoming popular in other fields like Computer Vision with Variational Autoencoder and Generative Adversarial Networks. GraphGAN is introduced as the first implicit generative model for graphs, aiming to address the challenges of adapting Generative Adversarial Networks (GANs) to handle discrete objects like graphs or text. The combinatorial structure of graphs, the lack of large repositories of graphs from the same distribution, and the need for permutation invariance are key obstacles in applying GANs to graphs. GraphGAN is the first implicit generative model for graphs, addressing challenges in adapting GANs for discrete objects like graphs. It formulates learning the graph topology as biased random walks distribution. The generator, a stochastic neural network, generates plausible random walks, while the discriminator distinguishes them from true ones. The model uses Wasserstein GAN for stable convergence and exhibits strong generalization properties, as seen in the experimental results. GraphGAN is the first GAN model that generates graphs through random walks, showcasing similar properties to input graphs but not exact replicas. The model can smoothly interpolate between graphs with varying properties and stays permutation invariant. It addresses challenges of learning from a single graph and generating discrete output, demonstrating strong generalization capabilities. GraphGAN is a model that can generate sibling graphs to a given input graph, showcasing similar topological characteristics but not exact replicas. It demonstrates strong generalization capabilities and competitive link prediction performance on real-world datasets. Additionally, the model's learned weights capture meaningful structural information, making it a valuable tool for node classification tasks. BID34's approach generates full adjacency matrices of graphs directly by applying random permutations to create additional training data. Their model's quadratic complexity limits it to processing only small graphs, with a reported runtime of over 60 hours for a graph with 154 nodes. In contrast, BID27 uses GANs to learn graph structures without focusing on generating full graphs. BID27 uses GANs to learn graph topological features by decomposing the graph into subgraphs processed by a GAN. Their approach makes spatial dependence assumptions about the adjacency matrix and requires manual copying of inter-subgraph edges. In contrast, our model does not make spatial dependence assumptions and generates the whole graph end-to-end. Our model differs from existing approaches by generating the entire graph end-to-end without spatial dependence assumptions. Previous methods focused on discrete data generation using GANs, mainly for sequences like text. Graph generative models have a long history, but none have considered graph structured data in this context. The generative power of different network models varies significantly. The configuration model rewires edges at random while preserving node degrees. The degree-corrected stochastic blockmodel (DC-SBM) captures power-law degree distributions and network topologies. Exponential random graph models (ERGM) preserve specified graph statistics. These models represent probability distributions over networks of a fixed size. GraphGAN is introduced as a Generative Adversarial Network model for graphs, aiming to capture real-world graph properties without manual specification. It learns the graph's topology by understanding the distribution over random walks on the input graph. The model samples random walks of length T from the adjacency matrix of the graph and uses a biased second-order random walk sampling strategy for training. GraphGAN utilizes a biased second-order random walk sampling strategy from the adjacency matrix to capture local and global graph structure efficiently. The model consists of a generator G and a discriminator D, trained end-to-end using backpropagation to generate synthetic random walks and distinguish them from real ones. This process allows for the generation of a new graph's adjacency matrix. The generator G in GraphGAN defines a probabilistic model for generating random walks using a neural network. It samples the next node based on a probability distribution and memory state at each step, initialized with a latent code. In this work, the focus is on the Long short-term memory (LSTM) architecture for generating random walks in GraphGAN. The latent code z initializes the memory state of the LSTM, consisting of cell state C t and hidden state h t. The use of memory and temporal dependencies in LSTM is questioned compared to Markov processes. The benefit of using random walks of length greater than 2 is discussed, highlighting the need for generalization rather than pure memorization in GraphGAN. GraphGAN focuses on generalization and generating similar sibling graphs, not exact replicas. Longer random walks with memory help the model learn topology and patterns in the data. The model outputs probabilities for the next node in the random walk, which are then up-projected to a high-dimensional space for computational efficiency. This approach is similar to context embeddings in representation learning. The GraphGAN model focuses on generalization and generating similar sibling graphs through longer random walks with memory. To address the non-differentiable operation of sampling from a categorical distribution, the Straight-Through Gumbel estimator is used, allowing gradients to flow through the differentiable v * t during backpropagation. The GraphGAN model uses the Straight-Through Gumbel estimator for sampling from a categorical distribution, allowing gradients to flow through the differentiable v * t during backpropagation. The new node v t is projected back to a lower-dimensional representation using the down-projection matrix W down before feeding into the LSTM. The matrices W down and W up serve as node embeddings, learning meaningful information about the nodes. The discriminator D, based on LSTM architecture, processes a one-hot vector v t at each time step and outputs a probability score after the entire sequence is processed. The Wasserstein GAN framework is used to train the model, with the discriminator enforcing the Lipschitz constraint using gradient penalty. WGAN objective prevents mode collapse and improves stability. Model parameters are trained with Adam and regularized with L2 penalty. Early stopping strategies are employed to prevent overfitting and generalize the input graph. Two early stopping strategies, VAL-CRITERION and EO-CRITERION, are used in GraphGAN training. VAL-CRITERION focuses on generalization properties by evaluating link prediction performance on a validation set. Training stops when validation performance plateaus. EO-CRITERION allows user control over graph generation by stopping training when a specified edge overlap is achieved between generated and original graphs. This flexibility enables users to generate graphs with varying edge overlaps while maintaining structural similarity. After training GraphGAN, the generator G constructs a score matrix S of transition counts from generated random walks. The raw counts matrix S is then converted to a binary adjacency matrix for reasoning about the synthetic graph. Symmetrization and binarization strategies are applied to handle overrepresentation of high-degree nodes in the generated random walks. To address the issue of leaving out low-degree nodes and producing singletons, a sampling approach is used to ensure every node has at least one edge. The edges are sampled without replacement using specific probabilities until the desired amount of edges is reached. GraphGAN is evaluated on various graph mining tasks and real-world datasets, comparing it with state-of-the-art methods. GraphGAN is evaluated on various datasets including well-known citation datasets and the Political Blogs dataset. Cora-ML, a subset of machine learning papers, is used in the evaluation. The goal is to generate graphs similar to the input graph using GraphGAN, by generating sibling graphs and comparing performance to baselines. 15% of edges are randomly hidden for training, and graphs are sampled from the generated models. GraphGAN and DC-SBM are trained on input graphs like CORA-ML, with GraphGAN able to model degree distributions accurately without access to them. GraphGAN closely matches the original graph in various important graph statistics, laying a foundation for implicit models in graph generation. The study focuses on implicit models for graph generation, comparing early stopping strategies VAL-CRITERION and EO-CRITERION. Higher EO leads to graphs closer to the original in statistics. Results and details on baselines can be found in the appendix. Graph statistics evolution during training on CORA-ML is shown in Figs. 3b and 3c. Assortativity value is reached after 40K iterations. Edge overlap increases smoothly with epochs. Various statistics for different models like node2vec, DC-SBM, and ERGM are presented. The study evaluates GraphGAN using link prediction in graph mining. EO is used as a measure of closeness in graph generation. Validation and test sets are created by holding out edges from the graph. Training network connectivity is maintained without singletons. GraphGAN's link prediction performance is evaluated by sampling random walks from the trained generator and using transition counts between nodes as a measure of edge likelihood. Different methods like Adamic/Adar, DC-SBM, and node2vec are compared, with GraphGAN showing competitive performance across all datasets. GraphGAN demonstrates competitive performance in link prediction tasks across various datasets, achieving state-of-the-art results for some. Increasing the number of random walks sampled from the generator improves performance, especially for larger networks like CORA, DBLP, and PUBMED. Despite having many hyperparameters, most are not critical for performance except for the length of random walks. The study confirms the use of neural networks for generating random walks of length T instead of just edges. Random walk length 20 shows marginal performance gain over length 16 but incurs higher computational cost. A recurrent discriminator is chosen over a convolution-based variant for better link prediction performance. Node classification is done using W up and W down as low-dimensional feature representations of nodes. GraphGAN is evaluated using W down, W up, and their concatenation for generalization properties. Logistic regression model is trained on a subset of nodes for node classification evaluation. W up is visualized using t-SNE to show community structure. Weighted macro F1 score for node classification is visualized in FIG3, showing comparable results with embedding-like weights produced by GraphGAN. The embedding-like weights in W up produced by GraphGAN show comparable performance with node2vec, indicating that GraphGAN captures meaningful structural information during the process of learning to generate random walks. The t-SNE visualization of W up reveals a clear grouping of nodes from the same community in the embedding space. While GraphGAN's learned node representations are a byproduct of the training procedure, they provide useful information about the nodes. For node classification, dedicated node embedding algorithms are recommended over GraphGAN. Interpolation is used to analyze the structure captured by the generator. The model is trained using noise drawn from a 2-dimensional latent space and divided into subregions for visualization. Random walks are generated for each subregion, and properties of the walks and resulting graphs are evaluated and visualized as heatmaps. In Fig. 6c and 6d, properties of graphs sampled from random walks in different bins are visualized. Distinct patterns are observed, such as higher average degree of starting nodes in certain regions. Sampling from the entire latent space yields sibling graphs with similar degree distributions. The experiment demonstrates that interpolating in the latent space produces graphs with smoothly changing properties. The model learns to map specific parts of the latent space to properties of the graph. Community distributions for input graph and sampled graph are compared. Transition between histograms is smooth, shown in trajectories. The transitions between histograms are smooth, indicating the need for implicit graph generators like GraphGAN. The model captures important graph characteristics and generalizes well for link prediction. However, GraphGAN has limitations that could be addressed in future works, such as scalability. In Sec. 4.2, it was noted that generating random walks for large graphs requires a large number of samples. One possible extension is to use a conditional generator for more even coverage. Speeding up the sampling process can be achieved by incorporating a hierarchical softmax output layer. Evaluating the realism of graphs quantitatively is essential, and developing new measures for implicit graph generative models will enhance our understanding. The current focus is on standard graph statistics in the experimental scope. In the current work, the focus is on a single connected graph, with potential applications in dealing with smaller i.i.d. graphs in fields like chemistry and biology. Exploring the impact of graph topology on GraphGAN performance is crucial. Adapting the model for attributed, k-partite, or heterogeneous networks, as well as dynamic settings with new nodes added over time, presents promising research directions. GraphGAN bridges implicit modeling and graphs, opening up new possibilities for future researchers. Our work bridges implicit modeling and graphs by using the GAN framework to generate realistic graphs through biased random walks. The generator can create sibling graphs with structural similarity to the original, controlled by defined stopping criteria. GraphGAN learns a semantic mapping from the latent space to the properties of the generated graph, allowing for smooth transitions in the output. GraphGAN demonstrates strong generalization properties in graph generation, with competitive performance in link prediction and promising results in node classification tasks. It defines key elements such as nodes, edges, and communities in a graph, and introduces concepts like node neighbors and degrees. The method involves generating graphs from node embeddings using logistic regression as a baseline strategy. The proposed strategy involves training a logistic regression model on node embeddings from node2vec for edge probabilities. First-order random walks are generated using link prediction scores, with starting nodes sampled based on degree. Subsequent nodes are sampled using the logistic regression model. This process is repeated for 16 time steps and 500K random walks to assemble an adjacency matrix. Additionally, random graphs with similar overlap to GraphGAN are generated using the configuration model by randomly rewiring a share of edges. The configuration model involves randomly selecting a share of edges and shuffling the remaining edges to create a graph with specified edge overlap. The ERGM used parameters such as edge count, density, degree correlation, deg1.5, and gwesp. Table 4 shows graph statistics used to measure properties. Graph statistics include the size of the largest connected component, exponent of the power law distribution, Gini coefficient, triangle count, wedge count, relative edge distribution entropy, and Pearson correlation of degrees of connected nodes. GraphGAN is trained with recurrent and convolutional discriminator variants. GraphGAN is trained with recurrent and convolutional discriminator variants, evaluated on the CORA-ML dataset for link prediction scores. Results show that the recurrent variant outperforms the convolutional variant in terms of average precision. Comparisons with graphs generated by DC-SBM show similarities to the ground truth graph."
}