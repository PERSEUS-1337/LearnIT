{
    "title": "Ske7V7qflQ",
    "content": "Sound correspondence patterns are essential for linguistic reconstruction, used to prove language relationships, reconstruct proto-forms, and for phylogenetic reconstruction. An automatic method for inferring sound correspondence patterns across multiple languages is presented, based on a network approach representing aligned cognate sets as nodes with edges indicating compatibility. This method can identify exceptions in sound change, such as analogy or assimilation of frequent words. The paper presents a method for automatic correspondence pattern recognition in linguistic reconstruction using a network approach. It focuses on inferring compatible correspondence sets by solving the minimum clique cover problem in graph theory. The method aims to identify core sound correspondences by excluding patterns occurring in few cognate sets. Concrete examples and tests are provided to illustrate the method's effectiveness, along with a Python library implementation. The study presents a method for automatic correspondence pattern recognition in linguistic reconstruction using a network approach. It includes a short interactive tutorial demonstrating how to use the new method. Early historical linguistic research identified structural similarities in genetically related languages due to systemic changes in their sound systems. These similarities are seen in correspondence relations between sounds in cognate words, such as English th [\u03b8] being reflected as d in German. Recognizing these regular sound correspondences is crucial for historical language comparison and genetic relationship proof. The increasing use of automatic methods in historical linguistics has led to attempts to infer regular sound correspondences across genetically related languages or integrate them into workflows for automatic cognate detection. However, most approaches only consider sound correspondences between pairs of languages. Anttila (1972, 229-263) presents the search for regular sound correspondences across multiple languages as the basic technique for historical language comparison. This involves arranging cognate word forms in a matrix to identify recurring sound patterns. The procedure is illustrated in Figure 1 with cognate sets from Sanskrit, Ancient Greek, Latin, and Gothic. Anttila (1972, 246) and additional examples show the phonetic alignment of sound sequences in historical language comparison. Regular sound correspondences for multiple languages can be detected and used for linguistic reconstruction. The patterns in the examples in Figure 1 reveal similarities, with Gothic reflexes as the main difference. In historical language comparison, regular sound correspondences for multiple languages can be detected and used for linguistic reconstruction. The patterns in the examples reveal similarities, with Gothic reflexes as the main difference, requiring decisions on assigning correspondence sets to solve missing reflexes. If the regular pattern is reflected by certain columns, predictions can be made about missing sounds in Gothic. Patterns of sound correspondences across languages can be used for linguistic reconstruction. Even if alignment columns are not identical, they can still be assigned to the same proto-sound if differences are phonetically conditioned. Gothic au [o] in pattern C reflects u when preceding h. Scholars typically reconstruct Proto-Indo-European *u for certain patterns. Regular sound correspondences across four Indo-European languages are illustrated using alignments. Lost sounds are represented by dashes, and missing words are denoted by \"\u00d8\". The paper aims to automate manual analysis in historical linguistics to handle missing reflexes in cognate sets, making it easier for linguists to group alignment columns accurately. This approach can be beneficial for computational and computer-assisted frameworks in exploring linguistic data. The paper introduces a computational method to automate the recognition of sound correspondence patterns across multiple languages, using a Python library. The method is based on the clique-cover problem in an undirected network and offers fast approximate solutions for linguistic analysis. The new method introduced in the paper is a Python library that automates the recognition of sound correspondence patterns across multiple languages. It can be applied to multilingual wordlist data and is compatible with software packages like LingPy and tools like EDICTOR BID38. The method's application and performance evaluation are detailed in Section 5, with an interactive tutorial available in the supplementary material. The tutorial also demonstrates how an extended version of the EDICTOR interface can be used to inspect correspondence patterns. The supplementary material includes code, data, and instructions for replicating all tests conducted in the study. The comparative method focuses on all languages considered, rather than specific sound correspondences for language pairs. The new method introduced in the paper automates the recognition of sound correspondence patterns across multiple languages. It can be applied to multilingual wordlist data and is compatible with software packages like LingPy and tools like EDICTOR BID38. Sound correspondences are most easily defined for pairs of languages, but become more complex as more languages are added to the sample. There is a one-to-n relationship between proto-sounds of a proto-language and regular correspondence patterns found in the data. The term sound correspondence pattern refers to regular sound correspondences across languages found in data, pointing to proto-sounds in ancestral languages. Proto-sounds can be reflected in multiple correspondence patterns, resolved by inferring phonetic contexts. Scholars emphasize that linguistic reconstruction is central to historical linguistics. The core of historical linguistics is inferring correspondence patterns, not just linguistic reconstruction. Reconstructions point to correspondences in short form. Regular correspondence patterns are clustered into groups to reflect one sound in the ancestral language. Linguists often select the most frequent correspondence patterns for reconstructions, leaving out exhaustive lists. When presenting linguistic reconstructions, scholars often focus on the most frequent correspondence patterns, leaving out irregular ones. However, a transparent reconstruction system should include all correspondence patterns, even rare ones. Instead of providing exhaustive lists, scholars typically use tables to summarize correspondence patterns, showing reflexes of proto-sounds in descendant languages. The curr_chunk discusses the limitations of sound correspondence tables in linguistic reconstruction, highlighting the lack of information on frequency and specific phonetic conditions. This makes it challenging for outsiders to evaluate the findings presented by scholars. The curr_chunk discusses the challenges in linguistic reconstruction, particularly in the absence of clear workflows for inferring sound correspondence patterns. It emphasizes that for well-studied language families like Indo-European, heuristic procedures are not necessary as major patterns have already been identified. The curr_chunk discusses the importance of identifying correspondence patterns in linguistic reconstruction, emphasizing the need for aligned data to infer these patterns. It illustrates the concept using aligned cognate sets and highlights the significance of alignment in the process. The curr_chunk emphasizes the necessity of alignment analyses in linguistic reconstruction to propose correspondence patterns. It showcases aligned cognate sets as examples and stresses the importance of identifying unique sounds in reflexes to confirm patterns. The curr_chunk discusses the concept of directly related words in linguistic reconstruction, emphasizing the importance of aligning sounds and vowels in cognate sets. It compares the development of words in different languages and highlights the use of alignment sites to derive correspondence patterns. The curr_chunk discusses how missing data in languages can affect correspondence patterns in linguistic reconstruction. It uses the example of German Dorf and Dutch dorp to illustrate the importance of aligning sounds in cognate sets. The gaps in reflexes of cognate sets are different from gaps in alignments. The curr_chunk explains the difference between gaps in reflexes of cognate sets and gaps in alignments. It introduces the use of symbols to represent missing data in correspondence patterns and alignment sites. The relation between correspondence patterns and alignment sites is illustrated in a figure. The author also introduces basic terms related to linguistic reconstruction. In this section, the author introduces basic terms and concepts for sound correspondence pattern recognition. They distinguish correspondence patterns from proto-forms and concrete alignment sites, emphasizing the abstract analysis of correspondence patterns. The handling allows for modeling sound correspondence pattern recognition as a network partitioning task. The new method for automatic correspondence pattern recognition is introduced as a network partitioning task. Alignment sites are modeled using an alignment site network to extract sound correspondences. The challenge lies in grouping alignment sites based on compatibility, which can be resolved by having reflexes for all languages in cognate sets. The new method for automatic correspondence pattern recognition introduces a network partitioning task to group alignment sites based on compatibility. Alignment sites are considered compatible if they share at least one non-gap sound and do not have conflicting sounds. Compatibility can be further weighted by counting shared sounds. In the example provided, only two sites are incompatible, A and C, due to different reflexes in Gothic. The concept of alignment site compatibility is used to model alignment sites in a network, where nodes represent sites and edges connect compatible sites. The edges can be weighted based on the number of matching sounds. A network can be created from the compatibility comparison, as illustrated in FIG3. The main challenge is deciding how to assign alignment sites to correspondence patterns. The task of identifying correspondence patterns in alignment sites can be modeled as a network partitioning task, specifically a clique partitioning task. Correspondence patterns should form a clique of compatible nodes in the network, reflecting the practice of historical language comparison. The practice of historical language comparison involves grouping alignment sites into cliques of compatible nodes, reflecting the task of identifying correspondence patterns. This process is essential for linguistic reconstruction, although determining how to partition the alignment site network into cliques remains a challenge. The problem of clique partitioning can be viewed as a minimum clique cover problem, guided by principles like Occam's razor in historical language comparison. The partitioning of alignment site networks is viewed as a minimum clique cover problem, aiming to minimize the number of cliques to which nodes are assigned. This problem is known to be NP-hard but approximate solutions like the Welsh-Powell algorithm are available. This approach is suitable for correspondence pattern recognition in historical language comparison. The curr_chunk discusses the challenges of pattern recognition in linguistic analysis, emphasizing the need for flexibility in assigning alignment sites to sound correspondence patterns. It introduces an algorithm for correspondence pattern recognition that allows for fuzzy partitions, enabling sites to be assigned to multiple patterns. This method will be detailed in the next section. The method for automatic correspondence pattern recognition involves using cognate-coded and phonetically aligned multilingual wordlists to deliver a list of correspondence patterns. The workflow includes stages such as searching for cognates and phonetically aligning words, constructing an alignment site network, and partitioning alignment sites into distinct patterns. The method involves partitioning alignment sites into distinct subsets using an algorithm for the minimum clique cover problem. Correspondence patterns are then extracted from these subsets, and alignment sites are assigned accordingly. The major contribution of the paper is providing algorithms for these stages. The method is implemented as a Python package for historical linguistics tasks. The method for automatic correspondence pattern recognition in historical linguistics involves partitioning alignment sites into subsets using an algorithm for the minimum clique cover problem. Correspondence patterns are then extracted and alignment sites are assigned accordingly. The input format generally follows LingPy's format of a tab-separated text file with specific entry types specified in the header. The method is implemented as a Python package and a tutorial with example data is provided. The method for automatic correspondence pattern recognition in historical linguistics requires specific data such as word identifier, language, concept, orthographic form, phonetic transcription, sound type, cognate set, and alignment. The format used is similar to the Cross-Linguistic Data Formats initiative. LingPy provides routines for data conversion. LingPy offers routines for automatic correspondence pattern recognition in historical linguistics, with output formats including wordlist format and tab-separated text files. The method requires specific data like word identifier, language, concept, orthographic form, phonetic transcription, cognate set, and alignment. The information can be automatically computed for columns like COGID and ALIGNMENT. The method for automatic correspondence pattern recognition in historical linguistics offered by LingPy includes various cognate detection and phonetic alignment methods. These methods, such as BID54, BID32, BID33, and BID49, have been tested and are available as a plugin for the LingPy library. LingPy offers various methods for automatic correspondence pattern recognition in historical linguistics, including cognate detection (BID49) and phonetic alignments. Users can utilize LingPy's partial cognate detection method (BID41) and two basic variants for phonetic alignments (BID33). The EDICTOR tool (BID38) can also be used to prepare data for creating, maintaining, and publishing etymological data. The method for correspondence pattern recognition in historical linguistics involves reconstructing an alignment site network, computing a minimal clique cover using a greedy algorithm, and exporting analyses into required input formats for new methods. The method for correspondence pattern recognition in historical linguistics involves reconstructing an alignment site network and computing a minimal clique cover using a greedy algorithm. This process assigns alignment sites to compatible correspondence patterns, reducing the number of fuzzy alignment sites. The method for correspondence pattern recognition in historical linguistics involves reconstructing an alignment site network and computing a minimal clique cover using a greedy algorithm. This approach is fast and can be applied to larger datasets, with the potential for extension to more sophisticated methods as more data becomes available. The clique cover algorithm consists of two steps, including sorting the data using a customized variant of the Quicksort algorithm. The customized Quicksort algorithm BID19 is used to sort patterns based on compatibility and similarity, assigning compatible patterns to the same cluster in the first pass. This rough partition quickly detects major signals in the data. The Welsh-Powell algorithm for graph coloring is then used to merge compatible partitions, starting with the largest ones. The algorithm merges compatible partitions, keeping incompatible ones in the queue. It sorts partitions based on non-missing segments and alignment site density. The density is calculated by dividing non-missing cells by total cells in the alignment site matrix. The algorithm merges compatible partitions, sorting them based on non-missing segments and alignment site density. It calculates the density by dividing non-missing cells by the total cells in the alignment site matrix. The algorithm merges compatible partitions based on non-missing segments and alignment site density, calculated by dividing non-missing cells by the total cells in the matrix. It assembles patterns and merges alignment sites to form new partitions, potentially changing the selected pattern during the merge procedure. This process may not always result in a true clique cover. The algorithm iterates multiple times to achieve a true clique cover, approximating the problem. It assembles correspondence patterns and assigns alignment sites based on compatibility, considering missing data. The algorithm iterates to compute clique cover patterns by selecting edges step by step, updating patterns with new alignment sites, and inferring a second cover. Some data points may have missing information, leading to ambiguity in assigning alignment sites to patterns. The study presents sound correspondences and patterns without predecessors in quantitative studies. Evaluation is challenging due to lack of exhaustive classical linguist discussions. Testing specific method characteristics and manual inspection of proposed patterns are suggested for evaluation. The study presents sound correspondences and patterns without predecessors in quantitative studies. Evaluation is challenging due to lack of exhaustive classical linguist discussions. Testing specific method characteristics and manual inspection of proposed patterns are suggested for evaluation. When applying the method to different datasets, general statistics and results are useful. The datasets used for testing are small and consist of closely related languages, making manual evaluation easier. Table 4 provides an overview of the datasets used for testing the new method. The curr_chunk introduces a new measure called cognate density to estimate genetic diversity in datasets. Cognate density is calculated using a formula based on the number of concepts, words, and cognate sets. High density indicates clustering in large cognate sets, while low density means isolated words. This measure helps identify strengths and weaknesses of the proposed method. The method proposed here works better on datasets with high cognate density. Phonetic alignments were computed using the SCA algorithm before applying the correspondence pattern recognition method in three different versions. The results are summarized in Table 5. The method proposed works better on datasets with high cognate density. Results in Table 5 show the number of alignment sites, correspondence patterns, unique patterns, and fuzziness across six datasets. The method successfully reduces alignment sites by assigning them to the same pattern, with a large proportion of unique patterns observed. The method proposed reduces alignment sites by assigning them to the same pattern, with a large proportion of unique patterns observed. The fuzziness of alignment sites for vowels is generally higher than for consonants, making it more challenging to establish sound correspondences among vowels. Irregular correspondence patterns may arise due to errors in data, cognate judgments, or alignments. Irregular sound change processes, analogy, and missing data can lead to errors in alignments when establishing sound correspondences among vowels. The multiple reasons for singleton correspondence patterns make it difficult to determine their exact origins without detailed data inspection. Automatic alignments and limited data availability can also contribute to irregular sound change processes. The alignments were carried out automatically, while cognate sets were assigned manually, leading to potential distortions. Manual coders may overlook partial cognacy or morphological differences. Automatic alignment methods may align parts that a human would exclude. LingPy's function for consensus computation includes a criterion to exclude alignment sites with a majority of gaps. The consensus sequences in LingPy's function exclude alignment sites with a majority of gaps, reducing singleton correspondence patterns. Manual alignment of cognate sets is crucial to avoid distortions and alignable parts should be marked. In the context of aligning cognate sets and marking affixes, experiments are conducted to analyze correspondence patterns. Three experiments are discussed, including cases of undetected borrowings in the data. In experiments analyzing correspondence patterns, a method inspired by BID8 and tested in BID35 involves seeding false borrowings among language pairs. This method transfers words from donor to recipient languages, marking them as cognates. The impact of introducing borrowings is tested using a measure of cognate set regularity derived from inferred correspondence patterns. The method involves using a measure called pattern regularity (PR) to assess the regularity of cognate sets based on alignment site density scores. If less than half of the alignment sites are deemed regular, the cognate set is considered irregular and split into independent sets. This process can result in a drop in cognate density in datasets with high irregularity. Comparing the cognate density of the original dataset with one distorted by artificial borrowings helps evaluate the impact of the PR measure. After applying the PR measure to a dataset distorted by artificial borrowings, the impact of undetected borrowings on the method was tested directly. Results in Table 7 show a drop in cognate density for most datasets when applying the PR measure, with the threshold set at 0.25 and 100 trials run for each dataset. The number of language pairs tested was intentionally high to simulate spurious borrowings, rather than intensive borrowings between only a few varieties. The same variant used for excluding gapped alignment sites was applied in these tests. After applying the PR measure to datasets with artificial borrowings, cognate density decreases for most datasets except Uralic. The low phonetic diversity of languages may explain this behavior. Undetected borrowings disrupt correspondence patterns, decreasing pattern regularity in the data. The table contrasts original density with density after applying pattern regularity measure to datasets, showing number of language pairs and borrowing events. Data can also have wrong cognate assignments, simulated by seeding erroneous words into cognate sets using LingPy's word generation method. The method utilizes a simple bigram model to generate new words with similar phonotactics. The results of the experiment are presented in Table 8, comparing pattern regularity for artificially seeded neologisms in the data. The experiment in Table 8 shows a drop in density for all datasets except Huon, with an unexplained outlier. The method correctly identifies disturbance signals in data, making it harder to find regular correspondence patterns. The predictive force of correspondence patterns is investigated in a final experiment. The experiment in Table 8 shows a drop in density for all datasets except Huon, with an unexplained outlier. Investigating the predictive force of correspondence patterns, the method can be used to predict missing data by recognizing patterns. For example, the correspondence pattern of Dutch dorp and German Dorf suggests that the English reflex would start with \"th\" if preserved. The method assigns patterns to alignment sites, allowing for the prediction of missing entries. The experiment involved excluding regular words from datasets to infer correspondence patterns for predicting missing data. The number of words excluded was determined automatically for each dataset in 100 trials. The experiment involved excluding regular words from datasets to infer correspondence patterns for predicting missing data. The number of words to be excluded was automatically derived for each dataset by selecting cognate sets and reflexes. Highly gapped sites were excluded from the analysis. The prediction rate per reflex was computed by dividing the number of correctly predicted sites by the total number of sites. The results of this experiment provide insights into the predictive force for vowels and consonants separately. The experiment involved excluding regular words from datasets to infer correspondence patterns for predicting missing data. The results in Table 9 show that the prediction based on inferred correspondence patterns does not work well, with only a small amount of missing reflexes correctly assigned. This reflects the challenges faced when working with datasets of limited size in historical linguistics. When working with small datasets in historical linguistics, predicting missing reflexes based on inferred correspondence patterns may not yield high accuracy. Vowel changes are more prone to idiosyncratic behavior than consonant changes due to the limited design space of vowels. The experiment on predictive force of inferred correspondence patterns reflects these challenges. The analysis aids in linguistic reconstruction by providing an example from the Chinese test set, showing how Middle Chinese reconstructions can be inferred from dialect patterns. The reconstructions are based on written sources and can be traced back to dental stops in Middle Chinese. The MC *d corresponds to two distinct patterns in different dialects, with some showing devoicing in contrastive outcomes. This aids in understanding proto-sounds and allows scholars to search for conditioning contexts. In pattern #197, F\u00fazh\u014du shows an unexpected sound compared to other patterns. The deviation is due to a weakening of syllable-initial sounds in non-initial syllables. This process can also be found in pattern #26, where the reflex in M\u00e9ixi\u00e0n is irregular. The example demonstrates that multiple correspondence patterns for the same proto-sound can evolve easily. Careful alignment and cognate annotation are crucial for the success of the method, as sparse data may lead to erroneous groupings. The new method presented in the study allows linguists to infer sound correspondence patterns in multi-lingual wordlists more efficiently. It can be used with tools like EDICTOR to browse correspondence patterns and identify issues in the data, making it easier to create reconstructions or address data problems. The method is integrated with the LingPy software package, enabling automated workflows for computing cognate sets, alignments, and correspondence patterns. The method presented allows for efficient inference of sound correspondence patterns in multi-lingual wordlists, enabling various approaches in computational historical linguistics and language comparison. This includes automatic linguistic reconstruction, alignment-based phylogenetic reconstruction, detection of borrowings and erroneous cognates, and prediction of missing reflexes in the data. The approach, while not perfect, provides a basis for further improvements and collaboration in the field. The supplementary material includes the Python package, a tutorial, data for analysis, code for replication, test and training data, and an expanded EDICTOR version for inspecting correspondence patterns. It has been submitted for anonymous review on the Open Science Framework and can be accessed at https://osf.io/mbzsj/?view_only=b7cbceac46da4f0ab7f7a40c2f457ada."
}