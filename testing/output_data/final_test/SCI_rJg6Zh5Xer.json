{
    "title": "rJg6Zh5Xer",
    "content": "Recent progress in hardware and methodology for training neural networks has led to the development of large models trained on abundant data, resulting in significant accuracy improvements across various NLP tasks. However, these gains come at a high cost in terms of financial resources and environmental impact due to the substantial energy consumption required for training. The paper highlights the financial and environmental costs associated with training successful neural network models for NLP and proposes actionable recommendations to reduce costs and promote equity in NLP research and practice. Recent advances in NLP research have led to impressive accuracy improvements, but at a high cost in terms of computational resources, energy consumption, and financial and environmental impacts. Training state-of-the-art models now requires specialized hardware, limiting access based on finances. Recent advances in NLP research have led to impressive accuracy improvements, but at a high cost in terms of computational resources, energy consumption, and financial and environmental impacts. Model training incurs a substantial environmental cost due to the energy required, which may not be derived from carbon-neutral sources in many locations. Cutting carbon emissions is crucial to deter escalating natural disasters, and model training likely contributes significantly to CO2 emissions. Model training and development contribute significantly to greenhouse gas emissions in NLP research. The study estimates the energy and carbon emissions from training popular NLP models and highlights the resources needed for developing new models. Recommendations include considering retraining time and hyperparameter sensitivity. The study analyzes the energy and resources required to train NLP models, emphasizing the importance of reporting retraining time and hyperparameter sensitivity. Researchers should prioritize efficient model development and equitable access to computational resources. During model training, power consumption is monitored using NVIDIA GPUs. The total power consumption is estimated by combining GPU, CPU, and DRAM consumption, then multiplied by Power Usage Effectiveness (PUE) to account for additional energy needed for infrastructure support. The total power consumption during model training is estimated by combining GPU, CPU, and DRAM consumption, then multiplied by Power Usage Effectiveness (PUE) to account for additional energy needed for infrastructure support. The U.S. Environmental Protection Agency provides average CO2 emissions data for power consumed in the U.S., which is used to convert power to estimated CO2 emissions, considering the energy sources used in the country. Comparing energy sources for China, Germany, and the United States to top cloud service providers, the U.S. breakdown is similar to Amazon Web Services, allowing for a reasonable estimate of CO2 emissions. The Transformer model BID18 is an encoder-decoder architecture known for efficient machine translation. It consists of 6 stacked layers of multi-head selfattention. The base model was trained on 8 NVIDIA P100 GPUs for 12 hours, while the big model was trained for 3.5 days. This model is also used for neural architecture search and NLP pipeline. The BID16 architecture search ran for 979M training steps, with the base model requiring 10 hours to train for 300k steps on one TPUv2 core. ELMo, based on stacked LSTMs, provides rich word representations in context and was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks. BERT, a Transformer-based model, offers contextual representations similar to ELMo. BERT and GPT-2 are advanced language models trained on large datasets for improved accuracy in tasks like question answering and language modeling. BERT was trained on 16 TPU chips for 4 days, while GPT-2 requires 1 week of training on 32 TPU v3 chips. The computational requirements of training and inference in modern neural network architectures have been studied in the computer vision community. Previous work analyzed energy use, model accuracy, power draw on GPUs, and hyperparameter tuning, but did not cover recurrent and self-attention models in NLP or extrapolate power to estimates of carbon and dollar cost of training. Analysis of hyperparameter tuning for neural network models in NLP has been conducted, with estimated costs of training models in terms of CO2 emissions and cloud compute cost provided in Table 3. The study includes lower and upper bounds for GPU and TPU resources, with TPUs excluded from power and carbon footprint analysis due to lack of public information. The study analyzed the cost and carbon emissions of training NLP models, highlighting the cost efficiency of TPUs over GPUs for certain workloads like BERT. Training models like NAS and LinguisticallyInformed Self-Attention BID17 incurred high costs and carbon emissions, with the latter serving as a case study for NLP pipelines. The NLP project involved 123 hyperparameter grid searches over 172 days, resulting in 4789 jobs. Training was done on NVIDIA Titan X and M40 GPUs, totaling 9998 days of GPU time. The estimated cost for developing and deploying the model is detailed in the report. The cost of tuning a model for a new dataset quickly becomes expensive. Standard measurements for training time and model sensitivity are needed. Academic researchers require equitable access to computation resources. Recent improvements in accuracy are often due to industry developments rather than academia. Recent improvements in state-of-the-art accuracy in NLP research are attributed to industry access to large-scale compute. Limiting this research style to industry labs stifles creativity, prohibits certain types of research based on financial resources, and promotes a \"rich get richer\" cycle of research funding. Resource-poor groups are forced to rely on cloud compute services like AWS, Google Cloud, and Microsoft Azure due to prohibitive start-up costs. It is more cost-effective for academic researchers to pool resources and build shared compute centers funded by government entities like the U.S. National Science Foundation, rather than using on-demand cloud services like AWS, Google Cloud, and Microsoft Azure. A government-funded academic compute cloud would provide long-term benefits as resources are shared across multiple projects. Researchers should prioritize computationally efficient hardware and algorithms to reduce energy consumption. A concerted effort by industry and academia is recommended to promote research on more efficient algorithms and hardware. NLP and machine learning software developers can help by providing easy-to-use APIs for hyperparameter tuning techniques like random or Bayesian search. Implementing tuning techniques for NLP models is rare due to lack of optimization for popular deep learning frameworks like PyTorch and TensorFlow. Integrating these tools into familiar workflows could significantly reduce the cost of developing and tuning NLP models."
}