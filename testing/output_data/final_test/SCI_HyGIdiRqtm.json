{
    "title": "HyGIdiRqtm",
    "content": "Neural networks can be fooled by adversarial examples, misclassified inputs with high confidence. Verification of networks helps assess vulnerability. Piecewise-linear neural networks are verified using a mixed integer program, achieving significant computational speedup. This allows verification of properties on large networks, including determining exact adversarial accuracy of an MNIST classifier. For the first time, the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l-\u221e norm \u03b5=0.1 was determined. Adversarial examples were found for 4.38% of samples, with the rest certified as robust. Various training procedures and network architectures were tested on MNIST and CIFAR-10 datasets, certifying more samples and finding more adversarial examples than previous methods. Defense methods are being developed to create more robust classifiers against adversarial attacks. Defense methods for creating robust classifiers against adversarial attacks are often evaluated using heuristic attacks like the Fast Gradient Sign Method (FGSM) or BID6's attack. However, these evaluations do not guarantee increased robustness, as many defense methods have later been found vulnerable to new attacks. Evaluating robustness in a principled manner can involve determining the minimum adversarial distortion or the adversarial test accuracy. The efficient implementation of a mixed-integer linear programming (MILP) verifier for properties of piecewise-linear feed-forward neural networks is presented. The formulation for nonlinearities and presolve algorithm minimize binary variables in the MILP problem, improving numerical conditioning. Performance optimizations in the MILP implementation make it significantly faster than SMT-based verifiers. Key contributions include demonstrating robustness despite considering the network's combinatorial nature. Our verifier can evaluate the robustness of larger neural networks efficiently by exploiting the properties of ReLUs and reducing the number of non-linearities considered. We also determine the exact adversarial accuracy for MNIST classifiers and certify more samples than the state-of-the-art. Our work certifies more samples than the state-of-the-art and finds adversarial examples across MNIST and CIFAR-10 classifiers with different architectures. Verification procedures are categorized as complete or incomplete, with incomplete verifiers reasoning over an outer approximation of the adversarial polytope. Incomplete verifiers can only certify robustness for a fraction of robust input, while complete verifiers reason over the exact adversarial polytope and can provide a definite answer to any query. Complete verifiers obtain valid adversarial examples or certificates of robustness for every input, but behave like incomplete verifiers when a time limit is set. Users can extend the time limit to answer a larger fraction of queries. Incomplete verifiers employ various techniques for evaluating network robustness. Our approach improves upon existing MILP-based approaches with a tighter formulation for non-linearities and a novel presolve algorithm that makes full use of all information available, leading to solve times several orders of magnitude faster than a na\u00efvely implemented MILP-based approach. Comparing our approach to the state-of-the-art SMT-based approach (Reluplex) on the task of finding minimum adversarial distortions shows significant advancements. Our verifier, based on a SMT approach (Reluplex), is significantly faster than existing methods, allowing verification of networks with over 100,000 units. Robust training procedures aim to minimize worst-case loss for each example, typically by estimating the worst-case loss due to bounded perturbations. Our verifier, based on a SMT approach, can verify networks with over 100,000 units by determining if a property holds for all inputs in a bounded domain. This verification problem can be expressed as solving an MILP, enabling accurate comparisons between different training procedures. The MILP framework requires P to be expressible as linear properties over a set of polyhedra C, with piecewise-linear layers in f(\u00b7). Shortcut connections in architectures like ResNet are linear, and batch normalization or dropout are linear transformations at evaluation time. Adversarial accuracy is evaluated by determining allowable perturbations of input x within G(x). The text discusses the concept of robustness of neural networks to perturbations in input data. It mentions that a network is considered robust if the predicted probability of the true label exceeds that of every other label for all perturbations. Adversarial examples are defined as inputs that do not meet these constraints. The adversarial accuracy of a network is the fraction of the test set that is robust, while the adversarial error is the complement of this accuracy. The text discusses robustness of neural networks to perturbations in input data. Adversarial error is the complement of adversarial accuracy. Robust training procedures aim to be robust to perturbations with bounded l \u221e norm. Minimum adversarial distortion is evaluated using a distance metric. Targeted attacks can generate adversarial examples classified in a set of target labels. The text discusses distance metrics for generating adversarial examples, including l1, l2, and l\u221e norms. Formulations of ReLU and maximum functions are crucial for MILP solver performance. ReLU units can be stably inactive, stably active, or unstable, with corresponding indicator variables. The text discusses formulating the maximum function using linear and integer constraints, introducing indicator decision variables for input variables, and the importance of determining tight bounds for problem tractability. The text discusses improving solve times by proving stability of ReLU phases and using INTERVAL ARITHMETIC (IA) and LINEAR PROGRAMMING (LP) to determine bounds. There is a tradeoff between build times for tighter bounds and solve times for the main MILP problem. Knowledge of non-linearities helps reduce average build times without compromising problem strength. Our network reduces build times by efficiently determining bounds for problem formulations, focusing on piecewise-linear non-linearities. We use a progressive bounds tightening approach, refining bounds only when necessary for improving the problem formulation. Pseudocode for ReLU and maximum function bounds determination is provided. Our network efficiently determines bounds for problem formulations using a progressive bounds tightening approach. Experiments are conducted on classifiers for the MNIST and CIFAR-10 datasets, with various feed-forward network architectures such as MLP and CNN. The network architectures used in the experiments include CNN B with four convolutional layers and RES with nine convolutional layers. Training methods involve regular loss function training and robust training using different methods like LP, SDP, and Adv. Experiments were conducted on a system with 8 CPUs@2.20 GHz and 8GB of RAM. The experiments were conducted on a system with 8 CPUs@2.20 GHz and 8GB of RAM. The MILP approach implemented three key optimizations: progressive tightening, using information from the restricted input domain G(x), and asymmetric bounds in the ReLU formulation. A comparison with four other MILP-based verifiers showed differences in performance, with ablation tests conducted to highlight these variances. The MILP approach implemented three key optimizations: progressive tightening, using information from the restricted input domain G(x), and asymmetric bounds in the ReLU formulation. Experiments on an MNIST classifier showed that each optimization is critical to the verifier's performance, with a runtime several orders of magnitude faster than verifiers not using asymmetric bounds. Our verifier utilizes progressive tightening and asymmetric bounds in the ReLU formulation, resulting in a runtime several orders of magnitude faster than other verifiers. Comparison with other verifiers on finding minimum targeted adversarial distortions for MNIST test samples shows significant speed improvements. Adversarial Distortions are compared with incomplete verifiers, showing significant gaps between the best lower bounds and the true minimum distortion. The gap is especially pronounced for deeper networks, with the best certified lower bound being less than half of the true minimum distortion under the l \u221e norm. This gap increases for deeper networks, as shown in Table 2. Trained classifiers on MNIST and CIFAR-10 datasets with robust training procedures. Tested error and estimated adversarial error. Lower bounds proven by providing adversarial examples for non-robust input. Upper bounds proven by providing certificates of robustness. Improved on both lower and upper bounds for every network tested. Our method successfully finds adversarial examples for every test sample within norm bounds. PGD may miss valid adversarial examples, especially for larger values of . The minimum adversarial distortion affects PGD's likelihood of missing adversarial examples. We improve on upper bounds for adversarial error, even with explicitly optimized worst-case loss during training. Our method scales well to the CIFAR-10 dataset and larger LP d -RES. Our method successfully finds adversarial examples for every test sample within norm bounds, improving on both lower and upper bounds for adversarial error. It scales well to the CIFAR-10 dataset and larger LP d -RES network, with verification time comparable to training time on a single GPU. The paper presents an efficient verifier for piecewise-linear neural networks, proving stability of ReLUs and eliminating labels from consideration. Solve times are correlated with the number of unstable ReLUs and labels that cannot be eliminated. The paper introduces an efficient verifier for piecewise-linear neural networks, focusing on evaluating networks' robustness to perturbations. The verifier can handle new classes of perturbations and suggests improvements for neural network verification, including combining optimizations in solving MILPs. The paper introduces an efficient verifier for piecewise-linear neural networks, focusing on evaluating networks' robustness to perturbations. Relaxing integrality constraints on a leads to (x, y) being restricted to a convex hull. Formulation for the maximum function is improved by adding constraints based on bounds on z. Equations ensure exactly one a i is 1, simplifying the problem. Introducing auxiliary variables helps bound absolute values and l \u221e norm. Our framework for determining bounds on decision variables involves viewing the neural network as a computation graph G, where directed edges represent function input to output and vertices represent variables. The computation graph starts with defined bounds on input variables and is augmented with bounds on intermediate variables. The graph is acyclic for feed-forward networks and can be expressed as an MILP for piecewise-linear networks. Integer constraints are added as needed. The process involves determining bounds on decision variables in a computation graph by adding integer constraints for non-linear relationships. The focus is on computing upper bounds for variables, with all necessary information contained in the subtree rooted at the variable. To optimize computation time, edges and vertices can be pruned by assuming independence of selected variables with existing bounds. This results in a smaller subgraph for more efficient calculations. To optimize computation time, edges and vertices can be pruned by assuming independence of selected variables with existing bounds. This results in a smaller subgraph for more efficient calculations. In-edges to vertices in V I are eliminated, and variables without children are removed, resulting in the smaller computation graph G v,V I. Relaxing integer constraints in the MILP M v reduces computation time. The objective value returned by maximizing the value of v over M v may not be the optimal upper bound, but is guaranteed to be a valid bound. FULL considers the full subtree G v without relaxing any integer constraints. The upper and lower bound on v is determined by maximizing and minimizing the value of v in M v respectively. To optimize computation time, edges and vertices can be pruned by assuming independence of selected variables with existing bounds. In-edges to vertices in V I are eliminated, and variables without children are removed, resulting in the smaller computation graph G v,V I. Relaxing integer constraints in the MILP M v reduces computation time. The objective value returned by maximizing the value of v over M v may not be the optimal upper bound, but is guaranteed to be a valid bound. FULL is a method that considers the full subtree G v without relaxing any integer constraints. It can find optimal bounds on the value of a single variable v, but may be inefficient for deeper layers due to exponential solve times. However, it is possible to terminate solves early and still obtain useful bounds by setting the objective of M v to maximize the value of v. This allows for progressively better certified upper bounds on the maximum value of v to be obtained during the solve process. LP is a linear program that relaxes integer constraints for more efficient optimization. IA selects parents for variables based on previous layer bounds, using interval arithmetic. GETBOUNDSFORMAX tightens bounds for constraints like y = max(xs), stopping when a variable's max value is lower than another's min value. GETBOUNDSFORMAX returns a tuple with elements in xs that can still have the maximum value, along with upper and lower bounds. The procedure involves tightening bounds progressively for xs. The source of weights for networks presented in the paper is also provided. The MNIST classifiers MLP-2\u00d7, -LP d -CNN B, -LP d -CNN A, and -Adv-CNN A are designed for robustness to perturbations with different l \u221e norm-bounds. They can be found at specific GitHub repositories and were trained using various techniques such as adversarial examples generated by PGD attacks. MNIST and CIFAR-10 classifiers for robustness to perturbations with different l \u221e norm-bounds were trained using specific parameters and techniques. MILP models were constructed in Julia using JuMP. The MILP models were constructed in Julia using JuMP and solved with Gurobi 7.5.2. Experiments were conducted on a virtual machine with 8 virtual CPUs and 8GB of RAM. Comparisons with open-source solvers like Cbc and GLPK showed that performance varied, with Gurobi outperforming the others. Despite some timeouts, the verifier still provided improved bounds compared to existing methods. The MILP models were solved using an open-source solver, improving on existing bounds. Additional solve statistics on nodes explored were provided, showing robustness for over 95% of samples. Information on verification time determinants for networks was also presented. The LP d-CNN A networks were trained for robustness to attacks with l \u221e norm-bound. The success rate of PGD declines as the minimum adversarial distortion increases, and success rates decline for networks with larger even at the same distortion. PGD succeeds if the starting point is in the basin of attraction of an adversarial example. The success rate of PGD is inversely related to the magnitude of the minimum adversarial distortion. The study investigates the relationship between the success rate of PGD in finding adversarial examples and the magnitude of the minimum adversarial distortion. Instead of re-running PGD with multiple starting points, the samples were binned based on their distortion level to estimate success rates. PGD is highly successful in finding adversarial examples when the minimum adversarial distortion is small. The success rate of PGD declines significantly as the minimum adversarial distortion, \u03b4, approaches zero. PGD is more successful at attacking networks trained to be robust to smaller attacks compared to larger attacks. This suggests a way to generate networks that appear robust but are actually vulnerable to bounded attacks by training them to maximize the number of adversarial examples with minimum distortion close to zero. When verifying the robustness of SDP d-MLP A, a significant proportion of kernel weights were close to zero. Setting tiny weights in kernel to zero can reduce verification time by reducing MILP formulation size and addressing numerical issues. A sparsification algorithm was used to create sparse versions of the network, with results summarized in TAB9. The algorithm sets a fraction of weights in each fully-connected layer to zero based on their absolute value, while maintaining the l1 norm of the kernel. Sparsification of weights in a neural network can improve verification time and reduce numerical issues. Results in TAB9 show that setting more kernel weights to zero leads to a significant improvement in mean time and fraction timed out. Sparsification also enhances the upper bound on adversarial error, with a noticeable impact when f1 exceeds 0.8. This approach demonstrates that starting with a robust network, simple sparsification can lower the upper bound on adversarial error significantly. Adopting a principled sparsification approach can improve verifiability without compromising on adversarial error. Robust networks must balance local robustness with global expressiveness. Different robust training approaches lead to a significant fraction of stable ReLUs in the network. The need for networks to be robust to perturbations in G drives more ReLUs to be provably stable. The number of stable ReLUs changes as the maximum perturbation norm varies. LPd-CNN A is sensitive to the 0.1 threshold, showing a sharp increase in unstable ReLUs. LPd-CNN A is highly sensitive to the 0.1 threshold, with a significant increase in the number of ReLUs that cannot be proven stable when the maximum perturbation norm exceeds 0.102. This sensitivity is not observed in the other two networks."
}