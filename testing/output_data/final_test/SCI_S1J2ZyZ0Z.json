{
    "title": "S1J2ZyZ0Z",
    "content": "Questions that require counting objects in images are a challenge in visual question answering (VQA). Traditional VQA approaches classify answers based on fixed representations or sum fractional counts from image sections. In contrast, our model treats counting as a sequential decision process, making discrete choices of what to count. It selects objects sequentially, learning interactions between them. Our approach provides intuitive and interpretable output, outperforming state-of-the-art VQA architectures in counting metrics. Counting-based questions in visual question answering (VQA) have seen limited improvement despite advancements in the field. The typical approach involves using convolutional neural networks (CNN) to represent visual input and classifying answers based on attention-weighted image features. However, this approach may obscure the notion of distinct elements in counting tasks. A new method is proposed to rethink counting in VQA by embracing the discrete nature of the task. Our approach, inspired by recent work, represents images as a set of distinct objects identified by object detection. We experiment with counting systems that utilize the relationships between these objects and introduce the Interpretable Reinforcement Learning Counter (IRLC) for learning to enumerate relevant objects in the scene. This iterative method not only returns a count but also the objects supporting the answer. The Interpretable Reinforcement Learning Counter (IRLC) is a model that uses an iterative approach to count objects in a scene. It selects objects to add to the count and adjusts the priority of unselected objects based on their configuration with the selected objects. The model is trained using reinforcement learning and achieves higher accuracy and lower count error compared to the state of the art model for Visual Question Answering (VQA). The model's grounded counts are compared to the attentional focus of the baseline model to demonstrate interpretability gained. The state of the art baseline focuses on interpretability gained through their approach to counting objects in images. Different methods for counting, such as using density maps or object detection, have been explored. The limitation of tuning redundancy reduction steps for counting is overcome by allowing flexible, question-specific interactions during the counting process. In contrast to previous studies on counting objects in images, this work focuses on counting during visual question answering, where the criteria for counting can vary and be complex. This sets it apart from approaches that count objects solely from the image. Visual question answering involves retrieving question-specific information from an image, requiring complex scene understanding and flexible reasoning. Recent progress has focused on datasets like \"VQA,\" where counting questions make up about 11% of the data. This study aims to learn counting directly from question/answer pairs, overcoming limitations of previous models that relied on object detection labels for training. Prior work on VQA includes using a \"bag of words\" embedding and spatially-pooled CNN outputs to classify answers. Recent approaches focus on representing images at a finer granularity using attention to focus on relevant regions. The winning submission for the VQA 2017 challenge used object detection as a key design choice. Interpretable VQA aims to ground model variables in concepts explicitly, making neural reasoning more transparent. Neural Module Networks gain interpretability by grounding the reasoning process in defined concepts. The concept of interpretable VQA has garnered recent interest. Interpretable VQA, focusing on counting, has gained recent interest. An intuitive approach shows improved performance and interpretability over the state of the art. Progress in VQA has been mainly on datasets like VQA 2.0 and Visual Genome, which include a large number of questions and images from COCO. In order to evaluate counting specifically, a subset of QA pairs called HowMany-QA was defined. The inclusion criteria filter QA pairs where the question asks for a count, requiring specific phrases and a ground-truth answer between 0 to 20. The original VQA 2.0 train set includes roughly 444K QA pairs, with 57,606 labeled as having counting questions. The HowMany-QA subset focuses on counting questions, excluding non-counting questions like time, general numbers, ballparking, and reading numbers from images. Official test data annotations are not available, so the validation data is divided into development and test sets. 5,000 QA pairs are selected as test data, while 17,714 pairs are used for development. In this work, the HowMany-QA training data is augmented with QA pairs from Visual Genome. Models are trained and evaluated on HowMany-QA for counting in visual question answering. Model interpretability is explored by experimenting with question-guided counts grounded in object proposals. Three models are compared without substantial modifications to existing counting approaches. Our approach, inspired by Chattopadhyay et al. (2017), compares three models in visual question answering. The models use object detection to produce a count and encode the question to compare with detected objects. The Faster R-CNN architecture is used to infer objects in the image, proposing regions and encoding bounding boxes and object features. The approach compares three models in visual question answering using object detection to produce a count and encode the question for comparison with detected objects. Object proposals provide rich representations for each image in the dataset, shared across different QA models. Each architecture encodes the question and compares it to detected objects using a scoring function. The scoring function f S is implemented as a layer of Gated Tanh Units (GTU) for caption grounding, supervised by region captions from Visual Genome. Caption grounding aims to identify the object described in a given caption using LSTM encoding and the scoring function f S. The weights of the scoring function are tied for counting and caption grounding. Experiments include results where caption grounding is ignored, and the model focuses on learning how to count by learning what to count. Our proposed model aims to learn how to count by understanding what to count in a scene. The model represents the probability of selecting actions and how they impact subsequent choices. Object scores are projected to logits to determine the likelihood of objects being counted. Interaction terms are computed to update the logits based on the question, object vectors, and coordinates. The proposed model learns to count by understanding what to count in a scene. It uses object scores projected to logits to determine the likelihood of objects being counted, updating based on question, object vectors, and coordinates. The counting sequence selects actions greedily, with a terminal action defined by a learnable scalar. Each object is counted once, with the count defined as the timestep when the terminal action is selected. This approach is similar to Non-Maximal Suppression in object detection. The proposed model learns to count in a scene by understanding what to count based on object scores projected to logits. It uses Reinforcement Learning techniques to generate a count, with rewards calculated using Self-Critical Sequence training. The approach is less rigid than Non-Maximal Suppression in object detection, allowing for interactions between similar and overlapping objects. The model learns to count in a scene by understanding what to count based on object scores projected to logits. It includes a counting loss, auxiliary objectives for learning, and a weighted sum of losses during training. The approach is less rigid than Non-Maximal Suppression in object detection, allowing for interactions between similar and overlapping objects. The model trains to count objects in a scene by projecting score vectors to scalar values and applying a sigmoid nonlinearity. The total count is the sum of fractional count values for each object. The model is trained by minimizing the Huber loss between predicted and ground truth counts. Evaluation involves rounding the estimated count to the nearest integer, with a maximum limit of 20. Another baseline approach, UpDown, is implemented for comparison, representing the current state of the art for VQA 2.0. The current state of the art for VQA 2.0 involves a two-stage approach using question-based attention over image regions to classify answers. Training involves cross entropy loss with ground-truth counts, and evaluation is based on accuracy and error metrics. The study evaluates VQA models using accuracy and root-mean-squared-error (RMSE) metrics. IRLC achieves the highest accuracy and lowest RMSE on the test set, while SoftCount performs well in RMSE despite lower accuracy. The models are compared to non-visual baselines for count prediction. IRLC is more accurate and less prone to small errors compared to UpDown, which improves accuracy at the cost of RMSE. Performance is better for common subjects during training, with IRLC showing substantial accuracy improvements without sacrificing overall RMSE. The performance improvements of IRLC over UpDown persist across all development data groupings. A novel analysis is introduced to measure how well counted objects match the question's subject, with a metric indicating relevance to the question's category. This analysis is conducted for each of the 80 COCO categories using the HowMany-QA development set images. The grounding quality of SoftCount and IRLC is compared using the HowMany-QA development set images. IRLC consistently grounds its counts in objects more relevant to the question than SoftCount. A paired t-test shows this difference is statistically significant. Failure cases with common and rare subjects are shown, highlighting IRLC's interpretability in predicting failure modes. IRLC demonstrates the ability to predict failure modes by identifying objects in the scene for counting. Failure cases show difficulties with rare subjects and referring phrases, leading to misidentifications. Grounded counts highlight these failures, unlike UpDown which lacks clear attention patterns. Both models exhibit similar deficits in counting accuracy. Our approach to counting in visual question answering involves using RL to make binary decisions about object enumeration in a scene. We compare our model with two baselines, control for variations in visual representations, and achieve state-of-the-art results in evaluation metrics. The Appendix provides visualizations and comparisons of model output, demonstrating the effectiveness of our interpretable approach. Our approach achieves state of the art results for evaluation metrics by identifying objects contributing to each count, improving performance and interpretability. Example outputs from different models are shown in Figure 8, illustrating the counting sequence termination at t = 3. The counting sequence terminates at t = 3, with the correct answer being 3. Caption grounding involves identifying objects described in a caption using LSTM encoding and a scoring function. Subtle failures are revealed with grounded counts. The scoring function in caption grounding involves encoding object embeddings and relevance scores to estimate the probability that a caption describes a specific object proposal. Training data is created by assigning captions to detected object proposals based on intersection over union calculations. Only four images in a batch are used for caption grounding during training. The caption grounding process involves assigning captions to detected object proposals based on intersection over union calculations. The grounding probability for each caption is computed, and the training loss is weighted by 0.1 relative to the counting loss. Different counting models use the same basic architecture but vary in the hidden size of the scoring function. Word embeddings are initialized from GloVe BID18, and questions are encoded with an LSTM of hidden size 1024. SoftCount and UpDown models use a hidden size of 512, while IRLC uses a hidden size of 2048. When training different models for caption grounding, SoftCount and UpDown use a hidden size of 512, while IRLC uses a hidden size of 2048. Training optimization includes using Adam BID15 for counting, with varying learning rates and decay strategies for each model. IRLC applies a sampling procedure 5 times per question and averages the losses, with penalty weights yielding the best accuracy. During the hyperparameter search, a grid search was performed to optimize the weights of auxiliary losses for training IRLC. The entropy penalty helps balance model exploration, while the interaction penalty prevents degenerate counting strategies. Results suggest that these auxiliary losses improve performance but can be unhelpful if given too much weight. IRLC outperforms baseline models across various settings. Additionally, data augmentation with Visual Genome enhances performance on the HowMany-QA test set. Performance benefits from additional training data from Visual Genome QA, with models showing increased accuracy and decreased RMSE. IRLC demonstrates the most robust performance when training data is excluded. The ordinal nature of UpDown output is discussed, highlighting differences in training objectives compared to SoftCount and IRLC. Despite its non-ordinal training objective, UpDown's output count probabilities are examined to determine if the model learns an ordinal representation. The model learns an ordinal representation despite its non-ordinal training objective, as shown in Figure 12. For estimated counts less than 5, the second-most probable count is often close to the most probable count. However, for larger estimated counts, the probability distribution is less smooth, indicating a failure to generalize ordinality for higher count values. The VQA dataset includes annotations from ten human reviewers per question to calculate accuracy. Answers are scored as correct if at least 3 humans agree. RMSE quantifies the deviation between model count and ground truth. The evaluation metric RMSE measures error in predicting counts, with lower values indicating better performance. A new method for assessing the relevance of counted objects to the object type is introduced, utilizing GloVe embeddings for semantic similarity computation. This evaluation leverages ground truth labels from the COCO dataset, assigning categories to object proposals for analysis. The method involves assigning object proposals to COCO categories based on IoU values. Each proposal is assigned a category, and questions are generated for each category present in the image. The count for each category is the sum of inferred count values for object proposals. Weighted sum of semantic similarity between assigned categories and question category is computed using count values. The method assigns object proposals to COCO categories based on IoU values. Each proposal is assigned a category, and questions are generated for each category in the image. The final metric is computed for each COCO category by accumulating results over all images containing a label for that category and normalizing by the net count to get an average, indicating the relevance of counted objects to the question subject."
}