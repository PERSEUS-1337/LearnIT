{
    "title": "BkxmKgHtwH",
    "content": "Attacks on natural language models are categorized using a taxonomy of constraints to evaluate their success. Two state-of-the-art attacks claiming to preserve semantics and syntax show weak enforcement of constraints. Grammar checkers detect errors in many adversarial examples, and human studies reveal divergence in semantic meaning or non-human-like writing. Standardized evaluation of attacks with shared constraints is needed. Advances in deep learning have led to impressive performance on tasks, but models are vulnerable to adversarial examples. Research has focused on applying adversarial examples to text in natural language processing tasks. Standardized evaluation of attacks with shared constraints is recommended to determine the trade-off between attack quality and success. In recent literature, there are varying definitions of adversarial examples in natural language processing. It is crucial to have standardized evaluation methods with shared constraints to compare attack success rates effectively. In this paper, a taxonomy of constraints specific to adversarial examples in natural language is introduced, building on previous work by Gilmer et al. (2018). The study provides a comprehensive framework for categorizing and evaluating attack constraints, proposing standardized evaluation methods. Evaluation of synonym-substitution based attacks by Jin et al. (2019) and Alzantot et al. (2018) reveals additional grammatical errors and changes in meaning, questioning the effectiveness of these attacks. The study introduces a taxonomy of constraints for adversarial examples in natural language, challenging the effectiveness of synonym-based attacks. It emphasizes the importance of standardized human evaluations to determine the true threshold value for measuring semantic similarity. The paper quantitatively disproves claims that state-of-the-art synonym-based substitutions preserve meaning. The study disproves claims that synonym-based substitutions preserve semantics and grammatical correctness. It highlights the sensitivity of attack success rate to changes in semantic similarity thresholds and the need for standardized human evaluation studies. Adversarial examples in deep neural networks expose issues with classification accuracy and the black-box nature of predictions, emphasizing the importance of studying them for building secure applications. The search for adversarial examples in deep learning involves optimizing prediction changes while minimizing input changes. Attacks on natural language models include character substitution methods using deliberately misspelled words. These attacks aim to deceive classification models by altering input text. Character replacements are used to change words in a way that the model doesn't recognize, but can be detected with spellchecking software. Attacks by Word Insertion or Removal involve determining important words and adding or removing them. Some studies point out flaws in natural language models but do not create adversarial examples that retain the original input's semantics. Recent work has focused on defining adversarial examples in natural language as sequences that fool the model while retaining the meaning of the original input. Paraphrasing and synonym substitution are common techniques used to generate adversarial examples, but they often struggle with maintaining grammar and fooling the model effectively. Recent studies have developed easier ways to generate adversarial examples in natural language by replacing words with synonyms or generating word-level swaps. These techniques aim to fool sentiment analysis and textual entailment models by creating semantically and syntactically similar examples. The TextFooler system was proposed to attack various DNN models, including BERT, by identifying the most effective word replacements. The method involves identifying important words in the input sequence and replacing them with synonyms to alter model output. Counter-fitted word embeddings are used for selecting replacement words, and the Universal Sentence Encoder measures similarity scores. Interval Bound Propagation (IBP) is used to train models resistant to synonym attacks, ensuring similar prediction scores for sentences with synonym swaps. IBP is currently feasible for simple models like feedforward networks, CNNs, and LSTMs with few layers. When constructing adversarial examples, defining the space of inputs available to the attacker is crucial. Successful adversarial examples fool the model, but constraints are often imposed to restrict the outputs an attacker can produce. Various frameworks and constraints have been proposed to evaluate attacks on machine learning models, focusing on different aspects such as meaning preservation and sequence-to-sequence models. Adversarial examples aim to induce misclassification by restricting outputs an attacker can produce. Text perturbations differ from images as they are never indistinguishable, requiring clear constraints to define attack scenarios. Different sets of constraints are needed for various use cases, emphasizing the importance of categorizing attack spaces for adversarial inputs. In the context of adversarial examples in natural language, a taxonomy of constraints is proposed to evaluate attacks. Three categories of semantics-related constraints are defined: morphologically-preserving, semantics-preserving, and semantics-constrained. Additional constraints include grammatical correctness and human-like writing. The attacker may change the semantics of the input as long as the changed sentences read the same to a human. Morphological perturbations can be used to transmit a message by adding or deleting characters. A perturbation is morphology-preserving if the morphological distance between two strings is within a certain threshold. The attacker can modify input sequences while preserving semantics, aiming to trick plagiarism detection software. A perturbation is semantics-preserving if human labelers agree that the meaning remains unchanged. The score d sem (x, x adv) is based on human ratings of semantic preservation on a Likert scale. Semantics-preserving perturbations should be evaluated by humans or through machine assessment of semantic similarity. Attackers can manipulate input sequences to convey specific meanings without a starting input sequence. Semantics-constrained input can be used to deceive spam filters. Semantics-constrained input can be used to deceive spam filters by creating sequences that evade detection. Evaluation of semantics-constrained input is challenging as there is no starting point for comparison, requiring human judgment. The attacker must ensure the input is grammatically valid under the syntactic constraint. Semantics-preserving perturbations can be made with or without syntactic constraints. For example, a student altering a copied assignment must maintain meaning without introducing grammar errors to avoid detection, while an attacker distributing a PDF online only needs to retain content, regardless of grammar errors. Semantics-constrained input can be made with or without syntactic constraints. Adversarial examples may involve colluding with attackers to evade detection, such as posting offensive content with intentional grammar errors. Evaluating grammatical constraints in documents requires measuring the amount of errors present. The grammatical constraint in adversarial examples involves introducing a specific amount of errors, evaluated using tools like LanguageTool. The non-suspicious constraint requires the adversarial example to appear human-written, even if it contains grammar errors. The non-suspicious constraint in adversarial examples involves ensuring that the modified text appears human-written, even if it contains errors. Evaluation of this constraint requires human judgment through a method where individuals determine if a sequence is real, computer-altered, or computer-generated. The non-suspicious constraint for adversarial examples requires modified text to appear human-written, with a smaller value enforcing stricter standards. Evaluation of attack techniques by Alzantot et al. (2018) and Jin et al. (2019) on LSTM and BERT models revealed syntax errors and semantic changes in the generated examples. Jin et al. (2019) and Alzantot et al. (2018) evaluated adversarial examples on classification tasks using different methods. They tested the examples on sentiment classification datasets and assessed syntax, semantics preservation, and non-suspicion. LanguageTool was used to check for syntactic constraints in the generated examples. The study evaluated adversarial examples on classification tasks using different methods and LanguageTool was used to check for syntactic constraints in the generated examples. LanguageTool detected more grammatical errors in adversarial examples compared to original content, indicating a clear linear relationship between the number of words changed and the number of errors induced. This suggests that adversarial examples contain strange phrasings and unnatural errors that a human would almost never make. In a study on adversarial examples, human judgement is crucial to confirm semantic similarity evaluations. Previous studies used a single scale for rating similarity, which may not accurately measure semantic similarity. A survey was conducted to assess human judgement on adversarial examples, involving workers from Amazon Mechanical Turk to label data points across different datasets. The study on adversarial examples involved workers from Amazon Mechanical Turk to label data points across datasets. A survey was conducted to evaluate whether perturbations were semantics-preserving, with users rating the changes on a scale. Results showed users generally rated around 3 (\"Not Sure\"), indicating many examples were not semantics-preserving. The study also noted acquiescence bias, where labelers are more likely to agree than disagree. The study on adversarial examples found that inverting the question about preserving meaning resulted in examples being rated as less semantics-preserving. Future human evaluation studies should consider this bias. Additionally, the study investigated whether adversarial examples were non-suspicious by presenting them to humans for evaluation. A consistent method would produce examples that appear human-written, with human labelers averaging 50% accuracy in discerning perturbed inputs. The study evaluated adversarial examples by presenting them to humans for evaluation. Humans achieved 69.2% accuracy in discerning perturbed inputs. Workers identified computer-altered examples with 75.4% precision. Some perturbed examples are non-suspicious, while others are identified with high precision. Synonym substitution attacks consider sentences with a single word difference to have the same meaning if the word is a close synonym. Different attacks use varying thresholds for this metric, with researchers choosing the value at their discretion. The choice of threshold directly impacts the success of the attack. Jin et al. (2019) also used a distance metric based on cosine similarity between embeddings to determine if a synonym swap preserves semantic similarity. The success of synonym substitution attacks depends on the threshold for defining synonyms and the distance metric used to measure semantic similarity. Automated evaluation tools may not perfectly distinguish between paraphrases and non-paraphrases, but there are values that closely correlate with human judgement. Without standardized human studies, generated adversarial examples may not be reliable. The success of synonym substitution attacks depends on the threshold for defining synonyms and the distance metric used to measure semantic similarity. Standardized terminology and evaluation metrics will make it easier for defenders to determine which attacks they must protect themselves from-and how. Future work may expand the hierarchy to categorize and evaluate different attack constraints in natural language. BERT's robustness to synonym attacks is still uncertain, and future research will determine the prevalence of adversarial examples in paraphrases. Past research has categorized NLP attacks based on constraints like meaning, syntax, and word manipulation. The curr_chunk discusses the use of human studies in experiments, utilizing labor from Amazon Mechanical Turk for labeling tasks. Workers with \"Masters\" status were allowed to complete tasks, with each task estimated to take 15 seconds and workers paid $.05 per label. Results from a Mechanical Turk survey on distinguishing real vs. fake content are presented in Section 4.3. In Section 4.3, results from a Mechanical Turk survey on distinguishing real vs. fake content are presented using the Movie Review dataset. The survey asked users to determine if text samples were real or computer-altered. Task 2 involved judging semantic similarity or dissimilarity between text sequences, with a custom script highlighting differences. Participants were asked to compare two short English text pieces and determine if they meant the same or different things. \"Phrasing matters in distinguishing the meaning between sentences. Mechanical Turk uses pre-designed questionnaire interfaces, like \"Semantic Similarity,\" to assess sentence pairs. Cognitive biases in Likert scale responses were observed in Task 2 surveys. Labelers on Mechanical Turk tend to select \"Agree\" regardless of question formulation changes.\" Labelers on Mechanical Turk tend to select \"Agree\" regardless of the question, showing acquiescence bias. 55.9% of responses were simply \"Agree\", indicating a need to filter out bad labels in future surveys. The table displays error messages for grammatical rule violations, with examples like code PRP RB NO VB being more common in adversarial samples. Specific errors like using a past participle without a required verb are highlighted."
}