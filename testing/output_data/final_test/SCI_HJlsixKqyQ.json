{
    "title": "HJlsixKqyQ",
    "content": "In supervised learning problems, especially with social data, human interpretations can vary widely. Machine learning often simplifies multiple human opinions into a single \"ground truth\" label, masking the diverse perspectives present in the data. The challenge lies in gathering enough human labels to train machines to predict representative distributions accurately. One proposed solution is aggregating label distributions across individuals and data items to maximize human resources. This paper explores label aggregation in subjective domains, where different annotators may disagree. It highlights the importance of careful label aggregation methods in reducing the number of samples needed for representative distributions. Examples like Beauty.ai's beauty contest and Microsoft's Twitter-based learning agent, BID0 Tay, illustrate the consequences of ignoring diversity in machine learning. The Twitter-based learning agent BID0 Tay, created by Microsoft, falsely claimed the Holocaust was fabricated due to biased training. ProPublica found that Northpointe's risk assessment software recommended longer sentences for African-American men. Research on label aggregation in subjective domains suggests the importance of accurate methods to reduce sample sizes for representative distributions. Our research suggests that using a larger sample or a mix of cluster samples can better represent the population distribution of beliefs about data items, improving predictive supervised learning. Learning belief distributions instead of single labels poses challenges, requiring more data items and labels. Our method minimizes the number of labels needed to predict socially representative label distributions. The research suggests using a larger sample or mix of cluster samples to represent population beliefs about data items for improved predictive supervised learning. The method aims to minimize the number of labels needed by learning representative label distributions through unsupervised clustering algorithms. The research proposes using clustering algorithms to reduce the number of labels needed for predictive supervised learning. It acknowledges bias in information reduction processes and emphasizes the importance of human involvement in machine learning practices. The experimental workflow involves obtaining crowdsourced labels for raw data, trying various aggregation strategies, and testing how each method affects machine learning prediction accuracy. There are two testing phases: one for aggregation method fit to data and another for supervised learning algorithm performance. The study focused on the accuracy of machine learning prediction through two testing phases: one for aggregation strategy fit to data and one for machine learning performance. Extensive experiments were conducted on publicly available datasets, with a focus on two sets due to their representative performance. The research was conducted after consulting with the institutional review board to ensure compliance with guidelines. The study ensured privacy of human-generated data, replaced mentions and URLs in Twitter data, and obtained job-themed tweets for human annotations from crowdsourcing platforms. Basic properties of label sets were also provided. The study focused on privacy of human-generated data and obtained job-themed tweets for annotations from crowdsourcing platforms. Basic properties of label sets were provided, including divergence measures for estimating uncertainty. Six distinct label sets were created for different platforms and questions, with additional sets combining labels from both platforms. Another data set of 2000 tweets filtered for suicide-related discourse was also obtained. The study obtained a data set of 2000 tweets filtered for suicide-related discourse. Labels were provided by CrowdFlower workers and suicide prevention experts, with each tweet having up to 7 labels. Two different train/dev/test splits were used for the datasets. The study collected a data set of 2000 tweets on suicide-related topics, labeled by CrowdFlower workers and experts. Each tweet could have up to 7 labels. The test set consisted of 50 items, with 50 additional labels obtained from new crowdworkers for each item. Aggregation involved clustering and reduction, with majority voting used when annotators disagreed on the best label for a tweet. The curr_chunk discusses a method for predicting population distributions over label choices by using a d-dimensional vector to represent the distribution of labels for each data item. It treats each empirical label distribution as a frequentist sample of the true distribution of beliefs. The curr_chunk discusses clustering strategies to improve prediction accuracy by associating data items with probability distributions over clusters, revealing true label distributions. This method aims to replace empirical labels with cluster-based predictions. The curr_chunk discusses five clustering strategies: MMM, GMM, DS, LDA, and a custom MMM. LDA is used to cluster both empirical labels and bag-of-word representations. The central hypothesis is that label distributions in subjective domains cluster around a finite number of true label distributions, with MMM being the best model. The curr_chunk discusses the distribution of cluster items in DS and LDA models, which can provide latent classes and likelihood estimates. DS is effective in collaborative filtering settings, incorporating labeler accuracy. In contrast, LDA clusters data items and provides useful comparisons to true clustering models. The curr_chunk compares MMM and LDA models, highlighting differences in data generation methods. MMM assumes label distribution from choosing a cluster, while LDA assigns a new cluster for each sample. This distinction is crucial in scenarios where individual beliefs are represented by clusters. In our setting, MMM is chosen as the best model for fitting the hypothesis of belief distribution across society. Different models require the number of clusters as a hyperparameter, with various model selection strategies considered. The native likelihood function of each model is ultimately chosen for selecting the best number of clusters within each strategy. Cluster-based reduction strategies were used to replace empirical label distributions with cluster centroids and likelihood estimates. Maximum a posteriori (Max) selection and expected distribution (Avg) methods were employed, with the former selecting the most likely cluster centroid and the latter integrating out clusters. These strategies were applied to models from different families to compare their performance. The study compared Max and Avg reduction strategies in clustering models to understand their performance differences. Only MMM and GMM aggregations were used for supervised learning with a CNN architecture in Keras. The CNN architecture used for sentiment analysis and topic categorization tasks in text classification experiments involves input layer with pre-trained word embeddings, convolutional layer with filters, max-pooling layer, and softmax classifier. Various label aggregation strategies were tested to obtain ground truth labels, including clustering approaches. Hyper-parameter settings depend on dataset splits, with GloVe pre-trained word embeddings trained on Twitter data. In text processing, GloVe pre-trained word embeddings from Twitter data are used with a vector size of 100. The Adam optimizer is utilized with a batch size of 32 and 25 epochs for training. Three probability distributions are associated with each data item, including labels, cluster likelihoods, and new label distributions. Clustering strategies are employed to test the main hypothesis, aiming to minimize entropy over cluster likelihoods. The entropy gap (EG) is a score that compares the entropy of cluster likelihoods to empirical distributions. It normalizes by dividing by the logarithm of the number of items in each distribution. This metric applies to label aggregation models or clustering approaches with likelihoods for each data point and cluster pair. The Kullback-Leibler divergence is also used to measure the divergence between probability distributions. The Kullback-Leibler divergence is used to measure the difference between probability distributions in the context of evaluating CNN model performance. It assesses the deviation of predicted probabilities from empirical label distributions and aggregation phase distributions. The Score function, categorical cross entropy, is utilized for CNN training, while Accuracy evaluates prediction correctness. KL divergence and cross entropy are standard tools for comparing distributions, while accuracy assesses prediction accuracy. The Kullback-Leibler divergence is used to measure differences between probability distributions in evaluating CNN model performance. Table 2, 3, 4 display performance results for each dataset in Table 1 using the best model chosen by the likelihood criterion. Visual clustering of 50 data items with 50 extra labels shows clear grouping into seven clusters. Group 1 (Red) focuses on job seeking, Group 2 (cyan) on getting hired, and Group 3 (brown) on complaining about work. Group four (green) consists of tweets complaining about work while at work, while groups five and six (blue and orange) have labels related to job and non-job topics. Group six mainly discusses road work, while group five mentions work in a more complex manner. In a study analyzing job-related tweets, CNN-based text classifiers were evaluated for accuracy and KL divergence metrics. Among the aggregation methods tested, MMM and LDA showed the best KL scores, supporting the clustering hypothesis. LDA was found to be a better model for underlying label distributions, possibly due to independent classes of labelers. The study compared CNN-based text classifiers using different aggregation methods. MMM and LDA performed well, supporting the clustering hypothesis. GMM performed worse than expected, except for LDA which used a different feature set. The entropy gap was calculated using optimal label aggregation models on each dataset. The study compared CNN-based text classifiers using different aggregation methods. GMM and DS tend to outperform other models in terms of entropy gap. CNNs trained and tested on MMM AV G outperform all other models most of the time. The study compared CNN-based text classifiers using different aggregation methods, with GMM and DS outperforming other models in terms of entropy gap. MMM Av\u0434 showed mixed results, with better performance when using deep label distributions for evaluation. MMM Av\u0434 draws from predicted ground truth label distributions, similar to those produced by LDA. KL divergence was obtained using the optimal label aggregation model on each dataset. The study compared CNN-based text classifiers using different aggregation methods. KL divergence was used to obtain the optimal label aggregation model on each dataset. Among the clustering models, KL2 outperformed KL1, supporting the effectiveness of principled aggregation processes for training and prediction. Majority was the standard approach for learning a single label for each dataset in the accuracy tests. The study compared CNN-based text classifiers using different aggregation methods. Majority and probability showed similar performance, suggesting that modeling the underlying distribution is important. Repeated, implicitly a Bayesian approach, underperformed, possibly reflecting reality. The study compared CNN-based text classifiers using different aggregation methods, with majority and probability showing similar performance. Repeated, implicitly a Bayesian approach, underperformed, possibly reflecting the reality of population beliefs being samples rather than degrees of belief. Limitations include uncertainty in human labeling caused by various factors beyond subjectivity, such as data encoding errors and unreliable annotators. Future work aims to explore the causes of uncertainty further. Understanding the social impact of representative learning requires knowledge of the underlying demographics of sampling frames like AMT and CrowdFlower. The study compared CNN-based text classifiers using different aggregation methods, with majority and probability showing similar performance. Repeated, implicitly a Bayesian approach, underperformed. Several studies have investigated demographics of platforms like Mechanical Turk and CrowdFlower, revealing differences in workforce participation levels. The worker population on platforms like Mechanical Turk and CrowdFlower is dynamic, with a steady number of workers available. Most workers are young adults aged 20-35, earning below the median salary range in the US. The majority of workers are white and speak a diverse set of languages. Office and administrative support workers are major contributors to platforms like AMT. Office and administrative support workers are significant contributors to platforms like AMT. In supervised learning, data labels are modeled as probability distributions, with label probabilities being frequentist, representing an estimate of event frequency in a population sample. Uncertainty arises when humans are involved in the loop, but studies often assume an underlying true label without considering the subjective nature of human comprehension and beliefs. Recommender systems study individual preferences in online settings to personalize user experiences. Our research focuses on how beliefs vary in populations, especially in annotation tasks where little information on annotators is available. In crowdsourcing settings, little information on annotators may be available. Multilabel classification allows data items to belong to multiple classes simultaneously. It is important to distinguish between valid labels and disagreements, especially along key demographic boundaries. Multilabel models do not detect disagreements but focus on a rich collection of individualized labels for each data item. Our study focuses on predicting the diversity of beliefs in supervised learning domains by clustering label sets. Results show the feasibility of predicting label distributions, a crucial step in creating intelligent agents that understand societal beliefs. Clustering to aggregate labels can reduce labeling costs effectively. The paper presents a framework for exploring the effectiveness of socially-aware intelligent systems. It suggests using LDA-based distributions in supervised learning and exploring more powerful variants of MMM. The project aims to develop active learning methods that are socially aware, building on limited existing research in this area. The paper aims to develop new active learning query strategies for socially representative labels, building on limited existing research in this area."
}