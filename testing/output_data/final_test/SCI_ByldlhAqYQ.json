{
    "title": "ByldlhAqYQ",
    "content": "Transfer learning, facilitated by recurrent neural networks (RNN), addresses data sparsity in specific domains by leveraging information from another domain. RNN uses repeating cells to model sequential data, but previous neural network transfer learning methods transfer information across entire layers, which is not suitable for seq2seq and sequence labeling tasks and results in loss of fine-grained cell-level information from the source domain. In this paper, the aligned recurrent transfer (ART) method is proposed for cell-level information transfer. ART shares parameters among different cells and transfers information from collocated words in the source domain. This approach allows ART to capture word collocation across domains more flexibly. Extensive experiments on sequence labeling tasks and sentence classification show that ART outperforms existing methods. Most NLP studies focus on open domain tasks, but models struggle to adapt to different domains due to overfitting. Training specific models for specific domains is challenging due to the lack of labeled data. Transfer learning is a promising solution as NLP models in different domains share common features, and open domain corpora are richer. Transfer learning in NLP involves pre-training a model on a source domain before fine-tuning it for a target domain. While most models use a large, domain-independent corpus for pre-training, this study argues for using a small, domain-dependent corpus. This approach, known as \"layer-wise transfer learning,\" aims to overcome information loss from cells in the source domain by representing the whole sentence. The approach of \"layer-wise transfer learning\" involves representing the whole sentence by a single vector and transferring information at the vector level. It is effective in capturing and transferring information from each cell in the source domain. This method is particularly useful for tasks like seq2seq and sequence labeling, where all cells impact the results directly. Even for sentence classification, transferring fine-grained information from the source domain helps in better understanding the target domain. The transfer learning algorithm captures cross-domain long-term dependencies by incorporating information from collocated words. Collocated words are learned via the attention mechanism BID1 to understand the semantics of words in the target domain. For example, \"hate\" is modified by the adverb \"sometimes\" in the context of insufficiently trained parameters. In this paper, ART (aligned recurrent transfer) is proposed as a novel transfer learning mechanism to align word collocations between source and target domains for long-term dependency representation. ART extends each RNN cell to transfer cell-level information by learning to collocate cross-domain words. Transfer ART extends each recurrent cell by incorporating states from the source domain, enabling fine-grained information transfer. It learns to incorporate two types of information from the source domain for each word in the target domain, capturing cross-domain long-term dependency. Before learning to transfer, pre-training of the neural network is conducted. Transfer ART extends each recurrent cell by incorporating states from the source domain for fine-grained information transfer. The architecture involves a shared RNN layer between the source and target domains, with the target domain having an additional RNN layer to accept transferred information. This information includes the same word information from the source domain and information from collocated words. The architecture of ART involves a shared RNN layer between the source and target domains, with the target domain having an additional RNN layer to accept transferred information. ART uses attention to decide the weights of candidate collocations and controls the weights between source and collocated words. The neural networks for the source and target domains overlap, transferring information through the overlapping modules. The attentive transfer mechanism of ART can be deployed over other structures besides RNNs. The architecture of ART involves a shared RNN layer between the source and target domains, with the target domain having an additional RNN layer to accept transferred information. Each RNN cell in the target domain leverages transferred information from the source domain. The i-th hidden state in the target domain is computed using information from the previous time step and transferred information from the source domain. Common information is transferred across domains using the same RNN function with different parameters. \u03c8 i is computed by aligning its collocations in the source domain. The architecture of ART involves a shared RNN layer between the source and target domains, with the target domain having an additional RNN layer to accept transferred information. Each RNN cell in the target domain leverages transferred information from the source domain. \u03c8 i is computed by aligning its collocations in the source domain using a \"concentrate gate\" to control the ratio between corresponding and collocated positions. Transferred information from collocated words is denoted as \u03c0 i, computed using attention to incorporate information from all candidate positions in the sequence from the source domain. The model in the current chunk calculates collocation intensity between cells in the source and target domains using weighted sums. It requires O(n^2) evaluations per sentence due to the enumeration of indexes. A single-layer perception is used with parameter matrices W a and U a. An update gate z i determines the transfer of information from the source domain \u03c8 i. The model calculates collocation intensity between cells in the source and target domains using weighted sums. It uses a reset gate to determine how much of the previous hidden state should be reset. Model training involves pre-training the parameters of the source domain and fine-tuning with additional layers from the target domain. LSTM addresses long-term dependency by having two hidden states for long-term and short-term memory. ART adaptation needs to separately represent information for the long-term memory. In the target domain, the ART adaptation incorporates short-term and long-term memory separately from the source domain. A bidirectional architecture is used to gather information from all words for each cell. The final output of the ART over LSTM is computed by combining the states from both the forward and backward neural networks for each cell. The proposed approach is evaluated for sentence classification. The proposed approach involves using a neural network with a simple structure for sentence classification and sequence labeling tasks. The network includes an embedding layer, an ART layer, and a task-specific output layer. Experiments were conducted using a computer with specific hardware specifications. The approach is compared with LSTM models with and without transfer learning. The proposed approach involves using a neural network with a simple structure for sentence classification and sequence labeling tasks. It includes an ART layer for transfer learning. The approach is compared with LSTM models with and without transfer learning, as well as other transfer learning algorithms like Layer-Wise Transfer (LWT) and Corresponding Cell Transfer (CCT). These methods are used to verify the effectiveness of cell-level and collocation information transfer. The proposed approach involves using a neural network with an ART layer for transfer learning in sentence classification and sequence labeling tasks. The approach is compared with other transfer learning algorithms like LSTM models, Layer-Wise Transfer (LWT), and Corresponding Cell Transfer (CCT) to verify effectiveness. The dataset used is the Amazon review dataset BID2, containing reviews for four domains: books, dvd, electronics, and kitchen, with positive or negative reviews. Training and validation are done using data from both domains, with testing on the target domain. The approach involves using a neural network with an ART layer for transfer learning in sentence classification. Results show that ART outperforms other models in accuracy, demonstrating the effectiveness of cell-level transfer. The ART layer in neural network for transfer learning outperforms other models in accuracy by 8.4%, verifying its effectiveness in representing long-term dependency. In minimally supervised domain adaptation, ART surpasses all competitors by a large margin, showcasing its effectiveness in this setting. The ART layer in neural network for transfer learning outperforms other models in accuracy by 8.4%, showcasing its effectiveness in representing long-term dependency. It is evaluated for sequence labeling tasks like POS tagging and NER, using a CRF layer and LSTM cells with specific dimensions. The model utilizes a combination of word and character embeddings as input, along with CNN char embeddings. The Adagrad BID5 optimizer is used with a dropout mechanism. The ART layer in neural network for transfer learning outperforms other models in accuracy by 8.4%. It is evaluated for sequence labeling tasks like POS tagging and NER, using a combination of word and character embeddings. Different datasets are used for each task, with specific training sample percentages to simulate a low-resource setting. The model aligns and transfers information from different positions in the source domain using alignment and attention matrices. The attention matrices in neural network transfer learning highlight positions with strong word dependencies, aiding in sentiment analysis. Strong emotions like \"pleased\" and \"easy to move\" receive more attention, showing effectiveness in cell-level transfer. The attention reflects long-term dependencies, with discriminative features between short-term memory h and long-term memory c. The attention in neural network transfer learning highlights word dependencies, aiding in sentiment analysis. Different approaches transfer cell-level information in the neural network, such as using RNN with auxiliary labels or pivots. However, the semantics of a word can depend on its collocated words. The ART approach successfully represents collocations through attention in neural networks. It uses a pre-trained model from the source domain and fine-tunes it with additional layers for the target domain. Unlike BERT and ELMo, which focus on general word or sentence representations, ART aims at transfer learning for specific tasks across different domains. The ART model proposed in this paper focuses on transfer learning for sequences by transferring fine-grained cell-level information, aligning collocated words for cross-domain dependency, and being applicable to various tasks. It validates the effectiveness of pre-training models with limited relevant training data."
}