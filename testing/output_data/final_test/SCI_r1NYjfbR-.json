{
    "title": "r1NYjfbR-",
    "content": "Generative Adversarial Nets (GANs) and Variational Auto-Encoders (VAEs) can generate high-quality images from Gaussian white noise. Deep convolutional network generators are computed by inverting a fixed embedding operator, eliminating the need for optimization with a discriminator or encoder. The embedding, computed with a wavelet Scattering transform, allows linear interpolations between input noise vectors to transform into deformations between output images. Numerical experiments show that Scattering generators have similar properties to GANs and VAEs without the need for a discriminative network or encoder. The work demonstrates training generative networks to produce high-quality images from Gaussian white noise without the need for a discriminator or encoder. The generator, a deep convolutional network, uses a Lipschitz continuous embedding operator implemented with a wavelet Scattering transform to mimic properties of GAN image synthesis. This approach provides a mathematical framework closer to standard probabilistic models like Gaussian autoregressive models. An implicit generative model, consisting of a generative network or generator, transforms Gaussian white noise into high-quality images. BID17 introduced deep convolutional architectures for image synthesis, showing that modifying the input vector results in image deformation. BID6 and BID0 argue that GANs select the generator by minimizing Jensen-Shannon divergence or Wasserstein distance. However, BID1 disproves this approach due to the curse of dimensionality. Generative Latent Optimization (GLO) was introduced as an alternative to VAEs and GANs, aiming to produce sharper images without the need for a discriminator. GLO utilizes an autoencoder computational approach to achieve this goal. Generative Latent Optimization (GLO) uses an autoencoder structure to optimize latent space variables for generating sharp images. Linear variations in the embedding space do not map clearly into image deformations like in GANs, affecting image quality. Questions arise about the source of deformation properties, characteristics of the embedding operator \u03a6, and how these algorithms generalize despite high dimensionality. Learning a stable embedding for image generation without strong prior information is challenging. This paper demonstrates that prior information is available for image generation, allowing predefinition of the embedding up to a linear operator that must be Lipschitz continuous to translations and deformations. The generative model in this study uses wavelet Scattering transforms to linearize translations and generate images with similar quality to VAEs or GLOs. The model is inverted using a deep convolutional network and is regularized by the generative network's architecture. Experiments show that the synthesized images have properties similar to GAN generators. The model X is computed from Gaussian white noise Z using a deep convolutional network instead of a linear operator. The operator G is obtained by inverting an embedding of the realizations of X, with a predefined operator \u03a6(x). The parametrized convolutional network generators are defined by a specific architecture, and the L1 loss is minimized over this class to approximate the realizations of X. The L1 norm is used for better results in natural signals like images. The generator G depends on training examples and regularization. Generalization over \u03a6(X) is achieved when the error is small. If G generalizes over \u03a6(X), it can generalize over Gaussian white noise Z by transforming X into Gaussian white noise. The role of the embedding \u03a6 is to specify properties of x = G(z). \u03a6 = A\u03a6, where \u03a6 is a fixed normalized operator and A is an affine operator for whitening and projection. The affine operator A performs whitening and projection on the Gaussian process {\u03a6(x i )} i\u2264n by subtracting the mean \u00b5 and normalizing the covariance matrix. Eigendecomposition is used to project onto the space of eigenvectors with the largest eigenvalues, ensuring a bi-Lipschitz embedding of the samples. The bi-Lipschitz property is maintained by adjusting the dimension d of the embedding operator \u03a6. The generator network G acts as an associative memory, associating training images with lower dimensional addresses for generation. The generator network G acts as an associative memory, associating training images with lower dimensional addresses for generation. The inner network coefficients include a form of distributed memory of these training images, allowing for content addressable memory. The normalized embedding operator \u03a6 is designed to have properties similar to GANs or VAEs, aiming for the realization of a Gaussian process concentrated over an ellipsoid. Additionally, \u03a6 must be covariant to translations to align with the deep convolutional generator's layers. The generator network G acts as an associative memory, associating training images with lower dimensional addresses for generation. The first layer reshapes input white noise Z and captures non-stationary aspects of the process X = G(Z). Subsequent convolutional layers invert the normalized embedding operator \u03a6 over training samples, requiring it to be Lipschitz continuous to translations and deformations. The deformation of an image x(u) is defined by x \u03c4 (u) = x(u \u2212 \u03c4 (u)), where u is the spatial coordinates. The maximum translation amplitude is denoted by |\u03c4 | \u221e. The deformation induced by \u03c4 is measured by |\u2207\u03c4 | \u221e, which specifies the maximum scaling factor. The operator \u03a6 is Lipschitz continuous to translations and deformations over domains of scale 2 J. The Gaussianization property implies that {\u03a6(x i )} i\u2264n should be concentrated on an ellipsoid. In higher dimensions, Gaussianization can be achieved by iteratively Gaussianizing one-dimensional variables through independent component analysis or random rotations. Another strategy involves averaging nearly independent random variables with similar variances, which can be covariant to translations when implemented with convolutions. The text discusses how averaging operators can be covariant to translations using convolutions with a low-pass filter. It emphasizes the importance of preserving high-frequency information despite the averaging process. The Scattering transform is introduced as a method to maintain high-frequency content for image generation without the need for learning. The text also outlines the architecture of a deep convolutional generator that can invert this embedding, along with the algorithm for performing the regularized inversion. The text discusses the use of wavelet transforms and Scattering operators to preserve information across different scales. A Scattering transform utilizes non-linearities to capture interactions between coefficients at various scales, maintaining information lost during averaging. The operator transforms the input signal into a tensor with spatial and channel parameters, adjusting the trade-off between Gaussianization and distance contraction. The text discusses the use of wavelet transforms and Scattering operators to preserve information across different scales. The number of channels increases with size J to compensate for spatial resolution loss. A non-linear translation covariant transformation \u03a8 J is computed using convolutions with complex wavelets and a complex modulus. The Scattering operator S J computes sequences of wavelet convolutions and complex modulus, resulting in K J channels. A Scattering transform is obtained by averaging each channel with a Gaussian low-pass filter \u03c6 J and subsampling. The text discusses wavelet transforms and Scattering operators to preserve information across scales. Wavelets separate variations at different scales and directions, while Scattering coefficients compute interactions. S J is Lipschitz continuous to translations and deformations, and wavelets satisfy a Littlewood-Paley condition for contractive operators. The text discusses the contractive properties of wavelet transforms and S J operators. Wavelet coefficients become more Gaussian as scale 2 J increases due to spatial averaging. The Central Limit Theorem shows that S J (X) converges to a Gaussian distribution with increasing scale. However, increasing scale can lead to instabilities affecting bi-Lipschitz bounds. The optimal scale choice balances Gaussianization and stability. Scattering transforms can be implemented using small size filters and subsamplings. A Scattering transform is a deep convolutional network using wavelet filters and a modulus nonlinearity. The normalized embedding operator \u03a6 is chosen as S J. A generative Scattering network inverts the whitened Scattering embedding \u03a6 using L1 loss minimization. The embedding operator \u03a6 consists of a Scattering transform S J computed by convolutional wavelet operators V j. The generative network uses Relus \u03c1 and linear operators plus biases W j. The minimization is done with the Adam optimizer. The generator is a DCGAN generator of depth J + 2. The operators W j perform bilinear upsampling and multichannel convolution. The generative Scattering networks use convolutional wavelet operators to compute a progressive inversion of images. Experiments evaluate accuracy, generalization capabilities, visual quality, and morphing of generated images. The embedding variable z = \u03a6(x) morphs generated images like in GANs. Three datasets with varying levels of variabilities are considered: CelebA, LSUN (bedrooms), and Polygon5. Each dataset contains RGB color images with shape 128x128x3. Scattering averaging scale is set to 16, linearizing translations and deformations up to 16 pixels. S4(x) is computed for each color channel of an RGB image x, resulting in a spatial resolution of 8x8 with 1251 channels. The dimension of the input image x is reduced to d = 512 by the whitening operator, resulting in a bi-Lipschitz embedding of training images. The Lipschitz constant is \u03b1 = 5, preserving 99.5% of distances between image pairs. Reducing the dimension further to 100 has a marginal effect on the Lipschitz bound and numerical results. Generalization properties of G are assessed by comparing reconstructions of training and test samples. The paper at ICLR 2018 discusses the average training and test reconstruction errors in dB for different datasets, showing signs of overfitting but not significantly compared to error variability. Overfitting is not ideal for unsupervised learning but can be beneficial for associative memory networks. Polygons are better recovered than faces in CelebA due to higher wavelet coefficient sparsity. Wavelet sparsity influences the properties of Scattering. The network regularizes the inversion by storing information needed for image reconstruction, with wavelet coefficient sparsity affecting memory usage. The generative network exhibits sparse activations, with 70% being zero on average. Scattering image generations eliminate high-frequency details due to memory capacity constraints. The generative network struggles with memory capacity constraints, leading to the loss of high-frequency details in image generations. In contrast, GANs handle this limitation differently by \"forgetting\" some training samples, resulting in mode-dropping errors that affect diversity and recovery precision. The generative network struggles with memory capacity constraints, resulting in missing high frequencies in LSUN images. Figure 4 displays generated images resembling training set images for polygons, faces, and LSUN, with deformations evaluated using linear interpolation. This paper demonstrates that the morphing properties of GANs can be replicated using a Scattering transform, eliminating the need for a discriminator or learning the embedding. The convolutional generator's architecture can be adjusted based on the Scattering operator's properties. The study presents preliminary numerical results without hyperparameter optimization. The paper discusses the use of the Scattering operator Sj and a \"plain\" Scattering transform without considering interactions between angle and scale variables, which could enhance the representation."
}