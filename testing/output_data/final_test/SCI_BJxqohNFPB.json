{
    "title": "BJxqohNFPB",
    "content": "Our work introduces a new method for domain translation using a Generative Adversarial Network (GAN) to generate photo-realistic images from Computer Graphic (CG) simulation edge map images. By embedding edge maps and training in an adversarial mode, we address the lack of photo-realism in existing GAN architectures for training DNNs in computer vision tasks. Additionally, we offer an extension to our model utilizing the GAN. The text discusses the importance of image to image translation for training autonomous systems. It highlights the challenges of using simulated scenes for training due to the lack of photorealism and variability compared to real environments. The paper proposes a new approach to generate images from semantic label maps using a Deep Convolution Neural Network. The paper introduces a new method called Deep Neural Edge Detector (DNED) to generate photo-realistic images from semantic labels of a simulator scene. The model combines embedded edge maps with semantic maps to output realistic scenes, and extends this idea to generate photo-realistic videos using an optical flow algorithm for pixel coherency. The paper presents a new method, Deep Neural Edge Detector (DNED), for generating photo-realistic images from semantic labels of a simulator scene. It compares different image generation techniques and emphasizes the importance of using perceptual loss for finer details in the generated images. The paper introduces s-Flow GAN, a method that utilizes instance and label maps for image synthesis, particularly useful for frames with multiple instances of the same semantics. It employs two generators trained with an optical flow-based loss for video generation, simplifying the process compared to previous methods. Evaluation is done using FID and FVD metrics, with spatial information from dense optical flow embedded in the neural network for image and video coherence. The paper introduces s-Flow GAN, a method that generates photorealistic images from semantic maps with high definition details. It incorporates a neural network to embed edge maps for generating diverse versions of scenes and introduces a new loss function for natural looking video generation. The use of Generative Adversarial Networks (GAN) is also discussed for authentic image generation. Generative Adversarial Networks (GANs) are widely used for image generation, with various schemes generating images of different resolutions. Generating high-resolution images is challenging due to the task's high dimensionality. The proposed DNED module uses edge map skeletons to assist in generating high-resolution images. The CG2real model uses a Computer Graphics edge map to generate high-resolution images, with the real image edge map being used for training the discriminator. The generator does not use the real image, only its edge map. This model allows the generator to create fine details in the output image by using the simulator's image to extract the edge map. In contrast to image-to-image models, the CG2real model provides the simulator's image to the generator at test time. In the CG2real model, the generator uses the simulated image's edge map to create high-resolution details in the output image. This approach allows for the generation of photo-realistic images by providing the generator with instance maps in addition to label maps. The generator in the CG2real model uses edge maps of simulated images to create high-resolution details for photo-realistic image generation. The edge maps are obtained using a neural network trained to learn edge maps of real images, inspired by the Holistically-Nested Edge Detection method. The CG2real model uses a spacial Laplacian operator with threshold to generate photorealistic images by training the DNED to learn and produce edge maps. This approach allows for the generation of fine details within a class without the need for instance map information. Additionally, the model addresses the challenge of generating temporally coherent image sequences. The CG2real model generates photorealistic images by training the DNED to produce edge maps using a spacial Laplacian operator with threshold. It can generate fine details within a class without instance map information and addresses the challenge of generating temporally coherent image sequences. Other works like vid2vid offer conditional video generation using FlowNet 2.0 to predict optical flow and combine details from previously generated images. Our approach for video generation focuses on using flow maps of consecutive images to ensure temporal coherence. Unlike previous methods, we employ a classical computer vision approach instead of a CNN for better generalization. The CG2real model learns the conditional distribution of an image based on a semantic map, enabling the generation of coherent videos. By utilizing a conditional GAN, we can generate images from semantic maps, enhancing the overall image generation process. The generator in the CG2real model maps semantic segmentation images to photo-realistic images using a supervised learning scheme. The discriminator distinguishes between real and generated images in a min max game. An edge map is provided to the generator to learn the conditional distribution of real images given semantic and edge maps. During training, the edge map is estimated using a spatial Laplacian operator and concatenated with the semantic label map as priors. The edge map is concatenated with the semantic label map and used as priors for adversarial training of the fake image vs. the real image in the GAN. The GAN is initially trained with edge maps from the Laplacian operator for stability before incorporating edge maps from the DNED. The DNED architecture, a modified version of HED, generates multiple edge maps with varying receptive fields to create an ensemble with coarse-to-fine details for diverse image generation. The DNED loss function for training involves multiple scales contributing to the ensemble, with a focus on generating diverse edge maps. Increasing image resolution can be challenging for GAN training due to the need for a large receptive field, which is typically addressed by a multi-scale approach to prevent overfitting and instability. Our architecture, inspired by various studies, excels in generating high-resolution images. We have successfully trained our model to produce images of size 768x384, 1.5 times larger in each dimension, while maintaining stability with a single discriminator. A comparison with pix2pixHD shows our model's ability to capture finer details, enhancing photo-realism. This demonstrates the feasibility of generating high-quality images with a single discriminator, as opposed to a multi-scale approach. Our model excels in generating high-resolution images, trained with a single discriminator. Comparison with pix2pixHD shows its ability to capture finer details, enhancing photo-realism. The use of perceptual loss and a pre-trained VGGnet further improves visual performance. Our model aims to generate photo-realistic, diverse images by minimizing L CG2real. Using edge maps, it can separate objects like buildings more effectively compared to previous work by Wang et al. (2018b). The model can generate unique adjacent buildings with better quality from semantic label maps. Our model generates photorealistic images with low-level details, achieving desired realism. It uses pre-trained CG2real networks to generate consecutive images and flow maps, enforcing temporal coherency without the need for a sequential generator. The model utilizes pre-trained CG2real networks to generate consecutive images and flow maps, ensuring temporal coherency without the use of a sequential generator. By incorporating Lf low into the LCG2real loss, the network learns to generate temporally coherent images, leading to photorealistic videos. Our goal is to generate photo-realistic images and videos using CG2real models. We evaluate our model's performance by comparing it to previous works using semantic segmentation. Our network outperforms previous works in terms of pixel accuracy and mean intersection over union. The FID metric is used to evaluate image generation quality by measuring the distance between real and generated samples. Lower FID scores indicate higher similarity between the two. Results show that pix2pixHD outperforms pix2pix in pixel accuracy and mIoU, with results almost matching those of the oracle on Synthia dataset. Our video generation model outperforms vid2vid in FVD score, with a substantial margin indicating our videos are more than twice similar to the oracle. This is attributed to errors accumulated in vid2vid's video generation model. Our generated images are visually appealing and temporally coherent. Our video generation model produces visually appealing and temporally coherent images with finer details in sky, road signs, and buildings. Unlike vid2vid, our model does not accumulate errors, resulting in non-flickering images. We utilize a CG2real conditional image generation approach and DNED to generate diverse and highly detailed images for better photo-realism. Our model utilizes DNED to generate diverse and photo-realistic images without instance maps. For video generation, we use flow maps for better temporal coherence. Compared to other works, our model outperforms in quantitative results and produces visually appealing images. Additionally, our video generation model creates consistent and coherent videos with improved image quality and finer details in objects, buildings, and vegetation. Additional test images on Synthia dataset show improved image quality and finer details in objects, buildings, and vegetation. Test video on CityScapes demonstrates better temporal coherency in the generated images, with buildings showing more reasonable textures and road signs being better emphasized."
}