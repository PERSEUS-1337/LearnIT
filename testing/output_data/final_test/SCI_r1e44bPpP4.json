{
    "title": "r1e44bPpP4",
    "content": "Current classical planners are successful in finding plans, but struggle with large planning instances due to the grounded task being too big. A partial grounding approach is introduced to address this issue by grounding only a projection of the task. A guiding mechanism using machine learning methods identifies relevant parts of a task to find a plan. Empirical evaluation shows the approach can solve planning instances that are too big to be fully grounded. Planning models are typically described in PDDL in terms of predicates and action schemas. Most planners work on a grounded representation without free variables, like STRIPS or FDR. Grounding is the process of translating a task from a lifted representation to a grounded representation, which can be prohibitive for large planning instances. Grounding is essential for planners like FF BID9 or LAMA BID24 to find plans for large planning tasks, but it can be a bottleneck for satisficing planners. Grounding becomes particularly challenging in open multi-task environments where the planning task includes all available objects, even if only a few are relevant. This can be seen in robotics and network-security environments like the Caldera domain. In the Caldera domain BID17, grounding is crucial for planners to find plans. A new approach called partial grounding focuses on relevant parts of the task to simplify planning. By prioritizing operators deemed important for goal achievement, machine learning methods predict operator inclusion in a plan. This method aims to solve partially grounded tasks efficiently by generalizing from small training instances to larger ones using relational features. Our learning models use relational features in classification and regression algorithms to predict relevant operators with high accuracy, reducing task size significantly. The tasks are specified in the STRIPS subset of PDDL, but our algorithms can be applied to a larger subset containing ADL expressions. A lifted (PDDL) task consists of atomic predicates, action schemas, objects, initial state, and goal state. In a lifted (PDDL) task, objects consist of constants \u03a3 C and non-constant objects \u03a3 O, with initial state I and goal G. Action schemas have parameters denoted by x, y, z and sets of parameters X, Y, Z. An action schema a[X] includes preconditions, add list, and delete list subsets of P, possibly pre-instantiated with objects from \u03a3 C. The task can be divided into domain and problem specifications. The plan is optimal if its length is minimal among all plans. The delete-relaxation of a task \u03a0 is defined as the task \u03a0. In a lifted (PDDL) task, objects consist of constants \u03a3 C and non-constant objects \u03a3 O, with initial state I and goal G. Action schemas have parameters denoted by x, y, z and sets of parameters X, Y, Z. An action schema a[X] includes preconditions, add list, and delete list subsets of P, possibly pre-instantiated with objects from \u03a3 C. The task can be divided into domain and problem specifications. The plan is optimal if its length is minimal among all plans. The delete-relaxation of a task \u03a0 is defined as the task \u03a0. In DISPLAYFORM3, the corresponding STRIPS task \u03a0 is computed by instantiating predicates and action schemas with objects in \u03a3. F contains a fact for each assignment of objects in \u03a3 to the arguments of each predicate P[X] \u2208 P, and O contains an operator for each assignment of objects in \u03a3 to each action schema a[X] \u2208 A. Only facts and operators delete-relaxed reachable from the initial state are instantiated based on the grounding algorithm of Fast Downward. The algorithm processes elements from a queue, adding operators with all preconditions met and pushing add effects. It terminates when the queue is empty, and all processed facts and operators are delete-relaxed reachable from the initial state. The algorithm can stop before the queue is empty and instantiates operators in a specific order to minimize the size of the partially grounded task. The algorithm focuses on minimizing the size of the partially grounded task by considering operator ordering and a simple stopping condition. In partial grounding, the algorithm can stop earlier than typical grounding approaches, potentially finding a plan sooner if the correct operators are selected. The key issue is determining when the probability of finding a plan using the grounded operators so far is sufficient. In partial grounding, the algorithm aims to minimize the size of the task by selecting operators strategically and using a simple stopping condition. The main constraint is the available resources, with a parameter N op guiding the number of operators to ground. The algorithm continues grounding as long as the number of grounded operators is within the specified limit. Our algorithm incrementally grounds actions to converge to full grounding, ensuring all grounded operators' effects are included in the task. The algorithm prioritizes grounding facts in the queue and selects operators based on a priority function. The algorithm prioritizes grounding facts in the queue and selects operators based on a priority function, such as FIFO, LIFO, or random. Machine learning techniques are used to estimate probabilities of operators belonging to a plan. A round robin criterion is also considered to increase operator diversity and avoid bias. A novelty criterion is defined as a non-trivial priority function inspired by novelty pruning. The algorithm prioritizes grounding facts and selecting operators based on a priority function. A novelty criterion is defined as a non-trivial priority function inspired by novelty pruning techniques. It defines the novelty of a state and operators in the grounding process based on the minimum number of unique facts. The grounding process prioritizes operators with higher novelty to generate new facts. A priority queue guides the selection of promising operators using a priority function. The goal is to assign a value between 0 and 1 to estimate the usefulness of operators in an optimal plan, despite the challenge of limited knowledge about the fully grounded task. The training approach involves using small instances of a domain to guide grounding in larger instances. Training data consists of tuples for each operator, where a value of 1 indicates it belongs to an optimal solution. Priority functions are formulated as a classification task to order operators based on confidence in their optimality. To learn a model for ordering operators by confidence in belonging to the optimal class, relational rules are proposed to connect objects to training samples. Separate models are trained for different action schemas, predicting the probability of an operator being in an optimal plan. Two learning approaches considered are inductive relational trees. Inductive Relational Learning Trees are used in machine learning to learn domain control knowledge for planning. The Aleph tool is utilized to create a tree where each inner node is a predicate connecting parameters to facts or objects. The nodes are evaluated based on predicate instantiation with given objects in the initial state or goal. In machine learning, Inductive Relational Learning Trees are used to learn domain control knowledge for planning. The tree evaluates nodes based on predicate instantiation with objects in the initial state or goal. The tree learned for the turn-to action schema in Satellite involves evaluating paths from root to leaf nodes to estimate the probability of an operator being part of an optimal plan. The goal in this domain is to take pictures in different modes using various satellites and instruments. The trained model evaluates the usefulness of turning the satellite in different directions by considering the presence of image goals. If there is no goal to take an image in a certain direction, the operator is deemed not useful with a 1% probability of being in an optimal plan. Conversely, if there is an image goal but no goal pointing in that direction, the operator is considered most useful with a 38% probability of being in an optimal plan. This information helps predict the effectiveness of the turn-to action. The usefulness of turn-to can be predicted by using relational rules as features for classification and regression algorithms. These rules consist of action schemas and goal or initial-state predicates, similar to paths in relational trees. Rules are generated by considering all possible predicates and parameter instantiations with the restriction that one argument in the first predicate must be bound to a parameter of the action schema. The rules generated for predicting the usefulness of turn-to involve binding predicates to action schema parameters and filtering out irrelevant features. This filtering process removes rules that do not provide useful information for classification tasks. The rules generated for predicting the usefulness of turn-to involve binding predicates to action schema parameters and filtering out irrelevant features. Attribute selection techniques are used to filter out unhelpful features for predicting if the operator is part of an optimal plan. For example, a relevant rule for the turn-to schema accurately describes when turn-to is relevant. Rules are evaluated by replacing arguments in the rule head with objects used to instantiate the operator and checking for assignments to free variables. In predicting the usefulness of actions like turn-to, rules are generated by binding predicates to action parameters and filtering out irrelevant features. Feature vectors are created for each grounded action, with binary features indicating if a rule evaluates to true for that action. Classification or regression methods can then be used to map each action to a real number, with the goal of determining if the action is part of an optimal plan. During grounding, operators are assigned values of 1 for an optimal plan and 0 for others. Training examples with the same feature vector are merged, with a single one belonging to the 1 class if any example did. Rules are evaluated for each operator inserted in the queue, with precomputed assignments to action schema arguments to speed up the process. The computational cost is exponential in the number of free variables but typically negligible. Relational trees are evaluated similarly. The partial grounding approach was evaluated using the \"translator\" component of the Fast system. Experimental Setup. The partial grounding approach was adapted using the \"translator\" component of the Fast Downward planning system. Changes made were minimally invasive, affecting only the ordering of actions and termination conditions. These changes do not impact the correctness of the translator, ensuring the generated grounded planning task is always a proper FDR task. Performance is minimally affected, except with computationally expensive priority functions. The evaluation requires domains with instance generators to create diverse instances for training, with grounded instances growing at least cubically in size. The study used four domains from IPC 2011 and two from IPC'18, with 25 large instances generated for each domain. Training data consisted of 40 to 250 small instances per domain to adapt to varying numbers of grounded actions. Large instances were scaled from the largest IPC instances for evaluation. In the study, the generator scaled parameters linearly to create instances similar in size to the largest IPC instances. For example, in Satellite, the smallest instances had 10 satellites and 20 instruments, while the largest had up to 15 satellites and 60 instruments. In Blocksworld, the scaling started at 75 blocks and went up to 100. Different domain encodings were used, with Satellite using a typed domain encoding and Blocksworld using a \"no-arm\" encoding, resulting in slightly worse results. In contrast to the \"arm\" encoding, the grounded task size is only quadratic in the PDDL description. Experimenting with various methods including FIFO, LIFO, and novelty-based approaches, learning-based methods using classification and regression models are also explored. The study combines these methods with the round robin queue setup (RR) and reports results for logistic regression classifier (LOGR), kernel ridge regression (KRN), linear regression (LINR), and support vector machine regressor (SVR). LINR and LOGR learn linear functions while KRN and SVR can learn non-linear functions, which are expected to be useful for combining features that cannot be done with linear functions. We used decision tree regressor for feature selection in machine learning algorithms, connecting to the translator in FD. Models were evaluated on validation instances to predict rule usefulness. The priority function learned by LOGR in Blocksworld effectively distinguishes optimal from non-optimal operators, especially for the action schemas move-t-to-b and move-b-to-t. The distinction is not as clear for move-b-to-b. The priority function learned by LOGR in Blocksworld effectively distinguishes optimal from non-optimal operators, especially for move-t-to-b and move-b-to-t action schemas. The total number of grounded move-b-to-b actions is much higher than the other two action schemas. The model works well when used in a single priority queue, prioritizing move-t-to-b and move-b-to-t over move-b-to-b. Grounding all operators with a priority of > 0.6 suffices to solve tasks on the validation set. RR grounds an equal number of all action schemas, including unnecessary operators. These observations are supported by plots in Figure 3. When working with machine learning techniques, overfitting is a concern, but our results show it's not an issue in our setup. We use an incremental grounding approach, where tasks are grounded until the goal is relaxed-reachable. If the first iteration fails, we set a minimum number of operators to be grounded in the next iteration. This strategy aims to find the minimum number of operators needed. The strategy aims to minimize the number of operators needed for task completion. Incremental grounding is used with different priority functions, allowing for a total of 5 hours and 4GB for the process. The LAMA planner is used as a baseline, with runtime and memory limits set for the experiments. The experiments show coverage in Table 1 with time and memory limits. Instances are considered solved within 30 min. The incremental grounding approach outperforms the baseline in most cases, except for large instances with up to 9 million operators. The incremental grounding approach significantly outperforms the baseline in solving instances in Blocksworld and Caldera. The plots in Figure 3 provide insights on the number of operators needed for goal relaxation and task completion. The plots in Figure 3 show the reduction in the number of grounded actions for different priority functions in various domains. Our models achieve significant reductions in Agricola, Blocksworld, and Caldera, moderate reductions in Depots and Satellite, and a small reduction in TPP. Different learning models perform best in different domains for partially grounded tasks. Baseline methods like FIFO, LIFO, and Random do not show significant reductions. The baselines FIFO, LIFO, and Random do not significantly reduce the size of the grounded task in most cases. The novelty criterion is often the best method among those without learning. Grounding a delete-relaxed reachable task with fewer operators can be beneficial but may be detrimental for coverage if the task is unsolvable. Learning models with the highest reductions are not always the same as the ones with the highest coverage. The RR queue mechanism often grounds more operators before reaching the goal, leading to more stable results. Aleph is an exception where RR makes partially grounded tasks unsolvable. Some approaches in the literature aim to alleviate the grounding problem by avoiding unreachable facts and operators, splitting action schemas, or using symmetries. Lifted planning approaches have lost popularity due to the advantages of grounding for speeding up search and providing informative heuristics. BID25 adapted the delete-relaxation heuristic to the lifted level, which could enhance novelty and learning functions. In the context of addressing the grounding problem in planning tasks, various techniques have been explored to eliminate irrelevant facts and operators. One approach involves under-approximation refinement, which focuses on searching with a subset of operators. However, existing methods rely on fully grounded representations, making them not directly applicable in certain settings. Recent research has shown promise in using machine learning to identify irrelevant operators, similar to learning heuristic functions for specific domains. This approach involves training models to learn heuristic estimates for states in the search, utilizing neural networks as opposed to classical models. The authors proposed an approach using neural networks for partial grounding of planning tasks, guiding the process to prioritize relevant operators. They trained machine learning models on optimal plans from small instances of the same domain, showing high accuracy in identifying relevant operators and reducing the number of grounded operators significantly. The authors proposed a neural network approach for partial grounding of planning tasks, prioritizing relevant operators. Machine learning models trained on optimal plans from small instances showed high accuracy in reducing the number of grounded operators significantly. This approach outperformed traditional methods by several orders of magnitude, increasing coverage in large instances."
}