{
    "title": "H1ERcs09KQ",
    "content": "The recent breakthrough in joint optimization of representation learning and clustering has led to flat-level categories clustering with a focus on instance relations. To address the limitations of flat clustering, hierarchically clustered representation learning (HCRL) is introduced, optimizing representation learning and hierarchical clustering simultaneously. HCRL utilizes a nonparametric Bayesian prior on embeddings to handle dynamic mixture hierarchies and adopts a hierarchical-versioned Gaussian mixture model for generative processes. Compared to prior works, HCRL is the first model to generate deep embeddings from every component. HCRL is the first model to generate deep embeddings from every component of the hierarchy, enabling more meaningful separations and mergers of clusters. It reconstructs data by various abstraction levels, infers hierarchical structure, and learns level-proportion features. Evaluations in image and text domains showed competent likelihoods and best accuracies compared to baselines. Representation learning has enhanced data representation and clustering models aim to represent intrinsic data structures. The paper introduces a unified model for nonparametric Bayesian hierarchical clustering with neural-network-based representation learning. Autoencoder is used for unsupervised representation learning, while variational autoencoder places a Gaussian prior on the embeddings. This approach aims to enhance data representation and clustering models by merging representation learning and clustering, as well as structuring the clustering results. The autoencoder, whether probabilistic or not, has limitations in capturing the hierarchical structure of data. Prior works have combined autoencoders with clustering algorithms to address this issue. Recent developments have explored nonparametric Bayesian approaches to overcome these constraints. The nonparametric Bayesian approach is used to overcome constraints in capturing hierarchical data structure. Hierarchically clustered embeddings on MNIST with three levels of hierarchy are shown, along with reconstructed digits from Gaussian mixture components. IMVAE explores VAE mixtures in an infinite space using the Chinese restaurant process, while VAE-nCRP captures a more complex hierarchical structure by adopting the nested Chinese restaurant process into the cluster assignment of the Gaussian mixture model. The paper introduces hierarchically clustered representation learning (HCRL), a joint model of nonparametric Bayesian hierarchical clustering and representation learning with neural networks. HCRL extends previous work by incorporating inter-cluster relation modeling, allowing for the full spectrum of hierarchical clusterings to be learned, including level assignment and proportion for generating a component hierarchy. This enables data instances to be analyzed from the perspective of generalization and specialization in a hierarchy. Hierarchical mixture density estimation is a flexible framework for modeling internal and leaf components to generate data. HCRL optimizes hierarchical clustering in an embedding space from VAE using a hierarchical-versioned Gaussian mixture model (HGMM) and an nCRP prior for dynamic hierarchy structures. HCRL optimizes hierarchical clustering in an embedding space using a unified objective function that combines VAE with a hierarchical-versioned Gaussian mixture model and an nCRP prior. The quantitative evaluations show competent likelihoods and the best accuracies compared to baselines, with experiments conducted on benchmark datasets including MNIST, CIFAR-100, RCV1 v2, and 20Newsgroups. Visualizations include hierarchical clusterings, embeddings under the hierarchy modeling, and reconstructed images from Gaussian mixture components. VaDE utilizes mean-field variational inference to maximize marginal log likelihoods with a Gaussian mixture model as the prior. It merges the ELBO of VAE with the likelihood of GMM. VAE-nCRP employs a nonparametric Bayesian prior for learning tree-based hierarchies. The nCRP prior defines distributions over children components for each parent component in a hierarchical manner. Variational inference of the nCRP is formalized using nested stick-breaking construction. The weight for each node follows the GEM distribution, constructed through a stick-breaking process. VAE-nCRP combines the ELBO of VAE with the nCRP, offering potential improvements. The ELBO of VAE-nCRP presents potential improvements in modeling hierarchical relationships among clusters. It highlights challenges in training higher clusters and the limited summarization ability of leaf clusters. The shared variance parameter and lack of variance parameters for local density modeling are addressed through level proportion modeling and HGMM prior. The generative process of HCRL involves selecting a path from the nCRP prior, sampling a level proportion and a level to find the mixture component, generating a Gaussian distribution for latent representation, and using it to generate an observed datapoint. The generative process of HCRL involves selecting a path from the nCRP prior, sampling a level proportion and a level to find the mixture component, generating a Gaussian distribution for latent representation, and using it to generate an observed datapoint (phase 5). The neural architecture consists of two probabilistic encoders on z and \u03b7, and one probabilistic decoder on z. This unbalanced architecture originates from the modeling assumption of p(x|z), not p(x|z, \u03b7), leading to a large variance in reconstruction on x. The reconstruction variance on x is affected by z and \u03b7 in HCRL. The decoding structure of \u03b7 is not explicitly included in the neural network architecture. The formalization of p(\u03b7|z) is provided based on generative assumptions. The reconstruction process is a generative process of the traditional probabilistic graphical model. The formal specification involves a factorized probabilistic model with latent variables \u03a6 = {v, \u03b6, \u03b7, l, z}. \u03b6, \u03b7, and l model the mixture components for data instances. The latent variables in HCRL are inferred through MFVI using variational distributions. The ELBO of HCRL is defined with generative and variational distributions. The Laplace approximation with logistic normal distribution is applied to model the prior of the level proportion, \u03b7. The Laplace approximation is used to model the prior of the level proportion, \u03b7 in HCRL. A heuristic algorithm is devised for operations like GROW, PRUNE, and MERGE to refine the hierarchy structure in the stick-breaking process scheme. Appendix C provides details on these operations and the overall training algorithm of HCRL. The Laplace approximation is utilized to model the prior of the level proportion, \u03b7 in HCRL. A heuristic algorithm is developed for operations like GROW, PRUNE, and MERGE to refine the hierarchy structure in the stick-breaking process scheme. Various benchmark datasets including MNIST and CIFAR-100 are used for experimentation. MNIST consists of 28x28x1 handwritten image data, while CIFAR-100 contains 32x32x3 colored images with 20 coarse and 100 fine classes. The text discusses benchmark datasets such as CIFAR-100 and 20Newsgroups used for evaluation in density estimation and hierarchical category clustering. The datasets consist of colored images with coarse and fine classes, and text data from newsgroups with hierarchical labels. HCRL was evaluated for density estimation and accuracy perspectives compared to flat clustered representation learning models and VAE-nCRP. Baselines and two-stage pipeline approaches were used for evaluation. HCRL outperformed in negative log likelihood (NLL) and was competitive in reconstruction errors (REs) compared to other models like IDEC and DCN. HCRL outperformed in negative log likelihood (NLL) and was competitive in reconstruction errors (REs) compared to other models like IDEC and DCN. VaDE generally performed better than VAE, with HCRL showing competent performance and better results with a deeper hierarchy. HCRL also had significantly better microaveraged F-scores compared to every baseline, reproducing the ground truth hierarchical structure of the data consistently. HCRL outperformed other models in hierarchical clustering accuracies, including VaDE and VAE-nCRP. The joint optimization of hierarchical clustering in the embedding space improved accuracies. In MNIST, HCRL grouped digits {4, 7, 9} and {3, 8} hierarchically, with some digits {0, 4, 2} in a round form also grouped together. The reconstructed digits from hierarchical mixture components showed blended shapes from the root. The hierarchical clustering results on CIFAR-100 show blended shapes from 0 to 9, with color dominantly reflected in the criteria. In RCV1 v2, VAE and VaDE show no hierarchy, while VAE-nCRP agglomerates components at the center. HCRL demonstrates clear separation between sub-hierarchies. The hierarchical clustering results on CIFAR-100 show blended shapes with color dominantly reflected in the criteria. HCRL introduces a hierarchically clustered representation learning framework for deep embeddings, aiming to encode relations among clusters and instances to preserve the internal hierarchical structure of data. Key features of HCRL include the assumption regarding internal mixture components and the unbalanced autoencoding neural architecture for level proportion modeling. HCRL enables improvements in modeling flexibility compared to baselines. A synthetic dataset with a hierarchical structure was created and clustered using HCRL, showing hierarchically clustered embeddings in a latent space. Confidence ellipses were also plotted for each learned Gaussian mixture component, with the root component forming a large ellipse and the leaf component summarizing local density with a small ellipse. The HCRL model enables flexibility in modeling compared to baselines. Experiments were conducted with autoencoder-based models using fully connected layers. The hyperparameters for HCRL were set to \u03b3=1.0 and \u03b1=1.0. The Adam optimizer was used with different learning rates for datasets. VAE-nCRP is designed for grouped data. The HCRL model allows for flexible modeling compared to baselines, with experiments conducted using autoencoder-based models. Hyperparameters were set to \u03b3=1.0 and \u03b1=1.0, and the Adam optimizer was used with varying learning rates for datasets. VAE-nCRP is tailored for grouped data, while the algorithm for HCRL involves selecting operations like GROW, PRUNE, and MERGE based on a tree-based hierarchy defined by nodes and paths. The HCRL model involves selecting operations like GROW, PRUNE, and MERGE based on a tree-based hierarchy. Algorithm 1 selects an operation out of three: GROW, PRUNE, and MERGE, with specific iteration periods. After a certain number of iterations, PRUNE or MERGE is performed, prioritizing PRUNE first. After any operation, n b is initialized to lock the hierarchy for a minimum number of iterations. The HCRL model involves selecting operations like GROW, PRUNE, and MERGE based on a tree-based hierarchy. Algorithm 1 selects an operation out of three: GROW, PRUNE, and MERGE, with specific iteration periods. After a certain number of iterations, PRUNE or MERGE is performed, prioritizing PRUNE first. After any operation, n b is initialized to lock the hierarchy for a minimum number of iterations. The maximum length of Q, Q max; grow scale, s grow. Update network weight parameters using gradients. Sample a path \u03b6* with probability Q. The PRUNE operation cuts a minor path sampled according to a pre-defined threshold parameter. The MERGE operation combines two full paths with similar posterior probabilities by merging Gaussian components. The estimation of merged Gaussian parameters involves a weighted summation of two subject Gaussian parameters. The MERGE operation merges Gaussian components of two paths with similar posterior probabilities by combining their parameters. The probability of a node lying on a path is calculated based on certain conditions. Various notations and networks are defined in the paper. The variational parameters and weights of the encoder and decoder networks for Gaussian distributions, cluster assignments, and prior parameters are defined in the VAE-nCRP & HCRL model. The variational parameters and weights of the encoder and decoder networks for Gaussian distributions, cluster assignments, and prior parameters are defined in the VAE-nCRP & HCRL model. The curr_chunk discusses datapoints, latent representations, Beta draws, path assignments, variational parameters for distributions, and encoder network weights. The variational mean and variance for Gaussian distribution q \u03c6z (z|x) \u00b5\u03b7,\u03c3 2 \u03b7The variational mean and variance for logistic normal distribution q \u03c6\u03b7 (\u03b7|x) \u03b1The variational parameter for Dirichlet distribution q \u03c6\u03b7 (\u03b7|x) viThe Beta draws for the tree-based stick-breaking construction of node i \u03b3The prior parameter for Beta distribution p(vi) ai, biThe variational parameters, for Beta distribution q(vi|x) \u03b6nThe path assignment of zn SnThe variational parameter for multinomial distribution q(\u03b6n|xn) \u03b7nThe level proportion of zn \u03b1The prior parameter for Dirichlet distribution p(\u03b7n) lnThe level assignment of zn, \u2208 {1, ..., L} \u03c9nThe variational parameter for multinomial distribution q(ln|xn) \u00b5i, \u03c3 E GENERATIVE AND INFERENCE MODEL FOR HCRL HCRL assumes the generative process as described in Section 3.1. Section E.1 describes the joint probability distribution, and Section E.2 presents the corresponding variational distributions. We adopt the much notation-related conventions The variational distributions are modeled for probabilistic decoding of x parametrized by \u03b8. Beta draws are denoted as v, with each draw independent from Beta(v|1, \u03b3). In VAE, random variables are inferred using mean-field approximation with variational distribution q \u03c6\u03b7,\u03c6z (v, \u03b6, \u03b7, l, z|x). The level of mixture component i is determined by paths in the truncated tree T, either inner paths or full paths. In this section, the detailed derivation of the ELBO in Equation 6 for learning HCRL is presented. It involves various probabilistic distributions and notations, including the digamma function \u03c8. The objective is to infer random variables v, \u03b6, \u03b7, l, z|x using variational distributions q. The curr_chunk presents mathematical equations related to the ELBO for learning HCRL, involving probabilistic distributions and variational distributions q."
}