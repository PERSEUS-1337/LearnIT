{
    "title": "BJxg7eHYvB",
    "content": "Deep neural networks use deeper and broader structures to achieve better performance, leading to increased GPU memory usage. A reinforcement learning algorithm is proposed in this paper to reduce memory cost without compromising model accuracy. Variable swapping transfers variables between CPU and GPU memory, while recomputation removes some feature maps during forward propagation. A deep Q-network is used to automatically decide which variables to swap or recompute. Results show that this approach outperforms several benchmarks in addressing limited GPU memory. Limited GPU memory restricts model performance due to deeper neural networks requiring more memory and bigger input batch sizes speeding up training but demanding more GPU memory. Utilizing CPU memory for variable swapping can improve performance by offloading and prefetching variables. This is beneficial because CPU memory is usually larger than GPU memory. Variable swapping allows for overlap of data transfers with kernel execution, making it efficient. Recomputation can also help manage GPU memory limitations. Variable swapping and recomputation are used to optimize memory usage in neural networks. Swapping is efficient as it allows for overlap of data transfers with kernel execution. Recomputation reduces memory usage by using GPU computing engines, while swapping saves memory using DMA engines. It is challenging to determine which variables should be swapped or recomputed due to the complexity of neural network structures and the large number of variables involved in training. Some works use heuristic search algorithms to address this issue. Our work proposes a method for optimizing memory usage in neural networks by utilizing more information from computational graphs and automatically making plans for users. This approach considers the time cost of recomputing different layers and swapping variables to increase GPU utilization. Our paper introduces a DQN algorithm for optimizing memory usage in DNNs by automatically making plans for swapping and recomputation. This method does not require users to have background knowledge on DNNs and ensures that variable swapping does not compromise network accuracy. Previous methods like greedy algorithms and heuristics require user input, while our approach leverages computation graphs to provide automatic plans based on memory limits. Our method utilizes computation graphs to automatically provide plans for memory optimization in DNNs. Previous approaches like recomputation and memory sharing have limitations in adapting to different memory limits. Our method addresses these challenges by using DQN to minimize memory load during backward propagation. Our method uses DQN to minimize computation overhead with a GPU memory limit by optimizing GPU operations in a training iteration. The algorithm interacts with an environment simulator to provide a strategy that minimizes training time while adhering to the memory limit. Choices include offloading, prefetching, removing, recomputing, or doing nothing with variables in GPU memory. Our algorithm focuses on optimizing DNNs and machine learning algorithms by utilizing GPU operations and memory limits. It uses DQN to find an optimal plan for swapping and recomputing, ultimately reducing computation overhead. Actions in each GPU operation include swapping, recomputation, or doing nothing. The algorithm utilizes DQN to optimize DNNs and machine learning algorithms by managing GPU operations and memory limits. Actions involve swapping, recomputation, or doing nothing to control memory usage within set limits. GPU operations are represented as nodes in a graph, with edges connecting operations that are related. The algorithm uses DQN to optimize DNNs and machine learning algorithms by managing GPU operations and memory limits. Actions involve swapping, recomputation, or doing nothing to control memory usage within set limits. Nodes in a graph represent GPU operations, with edges connecting related operations based on time lag. Agent states for DQN include information on variables, attributes, and DNN structures mapped into vectors. Node states are combined into graph states for the agent. The algorithm utilizes DQN to optimize DNNs and machine learning algorithms by managing GPU operations and memory limits. Nodes represent GPU operations, with edges indicating time lag between operations. The parameter matrix W can be learned, and variables can be offloaded and recomputed in different states. x v includes six features related to node v, such as variable size and transfer duration. The graph state concatenates the summation over each node state with the state of the node under execution. Actions include offloading a variable into CPU memory, removing a variable during forward propagation, or doing nothing. Variable swapping involves swapping out a variable. The graph feature representation allows training a DQN on a small graph and fine-tuning on a large graph. Variable swapping and recomputation involve two phases: swapping out a variable and removing a variable during forward propagation. Actions only include the first phase for variable swapping and recomputation, known as the swap-out action and the removing action. Prefetching involves prefetching the earliest reused variable not in GPU memory first, followed by the second earliest reused variable. If swapping in a variable does not exceed memory usage until the end of backpropagation, prefetching will begin. The text discusses the time relation between nodes and agent transitions in GPU operations. Each node represents a GPU operation and actions can be chosen from candidate actions. Variable swapping and recomputation procedures are used. Actions change the state of the agent and the node under execution. Each GPU operation takes one second to finish. In Figure 4, the agent is in the 3rd node, with each GPU operation taking one second. Offloading X0 takes 1.5 seconds. Choosing to offload X0 moves the agent to the 5th node, but if memory usage equals the limit, the agent stays in the 4th node. Forward overhead for offloading X0 is 0.5 seconds. Backward overhead can be calculated based on prefetching order. No new variables are allocated until there is enough GPU memory space. In Figure 4, the agent's actions impact forward and backward propagation times. Offloading X0 incurs negative overhead, while removing X0 results in negative time for recomputing forward layer functions. To address memory constraints, a solution involving free and malloc operations was proposed by NVIDIA in 2019. This approach allows for roughly correct training time estimation, although it may not provide the exact derivative of weights. When applying an action to the agent, it transitions to the next state and receives a reward from a simulator based on defined criteria. The simulator is used to update DQN in a training environment. Assumptions include estimating recomputing time and parallel running of variable swapping with layer functions. Graph states are converted to Q values by concatenating with action node states, mapping to a value. The text discusses the process of transitioning states in an end-to-end DQN by applying actions to the agent. It mentions the removal of variables from GPU memory and the training of the DQN using a specific loss function. The text also touches on the reward system and memory constraints during the iteration process. In the current iteration, the state becomes terminal if memory limit is exceeded. Algorithm 1 updates weights using mini-batches from experience replay dataset. Actions are chosen greedily from a set. DNN training follows the plan generated by DQN. GPU operations are executed sequentially. Actions are executed based on set criteria. Memory reduction and usage are evaluated. Performance of variable swapping and recomputation is tested on different architectures. Our method involves variable swapping and recomputation on various architectures like ResNet, VGG, UNet, Kmeans, and LSTM. Experiments are conducted on CIFAR-10 and Karpathy's char-RNN dataset. The k-means dataset is randomly generated. The system used for experiments includes a workstation with CPU Intel Xeon E5, NVIDIA GeForce GTX 1080 Ti, 64GB RAM, and Ubuntu 16.04 with CUDA 9.0 and cuDNN 7.0. Our method involves variable swapping and recomputation on various architectures like ResNet, VGG, UNet, Kmeans, and LSTM. Experiments are conducted on CIFAR-10 and Karpathy's char-RNN dataset. The cuDNN algorithm requires extra workspaces to store intermediate results. Comparison with other baselines like MXNet-memonger, SuperNeurons, and TFLMS is done based on computation overheads versus memory reductions. Different data points are obtained by changing recomputation layers and using different modes in SuperNeurons. Our method involves variable swapping and recomputation on various architectures like ResNet, VGG, UNet, Kmeans, and LSTM. Experiments are conducted on CIFAR-10 and Karpathy's char-RNN dataset. Comparison with other baselines like MXNet-memonger, SuperNeurons, and TFLMS shows that our method saves more GPU memory and requires less extra computation time. SuperNeurons use the LRU algorithm for variable swapping, while TFLMS only uses variable swapping with manual setting of swap variables. Our method utilizes more information from computation graphs and provides plans automatically for users. Our method offers higher GPU utilization compared to existing methods, supporting a wider range of architectures like LSTM CuDNNLSTM, ResNet with stochastic depth, and K-Means. It allows for easy memory limit adjustment, works well with iterative machine learning algorithms, and provides automatic plans for users without requiring expert knowledge. UNet's structure differs from ResNet and VGG, leading to worse results due to memory usage issues. LSTM, lacking convolutional layers, operates slower with kernel operations on GPU. In this paper, a DQN is proposed to reduce memory usage by devising plans for variable swapping and recomputation. The method can automatically provide plans for users based on a set memory limit, without requiring background knowledge on DNN or machine learning algorithms. It is effective for various network structures such as ResNet, VGG, K-means, SD ResNet, and LSTM. The method proposed in the paper uses variable swapping and recomputation to reduce memory usage in DNNs like K-means, SD ResNet, and LSTM without affecting network accuracy."
}