{
    "title": "HkfXMz-Ab",
    "content": "We study generating source code in a Java-like language based on labels with information. Programs must reflect a realistic relationship with labeled examples during training. Generating programs that meet syntactic and semantic constraints is challenging. To address this, a neural generator is trained on program sketches instead of code, abstracting out non-generalizable elements. This system can predict entire method bodies in API-heavy Java code with just a few API calls or data types. In this paper, a method combining neural and combinatorial techniques is presented for generating highly structured text, specifically the source code of programs in Java-like languages. The goal is to learn a function that can produce compilable, type-safe programs from annotated program labels, with applications in assisting humans with programming tasks. The paper presents a method for generating program code from annotated labels, with applications in assisting programmers. Conditional program generation is a form of program synthesis that has gained interest recently, particularly with neural approaches driven by input-output examples. These methods aim to associate a program's syntax with its semantics in controlled domains. Conditional program generation involves generating programs in a rich, Java-like language with thousands of data types and API methods, sidestepping the need to learn the semantics of the programming language from data. This approach contrasts with methods that generate programs in highly controlled domain-specific languages with limited data types and functions. Conditional program generation involves creating programs in a Java-like language with numerous data types and API methods, focusing on satisfying structural and semantic constraints for compiler acceptance. The approach combines neural learning and type-guided combinatorial search to generate programs based on tree-structured syntactic models or sketches, abstracting source code for efficient learning. The approach combines neural learning and type-guided combinatorial search to generate programs based on tree-structured syntactic models or sketches, abstracting source code for efficient learning. A Gaussian Encoder-Decoder (GED) is proposed to learn a distribution over sketches conditioned on labels, enabling the synthesis of type-safe programs using a combinatorial method. The system BAYOU, evaluated in generating API-manipulating Android methods, shows promising results with a corpus of 150,000 methods from an online repository. Conditional program generation involves using a training set of labeled programs to generate new program instances. Functional equivalence is defined by an equivalence relation between programs, where two programs are considered equivalent if they produce the same output. The goal is to leverage the training examples to generate new programs that are functionally equivalent to those in the training set. Conditional program generation aims to learn a function g : X \u2192 P from a training set, maximizing the expected value E[I((g(X), Prog) \u2208 Eqv )]. The domain P consists of programs in AML, a language for API-heavy Java programs with complex control flow and Java API access. Conditional program generation in AML involves learning a function to generate type-safe programs using API method names as labels. The space of possible labels is defined by sets of API calls, object types, and keywords. Determining equivalence between programs in AML is challenging due to the language's complexity, making it difficult to ascertain if two programs always produce the same output. Determining if two AML programs always produce the same output is undecidable. Success is measured indirectly by comparing control structures and API call sequences. A program in Figure 1 reads lines from a file using a special variable as input, handling exceptions and closing the reader. Failures can occur, such as generating a program using an InputStreamReader instead of a FileReader. Our approach involves using an InputStreamReader instead of a FileReader to rule out certain programs. By amending X Types to {FileReader}, BAYOU generates programs that specifically use FileReader. Learning occurs at a higher level of abstraction than individual program pairs, focusing on maximum conditional likelihood estimation. This approach helps to uncover patterns in code by abstracting away low-level details and semantic rules. Our approach involves using a statistical learner to generate programs from sketches that capture key facets of program syntax. The sketches do not contain low-level details but focus on types and API calls. This method aims to simplify program synthesis by abstracting away complex semantic rules and focusing on shared program structures. The sketch for a program is denoted by Y, with an abstraction function \u03b1 : P \u2192 Y. A sketch Y is satisfiable if \u03b1 \u22121 (Y) = \u2205. Generating programs from satisfiable sketches is probabilistic, with a fixed concretization distribution chosen heuristically. Learning this distribution from source code is challenging due to the variability in program details. Our heuristic approach leverages known semantic properties of programming languages to simplify the process. The text discusses simplifying program generation by leveraging semantic properties of programming languages. It introduces a random variable Y = \u03b1(Prog) and assumes conditional independence between variables X, Y, and Prog. The problem is framed as learning over sketches, with a grammar defined for sketches in the implementation. Sketches do not contain constants or variable names. The text discusses simplifying program generation by leveraging semantic properties of programming languages. It introduces a random variable Y = \u03b1(Prog) and assumes conditional independence between variables X, Y, and Prog. Sketches in the implementation do not contain constants or variable names. The abstraction function for AML is defined in Appendix B, with API calls abstracted into method calls. Control flow and exception handling information is preserved using keywords like skip, while, if-then-else, and trycatch. The learning approach involves computing arg max \u03b8 i log P (Y i |X i , \u03b8) using an encoder-decoder. The text introduces an encoder-decoder model with a latent variable Z to link labels and sketches. A Normal (0, I) prior is assumed on Z for regularization. Labels are in the form X = (X Calls, X Types, X Keys), with elements generated independently and encoded using a function f. The text introduces an encoder-decoder model with a latent variable Z for linking labels and sketches. The encoded values are sampled from a high-dimensional Normal distribution centered at Z. The probabilistic encoder can still be used even if the encoding function is not 1-1 and onto, with benefits of a regularizing prior on Z. The text introduces a Gaussian encoder-decoder model with a latent variable Z for linking input and output. The algorithm maximizes a lower bound on log-likelihood using stochastic gradient ascent and reparameterization trick. The final step involves \"concretizing\" sketches into programs based on the distribution P(Prog|Y). Our method involves a stochastic search procedure that transforms sketches into programs by concretizing abstract method calls and expressions. Each state in the procedure represents a partially concretized sketch, with neighbors obtained by concretizing a single abstract method call or expression. The stochastic search procedure transforms sketches into programs by concretizing abstract method calls and expressions. Each state in the walk is a sample from a predefined distribution, prioritizing simpler programs. The walk ends when a state with no neighbors is reached. If the final state is fully concrete, it is returned as a sample. Otherwise, a new walk starts from the initial state. The concretization distribution is only defined for satisfiable sketches, and if not satisfiable, the walk ends with rejection. The stochastic search procedure transforms sketches into programs by concretizing abstract method calls and expressions. The walk ends with rejection if the final state has no neighbors. The method performs well due to the chosen language of sketches, limiting the search space. An empirical evaluation was conducted on 1500 Android apps, extracting 150,000 methods using Android APIs or Java library. The text discusses the conversion of Java code to AML, extracting sets of API calls, data types, and keywords from programs. It also mentions the selection of 10,000 programs for testing and validation data, implemented in a tool called BAYOU using TensorFlow BID1 for neural networks. Our approach in the tool BAYOU involved using TensorFlow BID1 for the GED neural model implementation and Eclipse IDE for abstraction from Java to sketch language. Cross-validation was performed through grid search to select the best model. Hyper-parameters included 64, 32, and 64 units in the encoder for API calls, types, and keywords respectively, with 128 units in the decoder. The latent space was 32-dimensional, with a mini-batch size of 50 and a learning rate of 0.0006 for the Adam optimizer. Training was done on an AWS \"p2.xlarge\" machine with an NVIDIA K80 GPU, taking 10 hours to complete. The model trained for 10 hours with a batch size of 50 on an AWS machine. Clustering in the 32-dimensional latent space was visualized using t-SNE BID17. The model neatly clustered APIs based on usage scenarios, such as reading versus writing. Prediction accuracy was evaluated by concretizing sketches into AML programs. The model trained on AWS for 10 hours and clustered APIs in a 32-dimensional latent space. Prediction accuracy was evaluated by converting sketches into AML programs and measuring equivalence using various metrics. In evaluating prediction accuracy, the program's Abstract Syntax Tree (AST) is compared using various metrics. These metrics measure the similarity in API call sequences, number of statements, and control structures between expected and predicted programs. Control structures include branches, loops, and try-catch statements. The study evaluated prediction accuracy by comparing the program's AST using different metrics. Control structures like branches, loops, and try-catch statements were considered. The testing data varied in observability percentages, affecting the number of items in labels. Different models were tested, including GED-AML and GSNN-AML trained over AML ASTs, and GED-Sk and GSNN-Sk trained over sketches. The Gaussian Stochastic Neural Network (GSNN) was implemented for comparison, with a decoder conditioned on input labels. The study compared different models for program generation, including GSNN with a decoder conditioned on input labels. The GSNN loss function included a KL-divergence term weighted by a hyper-parameter \u03b2. A model with similar hyper-parameters (\u03b2 = 0.001) was selected for evaluation. Another model was trained on production paths in ASTs of AML programs, with a larger decoder size (256 units) due to the complexity of AML grammar. The study compared different models for program generation, including GSNN with a decoder conditioned on input labels. A model trained on AML ASTs showed that 3% of the sampled ASTs were not well-formed and had to be discarded. The GED-Sk model performed best overall, with GSNN-Sk as a reasonable alternative. The GED-Sk model performs slightly better due to regularizing prior on Z, making it a suitable option for conditional program generation. GED-AML and GSNN-AML models perform worse, highlighting the importance of sketch learning. Evaluation on unseen data shows similar trends, with sketch learning models performing well. The GED-Sk model outperforms other models in generalization, especially in conditional program generation. Previous studies have focused on generating code snippets using n-grams and recurrent neural networks, but these models struggle to ensure semantic properties in generated programs. Some exceptions include log-bilinear tree-traversal models and probabilistic higher-order grammars. These papers, like our work, aim to generate programs that meet specific criteria. The curr_chunk discusses the potential of replacing the decoder and concretizer in program generation models with existing approaches, but highlights that it may not lead to optimal performance. It also compares the use of attention mechanisms in text-to-code models with BAYOU, emphasizing the difference in learning abstraction levels. BAYOU uses neural methods to generate higher-level sketches that are then translated into program code using symbolic methods, ensuring key semantic properties and abstracting away lower-level details to facilitate learning. Experimental evidence suggests this approach yields better results than direct code translation. In contrast, BID15 propose a variational autoencoder for context-free grammars, which can search for similar valid structures in molecular synthesis applications. In contrast to BAYOU's neural methods for generating higher-level sketches, recent work has focused on neural approaches to program synthesis, using examples to complete sketches. Neural techniques are used to complete user-provided sketches in neuro-symbolic synthesis and RobustFill. DeepCoder uses neural methods to speed up Flashfill program synthesis. Unlike other approaches, this method focuses on syntactic aspects of programs, allowing for generation without human sketches. The key difference lies in the use of data to generalize from minimal specification, combining learning and combinatorial techniques. Neural approaches to program synthesis focus on combining learning and combinatorial techniques. A method for generating type-safe programs in a Java-like language is presented, where a model predicts sketches of programs based on a label. These sketches are then turned into code using combinatorial techniques in the BAYOU system. Experiments show that the system can generate complex method bodies from minimal information, emphasizing the importance of learning at the sketch level for effective generation. The generator in this work is conditioned on uncertain syntactic information about the target. The generator in this work is conditioned on uncertain syntactic information about the target program, rather than hard constraints on semantics. Future work aims to condition program generation on syntactic labels and semantic constraints, with the goal of learning correlations between syntax and semantics in complex languages. Sketches could be generated with limited semantic information, and logic-based techniques could be used to ensure synthesized programs match the constraints. The AML language is designed to capture API usage in Java-like languages. It uses a finite set of API data types and method names with type signatures. The grammar for AML includes variable names, constants, and methods with return types. The AML language captures API usage in Java-like languages with variable names, constants, and method calls. Programs in AML include method calls, loops, branches, statement sequencing, and exception handling. The neural networks used by BAYOU are detailed in this section. The neural networks used by BAYOU implement an encoding function for labels, converting elements into one-hot vectors and mapping them into a d-dimensional space. The encoding function uses real-valued weight and bias matrices, with tanh as a non-linearity function. Matrices are learned during training for both API calls and types. The neural decoder implements a sampler for Y \u223c P (Y |Z) recursively via production rules in the grammar of sketches. It uses a top-down tree-structured recurrent neural network similar to BID35, defining production paths as sequences of node pairs connected by sibling or child edges. The neural decoder uses a tree structure with sibling and child edges to represent production paths in a grammar of sketches. The tree starts with a special node named root and ends with an irrelevant edge. An example sketch is shown in Figure 10. The neural decoder utilizes a tree structure with sibling and child edges to depict production paths in a sketch grammar. The tree includes hidden units in the decoder and output vocabulary size. Matrices for input weight, bias, output weight, and bias are used, along with \"lifting\" matrices to elevate vectors onto the decoder's hidden state space. Hidden states and outputs at different time points are computed using a non-linear activation function. The neural decoder utilizes a tree structure with sibling and child edges to depict production paths in a sketch grammar. It includes hidden units in the decoder and output vocabulary size. Matrices for input weight, bias, output weight, and bias are used, along with \"lifting\" matrices to elevate vectors onto the decoder's hidden state space. Hidden states and outputs at different time points are computed using a non-linear activation function. The quantities in Figure 11 involve tanh as a non-linear activation function and softmax for converting vectors to probability distributions. Training involves learning values for matrices W and b, optimizing a cross-entropy loss function, and inferring probability distributions during inference. The neural decoder uses a tree structure with sibling and child edges to represent production paths in a sketch grammar. Trees are generated in a depth-first manner, exploring child edges before sibling edges if both are possible. If a node has two children, information from the left subtree is carried onto the right subtree for generating the tree. The neural decoder utilizes a tree structure with sibling and child edges to represent production paths in a sketch grammar. Additional information about previously generated siblings can aid in the generation of the tree. Experimental evaluation results are provided in this section, including a visualization of the latent space of the GSNN-Sk model trained on sketches. The latent space is observed to be clustered more densely compared to another model. Usage scenarios for the system, BAYOU, are discussed, where API calls, types, or keywords are used as labels for generating code. Examples of generated code are presented and discussed. In the top-5 results returned by BAYOU, a program is generated to write to a file using FileWriter. The program uses a BufferedWriter for buffered file writes and correctly flushes the buffer before closing. In another scenario, BAYOU generates a program to set the title and message of an Android dialog box based on keywords provided. BAYOU generated a program to build an Android dialog box using AlertDialog.Builder, setting its title and message."
}