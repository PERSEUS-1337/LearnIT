{
    "title": "ByxBFsRqYm",
    "content": "The idea of learning heuristics for combinatorial optimization problems is promising for cost-saving development. A new model based on attention layers is proposed, along with a training method using REINFORCE with a deterministic greedy rollout. Significant improvements are shown for the Travelling Salesman Problem (TSP) and two variants of the Vehicle Routing Problem (VRP), achieving close to optimal results for problems up to 100 nodes. The Travelling Scientist Problem (TSP) is equivalent to the Travelling Salesman Problem (TSP), which is known to be (NP-)hard. However, a special algorithm is needed to solve this type of problem instance efficiently. As a machine learner, you wonder if your algorithm can be learned. Machine learning algorithms have replaced humans in solving various tasks, with Deep Neural Networks outperforming classic approaches in speech recognition, machine translation, and image captioning. Reinforcement Learning has enabled algorithms to learn to make decisions by interacting with an environment or inducing knowledge through look-ahead search. In the realm of machine learning, Deep Neural Networks have excelled in tasks like speech recognition and image captioning. Now, researchers aim to use DNNs to enhance algorithms for solving combinatorial optimization problems, specifically focusing on routing problems in this study. In this paper, the focus is on routing problems in combinatorial optimization. The proposal is to use a powerful attention-based model trained with REINFORCE and a greedy rollout baseline. The goal is not to outperform specialized algorithms like Concorde, but to show flexibility on multiple routing problems with a single set of hyperparameters. This approach aims to learn strong heuristics for practical problems where good heuristics do not exist. The use of Neural Networks (NNs) for combinatorial optimization dates back to Hopfield & Tank (1985). Recent advancements include the Pointer Network (PN) model trained offline to solve the TSP problem. Actor-Critic algorithms have been introduced to train the PN without supervised solutions, using cost as a training sample for policy estimation. The curr_chunk discusses different approaches to solving combinatorial optimization problems using neural networks. It mentions using the cost of a sampled solution for policy gradient estimation, introducing extra model depth in the decoder, replacing the LSTM encoder with element-wise projections, and training a single model based on graph embeddings. The models are applied to the Vehicle Routing Problem with split deliveries and a stochastic variant. Dai et al. (2017) use a 1-step DQN training approach. The curr_chunk discusses training methods for solving combinatorial optimization problems using neural networks. Nowak et al. (2017) train a Graph Neural Network to output a tour as an adjacency matrix, while Kaempfer & Wolf (2018) use the Transformer architecture to output a fractional solution. The curr_chunk focuses on the Transformer architecture for solving the multiple TSP problem, using a different decoder and training algorithm to improve results without 2OPT. The model is defined in terms of TSP but can be adapted for other problems by adjusting input, mask, and decoder context. The curr_chunk discusses defining a problem instance as a graph with nodes represented by features, with a fully connected graph structure. A solution is defined as a permutation of nodes, and an attention-based encoder-decoder model is used to select a solution. The encoder produces node embeddings, and the decoder generates the sequence of nodes one at a time. The encoder-decoder model selects a solution by generating a sequence of nodes one at a time. The decoder uses attention layers to update embeddings, with the final node embeddings used as input. The problem instance is defined as a graph with fully connected nodes, and the decoder context includes embeddings of the first and last node. The decoder in the encoder-decoder model utilizes attention layers to update embeddings, with node embeddings and graph embedding used as input. The decoder outputs nodes sequentially based on encoder embeddings and previous outputs. The decoder in the encoder-decoder model utilizes attention layers to update embeddings and outputs nodes sequentially based on encoder embeddings and previous outputs. During decoding, a special context node is added to the graph for efficiency, with final probabilities computed using a single-head attention mechanism. The context of the decoder at each time step includes the graph embedding and embeddings of the first and last nodes of the partial tour. Nodes that are already visited are masked, and a tour is constructed based on the example provided. The decoder in the encoder-decoder model utilizes attention layers to update embeddings and outputs nodes sequentially based on encoder embeddings and previous outputs. The context of the decoder at each time step includes the graph embedding, the previous node, and the first node of the tour. Nodes that are already visited are masked during decoding. The decoder in the encoder-decoder model uses attention layers to update embeddings and outputs nodes sequentially based on encoder embeddings and previous outputs. It computes final multi-head attention values for the context node and log-probabilities for output probabilities. The mechanism is similar to the encoder but more efficient, without skip-connections, batch normalization, or feed-forward sublayer. The model defines a probability distribution p \u03b8 (\u03c0|s) to sample a solution \u03c0|s. Training involves optimizing the loss function L(\u03c0) using the REINFORCE gradient estimator with a baseline b(s) to reduce variance. A learned value function critic v(s, w) can also be used. The proposed method involves using a rollout baseline similar to self-critical training to estimate the difficulty of an instance and relate it to the cost of the solution selected by the model. The baseline is defined as the cost of a solution from a deterministic greedy rollout of the policy defined by the best model so far. This approach aims to reduce variance in optimizing the loss function L(\u03c0) using the REINFORCE gradient estimator. The method involves using a rollout baseline to estimate instance difficulty and relate it to the selected solution cost. The baseline is the cost of a solution from a deterministic greedy rollout of the best model's policy. The baseline is stabilized by freezing the policy parameters for a fixed number of steps. The current training policy is compared with the baseline policy at the end of each epoch, and parameters are updated if there is a significant improvement. The model is trained to improve over its greedy self by comparing sampled solutions with the greedy rollout baseline. Sampling replaces tree search for exploration, and the model is rewarded for yielding improvement. Efficiency is maintained by sampling data and computing baselines per epoch using larger batch sizes. The model focuses on routing problems such as TSP and VRP, adjusting input and training on instances of different node sizes. Hyperparameters are kept consistent across all problems. The model is trained on routing problems like TSP and VRP, with consistent hyperparameters. Parameters are initialized uniformly, and training involves processing batches of instances. Training is stable and robust, with N = 3 layers in the encoder and a constant learning rate of \u03b7 = 10 \u22124. The model is trained on routing problems like TSP and VRP with stable and robust training involving batches of instances. Decoding strategy includes greedy decoding or sampling for solution quality improvement. Code in PyTorch is publicly available. The study evaluates the performance of the model on routing problems like TSP and VRP using either sampling or local search. The 'best possible solution' is reported for each problem, with run times compared on different hardware setups. The model is parallelizable, unlike most baselines, and run time can potentially be reduced through model compression. Our model and implementation show fast running times compared to others, with examples like sampling 1280 TSP solutions in less than one second on a 1080Ti GPU. We report optimal results for the TSP using Gurobi, Concorde BID0, and LKH3 solvers, comparing against various insertion methods. Our model demonstrates fast running times, such as sampling 1280 TSP solutions in less than one second on a 1080Ti GPU. We achieve optimal results for the TSP using Gurobi, Concorde BID0, and LKH3 solvers, comparing against different insertion methods. Additionally, we outperform traditional baselines and previous learned heuristics, getting significantly closer to optimal results. Our model outperforms the Encode-Attend-Navigate (EAN) code, even with 2OPT local search improvement. We consider the Capacitated VRP (CVRP) and Split Delivery VRP (SDVRP) problems, implementing datasets from Nazari et al. (2018) and comparing against their Reinforcement. Our model outperforms the Encode-Attend-Navigate (EAN) code, even with 2OPT local search improvement. Implementing datasets from Nazari et al. (2018), we compare against their Reinforcement Learning (RL) framework and the strongest baselines they report. Our greedy decoding method significantly outperforms their beam search in most cases, getting much closer to LKH3 (Helsgaun, 2017) in <1 second/instance. See Appendix C.4 for greedy example solution plots. The Orienteering Problem (OP) is an important problem used to model real-world scenarios, aiming to maximize the sum of prizes of nodes visited while being shorter than a maximum. Prize Collecting TSP (PCTSP) involves maximizing the sum of prizes of nodes visited while staying under a maximum length. Different prize distributions are considered, with Gurobi and Compass being top solutions. Our model outperforms Python GA and Tsiligirides' heuristic, while OR Tools struggles with feasibility for larger instances. In PCTSP BID3, nodes have prizes and penalties. The goal is to collect a minimum total prize while minimizing tour length and penalty sum of unvisited nodes. The Attention Model performs almost as well as OR Tools with 60s of local search but in less time. It also compares favorably to open-source C++ and Python implementations of ILS. The Stochastic variant of the PCTSP (SPCTSP) deals with uncertainty naturally by considering expected node prizes known upfront but real collected prizes only known upon visitation. The model constructs a tour one node at a time, ensuring the prize constraint is satisfied. An adaptive algorithm is necessary as fixed tours may fail to meet the constraint. A baseline algorithm plans a tour, executes part of it, and re-optimizes using the C++ ILS algorithm. Our model outperforms baselines for n = 20 by providing competitive solutions at a fraction of the computational cost. The failure of baselines to account for uncertainty in the prize may result in the need to visit additional nodes, which is costly for small instances but cheaper for larger n. The validation set shows the optimality gap as a function of the number of epochs for the Attention Model (AM) and Pointer Network (PN) with different baselines. The validation set compares the performance of the Attention Model (AM) and Pointer Network (PN) with different baselines. The AM outperforms the PN with any baseline, and the rollout baseline improves quality and convergence speed for both models. The PN with critic baseline did not reproduce the reported results, but our reproduction is closer. Comparisons against original results are shown in TAB1, with the exponential baseline being 20% faster per epoch compared to the rollout baseline. In this work, a model and training method are introduced to improve results on learned heuristics for TSP and multiple routing problems. The method shows promise for learning heuristics for other combinatorial optimization problems on graphs. The ability to learn heuristics using attention instead of recurrence (LSTMs) is introduced for increased learning efficiency and computational efficiency. The multi-head attention mechanism allows nodes to communicate relevant information in a graph-based model with increased scaling potential. The curr_chunk discusses the potential of using a graph-based method with sparsification for improved computational efficiency in addressing problems with feasibility constraints. It also mentions the use of heuristic learning and backtracking to further enhance the method's capabilities. Additionally, it highlights the attention mechanism as a weighted message passing algorithm in a graph. The curr_chunk explains how FORMULA9 functions as a weighted message passing algorithm in a graph, where nodes receive messages based on query-key compatibility. It involves computing dimensions for nodes and determining compatibility through dot-products and attention weights. The received vector by a node is a combination of messages from other nodes. The curr_chunk discusses the benefits of multiple attention heads in a multi-head attention mechanism, where different messages are received by nodes from various neighbors. It involves computing values with different parameters and projecting them back to a single vector. Additionally, it mentions the feed-forward sublayer and batch normalization techniques used in the process. The critic network architecture for TSP uses 3 attention layers similar to the encoder, followed by node embeddings averaging and processing by an MLP with 128 neurons. The heuristics implemented for TSP construct a single tour in one pass, with the nearest neighbor heuristic extending a partial solution one node at a time. The heuristics for TSP construct a tour by adding nodes one at a time, starting with the nearest node to the end of the partial path. The cheapest insertion cost is used to determine where each node is inserted in the tour. The insertion heuristic for TSP involves selecting the place of insertion between adjacent nodes to minimize insertion costs. Variants include nearest, farthest, and random insertion methods. Deudon et al. (2018) use embeddings of the last K = 3 visited nodes as context for the decoder, while this paper uses a different approach. The decoder in Deudon et al. (2018) uses embeddings of the last K = 3 visited nodes, while this paper uses a different approach by only using the last node and adding the first visited node. They also experiment with using a critic based on the Transformer architecture, but found that using a rollout baseline is more effective. Additionally, they report results without 2OPT local search, using either a single greedy solution or sampling 1280 solutions, which improves performance compared to BID4. In contrast to Deudon et al. (2018), this study explores a 'hybrid' approach combining learned algorithms with local search to improve results. It includes an empirical comparison of performance, specifically focusing on TSP and variants of VRP. The study also evaluates results under similar circumstances as Deudon et al., using greedy decoding and sampling 1280 solutions on a test dataset with the same generative procedure. The study compares their model with 2OPT, showing better performance even without 2OPT. They adjusted hyperparameters for fair comparison, using larger batch size and training steps. Training on n = 20 and n = 50 due to memory issues, reporting results for n = 100 from model trained on n = 50. Larger learning rate of 10^-3 works better with decay but may be unstable, while 10^-4 is more stable and does not require decay. Validation results over time for TSP20 and TSP50 with and without decay are shown. The method has not fully converged after 100 epochs, indicating potential for further improvement with longer training. Results for different learning rate schedules are compared, with N = 3 and N = 5 showing the best performance. The performance of the model is evaluated with N = 3 as a good trade-off between quality and computational complexity. Generalization testing on different sizes shows models specialize on trained problem sizes, with quality degrading as the difference increases. The trained model with the highest validation performance for each size is selected to create a strong overall algorithm. The Capacitated Vehicle Routing Problem (CVRP) is a generalization of the TSP where multiple routes start and end at a depot. Each route has a capacity D and nodes have demands that should not exceed the capacity. The Split Delivery VRP (SDVRP) allows nodes to be visited multiple times with only a subset of the demand needing to be fulfilled. The CVRP and SDVRP instances are generated for depot and node locations with normalized demands. The demands are sampled uniformly and normalized by capacities. Separate parameters are used for the depot node, and normalized demands are provided as input features for the Attention Model. Capacity constraints are maintained by tracking remaining demands and vehicle capacity. The decoder context for the VRP includes the current location and remaining capacity. The depot can be visited multiple times but not in subsequent timesteps. In the VRP decoder, capacity constraints are enforced by adjusting masking for depot and nodes. For depot j=0, u(c)0 is set to -\u221e if t=1 or \u03c0t-1=0. Nodes are masked based on remaining demand and capacity, with u(c)j set to -\u221e if \u03b4i,t=0 or \u03b4i,t>Dt. Split deliveries allow for any remaining demand value 0\u2264\u03b4i,t\u2264\u03b4i. When split deliveries are allowed, the remaining demand \u03b4 i,t can vary from 0 to \u03b4 i. This information is incorporated in the computation of keys and values in the attention layer and output layer of the decoder. The model's output length in the VRP depends on the number of times the depot is visited. The depot is visited multiple times in the VRP, with some regular nodes visited twice. The solution length is larger than n, requiring a batch size limit of 256 for n = 100 on 2 GPUs. Despite this limit, 2500 batches are processed per epoch, totaling 0.64M training instances. Example solutions for the CVRP with n = 100 are shown using greedy decoding. The model constructs routes from bottom to top, densely packed with some suboptimal stops. Further optimizations like beam search or TSP solver may improve results. Example greedy solutions for CVRP (n = 100) are shown. In the CVRP (n = 100), routes are generated with legends indicating stops, capacity, and distance per route. The Orienteering Problem (OP) aims to maximize total prize while staying below a maximum route length, allowing optional node visits. A special depot node is added, and infeasible solutions are prevented by only allowing node visits if a return to the depot is still possible within the length constraint. The Orienteering Problem involves visiting nodes with prizes in a unit square. Prizes are normalized between 0 and 1, with every node having the same prize. The goal is to visit as many nodes as possible within a length constraint, with prizes proportional to distance from the depot. The Orienteering Problem involves visiting nodes with prizes in a unit square. Prizes are normalized between 0 and 1, with every node having the same prize. The goal is to visit as many nodes as possible within a length constraint. The maximum length constraint is set to be approximately half of the average TSP tour length for uniform TSP instances with n nodes. This ensures that a little more than half of the nodes can be visited, making it more challenging. Maximum lengths T 20 = 2, T 50 = 3, and T 100 = 4 are fixed instead of adjusting per instance. The encoder uses separate parameters for the depot node embedding and includes the node prize as an input feature. The Orienteering Problem (OP) involves visiting nodes with prizes in a unit square. To satisfy the max length constraint, the remaining max length T at time t is tracked. The decoder context includes the current/last location and the remaining max length T. Masking is used to determine which nodes can be visited in the OP. The depot node can always be visited, while regular nodes are masked if they are already visited or cannot be visited. The Orienteering Problem (OP) involves visiting nodes with prizes in a unit square. Tsiligirides (1984) describes a heuristic procedure for solving the OP by sampling tours through a randomized construction procedure and applying local search. The final heuristic uses a formula with multiple terms to define node selection probability, but a simpler form works best. Comparing this heuristic to a model without local search shows the difficulty of the problem. The Orienteering Problem involves visiting nodes with prizes in a unit square. Tsiligirides (1984) describes a heuristic procedure for solving the OP by sampling tours through a randomized construction procedure and applying local search. The final heuristic uses a formula with multiple terms to define node selection probability, but a simpler form works best. By tuning the weights, the form with only one simple term works best, showing the difficulty of manually defining a good probability distribution. The heuristic defines a score for each node at time t as the prize divided by the distance from the current node, raised to the 4th power. The node probabilities at time t are defined based on the set of unvisited nodes with maximum score. For the Google OR Tools implementation, modifications are made for the CVRP, such as replacing the Manhattan distance by the Euclidean distance and setting the number of vehicles to 1. Constraints are added for individual nodes and the capacity constraint is replaced by a maximum distance constraint. The objective to minimize the length is removed. In the Prize Collecting TSP (PCTSP), each node has a prize and penalty. The goal is to minimize tour length and penalties for unvisited nodes while collecting a minimum total prize. Results show that constant and uniform prize distributions are easier to compute compared to distance-based distributions. In the Prize Collecting TSP (PCTSP), the problem involves minimizing tour length while collecting a minimum total prize. A special depot node is added, and penalties are incurred for unvisited nodes. It is beneficial to visit additional nodes to avoid penalties, even if the minimum total prize constraint is met. The depot and node locations are randomly sampled, and the distribution of prizes and penalties is chosen so that approximately half of the nodes should be visited. In the Prize Collecting TSP (PCTSP), the problem involves minimizing tour length while collecting a minimum total prize. A special depot node is added, and penalties are incurred for unvisited nodes. It is important to balance prizes and penalties in node selection to ensure a meaningful solution. In the Prize Collecting TSP (PCTSP), penalties should contribute to the objective equal to the tour length. Sampling is done to balance prizes and penalties, with separate parameters for the depot node. A minimum total prize constraint is maintained by tracking remaining total prize to collect at each node. In the Stochastic PCTSP, real prizes are assumed to follow a uniform distribution. The Attention Model is applied by updating the remaining prize to collect using the real prize collected at each node. Retraining the model allows for learning optimal node selection based on expected prizes and real prize probabilities. In the Stochastic PCTSP, real prizes are assumed to follow a uniform distribution. A strategy is implemented that plans a tour using expected prizes, executes part of the tour to observe real prizes, and computes the remaining total prize needed. This approach helps reduce variance and accounts for the infinite scenarios in constructing an optimal tour online. The strategy involves planning a tour using expected prizes, observing real prizes, computing remaining total prize, and repeating the process until minimum total prize is collected or all nodes are visited. The ILS C++ algorithm is used for solving a deterministic PCTSP, with a variant where separate start and end points are used instead of a single depot. The strategy involves planning a tour with a start and end node, adjusting distances asymmetrically, and removing visited nodes from consideration. Three variants are considered based on the number of nodes visited before replanning the tour, balancing adaptivity and run time."
}