{
    "title": "ryZ283gAZ",
    "content": "Deep neural networks like ResNet, PolyNet, FractalNet, and RevNet can be seen as numerical discretizations of differential equations. This insight allows for the design of more effective deep architectures, such as the linear multi-step architecture (LM-architecture) inspired by solving ordinary differential equations. This approach can enhance ResNet-like networks like LM-ResNet and LM-ResNeXt. LM-ResNet and LM-ResNeXt show higher accuracy than ResNet and ResNeXt on CIFAR and ImageNet with fewer parameters. They can compress the networks by over 50% while maintaining performance. The concept of modified equations from numerical analysis explains this mathematically. Stochastic control and noise injection in training improve network generalization. Stochastic training strategies can be applied to networks with the LM-architecture, such as introducing stochastic depth to LM-ResNet. Network design in deep learning aims to grant networks with strong generalization power using as few parameters as possible. ResNet introduced skip connections to prevent gradient vanishing, while other structures like dense connections, fractal path, and Dirac initialization were also introduced for the same purpose. In attempts to improve image classification accuracy, modifications to ResNet's residual blocks have been explored. BID49 suggested doubling the layers for a slight accuracy boost, while BID51 emphasized the importance of exploring structural diversity in network design. Authors in BID43, BID51, BID47, and BID19 enhanced network accuracy by carefully designing residual blocks through width adjustments and topology changes. The empirical nature of network design remains prevalent in the literature. In the literature, network design is mainly empirical. ResNet's residual blocks can be seen as discretizations of ODEs, connecting deep networks with skip connections to dynamic systems. State-of-the-art architectures like PolyNet, FractalNet, and RevNet efficiently approximate dynamic systems. Differential equations are powerful tools in low-level computer vision. Equations are powerful tools in low-level computer vision for tasks like image denoising and segmentation. The connection between deep neural network architectures and numerical approximations of ODEs can lead to more effective designs. A new linear multi-step architecture (LM-architecture) inspired by numerical ODE methods shows improvements in ResNet and ResNeXt models on CIFAR and ImageNet datasets. In this work, the concept of introducing randomness through noise injection in deep residual networks is explored. It is shown that any ResNet-like network with noise injection can be seen as a discretization of a stochastic dynamic system, providing a unified explanation for stochastic learning processes. By connecting stochastic training strategies with stochastic dynamic systems, stochastic training can be easily applied to networks with the proposed LM-architecture. For example, stochastic depth is introduced to LM-ResNet, resulting in significant improvements on CIFAR10. The link between ResNet and ODEs was first observed by E (2017), bridging ResNet with RNN as an approximation of dynamic systems. ResNet is seen as characteristic lines of a transport equation on the data set distribution. Reversible architectures were designed for stability in dynamic systems. Deep network designs were inspired by optimization algorithms like LISTA and ADMM-Net, which can be seen as discretizations of ODEs, with gradient flow being a simple example. Recent attempts have been made to combine deep learning with partial differential equations (PDEs) for computer vision tasks, aiming to balance handcrafted modeling and data-driven modeling. Linear combinations of handcrafted PDE-terms have been used, along with optimal control methods to learn coefficients. An extended model, L-PDE, was proposed for classification tasks, where the dynamics generated by passing data through the network are interpreted as characteristic lines of a PDE on the data set distribution. Using spatial differential operators in the network is not essential for classification tasks. In recent work, BID28 and BID34 proposed networks for image denoising and learning evolution PDEs from sequential data, respectively. These networks use trainable convolution kernels and radio basis functions to approximate nonlinear diffusivity functions. Our focus is on a different perspective, not requiring the ODE u t = f (u) for optimization. The curr_chunk discusses the connection between deep networks and numerical ODEs, leading to the design of more effective networks like the LM-architecture for ResNet and ResNeXt. It also explains why dropping out residual blocks in ResNet can improve accuracy, as each block represents a step in the discretized ODE. The curr_chunk introduces the concept of the linear multi-step architecture (LM-architecture) inspired by numerical ODEs, which can be applied to ResNet-like networks for improved performance on CIFAR and ImageNet datasets. The PolyNet model, proposed by BID51, enhances network performance by introducing a PolyInception module in each residual block. This module includes polynomial compositions that approximate the backward Euler scheme, allowing for a larger step size and fewer residual blocks. This design reduces depth by increasing the width of each block, leading to state-of-the-art classification accuracy. FractalNet BID27 FIG0 is a deep network designed based on self-similarity, resembling the Runge-Kutta scheme in numerical analysis. RevNet FIG0 proposed by BID12 is a reversible network that does not store activations during forward propagations. The LM-architecture is a new structure based on linear multi-step scheme in numerical ODEs. It includes trainable parameters for each layer and can be interpreted as a discrete approximation of dynamic systems. The architecture is illustrated in Figure 2. The LM-architecture, a 2-step method approximating ODEs, can be applied to ResNet-like networks. LM-ResNet and LM-ResNeXt were trained on CIFAR and ImageNet, showing improvements over the original models. Implementation details include training on CIFAR10, CIFAR100, and ImageNet datasets with data augmentation. For training on CIFAR, data augmentation involves padding 4 pixels on each side and randomly sampling a 32\u00d732 crop from the image. Testing evaluates the single view of the original 32\u00d732 image. On ImageNet, images are resized with the shorter side randomly sampled in [256, 480] for scale augmentation. The input image is 224\u00d7224 randomly cropped from a resized image using scale and aspect ratio augmentation. ResNet/LM-ResNet on CIFAR use a two-layer neural network as the residual block. LM-ResNeXt uses a bottleneck structure of 1 \u00d7 1, 64 3 \u00d7 3, 64 1 \u00d7 1, 256. The LM-architecture parameters are initialized by random sampling. SGD is used for training with specific batch sizes and weight decay values. Learning rates are adjusted during training epochs for LM-ResNet and LM-ResNeXt on CIFAR and ImageNet datasets. LM-architecture enables ResNet to achieve high accuracy with fewer parameters on CIFAR10. LM-ResNet shows improvements over ResNet on CIFAR10 with varying layers. LM-ResNeXt also performs well on CIFAR100. LMResNeXt with pre-activation achieves lower testing errors than the original ResNeXt without pre-activation. Training and testing curves of LM-ResNeXt are plotted. Testing errors of FractalNet and DenseNet on CIFAR100 are also presented. LM-ResNet and LM-ResNeXt outperform FractalNet and DenseNet BID20 on CIFAR 100. LM-ResNet shows improvement over ResNet on ImageNet with comparable parameters. Results show that LM-architecture can compress ResNet/ResNeXt without losing performance. The concept of modified equations from numerical analysis justifies the performance boost of numerical schemes approximating differential equations. For example, LM-ResNeXt achieves higher accuracy than ResNeXt on CIFAR100. The Lax-Friedrichs and Lax-Wendroff schemes approximate the transport equation with associated modified equations. The Lax-Friedrichs and Lax-Wendroff schemes approximate the transport equation with modified equations. The forward Euler scheme associated with ResNet has a modified equation that introduces dispersion. Comparing different numerical schemes, acceleration leads to faster convergence when certain conditions are met. The LM-architecture introduces dissipation to speed up flow. The network accelerates at the end with negative coefficients close to -1. Stochastic learning strategies can be seen as approximations to a stochastic dynamic system, shedding light on guiding principles for training. To demonstrate the advantage of bridging stochastic dynamic system with stochastic learning strategy, stochastic depth is introduced during training of LM-ResNet. Results show that networks with this LM-architecture can benefit from stochastic learning strategies, which can be considered as weak approximations of stochastic dynamic systems. Shake-Shake Regularization, introduced by Gastaldi (2017), involves a stochastic affine combination of multiple branches in a residual block. This regularization can be expressed as a stochastic dynamic system with a time step size, reducing to shake-shake regularization under certain conditions. The shake-shake regularization is a weak approximation of the stochastic dynamic system. Stochastic depth involves randomly dropping out residual blocks during training to improve network robustness. Incorporating \u2206t in training reduces to the original stochastic drop out training when \u2206t = 1. The variance of the training process is 1. The stochastic depth training strategy extends to networks with the proposed LMarchitecture, approximating a stochastic control problem with running cost. The assumption (1 \u2212 2p n ) = O( \u221a \u2206t) suggests setting p n closer to 1/2 for deeper blocks of the network. In applying the theory of It\u00f4 process to the LM-structure, the 2nd order equation is rewritten as a 1st order ODE system. This leads to a stochastic training strategy for LM-architecture, where residual blocks are randomly dropped out with a certain probability. Implementation details involve testing LM-ResNet with this strategy on CIFAR10 dataset, using hyper-parameters from a previous study. The stochastic training strategy for LM-architecture involves randomly dropping out residual blocks with a certain probability. During training, the dropping out probability is set based on the current layer and previous layer. The initial learning rate, weight decay, and momentum values are also specified. LM-ResNet110 with stochastic depth achieved a 4.80% testing error on CIFAR10, outperforming the ResNet1202 model. The stochastic training strategy involves randomly dropping out residual blocks with a certain probability to introduce diffusion for information gain and robustness in dynamic systems. Numerical ODEs are used to approximate the time derivative, with forward and backward Euler schemes for stability. The backward Euler scheme is more stable than the forward Euler, requiring solving a nonlinear equation at each step. Runge-Kutta methods offer higher order accuracy by using intermediate approximations and adjustable coefficients. Linear multi-step methods extend the forward Euler scheme to higher orders with explicit formulations. The linear multi-step method is explicit if \u03b2 0 = 0, used in designing the structure. Following the setting of Kesendal and BID9, the definition of Brownian motion is given as a stochastic process satisfying certain assumptions."
}