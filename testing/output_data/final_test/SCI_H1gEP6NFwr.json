{
    "title": "H1gEP6NFwr",
    "content": "In this work, the concept of 'ease-of-use' for optimization methods is defined by the tunability of an optimizer, measured by how easily good hyperparameter configurations can be found using automatic random hyperparameter search. Adam is identified as the most tunable optimizer for the majority of problems, especially with a limited budget for hyperparameter tuning. Various first-order stochastic optimizers with different algorithmic components like momentum and adaptive learning rates have been popular in deep learning applications. The choice of optimizer is crucial in machine learning, with factors like generalization performance and resource efficiency to consider. Optimizer performance is heavily influenced by hyperparameter values, such as the learning rate. Recent debates in the research community highlight the importance of hyperparameter tuning in achieving optimal model performance. Automated machine learning (AutoML) emphasizes automatic hyperparameter optimization for better model performance. Optimizers should achieve good results with minimal tuning effort, rather than requiring extensive fine-tuning. The cost of tuning hyperparameters should be considered when evaluating optimizer performance. The concept of tunability in automated machine learning (AutoML) involves optimizing hyperparameters to achieve good performance while considering the cost of tuning. However, there is no standard way to measure tunability, hindering fair comparisons between optimization techniques. Previous studies have focused on measuring tunability based on the best performance obtained or the improvement in performance through hyperparameter tuning. The experimental settings for popular optimizers involve tuning hyperparameters like learning rate, momentum, and weight decay coefficient on various datasets and network architectures. Different tuning methods and performance metrics are used, making direct comparisons challenging. In this paper, a fair evaluation protocol for tunability based on automatic hyperparameter optimization is introduced. The study compares the performance of optimizers under varying resource constraints across 9 diverse tasks, favoring adaptive gradient methods over SGD variants for overall results. The study compares the performance of optimizers under varying resource constraints across 9 diverse tasks, favoring adaptive gradient methods over SGD variants. It concludes that adaptive gradient methods have substantial value, considering the difficulty in finding a good hyperparameter configuration and the absolute performance of the optimizer. The need to balance both aspects is illustrated with an example in Figure 1. In Section 5, a new formulation for hyperparameter optimization (HPO) is presented to address the challenge of balancing the ease of finding good minima and the difficulty in finding the optimum. The study uses Random Search algorithm for HPO experiments and emphasizes the importance of tunability in characterizing HPO's performance. The study introduces a new approach for hyperparameter optimization (HPO) focusing on balancing the search for good minima and the optimal solution. It emphasizes the significance of tunability in evaluating HPO's effectiveness. The quantification of this concept is illustrated with two optimizers, E and F, showing that F performs better in most parameter spaces, making it easier to find configurations that perform well without prior knowledge. This highlights the choice between achieving 'good enough' performance with fewer searches (F) or investing computational time for the best results. In this work, the concept of tunability in hyperparameter optimization is explored through the use of two optimizers, E and F. The difference lies in valuing results from late stages (E) versus early stages (F) of the optimization process. A new metric, \u03c9-tunability, is proposed as a weighted sum of the best performance attained at each iteration of the algorithm. In this study, the concept of tunability in hyperparameter optimization is examined using two optimizers, E and F, which prioritize late and early stage results respectively. A new metric, \u03c9-tunability, is introduced as a weighted sum of the best performance achieved at each iteration. Different weighting schemes are explored to evaluate optimizer performance with varying budgets of iterations. In this study, three additional weighting schemes are employed to evaluate optimizer performance in hyperparameter tuning. The schemes include Cumulative Performance-Early (CPE), Cumulative Performance-Late (CPL), and Cumulative Performance-Uniform (CPU). The comparison is made between adaptive gradient methods (Adagrad and Adam) and non-adaptive methods (SGD and SGD with momentum). The default version of Adam is also tested with only the initial hyperparameters being tuned. In addition to tuning the initial learning rate, a default version of Adam is employed with other hyperparameters set to recommended values. Similar experiments are conducted with SGD with momentum, including fixed momentum values and weight decay. Two additional experiments are devised for SGD with weight-decay, tuning weight-decay along with momentum or fixing it to a specific value. To prevent bias towards a specific optimizer, the number of training epochs can be added as a hyperparameter. Validation set performance is used as a stopping criterion, stopping training when validation loss plateaus for more than 2 epochs or exceeds the maximum number of epochs. Random Search is used for hyperparameter optimization, requiring distributions of random variables for sampling. Optimizing hyperparameters involves sampling from distributions of random variables. Poor choices can impact performance and break requisite properties. Some parameters have obvious bounds due to mathematical properties or optimizer design. A method is devised to determine priors by training tasks with various hyperparameter samplings and fitting distributions using maximum likelihood estimation. The study focuses on optimizing hyperparameters by fitting distributions using maximum likelihood estimation. The distributions for learning rate, momentum, and \u03b2s in Adam are discussed, with parameters reported in Table 2. Tunability of optimizers' hyperparameters is assessed using DEEPOBS, with details of architectures and datasets provided in Table 3. In Table 3, architectures are detailed. An LSTM network for sentiment classification in the IMDB dataset is added for balance between vision and NLP. Performance of hyperparameter search methods depends on its own hyperparameters. 100 configurations are evaluated with each method, using random search for multiple runs. Tunability for proposed weighting schemes is analyzed, showing performance and variance for increasing values of K in Figure 3. In Figure 3, the performance and variance of different optimizers are compared for various tasks. SGD with momentum and weight decay performs similarly to Adam in most cases, but AdamLR outperforms SGD in VAE and IMDB tasks. For image classification, SGD variants excel. The variance decreases in the order of SGDMW, Adam, Adagrad, AdamLR for K=4. The best optimizer choice for a given HPO budget is discussed in Appendix D. The study compares different optimizers for various tasks, showing that no single optimizer is best across all schemes. SGD performs better than adaptive gradient methods for image classification tasks. AdamLR performs well in some tasks, while SGDMCWC performs best in others. Overall, SGD variants are better for peak performance, while AdamLR is competitive for good performance in early hyperparameter search iterations. In earlier hyperparameter search iterations, AdamLR is competitive and default parameters of Adam optimizer perform well. Adam is rarely a better alternative over AdamLR, except for training Inception networks where a specific parameter setting is recommended. AdamLR requires fewer epochs than SGDMC for CIFAR-10, making it faster in wall-clock time. Wall-clock time can be a practical hyperparameter tuning budget consideration. In the study, the performance of different optimizers during the hyperparameter optimization process was analyzed. AdamLR showed competitive performance, especially in the early stages, while SGD variants improved over time. AdamLR performed well in comparison to other optimizers, with AdamLR being the best performer up to the 60th iteration. Few studies have formally defined and investigated tunability in this context. Few studies have formally defined and investigated tunability in the context of hyperparameter optimization for ML algorithms. Mantovani et al. (2018) and Probst et al. (2019) define tunability as the impact of hyperparameter tuning on decision tree models and ML algorithms, respectively. Schneider et al. (2019) assess tunability by evaluating the sensitivity of performance to changes in the learning rate. The study by Shah et al. (2018) and Choi et al. (2019) both discuss the importance of hyperparameter tuning in determining the superiority of optimizers. They highlight the need for a specific hyperparameter tuning protocol and optimizer-specific search space to accurately compare optimizer performance. The authors argue that the search space should be chosen optimizer-specific for optimal performance, without considering the tuning process. They manually select search spaces per dataset, unlike the AutoML scenario. Tunability involves measuring hyperparameter importance, with recent interest in robust optimizers like the APROX family. Experimentally, SGD converges only for a small Residual network trained on CIFAR-10. Our work introduces the concept of tunability for optimizers, highlighting that Adam shows better robustness to learning rate choices compared to SGD when training on CIFAR-10. Adaptive gradient methods are easier to tune than non-adaptive methods, with tuning only Adam's learning rate being a practical choice in low-budget hyperparameter tuning scenarios. While SGD may yield the best performance in some cases, finding its optimal configuration is challenging, whereas Adam often performs closely to it. This emphasizes the value of adaptive gradient methods in optimization tasks. The substantial value of adaptive gradient methods, specifically Adam, lies in its amenability to hyperparameter search. Unlike previous findings, our experiments show the importance of tuning Adam's parameters for better performance. Our study is not exhaustive, but we hope it encourages future research on optimizer performance from a holistic perspective. In addition to architectures examined by Schneider et al. (2019), an extra network and dataset were included in the hyperparameter search. A 32-dimensional word embedding table and a single layer LSTM with memory cell size 128 were used in the model. The IMDB sentiment classification dataset was utilized, consisting of 50,000 movie reviews split into training and test sets. 20% of the training set was allocated as the development set. For more details on other architectures, refer to DEEPOBS (Schneider et al., 2019). Additional methods for analyzing tunability are also provided. The \u03b1-tunability metric measures the ease of tuning an optimizer to reach desired performance levels. It is calculated as the ratio of the number of times a neural network needs to be retrained with optimizer's hyperparameters provided by an automatic method to the total budget of configurations tested. Sharpness is defined as the difference in \u03b1-tunability values for different performance levels, indicating the flatness of the minima in the optimization space. The \u03b1-tunability metric measures the ease of tuning an optimizer to reach desired performance levels. Sharpness quantifies the flatness of the minima in the hyperparameter space. Optimizer's \u03b1-tunability depends on how fast it can reach peak performance. The top performance does not always imply lower sharpness, as seen in the case of IMDB Bi-LSTM with SGDM C W C having lower sharpness but AdamLR achieving better performance. The study compares the performance of different optimizers in hyperparameter optimization. AdamLR consistently outperforms other optimizers for most problems, while SGD variants perform better for CIFAR-100 and Char-RNN. The probability of each optimizer finding the best configuration is computed and plotted in Figure 8. In hyperparameter optimization, AdamLR is consistently the best choice for problems like VAEs and LSTM. Adaptive gradient methods are more tunable, especially in constrained HPO budget scenarios. The analysis incorporates the number of epochs per configuration to measure tunability, considering the minimum total epochs for running all trials across different optimizers. In hyperparameter optimization, AdamLR is the top choice for VAEs and LSTM. Adaptive gradient methods are highly tunable, particularly in limited HPO budget situations. Tunability is measured by the number of epochs per configuration, with a focus on the minimum total epochs needed for all trials across various optimizers. The definition of \u03c9-tunability is modified based on the maximum budget, divided into intervals, and performance metrics are compared within each interval. In hyperparameter optimization, AdamLR is the top choice for VAEs and LSTM due to its tunability in limited HPO budgets. Comparing optimizer performances in Table 8, Adam shows strengths over SGD variants, especially in cases where Adam was already better. The adaptive gradient methods tend to converge faster, leading to fewer epochs needed for optimization. For VAEs and LSTM, AdamLR is the preferred optimizer for hyperparameter optimization with limited budgets. Adam outperforms SGD variants, converging faster and requiring fewer epochs for optimization. The stacked area plots illustrate the likelihood of each optimizer yielding the best result based on the tuning budget. For the IMDB LSTM problem, 'AdamLR' is optimal for small budgets, while tuning additional parameters of 'Adam' is beneficial for larger budgets exceeding 50."
}