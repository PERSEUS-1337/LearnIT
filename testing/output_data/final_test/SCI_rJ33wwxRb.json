{
    "title": "rJ33wwxRb",
    "content": "Neural networks show good generalization in the over-parameterized regime, where parameters exceed observations. Current generalization bounds for neural networks do not explain this. Studying learning in a two-layer over-parameterized neural network with data from a linearly separable function, we offer optimization and generalization guarantees for networks with Leaky ReLU activations. Convergence rates of SGD to a global minimum are proven, along with generalization guarantees independent of network size. The use of SGD in over-specified neural network classifiers can find a global minimum and avoid overfitting. Despite the high model capacity, SGD can learn networks that generalize well in overparameterized settings. This phenomenon remains largely unexplained in the success of neural networks. In over-parameterized neural networks, SGD can find models with low training error and good generalization performance, suggesting an inductive bias towards low complexity solutions. This bias needs to be characterized to understand generalization guarantees. This study focuses on a binary classification setting where SGD optimizes a two-layer network to learn a linearly separable function. In the context of over-parameterized neural networks, the weights of the second layer remain fixed while only the first layer is updated during training. Despite the network's capacity to overfit data, using SGD as an optimization method introduces an inductive bias that enables the network to generalize well. This scenario serves as a valuable test case for studying the impact of over-parameterization in machine learning. We consider a distribution over linearly separable points with a training set sampled i.i.d. from D. A two-layer neural network with hidden units is used, with fixed second layer weights. The network output is defined by a non-linear activation function. The empirical loss over the training set is the mean hinge-loss. In the context of a two-layer neural network with fixed second layer weights, the objective is to minimize the loss function L S (W) for positive homogeneous activations. The optimization problem aims to find the weight matrix W that satisfies the condition L S (W) = 0 for Leaky ReLU and ReLU activations. The results hold for networks with an even number of hidden neurons. Bias terms in the first layer can be accounted for by adding an extra dimension to the input. The network can be expressed with positive homogeneous activations and weight vectors. The network with an even number of hidden neurons can be represented with positive homogeneous activations and weight vectors. The network minimizes L S (W) using an SGD algorithm with batch size 1, updating only the weights of the first layer. SGD randomly selects a point (x t , y t ) \u2208 S at iteration t and updates the weights with a constant learning rate \u03b7. The update at iteration t is determined by \u2202 \u2202W L {(xt,yt)} (W t\u22121 ) = 0. Notation is defined for incoming weight vectors of neurons at each iteration. The Leaky ReLU function \u03c3(z) = max{\u03b1z, z} is used in the context of a neural network with an even number of hidden neurons. The study shows that SGD can find a global optimum of L S (W) with a non-convex function, making a limited number of non-zero update steps. Additionally, a lower bound for the number of non-zero updates is proven. The issue of generalization in large networks is also addressed, highlighting the potential for overfitting. When using Leaky ReLU activation and linearly separable data, SGD optimization provides guarantees of convergence to a global minimum with good generalization behavior. The model learned by SGD has a generalization error of O(M log n). Despite the presence of multiple global minima that can overfit the training set, SGD biases towards solutions expressible by a small set of training points, leading to good generalization. In the over-parameterized setting with ReLU activation, there are spurious local minima and SGD may not converge to a global minimum. Overparameterization can be beneficial for optimization, with small hidden neuron numbers leading to convergence to a local minimum with high probability. In the over-parameterized setting with ReLU activation, small hidden neuron numbers may not converge to a global minimum with high probability, whereas a large number of hidden neurons can lead to convergence to a global minimum with high probability. The paper discusses related work, proves convergence bounds, provides generalization guarantees, and presents results for the ReLU activation. Various notions of complexity have been studied, but none offer provable guarantees for the generalization performance of over-parameterized networks trained with gradient-based methods. In a recent paper, Dziugaite & Roy numerically optimize a PAC-Bayes bound of a stochastic over-parameterized network in a binary classification task to obtain a nonvacuous generalization bound. This analysis results in generalization bounds that are independent of the network size and hold for over-parameterized networks. Stability bounds for SGD in non-convex settings were provided in previous works, but they focused on smooth loss functions. Our work considers non-smooth activation functions like Leaky ReLU and ReLU, leading to different challenges. Other studies have looked at generalization of neural networks in a model recovery setting, but they did not deal with over-parameterized networks like we do. Our analysis focuses on the optimization landscape of over-parameterized networks, providing convergence guarantees for gradient descent to a global minimum under certain conditions. Unlike previous works, we do not assume anything about the distribution of feature vectors. In recent work, BID19 showed that for linearly separable training points, every critical point of the loss function is a global minimum under certain conditions on the weight matrices of a fully-connected neural network. This extends previous results in BID9, BID7, and BID25. The current work differs by providing global convergence guarantees of SGD, generalization bounds, and considering non-differentiable activation functions like Leaky ReLU and ReLU. In Section 5.1, it is shown that SGD converges to a globally optimal solution with leaky ReLU activation. The results are significant as they demonstrate convergence for a non-convex objective and provide lower bounds on the rate of convergence. Additionally, every critical point is proven to be a global minimum before proving convergence to a global minimum. SGD converges to a global minimum after performing at most M k non-zero updates. The proof is similar to the Perceptron convergence proof but with adjustments for the non-linear architecture. The full proof is in the Appendix. SGD converges to a global minimum after performing at most M k non-zero updates. The bound obtained in Theorem 2 can be simplified by setting R, v arbitrarily. The bound consists of two terms, one depending on the margin and the other scaling inversely with \u03b7, independent of network size. The lower bound in Theorem 4 states that for any d, there exists a sequence of linearly separable points on which SGD will make at least \u2126 w * \u03b7 + w * 2 mistakes. The lower bound in Theorem 4 shows that there is a sequence of linearly separable points where SGD will make at least \u2126 w * \u03b7 + w * 2 mistakes. This bound indicates that the upper bound in Corollary 3 cannot be significantly improved. Additionally, the example in the proof of Theorem 4 illustrates that \u03b7 \u2192 \u221e can be optimal for optimization and generalization, resulting in the minimum number of updates (w * 2) and an equivalent learned model to the true classifier w *. This observation is used to discuss the dependence of the generalization bound in Theorem 6 on \u03b7. The provided bounds in this section assume constant weights in the second layer during training, which does not limit the network's expressive power but updating both layers can change the problem dynamics. The upper bound in Section 9.1.2 is relaxed to allow for changing weights in the second layer during training. Experimental results in Figures 2 and 3 confirm that performance remains consistent with appropriate learning rates. SGD performance is unaffected by the choice of learning rate. In this section, generalization guarantees for SGD learning of over-parameterized networks with Leaky ReLU activations are discussed. The results show that SGD effectively avoids overfitting models and finds those that generalize well. Theoretical analysis of training both layers is left for future work. Theorem 5 states that with probability at least 1 - \u03b4, if n \u2265 2c k, the true risk of SGD k (S, W 0) and empirical risk of SGD k (S, W 0) on a set V can be derived easily. This observation is based on a compression scheme for hypothesis class H k, showing that SGD effectively avoids overfitting models and finds those that generalize well. With n \u2265 2c k and probability at least 1 - \u03b4, SGD converges to a global minimum of L S with low test error. The sample complexity guarantee is independent of network size, despite the presence of global minima with high test errors. SGD's inductive bias directs it towards global minima with low test error, as demonstrated empirically in FIG0. Theoretical results show that SGD converges to a global minimum with good generalization for a linearly separable dataset. The generalization bound is optimal for large learning rates, although in practice, using a large learning rate may not be ideal. The bound holds for any learning rate and is applicable to realistic applications of SGD. Theorem 7 states that for sufficiently large networks, the optimization problem can have bad global minima that do not generalize well on a test set. This is shown in the context of using the ReLU activation function. For a sufficiently small network, SGD will converge to a local minimum with high probability, while for a sufficiently large network, SGD will converge to a global minimum. The construction of a network with weight parameter W ensures that for linearly separable examples, there exists a local minimum point with specific properties. The main result of this section is that SGD will converge to a non-global minimum depending on the initialization, with the convergence occurring if there exists a point x such that for all neurons, the initialized weight vector w satisfies w, x \u2264 0. Theorem 9 states that running SGD with examples from a specific set S will lead to convergence to a non-global minimum point with high probability. The text discusses the convergence of SGD to global and non-global minimum points in over-parameterized neural networks. It provides provable guarantees for generalization performance in a setting where data is linearly separable and the network uses Leaky ReLU activation. The theorem shows that with certain conditions, SGD can converge to a global minimum point. SGD compresses output in over-parameterized networks with Leaky ReLU activations, leading to good generalization. Analysis doesn't apply to networks with ReLU activations due to spurious local minima. Convergence guarantees and generalization bounds for this case are of interest for future research. The analysis focuses on over-parameterized networks with Leaky ReLU activations under the assumption that weights of the second layer do not change signs and are bounded. It aims to show that for any critical point, the function is not convex and a global minimum is reached when yN W (x) \u2265 1 for all (x, y) \u2208 S. The study also aims to demonstrate that the number of non-zero updates in Stochastic Gradient Descent (SGD) is bounded. The analysis focuses on over-parameterized networks with Leaky ReLU activations, aiming to show that for any critical point, the function is not convex and a global minimum is reached when yN W (x) \u2265 1 for all (x, y) \u2208 S. The study also demonstrates that the number of non-zero updates in Stochastic Gradient Descent (SGD) is bounded, with the update rule given by certain equations. The text discusses classifying MNIST images with over-parameterized networks, training both layers, and choosing appropriate learning rates. The analysis shows that for any critical point, the function is not convex, and a global minimum is reached under certain conditions. The study also bounds the number of non-zero updates in Stochastic Gradient Descent. In Section 9.1.2, the proof discusses the train and test errors of MNIST classification for different network sizes and learning rates. SGD shows similar performance in training and generalization. The second layer weights remain above zero, satisfying the sign condition. When the weights are fixed, the values are equal to M k, otherwise, they are bounded by a constant factor. Theorem 8 is extended to hold for labels in {\u22121, 1} by constructing networks N W1 and N W2. Lemma 11 states the existence of \u0175 \u2208 R d with specific properties for each (x, y) \u2208 S. Lemma 11 states the existence of \u0175 \u2208 R d with specific properties for each (x, y) \u2208 S. For (x, y) \u2208 S, | x,\u0175 | > \u03b1. Let \u03b2 = min (x,y)\u2208S {| \u0175, x |}, and if \u03b1 > 0, choose \u0175 and \u03b1 = \u03b2 2. If not, choose \u2212\u0175 and \u03b1 = \u03b2 2. To prove the theorem, choose \u0175 that satisfies the lemma's assumptions. If \u0175, x > \u03b1, yN W (x) > 1, resulting in zero loss. If \u0175, x < \u2212\u03b1, the loss gradient will also be zero. In this case, the loss on the example would be max{1 \u2212 yN W (x), 0} = 1, but the gradient will also be zero. Therefore, W \u2208 R 2k\u00d7d is indeed a local minimum. With probability at least 1 \u2212 \u03b4, there exists j \u2208 [k] for which e j \u2208 K 0. This implies that the algorithm converges to a stationary point that is not a global minimum. The convergence to a saddle point is possible only if \u03c3(0) = 0 and for all i \u2208 [k], w(i)t, ej = 0 at convergence. The probability of convergence to a non-global minimum point is almost surely due to the initialization of w(i)t. With k \u2265 log2(d/\u03b4), the probability of K0 = \u2205 is at least 1 - \u03b4. If ej \u2209 Kt for all t, then there exists i \u2208 [k] such that w(i)t, ej > 0 for all t."
}