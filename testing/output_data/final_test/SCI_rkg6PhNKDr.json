{
    "title": "rkg6PhNKDr",
    "content": "In the context of optimization, the gradient of a neural network indicates how much a specific weight should change in relation to the loss. Small gradients suggest that the weight is at a good value and can be frozen during training. An experimental study explores the importance of updating neural network weights and suggests that freezing uninformative weights after the third epoch may result in a slight drop in accuracy or even improvement. The study was conducted on MNIST, CIFAR10, and Flickr8k datasets using various architectures. The study experimented with freezing a percentage of parameters in different neural network architectures (VGG19, ResNet-110, DenseNet-121) on CIFAR10. Freezing 80% of VGG19 parameters resulted in a 0.24% accuracy drop, 50% of ResNet-110 led to a 0.9% drop, and 70% of DenseNet-121 resulted in a 0.57% drop. Additionally, an image captioning model with attention mechanism on Flickr8k dataset using LSTM networks showed better BLEU-4 score when 60% of parameters were frozen from the third epoch onwards. The success of deep neural networks in various vision and language applications has been remarkable since the deep learning revolution. Training deep neural networks for tasks like image super-resolution and image captioning requires powerful GPUs due to their complexity. A recent project by NVIDIA showed that training a GAN took over 6 days on 8 Tesla V100 GPUs. However, freezing parameters after the initial epochs can optimize training efficiency. Backpropagation is the key algorithm for optimizing deep neural networks by computing gradients with respect to the loss to determine weight adjustments. In this paper, the authors discuss the redundancy of weights in neural networks and propose Partial Backpropagation as an experimental technique. This method freezes weights with gradients near zero, allowing for more efficient training by keeping certain weights frozen. This approach leads to a slight drop in accuracy but demonstrates that fully training all weights is only necessary for the first two epochs. The experimental technique involves freezing weights with gradients near zero to improve training efficiency. Visualizations of gradient distributions in network layers show that many weights have already been optimized and are less likely to change during training. This method leads to a slight drop in accuracy but demonstrates that full training of all weights is only necessary for the first two epochs. Weight pruning and progressive freezing of hidden layers aim to reduce the heavy inference cost of deep networks in low-resource settings by removing redundant parameters, resulting in a compressed network with reduced energy and storage requirements for processing inference on embedded devices. The main objective is to improve efficiency while maintaining accuracy. In weight pruning, a neural network is trained to learn significant connections, then insignificant connections are pruned out. Remaining weights are fine-tuned, resulting in a sparse network. Pruning and retraining is an iterative process to find the minimum number of weights. Some approaches merge highly correlated neurons to maintain network signals. Other methods focus on dense-sparse-dense training flow. In weight pruning, neural networks are trained to learn significant connections and then prune out insignificant connections, resulting in a sparse network. Various methods focus on pruning convolutional filters for efficient inference, such as using Taylor expansion for approximating the change in the cost function induced by pruning network parameters. Additionally, an acceleration method prunes filters with small influence on output accuracy, reducing computation costs. Channel-level sparsity is enforced to decrease model size and run-time memory without compromising accuracy. A systematic weight pruning framework using ADMM has also been proposed for DNNs. The slimmable neural network (Yu et al., 2018) offers a method to train a single neural network that can adjust its width during runtime based on resource limitations. This approach contrasts with traditional methods that involve training separate networks with different width configurations. Weight pruning models aim to reduce computation at testing time for efficient hardware implementation but require iterative training processes, increasing training time. Other works on compressing deep neural networks include (Luo et al., 2017) and (Ullrich et al., 2017). In contrast to weight pruning methods, our experimental technique focuses on demonstrating the impact of redundant parameters when frozen. Previous work by Brock et al. (2017) proposed freezing out layers during training to reduce training time by excluding them from the backward pass. This technique targets initial layers with fewer parameters that converge to simple structures, aiming to optimize training efficiency. The study discusses freezing layers to reduce training time by excluding them from updates, leading to improvements in training time without loss in accuracy. The technique involves progressively freezing some layers with weights near zero gradients to keep them optimized. The experimental technique freezes weights with near-zero gradients from the third epoch onwards while training the rest normally. The technique involves freezing weights with near-zero gradients from the third epoch onwards while training the rest normally. Gradients below a threshold are frozen, and non-zero gradients are extracted for analysis. The magnitude of the gradient is of interest, not its direction, and kernel density estimation is used to plot the distribution. The technique involves freezing weights with near-zero gradients from the third epoch onwards while training the rest normally. Gradients below a threshold are frozen, and non-zero gradients are extracted for analysis. The magnitude of the gradient is of interest, not its direction, and kernel density estimation is used to plot the distribution. The new range of gradients G n is divided into b sub-ranges, and the mean of each sub-range is obtained. Otsu thresholding technique is then applied to find the optimal threshold value based on the mean values in G n. The final threshold value is selected based on the mean value with the least within-class variance from G d after training the network with full backpropagation for 2 epochs. After training the network with full backpropagation for 2 epochs, the technique involves freezing weights with near-zero gradients from the third epoch onwards. Gradient mask generation is performed by dividing each gradient by the learning rate to obtain the original non-scaled gradient. The magnitude of the gradient is of interest, and kernel density estimation is used to plot the distribution. \u2126 is set to 2 epochs for full backpropagation before starting partial backpropagation. For partial backpropagation, a mask is created for each layer based on gradient values. Only weights corresponding to a mask entry of 1 are updated, while frozen weights with a mask entry of 0 do not receive updates. The mask is calculated once in the third epoch and remains constant for subsequent epochs. In experiments on the CIFAR-10 dataset with CNN architectures, a batch size of 100 was used for backpropagation. Changing the mask every 35 epochs did not affect performance, and fine-tuning the network in the last epoch did not improve accuracy. This suggests that frozen weights are already optimized from the first two epochs. The CIFAR-10 dataset contains 60000 32x32 color images with 10 classes. Training and validation plots are shown for full backpropagation (FB) and experimental technique (PB) using various CNN architectures like VGG19, ResNet-110, DenseNet-121, and LeNet-5. Results are presented in Table 1, with the lowest freezing percentage observed in residual networks. VGG19 shows 9.4% parameters with zero gradients. In experiments with various CNN architectures on the CIFAR-10 dataset, VGG19 has 9.4% parameters with zero gradients by default. The methodology focuses on delivering experiments under normal settings without regularization techniques like data augmentation, weight decay, or dropout. Models are trained for 110 epochs with a learning rate decay at epoch 40 and 80. AMSGrad is used for training by taking the maximum between the current and previous exponentially weighted average of gradients. In experiments with various CNN architectures on the CIFAR-10 dataset, VGG19 has 9.4% parameters with zero gradients by default. The methodology focuses on delivering experiments under normal settings without regularization techniques like data augmentation, weight decay, or dropout. Models are trained for 110 epochs with a learning rate decay at epoch 40 and 80. AMSGrad is used for training by taking the maximum between the current and previous exponentially weighted average of gradients. We trained an image captioning model with visual attention mechanism on the Flickr8k dataset using our experimental technique. The dataset contains 8,000 images with up to 5 captions for each image, divided into training and validation/testing sets. For the image captioning model, a single layer LSTM with a hidden size of 512 is used. The batch size is set to 60, and the Adam optimizer with an initial learning rate of 5e-4 is employed. Training is done for a maximum of 15 epochs with early stopping if the validation BLEU-4 score does not improve for 10 consecutive epochs. Sampling is done with a beam size of 3, and BLEU with up to 4 grams is used as the evaluation metric. The best BLEU-4 score is obtained when training with full backpropagation. The soft attention variant of Xu et al. (2015) achieved a BLEU-4 score of 0.099 with full backpropagation training. By using an experimental technique (PB) with a freezing ratio of 61.81% redundant parameters, a higher BLEU-4 score of 0.101 was obtained. Evaluation results for BLEU scores are shown in Figure 5. The predicted word and attention weights for generated captions are illustrated in Figure 4. The experimental study emphasized the importance of neural network weights and the impact of freezing redundant parameters. Experiments were conducted on MNIST, CIFAR10, and Flick8k datasets using various CNN and LSTM architectures. The concern of the paper was successfully proven through the experiments. The base code for the experiments was provided in the appendix, implemented using PyTorch. Publicly available codes for the architectures were modified for the experiments."
}