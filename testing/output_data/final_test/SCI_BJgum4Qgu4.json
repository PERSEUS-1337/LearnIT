{
    "title": "BJgum4Qgu4",
    "content": "Language modeling tasks have been effective for learning word embeddings and context-dependent representations. A fill-in-the-blank task is used to learn context-independent representations of entities. Large-scale training of neural models allows for high-fidelity entity typing information to be learned, demonstrated through few-shot reconstruction of Wikipedia categories. Recent advances in deep learning have enabled the creation of robust general purpose representations of words and contextualized phrases directly from large textual corpora. This allows for the learning of context-independent representations of entities through a fill-in-the-blank task, leading to high-fidelity entity typing information. The central hypothesis of this paper is to build a representation for Valentina Tereshkova as the first woman in space by matching entities to their contexts using BERT BID6. They experiment with RELIC, a dedicated entity representation, to capture knowledge not present in BERT. The study focuses on encoding Wikipedia categorical information using RELIC to recover entities within a category with precision. Previous methods in learning entity representations in knowledge bases rely on ontology and vector space embeddings for KB completion. The study aims to induce entity representations from raw text alone, focusing on answering entity typing queries. Unlike previous methods that combine text with KB structure, the goal is to understand the limits of using raw text for entity representations. The RELIC task is similar to entity linking and typing, but this work considers types of RELIC from a global perspective. The study focuses on inducing entity representations from raw text for entity typing queries. It aims to understand the limits of using raw text for entity representations, specifically considering types of RELIC from a global perspective. The training procedure involves learning entity encoders directly from contexts in which entities are seen, mapping each entity to a vector in the latent space. The study extracts clean training data from English Wikipedia to induce entity representations for entity typing queries. It introduces a context encoder and defines a compatibility score between entities and contexts. The model is trained by maximizing the average log probability and using noise contrastive loss. The study utilizes a context encoder, specifically the pretrained BERT model, to encode contexts into fixed-size representations. Results for the Wikipedia category population task are presented, showing Mean Average Precision for ranking entities based on exemplars of a given Wikipedia class. The study explores different encoders for entity representations, including embedding lookup, BERT name encoder, and BERT description encoder. Two BERT-based options are considered for distributed encoding of entities, with the name encoder using the entity's canonical name and the description encoder using the entity's description. Data from the 2018-10-22 dump of English Wikipedia is used to train RELIC, with the goal of mapping entities to fixed-size representations. The study uses data from the 2018-10-22 dump of English Wikipedia to train RELIC, aiming to map entities to fixed-size representations. The model is trained using TensorFlow BID0 with specific parameters and achieves roughly 85% accuracy in-batch negative prediction. A fine-grained entity typing task based on Wikipedia categories is introduced. The study evaluates if RELIC benefits from dedicated embeddings over BERT encoders for populating Wikipedia categories with a few exemplars. They use Yago 3.1 BID12 categories, provide 3 or 10 exemplars per category, and rank candidate entities based on their RELIC embeddings. Mean average precision (MAP) is reported for entities in the query class. The study compares RELIC with BERT encoders for populating Wikipedia categories with exemplars. Results show RELIC outperforms BERT on frequent entities but lags on infrequent ones. Examples of category predictions demonstrate high precision, except for categories with very few exemplars. The RELIC model outperforms BERT in populating Wikipedia categories with exemplars, especially for frequent entities. It successfully predicts categories like Giro d'Italia cyclists and Video games featuring female protagonists. The RELIC fill-in-the-blank task helps learn interesting entity representations with their latent ontology, verified through a Wikipedia category reconstruction task. Researchers are encouraged to explore these entity representations and the BERT context encoder, which will be publicly released."
}