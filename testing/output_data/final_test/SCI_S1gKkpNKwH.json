{
    "title": "S1gKkpNKwH",
    "content": "We propose a neural architecture search algorithm for compact reinforcement learning policies, combining ENAS and ES. The algorithm defines a search space of edge-partitionings to represent compact architectures, achieving significant compression while maintaining good reward for RL tasks. This approach is particularly useful for mobile robotics with limited resources. The text discusses constructing a policy for a Markov Decision Process (MDP) to maximize rewards in a given environment. The policy, parameterized by vector \u03b8, maps states to actions and is often implemented as a neural network with thousands or millions of parameters. Training such high-dimensional policies poses challenges due to limited resources and computational constraints. In resource-constrained settings like mobile robotics, the question addressed is whether high-dimensional architectures are necessary for efficient policies. Despite some structured families offering compactification and accuracy, finding compact representations remains a challenging optimization problem. The approach involves a joint objective considering the network's parameter sharing profile and reward maximization in reinforcement learning optimization, leveraging ENAS literature and pointer networks. In the ENAS literature, pointer networks are used to optimize the combinatorial component of the objective. State of the art evolution strategies (ES) methods are employed to optimize the RL objective. The combinatorial search space is defined as edge-partitioning into same-weight classes, with policies constructed using weight-sharing mechanisms. Inspired by recent papers, networks encoding these policies are called chromatic networks. The Toeplitz policy in Figure 1 encodes a policy with 24 entries into a 9-dimensional vector, reducing the number of parameters. Weight sharing patterns can be learned to further reduce distinct parameters, making policies more efficient. This approach can train effective policies for OpenAI using architectures similar to previous studies. Our approach aims to train effective policies for OpenAI Gym tasks with as few as 17 distinct weights by partitioning weights into groups and learning them in a combinatorial fashion using reinforcement learning. This method strikes a middle ground between extremal weight selection and weight agnostic neural networks, offering a novel approach to encoding effective policies for RL problems. Our method combines ENAS and ES in a scalable way, using weight sharing across CPU workers to reduce training time and controller gradient variance. This approach is not feasible in the classical supervised learning setting. Our method combines ENAS and ES to train compact neural network architectures for RL problems, which is not possible in classical supervised learning due to limitations in training large image-based classifiers. This approach addresses challenges with scaling by numerous workers and opens new research directions in structured policies for robotics, particularly in mobile robotics with limited computational resources. Interest in Neural Architecture Search (NAS) algorithms has grown rapidly, showing they can design state-of-the-art architectures for image recognition and language modeling. Recent advancements have demonstrated that NAS network generators can outperform human-designed networks on image recognition tasks. However, applying NAS to construct compact RL policy architectures is a novel area that has not been explored before. Unlike supervised learning, RL policies are small and do not require a search space involving hyperparameter primitives. In the context of Neural Architecture Search (NAS) algorithms, compact RL policies do not require a search space involving hyperparameter primitives. The search space for RL policies can still be combinatorial, focusing on partitionings of weights. Before applying NAS, a specific parameterization of a compact architecture defining the search space needs to be chosen, leading to various techniques for network sparsification such as pruning already trained networks. Various methods have been proposed for network sparsification, including regularization, weight pruning, dropout, and the Lottery Ticket Hypothesis. These approaches aim to achieve competitive results with significantly fewer parameters, particularly in classification networks. However, these methods are not specifically designed for constructing RL policies. Quantization techniques like weight sharing and centroid-based quantization are explored for network compression. RL policies can benefit from compact linear policies for tasks like MuJoCo locomotion and vision-based tasks. Recent research has shown that smaller policies can be effective for vision-based tasks by separating feature extraction and control. Additionally, small, sparse sub-networks have been found to outperform larger over-parameterized ones, leading to applications in reinforcement learning. A highly scalable algorithm combining ENAS and ES has been proposed to learn compact representations and effective policies with a significant reduction in the number of neural network parameters. Furthermore, the impact and limitations of pruning neural networks for RL have been demonstrated through state-of-the-art results achieved by training both masks defining combinatorial structure and weights of deep neural networks concurrently. The text discusses the effectiveness of smaller policies and sparse sub-networks in reinforcement learning. It highlights the challenges of weight-partitioning mechanisms and the use of NAS to construct efficient distributions. The algorithm for learning structured compact policies is based on ENAS methods designed for neural network architectures. In reinforcement learning, architectures are cast as MDPs with a controller encoded by LSTM-based policy trained to propose good-quality distributions over architectures. The controller constructs architectures using softmax classifiers via autoregressive strategy and utilizes a weight-sharing mechanism introduced by ENAS. The core concept involves embedding different architectures into a combinatorial space represented by a directed acyclic graph. The weights of the graph edges correspond to a shared pool from which architectures inherit differently. Despite potential conceptual issues, updating weights based on evaluations of multiple architectures is desirable in practice, supported by transfer and multitask learning theories. ENAS optimizes a distribution over architectures with shared weights. The weights of the shared pool are updated using signals from different architecture realizations. The algorithm alternates between optimizing the controller's policy and the weights of models using multiple sampled architectures. After weight optimization, ENAS updates the controller's parameters through reinforcement learning to maximize expected reward based on accuracy. The shared weights are frozen during this process. The controller's architecture uses pointer networks with LSTM-based approach for attention-enriched models. Chromatic networks are feedforward NN architectures with shared weights learned through a modified ENAS algorithm. Weight-sharing mechanisms in architecture design, such as Toeplitz sharing, lead to smaller partitionings. The search space involves mappings of edges to partitions, rather than subgraphs. The controller learns distributions over these partitionings using a shared pool of weights. The architecture includes encoder and decoder RNNs for standard ENAS approach. The controller in standard ENAS consists of an encoder and decoder RNN, with the encoder processing input data and the decoder outputting primitive components of the final output. An embedding is trained for partition numbers and edges using tables. The reward obtained by the controller is based on a fixed distribution produced by its policy, estimated by averaging over multiple workers. The updates of the controller's parameter vector are based on maximizing rewards during weight optimization. The updates of \u03b8 in the proposed method are conducted using ES blackbox optimization techniques for RL, unlike standard ENAS which uses backpropagation. The loss function is defined with respect to weights and involves calls to the simulator, making explicit backpropagation impossible. The gradient is estimated using Gaussian smoothing with a fixed parameter \u03c3. The proposed method utilizes ES blackbox optimization techniques for RL, with a forward finite difference unbiased estimator. The pivot point is defined as an average loss for a set of weights shared over sampled partitionings. This approach differs from vanilla ES by using the expectation over a distribution of partitions rather than a static single-query objective. The experimental section discusses the limitations of sparse networks. In Subsection 4.1, limitations of sparse network approach for compactifying RL policies are demonstrated using state-of-the-art algorithms. Subsection 4.2 presents results on training chromatic networks with ENAS on OpenAI Gym tasks. Subsection 4.3 analyzes the impact of ENAS steps on learning partitions compared to random partitionings. Additional experimental results are provided in the Appendix. The mask m, trained using ES and element-wise multiplied by weights, is shown to produce compact effective policies through pruning. Results demonstrate a decrease in performance at high compression levels, highlighting the limits for RL tasks. The mask is initialized with 50% of total parameters in the network. Experiments were conducted on various OpenAI Gym tasks and the performance of constructing chromatic networks is summarized in Table 1. Increasing hidden layers or partitions improves performance in feedforward architectures. The comparison between linear high-partition and hidden-layer low-partition policies is not well understood, depending on the environment's complexity. In experiments with HalfCheetah and Minitaur, different partition policies perform better. Chromatic networks are compared with masking and other structured policies, using the same hyper-parameters and training until convergence for five random seeds. Weight parameters are compared for each class of policies. The curr_chunk discusses the comparison of weight parameters and compression in structured networks, highlighting the importance of reducing sampling complexity in ES training. It also mentions the total number of bits required to encode a working policy. The top two performing networks for each environment are bolded, with chromatic networks showing significant compression. Additional details can be found in the Appendix (Section E). Chromatic networks offer significant compression and quality across all tasks, with computational gains similar to Toeplitz. The matrix-vector multiplication part of the inference can be run on the chromatic network using a constant number of distinct weights. Training joint weights for fixed population of random partitionings without NAS also shows promise. Statistics for training chromatic networks are provided in Table 1. The average rollout-rewards for each step are calculated over 301 workers, with the highest average reported. The maximum reward obtained during training is also noted. Different policies are compared across various classes of RL networks, showing that some approaches produce suboptimal policies for harder tasks. During optimization, training policies and restarting for partitioning or distribution can waste computational resources. Random partitionings can still yield rewards, similar to NAS for supervised learning. Training with ENAS benefits by selecting good partitionings and resampling based on the controller replay buffer to overcome local minima. The weight-sharing mechanism in ENAS maximizes average reward, leading to benefits when new iterations increase rewards. Chromatic networks use partitionings of weights learned via ENAS for structured neural network architectures in RL policies. This approach allows for flexible construction of combinatorial structures like node deletions and edge removals. Our work on chromatic networks demonstrates aggressive compression compared to existing methods, maintaining policy efficiency. The structured matrices show unique characteristics, such as high displacement rank. Further research is needed to explore the transferability of learned partitionings across different RL tasks. Key parameters used include LSTM hidden layer size of 64, learning rate of 0.001, and entropy penalty strength of 0.3. Training was done using REINFORCE algorithm with specific settings for critic and softmax. The critic used a temperature of 1.0 for softmax and the training algorithm was REINFORCE. Non-linearities were tanh, with reward and state normalization applied. Action normalization was used for Minitaur tasks. ES-optimization utilized Monte Carlo estimators with smoothing parameter \u03c3 = 0.1 and learning rate \u03b7 = 0.01. Partitionings were analyzed for simpler representations and relation to Toeplitz-like matrices. Entropies of partitionings were studied by analyzing color assignments for network edges. The entropies of partitionings were computed by analyzing color assignments for network edges. The large entropies indicated that further compacification using Huffman coding would not be beneficial. The displacement rank of Toeplitz-type matrices with respect to specific bands was also discussed. The displacement ranks of weight matrices for chromatic networks are analyzed, showing full displacement rank using band matrices. Partitionings produced by the controller are examined throughout optimization, with convergence analyzed using cluster similarity metrics. The displacement ranks of weight matrices for chromatic networks are analyzed, showing full displacement rank using band matrices. Standard cluster similarity metrics like RandIndex and Variation of Information are used to compare partitionings. No convergence is observed in the metrics, indicating complexity in the space of partitionings. Transferability of partitionings across RL tasks is tested using top-5 partitionings from HalfCheetah to train distinct weights for Walker2d. Results show differences compared to random partitionings. The Chromatic network's weight matrices have full displacement rank using band matrices. Comparison with random partitionings shows no underperformance. Transferability of learned partitionings across tasks is left for future work. The network architecture includes 1-hidden layer with h = 41 units and tanh activation. Different frameworks like Unstructured, Toeplitz, Circulant, and a masking mechanism are compared. All baseline networks share the same general architecture with differences in weight matrix parameterization. The weight matrices in different network architectures vary in parameterization. Unstructured layers have ab independent parameters, Toeplitz layers have a+b\u22121 parameters, and Circulant layers have n independent parameters. Masking is another technique to reduce parameters by masking out redundancies. The masking mechanism in neural network architectures allows for parameter sharing and pruning. A mask weight is generated using softmax with a thresholding function to create near binary masks. The concatenated parameters are optimized using Evolution Strategies (ES) methods to encourage sparsity in the network. The objective function includes maximizing the environment return and the number of zero mask entries. The ultimate ES objective is to combine environment return and sparsity in the network using a coefficient \u03b2 and normalization."
}