{
    "title": "ryf6Fs09YX",
    "content": "Within machine learning algorithms, a General and One-sample (GO) gradient is proposed to efficiently calculate unbiased gradients for expectation-based objectives. This method applies to various distributions associated with non-reparameterizable continuous or discrete random variables, with low variance similar to the reparameterization trick. The GO gradient shows promising results in practice with just one Monte Carlo sample. Neural networks are coupled with common random variables using statistical back-propagation, allowing for efficient gradient propagation through distributions. This approach aims to enhance the descriptive capacity of neural networks and has been applied in various methods such as black-box variational inference and generative adversarial networks. The bottleneck of efficiently backpropagating gradients through general distributions, especially with continuous random variables, remains a challenge. The reparameterization trick is crucial for distributions with continuous random variables, limiting variational approximations to reparameterizable distributions. This constraint affects the applicability of methods like BBVI and GAN, which are mostly limited to continuous observations. The challenge lies in efficiently calculating unbiased low-variance gradients for general distributions with both continuous and discrete components. The components of y have a hierarchical structure, with some playing a role in evaluating f(y). Classical methods for estimating gradients of Eq\u03b3(y)[f(y)] wrt \u03b3 have limitations. The REINFORCE gradient has high variance with Monte Carlo estimation, while the reparameterization trick works well for continuous y. Efforts have been made to improve these methods, but none offer generalization and efficiency with as few as one MC sample. The key contributions of this work focus on the connection between REINFORCE and Rep, highlighting the high variance in gradient estimation. A new General and One-sample (GO) gradient is proposed to generalize Rep to non-reparameterizable distributions, with low variance and the flexibility to use more than one sample if needed. The core of the GO gradient is a variable-nabla, interpreted as the gradient of a random variable with respect to a parameter. Statistical back-propagation is introduced as a generalization of classic back-propagation, allowing neural networks to be coupled with random variables for gradient computation with low variance. The paper addresses the need to efficiently estimate gradients for machine learning problems by learning a model to approximate the underlying distribution. The text discusses learning a model to approximate a distribution by maximizing the expected log likelihood with added regularization. It introduces the concept of latent variables and the use of an approximate posterior to deal with intractable integrals. Variational learning aims to maximize the evidence lower bound, which involves the Kullback-Leibler divergence. The text discusses adversarial learning of p \u03b8 (x) using generative adversarial networks (GANs) to draw samples similar to x \u223c q(x). The optimization involves computing challenging gradients like \u2207 \u03b3 E q\u03b3 (y) [f (y)], which is relevant in various machine learning problems. The reparameterization trick (Rep) is limited to reparameterizable random variables y with continuous components. We propose a General and One-sample (GO) gradient approach to generalize the types of random variables y for which Rep may be effective, even in situations where Rep is not readily applicable. This approach works well with as few as one sample for evaluating the expectation and is applicable to more general settings than Rep. The reparameterization trick (Rep) is extended to generalize random variables y beyond continuous components. The approach, known as General and One-sample (GO) gradient, allows for effective application of Rep in situations where it may not be readily applicable. This method works with just one sample for evaluating expectations and is suitable for more general settings than Rep. The technique involves setting the \"0\" term to zero to reduce variance, while REINFORCE implements both terms numerically. The General and One-sample (GO) gradient extends the reparameterization trick to handle random variables y beyond continuous components. It allows for effective application in situations where the traditional method may not be suitable. The GO gradient sets the \"0\" term to zero to reduce variance, while REINFORCE implements both terms numerically. The General and One-sample (GO) gradient provides a principled explanation for low variance in gradients for continuous y, while also addressing computational challenges for high-dimensional discrete y. Special properties of f(y) can be exploited for efficient parallel computing. The GO gradient can handle single-layer mean-field q\u03b3(y) with an independence assumption on y components. Appendix I illustrates practical examples for handling these issues. The text discusses modeling q \u03b3 (y) as a marginal distribution of a deep model BID32 BID2 to enhance its descriptive capability. It focuses on a 2-layer model where components of y are conditionally independent given \u03bb. The Rep gradient can be recovered using Equation (10) if \u03b3 = \u03b3 y and q \u03b3 (y) has reparameterization y = \u03c4 \u03b3 (\u03bb), \u03bb \u223c q(\u03bb). Rep is shown to be a special case of the deep GO gradient. The text discusses recovering the Rep gradient using a neural network parameterized by y = \u03c4 \u03b3 (\u03bb) and the non-trivial CDF Q \u03b3 (y). The deep GO gradient in Theorem 2 generalizes back-propagation when element distributions are specified as the Dirac. The text discusses the deep GO gradient in Theorem 2, which generalizes back-propagation in deterministic deep neural networks. It involves forward-propagating information using activation functions and back-propagating gradients through random variables to each parameter \u03b3 (l). In Section 2, generative model p \u03b8 (x, z) and inference model q \u03c6 (z|x) are discussed for synthesis and inference of data. Recent deep architectures consider a hierarchical representation of latent variables z = (z (1), ..., z (L)). Models with first-order Markov chain structure are considered for inference. This discussion is relevant for variational inference and computation of DISPLAYFORM2. In Section 4, the deep set of random variables z = (z (1), ..., z (L)) are key components of the generative model for x. No specific structure is imposed on p \u03b8 (x, z) and q \u03c6 (z|x) in this section, moving beyond first-order Markov structure. Domain knowledge can be used to design suitable graphical models for practical applications. Theorem 3 (Statistical Back-Propagation) states that for expectation objectives where L \u2212 I continuous or discrete leaf variables are involved, hierarchical probabilistic graphical models can be used for training. The General and One-sample (GO) gradient for \u03c6 k is defined, relevant to hierarchical variational inference (HVI). Hierarchical variational inference (HVI) utilizes GO gradients for low-variance training of neural network constructed graphical models. Various methods aim to reduce variance in expectation-based objectives, including control variants and data augmentation techniques. Research often focuses on discrete random variables due to challenges with continuous relaxation for Rep. Efforts are made to relax discrete variables and combine REINFORCE and Rep for variance reduction. Methods like Generalized Rep (GRep) and rejection sampling variational inference (RSVI) aim to generalize the scope of Rep. The proposed GO gradient applies to both continuous and discrete random variables with low variance, similar to Rep. Other recent low-variance methods include Implicit Rep gradients and pathwise derivatives. Recent low-variance methods like Rep gradients BID5, pathwise derivatives BID15, RE-LAX BID9, SCG BID40, and Stochastic back-propagation BID33 BID4 exploit gradient backpropagation through random variables in various ways, offering alternatives to high-variance methods like REINFORCE, especially for high-dimensional problems. The proposed statistical back-propagation method based on the GO gradient allows for gradient backpropagation through continuous random variables, applicable to various distributions. It can generalize to hierarchical probabilistic graphical models with both continuous and discrete variables. Four experiments are conducted to validate the GO gradient, including examples with gamma and negative binomial distributions, comparison with variance-reduction methods in a discrete variational autoencoder experiment, a multinomial GAN for discrete observations, and hierarchical variational inference. The experiments demonstrate the deep GO gradient in Theorem 2 and hierarchical variational inference for deep non-conjugate Bayesian models in Theorem 3. The gradient calculations in the theorems follow the chain rule in expectation, making it easy to integrate into existing machine learning frameworks like TensorFlow and PyTorch. Experiments were conducted using TensorFlow or PyTorch with a Titan Xp GPU. The text discusses various probability distributions like NB, Bern, Mult, Pois, and Dir. It explores one-dimensional \"toy\" problems for continuous and discrete random variables using stochastic gradient ascent. The optimization objective involves minimizing KL divergence between q and p distributions. The text introduces RSVI, GRep, and their modified versions GRep-Stick and RSVI-Stick for minimizing KL divergence. GO method shows stable learning curves compared to REINFORCE in the discrete NB distribution case. GO gradient has lower variance due to analytically removing \"0\" terms. The proposed GO gradient demonstrates low variance in the discrete VAE experiment, compared to state-of-the-art variance-reduction methods like REBAR and RELAX. The focus is on single-latent-layer settings with neural networks projecting latent binary codes. All methods are run with the same learning rate for 1,000,000 iterations. The black line shows the best training ELBO of REBAR and RELAX. GO outperforms with faster convergence and better efficiency due to low variance, fewer parameters, and efficient batch processing methods. GO consistently provides the best performance in various experimental settings. Additional results can be found in Appendix I. GO can further reduce variance using variance-reduction techniques, especially for complex models. It may not work directly when f(y) is not computable or with models having discrete internal variables. A procedure in Appendix B.4 helps GO handle discrete variables. The deep GO gradient is demonstrated in Theorem 2 using multinomial leaf variables for a new multinomial GAN (MNGAN-GO) generating discrete observations. The generator's parameters are integrated into the NN notation for brevity. MNGAN-GO is compared with BGAN on discrete image generation tasks using quantized MNIST datasets. MNGAN-GO outperforms BGAN and shows potential for richer information. Generated samples from the 4-bit experiment demonstrate better image quality and diversity with MNGAN-GO. Statistical back-propagation is demonstrated with variational inference nets for deep exponential families and deep latent models. Hierarchical Bayesian models like deep exponential families (DEF) and deep latent Dirichlet allocation (DLDA) are discussed. Inference nets are designed following a first-order Markov chain construction. HVI is performed for a 2-layer DEF and a 3-layer DLDA, showing stable ELBO curves. The use of the GO gradient allows for efficient training of nonconjugate models for meaningful dictionaries. The GO gradient method is proposed for training nonconjugate models efficiently, including deep models with hierarchical latent variables. It provides low-variance estimation similar to the reparameterization trick for continuous random variables. Statistical back-propagation is introduced to integrate deep neural networks with various random variables. The GO gradient method is introduced for training nonconjugate models efficiently, providing low-variance estimation for deep models with hierarchical latent variables. Statistical back-propagation integrates deep neural networks with various random variables. The proof of Theorem 1 involves calculating equations and applying integration by parts for continuous and discrete variables. To develop the discrete counterpart of FORMULA3 using Abel transformation, two sequences {a n } and {b n } are defined. The equations are manipulated with substitutions to derive DISPLAYFORM2, where the first term is zero for both finite and infinite alphabets. Theorem 1 can be verified using the proofs for Eqs. FORMULA3 and (8). The equations involve functions such as the digamma function, upper incomplete gamma function, incomplete beta function, and generalized pFq function. The incomplete beta function, generalized hypergeometric function, and special case of Meijer G-function are discussed. A 2-layer model is used to demonstrate challenges with discrete internal variables. An importance-sampling proposal and a strategy for learning discrete internal variables are presented. The objective function and gradients with respect to internal variables are derived. The text discusses deriving gradients with respect to internal variables using Theorem 1 and applying partial integration and Abel transformation. Different scenarios for continuous and discrete variables are separately analyzed, leading to the unification of the derived formulas. The text discusses deriving gradients with respect to internal variables using Theorem 1 and applying partial integration and Abel transformation for continuous and discrete variables. It presents formulas for multidimensional situations and iteratively calculates A \u03bb k (y, V ) in a computationally expensive manner. The proposal introduces a method for easy-to-use expressions in deeper models. It involves applying importance sampling and defining a generalized variable-nabla to unify equations for discrete and continuous variables, following the chain rule to calculate gradients for internal variables. The rule applies to DISPLAYFORM6, allowing for the specification of continuous or discrete variables. However, importance sampling may not work well with discrete internal variables. To address this, additional continuous variables can be added to improve performance. The strategy in Figure 7 illustrates how to handle discrete internal variables using a neural network. The text discusses using neural networks to connect discrete variables and propagate their parameters. It suggests extracting the discrete variable as a leaf one and propagating its parameters to the next. The process involves expressing a marginal distribution as a joint distribution and aligning notations to calculate the multi-dimensional Rep gradient. The text discusses utilizing neural networks to connect discrete variables and propagate their parameters. It involves expressing a marginal distribution as a joint distribution and aligning notations to calculate the multi-dimensional Rep gradient. Theorems are verified by generalizing derivations to deep models and utilizing chain rule for models with continuous internal variables. The objective is transformed using Dirac delta functions, leading to expressions for Back-Propagation and Deep GO Gradient. The text discusses the application of neural networks to connect discrete variables and propagate their parameters. It involves expressing a marginal distribution as a joint distribution and aligning notations to calculate the Rep gradient. Theorems are verified by generalizing derivations to deep models and utilizing the chain rule for models with continuous internal variables. The objective is transformed using Dirac delta functions, leading to expressions for Back-Propagation and Deep GO Gradient. In Hierarchical Variational Inference, the goal is to maximize the evidence lower Bound (ELBO) by optimizing \u03c6 with the application of Theorem 3. In Hierarchical Variational Inference, a new lower bound ELBO2 is introduced to address the issue of non-trivial log q \u03c6 (z|x) when q \u03c6 (z|x) is marginal. This involves using an additional variational distribution r \u03c9 (\u03bb|z, x) to approximate the variational posterior q \u03c6 (\u03bb|z, x). The unnecessary \"0\" term related to log q \u03c6 (z, \u03bb|x) is also removed to improve the ELBO2. Theorem 3 is applied to provide GO gradients for one-dimensional \"toy\" problems with continuous and discrete random variables. The optimization objective is to maximize a function, using stochastic gradient ascent with one-sample-estimated gradients. Experimental results are shown in FIG8, comparing the GO gradient with other methods for gamma distribution. The GRep-Stick and RSVI-Stick versions do not analytically express entropy. GO gradient has lower variance and faster convergence compared to REINFORCE in one-dimensional cases. The experimental settings and results for the discrete VAE experiment are presented, focusing on single-latent-layer settings with Bernoulli random variables. Gradient variances are estimated with 20 Monte Carlo samples in each iteration for different toy examples. In single-latent-layer settings with Bernoulli random variables, gradient variances are estimated with 20 Monte Carlo samples. The last sample is used to update parameters, while 100 samples are used to calculate the ELBO. Different models are used, including a 1-layer linear model and a nonlinear model. The experimental settings and hyperparameter search strategy are similar to previous work. Theorems are applied straightforwardly, but expectations for lower variance are expressed analytically. The text discusses expressing expectations analytically in the context of Bernoulli random variables and deep learning frameworks like TensorFlow or PyTorch. An algorithm is presented to demonstrate how to practically cooperate GO gradients with these frameworks. The text also mentions a trick for changing gradients of any function using auto-differential software. The text discusses efficiently implementing the gradient of a function using prior knowledge, such as neural network parameterization. It also mentions modifying functions for discrete VAE experiments with Bernoulli random variables. The text discusses implementing the gradient of a function efficiently using prior knowledge, such as neural network parameterization, and modifying functions for discrete VAE experiments with Bernoulli random variables. ELBOs for the experiments on the MNIST/Omniglot dataset are shown, with the best training ELBO of REBAR and RELAX highlighted. Batch processing procedure is used to calculate f (y \u2212v , y v + a v ) efficiently. The text discusses efficient implementation of function gradients using neural network parameterization for discrete VAE experiments. Batch processing is used for efficient calculation of f(y-v, yv+av). GO algorithm shows better performance and faster convergence compared to other methods. The GO algorithm is 2-4 times faster than other methods in finishing training iterations and 5-10 times more efficient in achieving the best training ELBO. All methods show overfitting in experiments with nonlinear models, highlighting the need for model regularizations. The GO algorithm demonstrates superior optimization capacity and faster convergence compared to other methods, leading to better validation ELBOs. It benefits from early-stopping to enhance generalization ability, showcasing lower variance and more powerful optimization capabilities. The GO algorithm benefits from early-stopping for better generalization ability. The PyTorch code takes 30 minutes for the most challenging 4-bit task. The generator in the multinomial GAN is expressed as N(0, I), x \u223c Mult(1, NN P()). Training uses the vanilla GAN loss BID8 and a deconvolutional neural network for mapping in the generator. The generator in the MNGAN-GO algorithm uses a neural network to map to P, while the discriminator is a multilayer perceptron. MNGAN-GO has fewer parameters compared to BGAN. Data preprocessing involves rescaling and quantizing pixel intensities for different experiments, such as the 2-bit case. The 1-bit special case uses the Bernoulli distribution. The MNGAN-GO algorithm uses a neural network generator and a multilayer perceptron discriminator. It provides images of better quality and wider diversity compared to other models. The challenge in the DLDA task is applying pure-gradient-based learning methods due to the sparsity of latent code and gamma shape parameters, leading to unstable learning procedures. In the DLDA task, the MNGAN-GO algorithm utilizes neural networks for image generation. To improve results, certain tricks are employed such as setting thresholds for z and c values, as well as using a compromise factor for likelihood and prior. Higher-order information like Hessian is being explored for further enhancements."
}