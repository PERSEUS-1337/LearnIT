{
    "title": "rklaWn0qK7",
    "content": "Partial differential equations (PDEs) are widely used in physical and computational sciences. A new approach using deep neural networks to modify existing solvers has been proposed, preserving correctness and convergence guarantees. The model achieves 2-3 times speedup compared to state-of-the-art solvers, generalizing well to different geometries and boundary conditions. Recent advancements in solving Partial Differential Equations (PDEs) have shown that complex updates learned from data or experience can outperform hand-crafted approaches. For example, learned proposal distributions in Markov chain Monte Carlo have led to significant speedups compared to traditional methods. This approach has also been successful in domains such as learned optimizers and learned data structures. Our goal is to bring similar benefits to PDE solvers by building a learned iterator on top of an existing standard iterative solver. The iterative solver updates the solution at each step, and we learn a parameterized function to modify this update, ensuring correctness and enhancing convergence speed. Training is done on a single problem instance, but the model generalizes well to different geometries and boundaries. Our approach for PDE solving provides theoretical guarantees of convergence, faster convergence than existing solvers, and generalizes to different geometries and boundary conditions. It applies to any PDE with linear iterative solvers, achieving a speedup on multiply-add operations for 2D Poisson equations. Our method outperforms state-of-the-art solvers in terms of CPU time, utilizing standard convolutional networks for a 30\u00d7 speedup on GPU. Linear PDE solvers aim to find functions satisfying linear differential equations, with a linear operator A mapping candidate functions to satisfy Au = f. Linear PDE solvers aim to find functions u \u2208 F that satisfy Au = f, where f is a function R^k \u2192 R. Heat diffusion is an example that falls into this framework, with u mapping spatial coordinates to temperature and f mapping coordinates to heat flow. Additional equations, called boundary conditions, are needed to ensure a unique solution. Dirichlet boundary conditions, fixing function values on a subset G to a fixed value b, are common in physical problems. In linear PDEs, the boundary condition (G, b) defines the geometry and boundary value. Discretization of the solution space is done on a uniform grid with mesh width h. The PDE solution u is approximated as a vector in R^n^k, with a focus on 2D problems in this paper. The PDE solution u is discretized on a grid with values corresponding to grid points. The linear operator A is a combination of partial derivative operators. Finite difference approximates partial derivatives in a discretized space. After discretization, Au = f is rewritten as a linear matrix equation. In PDE problems, the matrix A is sparse, banded, and symmetric. Boundary conditions are set for u(x) = b(x) in the domain G. A vector e is defined to indicate points in G. A \"reset\" matrix is used to mask points in G. The solution u to the PDE under geometry G satisfies specific equations. The PDE problem involves discretizing on an n \u00d7 n grid to obtain a solution u that satisfies the boundary condition. A linear iterative solver is defined to update the solution u. The solver should map any initial u to a correct solution of the PDE problem. The PDE problem involves discretizing on an n \u00d7 n grid to obtain a solution u that satisfies the boundary condition. A linear iterative solver is defined to update the solution u, ensuring convergence to a unique fixed point u * that satisfies the linear system Au = f under boundary condition (G, b). The iterative update is designed using matrix splitting to satisfy the boundary condition and converge to the solution. The iterative update rule for solving the PDE problem on an n \u00d7 n grid involves satisfying the boundary condition using a \"reset\" operator and a linear update rule. The update rule, despite its complexity, remains linear and can converge to a unique fixed point u * that satisfies the linear system Au = f under the boundary condition (G, b). The Jacobi method is suggested as a simple and effective way to choose the matrix M for the update rule. The Jacobi method is a valid iterative update for solving Poisson PDE problems on a grid. It has slow convergence due to its local influence on grid points. The Multigrid method improves this by performing updates on a coarser grid and then upsampling the results. The Multigrid method involves using a V-cycle structure with downsampling and upsampling layers, performing multiple Jacobi updates at each resolution. Downsampling and upsampling are also known as restriction and prolongation, implemented using weighted restriction and linear interpolation. This method allows for faster information propagation and requires fewer operations compared to the original grid. A PDE problem consists of components A, G, f, b, and n, with interest in solving the same PDE class A under varying conditions. In this paper, the goal is to improve upon a standard iterative solver for a class of PDE problems by learning a new solver that has correct fixed points and fast convergence. The iterator family is parameterized to achieve these goals through optimization. The family of iterators shows good generalization properties, performing well on different problems even when trained on a single problem. The iterator family, trained on a single problem, performs well on different choices of parameters and grid sizes. A standard linear iterative solver \u03a8 is used for a fixed PDE problem class A, with a learned linear operator H. The term GHw acts as a correction term to \u03a8. \u03a6 H, with similar complexity as \u03a8, is chosen to be a convolutional operator. The chosen convolutional operator H, parameterized by a deep linear convolutional network, is discussed in detail. The correct PDE solution is a fixed point of \u03a6 H, which includes the standard solver \u03a8. If H = T, \u03a6 H computes two iterations of \u03a8 with two convolutions. The iterator \u03a6 H is trained to converge quickly to the ground truth solution using two convolutions of \u03a8. The learning objective is to find a matrix H that allows \u03a6 H to approximate the solution in k steps, with k chosen from [1, 20]. Combining smaller and larger k values yields the best performance in practice. The learning algorithm aims to find a matrix H that enables the iterator \u03a6 H to approximate the solution in a limited number of iteration steps. The spectral norm of \u03a6 H is a convex function of H, with a set of H values leading to a spectral norm less than 1 forming a convex open set. This implies that exploring a convex open set is sufficient to find an iterator with a small spectral norm. The learning algorithm aims to find a matrix H for the iterator \u03a6 H to approximate solutions efficiently. The optimization process tends to find symmetric iterators automatically. Training is done on a single domain with surprising generalization properties. Proposition 2 shows that generalization to different f and b is possible, but empirical verification is needed for different geometries and grid sizes. In our experiments, our learned iterator converges to the correct solution for various grid sizes and geometries, even though it was trained on only one. Generalization to different scenarios has to be empirically verified, but there is no risk of incorrect results. The iterator will fail to converge if generalization fails. Our linear function GH is trained to approximate T (I \u2212 T ) \u22121, aiming for the iterator \u03a6 H to converge to the correct solution in a single iteration. The current error u* - u and the new error u* - \u03a8(u) are related by equations. The linear operator GH approximates the error e, aiming for convergence in a single iteration. The linear function GH is trained to approximate T(I - T)^-1 for convergence. In iterator design, a linear function GH approximates T(I - T)^-1 for faster convergence to the solution u*. The linear deep network used for H does not include nonlinearity or bias terms. Deep networks can optimize linear functions faster than linear ones with gradient descent, even with non-convexity. H is modeled as a network with 3x3 convolutional layers without non-linearity or bias, referred to as \"Convk\". The Conv models, like Conv3, suffer from limited receptive field growth. To address this, a deep network version of the Multigrid method is designed using a U-Net architecture BID16. The U-Net operates on reduced grid sizes, leading to minimal computation increase compared to two-layer convolution. Models with k sub-sampling layers are referred to as Multigridk and U-Netk. The method is evaluated on the 2D Poisson equation with Dirichlet boundary conditions. The study evaluates the Jacobi method as the standard solver for the 2D Poisson equation with Dirichlet boundary conditions. Training is done on the Laplace equation on a square domain with random fixed value boundary conditions. Testing involves larger grid sizes and challenging geometries to test model generalization. The study evaluates the Jacobi method as the standard solver for the 2D Poisson equation with Dirichlet boundary conditions. Testing involves larger grid sizes and challenging geometries to test model generalization, including L-shape and Cylinders geometry. The convergence rate is evaluated by calculating the computation cost required for the error to drop below a certain threshold, measured by the number of convolutional layers on GPU. The study evaluates the Jacobi method as the standard solver for the 2D Poisson equation with Dirichlet boundary conditions. Testing involves larger grid sizes and challenging geometries to test model generalization. The computation cost is measured by the number of convolutional layers on GPU. The Conv model results show that it converges to the correct solution and requires less computation than Jacobi. The best model, Conv3, is approximately 5 times faster in terms of layers and 2.5 times faster in terms of multiply-add operations compared to Jacobi. The experiment results demonstrate that our model converges faster than the standard solver, even on a smaller domain. Comparisons with Multigrid models show that our U-Net models outperform them in all settings. The model is trained on a 64 \u00d7 64 domain and tested on 256 \u00d7 256, with lower computation costs. The experiment results show that our models achieve a speed up on every problem compared to baselines. U-Net3 converges faster than U-Net2, and the FEniCS package BID13 provides tools to solve differential equations efficiently. The FEniCS model is compared with our model in terms of wall clock time on the same hardware. Our model, utilizing the minimal residual method with algebraic multigrid preconditioner, outperforms other methods like Jacobi or Incomplete LU factorization. We focus on solver time, setting an error threshold of 1 percent. Different mesh types are used for square, L-shape, and cylinder domains, with our model showing comparable or faster performance than FEniCS on CPU. Our model efficiently runs on GPU, providing a 30\u00d7 speedup compared to FEniCS on Tesla K80 GPU. Previous works have applied deep learning to solve the Poisson equation, but they lack correctness guarantees and generalizability. BID6 and BID17 used deep networks for the 2D Laplace equation. A U-Net model trained by L1 loss and an adversarial discriminator loss was used in BID4. Other works solved the Poisson equation in specific physical problems, like BID21 solving for electric potential in 2D/3D space. In various physical problems, different methods have been used to solve PDEs, including electric potential, pressure fields for fluid simulation, and particle simulation of a PN Junction. Some studies have focused on using deep learning to accelerate fluid dynamics, while others have applied Bayesian optimization for shape design in fluid flow. Additionally, applications include solving the Schrodinger equation, turbulence modeling, and the Black Scholes PDE for American options. The challenge lies in dealing with nonlinear PDEs that may not have standard linear iterative solvers, limiting the current methods. Future work involves extending the approach to different types of PDEs. The method presented improves on existing standard solvers for PDEs, guaranteeing correct solutions as fixed points of the iterator. The model can generalize to different grid sizes, geometries, and boundary conditions, achieving significant speedups. Theorem 1 states that a linear iterator converges to a stable fixed point if the spectral radius is less than 1. The method presented guarantees correct solutions as fixed points of the iterator for PDEs. If the spectral radius is less than 1, a linear iterator converges to a stable fixed point. If M is a full rank diagonal matrix, u* satisfies certain equations. The spectral norm of \u03a6 H (u; G, f, b, n) is a convex function of H, and the set of H such that the spectral norm is less than 1 is a convex open set. The set of H must be convex because it is a sub-level set of a convex function. The iterator \u03a6 H (u; G, f, b, n) is valid for the PDE problem (A, G, f, b, n) if \u03c1(T + GHT \u2212 GH) < 1. The validity of the iterator is independent of f and b, and it holds for any choice of f and b. The spectral radius of any matrix T is bounded by the spectral norm, with equality if T is symmetric. Therefore, \u03c1(G(I \u2212 A)) < 1 for any G. The spectral norm bounds the spectral radius of a matrix T, with equality for symmetric matrices. For the Poisson equation, \u03c1(I - A) < 1 and G2 = 1. The Jacobi method is valid for any geometry due to sub-multiplicative matrix norms."
}