{
    "title": "HJgExaVtwr",
    "content": "Deep neural networks require a lot of annotations, leading to high costs. To address this, DivideMix is introduced as a framework that combines learning with noisy labels and semi-supervised learning. It divides the data into clean and noisy samples, training the model in a semi-supervised manner to avoid bias. The MixMatch strategy is improved by incorporating label co-refinement and label co-guessing during training. The MixMatch strategy enhances training by refining labels and guessing labels on labeled and unlabeled samples, respectively. It shows significant improvements on benchmark datasets. Alternative methods for obtaining labeled data include querying search engines, downloading social media images, using machine-generated labels, or employing a single annotator for each sample. Existing methods for learning with noisy labels primarily focus on loss correction approaches. Some methods estimate noise transition matrices to correct the loss function, while others leverage DNN predictions to modify labels and adjust the loss. However, these methods struggle with high noise ratios, leading to overfitting. To address this issue, Arazo et al. (2019) introduced MixUp augmentation and another approach involves selecting or reweighting samples to reduce the impact of noisy labels on the loss function. Approaches aim to reduce the impact of noisy labels on the loss function by selecting or reweighting samples, with methods like Co-teaching and Co-teaching+. Another area of research is semi-supervised learning, which leverages unlabeled samples to produce low entropy predictions. In this work, DivideMix is proposed to address learning with label noise in a semi-supervised manner. It discards noisy sample labels and uses them as unlabeled data to improve generalization performance. The key contribution is the introduction of co-divide, which trains two networks simultaneously using a Gaussian Mixture Model on per-sample loss distribution. During SSL phase, DivideMix enhances MixMatch by refining labels with a Gaussian Mixture Model and ensemble predictions. It significantly improves results on various benchmarks with label noise, showcasing its effectiveness through experiments and ablation studies. Existing methods for training DNNs with noisy labels focus on correcting the loss function, categorized into two types: treating all samples equally and correcting loss explicitly or implicitly, and relabeling noisy samples using various models like directed graphical models, Conditional Random Fields, knowledge graphs, or DNNs. Some recent iterative methods propose relabeling samples using network predictions, while explicit loss correction methods modify the loss with model predictions. Several methods have been proposed to correct loss functions in training deep neural networks with noisy labels. These methods include estimating label corruption matrices, reweighting training samples, separating clean and noisy samples, and training mentor networks to guide student networks. Some approaches involve selecting small-loss samples, reweighting samples based on gradient directions, applying cross-validation to identify clean samples, and calculating sample weights with a mixture model. Our method improves training by discarding noisy labels and using them as unlabeled data for regularization in a SSL manner. It can distinguish and utilize noisy samples better than other SSL methods. Additionally, it avoids confirmation bias by training two networks to filter errors for each other, unlike self-training methods. Our method improves training by discarding noisy labels and using them as unlabeled data for regularization in a SSL manner. It can distinguish and utilize noisy samples better than other SSL methods. Additionally, it avoids confirmation bias by training two networks to filter errors for each other, enabling implicit and explicit teaching at each epoch and mini-batch. In this section, DivideMix is introduced as a method for learning with noisy labels. It involves training two networks simultaneously to filter errors for each other through implicit and explicit teaching at each epoch and mini-batch. The DivideMix method involves training two networks simultaneously to filter errors for each other through implicit and explicit teaching. Deep networks learn clean samples faster than noisy samples, leading to lower loss for clean samples. The goal is to find the probability of a sample being clean by fitting a mixture model to the training data. The DivideMix method involves training two networks simultaneously to filter errors for each other through implicit and explicit teaching. Deep networks learn clean samples faster than noisy samples, leading to lower loss for clean samples. Arazo et al. (2019) used a Beta Mixture Model (BMM) to model the distribution of clean and noisy samples, but found that a Gaussian Mixture Model (GMM) is more effective in distinguishing clean and noisy samples. They fit a two-component GMM using the Expectation-Maximization algorithm, with clean probability determined by the posterior probability of the Gaussian component with smaller mean. The training data is divided into labeled and unlabeled sets based on a threshold on clean probability to avoid confirmation bias. The DivideMix method involves training two networks simultaneously to filter errors for each other through implicit and explicit teaching. Arazo et al. (2019) used a Beta Mixture Model (BMM) to model the distribution of clean and noisy samples, but found that a Gaussian Mixture Model (GMM) is more effective in distinguishing clean and noisy samples. They fit a two-component GMM using the Expectation-Maximization algorithm, with clean probability determined by the posterior probability of the Gaussian component with smaller mean. The training data is divided into labeled and unlabeled sets based on a threshold on clean probability to avoid confirmation bias. The model is prone to confirm its mistakes, as noisy samples wrongly grouped into the labeled set would keep having lower loss. The proposed co-divide method aims to prevent error accumulation by using two networks with different training data divisions, parameter initializations, and training targets. This divergence allows the networks to filter different types of errors, making the model more robust to noise. Additionally, a warm-up phase using cross-entropy loss is suggested for initial convergence, especially effective for symmetric label noise but less so for asymmetric label noise. To address the issue of overfitting to noise in asymmetric label noise scenarios, a negative entropy term is added to the cross-entropy loss during warm up. This helps in penalizing confident predictions from the network, leading to a more evenly distributed prediction distribution that is easier to model. The proposed method, called DivideMix, significantly reduces loss for clean samples while keeping it higher for noisy samples. The proposed method, called DivideMix, reduces loss for clean samples while penalizing confident predictions for noisy samples. To address label noise, improvements are made to MixMatch by performing label co-refinement for labeled samples and using an ensemble of predictions from both networks to \"co-guess\" labels for unlabeled samples. This approach aims to produce more reliable guessed labels by teaching the two networks to work together. The proposed method, DivideMix, enhances MixMatch by reducing loss for clean samples and penalizing confident predictions for noisy samples. It involves label co-refinement for labeled samples and using an ensemble of predictions from two networks to \"co-guess\" labels for unlabeled samples, aiming to produce more reliable guessed labels. The total loss is calculated using a combination of cross-entropy loss and mean squared error, with a regularization term to prevent assigning all samples to a single class. Extensive validation is done on benchmark datasets like CIFAR-10 and CIFAR-100. The method is validated on benchmark datasets CIFAR-10, CIFAR-100, Clothing1M, and WebVision. Label noise types include symmetric and asymmetric, with symmetric noise randomly replacing labels and asymmetric noise mimicking real-world noise structures. An 18-layer PreAct Resnet is used for training with specific parameters. For CIFAR experiments, a PreAct Resnet is trained with specific parameters using SGD. Hyperparameters introduced by DivideMix do not require heavy tuning. Clothing1M and WebVision 1.0 are large-scale datasets with real-world noisy labels. Clothing1M consists of 1 million images with labels generated from surrounding texts, while WebVision contains 2.4 million images. ResNet-50 with ImageNet pretrained weights is used for WebVision. WebVision dataset contains 2.4 million images from the web with 1,000 concepts from ImageNet ILSVRC12. Baseline methods are compared on the first 50 classes of the Google image subset using inception-resnet v2. DivideMix is compared with state-of-the-art methods like Meta-Learning, Joint-Optim, P-correction, and M-correction. Table 1 displays results on CIFAR-10 and CIFAR-100 with symmetric label noise levels from 20% to 90%. DivideMix outperforms other methods significantly, especially on CIFAR-100 with high noise ratios. Results for CIFAR-10 with asymmetric noise are shown in Table 2, using 40% as classes become indistinguishable beyond 50% noise. Table 2 compares state-of-the-art methods in test accuracy on CIFAR-10 with 40% asymmetric noise. DivideMix consistently outperforms other methods across different datasets with label noise, achieving over 12% improvement in top-1 accuracy for WebVision. The ablation study results on CIFAR-10 and CIFAR-100 show the impact of model ensemble and co-training on test accuracy. Using a single model instead of averaging predictions from two networks decreases accuracy, indicating the benefit of ensemble learning. Training a single network with self-divide leads to further performance decrease compared to ensemble learning. In this paper, DivideMix is proposed for learning with noisy labels by leveraging SSL. The method trains two networks simultaneously and achieves robustness to noise through dataset co-divide, label co-refinement, and co-guessing. Extensive experiments across multiple datasets show that DivideMix consistently exhibits substantial performance improvements. Incorporating additional SSL ideas to LNL and vice versa, adapting DivideMix to other domains like NLP. Figure 3 shows AUC for clean/noisy sample classification on CIFAR-10 data. Method effectively separates clean and noisy samples, even at high noise ratio. Figure 4 displays noisy samples in Clothing1M identified by the method. Noise filtering achieved by discarding noisy labels and using co-guessed labels for training regularization. Figure 5 visualizes training image features using t-SNE. Model trained with DivideMix. The model is trained using DivideMix for 200 epochs on CIFAR-10 with 80% label noise, forming 10 distinct clusters corresponding to true class labels. The only hyperparameter tuned is the unsupervised loss weight \u03bb u, with larger values needed for stronger regularization. Hyperparameters for Clothing1M and WebVision experiments include M = 2, T = 0.5, \u03c4 = 0.5, \u03bb u = 0, \u03b1 = 0.5, trained using SGD with momentum 0.9, weight decay 0.001, and batch size 32. Clothing1M network is trained for 80 epochs with initial learning rate of 0.002. For CIFAR experiments, the initial learning rate is set at 0.01 and reduced by a factor of 10 after 50 epochs. Unsupervised loss weight \u03bb u is crucial for higher noise ratios, requiring stronger regularization. Baseline methods in the ablation study show the importance of label refinement and augmentation for improved performance. DivideMix improves performance through augmentation and temperature sharpening. It is compared to other methods on CIFAR-10, showing efficiency on a single Nvidia V100 GPU. Computation time breakdown is also provided."
}