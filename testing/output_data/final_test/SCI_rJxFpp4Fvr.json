{
    "title": "rJxFpp4Fvr",
    "content": "The performance of deep neural networks is often attributed to their automated feature construction. It remains a question why this leads to good generalization, even with more parameters than samples. Hochreiter and Schmidhuber observed in the 90s that flat loss surfaces around local minima correlate with low generalization error. Existing flatness measures have been empirically validated but cannot be theoretically related to generalization in networks using ReLU activations. This paper proposes modified flatness measures that are invariant to reparameterization, implying network robustness. The proposed measures in the curr_chunk imply network robustness to changes in input and hidden layers, leading to a generalized definition of data representativeness. This can bound the generalization error of a model trained on representative data based on feature robustness. Neural networks excel in learning features from data, minimizing empirical error for supervised learning. The ability to learn suitable features is a key factor in their superior performance. Minimizing empirical error during deep neural network training leads to good generalization, even with more parameters than training examples. Good generalization is linked to flat minima of the loss surface, supported by empirical evidence. This could explain the generalization performance of deep neural networks trained with stochastic gradient descent. The idea of favoring flat minima over sharp minima in deep neural network training has been explored by various researchers, leading to improved generalization performance. However, current flatness measures based on the Hessian of the loss function may not be directly related to generalization in deep neural networks with ReLU activation functions. The function generalizes well if the extracted features encode a semantic similarity of the input that is robust to small changes, allowing for generalization to novel, similar data. A measure of flatness is derived that is invariant under reparameterizations and reduces to ridge regression penalty in linear regression. This highlights the relationship between flatness, robustness, and generalization in deep neural network training. This paper explores the relationship between flatness, robustness, and generalization in deep neural network training. It introduces the concept of feature robustness to measure a function's resilience to local changes in a feature space. The function is split into two components: a feature extraction mapping the input to a feature space, and a model (e.g., a classifier) operating in that space. The robustness is evaluated in the feature space towards small perturbations. The concept of feature robustness in neural networks measures resilience to local changes in a feature space. It is different from robustness defined by Xu & Mannor (2012) and is connected to generalization. Flatness of the loss surface is a local property, and feature-robustness is upper bounded by flatness. A suitable notion is needed to describe how representative samples are for the true distribution, leading to an upper bound for generalization error. Our contributions include defining feature robustness for models like neural networks, proposing a novel flatness measure invariant under reparameterization, connecting feature robustness to generalization error through dataset representativeness, and empirically showing the effectiveness of the proposed flatness measure. The proposed flatness measure strongly correlates with good generalization performance for models like neural networks. Feature robustness in feature space R m is defined based on a small number \u03b4 > 0, a training set S, and a feature selection matrix A \u2208 R m\u00d7m. Activation values of neurons are considered as feature values, and the feature value defined by the j-th neuron can be written as \u03c6 j (x) = \u03c6(x), e j. The feature robustness is defined based on a matrix A in the feature space R m, determining which features to perturb for a mean change in loss over a dataset under small feature changes. Non-activated neurons are considered non-expressed features, and a matrix A selects features to be perturbed. The matrix A in the feature space R m determines which features to perturb for a mean change in loss over a dataset under small feature changes. This perturbation is linear in the expression of the feature, only perturbing relevant features for the output of a given sample. The model bounds feature robustness at local minima for a dataset S uniformly over all feature selections A and dependent on \u03b4. The function f (x, w) = \u03c8(w, \u03c6(x)) involves a twice differentiable function g and a matrix product with w. The empirical error E emp (w, S) is minimized at parameters w * on the training set S. By Taylor expansion around the critical point w * , the Hessian HE emp (w * , S) can be calculated. The Hessian of the empirical error is denoted by HE emp (w * , S), with key details on feature robustness and flatness in terms of the Hessian. The connection between feature robustness and flatness is summarized in Theorem 2, linking loss function, matrix A, and model f (x, w) = g(w\u03c6(x)). The Hessian of the empirical error, denoted by HE emp (w * , S), is related to feature robustness and flatness in terms of the loss surface defined by the flatness measure \u03ba \u03c6 (w). Small values of \u03ba \u03c6 (w) indicate flatness, while high values indicate sharpness in the context of linear regression with squared loss. The text discusses the relationship between flatness and high values indicating sharpness in linear regression with squared loss. It also delves into the computation of second derivatives with respect to parameters and the use of neural network functions with nonlinear activation functions. The text explores the relationship between flatness and sharpness in linear regression with squared loss, focusing on the composition of activation functions in neural networks. It discusses measuring flatness at parameter values and the feature robustness of neural networks at local minima of the empirical error. The text discusses the relationship between flatness and sharpness in neural networks with ReLU activation functions. It mentions how a linear reparameterization can lead to the same network function but changes the Hessian of the loss, affecting generalization. To counteract this, the multiplication with ||w l || 2 is proposed. The text discusses the connection between feature robustness and the Hessian spectrum in neural networks with ReLU activation functions. It introduces a flatness measure as an upper bound for feature robustness, with different eigenvalues governing feature robustness for various feature selections. The trace is considered as an average of the spectrum, corresponding to feature robustness on average over all orthogonal feature selection matrices. Theorem 6 specifies the connection between feature robustness and the unnormalized trace of the empirical error at a local minimum. Theorem 6 in Appendix A.3 establishes a connection between feature robustness in neural networks and the unnormalized trace of the empirical error at a local minimum. It involves a model with parameters, a loss function, and orthogonal matrices, providing insights into the flatness measure for each layer of a neural network. Corollary 7 extends these findings to fully connected neural networks. The neural network is ((\u03b4, S, O n), )-feature robust in layer l on average over O n at w * for = T, considering all directions of moving away from a local minimum. This measure of flatness takes the full spectrum of the Hessian into account, providing a natural measure of flatness around a local minimum. Other studies have proposed similar measures of flatness derived from different approaches. In this section, the relation between flatness, feature robustness, and generalization error is studied. Previous works have explored the connection of flat local minima with generalization in PAC Bayesian bounds. The PAC-Bayesian approach to generalization, introduced by McAllester and Langford & Caruana, measures the generalization error of stochastic classifiers. Arora et al. relate noise injection to generalization under the same setting. The text discusses the relationship between flatness, feature robustness, and generalization error in machine learning models. It explores how the generalization error can be measured over a distribution of models using the expectation of empirical error and the Kullback-Leibler divergence. The connection between local properties of flatness and feature robustness to the generalization of a specific model is examined through a different approach. The text discusses the relationship between feature robustness, generalization error, and flatness in machine learning models. It explores connecting feature robustness to generalization through a different approach, independent of the loss function. The approach focuses on local properties in neighborhoods around data points to establish representativeness of the data samples. The text also delves into measuring feature robustness on average over a probability distribution and defining a notion on datasets to approximate loss on the true distribution. The text discusses representativeness of data samples based on local distributions around the origin. It introduces a generalization of classical -representativeness and explores the concept in the context of a model split into a feature extractor and a model. The text introduces a family of probability distributions induced by a distribution A on feature matrices A, emphasizing the need for a feature-robust model and densely sampled training data for generalization. The flexibility in choosing nontrivial A can lower the bound on representativeness and improve generalization. In this section, the practical usefulness of the proposed flatness measure is empirically validated. The measure shows a strong correlation with generalization error, overcoming theoretical issues observed in previous works. Previous methods evaluating generalization properties mostly rely on accuracy on testing data, which may not directly correspond to the theoretical definition of generalization error. The flatness measure is validated for practical use, showing a strong correlation with generalization error. Training the model with small and large batch sizes is a popular technique to achieve different minima. A neural network is trained on CIFAR10 multiple times with various setups to obtain network configurations in multiple local minima. Four different initialization schemes were considered. The study considered different initialization schemes and mini-batch sizes with corresponding learning rates for training a neural network on CIFAR10. The generalization error was approximated based on the flatness measure, showing a significant correlation with the full spectrum of the Hessian. The proposed flatness measure based on the trace is invariant to reparameterization, unlike the traditional one based on the Hessian. Feature robustness is shown to be bound by the flatness measure, as demonstrated in experiments on the MNIST dataset. Theoretical connection between feature robustness, flatness measures, and generalization error is established. The tracial measure based on the trace of the Hessian shows a strong correlation with generalization error, considering the whole spectrum of the Hessian. Feature robustness can be related to the tracial measure by bounding the maximum eigenvalue of the loss Hessian or averaging feature robustness over all orthogonal matrices. Our proposed measure of feature robustness is invariant to layer-wise reparameterizations of ReLU networks, unlike existing measures of flatness. Adversarial examples can still exist despite strong feature robustness, as individual sample changes may be hidden in the mean definition. Other reparameterizations are possible, such as using positive homogeneity to adjust weights in a neuron. Our proposed measures of flatness, \u03ba l and \u03ba l T r, are not invariant to reparameterizations unlike the norm suggested by Liang et al. The second term in the generalization bound is based on our notion of representativeness, which requires a distribution over matrices similar to localized kernel density estimation. It is uncertain if our notion of representativeness is efficiently computable. The feature robustness of a model allows for more freedom in finding specific distributions over matrices for generalization bounds. In this section, a novel flatness measure strongly correlated with generalization error is proposed, along with a computation of representativeness for a KDE with Gaussian kernels. Theoretical investigation relates this measure to feature robustness, providing a link to generalization error. The first theoretical connection between robustness, flatness of the loss surface, and generalization error is discussed, aiding in understanding deep neural network performance. The proof for Theorem 5 is also presented. The proof for Theorem 5 discusses the second derivative of a function in the context of neural networks, showing a relationship between the Hessian matrices before and after reparameterizations. The text discusses a neural network function parameterized by weights and positive numbers, with a focus on reparameterization and the relationship between Hessian matrices before and after. It involves loss functions, linear functions, and chain rule computations. The text discusses the relationship between Hessian matrices before and after reparameterization in a neural network function. It involves loss functions, linear functions, and chain rule computations. The Hessian of the empirical error is compared before and after reparameterization, showing a corollary of Theorem 2 and utilizing orthogonal matrices. The text discusses the positive semidefinite matrix H in R 2m\u00d72m and its properties. It involves the empirical error, derivatives, and the Hutchinson's trick. The function f(x) = (\u03c8 \u2022 \u03c6)(x) is also considered under certain assumptions. The function f(x) = (\u03c8 \u2022 \u03c6)(x) is assumed to be ((\u03b4, S, A), )-feature robust for all matrices ||A|| \u2264 1. Additionally, \u03c6(S) is ( , A \u03b4 )-representative for a hypothesis space H with \u03c8 \u2208 H. This implies the existence of a probability distribution A of matrices ||A|| \u2264 1 such that the generalization error E gen (f, S) of model f is bounded. Other measures of flatness for fully connected ReLU networks are also considered. The previous paragraphs discuss the feature robustness of a neural network function under certain conditions. The current chunk introduces a method to measure flatness in neural networks by reparameterizing weights. This measure is invariant under linear reparameterizations that do not alter the network function. Theorem 12 provides further details on this measure. The current chunk introduces a measure of flatness in neural networks by reparameterizing weights, which is invariant under linear reparameterizations. Theorem 12 provides details on this measure, which is related to feature robustness. The measure of flatness in neural networks is related to feature robustness and evaluates sensitivity to small changes in features. A common bound for all layers is specified, ensuring good performance is not solely dependent on a low flatness measure for a specific layer. The reparameterized weights provide a common measure for all layers, ensuring generalization behavior correlates with changes in discriminating features. The measure of flatness in neural networks evaluates sensitivity to small changes in features. Common bounds for all layers are specified to ensure good performance is not solely dependent on a low flatness measure for a specific layer. Different reparameterizations are considered to provide common measures for all layers. The second derivative of a neural network function with respect to weights is calculated using differential quotients. Reparameterizations are used to ensure common measures for all layers in the network. The Hessian of the empirical error with respect to weight parameters is denoted as HE emp (w l (j), S). Feature robustness is linked to noise injection in the layer under consideration. Probability measures on matrices and vectors are used to calculate expectations over samples and distributions. The induced probability distribution P x is defined by P x (T) = P A ({A | A\u03c6(x) \u2208 T}) for a measurable subset T \u2286 R m, showing robustness to noise injection in the feature space. Large changes in loss can be hidden in the mean, related to flatness of the loss curve with respect to weights. Matrices ||A|| \u2264 1 restrict perturbations, allowing consideration of individual points within bounds. The text discusses the limitations of considering perturbations within bounds set by matrices ||A|| \u2264 1, which may hide larger changes in loss when perturbing data points in arbitrary directions. It gives an example of perturbing two close samples orthogonally to each other to illustrate this limitation. The text discusses feature robustness in convolutional layers, considering changes in weights and matrices to maintain flatness and robustness. It highlights the importance of considering larger matrix norms for desired alterations in the input. The text discusses feature robustness in convolutional layers, focusing on the importance of considering larger matrix norms for desired alterations in the input. Proposition 14 introduces \u039b = (\u03bb i \u00d7 \u03bd i ) chosen with a Gaussian kernel, but Theorem 10 requires a probability distribution A on feature matrices A with A \u2264 \u03b4, which is not satisfied by Gaussians. Truncated Gaussian kernels could be a solution, but the existence of a suitable probability distribution A for practical bounds on generalization error remains an open question. The proof of Proposition 14 involves defining representativeness of a sample S from distribution D, using a Kernel Density Estimator with approximation error. Experiments were conducted on CIFAR10 and MNIST datasets using different neural network architectures. The study focused on training neural networks on the MNIST dataset, filtering out configurations with training error larger than 0.07. Different convergence minima were obtained by varying batch sizes and learning rates. The correlation between layer-wise flatness measure based on the Hessian trace and generalization error was analyzed, showing a strong correlation (\u03c1 \u2265 0.7) across all hidden layers. It was challenging to identify the most influential layer for generalization properties. The study analyzed the correlation between layer-wise flatness measures and generalization error in neural networks trained on the MNIST dataset. Different network configurations were filtered based on training error. Neuron-wise flatness measures were also calculated. Network-wise flatness measures showed a larger correlation with generalization loss compared to original measures."
}