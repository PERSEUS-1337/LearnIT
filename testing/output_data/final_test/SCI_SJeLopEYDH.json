{
    "title": "SJeLopEYDH",
    "content": "Most existing 3D CNN structures for video representation learning are clip-based methods and do not consider video-level temporal evolution of spatio-temporal features. The proposed Video-level 4D Convolutional Neural Networks (V4D) model the evolution of long-range spatio-temporal representation with 4D convolutions and preserve 3D spatio-temporal representations with residual connections. Extensive experiments show that V4D achieves excellent results, surpassing recent 3D CNNs by a large margin. During training, clip-based methods sample short clips for representation learning, while testing involves sampling multiple clips and averaging prediction scores. However, these models overlook video-level structure and long-range dependencies, potentially hindering action recognition with partial observations. Simply averaging prediction scores may not capture the full video context. The Temporal Segment Network (TSN) samples multiple clips from a video and uses their average score for training. However, TSN lacks inter-clip interaction and fine temporal structure capture. In contrast, the V4D framework proposes a holistic sampling strategy and 4D convolutional interaction for efficient video-level representation learning. The V4D framework introduces a unique 4D residual block for long-range spatio-temporal dependency modeling in video action recognition. It includes a 4D convolutional operation to enhance inter-clip interaction and integrates easily with existing 3D CNNs. V4D achieves competitive performance on benchmarks like Mini-Kinetics, Kinetics-400, and Something-Something-V1, outperforming 3D counterparts. The architectures for video recognition can be categorized into three groups: Two-stream CNNs, 3D CNNs, and long-term modeling framework. Two-stream CNNs use one stream for RGB images and another for optical flow, with results fused later for final prediction. Recent efforts focus on reducing computational cost for optical flow modeling. Recent advancements in video recognition have led to the development of 3D CNNs, which directly apply 3D convolutions on video data. These models have shown promising results on benchmarks like Kinetics-400, outperforming 2D CNNs in many cases. However, 3D CNNs typically have a larger number of parameters, requiring more training data for optimal performance. Additionally, most 3D CNNs are clip-based methods, focusing on specific parts of the video rather than the entire sequence. To address this limitation, long-term modeling frameworks have been introduced to capture more complex temporal structures for video-level representation. Video compositional models were proposed in the past to capture long-term temporal structure, but they had limitations. Mainstream methods now use recurrent neural networks and 2D CNNs for frame-level feature extraction. The Temporal Segment Network (TSN) employs sparse sampling and aggregation to model video-level temporal information. Our proposed V4D framework surpasses previous video-level learning methods by modeling both short-term and long-term temporal structures with a unique design of 4D residual block. It outperforms TSN and TRN in video recognition benchmarks like Kinetics and Something-Something. The V4D framework introduces novel 4D Convolution Neural Networks for video action recognition, processing RGB frames as 3D data. Existing methods utilize 4D CNNs for point cloud videos, while V4D focuses on holistic video representation. Our goal is to model 3D spatio-temporal features globally using new Residual 4D Blocks to convert 3D CNNs into 4D CNNs for learning long-range interactions. To create a video-level representation for action recognition, we divide the video into sections and select snippets to represent short-term action patterns. To create a video-level representation for action recognition, the video is divided into sections, and snippets are selected to represent short-term action patterns. A new approach is proposed to model both short-and long-term spatio-temporal representations simultaneously, with easy implementations and fast training, using 4D Residual Blocks to convert 3D CNNs into 4D CNNs for learning long-range interactions. 4D convolutions are proposed for better modeling long-range spatio-temporal interactions in action recognition. The input tensor V has dimensions (C, U, T, H, W), where C is the number of channels and U represents action units. The 4D convolution operation involves bias terms, weights, and kernel shapes for feature mapping. The proposed 4D convolutions aim to model long-range spatio-temporal interactions in action recognition. By implementing 4D convolutions with 3D convolutions, short-term 3D features and long-term temporal evolution can be simultaneously modeled in the 4D space. This approach allows for more comprehensive feature mapping compared to traditional 3D convolutions. The Residual 4D Convolution Block is designed to incorporate 4D convolutions into existing CNN architecture for action recognition. It allows for modeling videos in a more meaningful 4D feature space, capturing long-range spatio-temporal interactions. This approach combines short-term 3D features with long-term evolution for video-level action recognition. The Residual 4D Convolution Block integrates 4D convolutions into CNN architecture for action recognition. It transforms 3D data to 4D for processing, then back to 3D. Batch Normalization and ReLU activation are applied. This approach enhances performance by combining 2D convolutions at lower layers and 3D convolutions at higher layers. In our framework, we use the \"Slowpath\" from Feichtenhofer et al. (2018) as our backbone, extended to I3D-S ResNet18 for experiments. The 3D backbones consist of 3D convolution layers and Residual 4D Blocks for training action units individually. The Residual 4D Convolution Block integrates 4D convolutions into CNN architecture for action recognition, enhancing performance by combining 2D and 3D convolutions. The Residual 4D Block is used for modelling the long-term temporal evolution of action units in videos. Global average pooling is applied to form a video-level representation. A new video-level inference method is developed, involving 3D convolutional layers and 4D Blocks for prediction scoring and final video-level prediction. The proposed V4D is considered a 4D enhancement for action recognition. The proposed V4D is a 4D generalization of recent methods like Temporal Segment Network (TSN) for learning video-level representation. TSN uses a video-level sampling strategy with segments and averages prediction scores after a fully-connected layer, similar to global average pooling. V4D can be seen as 3D CNN + TSN, enhancing action recognition. The V4D is a 4D extension of methods like TSN for video representation. It combines 3D CNN with TSN, utilizing dilated temporal convolution for stronger video features. The 4D Blocks incorporate complex kernels and residual connections for learning both long-term and short-term representations effectively. In experiments on three benchmarks (Mini-Kinetics, Kinetics-400, Something-Something-v1), datasets contain varying numbers of videos for training and validation subsets. Each video has a specific number of frames, and pre-trained weights from ImageNet are used to initialize the model. Holistic sampling strategy is adapted for training. The model is trained using a holistic sampling strategy, dividing the video into sections and selecting clips of 32 frames. Frames are resized and cropped before being further resized. SGD optimizer is utilized with specific parameters, and the learning rate decreases at certain epochs. Spatial fully convolutional testing is employed for a fair comparison with other studies. The V4D inference method is compared with I3D-S and TSN+3D CNN. V4D achieves higher accuracy with fewer frames and parameters than I3D-S, showcasing its effectiveness. V4D outperforms I3D-S and TSN+3D CNN in accuracy. Different forms of 4D convolution kernels are explored, with the 3x3x1x1 kernel showing the best performance. The position and number of 4D blocks in V4D also impact accuracy, with placing the block at res3 or res4 yielding higher accuracy. The 4D block at res3 or res4 in V4D needs further refinement by 3D convolutions for higher accuracy. Inserting one 4D block at res3 and one at res4 improves accuracy. The number of action units (U) does not significantly impact performance, suggesting V4D is robust against the number of short-term units. Our V4D structure utilizes 4D Residual Blocks in res3 and res4, achieving higher accuracy than previous methods on Mini-Kinetics. Even with fewer frames used, V4D ResNet50 outperforms all reported results, including 3D ResNet101. Additionally, V4D ResNet18 surpasses 3D ResNet50 in accuracy, demonstrating the effectiveness of our V4D structure. In experiments on Kinetics-400, V4D with ResNet50 backbone shows promising results, confirming the capability of our approach. The V4D model uses ResNet50 as a backbone and incorporates 4D Residual Blocks in res3 and res4 for improved accuracy on Mini-Kinetics. The training strategy involves 8-frame inputs and multiple stages of training due to computational constraints. The V4D ResNet50 is fine-tuned with 8x4 input frames and achieves competitive results. The V4D model utilizes ResNet50 with 4D Residual Blocks in res3 and res4 for enhanced accuracy on Mini-Kinetics. Trained with 8-frame inputs, the V4D achieves competitive results on Kinetics-400 benchmark, outperforming other state-of-the-art models. Something-Something dataset focuses on modeling temporal information and motion, distinct from Mini-Kinetics and Kinetics datasets. The Something-Something dataset focuses on modeling temporal information and motion, with each video containing a single continuous action. Our V4D model achieves competitive results on Something-Something-v1, using ResNet50 pre-trained on Kinetics for experiments. Comparison with state-of-the-art models shows V4D outperforming others with a top-1 accuracy of 50.4%. Our V4D model, designed for video recognition, effectively captures both short-term and long-term temporal order information. By reversing the temporal order of 3D features, our model demonstrates strong temporal evolution capabilities. Through experiments on three video recognition benchmarks, V4D achieved state-of-the-art results, showcasing its ability to learn spatio-temporal representations and retain 3D features with residual connections. Our V4D model achieved state-of-the-art results on three video recognition benchmarks, including ActivityNet v1.3, with a mAP of 88.9 using V4D ResNet50. The evaluation metric used was mean average precision for action recognition, with only RGB modality as input. The V4D model achieved state-of-the-art results on ActivityNet v1.3 with a mAP of 88.9 using V4D ResNet50, implementing 3D CAM based on Zhou et al. (2016) for action recognition."
}