{
    "title": "rygjcsR9Y7",
    "content": "High-dimensional time series data can benefit from interpretable low-dimensional representations, as human cognition struggles in high-dimensional spaces. However, current representation learning algorithms for time series are often hard to interpret due to non-intuitive mappings and non-smoothness over time. The proposed framework combines interpretable discrete dimensionality reduction and deep generative modeling to learn smooth and interpretable embeddings of time series data. It overcomes non-differentiability in learning discrete representations and integrates a Markov model for a probabilistic interpretation, improving clustering performance and providing insights into temporal transitions and uncertainty. Our model is evaluated on various datasets including static and time series data, showing favorable results compared to competitors. Interpretability in representation learning for time series is crucial for understanding complex systems like chaotic dynamics or medical data, especially in decision-making processes in fields like finance or medicine. In this work, a method of clustering is presented using deep neural networks to preserve smoothness in time series representations. The need for intuitive interpretations in fields like finance or medicine is emphasized, with a focus on interpretability in representation learning for complex systems. In order to improve interpretability in representation learning for complex systems, work has been done on continuous representations in GANs and VAEs. However, discrete representations are still underexplored. The self-organizing map (SOM) provides a topologically interpretable structure for mapping states from an uninterpretable continuous space to a lower-dimensional space. While the classical SOM formulation lacks a time component, it can be incorporated using a probabilistic approach. In this work, a novel deep architecture is proposed for learning topologically interpretable discrete representations in a probabilistic fashion. A new method is introduced to overcome non-differentiability in discrete representation learning architectures, along with a gradient-based version of the classical self-organizing map algorithm. Extensive empirical evidence is presented for the model's performance on synthetic and real-world time series data sets. The proposed model combines ideas from self-organizing maps, variational autoencoders, and probabilistic models to improve clustering and interpretability of representations on time series data. The model maps input data to a latent encoding through an encoder neural network, enhancing clustering performance on benchmark and real-world medical datasets. The model encodes input data using an encoder neural network and assigns it to an embedding in a dictionary. A reconstruction of the input is then computed using a decoder neural network. The embeddings are connected to achieve a topologically interpretable neighborhood structure. The embeddings are connected to form a self-organizing map with k nodes, each corresponding to an embedding in the data space and a representation in a lower-dimensional discrete space. During training, a winner node is chosen for each data point, and a Markov transition model is learned to predict the next discrete representation. The discrete representations can be decoded back into the original data space. The self-organizing map algorithm involves updating nodes in a representation space using a neighborhood function. A two-dimensional SOM is chosen for visualization purposes, with a modified training algorithm to allow end-to-end training. The loss function incorporates a weighted version of the original SOM update rule, updating embeddings and their immediate neighborhood during training. The loss function for a single input x in the self-organizing map algorithm is designed to optimize different model components with specific terms. It includes a reconstruction loss that encourages informative representations from the assigned SOM nodes and encoding, facilitating training. The loss function for a single input x in the self-organizing map algorithm includes a term L commitment that encourages encodings and assigned SOM nodes to be close to each other. This term is necessary due to the non-differentiability of the embedding assignment in the model. The loss function in the self-organizing map algorithm includes a term that encourages neighbors of the assigned SOM node to be close, enforcing a neighborhood relation between discrete codes and all SOM nodes to receive gradient information. Gradient stopping is used to prevent encoding from being pulled towards neighbors, optimizing embeddings based on neighbors but not respective encodings. The gradient update in the SOM algorithm depends on distance to the encoding, offering optimization benefits. The non-differentiability of the cluster assignment step poses a challenge, similar to the VQ-VAE model. To address this, gradients are copied from z q to z e as an approximation. In order to address the non-differentiability issue in the model, a second reconstruction subterm is proposed to be added to the loss function. This subterm allows for a fully differentiable credit assignment path from the loss to the encoder, encouraging the encoding to be an informative representation of the input. This approach has been found to work well in practice. The approach of adding a second reconstruction subterm to the loss function has been found to work well in practice. The continuous term z e is optimized easily early in training, while the z q -term contributes more to the reconstruction. The ultimate goal is to predict time series in an interpretable way using a temporal probabilistic model in a low-dimensional discrete space induced by a SOM. The system state is defined as the assigned node in the SOM, and a Markov model is learned for transitions between states. The model is learned jointly with the SOM-VAE, with a loss function incorporating weighting hyperparameters \u03b3 and \u03c4. The terms L transitions and L smoothness encourage high probabilities for observed transitions and low probabilities for transitions to far nodes, respectively. This probabilistic model informs the evolution of the SOM based on prior beliefs about transitions. The evolution of the SOM in this term encodes prior beliefs about smooth transitions in natural data, improving clustering by acting as a temporal smoother. Various methods have been developed for unsupervised clustering, including using autoencoders or a mixture of autoencoders. The VQ-VAE model focuses on discrete representation for compression, differing from our model in certain implementation considerations. The self-organizing map (SOM) is an algorithm that provides an interpretable structure by mapping data to a lower-dimensional space. It has been extended for modeling dynamical systems and combined with probabilistic models for time series. There are approaches to deepen the SOM model by combining it with multi-layer perceptrons or metric learning. However, it has not been proposed to use SOMs in the latent space of autoencoders for unsupervised deep learning. The self-organizing map (SOM) algorithm is used for interpretable clustering and temporal predictions, especially in fields like healthcare. While probabilistic models like Gaussian processes have been successful in predicting patient outcomes, deep generative models, including SOMs, have been proposed for learning static representations of patients. However, SOMs have not been used for dynamic representations. Experiments have been conducted on various datasets, including MNIST handwritten digits and Fashion-MNIST images. The study utilizes the SOM-VAE algorithm to achieve superior clustering performance compared to other methods. It also demonstrates the ability to learn a temporal probabilistic model concurrently with clustering, leading to interpretable state representations in chaotic dynamical systems and patterns in real medical data. Experiments on MNIST and Fashion-MNIST datasets were conducted to test the clustering component of SOM-VAE. The study compared the SOM-VAE model with k-means, VQ-VAE, standard SOM, and GB-SOM on MNIST and Fashion-MNIST datasets. Results showed SOM-VAE outperformed other methods in clustering performance measures like purity and NMI. The SOM-VAE model provides a more balanced clustering performance measure than purity, as shown in experiments with MNIST and Fashion-MNIST datasets. Using 512 embeddings in the SOM yields a lower NMI due to the penalty term for the number of clusters, but it offers an interpretable two-dimensional representation of the data manifold. The VAE model benefits from both parts, with the reconstruction loss term on z e outperforming the gradient copying trick from VQ-VAE. Removing the z e reconstruction loss or not copying gradients leads to learning failure. Stochastic optimization with Adam in SOM loss discovers a more effective solution than the classical SOM algorithm. K-means is a strong competitor included as a reference baseline in experiments. The probabilistic model in the architecture was tested using synthetic time series data sets of (Fashion-)MNIST images. The maximum likelihood estimate for the Markov model's transition matrix was constructed, with the model performing on par with the MLE solution. The probabilistic model in the architecture was tested using synthetic time series data sets of (Fashion-)MNIST images. The model's performance in terms of NMI was not affected by the inclusion of the probabilistic model, and even slightly improved on Fashion-MNIST. The experiment showed that fitting a valid probabilistic transition model alongside SOM-VAE training did not impact clustering performance, and in some cases, improved it. The probabilistic model improved clustering performance on certain types of data, including synthetic trajectories from the Lorenz system. The model learned to ignore chaotic noise and focus on changes between attractor basins. Entropy distributions were computed to compare interpretability of learned representations. The experiment compared entropy distributions of representations learned by SOM-VAE and k-means models. SOM-VAE representations were closer to groundtruth attractor basin assignments, showing higher interpretability compared to k-means representations. The SOM-VAE representation outperforms the k-means representation in interpretability, purity, and NMI. The probabilistic model is used to sample new latent system trajectories and compute their entropies, showing qualitative similarity to real data. Performance comparison shows SOM-VAE with probabilistic model excels against k-means in unsupervised prediction tasks on real eICU data. Our model outperforms others in learning a topologically interpretable structure and slightly overestimates attractor basin change probabilities. Trained on ICU patient vital sign time series, our model's clustering performance is assessed for predicting future physiology states. The full model, including the latent Markov model, performs best compared to k-means. The latent Markov model outperforms k-means and SOM-VAE without probabilistic model on medical data due to its smoothing tendency. Clusters in the model show higher future physiology scores and compact structures for interpretability. Additional results on acute physiology scores and mortality risk are available in the appendix. The SOM-VAE method visualizes patient trajectories in a representation space, showing different outcomes based on physiology scores. This information can help doctors assess patient risk for deterioration scenarios quickly. The SOM-VAE provides interpretable state representations for time series and static data, improving clustering performance and offering two-dimensional representations. The SOM-VAE method enhances clustering performance by learning two-dimensional representations of data and overcoming non-differentiability issues. It outperforms traditional self-organizing maps and provides more informative representations for medical data analysis. The model's visualizations can aid clinicians in understanding patients' health states and trajectories intuitively. Future work may explore extending the probabilistic component to improve clustering, interpretability, and predictive capabilities. Promising research avenues include exploring higher order Markov Models, Hidden Markov Models, and Gaussian Processes to increase model complexity and improve predictions. Additionally, investigating theoretically principled methods to address non-differentiability and comparing them with empirically motivated approaches is suggested. Another potential direction is to deviate from the traditional SOM concept of a fixed latent space structure and instead learn the neighborhood structure directly from data as a graph. The self-organizing map (SOM) aims to approximate data manifold in a high-dimensional continuous space with a lower dimensional discrete one, serving as a nonlinear discrete dimensionality reduction technique. The self-organizing map (SOM) is optimized to approach the data manifold closely by embedding nodes in a lower-dimensional discrete space. The optimization process couples the neighborhood function in the low-dimensional map space with the Euclidean distance in the high-dimensional data space to reflect the data's geometrical structure. The SOM training procedure involves updating node embeddings based on the Euclidean distance between data points and nodes in the data space. The winner node's embedding is adjusted towards the data point, with neighboring nodes potentially updated with a smaller step size based on a defined neighborhood function. In this paper, a gradient-based optimization approach is used for training Self-Organizing Maps (SOM) on a two-dimensional rectangular grid. The nodes are updated in minibatches with varying step sizes based on their distances to data points. This method outperforms the original formulation and addresses the challenge of fitting disjoint manifolds by mapping the data into a lower-dimensional latent space using a variational autoencoder. The VAE is used to map data into a lower-dimensional latent space, which is then modeled with the SOM. Hyperparameters were optimized using Robust Bayesian Optimization. Competitor models are special cases of this general framework. For clustering data, performance measures like purity and normalized mutual information (NMI) are used to compare the quality of clustering methods. Purity compares ground truth classes with algorithm clusters, while NMI measures the similarity between them. The purity of a clustering algorithm is defined as the accuracy of assigning the most prominent class label in each cluster to all data points. However, purity may not be informative when there are too many clusters. Normalized mutual information (NMI) is a measure that penalizes the number of clusters and provides a more sensible evaluation. In experiments, different values of clusters were tested to assess clustering performance. The clustering performance was tested on MNIST and Fashion-MNIST datasets with different values of k between 4 and 64. Purity increases with k, while NMI penalizes too many clusters. The optimal k according to NMI is between 16 and 36. The Lorenz system, defined by coupled differential equations, exhibits chaotic behavior with a strange attractor. 100 trajectories were simulated from the chaotic system and used to train SOM-VAE and k-means with 64 clusters/embeddings. The system switches between two attractor basins chaotically by computing the Euclidean distance to identify the current basin. Interpretability is defined as the similarity between the representation and the system's ground truth macro-state, assessed by comparing their evolution over time in terms of entropy. Simulated trajectories were divided into spans of 100 time steps to compute entropies in different spaces. The study focused on analyzing attractor basin spaces, SOM-VAE and k-means representations, and entropy computation for system space discretization. Experiments were conducted on dynamic data from the eICU Collaborative Research Database. Time series data was irregularly sampled and resampled to a regular grid for analysis. The study analyzed attractor basin spaces, SOM-VAE, and k-means representations, and entropy computation for system space discretization using dynamic data from the eICU Collaborative Research Database. Time series data was resampled to a regular grid with a one-hour interval for ICU patient monitoring based on APACHE score variables. Predictive scores were defined for the next 6, 12, and 24 hours based on the worst physiological state. The study used dynamic data from the eICU database to analyze attractor basin spaces, SOM-VAE, and k-means representations for ICU patient monitoring. Predictive scores were defined for the next 6, 12, and 24 hours based on the worst physiological state, with patients being stratified by expected pathology. The training set had 7000 unique patient stays, and the test set had 3600 unique stays. The SOMVAEProb uncovered compact structures in the latent space related to future physiology scores, focusing on acute physiology scores, mortality risk, and physiological abnormalities. The SOMVAEprob map clearly distinguishes abnormal cells from normal cells, with significant differences in distribution and location parameters. Enrichment patterns for acute physiology scores in the next 6 and 12 hours show temporal stability. Regions enriched for higher acute physiology scores also exhibit elevated mortality. Patients on the SOMVAEprob map enriched for higher acute physiology scores exhibit elevated mortality rates. Low sodium and high potassium states are found near the left and right edges, respectively, representing sub-types of the high-risk phenotype. Elevated creatinine is present in both regions, while a compact structure associated with elevated HCO3 in the center may indicate a distinct phenotype with lower mortality risk. The SOM-VAE model shows compact structures in all phenotypes, providing interpretable two-dimensional representations of data manifold in latent space. Images generated from the model trained on MNIST and Fashion-MNIST demonstrate this capability."
}