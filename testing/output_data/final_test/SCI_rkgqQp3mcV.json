{
    "title": "rkgqQp3mcV",
    "content": "In competitive situations, agents may unwittingly help opponents achieve their goals. Three agents operate: a user, an attacker, and an observer. The attacker may exploit the user's lack of domain knowledge to gain an advantage. The observer may need to intervene online to support the user and prevent agents from facilitating an opponent's goal. A decision tree classifier is used to identify intervention points based on the criticality of the current state, laying a foundation for future work in deciding when to intervene during plan execution. Intervention by a passive observer is beneficial when unexpected modifications to the environment or adversaries subverting the agent's goal occur. The observer must monitor actions unobtrusively to predict trajectories of the observed agent and assist in completing the intended task safely. For example, a user checking email on a computer may need protection from attackers trying to steal their password through phishing emails or malicious attachments. The observer must monitor actions unobtrusively to predict trajectories of the observed agent and assist in completing the intended task safely. Intervening at the right time is crucial to avoid wasted effort or undesirable outcomes. Customized interventions are needed for users with different skill levels to identify actions that warrant intervention over three different time horizons. In this work, the focus is on identifying the first horizon for intervention in a competitive environment involving a user, an attacker, and an observer. The observer passively monitors the user and attacker competing to achieve different goals, with the attacker attempting to leverage the user's progress for its own goal. The attacker may mask domain knowledge to expand the attack vector. In a competitive environment, an observer uses domain-independent features to train a decision tree classifier for intervention. The classifier recognizes intervention points based on risk, desirability, distances to goals, and active landmarks. The model is evaluated on unseen observations to assess accuracy and provide explanations for interventions in real-time. The paper formalizes the online intervention problem by introducing an intervention graph and domain-independent features. It presents an approach to classify observations for intervention, incorporating salient features for better prediction and explanations. The approach is shown to work well with benchmarks. The paper introduces the concept of online intervention using an intervention graph and domain-independent features. It discusses active and passive attackers manipulating user actions to reach an undesirable state. The observer monitors these actions and aims to prevent the user from reaching the undesirable state. In domain BID5, the observer watches the user stacking blocks to spell words. The user struggles to recognize block B, hindering their ability to identify states and leading to failure in circumventing the attacker's goal. The attacker manipulates the user to achieve their own goal. In the initial state, all blocks are on the table. The user picks up block A and stacks it on D. Two timelines branch out from this event, with the user and attacker making different moves. In timeline T1, the user stacks block T on A to spell TAD and reaches point Gd successfully. In timeline T2, the attacker reaches point Gu by stacking block B on A before the user, leveraging the user's progress. Passive Attacker: Considers the 3x3 grid world domain BID12. The observer watches the user navigating from start point to reach Gd in 1-step actions. The user aims to avoid the trap at point X to reach Gu, but needs the observer's intervention. Our formulation of the intervention problem involves an observer making decisions in an online setting based on incremental observations. The observer has full observability in a discrete domain with known goals Gd and Gu, but hidden plans to reach them. The environment is deterministic, allowing the observer to determine immediately applicable actions. The user's actions are influenced by the observer's interventions. The observer in the intervention problem has full observability in a discrete domain with known goals Gd and Gu. The user follows a plan to reach Gd but may reach Gu unwittingly, seeking the observer's help to avoid Gu. The attacker follows a plan to reach Gu with full observability of the domain and user's actions. The planning problem is defined as a tuple P = F, A, I, G where F is the set of fluents, I is the initial state, G represents the goal states, and A is the set of actions. Each action consists of preconditions, add, and delete effects. The plan recognition problem involves a triple a = P re(a), Add(a), Del(a) where actions are applicable if preconditions are true in the state. Executing an action results in a new state. The solution is a plan that modifies the initial state into the goal state. The plan intervention problem uses observations of actions to find the most likely plans and goals. The intervention problem aims to assess the current state's ability to cause G u and determine if the user needs to be blocked from further progress. Our solution includes a decision tree classifier model that classifies observations as requiring intervention or not based on a domain-independent feature vector. The solution involves a vector of decision points for actions in O to identify those requiring intervention. The observer's decision space is represented by an intervention graph, with action sequences leading from the current state to goal states. The graph helps identify attack plans and desirable plans for intervention. The observer uses an intervention graph to identify attack plans and intervention strategies. The attacker can disrupt the user's progress by inserting a hidden block into the stack. If the user reaches goal states 1 or 4, they win despite the attacker's interference. State 2 is critical for the attacker, as it signifies successful disruption of the user's goal. The attacker has reached state 2, prompting the need for intervention. The observer must hypothesize the user's goals to prevent reaching state Gu. Different paths are shown, with path 4 being safe. An intervention graph is built based on domain theory to determine actions for interruption. The observer builds an intervention graph based on domain theory to determine actions for interruption. The graph is constructed by adding actions whose preconditions are satisfied at the current state, spawning possible states for subsequent levels. The graph is updated with new observations, and features are extracted to determine when to intervene. The construction terminates once a certain state is reached, and backtracking actions are not allowed to prevent cycles. The intervention graph is built based on domain theory to determine when to intervene. Features like Risk, Desirability, and Distance to goals are used to train a decision tree. The graph is updated with new observations, and Risk quantifies the likelihood of reaching a goal while considering uncertainty. FIG4 shows a fragment of the intervention graph after a specific action, illustrating how goals can be reached safely. The posterior probability R is computed for paths leading to the attacker winning state in the intervention graph. Paths causing the user to reach a certain point before the goal is reached are considered candidates. The plan set \u03a0u contains action sequences reaching a different state, and the posterior probability of reaching that state is computed using the chain rule in probability. The observer assigns probabilities to action sequences leading to the user reaching a goal state in the intervention graph. The root state is assigned a probability of 1.0, with subsequent actions having probabilities based on a uniform distribution. The chain rule of probability is applied to calculate the probability of reaching the goal state. In the intervention graph, desirability (D) measures the effect of observed actions on helping the user achieve a goal safely. The plan set \u03a0d contains action sequences that reach a goal state without reaching a specific state G u. Probabilities are assigned to actions based on confidence in the next observation, and distance measures are used to assess progress towards the goal. Distance measures in the intervention graph assess progress towards the goal by measuring the distance to states G u and G d in terms of actions required. The path set \u03a0 u contains action sequences reaching state G u, with \u03b4 u as the average distance before reaching G u. If G u is unreachable, \u03b4 u is -1. The distance to Gd from the current state is measured by \u03b4d, which is the average distance to Gd without reaching Gu. The percentage of active attack landmarks (Lac) indicates the criticality of the current state towards achieving Gu. Landmarks BID6 are essential predicates for planning problems, and fact landmarks are extracted using the BID6 algorithm. Landmarks for planning problem P = D, Gu are attack landmarks that establish predicates for reaching Gu. The Landmark Generation Graph (LGG) BID6 for the active attacker case shows predicates that must be true together. Observing actions activating attack landmarks signals an undesirable state. Landmarks have been used in deriving heuristics in plan recognition. The text discusses using attack landmarks to compute a feature for generating alternative plans. A decision tree classifier is trained to categorize actions as warranted or unwarranted interventions. The labeled data set will be made available for the community to use. The decision tree classifier was trained with 10-fold cross validation to predict interventions for new problems. It outperformed random forests BID2 and Naive Bayes in accuracy. Training data was generated by creating planning problems and observation traces. Algorithm 2 was used to produce observation-feature vector relations for the classifier. The decision tree classifier, trained with 10-fold cross validation, outperformed random forests and Naive Bayes in accuracy for predicting interventions. It used the C4.5 algorithm to build a decision tree based on information entropy. The focus was on determining salient features for intervention and predicting intervention on unseen problems in different domains. In the study, various domains were used to simulate interventions, including Navigator and Ferry domains from IPC benchmarks. The Block-Words domain was also utilized to mimic an active attacker scenario. Traps were designated in Easy-IPC and Navigator domains, while in the Ferry domain, a compromised port posed a challenge for the ferry carrying cars. The study involved generating 3 instances of 20 problems each for benchmark domains to create testing data for the model. Each instance had intervention problems different from the trained data, such as varying block numbers, grid sizes, accessible paths, and artifact properties. True-positive, true-negative, false-positive, and false-negative instances were defined based on the classifier's predictions. The study used a correlation-based feature selection technique to identify the top predictors for intervention. Risk and distance to desirable state were found to be the best features, while the percentage of active attack landmarks was the weakest predictor. Feature selection helps simplify the model, improve interpretability, and prevent overfitting. The study identified risk and distance to desirable state as the top predictors for intervention. The percentage of active attack landmarks was the weakest predictor and was removed from training. The classifier showed high true-positive and true-negative rates (>95%) across domains, indicating its effectiveness in predicting intervention. The model remained consistent in accuracy across test instances, showing tolerance to modifications in the domain. Delaying intervention decisions may be costly in real-life scenarios. The study focused on establishing a wait time for intervention to avoid frequent interruptions for human users. By using a specific feature as a checkpoint, the decision to intervene was delayed until certain percentages of active landmarks were reached. The accuracy of predicting interruptions remained high even with delayed decisions, showing tolerance to modifications in the domain. The study focused on establishing a wait time for intervention to avoid frequent interruptions for human users. Delaying the decision until certain percentages of active landmarks were reached showed no significant impact on accuracy. However, delaying intervention led to missing true positives, with the most significant loss occurring in the Navigator domain. Delaying until 50% was the safest choice in some domains, while delaying until 75% resulted in a loss of true positives. Delaying interruptions can be controlled by the percentage of active landmarks in the current state, leading to a loss of 8%-18%. When intervention is needed, the agent issues a warning to the user, who must take corrective actions. Decision trees can help explain intervention, with rules based on risk values. Intervention points are identified based on risk levels and distance to undesirable states. If risk level is greater than 0.5 and the action triggers the undesirable state, intervention is warranted. Decision trees can explain interventions in the Blocks-words domain. The learned model generates simple trees for passive attacker domains, using one feature to determine intervention. Risk feature determines intervention in Easy-IPC and Navigator domains, while Distance to G d determines intervention in Ferry domain. Plan recognition is a closely related area of literature for this work. Plan recognition is a key area of literature for this work, focusing on inferring an actor's plan towards achieving a goal from observations. Approaches explore domain-dependent and independent methods, with some relying heavily on domain knowledge for inference. For example, one solution maps observations to a plan library to recognize adversarial intent, while another constructs domain models to generate plans in computer security. Goal Driven Autonomy (GDA) is also used to allow agents to achieve goals. Recent work in plan recognition explores various approaches such as Goal Driven Autonomy (GDA) and domain-independent goal recognition. One approach involves agents learning knowledge from observations, while another uses existing planners to infer a single agent's plan from observations. These methods offer adaptability to input and leverage existing planning systems. Recent work in plan recognition explores various approaches such as Goal Driven Autonomy (GDA) and domain-independent goal recognition. BID10 introduced the worst-case distinctiveness (wcd) metric for goal recognition ease. BID18 proposed a method combining goal-mirroring and landmarks for efficient goal inference. BID13 integrated plan recognition with landmarks to counterplan and block an opponent's goal achievement. The main difference between plan intervention and recognition lies in their approaches. In plan intervention, timing is crucial compared to plan recognition. User preferences and environmental uncertainties impact the decision-making process. A decision tree is used to identify events for intervention and explain the intervention process. The intervention graph models decision-making in a competitive domain where an attacker aims to disrupt the user's progress. The intervention graph models decision-making in a competitive domain where an attacker aims to disrupt the user's progress. A classifier using domain-independent features predicts intervention accuracy for benchmark domains, but faces challenges with state space explosion in large domains. Sampling from alternative plans generated by off-the-shelf planners is suggested as a solution. The uncertainty model can limit the observer's ability to perceive the current state, and the attack models can be expanded to different threat models. The attacker can disrupt the user's progress by behaving adversarially and guiding them towards different goals. Suggestions for actions to avoid undesirable states during intervention are proposed, integrating causal reasoning into explanations. These extensions pave the way for applying classical planning techniques for decision support and assistive agents."
}