{
    "title": "HJr4QJ26W",
    "content": "Generative Adversarial Networks (GANs) are used to train models that mimic a data distribution. Sometimes, we want to optimize a generative model for specific objectives like creating aesthetically pleasing images. Evaluating these objectives can be challenging, often requiring human input. A system has been developed to efficiently train a GAN to increase positive user interactions, such as aesthetic ratings. By creating a model of human behavior from a small set of interactions, this system uses it as an auxiliary loss function to enhance the generative model. This approach has shown success in improving positive interaction rates across various objectives. Generative image models, like Generative Adversarial Networks (GANs), have advanced rapidly in recent years. GANs train a \"generator\" to create realistic images by fooling a \"discriminator.\" This approach addresses the challenge of learning without a defined objective function for image quality. By incorporating additional constraints into the loss function, such as creativity and innovation in generating art, the network can achieve specific goals beyond simply matching empirical data. In recent years, Generative Adversarial Networks (GANs) have advanced rapidly, training a \"generator\" to create realistic images by fooling a \"discriminator.\" Incorporating creativity and innovation in the loss function makes GANs more creative. Conditioning on image content class and fooling the discriminator on class allows for targeted image generation and improved performance. Evaluating machine learning systems on complex tasks often requires human feedback or real-world testing. The challenge is to efficiently guide generative models towards better results without extensive data collection. In this paper, the focus is on generating images that lead to more positive user interactions, measured by a Positive Interaction Rate (PIR). The PIR can be calculated based on user ratings or interactions with web pages. The challenge lies in determining the features that influence PIR and how to compute it for an image. Empirical testing with users is used to evaluate image quality. In this paper, the focus is on using a small amount of data to tune a generative model to produce images that increase Positive Interaction Rate (PIR). The goal is to collect PIR data on a batch of images and use it to improve the generative model for many gradient steps efficiently. Our approach involves training a \"PIR Estimator Model\" using image and PIR data to predict PIRs on images. Inspired by previous work, we optimize a generative image model with PIR feedback, which differs from traditional RL models. This presents challenges due to the high-dimensional output space, but we benefit from direct PIR information for image evaluation. Our system consists of three components: A generative image model, users who interact with the generated images, and a PIR estimator that models user interactions. The PIR estimation model is used as a loss function for training, allowing us to exploit its knowledge of the objective function. The system includes a generative image model trained with a GAN on landscape images of mountains and coasts. The discriminator and generator are optimized with specific losses to improve image quality. The approach differs from standard GAN formulations, resulting in better image generation. The GAN used in the system was trained on landscape images of mountains and coastlines, producing non-photorealistic images with limited variability. Despite its limitations, tweaking the model for multiple objectives may be more interesting, as it reduces the space for optimal image generation. The generative model used in the system was trained on landscape images of mountains and coastlines, creating non-photorealistic images with limited variability. To improve performance, the model needs to produce data points that accurately estimate a PIR. However, it is challenging to collect user data on a large scale, so the model was trained on a smaller dataset of 1000 images shown multiple times. The system used user data on a number of images to generate datasets. The PIR estimator model is a deep neural network based on the Inception v2 architecture. The model was trained on a smaller dataset of 1000 images shown multiple times. The PIR estimator model is separated from the discriminator in order to keep it constant for accurate training. Once trained, the PIR estimator is used to improve the GAN by adding the estimated PIR to the generator loss with a weight of 1000. In this study, the PIR estimator model is utilized to enhance the GAN by incorporating the estimated PIR into the generator loss with a weight of 1000. The system is trained with adjusted parameters and simulated interaction data to explore various ways of altering the image generation model. The study utilizes the PIR estimator model to enhance the GAN by incorporating the estimated PIR into the generator loss. The system is trained with adjusted parameters and simulated interaction data to explore ways of altering the image generation model, from low-level features to complex semantic features. The approach involves evaluating the system's ability to train for different features using activity from hidden layers of a computer vision model like VGG 16. The VGG features in a hierarchical organization respond to different levels of features, from edges to semantic features like faces. However, optimizing inputs for high-level features does not always produce meaningful images, as CNNs can be fooled by adversarial images. This highlights the challenge of exploiting the parameterization of classification problems in systems targeting human perception. The VGG 16 model may exploit simple features of the given objective to increase performance, but success on this task does not guarantee success in modifying semantic content when interacting with humans. Adversarial examples can transfer between networks with different architectures, suggesting that CNN objectives may be easier for the model to learn than human ones. The system's performance on tasks using different network architectures and datasets may not be reliable, as there is a possibility of the network \"cheating\" on certain tasks. Despite this, evaluating the system's ability to optimize for various objectives from VGG layers can indicate its potential to improve PIRs from real users. Performance was notably poor at layers 5, 6, and 7 of VGG, suggesting a struggle to capture complex features. The system's poor performance at higher layers of VGG may be due to sparse feature representations. To address this, less sparse features were created by targeting a set of k filters sampled without replacement from the layer. This approach aims to simulate a more realistic behavior compared to using a single filter. The system aims to simulate human behavior by targeting multiple features that influence PIR in various ways. Simple objectives include targeting specific colors in output images or vertical bands of different colors. Splitting images along the width is important to capture semantically relevant features related to color divisions. The study focuses on optimizing objectives related to color divisions in images to improve the generality of the system. Results are presented in terms of changes in mean PIR before and after tuning, assessing significance using Cohen's d. Our system successfully improved PIRs across various objective functions, particularly on VGG objectives. However, targeting single filters in certain layers was less successful due to sparse activation. Optimizing for sets of 20 filters showed more consistent results. The system showed a decline in improvement at higher layers of VGG when using 20 filters, indicating less accurate approximations to complex objectives. Despite this, it was successful at optimizing for color objectives, especially single and two-color results. Difficulty was encountered with three-color results initially, but significant improvements were achieved after 500,000 tuning steps. After 500,000 tuning steps, significant improvements were achieved for two out of three color objectives. The system struggled initially with three-color images but showed visible signs of improvement, such as producing images with a blue streak in the middle for the green-blue-red task. Additional analyses can be found in Appendix A. The system can optimize a generative model to produce images targeting various objectives, from low-level visual features to top layers of VGG. It consistently underestimates its performance due to a training procedure detail. Iterating to improve PIRs yields better results for lower VGG layers but not higher ones, showing relative success across a wide range of objective functions. The system successfully estimated various objective functions from only 1000 images, showing the ability to optimize for real human interactions with a small amount of data. This success suggests the model can approximate complex objectives effectively. The model can approximate complex objectives effectively from a small amount of data, even from biased distributions. It remains to be seen if it can capture how background images influence human behavior as well as deep vision architectures. The model can capture how background images influence human behavior and deep vision architectures. It has potential applications in improving GANs with limited human feedback, such as producing better music. Tuning the GAN may result in a trade-off between image diversity and optimality, depending on the desired application. The weight on the PIR loss can be adjusted to balance image diversity and optimality in GAN training. Keeping the generator loss helps prevent overfitting and allows for generating a variety of images. Future directions include potential applications in distillation and possible improvements discussed in the appendix. The work has potential applications in distillation and imitation approaches, showing that deep vision models can be tuned rapidly to emulate the behavior of hidden layers from another architecture. This suggests shared inductive biases among architectures and that final layers represent computations of earlier hidden layers. Our system offers a new approach to feature visualization by optimizing a distribution of images for objectives from CNN layers. It efficiently tunes a generative image model based on various objective functions from deep vision models, demonstrating success with minimal data. Our system efficiently optimizes a generative image model for various objectives from deep vision models, showing success with minimal data. The system's performance is affected by the variability of training data and the number of zeros it contains. It has potential applications in improving machine-generated images, music, and art. The system tends to underestimate its performance by a factor of more than 1.5. The system underestimates its performance by more than 1.5, driven by consistently underestimating PIRs due to changes in softmax temperature. This contrasts with the expectation of overestimation due to overfitting. Complex objectives show lower effect sizes and estimated changes in PIR, with true changes even lower. The system underestimates its performance by more than 1.5, driven by consistently underestimating PIRs due to changes in softmax temperature. Complex objectives show lower effect sizes and estimated changes in PIR, with true changes even lower. The system is somewhat aware of its reduced effectiveness but does not adjust estimates sufficiently. Initial standard deviation explains about 50% of the variance in the change in mean PIR. The system's performance underestimates by more than 1.5 due to changes in softmax temperature. Complex objectives result in lower effect sizes and estimated changes in PIR. Starting with an expressive generative model is crucial for better results. Iterating the process of increasing PIRs and training a new PIR estimator could lead to larger improvements. After optimizing the model, further steps may not yield significant improvements. Models trained on single filters were tuned for PIR estimators and generative models were adjusted using an ACGAN framework. This involved conditioning for user-specific features and evaluating the results. The discriminator and generator losses are modified in the ACGAN framework by adjusting weights for fake and real classes. The generator network includes one-hot class inputs, while the discriminator has class outputs alongside the source output. The generator is a deep neural network starting with a fully-connected mapping from the latent space to a 4x4x512 image, followed by upsampling, convolution, and leaky ReLU nonlinearity. This process is repeated 5 times. The generator in the ACGAN framework is a deep neural network with multiple layers, including upsampling, convolution, and leaky ReLU nonlinearity. It is trained on landscape images using the Adam optimizer, while the discriminator uses RMSProp. The final output images are 64x64 in size. The model in the ACGAN framework was trained with a latent size of 64 units and for 1.1 \u00d7 10^6 gradient steps. Instead of predicting PIR as a scalar directly, it was classified into 100 bins via a softmax for better performance. The PIR estimator was trained with the Adam optimizer and the output softmax's temperature was reduced to 0.01 to improve results. The system's training temperature allows for rapid gradient adjustments, while during actual use, a conservative approach is preferred. Evaluation of the system's training for different features involved using activity from hidden layers of a computer vision model. The PIR for an image was computed based on the 2 norm of filter activity normalized by the total layer's activity. After evaluating the system's training for different features using hidden layers of a computer vision model, it was observed that performance was poor at layers 5, 6, and 7 of VGG. These layers had sparse feature representations, with a high percentage of zero PIRs. This suggests that the system struggled to capture complex features at higher levels of VGG. The layers 5, 6, and 7 of VGG had over 90% zero PIRs, making learning infeasible. Sparse features in these layers affected system performance, leading to difficulties in capturing complex features. To address this, less sparse features were created by targeting a set of k filters sampled without replacement from the layer. To address the issue of sparse features in layers 5, 6, and 7 of VGG, a method was implemented to compute the PIR for an image by summing the squared norms of k filters. This approach aimed to create less sparse features by targeting a set of k filters sampled without replacement from the layer. The addition of binomially distributed noise to these PIRs simulated a more realistic human behavior scenario, where multiple related features influence PIR in various ways. This evaluation was crucial to assess the system's ability to target these types of features effectively. Finally, simpler objectives were considered based on targeting specific colors in output images. PIRs were computed from the vector norm of images in the targeted color, normalized by total image value. Different color objectives were explored, such as three color tasks splitting the image into thirds. These objectives complement the VGG objectives discussed earlier. Split color tasks are less likely to be relevant to classification, emphasizing the importance of splitting images along the width for semantically relevant features. Exploring techniques to improve the system, such as iterating for multiple steps of PIR collection and generative model tuning, and considering data scaling for better performance on tasks with low variance. Normalizing all data was briefly attempted. One alternative approach to training a GAN for high-PIR images is to use the PIR estimator objective in the Plug & Play Generative Networks framework instead of tuning the GAN itself. This could be a promising direction to explore, although success may depend on the initial generative model's expressiveness. It may be better to tune the model directly with the current mediocre model to explore new areas of image space."
}