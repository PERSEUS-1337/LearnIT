{
    "title": "SylnYlqKw4",
    "content": "To address the challenge of training multi-task neural networks, a method using knowledge distillation with teacher annealing is proposed. This approach helps a multi-task model surpass its single-task teachers and consistently improves performance on the GLUE benchmark. Jointly learning to perform multiple tasks effectively in Natural Language Processing has been difficult, but this method shows promise in enhancing multi-task training. Our work extends born-again networks to the multi-task setting by proposing a way of applying knowledge distillation so that single-task models effectively teach a multi-task model. Knowledge distillation transfers knowledge from a \"teacher\" model to a \"student\" model by training the student to imitate the teacher's outputs. In this approach, the teacher and student have the same neural architecture and model size, but surprisingly the student is able to surpass the teacher's accuracy. Distillation is effective because the teacher's output distribution over classes provides more training signal than a one-hot label, capturing additional information about training examples. We compare Single\u2192Multi born-again distillation with several other variants and explore performing multiple rounds. Our work proposes a teacher annealing method to improve student model performance, transitioning from learning from the teacher to learning from gold labels. The Born-Again Multi-tasking (BAM) training method consistently outperforms standard single-task and multi-task training, building upon self-supervised pre-training and multi-task fine-tuning success. The method involves distilling single-task \"teacher\" models to multi-task models, showing benefits in better regularization. The text discusses the use of Single\u2192Multi method for distilling single-task \"teacher\" models into multi-task \"student\" models, focusing on multi-task training algorithms in neural networks. It mentions the benefits of distilling large models into smaller ones and using knowledge distillation to aid in multi-task learning. The work is distinct from recent NLP research on neural architecture design and task hierarchies. In reinforcement learning and NLP, knowledge distillation is used to regularize multi-task agents and distill single-language-pair machine translation systems into a many-language system. Several recent works explore fine-tuning BERT using multiple tasks, focusing on finding beneficial task pairs or designing improved task-specific components on top of BERT. All models are built on top of BERT, passing input sentences through a Transformer network to produce contextualized token representations. In multi-task models built on top of BERT, token embeddings and Transformer are initialized with weights from a self-supervised pre-training phase. Training involves single-task training as in BID8 and multi-task training where examples of different tasks are shuffled together. Less than 0.01% of the parameters are task-specific, with task-specific classifiers on top of BERT. Knowledge distillation trains the model to match the predictions of a teacher model with the same architecture, improving training by utilizing the full distribution over labels provided by the teacher. For regression tasks, the student is trained to minimize the L2 distance between its prediction and the teacher's instead of using cross-entropy loss. In knowledge distillation, teacher annealing is proposed to address the concern that the student may be limited by the teacher's performance. This method mixes the teacher prediction with the gold label during training by linearly increasing a term in the summation from 0 to 1. During training, the model transitions from distilling knowledge to relying on gold-standard labels to surpass its teachers. The General Language Understanding Evaluation (GLUE) benchmark consists of 9 natural language understanding tasks. Task sampling is used to prevent tasks with large datasets from dominating training. During training, task sampling prevents large datasets from dominating. The layerwise-learning-rate trick is used to adjust learning rates based on layer proximity to output. Hyperparameters vary for single-task and multi-task models, with different decay rates and batch sizes. All models utilize BERT-Large pre-trained weights. Dev set results report average scores for different tasks. Dev set results show average scores on GLUE tasks, with a focus on the impact of different training methods. Multiple trials with varying seeds are crucial due to significant score variations. Single-task, multi-task, and distillation methods are compared, with distillation showing promising results across tasks. Knowledge distillation improves or matches the performance of other methods on various tasks, except for STS in GLUE, a regression task. The gain for Single\u2192Multi over Multi is significant, suggesting distillation works well with multi-task learning. Single\u2192Multi distillation performs better than Multi\u2192Multi, possibly due to exposure to a diverse set of teachers. Further research is needed to fully understand this phenomenon. Additionally, training Single\u2192Multi\u2192Single\u2192Multi models did not show a significant difference compared to Single\u2192Multi. The Single\u2192Multi distillation method consistently outperforms standard single-task and multitask training, showing robustness in performance. Comparisons against recent work on the GLUE leaderboard demonstrate the effectiveness of Single\u2192Multi distillation. Multiple models are trained, and the one with the highest average dev set score is submitted to the test set. Our work outperforms existing results without ensembling. Test set numbers should be interpreted cautiously due to variance between trials. We do not single-task fine-tune our model, unlike other methods in the comparison. Single-task fine-tuning after multi-task training may improve results but goes against the purpose of multi-task learning. It leads to individual models for each task instead of one model capable of performing all tasks. Evaluating the benefits of single-task fine-tuning, it shows slight improvements over Single\u2192Multi distillation, indicating that distillation captures many benefits of single-task training. The importance of teacher annealing and training tricks in improving scores is shown in the ablation study. Using pure distillation without teacher annealing performs no better than standard multi-task learning. Comparing combinations of tasks, training on a large number of tasks helps regularize multi-task models and transfer learned knowledge between related tasks. The study shows that training on a combination of tasks improves performance in multi-task models. Single\u2192Multi distillation with teacher annealing produces better results than standard single-task or multi-task training. This approach enhances multi-task gains across various tasks, making multi-task learning more useful in NLP. The study demonstrates that multi-task training enhances performance in NLP models, with gains from self-supervised tasks like BERT surpassing those from closely related tasks with small datasets. The effectiveness of \"self-supervised pre-training is all you need\" versus transfer/multi-task learning from supervised tasks remains to be fully understood."
}