{
    "title": "rkZvSe-RZ",
    "content": "Adversarial examples are used to deceive machine learning models, with adversarial training incorporating these examples to enhance robustness. However, this training method can lead to convergence to a degenerate global minimum, resulting in vulnerability to black-box attacks and a new single-step attack that evades defenses. Ensemble Adversarial Training enhances model robustness by adding perturbations from other models during training. This technique proved effective on ImageNet, winning a competition on Defenses against Adversarial Attacks. Adversarial examples can deceive machine learning models, making them vulnerable to attacks. Adversarial training can increase robustness, even against white-box attacks. Adversarial training on ImageNet with Inception v3 model was successful against white-box attacks but vulnerable to multi-step attacks. The model's robustness did not extend to black-box adversaries, as demonstrated formally and empirically. Adversarial training with single-step methods leads to a degenerate global minimum, making models vulnerable to simple attacks. The decision surface of the model shows sharp curvature near data points, reducing the effectiveness of attacks based on a single gradient computation. This vulnerability is observed in various models, including Inception ResNet v2 and models trained on MNIST. Single-step attacks can easily transfer perturbations from undefended models to adversarially trained ones. Our approach involves applying a small random perturbation before linearizing the model's loss, outperforming the Fast Gradient Sign Method. We also introduce Ensemble Adversarial Training, which increases robustness to adversarial examples transferred from other models. Our methods reduce the dimensionality of the space of adversarial examples. Our Inception ResNet v2 model won the NIPS 2017 competition on Defenses Against Adversarial Attacks, showing promise in learning robust models through adversarial training on MNIST. Recent works have even achieved certifiable robustness for small perturbations on MNIST. The MNIST dataset's unique denoising procedure leads to similarly robust models without adversarial training. The existence of a simple robust baseline for MNIST can help understand limitations of adversarial training techniques. Adversarial examples transfer between models, enabling black-box attacks on deployed models without access to training data. Adversarially trained models may still be vulnerable to black-box attacks, as shown by prior works. Adversarial training can be evaded by transferring larger perturbations. Adversarial training degrades the accuracy of linear approximations of the model's loss. The goal of the adversary is to find an adversarial example close to the original input but misclassified by the model. Adversarial training aims to improve robustness against attacks by minimizing risk over adversarial examples. White-box adversaries have access to the model's parameters, while black-box adversaries have limited information. Security against white-box attacks is ideal, but black-box security is more practical for deployed ML models. Adversarial training is a variant of Empirical Risk Minimization, with a focus on defending against attacks. Adversarial training uses attacks to approximate solutions for inner maximization problems and trains on both clean and adversarial examples. Three algorithms are considered for generating adversarial examples with bounded \u221e norm, including Fast Gradient Sign Method (FGSM) and Single-Step Least-Likely Class Method (Step-LL). The Iterative Attack method, known as I-FGSM or Iter-LL, iteratively applies FGSM k times with step-size \u03b1 \u2265 /k and projects each step onto the \u221e ball of norm around x. It uses projected gradient descent to solve the maximization problem. Iterative attacks induce higher error rates but transfer at lower rates compared to single-step attacks. Adversarial training with a single-step attack approximates the inner maximization problem by replacing the solution with adv FGSM. The alternative optimization problem involves finding global minima h* that are robust to perturbations and poorly fit the attack method. This \"degenerate\" minimum can be more subtle than overfitting and creates adversarial examples that are easy to classify. Adversarial training does not simply learn to resist the attack used during training. Adversarial training aims to make attacks perform worse overall, not just resist them. Reward Hacking can lead to unintended behavior in maximizing objectives. To avoid degenerate minima, a stronger adversarial example generation process or training an adversarial generator model like GANs is suggested. Decoupling the generation of adversarial examples from the model is proposed as a simpler approach. Ensemble Adversarial Training decouples the generation of adversarial examples from the model being trained, enhancing robustness to black-box adversaries by using perturbations crafted on external pre-trained models. This method increases resistance to black-box attacks and draws parallels to multiple-source Domain Adaptation. Ensemble Adversarial Training involves evaluating a model on samples from a target distribution using adversarial examples obtained from external pre-trained models. This method enhances robustness to black-box adversaries and improves resistance to attacks. The text discusses the limitations of current methods in providing guarantees against arbitrary future adversaries, especially for ImageNet-scale tasks. It mentions the challenges in extending guarantees to various non-interactive black-box adversaries and highlights the existence of a degenerate minimum in adversarially trained models. The text evaluates the effectiveness of the Step-LL attack on adversarially trained models, showing a drop in approximation ratio for models like Inception v3 and Inception ResNet v2. The results suggest that the learned models are less amenable to linearization compared to standard models. The text discusses the impact of the Step-LL attack on adversarially trained models, highlighting a decrease in similarity between perturbations for different models. It also explores the \"gradient-masking\" effect by visualizing the loss landscape of the models. The text discusses the impact of input dropout on adversarially trained MNIST models, showing that it can mitigate overfitting in some cases. However, input dropout significantly degrades accuracy on ImageNet. BID20 found their adversarially trained model to be robust to single-step attacks, but the robustness is misleading as the model has learned to degrade information. The v3 adv model is more vulnerable to single-step attacks than previously predicted, both in white-box and black-box settings. Similar results were found for the IRv2 adv model and other models like Inception v3, Inception v4, and Inception ResNet v2. Adversarial training increases robustness to white-box attacks but incurs a higher error rate in black-box attacks. The robustness gain observed in defended models against white-box attacks is misleading, as they incur a higher error rate in black-box settings. Researchers are advised to consider both types of adversaries when evaluating defensive strategies. Adversarial training not only overfits to perturbations affecting standard models but also degrades the linear approximation underlying single-step attacks. The loss function visualization in Figure 1 highlights the need to prepend single-step attacks with a small random step to escape sharp curvature artifacts near data points. The R+FGSM attack, requiring only a single gradient computation, is a computationally efficient alternative to iterative methods. It outperforms the Step-LL method in error rates for all models, even those without adversarial training. The addition of a random step in attacks improves model performance, even without adversarial training. Comparing R+Step-LL to two-step Iter-LL attacks, R+Step-LL is stronger for the Inception v3 model. However, the random step hinders transferability. Adversarial training using R+FGSM on MNIST shows little robustness to iterative attacks. Ensemble Adversarial Training strategy aims to improve model robustness by augmenting training data with adversarial examples from pre-trained models. The training batch rotates the source of adversarial examples between the current model and pre-trained models to diversify examples across epochs. Ensemble Adversarial Training reduces per-batch cost by precomputing gradients for the full training set. Synchronous distributed training on 50 machines with minibatches of size 16 is used. RMSProp with a learning rate of 0.045 is employed, and robustness to black-box attacks is evaluated by transferring attacks crafted on holdout models. Ensemble Adversarial Training involves using an ensemble of models and various attack methods to evaluate robustness. Results are shown in Table 4, with convergence speed being slower due to training on \"hard\" adversarial examples and lower batch sizes. The v3 adv model achieves 78% accuracy after 187 epochs, while Inception ResNet v2 model converges at around 160 epochs. Ensemble Adversarial Training on ImageNet involves models trained with various attack methods to evaluate robustness. The models are slightly less accurate on clean data compared to standard adversarial training and more vulnerable to white-box single-step attacks. The negative impact is noted for model v3 adv-ens4 due to a lower proportion of white-boxStep-LL samples seen during training. Ensemble Adversarial Training aims to improve robustness by diversifying adversarial examples seen during training. However, it shows a large negative impact on white-box attacks while providing only minor gains in robustness to transferred samples. The main benefit lies in decoupling attacks from the model being trained. Larger models may be needed to achieve robustness against white-box attacks. Ensemble Adversarial Training significantly boosts robustness to attacks transferred from holdout models, with FGSM attacks being the strongest. Attacking an ensemble of holdout models does not result in stronger black-box attacks. Results are consistent across different attack parameters and holdout models. In the NIPS 2017 competition on adversarial examples, the Inception ResNet v2 model was a baseline defense. The IRv2 adv-ens model ranked 1st with a score of 95.3%, outperforming other defenses. This demonstrates the effectiveness of the approach on different datasets and model architectures. The winning submission in the NIPS 2017 competition on adversarial examples achieved a score of 95.3% with a novel denoising technique. The second placed defense scored 92.4% by using the IRv2 adv-ens model with random preprocessing. The winning defense in the NIPS 2017 competition achieved a score of 95.3% with a denoising technique, while the second placed defense scored 92.4% using the IRv2 adv-ens model with random preprocessing. Ensemble Adversarial Training improved defenses against black-box adversaries, with the best defense achieving 53.6% accuracy against all attacks, including white-box attacks. Gradient masking was reduced with Ensemble Adversarial Training, resulting in lower losses for certain models. The text discusses the performance of different attack methods on models, showing improvements in approximation ratios and gradient masking reduction. Additionally, the \"Gradient-Aligned Adversarial Subspace\" method is revisited, highlighting the estimation of adversarial examples in the vicinity of a point. The text discusses finding orthogonal vectors aligned with the model's gradient for adversarial perturbations. Results for different models and perturbation limits are shown in FIG2. In FIG2, results show the proportion of points with orthogonal adversarial perturbations for different models. The Inception v3 model exhibits a bimodal phenomenon with a high-dimensional adversarial space. Ensemble Adversarial Training produces a more robust model with fewer points near large adversarial spaces. Previous work has shown strong robustness to adversarial examples. The results show that adversarially trained models are vulnerable to black-box and white-box attacks. Ensemble Adversarial Training improves robustness by transferring attacks from some models to others. Generative techniques may lead to stronger attacks, but Ensemble Adversarial Training has shown resilience. Interactive adversaries can exploit queries to the target model's prediction function in their attack, estimating the target's gradient and using the R+FGSM attack. The impact of black-box attacks on complex tasks and their scalability is an interesting area for future research. Formal definitions for the threat model are provided, with the hypothesis space describing the model's architecture. The target model h is trained over inputs sampled from a data distribution using a randomized training procedure. An adversary produces adversarial examples to evaluate the target model's error rate. Three types of adversaries are defined, including a white-box adversary with access to all elements of the training procedure and model architecture. The non-interactive black-box adversary only has access to the target model's training procedure and architecture. They can sample from the data distribution and use a local algorithm to craft adversarial examples. This type of adversary relies on transferability attacks, where they train a local model and compute adversarial examples using white-box attack strategies. The black-box adversaries in the study are stronger as they use the same training data as the target model and have knowledge of defensive strategies applied during training. The focus is on non-interactive black-box attacks that train a local model with a different architecture than the target model, which has been found to be stronger in this case. The main focus is on non-interactive black-box adversaries. A stronger notion of interactive black-box adversaries is also formalized. Adversaries issue prediction queries to the target model and craft adversarial examples using oracle queries. Adversaries may not have direct access to the model's query interface in certain cases. Interactive black-box adversaries can launch attacks with limited data samples from D. By utilizing queries to estimate the model's gradient, they can apply attacks even with minimal information. The generalization guarantees of Ensemble Adversarial Training are discussed, assuming training solely on pre-trained models' adversarial examples. The results can be extended to include clean training data and adversarial examples from the model being trained. At test time, the model is evaluated on adversarial examples from A*. The average discrepancy distance between distributions A i and A* with respect to a hypothesis space H characterizes the difference in robustness between models. The distance disc(A train, A*) is small when the robustness difference between models to target attack A* is similar to the robustness difference to attacks used for training. The ranking of model robustness should be consistent for both A train and A*. The average Rademacher complexity of distributions A1,...,Ak tends to 0 as N increases. The generalization bound for Domain Adaptation includes an extra term compared to standard supervised learning bounds. The generalization bound for Domain Adaptation in supervised learning includes an extra term to capture the divergence between target and source distributions. Ensemble Adversarial Training guarantees generalization bounds for future adversaries not too different from those during training. ImageNet experiments on MNIST show robustness to white-box attacks, extending observations on limitations of single-step adversarial training. The dataset serves as a baseline for defense assessment, but results may not generalize to harder tasks due to the near-binary nature of the data. Binarizing inputs of a standard CNN without adversarial training yields a model with robustness similar to adversarially trained models, with minimal differences in input dimensions. This simple robust representation raises questions on the necessity of adversarial training for achieving robustness. Techniques to enhance adversarial training performance, even on simple tasks, are sought after. Techniques to improve adversarial training performance, even on simple tasks like MNIST, could provide insights for more complex tasks. Positive results on MNIST for the \u221e norm raise questions about defining a general norm for adversarial examples. By slightly extending the threat model, models can be evaded, highlighting the need to enumerate all types of \"adversarial\" perturbations. In this work, the focus is on the limitations of single-step adversarial training on ImageNet and MNIST, showcasing the benefits of Ensemble Adversarial Training. Experiments are repeated on MNIST using specific architectures. Adversarial training is conducted using FGSM to avoid label leaking effects. The \"degenerate\" minimum of adversarial training is analyzed, computing the approximation-ratio of the FGSM for the inner maximization problem. The results of adversarial training using FGSM on different model architectures are compared in TAB7. Input dropout is found to limit the negative effects of adversarial training, especially in architecture B. Without input dropout, the single-step attack degrades significantly. For fully connected architecture D, the learned model is close to linear and less prone to degenerate solutions. TAB8 compares error rates of undefended and adversarially trained models on whitebox and black-box attacks. The positive effect of input dropout in adversarial training is discussed, particularly in model B. Input dropout helps avoid degradation of single-step attacks but delays model convergence. Model B retains high error rates on white-box FGSM examples. Input dropout can be compared to training with a randomized single-step attack. The effect of input dropout is specific to architecture and dataset. Ensemble Adversarial Training significantly increases robustness against black-box attacks on models A, B, C, and D. Input dropout has a marginal benefit on models A, C, and D but degrades accuracy on ImageNet, so it was not incorporated into the models. The models were trained for 12 epochs and evaluated using FGSM, I-FGSM, and PGD attacks with a loss function from BID7. Results are shown in TAB10, reporting worst-case and average-case error rates. Ensemble Adversarial Training improves robustness to black-box attacks, with model B adv-ens showing better performance than B adv. Using three pre-trained models can further enhance robustness, especially against attacks from models with the same architecture. The R+Step-LL attack introduces random perturbations to enhance robustness against adversarial attacks. Transferability of these perturbations on ImageNet is evaluated, showing lower transfer rates compared to the deterministic variant. The task of finding orthogonal vectors that create adversarial examples is discussed, with a focus on aligning vectors with the model's gradient. The optimal construction for the \u221e norm based on Regular Hadamard Matrices BID11 involves finding orthogonal vectors aligned with the signed gradient. A result bounds the number of orthogonal perturbations that can be found for a given alignment with the signed gradient. Trivial construction of k orthogonal vectors somewhat aligned with sign(g) is considered."
}