{
    "title": "SyVU6s05K7",
    "content": "Learning deep neural networks involves solving a complex optimization problem with a large number of terms. The current practice relies on stochastic gradient descent (SGD) or its adaptive variants, which require manual tuning of the learning rate. A new optimization method based on a composite proximal framework offers good generalization performance with only one hyper-parameter. This approach leverages the compositional nature of deep neural networks and utilizes the Frank-Wolfe (FW) algorithm for SVM. The Frank-Wolfe (FW) algorithm is used for SVM optimization in deep neural network design. It computes an optimal step-size at each time-step and provides a descent direction through a simple backward pass in the network. Experiments on CIFAR and SNLI datasets show the superiority of this method over other algorithms like Adam and Adagrad. The algorithm is compared to SGD with a hand-designed learning rate schedule, showing similar generalization but faster convergence. The code is available at https://github.com/oval-group/dfw. The SGD algorithm, while effective for learning tasks, requires a learning rate decay schedule. In convex settings, curvature properties can design hyper-parameter free schedules. However, for non-convex optimization like deep neural networks, there is no practical solution. Different learning rate schedules are used for training deep networks with SGD, resulting in competitive results but lacking a unified approach. Adaptive gradient methods have been developed to address the issue of finding a consistent methodology for learning tasks, but they have been shown to have worse generalization than SGD. To bridge this performance gap, a new optimization algorithm called Deep Frank-Wolfe (DFW) has been introduced, leveraging the composite structure of deep neural networks. The Deep Frank-Wolfe (DFW) algorithm leverages the composite structure of deep neural networks to design an optimization algorithm that uses efficient convex solvers. It considers a nested optimization problem with the loss as the outer function and the neural network function as the inner one. Each iteration defines a proximal problem with a linearized inner function and exact outer function, creating a linear SVM when the loss is the hinge loss. This method offers advantages over the SGD algorithm by exploiting more information about the learning objective while maintaining the same computational cost and computing an optimal step-size using the Frank-Wolfe (FW) algorithm. The Deep Frank-Wolfe (DFW) algorithm optimizes deep neural networks efficiently by computing an optimal step-size using the FW algorithm. It eliminates the need for a hand-designed learning rate schedule, providing competitive generalization error compared to SGD. Additional improvements include smoothing the loss function to avoid optimization difficulties and incorporating Nesterov momentum for acceleration. The method is demonstrated on image classification with CIFAR datasets using wide residual networks and densely connected convolutional neural networks. The DFW algorithm optimizes deep neural networks efficiently by computing an optimal step-size using the FW algorithm. It outperforms previous methods based on adaptive learning rates and provides comparable accuracy to SGD with hand-designed schedules. The algorithm preserves information from the loss function, gives an optimal step-size at each iteration, and offers comparable generalization to SGD with a hand-designed schedule. The DFW algorithm optimizes deep neural networks efficiently by computing an optimal step-size using the FW algorithm. It outperforms previous methods based on adaptive learning rates and provides comparable accuracy to SGD with hand-designed schedules. The algorithm offers comparable generalization to SGD with a hand-designed schedule, converging faster and requiring only a single hyperparameter. Gradient-based methods remain popular for learning deep neural networks, while more sophisticated techniques have limitations in scalability due to their large per-iteration cost. One challenge of using SGD is designing a learning rate schedule. Alternative methods have been proposed that modify the descent direction or adaptively rescale the step-size. However, adaptive variants of SGD may lead to subpar generalization. Learning to learn approaches optimize deep neural networks by learning the learning rate. Meta-learning approaches have also been suggested to learn the optimization algorithm. The optimization algorithm BID0 BID20 BID31 BID7 could benefit from using DFW to optimize the meta-learner. Path-SGD algorithm by BID15 promotes generalization within the optimization algorithm but has restrictions on the model's non-linearity. Various works analyze how optimization algorithms implicitly regularize deep neural networks, but the phenomenon is not fully understood. The back-propagation algorithm has been analyzed in a proximal framework, requiring the same hyper-parameters as SGD but with higher computational cost. Linear SVM sub-problems are formulated in our approach, with previous works showing training neural networks with piecewise linear activations using different algorithms. Closest to our approach are works that suggest creating local SVMs based on first-order Taylor expansion and proximal terms to minimize error and weight changes. Our approach involves formulating linear SVM sub-problems using a first-order Taylor expansion. Previous works, such as BID26, have also explored this method in a mini-batch setting. While their approach offers valuable insights from a statistical learning theory perspective, it is less practical due to higher costs per iteration. In contrast, our empirical work leverages a powerful solver for state-of-the-art scalability and performance. The derivative can be a gradient, Jacobian, or directional derivative. The first-order Taylor expansion of \u03c6 is introduced. Stochastic algorithms process one sample per iteration. A data set (x i , y i ) is given to estimate a parameterized model f. The model can be a linear map or deep neural network. The loss function computes the risk of prediction scores given the ground truth label. Cross-entropy and hinge loss are common examples. Hinge loss is more robust to over-fitting when combined with smoothing techniques. Regularization is denoted by \u03c1(w) and the learning problem is written under its empirical risk. Our main contribution is a formulation that exploits the composite nature of deep neural networks to improve the objective approximation. This approach generates sub-problems suitable for efficient optimization by convex solvers. The proximal gradient perspective on SGD is presented to understand the intuition of our approach. The SGD algorithm selects a random sample at each iteration and updates parameters based on the objective estimate and learning rate. SGD minimizes a first-order approximation of the objective while encouraging proximity to the current estimate. Linearizing only a part of the composite objective yields optimization problems with different properties. The suggested approach allows for fast optimization with convex solvers and preserves information about the learning task by keeping an exact loss function. The proximal problem linearizes the regularization \u03c1 and model f j, not the loss function L. Different approximations are illustrated on a synthetic composite objective function, showing how the LPL approximation matches the objective curve more closely than the SGD approximation. Adding an identical proximal term to both approximations results in the iterate w LPL t+1 getting closer to the solution w * than w SGD t+1. The LPL approximation gets closer to the solution than SGD, especially when the learning rate is large. By preserving the loss function, good performance can be achieved with a fixed hyper-parameter. This approach accurately learns deep neural networks with a single hyper-parameter, offering similar performance to hand-designed schedules. The focus is on optimizing a multi-class hinge loss. Our contribution is the Deep Frank-Wolfe (DFW) algorithm for neural networks. The optimal step-size \u03b3 t \u2208 [0, 1] can be obtained in closed-form for problem (6) with a hinge loss, making it computationally efficient. This approach is applicable to any convex and piecewise linear loss function. The Deep Frank-Wolfe (DFW) algorithm for neural networks efficiently computes the conditional gradient of the dual by taking a single step per proximal problem, at the same cost as a standard stochastic gradient. This allows for batch Frank-Wolfe updates in the dual with the same parallelism as SGD over mini-batch samples. The Deep Frank-Wolfe (DFW) algorithm efficiently computes the conditional gradient of the dual by taking a single step per proximal problem, with the same parallelism as SGD over mini-batch samples. The algorithm automatically adjusts the step-size \u03b3 t to decay the effective learning rate, eliminating the need for manual design of a learning rate schedule. The DFW algorithm performs one step per proximal problem, which helps it converge faster than SGD. Two improvements for deep neural networks are presented: smoothing the SVM loss and using Nesterov Momentum. The DFW algorithm incorporates Nesterov momentum for acceleration, with the momentum coefficient typically set to 0.9. The step-size is computed in closed-form in step 10 of the algorithm. Only the hyper-parameter \u03b7 will be tuned in experiments, with batch-size, momentum, and epochs kept the same as baselines. The DFW algorithm utilizes Nesterov momentum for acceleration, with the step-size and search direction computation detailed in the appendix. The algorithm's implementation is straightforward, with linear computational cost. While not guaranteed to converge, empirical results show effective minimization of the learning objective. Comparisons with state-of-the-art optimizers demonstrate DFW's superior performance across various datasets and architectures. The DFW algorithm shows competitive performance compared to AMSGrad and BPGrad, offering superior results in some cases. The algorithm uses Nesterov momentum for acceleration and has linear computational cost. Experiments were conducted on CIFAR-10/100 datasets with models trained on a single Nvidia Titan Xp card. The code is available on GitHub at https://github.com/oval-group/dfw. We split the training set into 45,000 training samples and 5,000 validation samples, and use 10,000 samples for testing. Images are centered and normalized per channel. Two modern architectures of deep convolutional neural networks are used: wide residual networks (WRN) and densely connected convolutional networks (DN). WRN has 8.9M parameters, while DN has 1.9M parameters. The experimental details follow the protocol of BID35 and BID3, with a slight variation in the training and validation sample sizes. We use 45,000 training samples and 5,000 validation samples for adaptive methods. Deep Frank Wolfe (DFW) uses SVM loss, while baselines use Cross-Entropy (CE) loss for better performance. We compare DFW to Adagrad, Adam, AMSGrad, and BPGrad for adaptive learning rates. Initial learning rate is cross-validated as a power of 10. SGD with momentum is also evaluated. A budget of 200 epochs for WRN and 300 epochs for DN is set, with batch sizes of 128 and 64 respectively. L2 regularization is 10^-4 for DN and cross-validated for WRN. Results show that DFW outperforms adaptive gradient methods, especially on the challenging CIFAR-100 dataset. DFW achieves higher testing accuracy on WRN-CIFAR-100 compared to other methods. DFW outperforms adaptive methods on WRN-CIFAR-100 with a testing accuracy 7% higher than others and 1% better than hand-designed SGD schedule. DFW converges faster than SGD, reaching final performance several times quicker. The step-size in DFW automatically decays, unlike the manual tuning in SGD. Data augmentation is used to boost final accuracy by randomly flipping and cropping images. Adaptive methods allow batch size to be chosen as 1x, 2x, or 4x the original value. Results are provided for SGD, DFW, and AMSGrad due to computational cost. Hyper-parameters for SGD are kept the same as in previous studies. The results show that DFW consistently outperforms AMSGrad on various tasks, with up to a 7% improvement in accuracy on WRN-100. The Stanford Natural Language Inference (SNLI) dataset consists of 570k sentence pairs labeled as entailment, neutral, or contradiction, aiding in learning text semantics through a three-way classification problem. The SNLI dataset is valuable for natural language processing transfer learning. The study uses a bi-directional LSTM model with various optimization algorithms. Nesterov momentum is not used, and results are reported on Adagrad, Adam, AMSGrad, and BPGrad. The DFW algorithm is implemented for optimization. In this experiment, the DFW algorithm is used for optimization, replacing the CE loss with an SVM. The conditional gradient direction is utilized, and the initial proximal term is cross-validated. Results show improved testing accuracy compared to previous studies. The DFW algorithm outperforms adaptive gradient methods and matches SGD performance with a hand-designed learning rate schedule. The impact of implicit regularization on optimization and generalization is discussed, showing how changing the learning rate and epochs affects training objectives and validation accuracy. The impact of decreasing momentum on accuracy across convolutional architectures is observed. Accurate optimization is less crucial for generalization compared to a high learning rate's implicit regularization. DFW optimizes the learning objective accurately, but its good generalization is attributed to maintaining a high learning rate early on. SGD's good generalization may stem from its schedule with many steps at a high learning rate. The importance of the initial learning rate \u03b7 on training and validation accuracy is quantitatively analyzed. The impact of the initial learning rate \u03b7 on training and validation accuracy is analyzed by comparing DFW and SGD algorithms on CIFAR datasets. While both methods achieve high accuracy on the training set, the validation performance is sensitive to the choice of \u03b7. SGD may outperform DFW in some cases due to its hand-designed schedule enforcing a decay of \u03b7, while DFW relies on automatic decay of the step-size for convergence. Choosing a larger batch size and a higher learning rate (\u03b7) can lead to better generalization in the DFW algorithm. Empirical evidence shows that a high initial learning rate is crucial for good generalization in convolutional neural networks. This approach resulted in improved test accuracy on CIFAR-100 dataset with the DN architecture. DFW is an efficient algorithm for training deep neural networks that outperforms adaptive gradient methods and achieves similar performance to SGD without needing a hand-designed learning rate schedule. The algorithm allows for the training of deep neural networks to benefit from advancements in optimization algorithms for linear SVMs. Further analysis is needed to understand the varying impact of learning rate and optimizers on different recurrent neural network architectures. The algorithm DFW enables efficient training of deep neural networks, leveraging optimization advancements for linear SVMs. It also supports structured prediction loss functions for tasks like image segmentation. The challenge lies in designing effective optimization algorithms that include all necessary regularization for good generalization in deep learning. This work was supported by various EPSRC grants and Yougov. In this section, we prove results for a specific instance of the Structural SVM problem using a hinge loss. The notation is simplified for clarity, and we rewrite the problem as the sum of a quadratic term and a pointwise maximum. In this section, we prove results for a specific instance of the Structural SVM problem using a hinge loss. We rewrite problem (9) as the sum of a quadratic term and a pointwise maximum of linear functions. The Lagrangian dual of the problem is given by a specific formula, and the primal can be computed using dual variables \u03b1 as \u0175 = \u2212A\u03b1. The KKT conditions of the inner minimization problem are derived, leading to the optimal step-size in the primal-dual algorithm for solving the Structural SVM problem with a hinge loss. The primal-dual algorithm presented here solves a specific optimization problem by initializing parameters and iteratively updating them until convergence. The algorithm involves choosing a direction, calculating optimal step sizes, and updating parameters based on conditional gradients. When a single step is taken, the algorithm simplifies, leading to a cost per iteration calculation. The update for linear SVMs involves the direction of the dual conditional gradient, which is determined by the negative sub-gradient of the primal. Applying this to the Taylor expansion of the network at iteration t = 1, the conditional gradient is derived. By utilizing a first-order Taylor expansion and the chain rule, it is shown that the conditional gradient is a stochastic gradient. The next lemma proves equation FORMULA5 for the Proximal Frank-Wolfe algorithm with a single step. The Proximal Frank-Wolfe algorithm with a single step involves rewriting steps 6, 8, and 9. The algorithm assumes a single step of FW and proves equations related to the SVM loss smoothing. This smoothing is necessary for effective training of deep neural networks. The Proximal Frank-Wolfe algorithm with a single step involves rewriting steps 6, 8, and 9 to smooth the dual BID12 without introducing a temperature hyper-parameter. DFW can be applied with any feasible direction in the dual, using the well-conditioned and non-sparse gradient of cross-entropy. Proposition 3 states that the CE gradient in the primal gives a feasible direction in the dual, allowing for inexpensive detection of when this direction cannot improve the dual. Switching to the conditional gradient is automatic in such cases. The gradient of cross-entropy in the primal provides a feasible direction in the dual, allowing for inexpensive detection of when this direction cannot improve the dual objective. This direction can be used as a dual ascent direction, ensuring improvement on the dual objective. The gradient of cross-entropy in the primal provides a feasible direction in the dual for inexpensive detection of when improvement is not possible. This direction can be used as a dual ascent direction, ensuring enhancement of the dual objective. A sufficient condition for s to be an ascent direction in the dual is when s v > 0, as shown through the Proximal Frank-Wolfe algorithm. The Proximal Frank-Wolfe algorithm has shown that if s v > 0, then \u03b3 t > 0, indicating s is an ascent direction. In practice, the approximate condition considers T w0 f x (w) \u2248 f x (w 0), allowing for an inexpensive computation. The initialization step in dual optimization is not informative and can be discarded from momentum velocity. In this section, convergence plots of different algorithms on CIFAR datasets without data augmentation are provided. Training performance may exhibit oscillations due to cross-validating the initial learning rate based on validation set performance. Sometimes, a lower learning rate yielding better training convergence is not selected for optimal validation performance."
}