{
    "title": "SJCPLLpaW",
    "content": "DeePa is a deep learning framework that optimizes parallelism at the granularity of each layer in convolutional neural networks, achieving up to 6.5\u00d7 speedup compared to other frameworks. It reduces data transfers by up to 23\u00d7, addressing the increasing compute-intensive and time-consuming nature of training CNNs. Existing frameworks like TensorFlow and PyTorch parallelize training using image parallelism on multiple processors. DeePa is a deep learning framework that explores parallelism in all dimensions to accelerate CNN training. It optimizes parallelism at the layer level, achieving significant speedups and reducing data transfers compared to other frameworks like TensorFlow and PyTorch. DeePa is a deep learning framework that optimizes parallelism at the layer level to accelerate CNN training. It uses an elimination-based algorithm to find the best parallelism configuration for each layer, achieving significant speedups compared to TensorFlow and PyTorch. DeePa, a deep learning framework, achieves speedups for AlexNet and Inception-v3 by reducing data transfers, overlapping computation with data movement, and accelerating throughput. Convolutional layers consume most training time in CNNs, and parallelizing training in different data dimensions yields varying performance. Speed of training different convolutional layers from AlexNet, VGG-16, and Inception-v3 varies significantly. Different parallelization strategies can significantly impact data movement and performance in training convolutional layers. Image parallelism may not always achieve optimal results, suggesting exploration of parallelism in other dimensions. For example, parallelizing the first fully-connected layer of VGG-16 on two GPUs in different dimensions can lead to varying amounts of data movement and performance outcomes. Parallelizing the fully-connected layer in different dimensions can reduce data transfer costs by 12\u00d7. Increasing the number of workers may not always improve execution time due to data transfer overhead. Training time for different layers in Inception-v3 on up to 16 GPUs is shown in FIG1. The figure illustrates that various layers in a neural network may require different hardware configurations for optimal performance. DeePa, like TensorFlow and PyTorch, utilizes computation graphs to depict operation dependencies. DeePa's computation graph includes configurations for parallelizing operations across workers, with each node representing an operation and its parallelization details. The configuration specifies the degree of parallelism for each dimension, ensuring equal workload distribution among workers. DeePa's computation graph includes configurations for parallelizing operations across workers. Each worker receives the same size input for well-balanced workload distribution. Workers compute disjoint subsets of the output tensor in parallel without data dependencies. DeePa provides functions to estimate processing time and data transfer time for each operation and edge in the graph. DeePa's computation graph includes configurations for parallelizing operations across workers. Each worker computes disjoint subsets of the output tensor in parallel without data dependencies. Functions estimate processing time and data transfer time for each operation and edge in the graph. For each edge, the xfer() function estimates data transfer time based on data size and communication bandwidth. For each node and configuration, update() estimates parameter update time using data transfer time approximation. Global configuration includes parallelism configuration for each node. DeePa's global configuration includes parallelism settings for each node in the computation graph. The per-step execution time estimation involves forwarding processing, backward propagation, and gradient aggregation. An algorithm is described for minimizing the global optimization problem of finding the optimal configuration for each node. The number of potential global configurations grows exponentially with the number of nodes, making it impractical to explore all possibilities. DeePa's global configuration involves parallelism settings for nodes in the computation graph. The number of potential configurations grows exponentially, making it impractical to explore all possibilities. To simplify the graph, node elimination strategies are used while preserving the optimal configuration. DeePa uses dynamic programming to compute optimal configurations for nodes in a computation graph. Node elimination reduces the graph to only 2 nodes, while edge elimination merges duplicate edges. The xfer() function is computed for each node and edge, resulting in a simplified graph structure. DeePa iteratively eliminates nodes and edges to reduce the Inception-v3 module from 120 nodes to 2 nodes. DeePa simplifies the Inception-v3 computation graph from 120 nodes to 2 nodes using node and edge eliminations. It then finds the optimal global configuration by minimizing a Cost function. The algorithm also determines configurations for eliminated nodes by undoing eliminations in reverse order. Experimental results show that DeePa can optimize the configuration for parallelizing Inception-v3 on 16 GPUs in about 100ms. DeePa simplifies the Inception-v3 computation graph and optimizes parallelization on 16 GPUs in about 100ms using Legion features for fine-grain control over parallelism and data placement. DeePa optimizes parallelization on 16 GPUs using Legion's features for control over data placement and asynchronous tasking. Critical optimizations include overlapping computation with data transfers and distributing parameter servers for improved performance. DeePa optimizes parallelization on GPUs by managing parameters separately and distributing the parameter server onto GPU memories. This eliminates data transfers and transforms GPU to CPU copies into faster GPU to GPU copies, optimizing neural network parallelism at the granularity of each operation. Existing frameworks like TensorFlow, Caffe2, and PyTorch only explore parallelism in the image dimension, resulting in large data transfers for synchronizing gradients. DeePa is the first to control and optimize parallelism in all dimensions. BID7 utilizes model parallelism to train Inception-v3 on dedicated processors, optimizing GPU device placement with reinforcement learning for a 19% speedup on 4 GPUs. Krizhevsky FORMULA4 combines image and model parallelism in AlexNet training, reducing data transfer costs. DeePa further improves AlexNet training by reducing data transfers by 3\u00d7 and training time by 2.3\u00d7 compared to OWT. BID5 shows no accuracy loss when training ResNet-50 on ImageNet with a large minibatch size using standard image parallelism. DeePa utilizes image parallelism on 256 GPUs to train benchmark CNNs like AlexNet, VGG-16, and Inceptionv3 on the ImageNet dataset. It compares performance against TensorFlow, PyTorch, and OWT, achieving significant reductions in data transfers and training time. A case study on a 16-GPU machine shows promising results with no accuracy loss. DeePa utilizes image parallelism on 16 NVIDIA Tesla K80 GPUs for training CNN models with a minibatch size of 512 images. The search algorithm in DeePa finds optimal parallelism configurations, achieving competitive performance compared to TensorFlow and PyTorch. Model parallelism in the OWT approach speeds up training throughput significantly, with DeePa achieving 6.5\u00d7, 1.9\u00d7, and 1.5\u00d7 speedup compared to TensorFlow and PyTorch. DeePa achieves significant performance benefits over other frameworks by reducing data transfers, optimizing overlapping computation with data transfers, and exploring parallelism in height and width dimensions for training CNN models on 16 NVIDIA Tesla K80 GPUs. DeePa optimizes training CNN models on 16 NVIDIA Tesla K80 GPUs by utilizing image and model parallelism to reduce data transfers and synchronize gradients efficiently. DeePa optimizes training CNN models on 16 NVIDIA Tesla K80 GPUs by utilizing image and model parallelism to reduce data transfers and synchronize gradients efficiently. DeePa uses model parallelism on a small number of GPU workers for fully-connected layers to reduce data transfers for synchronizing gradients and moving tensors. The global configuration for parallelizing AlexNet on 16 GPU workers is shown in Figure 9, with DeePa selecting the optimal parallelism configuration for each layer. TAB2 lists the cost for different configurations of the first fully-connected layer, with the OWT approach eliminating gradient synchronization by replicating input tensors on every GPU worker. DeePa optimizes training CNN models on 16 NVIDIA Tesla K80 GPUs using image and model parallelism to reduce data transfers and synchronize gradients efficiently. The configuration chosen by DeePa involves using 2 GPU workers for the first fully-connected layer, reducing costs significantly. DeePa utilizes image parallelism for convolutional and pooling layers, with specific configurations listed. Additionally, DeePa employs different configurations to accelerate the last three convolutional layers. DeePa optimizes training CNN models on 16 NVIDIA Tesla K80 GPUs using image and model parallelism to reduce data transfers and synchronize gradients efficiently. The optimal configuration for the last three convolutional layers involves using only four GPU workers to minimize data transfers. Parallelism is also utilized in the height and width dimensions to further reduce compute time. Different configurations are used to parallelize different branches for the InceptionE1 module in Inception-v3. The configuration for InceptionE1 module reduces data transfers by 30% in InceptionE1 and InceptionE2, and overall data transfers by 20%. Minibatch size impacts CNN performance, with DeePa showing speedups compared to PyTorch and TensorFlow for various sizes. DeePa achieves 4.6-6.5\u00d7, 1.6-1.9\u00d7, and 1.2-1.5\u00d7 speedup for AlexNet, VGG-16, and Inception-v3, respectively. DeePa demonstrates competitive performance compared to PyTorch and TensorFlow for training on a single GPU. When training on 4 GPUs on a single node, DeePa achieves significant speedups for AlexNet, VGG-16, and Inception-v3. The performance further improves when training on multiple nodes, where data transfer time plays a larger role in training efficiency. DeePa accelerates CNN training by optimizing parallelism configurations at the layer level, achieving up to 6.5\u00d7 speedup for AlexNet, VGG-16, and Inception-v3 on 4 nodes. It reduces data transfers by up to 23\u00d7 compared to other frameworks. The Cost function is defined in Equation 1, optimizing configurations for efficient training. DeePa optimizes parallelism configurations for CNN training, achieving significant speedups. The Cost function is defined to optimize training efficiency. Edge elimination in the computation graph results in a modified graph, maintaining the same cost for any global configuration. In Section 5, DeePa extends BID5's design for gradient aggregation in parallel training, enabling optimization for asynchronous training. Profiling results on a single node with four Tesla P100 GPUs show higher training throughput with image parallelism compared to PyTorch and TensorFlow. All GPUs are efficiently utilized during forward and backward passes, as shown in FIG7. The optimal parallelism configuration chosen by DeePa utilizes image parallelism on 4 GPUs for convolutional and pooling layers, and model parallelism on 2 GPUs for fully connected layers. This configuration reduces per-iteration data transfers, but still incurs small performance gaps during data transfers for fully connected layers. The optimal configuration chosen by DeePa reduces per-iteration data transfers, hides data transfer overhead, and improves GPU utilization, resulting in a decrease in training time. Performance of DeePa, PyTorch, and TensorFlow is compared on the ImageNet-22K dataset with modified networks trained on 16 Tesla K80 GPUs. DeePa's training throughput on the ImageNet-22K dataset only drops by 3%, while PyTorch and TensorFlow see reductions of 20%-45%. DeePa significantly reduces per-iteration data transfers compared to image parallelism, leading to a decrease of 3.7-44.5\u00d7 in data transfers for training on the ImageNet-22K dataset."
}