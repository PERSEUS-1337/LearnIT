{
    "title": "HkzSQhCcK7",
    "content": "Convolutional architectures have shown competitiveness in sequence modelling tasks compared to recurrent neural networks (RNNs), offering computational advantages due to parallelism. However, performance issues persist. In this work, a novel architecture called stochastic temporal convolutional networks (STCNs) is proposed, combining the advantages of temporal convolutional networks (TCN) with stochastic latent spaces to capture temporal dependencies at different time-scales. The architecture achieves state-of-the-art log-likelihoods across various tasks and can predict high-quality synthetic samples over long-range temporal horizons, particularly in modeling handwritten text. Capturing long-term dependencies and correlations between output variables at the same time, recurrent neural networks (RNNs) and temporal convolutional networks (TCNs) have been successful in various problem domains. TCNs have shown competitive performance in tasks like audio synthesis, language modeling, and machine translation without relying on recurrence, reducing computational costs for training. Both RNNs and TCNs model joint probability distributions over sequences by predicting the next step based on previous time-steps. RNNs use hidden states to propagate information for long-term dependencies, while TCNs leverage large receptive fields by stacking dilated layers. Temporal Convolutional Networks (TCNs) utilize large receptive fields through stacked dilated convolutions to model long time scales up to the entire sequence length. TCNs introduce a temporal hierarchy where upper layers access longer input sub-sequences, propagating local information from lower layers through residual and skip connections. While TCNs have shown comparable performance to standard recurrent architectures on specific tasks, there is still a performance gap compared to more recent stochastic RNN variants. BID21 present a significant improvement in log-likelihood by coupling TCN model with latent variables, despite limited receptive field size. The computational graph shows generative and inference models of STCN, where random latent variables at upper layers have access to long history. The proposed approach augments TCNs with random latent variables, leveraging increased modeling capacity efficiently. Incorporating a hierarchy of stochastic latent variables into TCNs enables learning representations at multiple timescales. STCN introduces latent random variables arranged in correspondence to the temporal hierarchy of TCN blocks, maintaining scalability without modifying the base architecture. Incorporating a hierarchy of stochastic latent variables into TCNs enables learning representations at multiple timescales. The conditioning of latent random variables via different timescales is effective in TCNs. Two inference networks are proposed, one passing samples down from layer to layer and the other utilizing samples from all latent random variables before computing the final prediction. This approach augments TCN models without modifying the base architecture, maintaining scalability. The STCN-dense design enhances temporal convolutional network models with stochastic latent variables, preventing the model from ignoring them in upper layers. It achieves state-of-the-art log-likelihood performance on various datasets and produces high-quality synthetic samples. Auto-regressive models like RNNs and TCNs factorize the joint probability of variable-length sequences, with the observation model typically being a Gaussian or categorical distribution. In TCNs, causal convolutions are used with zero-padding to maintain input-output size. Dilated convolutions are employed to capture long-range dependencies without excessive layers. Gated activation units from Wavenet are utilized for better performance. The model incorporates stochastic latent variables to prevent ignoring them in upper layers and achieves state-of-the-art log-likelihood performance. In our work, stochastic variables z l are conditioned on TCN representations d l constructed by stacking Wavenet blocks. VAEs introduce a latent random variable z to learn variations in non-sequential data. The joint probability distribution is parametrized by \u03b8, and the VAE framework optimizes a lower-bound on the marginal likelihood. The VAE framework introduces a latent variable z to capture variations in non-sequential data, optimizing a lower-bound on the marginal likelihood. The RNN model captures temporal dependencies by updating an internal state h at each time-step, with a latent variable z augmenting the RNN state at each sequence step. The STCN model introduces a factorization with prior distributions conditioned on the RNN hidden state and input sequence. It aims to predict the next step in a sequence by utilizing hierarchical latent variables, similar to VRNNs and LVAEs. The RNN state is replaced by temporally independent TCN layers in STCNs. The STCN model utilizes hierarchical latent variables conditioned vertically in the same time-step and independent horizontally across time-steps. The inference network computes the approximate likelihood corrected by the prior to obtain the approximate posterior. TCN layers are shared between the inference and generator networks, preserving parallelism without introducing explicit dependencies between time-steps. Conditioning a latent variable on the preceding one implicitly introduces temporal dependencies. Random latent variables in the upper layer have a larger receptive field due to deterministic input. The STCN model uses hierarchical latent variables conditioned vertically within the same time-step and independently across time-steps. The generative and inference models are jointly trained by optimizing a step-wise variational lower bound on the log-likelihood. Latent variables in upper layers have a larger receptive field due to deterministic input, while lower layers receive more local information. The latent variable z l\u22121 t may receive longer-range information from z l t. The prior and inference distributions are parameterized by a neural network, with two variants of the observation model proposed. The STCN model uses hierarchical latent variables conditioned vertically within the same time-step and independently across time-steps. Two variants of the observation model are proposed, with latent variables at different layers capturing complementary aspects of the temporal context. To ensure gradients flow through all layers, the output probability is directly conditioned on samples from all latent variables. The STCN-dense architecture conditions the output probability on samples from all latent variables. It introduces a top-down dependency structure shared across generative and inference models, combining the approximate Gaussian likelihood from the inference model with the Gaussian prior from the generative model. The approximate posterior in the STCN-dense architecture is computed top-down from the generative model. Parameters are computed for each stochastic layer, and the approximate posterior has the same decomposition as the prior. The variational lower-bound on the log-likelihood is defined using decompositions, with the KL divergence term being the same for both STCN and STCN-dense variants. The reconstruction term differs between the two, with STCN using samples only from the lowest layer of the hierarchy. In STCN-dense, all latent samples are used in the observation model through dense connections. Instead of summing individual samples, they are concatenated analogously to DenseNet. The proposed variants STCN and STCN-dense are evaluated quantitatively and qualitatively on modeling digital handwritten text and speech, compared with other models. The experiments involve two variants of the Wavenet model: the original model and a variant with skip connections similar to STCN-dense. The IAM-OnDB and Deepwriting datasets consist of digital handwriting sequences with pen coordinates and pen-up events. Additional details and code can be found at https://ait.ethz.ch/projects/2019/stcn/. The experiments involve comparing different models on handwriting datasets. The proposed architecture shows robust performance across datasets. The model generates more natural-looking handwriting samples with clear word spacing and distinguishable letters. In speech modeling, the models are trained and tested on standard benchmark datasets using real-valued amplitudes. A new model with increased capacity is introduced for this task. Total model parameters are comparable to SWaveNet. The STCN-dense TAB0 model outperforms TCN, RNN, and stochastic models on TIMIT dataset. It is marginally better than Variational Bi-LSTM on the Blizzard dataset. The model does not have access to future information like other models. Latent Space Analysis shows that upper layers tend to become inactive in hierarchical latent variable models. The STCN-dense model outperforms TCN, RNN, and stochastic models on TIMIT dataset. It is marginally better than Variational Bi-LSTM on the Blizzard dataset. The model utilizes many latent variables effectively, with different information contexts due to TCN block's receptive field. STCN-dense may have more flexibility in modeling temporal dependencies due to dense connections to the output layer. The proposed combination of TCNs and stochastic variables is effective. The ablation study compares TCN with RNN using LSTM cells in the LadderRNN configuration. Performance is similar to vanilla RNNs due to instability with 25 stacked LSTM cells. Another configuration with 5 stacked LSTM cells shows significant results on the TIMIT dataset. The LadderRNN configurations on the TIMIT dataset show significant improvement, especially with densely connected latent variables. The modular latent variable design allows for different building blocks, boosting log-likelihood performance when attached to LSTM cells. Empirical results suggest that densely connected latent hierarchy works well with dilated CNNs due to the hierarchical nature of the architecture. STCN models achieved the best performance on both datasets and improved significantly with dense connections, supporting the contribution of a latent variable hierarchy in modeling different aspects of information from input time-series. The Ladder Variational Autoencoder (LVAE) and PixelVAE incorporate hierarchical latent space decomposition for improved performance. However, stacking latent variables without direct conditioning on observation variables can be ineffective. Our approach differs due to the sequential nature of the problem domain. Our approach in the sequential problem domain differs by using dynamic priors at every level and introducing skip connections from latent variables to the output. Autoregressive TCN decoders are suggested to address the posterior collapse problem in language modeling. BID4, BID29, and BID9 utilize TCN decoders with latent variables for audio signal modeling. Different stochastic RNN architectures vary in how they use latent variables for variational inference. BID7 and BID3 use latent variables to capture high-level information in sequential data, with BID7 showing the effectiveness of a conditional prior in sequence modeling. In BID12, BID14, and BID25, the inference model receives past and future sequence summaries from RNN cells. BID21's SWaveNet, similar to our approach, introduces latent variables into TCNs. In contrast to SWaveNet, STCNs introduce a novel auto-regressive model that allows for flexible stacking of dilated convolutions in the base TCN. The deterministic TCN units in STCNs are independent of the stochastic variables, enabling the adjustment of the ratio of stochastic to deterministic units based on the task at hand. This approach aims to combine the computational benefits of convolutional architectures with the expressiveness of hierarchical stochastic modeling. The proposed model combines convolutional architectures with hierarchical stochastic latent spaces for sequence modelling tasks. The network architecture involves a modification to the vanilla Wavenet architecture by using latent samples for predictions. Different configurations are used for different datasets, such as 1D convolutions with ReLU nonlinearity for IAM-OnDB and Deepwriting datasets. The model utilizes ReLU nonlinearity and stacks 5 Wavenet blocks with 256 filters and filter size 1. Different configurations are used for various datasets, with 30 Wavenet blocks for handwriting datasets and 25 for speech datasets, each with 256 filters of size 2. The large model configuration uses 512 filters. The model uses different dimensional latent variables for handwriting and speech tasks. The Normal distributions' parameters are calculated by f (l) p and f (l) q networks. 1D convolutions with ReLU nonlinearity are stacked, with the number of filters matching the Wavenet block filters. Latent sigma predictions are clamped between 0.001 and 5. KL annealing is applied in all STCN experiments. The batch size varies across datasets. ADAM optimizer is used with exponentially decaying learning rate. The learning rate was initialized differently for handwriting and speech datasets, with exponential decay. Early stopping was implemented based on ELBO performance. STCN models were implemented in Tensorflow. Results and code are available online. Average log-likelihood per sequence on various datasets was provided, with different observation models used."
}