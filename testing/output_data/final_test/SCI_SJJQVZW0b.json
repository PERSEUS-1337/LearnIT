{
    "title": "SJJQVZW0b",
    "content": "Learning policies for complex tasks that require multiple different skills is a challenge in reinforcement learning. This paper proposes a framework for efficient multi-task reinforcement learning by training agents to use hierarchical policies for acquiring new skills. The framework includes a stochastic temporal grammar to modulate when to rely on previously learned skills and when to execute new skills. The framework proposed in this paper aims to efficiently train agents in multi-task reinforcement learning by using hierarchical policies to acquire new skills. It involves a stochastic temporal grammar to determine when to use previously learned skills versus learning new ones. Knowledge transfer techniques like distillation have been used to train policy networks for accumulating multiple skills. The proposed hierarchical policy network aims to enable transfer of previously learned skills to new tasks by discovering underlying relations between skills. It uses human instructions to represent tasks and allows the agent to communicate its policy effectively. The hierarchical policy network enables transfer of learned skills to new tasks by using human instructions for communication. It learns to compose instructions and take multiple actions through a multi-level hierarchy, generating high-level plans for tasks like stacking blocks. The hierarchical policy network allows for transferring learned skills to new tasks by using human instructions. It generates high-level plans for tasks and tracks temporal relationships between tasks using a stochastic temporal grammar model. This approach was validated through object manipulation tasks in a Minecraft world. The framework efficiently learns hierarchical policies and representations for multi-task RL in a Minecraft world. It can also utter human instructions, improve explainability, and predict future actions through self-supervision. The approach differs from previous work by not assuming global tasks can be executed by predefined sub-tasks. Hierarchical reinforcement learning extends Markov decision processes to capture longer temporal dependencies by introducing options on top of primitive actions. This two-layer policy design improves the ability to discover complex policies that cannot be learned by flat policies. In this work, the aim is to learn a multi-level global policy without strict assumptions, using human instructions to encode tasks for improved interpretability of hierarchical policies. Language grounding via reinforcement learning is utilized, building on previous work in grounding human language in game environments. Our approach in this study involves using reinforcement learning to ground human language instructions in games, enabling agents to pick up items based on sentences. The model learns to compose plans for complex tasks using simpler ones with human descriptions, achieving plan composition through a multi-task RL setting, hierarchical policy, and stochastic temporal grammar. Each task is uniquely described by a human instruction, simplifying the process to a two-word tuple template of skill and item. In this study, reinforcement learning is used to ground human language instructions in games, enabling agents to perform object manipulation tasks based on sentences. Tasks are represented as tuples of skill and item, with a Markov decision process defined for each task. A terminal policy is trained for basic tasks, with the task set progressively increasing as the agent learns more tasks from humans. Life-long learning of policies is illustrated by the \"task accumulation\" direction in Figure 1. In this study, reinforcement learning is used to ground human language instructions in games, enabling agents to perform object manipulation tasks based on sentences. Tasks are represented as tuples of skill and item, with a Markov decision process defined for each task. A terminal policy is trained for basic tasks, with the task set progressively increasing as the agent learns more tasks from humans. The global policy \u03c0 k is defined by a hierarchical design with the ability to reuse the base policy for performing base tasks as subtasks. The hierarchy includes four sub-policies: base policy for executing learned tasks, instruction policy for communication, augmented flat policy for direct action execution, and switch policy for deciding reliance on base or flat policy. The instruction policy informs the base policy of tasks to execute. The augmented flat policy ensures the global policy can perform new tasks. The global policy includes a switch policy to decide between base tasks and primitive actions. At each time step, the switch policy determines reliance on the base policy or the augmented flat policy. The instruction policy informs the base policy of tasks to execute. The global policy includes a switch policy to decide between base tasks and primitive actions, with the instruction policy informing the base policy of tasks to execute. Temporal transitions between tasks are summarized using a stochastic temporal grammar (STG) in the full model, interacting with the hierarchical policy through the modified switch policy and instruction policy. This guides the hierarchical policy on whether to defer to the base policy or employ its own actions. The policy defers to the base policy or uses its own augmented policy to execute tasks. The temporal sequence forms a Markov chain, and at each level, a task's STG is defined by transition probabilities and distributions. Sampling is done based on reshaped policies, allowing for episode execution. The hierarchical policy, combined with the STG, allows for composing a plan for a task specified by a human instruction. The learning algorithm involves stages of skill acquisition, with a base skill acquisition phase and a novel skill acquisition phase in a 2-phase curriculum learning. The global policy learns how to use previously learned skills by issuing instructions to the base policy. In the novel skill acquisition phase, tasks are sampled from the full task set for skill acquisition stages. Policies are trained using advantage actor-critic (A2C) and distributions in the STG are estimated based on positive episodes. Global policies are optimized with off-policy learning, focusing on when to rely on base or augmented flat policy for executing tasks. In the skill acquisition phase, policies are trained using advantage actor-critic (A2C) and value functions are updated using gradients. The model introduces a value function to represent branch switching and estimates advantage functions to improve episode efficiency. After training policies with A2C in the skill acquisition phase, the model conducts mini-batch updates using a Poisson distribution with \u03bb = 4. Different policy optimization methods like A3C can also be applied. To stabilize learning, an alternating update procedure is used where two sub-policies are fixed and one is trained for M iterations before switching. Positive rewards trigger updates to the stochastic temporal grammar. The agent updates the stochastic temporal grammar with positive rewards after an episode using maximum likelihood estimation. Random exploration is encouraged in early episodes to avoid local minima. The environment in Minecraft consists of two rooms with blocks of different colors, and the agent is tasked with finding specific blocks. The agent in Minecraft is tasked with various actions like finding, getting, putting, stacking, and placing blocks of different colors. There are 54 tasks in total and the agent can perform actions like moving and picking up blocks. The skill acquisition order is assumed to be in a specific sequence, resulting in policies for different task sets. Six tasks are held out for the last task set. In Minecraft, the agent is trained on various tasks involving blocks of different colors. Out of 30 tasks, 6 are held out for testing. A sparse reward function is used, with a +1 reward for reaching the goal, -0.5 for generating non-executable instructions, and no reward otherwise. The network is trained using RMSProp with a learning rate of 0.0001 and a batch size of 36. The discounted coefficient is set to \u03b3 = 0.95 for all tasks. In Minecraft, the agent is trained on various tasks with a sparse reward function. The discounted coefficient is set to \u03b3 = 0.95 for all tasks. The 2-phase curriculum learning involves setting an average reward threshold of 0.9. Exploration is encouraged through \u03b5-greedy decision sampling. The learning efficiency is evaluated by comparing different models, all rewards are converted to the range [0, 1] for fair comparison. Various methods are used to train policy \u03c0 1 for task set G 1 based on the base policy \u03c0 0. The curriculum learning in Minecraft involves training policy \u03c0 1 for task set G 1 based on base policy \u03c0 0. The full model and variants converge within 22,000 episodes, outperforming the flat policy. The full model shows faster convergence and higher average reward during phase 1. In phase 2, the full model has the fastest convergence and highest average reward. The full model demonstrates faster convergence and higher average reward in phase 1 compared to other variants. It also outperforms in phase 2, showing the importance of curriculum learning in accelerating convergence and encouraging global policy reuse of learned skills. The penalty has little impact on learning efficiency but helps shorten episode lengths. Hierarchical design and encoding tasks improve policy generalization by training in a simpler setting before testing in a more complex one. Both flat and hierarchical policies perform well in simple settings, but only hierarchical policy can differentiate target items in complex settings. The hierarchical policy maintains a high success rate of 94% in differentiating target items in complex settings, while the flat policy drops to 29%. The full model outperforms the flat policy in various tasks with multiple items in a room, as shown in TAB1. The full model demonstrates superior performance in executing tasks with multiple items in a room, achieving a 94% success rate in differentiating target items. Additionally, the model successfully learns the decomposition of human instructions and generates correct hierarchical plans for unseen tasks. In a novel framework for multi-task reinforcement learning, a hierarchal policy modulated by a stochastic temporal grammar is proposed. Tasks are described by human instructions, allowing the global policy to reuse learned skills for new tasks. The framework, evaluated in Minecraft games, shows higher learning efficiency, good generalization in unseen environments, and the ability to compose hierarchical plans efficiently. The proposed framework for multi-task reinforcement learning involves a hierarchal policy modulated by a stochastic temporal grammar. It shows higher learning efficiency, good generalization in unseen environments, and the ability to compose hierarchical plans efficiently. The algorithm includes a pseudo code for running the policy at different levels. The proposed framework for multi-task reinforcement learning involves a hierarchal policy modulated by a stochastic temporal grammar. The architecture designs of all modules in the model are shown in FIG2, including a Visual Encoder that extracts feature maps from an input RGB frame. The model includes layers with different filter sizes and strides for feature extraction. The visual feature is reduced to 256 dimensions through a fully connected layer. The Instruction Encoder converts words into a 128-dim vector. A Fusion layer combines visual and language representations into a 384-dim vector fed into an LSTM with 256 hidden units. The LSTM hidden layer output serves as input for policy and value function modules. Policy modules include Switch, Instruction, and Augmented, each with specific FC layers and softmax activations. Value Function modules output scalar values through FC layers. Tasks start from the top-level policy, with branches ordered in time for consecutive steps. Real episode examples show critical moments with egocentric views and items in hands."
}