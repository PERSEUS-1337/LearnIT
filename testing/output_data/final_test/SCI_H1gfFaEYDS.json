{
    "title": "H1gfFaEYDS",
    "content": "This paper addresses the issue of over-sensitivity of deep network representations to irrelevant data changes. The classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO), is identified as a cause of this problem. The ELBO fails to control the encoder's behavior outside the data distribution, leading to significant errors in learned representations. To tackle this issue, the proposal involves augmenting data with specifications to enforce insensitivity to certain transformations. A regularization method based on a selection mechanism is suggested to create fictive data points by perturbing observed data points. The paper proposes a method to minimize the entropy regularized Wasserstein distance between representations by perturbing observed data points. This approach improves adversarial accuracy without supervised training. Representation learning is a key problem in Machine learning, enabling data-efficient learning and transfer to new tasks. Deep architectures have shown effectiveness in generating features for various domains like Computer Vision and Natural Language Processing. Research in representation learning has focused on developing techniques for inducing desirable properties in learned representations. Many techniques for generating representations are based on Variational AutoEncoders (VAE) model. While the quality of generated examples can show flexibility in capturing data distribution, it may not guarantee suitable representations for other purposes. Evaluating generative models based solely on reconstruction fidelity and sample quality can be misleading. In this paper, the authors uncover a problematic failure mode in Variational Auto-encoders (VAEs) where learned representations are overly sensitive to irrelevant data changes. This behavior can lead to significant errors in the recovered representation, hindering effective data-efficient learning and transfer. To address this issue, they propose augmenting data with properties that enforce insensitivity to certain transformations through a regularization method. The authors propose a regularization method to minimize entropy regularized Wasserstein distance between representations, leading to improved adversarial accuracy without supervised training. An illustration shows the fragility of VAE representations. The encoder-decoder output is sensitive to small changes in input, indicating a lack of smoothness in the encoder. This can lead to incorrect predictions when inputs are altered. The authors propose a regularization method to improve adversarial accuracy without supervised training. The authors introduce a method for learning robust latent representations by targeting a structured model that includes the original VAE model. They also propose a modification to VAE training algorithms to enhance the robustness of learned representations. The authors introduce a method for learning robust latent representations by using attacks in adversarial supervised learning to transform examples and enforce close representations. They show that alternative models like \u03b2-VAE and Wasserstein GANs can be interpreted in their framework. Empirical studies on MNIST, color MNIST, and CelebA datasets demonstrate that their method produces representations with higher adversarial robustness. The method introduced by the authors focuses on learning robust latent representations through adversarial supervised learning attacks. It emphasizes the importance of models trained using this method in achieving higher adversarial robustness without the need for supervised adversarial training. The approach involves generating realizations from an ideal target distribution using generative models, such as the VAE, which corresponds to a latent variable model with a prior and a forward model represented by a neural network. The prior for latent variables in VAE is a standard Gaussian. The exact posterior is approximated by a multivariate Gaussian with parameters learned from data. VAEs are trained by maximizing the ELBO using stochastic gradient descent. The gradient of the KL divergence term is available in closed form, and the gradient of the first term can be estimated using the reparametrization trick. In VAE, the ELBO is maximized using stochastic gradient descent. The variational lower bound calculates the distance between the encoder and decoder under the empirical distribution. A VAE with discrete latents and observations is constructed, visualized by heatmaps of probability tables. The von-Mises parametrization chosen for VAE avoids boundary effects in latent and observable spaces. The prior is uniform. A dataset is sampled from a discrete target distribution, revealing important properties of VAE approximation. The VAE approximation reveals important properties of the encoder and decoder, showing smoothness in the representation and fitting quality. Close values of latent variables result in similar decoder mean parameters, indicating a smooth encoder. The VAE approximation highlights the smoothness of the encoder and decoder, with close latent variables leading to similar decoder parameters. Despite a good fit, the encoders found by the VAE are not smooth, emphasizing the importance of not solely judging generative models based on sample quality. The VAE approximation emphasizes the importance of not solely judging generative models based on sample quality. The fragility of representations is inherent from the ELBO objective, as the encoder structure is not enforced beyond the position of data points in the training set. The out-of-sample behavior of the encoder is initialization dependent, leading to no learning in those cases. The VAE approximation highlights the fragility of encoder representations due to initialization dependency. To address this, a strategy for training the encoder is proposed, involving an external selection mechanism to provide fictive data points near observations for smoother representations. The augmented distribution in the pairwise conditional Markov random field model includes a pairwise cost function. The encoder does not necessarily need to maintain a pairwise approximation distribution, as the final terms in the ELBO depend on the pair distribution, resembling the objective function of the entropy regularized optimal transport problem. The Wasserstein distance is defined as the solution to an optimal transport problem, where the cost of transferring probability mass is minimized. By choosing a specific form of the variational distribution, we can optimize a lower bound of the original problem. By integrating out X, we create a lower bound of the original problem, named SE-ELBO. This allows for concurrent optimization of the decoder and encoder in the VAE framework. The gradient of this lower bound is identical to the original VAE objective, facilitating efficient training. The regularization of the encoder enforces closeness between marginals obtained via amortized inference, smoothing representations without altering data distribution. This modification to the standard VAE includes a regularized Wasserstein distance and is aimed at improving model robustness against adversarial attacks. The perturbed input, known as an adversarial example, is used for training adversarially robust models through data augmentation. In unsupervised learning, data augmentation may not be valid, but by targeting a different distribution with the encoder, self-generated samples can improve model properties. In this approach, a 'self-supervised' learning method is used to improve model properties by biasing the search for a 'good encoder'. The selection mechanism involves using Projected Gradient Descent attacks to find data points that introduce maximum differences in Wasserstein distance of latent representations. This attack has a selection iteration budget and radius assigned for finding extra data points. In experiments, adversarial accuracy of VAE and smooth encoder representations was compared using a two-step protocol. Encoder-decoder pairs were trained first, then fixed representations were used for training a linear classifier. The robustness of the classifier is evaluated by searching for adversarial examples in the test set using an untargeted attack. Adversarial accuracy is reported as the percentage of examples where the attack fails to find an adversarial example. The VAE and SE decoder and encoder are implemented with standard architectures, and SE training uses a projected gradient descent optimization. The experiment focused on maximizing the Wasserstein distance between different perturbations on datasets like ColorMNIST, MNIST, and CelebA. Results showed that a PGD attack with 100 iterations and 10 restarts yielded the strongest attack, while weaker attacks had higher adversarial accuracy. Comparing representations learned by the method to a VAE, it was observed that VAE representation quickly lost adversarial accuracy, whereas SE could maintain it. The VAE representation quickly loses adversarial accuracy, while SE can maintain it. Adversarial accuracy drops with increasing radius size in PGD attacks, but SE degrades more gracefully. ConvNet performs better than MLP, showing VAE representation's lack of robustness. Limited improvements observed with SE using random selection in controlled experiments. In controlled experiments, limited improvements were seen with SE using random selection in adversarial accuracy compared to VAE. Training a SE with adversarial selection proved to be much more effective, with a lower selection iteration budget than the attack iteration budget during evaluation. The experiments were also repeated on the CelebA dataset, showing that essentially the same level of adversarial accuracy can be achieved with a small fraction of available labels. The results of using 17 attribute labels as targets for downstream classification tasks are shown in Table 2, demonstrating more robust representations than a VAE. Adversarial examples on CelebA show structured attacks on SE representations, with perturbations clearly identifiable, unlike VAE where perturbations lack visible structure. In structured attacks on SE representations, perturbations are clearly identifiable, unlike VAE where perturbations lack visible structure. The attacker can manipulate latent features to create adversarial examples that fool the classifier. Our approach aims to obtain more robust representations compared to VAE decoders, which are typically smooth. A comparison of nominal and adversarial accuracy on 17 downstream tasks using a VAE and a SE trained with a selection radius of 0.1 is presented. The experiment includes adversarial evaluation on CelebA with an attack radius of 0.1 and attack iteration budget of 100 with 10 restarts. The literature on deep generative models and representation learning is vast, with popular approaches including Generative Adversarial Networks (GANs) and VAEs. Our approach shows connections to both VAEs and GANs. Our method is closely related to \u03b2-VAE and GANs. In the appendix, we show that a GAN decoder can be viewed as a smooth encoder. Wasserstein distance minimization has been used in generative models as an alternative objective for fitting the decoder. The variational decomposition of the marginal likelihood terms can be modified to change the observation model or regularizer, as seen in Wasserstein AutoEncoders and sliced Wasserstein Autoencoders. Our approach differs from existing methods like Wasserstein AutoEncoders and Adversarial Variational Bayes by not replacing the likelihood as a fundamental principle for data fitting. Instead, the Wasserstein distance formulation naturally emerges from our model choice and variational approximation, involving an adversarial selection step. The approach presented does not replace the likelihood as a fundamental principle for data fitting, unlike existing methods such as Wasserstein AutoEncoders and Adversarial Variational Bayes. Instead, it focuses on developing a richer family for learning useful representations, without modifying the original VAE objective. Techniques like Adversarial Autoencoders, Adversarially Learned Inference, and BiGANs combine ideas from GANs and VAEs to match encoder and decoder processes using an alternative objective. The approach presented focuses on improving VAEs by capturing correlation structures between data points using MRFs and graphical models. This approach, similar to CVAEs, introduces correlation structures between true data points and artificially selected data points, which can enhance model performance if prior knowledge is available. The paper introduces a method to enhance the robustness of latent representations learned by a VAE. The goal is to learn generic representations that can be applied to various tasks, even unknown ones. While unsupervised approaches may have lower accuracy compared to supervised methods, the proposed approach focuses on improving adversarial robustness by enforcing smooth representations. The KL divergence between Gaussian distributions plays a key role in this method. The KL divergence between Gaussian distributions is a key factor in enhancing the robustness of latent representations learned by a VAE. It consists of a scale invariant divergence between covariance matrices and a Mahalonobis distance between means. The KL divergence is invariant to parametrization and coordinate system choices. The Wasserstein divergence is defined as the solution to an optimization problem with respect to pairwise distributions. The 2-Wasserstein distance W22 for two Gaussians has an interesting form, with an optimal transport plan that is degenerate. The Entropy Regularized 2-Wasserstein is the value attained by the minimizer of a functional involving the entropy of the joint distribution Q. The entropy of a Gaussian Q(za, zb) is given by the Schur formula. The entropy regularized problem involves minimizing a specific Matrix Ricatti equation to find the Wasserstein distance solution. For two univariate Gaussians, the solution is obtained from a scalar quadratic equation. The feasible solution is chosen as the minimizer, satisfying certain conditions. The optimization involves neural network outputs and observation vectors. The SE-ELBO gradient estimation involves selecting a fictive sample x through adversarial attack, calculating latent representation, and training linear classifiers using encoder activations for downstream tasks. Both encoder and decoder networks use MLP and ConvNET architectures with 200 ReLU units. The experiments involved using MLP and ConvNET architectures with 200 ReLU units for latent space dimensions of 32, 64, and 128. Training was done using the Adam optimizer for 300K iterations. GANs are neural sampling models with a generator function f transforming observations x from a simple distribution p(Z). The GAN approach fits a degenerate distribution to a dataset by co-training a generator with a discriminator network. The discriminator distinguishes between real data and generated data, with the objective of matching the empirical data distribution. This adversarial training process is analogous to a smooth encoder in GANs. The discriminator function d in GANs aims to increase correct classification of fake and real examples, while the generator f aims to decrease detection of fake examples. The objective can be written as l(x; w) = log d(x; w), highlighting a connection to optimal transport theory. This concept was recognized in a seminal paper by Arjovsky et al. (2017), framing the problem with the constraint |l(x; w) \u2212 l(x; w)| \u2264 c(x,x). The problem is framed with the constraint that l is a Lipschitz function, and D_fake is the fitted density of x. This is the dual formulation of the optimal transport problem, likened to an economic transaction between a customer and a shipment company. The customer aims to minimize the profit of the company by adjusting the desired delivery distribution D_fake, so that the transfer from the fixed source distribution D_real is minimized. The GAN objective aims to minimize the Wasserstein distance between the actual data distribution D_real and the fake data distribution D_fake generated by the generator. This can be viewed as maximizing a particular ELBO with specific forms for the variational marginals. By choosing a large coupling coefficient \u03b3, the ELBO simplifies, making the coupling term dominant. By setting a large coupling coefficient \u03b3, the Wasserstein minimization objective is obtained, where the random draws from p(Z) act as the selection mechanism. The problem simplifies to solving the optimal transport between Q a and Q b, making terms dependent on p(Z|X, \u03b8) irrelevant. The connection between entropic GANs and VAEs is discussed in the literature, with the GAN decoder seen as a smooth encoder. This allows for extensions like \u03b2-VAE within the same framework for controlling representations. The \u03b2-VAE is often used for controlling representations and replaces the original variational objective with a dispersion term \u03b2 in a Lagrangian formulation. This term is related to the number of points selected by the selection mechanism, leading to an identical variational lower bound as the \u03b2-VAE objective. The \u03b2-VAE objective involves minimizing the KL distance between exact and approximate factorizations of the joint distribution p(X, Z). In a VAE, the decoder's smoothness is implicitly enforced by the encoder. In a VAE, the smoothness of the decoder is enforced by the highly constrained encoder distribution and SGD training dynamics. When two latent coordinates are close, the decoder mean mapping becomes bounded. The encoder output is conditionally Gaussian, and the decoder parameters depend on the data fidelity term. During training, latent state vectors are sampled from the encoder distribution, with high probability on the typical set in a large latent space. The typical set of a nondegenerate Gaussian distribution is a compact hyper-ellipsoid. Training reduces error, forcing the decoder to give the same output for points on the ellipsoid. The Mahalanobis distance is bounded, with minimum distance related to the covariance matrix's smallest eigenvalue. The ELBO objective enforces the decoder to be invariant on the typical set of q(Z|X = x), promoting large hyper-ellipsoids in the latent space for each data point. The smooth encoder training leads to a small Lipschitz constant for the encoder mean mapping. The encoder variance mapping is assumed to be constant, leading to a simplified Wasserstein divergence calculation using PGD during training to minimize the distance between mean mappings. The ELBO objective aims to reduce the local Lipschitz constant around data points in the latent space. The Lipschitz constant L(x) around data point x promotes smoothness with an upper bound E on the change in representation."
}