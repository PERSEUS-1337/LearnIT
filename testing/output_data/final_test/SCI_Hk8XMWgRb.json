{
    "title": "Hk8XMWgRb",
    "content": "We propose a principled method for kernel learning based on Fourier-analytic characterization of translation or rotation-invariant kernels. Our method generates feature maps to refine the SVM margin, with guarantees for optimality and generalization. Evaluations show scalability and improvements over random features-based methods. The importance of selecting the right kernel in machine learning has inspired various studies, from feature selection to multiple kernel learning (MKL). Our approach offers a new, principled way to address this challenge. Our new approach for selecting translation or rotation-invariant kernels maximizes the SVM classification margin. It involves a kernel-alignment subroutine and an iterative procedure to improve the margin. The algorithm is simple, scalable, and can be interpreted as no-regret learning dynamics in a min-max game. Experiments on synthetic and benchmark datasets show consistent improvements over random features-based kernel methods. Our method for selecting translation or rotation-invariant kernels maximizes the SVM classification margin by utilizing a kernel-alignment subroutine and an iterative procedure. It differs from traditional MKL by drawing inspiration from random features, allowing for greater expressivity and improved generalization guarantees. The approach involves sampling feature maps based on the Fourier transform of a chosen kernel, optimizing kernel alignment through importance sampling on a fixed proposal measure. The proposal measure lacks informative features in high dimensions, focusing on efficiency rather than performance improvements over RBF features. Previous studies have explored learning a kernel in the Fourier domain, with limitations in expressivity and complex posterior inference procedures. Experimentally, learning a kernel in the Fourier domain with regression parameters can identify informative frequencies in the data, but may get stuck in poor local minima. Boosting methods have been used to sequentially build a kernel, but with linear feature maps and costly computations. Our theory provides a tighter characterization of SVM margin compared to previous work. We present an algorithm that converges to the maximum SVM classification margin by outputting a sequence of Fourier features. The algorithm maximizes kernel alignment with an adversarially chosen measure, building a diverse and informative feature representation. Our method is connected to von Neumann's min-max theorem for a zero-sum concave-linear game. Our method is connected to boosting and the min-max theorem for a zero-sum concave-linear game. It focuses on constructing a measure with small finite support to realize the kernel exactly, without randomness in the features. Two families of kernels are considered: translation-invariant kernels on R^d and rotation-invariant kernels on the hypersphere S^(d-1). The text discusses kernels on the hypersphere S^(d-1) that are invariant under rotations and depend only on x. Bochner's theorem provides a Fourier-analytic characterization for these kernels, with a similar characterization available for rotation-invariant kernels using spherical harmonics as basis functions. The text also introduces notation for valid index pairs and an involution on these pairs. The text discusses rotation-invariant continuous functions on the hypersphere S^(d-1) and their Fourier transforms. These functions can be decomposed into a non-negative combination of Fourier basis kernels, leading to a feature map that realizes the kernel under the codomain's inner product. The feature maps can be transformed to yield real features, preserving the inner product. This concept also applies to spherical harmonics. The text discusses the alignment of kernels in a classification task with training samples. It introduces the concept of kernel alignment using empirical measures of each class and dual measures. The alignment is linear in the dual measure and is related to the Fourier potential. The Fourier potential v(\u03c9) is the squared magnitude of the Fourier transform of the signed measure \u00b5 = P \u2212 Q. Maximizing alignment \u03b3 k (P, Q) involves finding a kernel k that maximizes the dual measure \u03bb k (\u03c9) subject to constraints. Placing a Dirac mass at opposite modes \u00b1\u03c9 * maximizes \u03b3 k. The classes' empirical distributions P and Q are used initially, with reweighting of data points as Algorithm 2 progresses. The reweighted Fourier potential v(\u03c9) is maximized to find global Fourier peaks in data, even in high dimensions. Realistic data is typically band-limited, making it easy to identify peaks. Enforcing a band-limitedness constraint is not necessary in practice. Langevin dynamics is used to find peaks of v(\u03c9) when a gradient is available. Theoretical hitting-time guarantees for Langevin dynamics for kernel alignment are discussed in Algorithm 1. It is recommended to use parallel initialization for efficient GPU implementation. The algorithm typically finds a peak within around 100 steps. A method is proposed to boost Algorithm 1 for SVMs by maximizing the classification margin with a dual kernel. The dual l1-SVM objective parameterizes the kernel by \u03bbk to maximize the classification margin. The method applies to l2 SVMs with stronger theoretical guarantees. The algorithm allows the min-player to play a pure-strategy best. Algorithm 1 enables the min-player to play a pure-strategy best response to the max-player, while Algorithm 2 combines this with online gradient ascent for the max-player. This process gradually emphasizes the margin, leading to the discovery of more informative features that can approximate the Nash equilibrium. Contingent on successful kernel alignment steps, the Nash equilibrium is approximated using Algorithm 2 for SVM margin maximization. Theoretical analysis is provided in Section 4.1, with detailed discussions on heuristics, hyperparameters, and implementation in Appendix A. Efficient computation of online gradient g t is highlighted, with Langevin dynamics being the primary time bottleneck. The main theoretical result quantifies the convergence properties of the algorithm. Algorithm 2 produces a dual measure \u03bb that satisfies certain conditions, with a margin bound stated in Corollary 4.2. Theoretical analysis in Section 4.1 quantifies convergence properties of the algorithm. The analysis in the current chunk presents generalization guarantees for SVM decision functions, with a focus on margin bounds and the impact of increasing random features on generalization error. The approach is similar to boosting methods and references previous work by Hazan et al. (2007) and Koltchinskii & Panchenko (2002). The current chunk discusses the generalization error bounds for SVM decision functions with a focus on margin bounds and the impact of increasing random features. The analysis improves on previous results for MKL by leveraging the rank-one property of each component kernel. Additionally, it addresses the sample size required for v(\u03c9) to approximate the ideal Fourier potential v ideal (\u03c9) for the shift-invariant case. The current chunk presents a result on polynomial hitting-time for approximate local maxima of v ideal, with a reference to a previous theorem on Langevin hitting time. It also mentions the experimental results and provides a link to the code. In an extended addendum to the experimental section, two binary classification tasks are demonstrated using a kernel selection method. The datasets have sharp boundaries, challenging standard kernels. Algorithm 2 shows significant improvement in classification accuracy on both tasks. The code can be found at github.com/yz-ignescent/Not-So-Random-Features. The curr_chunk discusses the high classification accuracy achieved using only 29 spherical harmonics as features in a kernel evaluation experiment. Visualizations in FIG6 show the evolution of weights and features, highlighting improvements in classifier decisiveness. The kernel is then tested on challenging binary classification tasks from MNIST and CIFAR-10 datasets. In a kernel evaluation experiment, Yu et al. use 512-dimensional HoG features for CIFAR-10 tasks instead of raw images. Their method outperforms baseline random features-based kernel machines, showing scalability and reliability, especially with few features. The theory predicts continuous improvement in margin regardless of feature map dimensionality. The classifier saturates on training data, test accuracy increases without overfitting, decoupling generalization from model complexity. Method robust with hyperparameter settings, outperforms RBF-RF and LKRF with fewer features, GPU implementation fast. Efficient kernel learning method using Fourier analysis and online learning, shows improvements on benchmark tasks compared to random features-based methods. Many theoretical questions remain. In future work, accelerating the search for Fourier peaks and applying learned kernels to convolutional kernel networks are exciting directions. Algorithm 2 outlines essential components but hides hyperparameter choices and algorithmic heuristics. Hinge-loss SVM classifiers with C = 1 are used in experiments, with convergence depending quadratically on C. Best samples in Langevin diffusion come from high temperatures and Gaussian parallel initialization. In Langevin dynamics, 500 parallel copies are initialized with Gaussian distribution. The step size is tuned based on the gradient magnitude, and running for about 100 steps is sufficient to find a good peak. Modifying the algorithm to select the top samples can improve efficiency without degrading features. The step size for online gradient ascent is also set. In experiments, the step size for online gradient ascent is crucial for balancing conservatism and diversity in samples. Saturating the peak-finding routine is beneficial for improving the margin bound. Refining samples with gradient descent or an accelerated algorithm can help find approximate local minima of smooth non-convex functions. Projection onto the feasible set K is also discussed. In experiments, the step size for online gradient ascent is crucial for balancing conservatism and diversity in samples. Saturating the peak-finding routine is beneficial for improving the margin bound. Refining samples with gradient descent or an accelerated algorithm can help find approximate local minima of smooth non-convex functions. Projection onto the feasible set K = {0 \u03b1 C, y T \u03b1 = 0} of the SVM dual convex program typically uses alternating projection onto the intersection of a hyperplane and a hypercube. The convergence rate of this method is linear, and in experiments, 10 alternating projections are used to obtain dual feasible solutions. This process is a negligible component of the total running time compared to parallel gradient computations. In Section 3.1, defining gradient Langevin dynamics on v(l, m) in the inner-product case is challenging due to the lack of topology on the indices (l, m) of spherical harmonics. One approach is to mimic Langevin dynamics by creating a discrete Markov chain that converges to \u03bb(l, m) \u221d e \u03b2v(l,m). However, it is sufficient in experiments to compute \u03bb(l, m) by considering all v(l, m) values with j less than a threshold J. This approximates the kernel-target alignment objective function through Fourier truncation, which is highly parallelizable and involves N(m, d) degree-J polynomial evaluations using matrix multiplication. In experiments, examining the first 1000 coefficients was enough, as real-world datasets do not typically have large values of v(\u03c9) outside the threshold J under Fourier truncation. In the gametheoretic view, an approximate Nash equilibrium can be achieved via Nesterov's excessive gap technique when the kernel player's actions are restricted to a mixed strategy over a finite set of basis kernels. Having a discrete set of Fourier coefficients allows for duplicate features to be scaled appropriately, leading to near-perfect classification accuracy with a smaller support of features. This section delves into spherical harmonics in d dimensions, a standard topic in harmonic analysis. The section discusses the characterization of rotation-invariant kernels using Gegenbauer polynomials and the involution \u03c9 \u2192 \u2212\u03c9 on indices of spherical harmonics in the X = S d\u22121 case. The Fourier coefficients are constrained to be symmetric in this context. In the X = S d\u22121 case, a permutation on indices \u03c3 is sought so that \u03c3(\u03c3(\u03c9)) = \u03c9, and \u03bb(\u03c9) = \u03bb(\u03c3(\u03c9)) for real kernel spherical harmonic expansions. The involution is well-defined in all dimensions, leading to the symmetry condition in Theorem 2.2. By orthonormality, every square-integrable function can be uniquely decomposed into spherical harmonics, with coefficients satisfying \u03bb \u2212\u03c9 = \u2212\u03bb \u03c9 for real-valued functions. In this section, the main theorem quantifies convergence of Algorithm 2 to the Nash equilibrium. The theorem states that Algorithm 2 returns a dual measure \u03bb that satisfies certain conditions. The regret bound for online gradient ascent is used in the proof, specifically for the case of l1-SVM with a box constraint C and under the assumption of \u03b5 t -approximate optimality of Algorithm 1. The proof can be extended to other cases straightforwardly. The proof extends to other cases straightforwardly, with Lemma C.1 providing a regret bound for online gradient ascent. The regret bound guarantees convergence to a Nash equilibrium, with the Rademacher complexity computed for the learned kernel composition. In this section, the Rademacher complexity of the composition of the learned kernel and the classifier is computed, proving Theorem 4.3. The theorem discusses concentration of the Fourier potential, showing that with high probability, the difference between empirical and ideal measures is within a certain bound. This result is derived from empirical measures obtained from i.i.d. samples. The text discusses the concentration of the Fourier potential, showing that the empirical and ideal measures are close with high probability. It is proven that the empirical Fourier potential is Lipschitz and scales linearly with the norm of the data."
}