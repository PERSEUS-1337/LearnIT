{
    "title": "r1gixp4FPH",
    "content": "Nesterov SGD is commonly used for training neural networks and machine learning models, but it does not offer acceleration over ordinary SGD, both theoretically and empirically. In some cases, Nesterov SGD may even diverge when ordinary SGD converges. This is contrary to classical results in the deterministic setting, where Nesterov's method shows accelerated convergence over optimal gradient descent with the same step size. MaSS is introduced as a compensation term to Nesterov SGD to address non-acceleration issues. It converges for the same step sizes as SGD and achieves accelerated convergence rates over SGD for any mini-batch size in the linear setting. The convergence rate of MaSS matches Nesterov's method for full batch, and the dependence of convergence rate and optimal hyper-parameters on mini-batch size is analyzed, showing three distinct regimes: linear scaling, diminishing returns, and saturation. Experimental evaluation of MaSS for various deep network architectures, including ResNet and convolutional networks, demonstrates superior performance compared to SGD, Nesterov SGD, and Adam. Over-parametrized modern neural networks are trained to achieve near-zero training loss, known as interpolation, leading to strong generalization performance. Stochastic gradient descent with constant step size converges to the optimum of a convex loss function due to automatic variance reduction in interpolation. The paper aims to explore momentum-based SGD in the interpolating setting. Nesterov SGD does not provide acceleration over ordinary SGD, and may even diverge in some cases. The lack of acceleration is due to the need for a smaller step size in Nesterov SGD compared to SGD to ensure convergence. The MaSS algorithm introduces a compensation term to address the non-acceleration of Nesterov SGD, allowing for convergence with the same range of step sizes as SGD. It updates weights using specific rules involving the momentum parameter and compensation parameter. The MaSS algorithm introduces a compensation term to accelerate convergence, showing exponential convergence over plain SGD. In the linear setting, MaSS converges exponentially with the optimal step size being \u03b7 * for both MaSS and SGD. In the full batch scenario, MaSS reduces to Nesterov's method with matching convergence rates. SGD+Nesterov does not converge faster than SGD. The MaSS algorithm accelerates convergence with exponential speed in a convex setting. The convergence rate and optimal hyper-parameters depend on the mini-batch size, with three distinct regimes identified: linear scaling, diminishing returns, and saturation. The critical values m * 1 and m * 2 are derived analytically, with a new \"diminishing returns\" regime not found in SGD. This is the first analysis of mini-batch dependence for accelerated stochastic gradient methods. The MaSS algorithm accelerates convergence in a convex setting with exponential speed. It outperforms SGD, SGD+Nesterov, and Adam in optimization and generalization on deep neural networks. The paper introduces MaSS, analyzes its convergence and optimal hyper-parameter selection, and discusses mini-batch MaSS. Experimental results are presented in Section 6. Over-parameterized models, especially neural networks, have shown strong generalization performance. The work discusses the non-acceleration of existing stochastic momentum methods like SGD+HB and SGD+Nesterov, showing that they converge at the same rate as SGD on certain data. It also suggests that for small step sizes, SGD+Nesterov and SGD+HB are equivalent to SGD with a larger step size. The work concludes that momentum hurts convergence near global optima in SGD+HB, consistent with analysis of standard SGD+Nesterov. However, MaSS improves convergence over SGD. Practical and theoretical studies on SGD with momentum include Adam and AMSGrad. ASGD algorithm proposes accelerated SGD with tail-averaging step. The ASGD algorithm proposes accelerated SGD with a tail-averaging step and yields a convergence rate of O(Poly(\u03ba,\u03ba) exp(\u2212 )) for batch size 1. Experimental comparisons show ASGD algorithm vs. MaSS. Another first-order momentum algorithm in (21) has slower convergence rates than SGD in some cases. MaSS matches optimal Nesterov rate in Gaussian setting. Our algorithm guarantees never slower convergence rate than SGD. In this work, the dependence of convergence rate and optimal parameter selection on mini-batch size is analyzed for momentum methods. The concepts of strong convexity and smoothness of functions are used, with the condition number defined as \u03ba = L/\u00b5. The Hessian matrix is also considered, with the condition number calculated based on the largest and smallest non-zero eigenvalues. The text discusses the m-stochastic condition number \u03ba m and the statistical condition number \u03ba, as well as the concept of interpolation in over-parametrized models. It also mentions Automatic Variance Reduction (AVR) in the context of interpolation. The text discusses the convergence of SGD with constant step size and AVR, showing exponential convergence for strongly convex loss functions. It also proves that SGD+Nesterov does not generally improve convergence over optimal SGD, with a specific setting where their convergence rates are the same. Theorem provides lower bound for convergence of SGD+Nesterov in linear regression on component decoupled data model, showing no acceleration over SGD. The non-acceleration of SGD+Nesterov in linear regression is due to a condition on the step size needed for non-divergence, which slows down the convergence. This contrasts with the deterministic scenario where Nesterov's method accelerates with the same step size as gradient descent. The non-acceleration of SGD+Nesterov in linear regression is due to a condition on the step size needed for non-divergence, which contrasts with the deterministic scenario where Nesterov's method accelerates with the same step size as gradient descent. MaSS introduces a compensation term onto SGD+Nesterov, showing exponential convergence for all step sizes leading to convergence of SGD. The convergence rate for MaSS is faster than that of SGD, with an optimal hyper-parameter setting provided. The bijection between hyper-parameters (\u03b7 1 , \u03b7 2 , \u03b3) and (\u03b7, \u03b1, \u03b4) is established. For square loss function, the solution set W * is an affine subspace in the parameter space R d. Different w may correspond to different w * due to gradients being perpendicular to W *. No actual update happens along W *. The solution set W* is an affine subspace in the parameter space R^d, with gradients perpendicular to W*. MaSS guarantees exponential convergence for quadratic loss functions in the interpolation setting, with specific conditions on hyper-parameters. The admissible step size \u03b7 for MaSS is the same as SGD for interpolated setting. The hyper-parameter setting of SGD+Nesterov does not satisfy the conditions for MaSS. The convergence rate of MaSS is determined by (1\u2212\u03b1) t, with larger \u03b1 preferred for faster convergence. The optimal selection of hyper-parameters \u03b7 * and \u03b1 * leads to a unique \u03b4 * = \u03b1 *. The optimal selection of hyper-parameters for MaSS is crucial for its convergence. The coefficient \u03b7 * 2 in the compensation term is non-negative, indicating the need for compensation along the gradient direction. The optimal step size for MaSS is the same as for SGD. Setting hyperparameters as in Eq.13 accelerates MaSS convergence after t iterations with a mini-batch of size m. The optimal selection of hyper-parameters for MaSS is crucial for its convergence. With the optimal hyper-parameters in Eq.13, MaSS has a faster asymptotic convergence rate compared to SGD. In the full batch scenario, MaSS reduces to Nesterov's method. The convergence rate in Theorem 3 reduces to O(e \u2212t/ \u221a \u03ba ) in the convex case. The selection of hyper-parameters in MaSS is crucial for convergence. The mini-batch size can be divided into three intervals, each with different effects on convergence. The optimal hyper-parameter selection follows a Linear Scaling Rule, different from SGD's regimes. The convergence speed dependence on m is empirically verified in Figure 3. MaSS shows fast convergence on synthetic data, compared to SGD and other methods. Hyperparameters are optimized for SGD, SGD+Nesterov, and MaSS, with different settings based on theoretical analysis and grid search. The fastest convergence of SGD+Nesterov is similar to SGD, indicating non-acceleration. MaSS outperforms other methods. Additional experiments are conducted on different data settings. Real data tasks include MNIST and CIFAR-10 classification, and Gaussian kernel regression on MNIST. Detailed architectures are described in the appendix. In all tasks, the best hyper-parameter setting is selected through dense grid search. Momentum parameter \u03b3 is fixed at 0.9 for SGD+Nesterov and MaSS. Mini batches of size 64 are used for neural network training. MaSS shows good generalization performance in classification accuracy compared to SGD, SGD+Nesterov, and Adam on CNN and ResNet. Standard data augmentation and learning rate reduction protocols are followed for training. Initial learning rate and number of epochs are kept the same for MaSS, SGD, and SGD+Nesterov in each task. The experimental settings for MaSS, SGD, and SGD+Nesterov are the same, with a comparison of their classification accuracy on the CIFAR-10 test set. MaSS shows the best performance, and increasing the initial learning rate can improve MaSS and SGD but degrade SGD+Nesterov. In experiments, SGD+Nesterov with a large step size often diverges, while MaSS and SGD converge consistently. The proposed algorithm initializes variables with the same vector. MaSS can be implemented using specific update rules. There is a bijection between hyper-parameters. A function is strongly convex if it meets certain conditions. Smoothness of a function is defined by specific criteria. Interpolation involves writing the square loss. The variance of the stochastic gradient decreases as weight approaches an optimal solution. The variance of the stochastic gradient decreases as weight approaches an optimal solution in the interpolation setting. The stochastic gradient is perpendicular to the solution set in the parameter space, leading to exponential convergence of SGD in certain convex settings. In the interpolation setting, the stochastic gradient is perpendicular to the solution set in the parameter space, leading to exponential convergence of SGD. The proof technique involves analyzing the asymptotic behavior of SGD+Nesterov in the decoupled model of data with a large condition number. Each component of the weight vector evolves independently due to the diagonal matrix H. The convergence behavior of SGD+Nesterov in the decoupled model of data with a large condition number is analyzed using eigen-systems of matrices B[j]. The top eigenvalue controls the convergence rate, with |\u03bb max| determining whether SGD+Nesterov diverges or converges at a certain rate. Different hyper-parameter settings lead to different convergence behaviors, with at least one eigenvalue of B[1] influencing the outcome. The convergence behavior of SGD+Nesterov in the decoupled model of data with a large condition number is analyzed using eigen-systems of matrices B[j]. The top eigenvalue controls the convergence rate, with |\u03bb max| determining whether SGD+Nesterov diverges or converges at a certain rate. Different hyper-parameter settings lead to different convergence behaviors, with at least one eigenvalue of B[1] influencing the outcome. When w is randomly initialized, the momentum parameter influences the eigenvalues of B[j], affecting the convergence of SGD+Nesterov. The convergence behavior of SGD+Nesterov in the decoupled model of data with a large condition number is analyzed using eigen-systems of matrices B[j]. The top eigenvalue controls the convergence rate, with |\u03bb max| determining whether SGD+Nesterov diverges or converges at a certain rate. Different hyper-parameter settings lead to different convergence behaviors, with at least one eigenvalue of B[1] influencing the outcome. When w is randomly initialized, the conditions for convergence are satisfied with probability 1. By combining Lemmas 1, 2, and 3, it is concluded that SGD+Nesterov either diverges or converges at a rate of (1 \u2212 O(1/\u03ba)) t, indicating the non-acceleration of SGD+Nesterov. The convergence behavior of SGD+Nesterov in the decoupled model of data with a large condition number is analyzed using eigen-systems of matrices B[j]. The top eigenvalue controls the convergence rate, with different hyper-parameter settings leading to various outcomes. At least one eigenvalue of B[2] is shown to be 1 \u2212 O(1/\u03ba) under certain conditions. The step size condition for convergence is discussed, highlighting the importance of \u03b7 being o(1) to prevent divergence. The eigenvalues of matrices B[j] in the decoupled data model are analyzed for convergence behavior of SGD+Nesterov. Different cases are considered based on the relationship between u and t 2, leading to eigenvalues of order 1 \u2212 O(1/\u03ba). The assumption of \u03ba being o(1) contradicts the case where u is O(t 2). In analyzing the eigenvalues of matrices B[j] in the decoupled data model for convergence behavior of SGD+Nesterov, different cases are considered based on the relationship between u and t 2. The analysis leads to eigenvalues of order 1 \u2212 O(1/\u03ba), where the momentum parameter \u03ba is \u03ba-independent. The proof of Lemma 3 follows a similar idea to the proof for the stochastic Heavy Ball method. The proof for stochastic Heavy Ball method involves examining the subspace spanned by \u03a6 t and proving that the eigenvector of B [j] corresponding to the top eigenvalue is not orthogonal to this subspace. This implies a non-zero component of \u03a6 [j] t in the eigen direction of B [j] with top eigenvalue, decaying/growing at a rate of \u03bb t max (B [j] ). The analysis focuses on showing that at least one of \u03a6 3 has a non-zero component in the eigen direction with the top eigenvalue. Since H is diagonal, w [1] and w [2] evolve independently, allowing separate analysis of each component. The analysis focuses on proving that at least one of \u03a6 3 has a non-zero component in the eigen direction with the top eigenvalue. The initial values act as scale factors during training, and the vector v is an eigenvector of M [j] with eigenvalue 0. The analysis aims to show that at least one of \u03a6 3 has a non-zero component in the eigen direction with the top eigenvalue. By verifying that M [j] is rank 3, it can be concluded that M [j] spans a three-dimensional space, including the eigenvector with the top eigenvalue of B [j]. This implies that at least one of \u03a6 t, t \u2208 {0, 1, 2, 3} has a non-zero component in the eigen direction with the top eigenvalue. The analysis shows that for any hyper-parameter setting (\u03b7, \u03b3) with \u03b7 > 0 and \u03b3 \u2208 (0, 1), the top eigenvalue of B [1] is larger than 1 or the top eigenvalue of B [2] is 1 \u2212 O(1/\u03ba). The convergence rate of SGD+Nesterov is lower bounded by (1\u2212O(1/\u03ba)) t for two-dimensional component decoupled data. The convergence rate of SGD+Nesterov is lower bounded by (1\u2212O(1/\u03ba)) t for two-dimensional component decoupled data. When \u03b7 = 1/L 1 and \u03b3 \u2208 [0.6, 1], the condition in Eq.31 is satisfied. Lemma 4 is useful for dealing with the mini-batch scenario. If the hyper-parameters are selected accordingly, the final conclusion is reached with C being a constant. The data points (x, y) \u2208 D are constructed with basis vectors and a square loss function in the interpolation regime. The Hessian matrix is diagonal, allowing for independent evolution in each coordinate. Useful results for analysis include the fourth-moment of Gaussian variables in Gaussian data. In this subsection, empirical verification for the fast convergence of MaSS is shown on synthetic data. The divergence of SGD+Nesterov is also demonstrated when using the same step size as SGD and MaSS. Two families of synthetic datasets are considered: component decoupled and 3-d Gaussian. Different step sizes and momentum parameters are tested on these datasets. In this subsection, empirical verification for the fast convergence of MaSS is shown on synthetic data. The divergence of SGD+Nesterov is also demonstrated when using the same step size as SGD and MaSS. Two families of synthetic datasets are considered: component decoupled and 3-d Gaussian. Different step sizes and momentum parameters are tested on these datasets. Specifically, we compare the performances of SGD, SGD+Nesterov, SGD+HB, and MaSS using their respective hyper-parameter settings. MaSS, with suggested hyper-parameter selections, converges faster than all other algorithms, especially SGD. The experiment illustrates the importance of the compensation term in MaSS compared to SGD+Nesterov. Divergence of SGD+Nesterov is shown with large step sizes, while MaSS shows accelerated convergence. The optimal step size for both SGD and MaSS is \u03b7 * = 1/L 1. The divergence of SGD+Nesterov with momentum parameters 0.9 and 0.99 is demonstrated. The convergence rate of SGD with parameter \u03c1 is proven to be 1 \u2212 1/\u03c1 2 \u03ba t. On a simple Gaussian distributed data, MaSS achieves a slower convergence rate compared to SGD, but a faster rate than SGD with Nesterov acceleration. The experiments involved running MaSS with various mini-batch sizes on the Gaussian dataset using a fully-connected neural network with 3 hidden layers. The fully-connected neural network has 3 hidden layers with 100 ReLU-activated neurons each, followed by dropout layers. It has 784-dimensional input vectors and 10 softmax-activated output neurons, with approximately 99k trainable parameters. The Convolutional Neural Network (CNN) has three convolutional layers with kernel size 5x5, 64 channels in the first two layers, and 128 channels in the last layer. Each convolutional layer is followed by a max pooling layer and a fully-connected ReLU-activated layer before the output layer. A dropout layer is applied after the fully-connected layer. The CNN has approximately 576k trainable parameters. A Residual Network (ResNet) with 32... The ResNet-32 model has 576k trainable parameters and consists of 15 residual blocks with varying output shapes. It includes an average pooling layer and an output layer with softmax non-linearity. The model is used to classify the MNIST dataset and the CIFAR-10 dataset."
}