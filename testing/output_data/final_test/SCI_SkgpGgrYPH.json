{
    "title": "SkgpGgrYPH",
    "content": "Energy-based models (EBMs) have been successful in continuous spaces but have not been effectively applied to model text sequences. Generating negative samples using pre-trained auto-regressive language models helps address the challenge of increasing energy in high-dimensional and discrete inputs. The EBM is trained to distinguish real text from text generated by auto-regressive models. Energy-based models (EBMs) have a long history in machine learning and are defined by a single scalar energy function. Residual EBMs show good generalization ability in classifying machine or human-generated inputs, but are more sensitive to the training set used by the generators. Energy-based models (EBMs) are a generalization of probability models with an energy function that does not need to be normalized. Training involves decreasing the energy function at observed data points and increasing it at other points. Different learning strategies vary in how negatives are mined, using methods like gradient descent, Monte Carlo sampling, or enforcing global constraints on the energy function. In this work, the authors propose using large pre-trained language models to mine negatives for Energy-based models (EBMs) in text data. Text poses challenges due to its discrete nature, making traditional gradient-based methods ineffective. By leveraging the strength of auto-regressive language models, they aim to learn in the residual space to constrain the search space of negatives. The authors propose using pre-trained language models to generate negatives for Energy-based models in text data. They aim to train the model to assign lower energy to true human text than to text generated by the language model. The generalization of this approach is crucial for text modeling and discriminating real from machine-generated text. The study focuses on the generalization ability of residual EBMs in detecting real text from machine-generated text. It shows that the energy function is robust to changes in generator architecture and data used for training. Larger models and longer generation improve performance, but sensitivity to the training dataset of the generator is observed. The study explores the use of EBMs with negatives generated by a pre-trained language model. The training process involves a generator and a discriminator, similar to GANs, but with the generator trained beforehand. The discriminator learns from the residual error of the generator, making it a \"cascade\" model. The approach of using a separately trained scoring function to evaluate outputs has a history in parsing and machine translation. The approach of using EBMs with negatives generated by a pre-trained language model has been recently re-discovered in dialogue modeling. Previous works have studied machine generation detection but not how findings generalize to different generator architectures and corpora. Our work addresses this question, providing a rigorous experimental protocol and quantitative analysis. Our work is a broader investigation on the generalization of discriminator architectures to various generator types and training datasets, including GPT-2. We aim to train an energy-based model to score the compatibility of input sequences with context, using a set of parameters. The context can vary, such as preceding text, keywords, or a title. In this work, the goal is to train an energy-based model using neural networks to assign lower energy to human-generated text sequences compared to others. The energy function is parameterized using different losses, such as binary cross-entropy and ranking loss. Positive and negative samples are considered during training, with results reported in the appendix. The energy-based model is trained using binary cross-entropy loss with negative samples generated from machine-generated tokens. Instead of traditional methods like MCMC or Langevin dynamics, modern auto-regressive language models are used for negative sampling in this work. The text chunk discusses the generation of negative samples using top-k sampling with a trained language model. It explains the process of generating negatives for both left-to-right and right-to-left models based on a given context. The section also covers dataset descriptions, preprocessing steps, architecture details for generators and scoring functions. The text chunk provides architecture details for generators and scoring functions, and introduces evaluation settings for models trained on three different corpora: Books, CCNews, and Wikitext. These datasets vary in size and domain, with CCNews being the largest and most factual. The datasets enable assessment of the energy function's ability to fit and generalize across various axes. Positive sequences are extracted from text windows of different lengths on Wikitext, Books, and CCNews. A common vocabulary of 50k tokens is used, and a transformer-based network is employed to generate negatives. The study utilizes a transformer-based network to generate negatives for language models of different sizes: TransfSmall, TransfBig, and TransfHuge. The models vary in the number of blocks and heads in the multi-head attention module. TransfHuge has significantly more parameters than TransfBig and is trained on CCNews. Different architectures, including a 12-layer convolutional model, are considered for training models on various datasets. The study explores different architectures, including a 12-layer convolutional model, and third-party trained GPT2 models for generating language model negatives. Context size and generation size are found to impact the learning task, with an analysis of the effect of context size provided. Three architectures for the energy function are considered: Linear, BiLSTM, and a bag of tokens approach. The study examines various architectures for the energy function, including BiLSTM and Transformer models with different units in the embedding and hidden layers. Two versions of BiLSTM, \"BiLSTMsmall\" and \"BiLSTMbig,\" are compared, each with 4 layers. Transformer computes energy similarly to BiLSTM but uses bidirectional Transformer layers instead of bi-LSTM layers. The study compares different architectures for the energy function, including BiLSTM and Transformer models with varying units in the layers. BiLSTM models are replaced by either bidirectional Transformer layers (BiTransf) or Transformers with causal self-attention (UniTransf). BiTransf uses BERT-Large architecture with 24 self-attention layers, while UniTransf has 12 layers and is initialized from a language modeling task. The study uses Adam optimizer with warmup and multi-GPU training for model training. Training stops after 2.5M samples without improvement. Mixed precision training and gradient clipping are used for faster training. Evaluation is done in four settings: in-domain, cross-architecture, cross-corpus, and unseen. Details on hyper-parameters and number of parameters are provided in the appendices. The study uses different settings for evaluation: in-domain, cross-architecture, cross-corpus, and unseen. The testing generator G test is trained using C test and architecture A test. Different configurations are used for C test and A test in each setting, with performance measured in terms of classification accuracy. The study evaluates the generalization ability of energy functions in distinguishing real from fake completions. Results show high accuracy on the Books dataset, moderate accuracy on the CCNews dataset, and lower accuracy on the Wikipedia dataset due to overfitting. The testing generator is trained using different configurations for evaluation in various settings. The study evaluates the generalization ability of energy functions in distinguishing real from fake completions using different model architectures and datasets. Results show high accuracy on the Books dataset, moderate accuracy on the CCNews dataset, and lower accuracy on the Wikipedia dataset due to overfitting. The testing generator is trained using various configurations for evaluation. In Table 3, the UniTransf energy function is evaluated on different generator architectures Conv and TransfSmall. Despite Conv having more parameters, both generators achieve similar perplexity. UniTransf struggles more with TransfSmall negatives, but is more robust when trained with them. Overall, UniTransf tested with mixed negatives performs slightly better when trained with TransfSmall negatives. When training with harder negatives from TransfSmall, models generalize less well across corpora. Training on the union of multiple corpora yields a robust energy function, with accuracy comparable to in-domain data. BiTransf with 355M parameters performs well on CC-News. The study compared the performance of BiTransf with 355M parameters to UniTransf on CC-News, showing a 5% improvement in accuracy when trained on a combination of all corpora. The model was pre-trained on Wikipedia, not Wikitext103. Test time negatives were generated by different models, including GPT-2 trained on WebText. This demonstrates unseen generalization in energy functions. The study demonstrated unseen generalization in energy functions by comparing the performance of BiTransf with 355M parameters to UniTransf on CC-News. The accuracy improved by 5% when trained on a combination of all corpora. Generations from the GPT2 small model showed similar accuracy to in-domain settings, while accuracy decreased with larger black-box generators like GPT-2 medium. A big enough energy model trained with a single big generator can efficiently discriminate a black-box generator. The study showed improved accuracy in energy functions with smaller GPT-2 models, but accuracy decreased with larger models like GPT-2 medium. The dataset included 250k generated texts using top-k or random sampling. Text segments were split into blocks of 160 tokens for training and evaluation. The energy function's generalization to unconditional generation from various GPT2 models was analyzed in Table 6. In Table 6, the BiTransf energy function outperforms the TF-IDF baseline in the in-domain setting, achieving nearly 100% accuracy. However, in generalization mode, BiTransf only surpasses TF-IDF when the generator is less than three times the size used during training. The energy function was trained with fixed length input and a prefix, leading to significant generalization results. The energy functions were trained with a fixed length input and a prefix, resulting in significant generalization results. The dependency between performance and prefix length was investigated, showing that as the prefix length increases, the discrimination task becomes harder. The impact of the number of negatives and using the most offending negative in the loss was also studied. The energy functions were trained with a fixed length input and a prefix, showing significant generalization results. Increasing the prefix length makes the discrimination task more challenging. The energy function is less robust to negatives from a different corpus, with harder negatives improving accuracy. Changing entities can drastically affect the energy, indicating vulnerability to out-of-domain samples. The energy function tends to score randomly generated text lower than real text, suggesting a focus on learning generated text regularities over real text regularities. The energy function assigns low scores to text not generated by its training generator, which may be a drawback. However, it focuses on learning generated text patterns rather than real text. The energy function's accuracy is not crucial for unlikely examples according to language models. The energy function in EBMs decreases in value with perturbations, while the language model's negative log likelihood increases. EBMs offer potential for more expressive text models by scoring sequences rather than single words. Training EBMs is challenging due to the difficulty in generating negatives using the energy function, but leveraging pre-trained language models for negative samples is a proposed solution. EBMs show good generalization ability when trained on large datasets, especially when tested with negatives from different generator architectures. However, generalization is weaker with generators trained on other corpora, but can be improved by training on even larger composite datasets. Scaling up EBMs for text involves increasing architecture size and dataset diversity. EBMs excel in real/fake text discrimination, outperforming other models. EBMs are powerful in discriminating real/fake text, outperforming other models. The number of parameters in generator language models and scoring functions directly affects computational cost. Models from HuggingFace and OpenAI GPT2 repositories are used, with sizes ranging from 124M to 355M. Ranking loss is also considered in the analysis. The ranking loss in the input embedding layer makes energy values local by comparing pairs of positive and negative examples with the same context. In contrast, binary cross entropy loss forces all positive examples to have negative energy and all negative examples to have positive energy, regardless of context. Both losses yield similar results empirically. The model is evaluated using precision at 1 (P@1) when trained with the ranking loss. The text discusses training models using PyTorch and Adam optimization. Large models were trained on multiple machines with GPUs, speeding up training with reduced precision and cosine scheduling of the learning rate. Training time was reduced significantly compared to single node training. In the context of training models with PyTorch and Adam optimization, the text demonstrates how changing a few words can alter the sentiment of a sentence using gradient information. By utilizing a language model like UniTransf, real and fake examples can be distinguished. The process involves perturbing negatives to violate the margin by replacing words based on gradient information from the energy function. By changing a few words in the original sample x, the score of x can be approximated to search for token replacements that increase/decrease energy the most. Replacing highlighted words can easily change a negative sample to a positive one, with reported score and language model perplexity in parentheses."
}