{
    "title": "H1fKsUQKjm",
    "content": "Recurrent convolution (RC) is a model compression strategy for deep convolutional neural networks. It reduces redundancy across layers but doesn't match the performance of standard networks. A variant with independently learned batch normalization layers for different unrolling steps improves RC networks. Unrolling a convolutional layer multiple steps can enhance performance and indirectly aid in model compression. Training a recurrent convolutional (RC) neural network is a potential compression strategy that complements existing approaches to reduce the computational and memory usage of deep convolutional neural networks (DCNNs). This strategy aims to improve performance and indirectly aid in model compression. Training a recurrent convolutional (RC) neural network involves unrolling convolutional kernels multiple times on the computational graph to share weights across the whole layer. This compression strategy aims to reduce redundancy across layers and can be further compressed using approaches like weight quantization. However, RC networks may not match the performance of DCNNs with the same depth. In this paper, the focus is on improving the performance of recurrent convolutional (RC) neural networks for model compression. The approach involves learning batch normalization layers independently at each unrolling step. Experimental results on the CIFAR dataset show promising results compared to standard networks. The study highlights the potential of RC as a model compression strategy for image classification using deep convolutional neural networks. The study focuses on image classification using deep convolutional neural networks and recurrent convolutional (RC) neural networks for model compression. The state equation of RC involves differentiable operators like convolutions and parameterized by w. Each res-block in RC consists of convolutional layers, BN layers, and Relu activations. Sharing BN layers across unrolling steps leads to unsatisfactory performance due to varying input statistics. The study focuses on image classification using deep convolutional neural networks and recurrent convolutional (RC) neural networks for model compression. Each res-block in RC consists of convolutional layers, BN layers, and Relu activations. Independent BN layers are applied at each unrolling step to avoid merging statistics and improve representation power. Parameters of BN layers act as additional sequential inputs to an RC module. The most general form of the state equation of recurrent neural networks involves shared convolution kernels and parameters of BN layers. Independent BN layers are cost-effective in terms of computation and memory compared to convolutional layers. Practical considerations include maintaining input and output sizes in each RC block, adjusting feature map size outside RC blocks, reducing spatial resolution through average pooling, and increasing channels via invertible downsampling. This helps avoid gradient explosion and stabilizes training. To stabilize training, gradient clip is used during training of RC networks with 2 \u00d7 2 spatial sub-sampling. Experiments on CIFAR-10 and CIFAR-100 datasets compare the effects of BN layer usage, RC networks with standard ones, and RC networks with different unrolling steps. Models are implemented using Pytorch and trained multiple times for averaging test errors. RC networks with 1, 2, and 4 unrolling steps are compared with standard DCNNs. In this paper, RC networks are compared with standard DCNNs using ResNet-18 as a base. Results show that using independent BN layers improves RC-4 accuracy significantly, while shared BN layers decrease accuracy. Unrolling RC networks more steps improves accuracy, but standard ResNet still performs better when depths are the same. RC-4 with original depth of 6 outperforms others. The study compares RC networks with standard DCNNs using ResNet-18 as a base. Results show that RC-4 with original depth of 6 outperforms Resnet-10. RC networks are easy to train and suggest recurrent convolution as a strategy for model compression. Unrolling the same convolutional layer multiple steps can improve network accuracy and play a role in model compression. Further improvements in RC performance are possible. The architecture of RC-4 involves unrolling steps of an RC res-block with i channels. The computational graph is modified to match that of the unrolled RC network. Hyper-parameters are kept the same for fair comparisons, with training done via SGD with specific settings. Learning rate is adjusted at specific epochs, with deeper networks trained for 160 epochs. The depth of networks determines the number of training epochs, with deeper networks trained for 160 epochs and shallower networks for 120 epochs. Gradient clipping improves stability and performance for RC networks but has minimal impact on standard networks. BN layer inference involves moving mean and variance, with \u03b3 and \u03b2 as learned parameters. Variables for BN layers in RC-4 on CIFAR-10 are shown in figures. The BN layer of RC-4 trained on CIFAR-10 is shown in figures, with parameters \u00b5, \u03c3, and \u03b2."
}