{
    "title": "S1lVhxSYPH",
    "content": "MobileNets have advanced resource-efficient architectures for computer vision neural networks. Further compression to binary or ternary values is needed for energy savings and speedups on specialized hardware. The proposal focuses on quantizing MobileNets to ternary values based on the observation that convolutional filters may respond differently to this quantization. The proposed quantization method combines full-precision and ternary weight filters to create energy-efficient MobileNets. It results in significant energy savings and model size reduction without sacrificing accuracy or throughput on specialized hardware. The MobileNets architecture introduces depthwise-separable convolution as an efficient alternative to standard convolution operations, enabling computation and size compression of CNN models for resource-constrained platforms. Further compression through model quantization has been valuable, with the proposed method combining full-precision and ternary weight filters to create energy-efficient MobileNets. Model quantization, specifically binary and ternary quantization of MobileNets weights, can lead to significant energy savings and improved throughput on custom hardware like ASICs and FPGAs. This is due to the reduction of multiplications and the increase in additions, which consume less energy and space on the chip. Specialized hardware can prioritize additions over multiplications to achieve high throughput and energy efficiency in binary and ternary-weight networks. Recent work on StrassenNets presents a mathematically profound way to approximate matrix multiplication computation using mostly ternary weights and a few full-precision weights. It exploits Strassen's algorithm to generate product matrix elements through combinations of intermediate terms, determined by the hidden layer width. The hidden layer width in StrassenNets determines the addition and multiplication budget of a convolutional layer, affecting the approximation error. While StrassenNets show no loss in predictive performance compared to full-precision models for some networks, its effectiveness varies based on neural network architecture. Strassenifying can reduce model size but may increase addition operations, impacting energy efficiency during inference. Wide hidden layers are used to approximate convolutional filters, leading to a significant increase in additions. Our observations suggest that not all filters in a network layer require wide strassenified hidden layers. Different filters may respond differently to ternary quantization and strassenified convolution. Some filters are harder to approximate using ternary bits and have a larger impact on model accuracy loss. We propose a layer-wise hybrid filter banks for MobileNets to achieve state-of-the-art accuracy levels with a fraction of the hidden layer budget. The proposed hybrid filter banks for MobileNets reduce model size and energy consumption while maintaining accuracy by using precision critical filters in full precision and ternary quantization for others. This approach results in a 46.4% reduction in multiplications and a 51.07% reduction in model size with a modest increase in additions. The hybrid filter banks for MobileNets achieve a 27.98% energy savings per inference with no throughput degradation compared to MAC-only hardware. There is a minimal 0.51% accuracy loss, making it a first step towards quantizing MobileNets to ternary values with negligible accuracy loss on ImageNet. The paper discusses the use of per-layer hybrid filter banks and current quantization algorithms for MobileNets. The paper discusses designing layer-wise hybrid filter banks for MobileNets to achieve a balance between accuracy and computation costs. It reviews existing works on ternary quantization and introduces per-layer hybrid filter banks to quantize MobileNets to ternary values. The study applies ternary weight quantization to MobileNets-V1, reducing model size but sacrificing predictive performance. StrassenNets offer a solution with near state-of-the-art accuracy while maintaining acceptable increase in addition operations. Strassen's matrix multiplication algorithm reduces the number of multiplications needed for 2 \u00d7 2 matrices from 8 to 7 by converting the operation into a 2-layer sum-product network computation using ternary matrices. The StrassenNets work builds on Strassen's algorithm to achieve approximate matrix multiplications in DNN layers using fewer hidden layer units compared to the standard algorithm. StrassenNets uses SPN-based DNN framework to learn ternary weight matrices for approximate matrix multiplications in DNN layers, achieving significant compression and control over computational complexity. Applying StrassenNets over MobileNets architecture reduces computational costs and model size, enabling energy-efficient networks for longer battery life and complex use-cases on limited memory DNN hardware accelerators. The quantization of MobileNets-V1 is extensively studied, which includes stacking 3x3 and depthwise separable convolutional layers. Strassenifying MobileNets reduces multiplications significantly but increases additions to achieve comparable accuracy to state-of-the-art MobileNets with 16-bit weights. The strassenified network with r = 2c out configuration reduces multiplications by 97.91% but increases additions by 317.59%. This leads to modest energy savings per inference but significant degradation in throughput. Different values for hidden layer width (r) were explored, with fewer hidden units resulting in a significant accuracy loss. Using fewer hidden units e.g. r = c out incurs a significant accuracy loss of 3.4%. Strassenifying MobileNets with DS layers increases additions due to the dominance of 1 \u00d7 1 pointwise convolutions, resulting in a 2 : 1 increase in additions compared to standard convolutions. In contrast to the significant accuracy loss incurred by using fewer hidden units, strassenified DS convolutions with ternary weight filters cause a marginal increase in additions compared to standard convolutions. The overhead of addition operations with strassenified DS convolutions increases with the width of the hidden layers, potentially offsetting the benefits. While previous studies showed better trade-offs with ResNet-18, applying StrassenNets to MobileNets does not yield the same results. When StrassenNets is applied over MobileNets, the efficiency of DS convolutions in terms of parameters can lead to quantization errors. This results in an increase in computational costs, diminishing energy savings and runtime speedups, especially in mobile applications with high accuracy and low latency requirements. Leveraging the compute efficiency of DS layers and reducing model size of strassenified convolutions is necessary to address these challenges. The accuracy drop in strassenified MobileNets with different configurations suggests quantization errors accumulate over layers. Using r = 2c out configuration recovers accuracy loss but assumes wider hidden layers for all filters, which may not be necessary for every filter due to varying sensitivity. Different convolutional filters have varying sensitivity to ternary quantization due to extracting different types of features. Some filters can be easily quantized with narrower hidden layers while maintaining low L2 reconstruction error, while others require wider layers for the same loss. This variability in filter response is illustrated in Figure 1, where certain filters can closely approximate output maps with fewer hidden layer units. Convolutional filters have different sensitivity to ternary quantization, with some requiring wider hidden layers to maintain low L2 loss. Groups of filters may share structural similarities, making them more amenable to quantization with fewer hidden layer units. Convolutional filters with common value structure are more suitable for quantization with fewer hidden layer units. This allows for exact convolution with fewer multiplications, leading to lower quantization error. This observation motivates the proposal of a novel quantization method. The proposed novel quantization method aims to quantize easy-to-quantize weight filters of a network layer to ternary values while preserving the representational ability of the overall network by relying on few full-precision difficult-to-quantize weight filters. This layer-wise hybrid filter bank strategy combines the strength of a full-precision network as a highly-accurate classifier with StrassenNets to reduce model size and number of multiplications, maintaining a balance between computational costs and predictive performance. The proposed quantization method aims to quantize easy-to-quantize weight filters to ternary values while preserving the network's representational ability. This approach combines full-precision difficult-to-quantize filters with StrassenNets to reduce computations and memory footprint without sacrificing accuracy. The proposed quantization method combines full-precision and ternary filters to generate an output feature map with identical shape as the baseline network. The technique involves using traditional convolution with full-precision weight filters and Strassen convolution with ternary weight filters to produce the desired feature map channels. The bias term is not considered in this discussion. The quantization technique for MobileNets involves using hybrid filter banks, where \u03b1% of output channels are generated using full-precision weight filters and the remaining channels using ternary weight filters. Depthwise convolutions of the depthwise-separable layers are not quantized using this method. The curr_chunk discusses the application of StrassenNets to 3x3 and 1x1 convolutions, comparing traditional full-precision weights with ternary weights. It mentions quantization hurting accuracy without significant savings, and quantizing hybrid filter banks using different r values for hidden layer width. The goal is to achieve savings in model size and addition operations without compromising accuracy. The presented quantization technique applies StrassenNets to compress the last fully-connected layer of MobileNets, inspired by the Inception module. End-to-end training involves joint training of full-precision and strassenified weight filters to maximize accuracy using a gradient-descent based algorithm. The hybrid filter banks in the quantization technique combine full-precision and ternary strassenified convolutions to train difficult-to-quantize filters with full-precision values and less susceptible filters with ternary values. This approach helps recover any accuracy loss during end-to-end training. The study utilizes knowledge distillation to recover accuracy loss in a hybrid network compressed with strassenified matrix computations. The MobileNets-V1 architecture with per-layer hybrid filter banks is evaluated on the ImageNet dataset and compared against state-of-the-art MobileNets with 16-bit floating-point weights. The network architectures use a width multiplier of 0.5 2 to reduce training costs with limited GPU resources. The MXNet framework is used for evaluation. In this work, the MobileNets-V1 architecture with a width multiplier of 0.5 achieves better accuracy using the GluonCV toolkit compared to Tensorflow. The baseline and proposed architectures use 16-bit floating-point weights, with activations quantized to 16-bit values. The GluonCV toolkit does not support training with 8-bit weights and activations at the moment. The Hybrid MobileNets architecture is trained using the Nesterov accelerated gradient (NAG) optimization algorithm with specific hyperparameters. Training starts with full-precision strassen matrices for 200 epochs, followed by quantization for another 75 epochs. After training with full-precision strassen matrices for 200 epochs, the Hybrid MobileNets architecture undergoes quantization for 75 epochs. Quantization converts matrices to ternary values with scaling factors. L2 distances between pre- and post-quantization weight vectors show most filters have low-to-moderate changes, while some have significant movement. This indicates that full-precision filters maintain the network's representational power. The strassen matrices of hybrid filter banks are fixed to ternary values, and training continues for 25 epochs to absorb scaling factors into full-precision matrix multiplication. The proposed hardware accelerator for Hybrid MobileNets replaces some MAC units with low-cost adders to improve energy and runtime efficiency. By optimizing the ratio of MAC units to adders, the accelerator aims to maximize throughput within a fixed silicon area. The evaluation focuses on how the parameter \u03b1 impacts the performance of models in Hybrid MobileNets. The study uses a consistent value of \u03b1 for all layers, but suggests that using different values for each layer could result in better trade-offs between cost and accuracy. The goal is to achieve maximum throughput by optimizing the ratio of MAC units to adders in the hardware accelerator for Hybrid MobileNets. The table compares computational costs, model size, and energy efficiency of baseline MobileNets-V1, ST-MobileNets, and Hybrid MobileNets on ImageNet dataset. Hybrid MobileNets achieve significant reduction in MAC and addition operations while maintaining baseline accuracy. The study systematically explores model hyperparameters to develop Hybrid MobileNets with varying \u03b1 and hidden layer width configurations. The study compares the Hybrid MobileNets with ST-MobileNets and baseline full-precision MobileNets in terms of model size reduction and computational costs. Different configurations of Hybrid MobileNets show a 50% decrease in model size compared to the baseline. However, certain configurations lead to a decrease in throughput on hardware accelerators due to increased additions. The Hybrid MobileNets with \u03b1 = 0.5 and r = c out configuration strikes a balance between accuracy, computational costs, energy, and throughput. It achieves comparable accuracy to baseline MobileNets while reducing MACs and multiplications by 47.26% and 46.4% respectively, with a modest increase in additions. The Hybrid MobileNets with \u03b1 = 0.5 and r = c out configuration significantly reduces the number of additions to about 142.37M compared to 624.27M additions of ST-MobileNets. This leads to a 27.98% energy savings per inference without compromising throughput. The reduction in additions is attributed to strassenifying easy-to-quantize filters and using ternary weights matrices, resulting in a smaller model size. The Hybrid MobileNets with \u03b1 = 0.5 and r = c out configuration reduces model size by 51.07% and energy per inference by 27.98%. It maintains accuracy and throughput compared to baseline MobileNets, with fewer accesses to energy-intensive DRAM. The use of knowledge distillation does not affect accuracy. Sparsity in ternary weight matrices of strassenified layers can improve energy savings and run-time performance. Weight pruning, sparsifying filters, and pruning channels are methods to make neural networks more efficient. Recent work on channel pruning shows a significant reduction in computational costs with negligible drop in accuracy for MobileNets. Hybrid filter banks successfully quantize weight filters of MobileNets to ternary values. Hybrid filter banks quantize weight filters of MobileNets to ternary values, achieving comparable accuracy to full-precision models on ImageNet. Tensor decomposition techniques can further reduce model size and computational complexity. The benefits of hybrid filter banks can extend to other resource-efficient architectures like MobileNets-V2 and ShuffleNet. In this work, hybrid filter banks are proposed for MobileNets to quantize weights to ternary values while maintaining high accuracy on large datasets. The use of 16-bit floating-point format for intermediate activations and weight filters is explored, with potential future investigation into quantizing to 8-bit or less. Channel pruning may assist in reducing computational complexity, and Strassen's algorithm is mentioned for efficient matrix multiplication. The proposed hybrid filter banks for MobileNets aim to quantize weights to ternary values while maintaining accuracy. Strassen's algorithm is mentioned for efficient matrix multiplication, reducing the number of multiplications required. Inspired by GoogLeNet's Inception module, the per-layer hybrid filter banks extract information from the previous layer. In a traditional convolutional network, each layer extracts information from the previous layer to transform input data. Salient features can vary in size, requiring the right kernel size. Training images from ImageNet are preprocessed and resized for evaluation. Hyperparameters values for training Hybrid MobileNets are shown in Table 3, similar values are used for baseline full-precision MobileNets and ST-MobileNets. The learning rate scheduling for training baseline full-precision MobileNets and ST-MobileNets involves a 'warm up' period where the rate is annealed from zero to 0.2 over the first 5 epochs, followed by a gradual reduction using a cosine decay function."
}