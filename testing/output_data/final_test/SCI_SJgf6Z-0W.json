{
    "title": "SJgf6Z-0W",
    "content": "Our approach introduces a new method for estimating continuous actions in reinforcement learning using actor-critic algorithms. Instead of predicting one continuous action estimate, we predict multiple actions with the policy network and sample one action uniformly during training and testing. This approach allows for better exploration of the state space and convergence to a better policy. Recent advancements in reinforcement learning have led to the development of novel algorithms for continuous control, such as Deep Deterministic Policy Gradient (DDPG) and Asynchronous Advantage Actor Critic (A3C). While DDPG produces point estimates for actions, A3C and other stochastic policy gradient algorithms output distribution parameters, allowing for better exploration of the state space and convergence to a better policy. In the inverted pendulum task, a deterministic agent choosing a single action for every state can lead to sub-optimal results. The distribution parameter estimation approach like A3C may work better in cases with only two good options, but not when there are more. Allowing the agent to suggest multiple actions helps resolve such cases easily and prevents sub-optimal convergence during training. The Multiple Action Policy Gradients (MAPG) algorithm addresses sub-optimal convergence during training by allowing the actor to suggest multiple actions at each time step, leading to better exploration of the solution space and potentially higher quality final solutions. This approach eliminates the need for external exploration mechanisms like OrnsteinUhlenbeck process noise, parameter noise, or differential entropy of normal distribution. The Multiple Action Policy Gradients (MAPG) algorithm allows predicting a pre-defined number of actions at each time step, improving exploration during training. It can provide insights into the decision-making process by analyzing the variance of predicted actions. MAPG outperforms DDPG in various experiments without changing hyper-parameters or training scheme. In experiments, MAPG shows improved performance over DDPG, especially in exploration. Deep neural networks are widely used in reinforcement learning, with methods like DQN and Guided Policy Search handling high and low dimensional inputs. Recent methods for continuous control include vanilla policy gradient and actor-critic methods like TRPO. Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization Algorithms are used as vanilla policy gradient and actor-critic methods. Deep Deterministic Policy Gradient (DDPG) and Asynchronous Advantage Actor Critic (A3C) utilize actor-critic architectures to learn state-action functions for policy gradients. Stochastic Value Gradients (SVG), Generalized Advantage Estimation (GAE), A3C, and TRPO all employ stochastic policy gradients to predict action probability distributions. The action values are sampled from the predicted distribution, typically using a parametrized normal distribution. DDPG extends DPG and models action noise instead of the true action distribution. DDPG extends DPG by using deterministic policy gradients with neural networks for actor-critic functions. BID5 estimates stochastic action values using a sequential Monte Carlo method. SMC learning works well in small state space problems but not in high dimensional non-linear action space problems. Multiple Hypothesis Prediction BID13 is related to Multiple Choice Learning BID7 and BID6. In this section, we will detail how multiple action policy gradients can be derived and compare it to DDPG in a reinforcement learning setup where an agent interacts with an environment. The agent observes the state, takes actions, and receives rewards, aiming to learn a policy that produces a probability distribution over actions for each state. In reinforcement learning, the agent aims to learn a policy that produces a probability distribution over actions for each state in a Markov Decision Process (MDP). Methods like DDPG use a deterministic policy, while Q-learning selects the highest value action for the current state. In reinforcement learning, the agent aims to learn a policy that produces a probability distribution over actions for each state in a Markov Decision Process (MDP). BID21 selects the highest value action for the current state by approximating the Q value of an action with a critic network. Multiple action prediction allows for learning a stochastic policy by predicting a fixed number of actions and sampling from them uniformly. This approach enables the agent to employ a stochastic policy when necessary and approximate the action distribution with multiple samples. The maximal value of 6 is achieved when all inner Q values are equal. If one action has a lower return than another, a policy can be modified to predict the higher return action. This contradicts the assumption of the MAPG algorithm. The actor network is modified to output multiple actions for learning a stochastic policy in reinforcement learning. The actor network outputs multiple actions for learning a stochastic policy in reinforcement learning. When selecting a random action from the proposed actions, they should all be equally good in an optimal policy. This implies that sometimes all proposed actions may be identical, especially in situations where there is only one correct action to take. In reinforcement learning, the actor network outputs multiple actions to learn a stochastic policy. If all proposed actions are equally good, they may be identical when there is only one correct action to take. The set of stochastic policies includes deterministic policies, and a deterministic algorithm could perform equally well by predicting identical actions for every state. The MAPG technique modifies the actor to produce multiple outputs, with a gradient applied only to the selected action during sampling. In this section, the performance of MAPG is analyzed by comparing scores with DDPG and A3C on different tasks, studying the impact of the number of actions on performance, observing variance in predicted actions, and comparing DDPG and MAPG performance without external exploration noise. The base actor and critic networks remain constant in all experiments. The actor and critic networks have fixed architectures with two fully connected hidden layers of 64 units each. The actor network takes the current state as input and outputs multiple actions, with one randomly chosen. The critic network takes the state and action as input and outputs a Q-value. The critic network is trained by minimizing the mean square loss, while the actor network is trained using policy gradients. Ornstein-Uhlenbeck process noise is added to the action. The actor-critic networks have fixed architectures with two hidden layers of 64 units each. Ornstein-Uhlenbeck process noise is added to the action values for exploration. A3C training was done for two million steps, but it performed poorly compared to DDPG. Average rewards over 100 episodes were reported for different tasks, with higher scores achieved in all environments except for the HUMANOID task. The HUMANOID task had lower performance due to higher dimensionality of the world state, making observation more difficult. Policy-based reinforcement learning scores vary based on hyper-parameters, reward function, and codebase. To minimize variation, parameters were fixed and only M was varied. Evaluation was based on average reward over 100 episodes after training for 2 million steps. Actors with higher M face challenges in learning a meaningful action distribution. Scores for HOPPER and WALKER2D are shown in a plot. In the HOPPER and WALKER2D environments, the overall score increases with M. The variance in action values decreases with time as the network converges to a good policy. Despite some spikes in standard deviation, the policy can exploit situations with multiple good actions. The variance during one episode in the Pendulum environment is analyzed. The analysis of the Pendulum environment shows that the agent has learned two ways to swing up the pole, either clockwise or counter clockwise. Once the target pose is reached, a slight jitter helps maintain balance. This demonstrates that a MAPG agent can learn meaningful policies and be more diverse in its actions. During training, MAPG leads to more diverse agents capable of exploring in multiple directions. Comparing DDPG and MAPG performance on Pendulum and HalfCheetah environments, MAPG achieves better average rewards without external noise. Even without exploration, MAPG performs comparably to DDPG with added noise. In HalfCheetah, the performance gap between DDPG and MAPG widens after 500 epochs, indicating initial similarity in predicted actions. MAPG, a technique leveraging multiple action prediction, leads to more diverse agents during training. In experiments, MAPG with M = 10 outperformed DDPG, completing multiple laps compared to DDPG's inability to complete even one lap. The average distance traveled over 100 episodes was 807 meters for DDPG and 5882 meters for MAPG. MAPG agents explore more possibilities due to their stochastic nature, leading to more stable and better policies. The method enables better exploration of the state space and shows improved performance over DDPG in continuous control problems. The study concludes with insights gained from action variance and suggests investigating the hyper-parameter M's task-specific selection in the model. The idea of predicting multiple action proposals can be extended to other algorithms like NAF BID1 or TRPO. Evaluating MA-NAF and MA-TRPO will help determine the generality of the approach. Box plots similar to FIG0 and 1b will be displayed for the remaining tasks."
}