{
    "title": "HyDMX0l0Z",
    "content": "Generative Adversarial Networks (GANs) can produce conflated images when trained on diverse datasets with separate modes. This is due to the continuous nature of the generator function and connected input noise set. To address this issue, a novel method introduces learnable discontinuities in the latent noise space to break the assumption of connected outputs. This approach involves training multiple generators to create discontinuities in the G function and is augmented with a classifier C. The GAN formulation is enhanced with a classifier C to predict noise partitions, promoting diversity. Experiments on various datasets show that noise partitions correspond to different data distribution modes, improving image quality. GANs face mode collapse issues, but introducing discontinuities in the G function helps address this instability problem. Despite efforts to address mode collapse in GANs, state-of-the-art models still struggle to generate meaningful samples on complex datasets like ImageNet. The continuous nature of the generator function and the connectedness of the latent noise space contribute to this issue, resulting in conflated images that do not distinctly belong to any specific mode in the dataset. Real-life images are believed to lie on low-dimensional manifolds, with distinct modes occupying separate manifolds. This connectivity problem poses challenges when dealing with diverse datasets with varied modes. The continuous nature of GAN generators and connected latent noise spaces lead to conflated outputs that include tunnels connecting disjoint manifolds. This results in mode dropping or garbled outputs when trained on complex datasets like ImageNet. New methods are needed to address this issue without compromising mode coverage. Our approach introduces learnable discontinuities in the latent noise space by creating N different linear mappings in the generator's input layer. This breaks the connectedness assumption, leading to the existence of tunnels. A classifier is used to predict which partition created a given input, and loss functions are modified accordingly. Experimental validation is conducted on various datasets including MNIST, celebA, STL-10, and a challenging artificial dataset. The paper introduces a novel GAN setup to address training issues on large and diverse datasets. It provides theoretical analyses, identifies key problems, and reports improvements over existing formulations. The paper discusses the challenges of training GANs and introduces DCGANs as a solution for better image quality. Despite advancements in architecture, GANs still face issues with unstable training. Several papers have proposed heuristics and regularization techniques to stabilize GAN training and address issues such as mode collapse and vanishing gradients. BID1 provided theoretical analysis of GAN training dynamics, highlighting problems with the original formulation and suggesting alternative objective functions. BID9 introduced a new loss function to approximate Wasserstein minimization. BID9 proposed a new loss function to minimize Wasserstein distance between real and generated data distributions, aiming for stable training without gradient saturation or mode collapse. BID15 used squared-loss for better gradient stability, while BID23 viewed the discriminator as an energy function to allow for additional loss functions. BID21 suggested training the discriminator based on linear separability between hidden representations of real and generated samples. In this paper, methods are proposed to enhance GAN performance on complex datasets without using labels. Theoretical analyses suggest that real image distributions have supports on low dimensional manifolds. The support set is represented by connected components, which are disjoint subsets that make up the distribution's support. The paper proposes methods to improve GAN performance on label-free complex datasets by considering that real image distributions have supports on low-dimensional manifolds. These manifolds are represented by connected components, which are disjoint subsets forming the distribution's support. In high-dimensional space, each component is a low-dimensional manifold. The components are disjoint subsets of S, forming a partition of S. The terms manifold of S and connected component of S are used interchangeably. Highly distinct images in a dataset cannot be path-connected, as there is no smooth sequence transitioning between them. The support set of real-life image probability distributions must have disconnected manifolds, with distinct images lying on separate components. This implies that the support set must have as many connected components as the number of distinct modes. The generator in GANs typically uses a continuous function on a noise vector drawn from simple distributions like Gaussian or uniform, resulting in connected output sets. Result 4 states that a continuous generator, when applied to a connected set, must either suffer from mode dropping or generate unrealistic images. This is because the output set produced by the generator must be connected, but diverse distributions require supports consisting of disconnected manifolds. The existence of tunnels in the generator's output space indicates unrealistic images. When a generator is K-Lipschitz, its output gradually shifts between manifolds in the latent noise space, leading to unrealistic outputs. This wasted probability distribution on unwanted outputs can be demonstrated easily. In a scenario where the latent noise z is drawn from a 1-D region [-1, 1] with uniform probability, and sets A and B are highly distinct, the distance between subsets mapping to A and B must be at least \u03b2 K due to the Lipschitz condition. This results in a gap of at least \u03b2 K between the sets, leading to a loss of probability measure to undesired outputs. Conditional GANs, known for their ability to learn complex datasets, incorporate label information in the generator inputs to improve output quality. However, the use of one-hot label representations introduces discontinuity that enhances performance by partitioning the input space into disconnected components. This breaks the connectedness assumption of the input space, which was a key factor in the problem. It is important to note that while conditional GANs require labeled inputs, they may not be available for all datasets. Novel GAN formulations introduce trainable discontinuities in the latent noise space without requiring labeled data. Linear mappings in the input layer create N different output sets, potentially disconnected, while maintaining the standard generator processing. Training N different generators with shared parameters introduces discontinuities in the latent noise space, potentially disconnecting output sets. A classifier is used to predict which generator created a given input, modifying the GAN value function accordingly. This generic formulation can be adapted with various types of generators, discriminators, partition mappings, and classifiers. To reduce training costs, a linear mapping C is applied to the last hidden layer of the discriminator. The discriminator's abstract features are reused for partition classification by adding a new loss function to the generator and discriminator losses. This includes the cross-entropy classification loss for input images generated by a specific partition. The text discusses the classification loss for input images generated by different partitions and introduces a hyperparameter \u03b1 to control sample generation importance and diversity. A method for implementing a softer version of disconnectedness for the latent noise distribution is proposed using a mixture-of-Gaussians with trainable means and covariance matrices. Our implementation uses a mixture-of-Gaussians with trainable means and covariance matrices to create a latent noise distribution that encourages diversity. The method involves sampling from a Gaussian distribution and using a classifier to predict which Gaussian generated a sample. Experiments were conducted on various image datasets including MNIST, celebA, STL-10, and CelebRoom. CelebRoom was created to challenge GANs with diverse modes. Results were presented on a toy dataset with 8 bivariate Gaussians. The WGAN-GP architecture was used with fixed parameters for each dataset. ADAM optimizer was utilized for all experiments. Samples were compared across different setups for each dataset. Results from the mixture-of-Gaussians latent distribution method were also presented for MNIST and celebRoom. Hyperparameters from original papers were used for each architecture. The ResNet BID10 architecture with 4 residual blocks was used with original hyperparameters. Hyperparameter tuning was avoided to promote methods with high insensitivity. Training multiple partitions with Least Squares GAN BID15 formulation led to divergence. Vanilla GAN struggled to cover all Gaussians, while WGAN-GP took a long time to converge and still had samples between different means. By reducing the gradient penalty significantly, faster convergence was achieved. Vanilla GAN with partitions covers all modes and converges quickly. Samples generated with the partition classifier are of higher quality, distinct, sharper, and without spurious appendages. Despite each partition not producing a single type of digit as expected, the partition classifier's loss was close to zero after successful training, indicating learning to map distinct outputs. Labels are not the sole way to classify digits due to various combinations of style, tilt, and width. The DCGAN architecture with mixture-of-Gaussians latent distribution showed slight improvements in sample quality compared to the WGAN + Gradient Penalty baseline. Different partitions in the model generated similar types of images, with some capturing smiling women and others generating faces of men. The ResNet architecture struggled with partitions, experiencing mode collapse and producing noisy outputs. Our experiments with GAN formulations show heavy mode collapse and noisy outputs, but the issue is partially resolved with partition GAN. Specific partitions in DCGAN and ResNet architectures generate distinct modes like vehicles, oceans, animals, birds, and planes. The baseline model struggles with conflation of bedrooms and facial images, which is alleviated by our model. Our model addresses the conflation of bedrooms and facial images in the dataset, creating distinct partitions for each. We introduced theoretical analysis and proposed adding discontinuity in latent noise space to cover diverse modes of the data distribution, improving upon existing models. In future, we aim to explore hyperparameter tuning and introduce discontinuities in the generator to enhance image quality across various datasets."
}