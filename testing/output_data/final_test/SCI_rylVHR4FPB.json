{
    "title": "rylVHR4FPB",
    "content": "In this paper, Bayesian quantized networks (BQNs) are proposed as quantized neural networks with a learned posterior distribution over discrete parameters. Efficient algorithms for learning and prediction in BQNs are provided, allowing for differentiable learning and reduced variance in gradient estimation. BQNs outperform an ensemble of QNNs on classification datasets like MNIST, Fashion-MNIST, and KMNIST, achieving lower predictive errors and better-calibrated uncertainties. A Bayesian approach to deep learning treats network parameters as random variables to infer their posterior. Bayesian neural networks (BNNs) consider network parameters as random variables to infer their posterior distribution. BNNs provide well-calibrated uncertainties for predictions, crucial in scenarios like active learning and reinforcement learning. Challenges include intractable posterior computation due to network complexity and high-dimensionality, and the difficulty in performing exact predictions in BNNs. The challenges in training Bayesian neural networks (BNNs) include the intractability of hidden activations distribution and the limitations of the classic evidence lower bound (ELBO) learning objective. To address these challenges, a sampling-free method using probabilistic propagation is proposed. This method aims to deterministically learn BNNs without making simplifying assumptions or relying on sampling-based approaches. Additionally, the advantages of quantized neural networks (QNNs) in computational and memory efficiency are noted, despite facing training challenges for different reasons. In this work, Bayesian quantized networks (BQNs) are proposed as a novel approach that combines the ideas of Bayesian neural networks (BNNs) and quantized neural networks (QNNs). BQNs address challenges in training both models by using categorical distributions for model parameters, enabling sampling-free backpropagation, and leveraging efficient tensor operations for probabilistic learning. Bayesian quantized networks (BQNs) leverage tensor operations for probabilistic propagation and provide a trade-off between computational resources and model complexity. They offer a sampling-free method for obtaining deterministic QNNs and demonstrate better-calibrated uncertainty compared to state-of-the-art methods. Trained BQNs achieve comparable log-likelihood against Gaussian BNNs trained with SGVB. BQNs can compress neural networks and obtain deterministic QNNs. Mean-field approximation in BQN shows similar accuracy and log-likelihood compared to Monte-Carlo realizations. Sampling-free probabilistic propagation justifies the use of mean-field approximation in BQNs. The text discusses various methods for Bayesian neural networks, including density filtering, variational inference, probabilistic neural networks, quantized neural networks, and tensor networks. An alternative evidence lower bound (ELBO) is proposed for Bayesian neural networks to optimize the variational objective with the backpropagation algorithm. The text introduces Bayesian quantized networks (BQNs) and their equivalence to hierarchical tensor networks. It presents a sampling-free approach for learning and inference in BQNs using probabilistic propagation, achieving better-calibrated uncertainty. Fast algorithms are developed for efficient learning and prediction in BQNs. Notation is used to denote random variables and their realizations in the context of learning a neural network with model parameters to predict outputs based on inputs. The text discusses Bayesian neural networks (BNNs) and their predictive distribution Pr[y|x]. It explains the hierarchical structure of BNNs and the marginalization process over the random variable \u03b8. The predictive distribution is computed by sequentially marginalizing over \u03b8 in multiple steps. The text discusses probabilistic propagation in Bayesian neural networks, where the predictive distribution of h (l+1) given input x can be obtained from its preceding layer h (l) through a deterministic process. This process is known as Challenge in Sampling-Free Probabilistic Propagation and involves approximations at each layer due to the difficulty in finding parameterized distributions for hidden variables. In Bayesian neural networks, approximations are needed at each layer for probabilistic propagation. Variational inference minimizes KL-divergence to find a good approximation Q(\u03b8; \u03c6). Optimization relies on gradient-based methods and backpropagation to obtain partial derivatives of the objective function. The output produced by a neural network is determined by a differentiable function g(\u03c6), and the objective L(g(\u03c6)) is explicitly related to g(\u03c6). The learning problem can be solved using first-order methods like stochastic gradient descent. For classification, g(\u03c6) returns probabilities after the softmax function. Regularizers on parameters do not complicate backpropagation. Bayesian neural networks pose a challenge for standard backpropagation due to the implicit nature of the ELBO objective function. The lower bound in Theorem 3.1 makes learning in Bayesian Neural Networks amenable to backpropagation by providing an explicit function of the predictive distribution g(\u03c6). This allows for maximizing the alternative objective via gradient-based methods, even in the presence of arbitrary distributions and problem settings. In this paper, two layers are developed for classification and regression tasks, focusing on the classification case due to space limitations. The last layer involves the pre-activations of a softmax layer, with a scaling factor adjusting its scale for pairwise independence. The regression case and proofs for both layers are deferred to Appendix C. The solution to learning in Bayesian Neural Networks relies on the ability to evaluate the function analytically or lower bound it in terms of parameters. Section 3 introduces Bayesian quantized networks (BQNs) for efficient probabilistic propagation in BNNs. The iterative step in probabilistic propagation is computed with a finite sum after quantization, resulting in categorical hidden units. All distributions are represented in high-order tensors. Bayesian quantized networks (BQNs) use high-order tensors for distributions in probabilistic propagation, establishing a link with hierarchical tensor networks. BQNs overcome training difficulties of QNNs by utilizing tensor contractions, which are differentiable. This approach converts the integer programming problem of QNNs into a continuous optimization problem in Bayesian settings. The computational complexity of evaluating Equation (7) for Bayesian quantized networks (BQNs) is intractable for large networks. To address this, a principled approximation using tensor CP decomposition is proposed to reduce complexity in probabilistic propagation. This rank-1 CP decomposition factorizes the joint distributions of P and Q into products of their marginal distributions, equivalent to the mean-field approximation. This simplifies the tensor contraction in (7) to a standard Tucker contraction. In our implementation, parameters are stored in log-space for a practical model. The approximate propagation reduces computational complexity from O(D IJK) to O(JD E), linear in the number of output units J. Faster algorithms are devised for layers with a fan-in number E greater than a small constant to avoid exponential complexity growth. In this part, faster algorithms are devised to lower the computational complexity for different fan-in layers. For small fan-in layers, tensor contraction is used with complexity O(JD E). Medium fan-in layers utilize discrete Fourier transform with complexity O(JE 2 D log(ED)). Large fan-in layers employ Lyapunov Central Limit Theorem to reduce complexity. Fourier transform is computationally expensive, so a faster algorithm based on the Lyapunov central limit theorem is derived to reduce complexity to O(JED). Different layers use CLT, DFT, or tensor contraction based on fan-in numbers. BQNs are tested on various datasets with MLP and CNN models, with image augmentation in training but not in testing. In experiments, quantized neural networks with binary weights and activations are considered. Models are trained using ADAM optimizer for 100 epochs (300 for CIFAR10) with batch size 100 and initial learning rate 10^-2. Performance of BQNs is compared against E-QNN, showing NLL and 0-1 test error averages over 10 runs. Training objective of BQNs allows for customized levels of quantization. The training objective of Bayesian Quantized Neural Networks (BQNs) introduces a regularization coefficient \u03bb to customize uncertainty levels in learned models. BQNs are compared against the baseline E-QNN, which is a Bootstrap ensemble of quantized neural networks. The evaluation of BQNs emphasizes the importance of assessing uncertainty in decision making beyond just 0-1 test error. The training objective of Bayesian Quantized Neural Networks (BQNs) introduces a regularization coefficient \u03bb to customize uncertainty levels in learned models. BQNs are compared against the baseline E-QNN, which is a Bootstrap ensemble of quantized neural networks. The evaluation of BQNs emphasizes the importance of assessing uncertainty in decision making beyond just 0-1 test error. In the experiments, the negative log-likelihood (NLL) is used to measure predictive performance. Three modes are used to evaluate the model: analytic inference (AI), Monte Carlo (MC) sampling, and Maximum a Posterior (MAP) estimation. Analytic inference involves integrating over the posterior to obtain the predictive distribution, while MC sampling involves drawing sets of model parameters independently from the posterior and performing forward propagation. The training objective of Bayesian Quantized Neural Networks (BQNs) introduces a regularization coefficient \u03bb to customize uncertainty levels in learned models. BQNs are compared against the baseline E-QNN, which is a Bootstrap ensemble of quantized neural networks. The evaluation of BQNs emphasizes the importance of assessing uncertainty in decision making beyond just 0-1 test error. In the experiments, the negative log-likelihood (NLL) is used to measure predictive performance. Three modes are used to evaluate the model: analytic inference (AI), Monte Carlo (MC) sampling, and Maximum a Posterior (MAP) estimation. MAP estimation is similar to MC sampling, except that only one set of model parameters \u03b8 is obtained \u03b8 = arg max \u03b8 Q(\u03b8). We will exhibit our model's ability to compress a Bayesian neural network by comparing MAP estimation of our BQN with non-Bayesian QNN. Expressive Power and Uncertainty Calibration in BQNs. We report the performance via all evaluations of our BQN models against the Ensemble-QNN in Table 1 and Figure 1. Compared to E-QNNs, our BQNs have significantly lower NLL and smaller predictive error (except for Fashion-MNIST with architecture CNN). As we can observe in Figure 1, BQNs impressively achieve comparable NLL. The evaluation of Mean-field Approximation and Tightness of the Alternative ELBO in Bayesian Quantized Neural Networks (BQNs) shows small gaps between analytic inference and Monte Carlo sampling, justifying the approximations used in BQNs. In Bayesian Quantized Neural Networks (BQNs), varying the regularization coefficient \u03bb in the learning objective affects the mean-field approximation and relaxation of the original ELBO. When \u03bb = 0, models become deterministic during training, leading to a gap between analytic inference and MC-sampling. Increasing \u03bb results in a slightly less accurate mean-field approximation with higher learned uncertainty in the model. The study compares Bayesian Model compression through direct training of Ensemble-QNN with Monte-Carlo sampling on BQNs, showing small gaps between analytic inference and Monte Carlo sampling. In Bayesian Quantized Neural Networks (BQNs), models are compressed using a sampling-free, backpropagation-compatible, variational-inference-based approach. The compressed models outperform their counterparts in Negative Log-Likelihood (NLL) due to improved calibration of uncertainty and stability in training. Our approach focuses on efficient inference in Bayesian Quantized Neural Networks (BQNs) through algorithms that scale to large problems. Evaluation using Monte-Carlo sampling demonstrates the ability to learn proper posterior distributions on QNNs. Additionally, our method can be applied to learning ensemble QNNs by sampling from the posterior distribution. However, there are limitations to this approach, such as the need to derive Bayes' rule case by case and the incompatibility with modern optimization methods like SGD or ADAM due to analytical optimization for each data point. Sampling-based Variational Inference (SVI) offers an alternative by formulating and solving an optimization problem. Sampling-based Variational Inference (SVI) utilizes Stochastic Gradient Variational Bayes (SGVB) to approximate model parameters through multiple samples, reducing computational costs. However, drawbacks include high variance from repeated sampling and computational expense in learning and prediction phases. Expensive learning and prediction phases; g n (\u03c6) differentiable w.r.t. \u03c6, f n (\u03b8) may not be differentiable w.r.t. \u03b8. Quantized neural networks use straight through estimator for backpropagation. Model could be stochastic. Two classes of quantized networks: partially quantized (only weights) and fully quantized (weights and hidden units). In contrast to previous works on quantized neural networks, this paper focuses on Bayesian learning of fully quantized networks. Optimization of these networks requires specialized loss functions, learning schedules, and initialization methods. Unlike some approaches that involve pre-training with continuous-valued networks, this paper considers learning from scratch with uniform initialization. Tensor networks (TNs) are used in numerical analysis, quantum physics, and machine learning to model interactions among multi-dimensional objects. Various tensorial neural networks (TNNs) have been proposed to reduce the size of neural networks. Recent research connects hierarchical Bayesian models with hierarchical TNs, advancing the understanding of probabilistic graphical models and TNs. The curr_chunk discusses the problem settings of general Bayesian models and Bayesian neural networks for supervised learning, illustrated using graphical models. It explains the joint distribution of model parameters, observed dataset, and unseen data points. The posterior predictive distribution is expanded by approximating the posterior distribution with a parameterized distribution. The curr_chunk discusses approximating the posterior distribution with a parameterized distribution in a hierarchical Bayesian model for Bayesian neural networks. It partitions model parameters into statistically independent layers and satisfies the Markov property. The curr_chunk discusses probabilistic propagation in Bayesian neural networks, showing how the output depends on the input through previous layers. The proof involves alternative evidence lower bounds and analytic forms, utilizing Jensen's inequality and chain rule for differentiation. The curr_chunk explains the backpropagation rule for Bayesian neural networks with softmax layers, deriving it from Equation (37) and using iterative chain rule. It also introduces a method based on Taylor's expansion to approximate softmax layers without Monte Carlo sampling. The curr_chunk discusses the scaling factor \u03c6 in Bayesian neural networks with softmax layers, providing an upper bound for L n (\u03c6) using analytic forms and derivatives. It also mentions updating the scale parameter s along with other parameters and using probabilistic backpropagation to obtain gradients. Additionally, it touches on prediction with softmax layers in Bayesian networks. The curr_chunk discusses computing the predictive distribution of y in Bayesian neural networks using Monte Carlo sampling or Taylor's expansion for an economical estimate. It highlights the drawbacks of Monte Carlo method and the simplification of Taylor's series for prediction. The curr_chunk introduces an alternative evidence lower bound (ELBO) for Bayesian neural networks with Gaussian output layers and derives gradients for backpropagation. It focuses on approximating the first and second-order derivatives of c(h) to incorporate variance into the approximation. The curr_chunk discusses deriving gradients for backpropagation in Bayesian neural networks with Gaussian output layers. It shows that central moments of the output distribution can be computed easily given learned parameters. Theorem C.2 provides an analytic form for L n (\u03c6) for regression, assuming pairwise independent hidden units. The gradient of L n (\u03c6) can be computed efficiently. The gradient of L n (\u03c6) can be backpropagated through the last layer by computing derivatives w.r.t. \u00b5 and \u03bd. Parameters {w, s} can be updated with their gradients. Predictive distribution Pr[y|x] for output y given input x can be computed using central moments. Skewness and kurtosis of the prediction y can also be easily computed. In this section, fast algorithms for probabilistic propagation are presented, divided into three parts based on fan-in numbers E. For small E, tensor contraction in Equation (8) is applicable, including shortcut layers with skip connections and depth-wise layers. Shortcut layers involve adding two previous layers, while depth-wise layers include dropout, nonlinear, and element-wise product layers. The time complexity for shortcut and depth-wise layers is O(JD^2) with medium fan-in number E. Pooling layers have a special structure allowing for faster computation. Max pooling picks the maximum value from inputs, while probabilistic pooling selects values based on a categorical distribution. Both have complexities of O(ID). Average pooling and depth-wise convolutional layers require additions of a medium number of inputs. The convolution theorem for discrete random variables and the use of discrete Fourier transform (DFT) with fast Fourier transform (FFT) can accelerate additive computation. Backpropagation rules are derived for gradient-based learning compatibility. The element-wise product of Fourier transforms is shown, and the convolution of probability vectors is explained using standard convolution notation. The characteristic functions C, C 1, and C 2 are defined as the discrete Fourier transform of probability vectors P, P 1, and P 2 respectively. The convolution of probability vectors is shown to correspond to the element-wise product of their characteristic functions. The summation of discrete random variables corresponds to the element-wise product of their characteristic functions. By using FFT for computing all DFT, the computational complexity can be reduced to O(ER log R) = O(E^2D log(ED)). Backpropagation rules are derived by breaking down the computation into three steps and projecting the gradients back to the real domain. Lyapunov CLT accelerates probabilistic propagation in linear layers, with backpropagation rules derived for the algorithm. Linear layers, including fully-connected and convolutional layers, are crucial in neural networks. Linear layers are parameterized by vectors \u03b8(l) and map inputs u to activations v. If inputs and parameters have bounded variance, the distribution can be approximated efficiently. The distribution of v j converges to a Gaussian distribution with mean and variance. Performance of different networks in terms of RMSE is evaluated for CIFAR10 using a smaller version of VGG. Results for BQN, PBP, EBP, and NPN are compared."
}