{
    "title": "SkeXL0NKwH",
    "content": "The recent success of neural networks for decision tasks has led to a focus on training at the edge, despite challenges like low weight update density and memory limitations in non-volatile memory systems. A low-rank training scheme is proposed to address these challenges while maintaining efficiency. The technique demonstrates improved performance on a convolutional neural network for adaptation problems, outperforming standard SGD in accuracy and weight updates. Deep neural networks excel in inference tasks, with models now deployed on edge devices for privacy, reduced bandwidth, and lower latency. However, training primarily occurs in the cloud, limiting privacy benefits and adaptability to evolving data distributions. Federated learning and on-device training efforts aim to address these challenges. On-device model customization and on-chip training using emerging non-volatile memories like RRAM show promise for energy-efficient inference. However, on-chip training of larger models is limited by memory size and compute power. RRAM writes consume more energy than reads, and its endurance is around 10^6 writes, shortening device lifespan. The paper presents an online training scheme for NVM memories to enable next-generation edge devices. It introduces the Streaming Kronecker Sum Approximation (SKS) algorithm to address low write density and low auxiliary memory challenges. Additionally, techniques like \"gradient max-norm\" and \"streaming batch norm\" are proposed for online training. The study includes adaptation experiments to showcase the benefits of the approach for efficient training on resistive arrays. The efficiency of training in NVM memories for edge devices is addressed through various techniques such as stochastic weight updates and online Manhattan rule updating. New memory structures have been proposed to improve training efficiency, and the number of writes has been quantified in chip-in-the-loop training. Distributed gradient descent is used for distributed training in data centers, where the model is replicated onto multiple compute nodes for gradient computation and weight updates. In distributed training systems, compressed gradient techniques have been developed to address communication bandwidth limitations. Gradients are accumulated over multiple training iterations on each compute node, and only gradients exceeding a threshold are communicated back to the central node. This method helps reduce the number of weight updates in on-chip training with NVM, but the gradient accumulator requires as much memory as the weights themselves, negating the density benefits of NVM. The backpropagation through time (TBPTT) training algorithm has led to the development of various techniques to address memory issues, such as Real-Time Recurrent Learning (RTRL), Unbiased Online Recurrent Optimization (UORO), Kronecker Factored RTRL (KF-RTRL), and Optimal Kronecker Sums (OK). These techniques focus on approximating weight gradients with a low-rank representation involving fewer sums, particularly in deep learning systems where weight matrix-activation vector products are crucial. Recurrent neural networks and convolutional layers can be interpreted as matrix-vector products. This allows for adapting techniques like \"Low-Rank Training\" to different network architectures. Memory reduction can be applied across training samples in feedforward neural networks, but may not be advantageous in traditional training platforms with sufficient memory for batch processing. Low-rank training benefits from the challenges of proposed NVM devices, such as low write density. NVM is the densest form of memory and should be used to store memory-intensive weights, minimizing the number of writes. Low-rank training techniques allow for decoupling auxiliary memory requirements from batch size, enabling a trade-off between write density and memory usage in NVM devices. Low-rank training techniques enable a trade-off between write density and memory usage in NVM devices by decoupling auxiliary memory requirements from batch size. The low-rank representation allows for larger bitwidth usage and potential gradient accumulation, enhancing training efficiency. The method involves iteratively updating rank-r matrices to approximate gradient accumulation over samples. Further discussion includes computing rankReduce to convert rank-q systems to rank-r. The OK algorithm 2 proposes a minimum variance unbiased estimator for rankReduce, converting rank q to rank r by efficiently computing the SVD of a Kronecker sum and splitting the singular value matrix \u03a3 into two rank-r matrices. This method aims to address computational infeasibility and bias in selecting top components of the SVD of X. The rankReduce algorithm efficiently computes the SVD of LR, making it feasible on small devices. It reduces the problem of finding a rank-r minimum variance unbiased estimator of LR to finding one for \u03a3. The optimal approximator for \u03a3 involves keeping the largest singular values and mixing the smaller ones within a submatrix. The algorithm efficiently computes the SVD of LR, reducing the problem to finding a rank-r minimum variance unbiased estimator of \u03a3. The main optimization involves avoiding recomputing the QR factorization of L and R at every step, using orthogonal matrices QL, QR, and weightings cx. This optimization is detailed in the Streaming Kronecker Sum Approximation (SKS) algorithm in the appendix. The algorithm efficiently computes the SVD of LR by avoiding recomputing the QR factorization of L and R at every step. It uses orthogonal matrices QL, QR, and weightings cx to update matrices efficiently. The optimization involves maintaining orthogonality in QL, QR, and orthogonalizing matrices efficiently. The next section discusses efficiently orthogonalizing \u03a3 L using Householder matrices. The OK/SKS methods require O((n i + n o + q)q^2) operations per sample. The low rank method is more efficient than minibatch SGD in terms of memory and computational cost. SKS introduces variance into gradient estimates, affecting online convex convergence. Analyzing strongly convex loss landscapes, it is shown that with inverse squareroot learning rate and specific constraints, online regret is sublinear in the number of steps. Convex convergence is likely under certain conditions, either in the biased, zero-variance case or with specific criteria met. In experiments validating linear regression with static input batch X and target Y, conditions for fast convergence are discussed. Gaussian noise affects convergence, with SVD and SKS cases tested with rank r = 10. SKS introduces too much variance, impacting convergence. The NN is quantized in both forward and backward directions with uniform power-of-2 quantization. Weights, biases, activations, and gradients are quantized to specific ranges. Gradient Max-Norming is used in training, contrasting with high bitwidth or floating point accumulators. See Appendix D for more details on quantization. In training, Gradient Max-Norming is proposed as an alternative to memory-intensive methods like Adam, by dividing each gradient tensor by its maximum absolute value to stabilize gradients. Additionally, Streaming Batch Normalization is suggested for improving training performance, especially in an online setting where samples are received one-at-a-time. In the online setting, a streaming batch norm using moving average statistics is proposed for training. Experiments are conducted on a CNN with convolution and fully-connected layers using offline and online datasets. Different training techniques are compared in various environments to model real-life scenarios. The study compares training techniques in offline and online settings, deploying models to edge devices for supervised predictions. Results are presented for scenarios involving input image distribution shifts and NVM memory degradation simulations. For different training scenarios, various techniques were compared: pure quantized inference, bias-only training, standard SGD training, SKS training, and SKS training with max-normed gradients. SGD training updates parameters online, while SKS training shows significant improvement, especially in weight drift cases. SKS outperforms SGD by three orders of magnitude in the worst-case scenario. The study compared different training techniques for weight updates, with SKS/max-norm performing best in accuracy. Experiments on ImageNet with ResNet-34 showed challenges in updating low-rank approximations at each pixel due to algorithm limitations and batch size variance. Focus was shifted to training final layer weights instead. The study compared different training techniques for weight updates, with SKS/max-norm performing best in accuracy. The convolution layers generate feature vectors for ImageNet training images, quantized and fed to a neural network. The unbiased SKS showed the strongest recovery accuracies compared to biased SVD and other methods. SKS algorithm demonstrated potential for solving challenges in online training on NVM-based edge devices, allowing for larger effective batch sizes and lower write densities. It can handle weight quantization constraints and may have better convergence properties compared to other methods. SKS algorithm shows promise for online training on NVM-based edge devices, offering larger batch sizes and lower write densities. It may outperform SGD with fewer updates and could be applied to a wider range of problems. The algorithm aims to bound regret in convex settings using noisy SKS estimates, with potential applications in federated learning for communication minimization. The proof in Zinkevich (2003) defines a convex feasible set F for weight tensors, assuming it is bounded with a maximum distance D between elements. Batch t of B samples corresponds to a strongly convex loss landscape ft(wt), with regret defined as the difference from the optimal offline minimizer w*. Gradients during SGD are bounded by G, and the minimum variance is s according to Theorem A.4 from Benzing et al. (2019). Total variance is calculated assuming uncorrelated errors between samples. The proof in Zinkevich (2003) establishes a convex feasible set F for weight tensors, assuming boundedness with a maximum distance D between elements. Batch t of B samples corresponds to a strongly convex loss landscape ft(wt), with regret defined as the difference from the optimal offline minimizer w*. Gradients during SGD are bounded by G, and the minimum variance is s according to Theorem A.4 from Benzing et al. (2019). Total variance is calculated assuming uncorrelated errors between samples. For sublinear-regret convergence, an approximately sufficient condition is given in Equation (19), where the constraints become more difficult to maintain as wt approaches w*. The behavior of the left-hand side of (19) is expected to decrease during training as wt approaches w*. The text discusses methods to improve convergence during training. One approach is to reduce batch size or decrease \u03c3 q to spread weight updates across more singular components. Another method is to focus on the lower bound on curvature of convex loss functions. Weight regularization can potentially increase this lower bound. Weight regularization can potentially increase the lower bound on curvature of convex loss functions, leading to faster convergence during training. This approach focuses on the importance of high-curvature directions for loss minimization. Additionally, quantization noise can introduce additional errors, which can be mitigated by adjusting weight LSB. In neural network layers, quantization noise has variance \u2206 2 /12. Dense layers transform input to output using non-linear activation functions. Gradients for weight parameters are found using per-sample Kronecker sum updates. Convolutional layers transform input feature maps through 2D convolutions and non-linear activation functions. Convolutions can be interpreted as matrix multiplications. The im2col operation converts input feature maps into matrices for convolution operations. Weight gradients can be represented as Kronecker sum updates. Low rank training technique is crucial for convolution layers due to memory efficiency. The importance of low rank training technique for convolution layers is emphasized, especially for compute-constrained edge devices. Clever dataflow strategies can help reduce intermediate activation storage during backpropagation. Training experiments are conducted with quantization in the loop, using fixed point arithmetic for real device operations. In low rank training for convolution layers, quantization is used with fixed clipping ranges and specified bitwidths for weight, bias, activation, and gradient quantization. Backpropagation follows standard rules with quantizer gradients quantized after passing through ReLU derivative and before feeding back into network parameters. This is crucial for training on edge devices. The SKS method is used for accumulating values before computing approximate \u2206W in low rank training for convolution layers. It collects information in two low rank matrices quantized to 16 bits with clipping ranges dynamically determined. While SKS accumulates for B samples, updates to W are reduced by a factor of B, but b is updated at every sample due to its small size. This approach prevents weight gradients from consistently quantizing to 0 and only applies updates when necessary. To prevent weight gradients from consistently quantizing to 0, updates are only applied when a minimum update density of 0.01 is achieved. Accumulating samples in matrices with higher bitwidths is done until an update occurs, with the \"effective batch size\" being a multiple of B. The learning rate is increased accordingly, with square-root scaling found to work better than linear scaling. Gradient magnitudes in weight tensors during training show a wide dynamic range, making quantization challenging. One potential method to address the wide dynamic range of gradient magnitudes in weight tensors is to scale tensors so that their max absolute element is 1. This approach, known as max-norming, aims to prevent noise amplification during quiet regions by normalizing tensors based on the maximum element and a moving average. Max-norming is a technique that scales tensors to have a max absolute element of 1, aiming to prevent noise amplification during quiet regions. It uses a moving average of the max element with parameters \u03b2 = 0.999 and \u03b5 = 10 \u22124, and modifies its internal state for a given input x to return x norm. This is different from standard batch normalization, which normalizes a tensor X along some axes and applies a trainable affine transformation using mean and standard deviation statistics of a minibatch. In streaming batch norm, adjustments are made to batch statistics in online training to account for noisy estimates of mean and variance. An exponential moving average is used to track statistics instead of a true average, with a weighting of 1/B on the current sample. In streaming batch norm, adjustments are made to batch statistics in online training to account for noisy estimates of mean and variance. Specifically, a dataset is constructed with offline and online training sets using the MNIST dataset. Elastic transforms are applied to augment the training samples for better performance. The 100k online training samples are randomly drawn with replacement, leading to data leakage. Experiments compare SKS to standard SGD to show improvement is not due to overfitting. A \"distribution shift\" dataset is generated from the online training set with unique augmentations. Class distribution clustering biases training samples to have similar indices based on classes. Spatial transforms rotate, scale, and shift images by random amounts, while background gradients scale image contrast and apply black-white gradients. White noise is added to each pixel. Weight drift is examined through analog and digital methods, introducing Gaussian noise and binary random flips to weights at intervals. In experiments comparing standard SGD with the SKS approach, learning rates were optimized to achieve optimal accuracies around 0.01. Different batch sizes and hyperparameters were used for the SKS method, with rank-4 SKS being utilized. In experiments, rank-4 SKS with batch sizes of 10 or 100 are used. Accuracy is reported averaged over the last 500 samples from a 10k portion of the online training set. SKS rank and weight bitwidth are swept for gradient max-norming, showing improved training accuracy with higher values. In dense NVM applications, higher bitwidths may allow for reductions in SKS rank and auxiliary memory requirements. Biased and unbiased versions of SKS are compared, with accuracy generally improving from biased to unbiased SKS. The choice between biased and unbiased SKS has a minor impact on accuracy in the max-norm case. Lower variance may be increasingly important with more accumulated samples, favoring biased SKS for convolutions. The study compares biased and unbiased versions of SKS in dense NVM applications, finding that biased SKS is preferred for convolutions due to lower variance with more accumulated samples. Weight training and streaming batch norm are crucial for accuracy, with bias-only training showing a significant accuracy decrease. High condition numbers in the SVD of a small matrix C may provide useless update information, especially in the presence of quantization. In dense NVM applications, biased SKS is preferred for convolutions due to lower variance with more accumulated samples. Weight training and streaming batch norm are crucial for accuracy, with bias-only training showing a significant accuracy decrease. High condition numbers in the SVD of a small matrix C may provide useless update information, especially in the presence of L, R quantization. A rough heuristic of \u03ba(C) \u2248 C 1,1 /C q,q can reduce computation load without significantly impacting accuracy. A threshold \u03ba th = 10 8 does not universally improve on \u03ba th = 100, despite being slower to compute. Selected ablations in Table 3 show varying impacts on accuracy. In dense NVM applications, biased SKS is preferred for convolutions due to lower variance with more accumulated samples. Weight training and streaming batch norm are crucial for accuracy, with bias-only training showing a significant accuracy decrease. \u03ba th = 10 8 instead of 100 does not universally improve accuracy, despite being slower to compute."
}