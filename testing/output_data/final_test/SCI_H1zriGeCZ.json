{
    "title": "H1zriGeCZ",
    "content": "Our algorithm for hyperparameter optimization, inspired by Boolean function analysis, is designed for high-dimensional scenarios like training neural networks with many hyperparameters. It utilizes compressed sensing techniques for orthogonal polynomials and only requires uniform sampling, making it easily parallelizable. Experimental results on Cifar-10 data show that our algorithm outperforms state-of-the-art tools like Hyperband and Spearmint, achieving significantly improved solutions and faster overall running times. Our method improves sample-complexity bounds for learning decision trees while matching state-of-the-art running time bounds. Large scale machine learning systems involve numerous parameters for users to adjust, such as architecture, network depth, connectivity, and optimization algorithms. The naive approach involves searching through all parameter assignments. Hyperparameter optimization (HPO) is crucial for automatic tuning of parameters in machine learning. While gradient descent is used for continuous hyperparameters, discrete parameters like architecture choice are more challenging. The goal is to find the best hyperparameter settings to minimize test error. Hyperparameter optimization (HPO) involves encoding choices as binary numbers to approximate the minimizer x * = arg min x\u2208{0,1} n f (x). HPO is crucial for tuning parameters in machine learning, especially for discrete parameters like architecture choice. The goal is to minimize test error by finding the best hyperparameter settings. Oracle model assumes expensive evaluation of f, parallelism reduces optimization time, and structured function f is addressed by Bayesian optimization BID32. The paper introduces a new spectral approach to hyperparameter optimization by assuming that the function can be approximated by a sparse and low degree polynomial in the Fourier basis. This allows for approximate minimization of the function over the boolean hypercube with function evaluations only linear in sparsity and can be carried out in parallel. The paper introduces a new spectral method called Harmonica for hyperparameter optimization. It offers sample-efficient recovery for sparse polynomials and is easy to implement on parallel architectures. The method shows improvements in accuracy, sample complexity, and running time for deep neural net training experiments compared to other optimization techniques. Improved bounds on sample complexity of learning decision trees over variables under uniform distribution. The literature on discrete-domain HPO divided into probabilistic and decision-theoretic methods. Grid search becomes prohibitive as hyperparameter grows. Gradient-based methods not considered. Neural network structural search based on reinforcement learning requires many samples. Probabilistic methods and Bayesian approaches are discussed. Bayesian optimization algorithms tune hyperparameters by assuming a prior distribution of the loss function and updating it based on new observations. The approach balances exploration and exploitation to improve results. Decision-theoretic methods involve random sampling of parameters to find the best evaluation, which is easy to implement and parallelize. The Successive Halving (SH) algorithm, based on adaptive resource allocation techniques from the multi-armed bandit literature, is easy to implement and parallelize. Hyperband improves SH by automatically tuning hyperparameters. Previous work on learning decision trees used random sampling to estimate low-degree Fourier coefficients accurately. This approach can be extended to learn functions efficiently. The approach taken by BID21 and BID27 extends to learning functions that are approximately sparse and low-degree, leading to the first decision tree learning algorithm with polynomial sample complexity handling adversarial noise. For learning exactly k-sparse Boolean functions, O(nk log n) uniformly random samples suffice, with the best algorithm requiring time 2 \u2126(n/ log n). The BID8 algorithm for learning parities with noise requires time 2 \u2126(n/ log n) and is based on the BID4 algorithm. The hyperparameter optimization problem involves minimizing a discrete, real-valued function f : {\u22121, 1} n \u2192 [\u22121, 1]. Function evaluation in this context is expensive, corresponding to training a deep neural net. Computation not involving function evaluation is considered less expensive, with runtimes that are exponential in n deemed costly. Refer to BID28 for a detailed treatment of Fourier analysis of Boolean functions. A Random Orthonormal Family of functions is defined with respect to a probability distribution on a domain. The family is considered K-bounded if the absolute value of each function in the family is less than or equal to K. An example is the class of parity functions with respect to the uniform distribution on a set of variables. The Fourier basis is a random orthonormal family for Boolean functions with respect to the uniform distribution on {\u22121, 1}n. Functions can be uniquely represented in this basis with Fourier coefficients. Results apply to any orthogonal family of polynomials, such as Hermite orthogonal polynomials with multivariate spherical Gaussian distributions. Low-degree, approximately sparse functions are defined. Low-degree, approximately sparse functions are defined as functions with at most s nonzero entries in their polynomial expansion. The class of functions with bounded L1 norm is more general than sparse functions. In sparse recovery, a learner tries to recover a sparse vector x from an observation vector y, where x has at most s nonzero entries. Compressed sensing involves recovering a sparse signal x from an observation vector y = Ax + e, where A is the observation matrix and e is noise. The recovery process includes solving a convex optimization problem with 1 minimization. Extensions to traditional sparse recovery have been explored, such as considering sparse recovery with evaluated measurements. Sparse recovery in compressed sensing involves recovering a sparse signal x from an observation vector y = Ax + e. BID29 addresses sparse recovery with random orthonormal families, providing a theorem for recovering sparse vectors x using a mathematical program. Recent work by BID5 and BID13 has improved the lower bound for m in the recovery process. The spectral algorithm for hyperparameter optimization is a key component in this context. The spectral algorithm for hyperparameter optimization, an extension of sparse recovery to polynomials, is outlined in Algorithm 1. The algorithm guarantees theoretical results for learning from the uniform distribution. Extensions to improve performance are discussed in the next section. The spectral algorithm for hyperparameter optimization, outlined in Algorithm 1, guarantees theoretical results for learning from the uniform distribution using a 1-bounded orthonormal polynomial basis. The algorithm returns x such that x \u2208 arg min f(x) in time n O(d) with sample complexity T =\u00d5(s \u00b7 d log n). The main recovery properties of Procedure 2 support these results, including noisy recovery in adversarial or agnostic noise settings. The noisy recovery lemma forms the basis for an enhanced algorithm and learning-theoretic results on decision trees. It guarantees recovery of a polynomial with squared-error at most \u03b5 + O(\u03b3) under certain conditions, but does not ensure recovery of the global optimum. However, it can prove recovery of optimality for functions computed by sparse, low-degree polynomials. Lemma 7 has applications for learning well-studied function classes in the PAC model. It provides a quasi-polynomial time algorithm for learning decision trees with polynomial sample complexity. Decision trees of size s on n variables are learnable with respect to the uniform distribution in time n O(log(s/\u03b5)) and sample complexity m =\u00d5(s 2 /\u03b5 \u00b7 log n). If labels are corrupted by noise vector v, the output classifier will have squared-error at most \u03b5 + O(\u03b3). Comparing with the \"Low-Degree\" Algorithm, prior work used a different approach for learning decision trees. Our approach uses compressed sensing algorithms to estimate coefficients for learning decision trees, providing improved results compared to the \"low-degree\" algorithm. For real-valued decision trees, our algorithm has a sample complexity of \u00d5(B^2s^2/\u03b5 \u00b7 log n) and is noise-tolerant, while the low-degree algorithm has a higher sample complexity of \u00d5(B^4s^2/\u03b5^2 \u00b7 log n) and lacks noise tolerance. Our approach utilizes compressed sensing algorithms to estimate coefficients for learning decision trees, offering improved results compared to the low-degree algorithm. The full algorithm involves iteratively using Procedure 2 to estimate influential hyperparameters and their optimal values, enhancing performance significantly. The space of minimizing assignments to a multivariate polynomial is non-convex, requiring multiple stages of Algorithm 1 and a base hyperparameter optimizer. The Harmonica-q algorithm involves averaging the best minimizers of subsets of hyperparameters during each stage. It utilizes the PSR subroutine with a restriction size t as a tie-breaking rule. The algorithm iteratively estimates influential hyperparameters and their optimal values to enhance performance significantly. The Harmonica-q algorithm utilizes the PSR subroutine to obtain the best minimizers of subsets of hyperparameters. It compares Harmonica with other algorithms like Spearmint, Hyperband, and Random Search, showing Random Search 2x as a competitive benchmark. The first experiment involves training a residual network on the Cifar-10 dataset with 39 binary hyperparameters. The study implemented a parallel version of Hyperband and SH in Lua, considering hyperparameters detailed in Table 1. Dummy variables were included to increase the task complexity, with Harmonica and Spearmint being sensitive to them. Spearmint was run without dummy variables for a fair comparison. Handtuning strategy involves tuning on a small network and applying the knowledge to a larger network. Harmonica can also utilize this strategy by selecting important features. During feature selection stages, Harmonica tunes an 8 layer neural network with 30 training epochs, extracting 5 important features from 300 samples at each stage. The algorithm then runs SH or Random Search on a 56 layer neural network for 160 epochs. Three versions of Harmonica were tested, each with different stages. The top 10 test error results and running times are compared in Figure 2. Harmonica algorithms show superior performance in terms of test error and scalability compared to other algorithms like Spearmint and Hyperband. Harmonica-1 is faster than Spearmint, Hyperband, and Random Search, while achieving better results. Harmonica-2 takes slightly more time but finds better results. Harmonica-3 outperforms human-tuned parameters with a lower test error rate and uses minimal GPU days. Harmonica algorithms demonstrate superior performance in test error and scalability compared to Spearmint and Hyperband. Harmonica-1 offers noiseless and noisy recovery guarantees, validated experimentally. The average test error drops significantly after selecting 5 important features in stage 1. However, the improvement becomes less significant in stage 3 as selected features have mild contributions. Harmonica has six hyperparameters to set, including the number of stages and features selected per stage. The Harmonica algorithm has six hyperparameters to set, including the number of stages, regularizer for Lasso, features selected per stage, base algorithm, small network configuration, and samples per stage. Different versions of Harmonica were tested with varying stages and base algorithms, showing improved results compared to SH. Most variants of the Harmonica algorithm outperform SH in terms of results and running time. Running SH for longer yields more stable solutions with less variance in test error. Lasso parameters have a stable range for regularization term \u03bb and number of samples, ensuring consistent feature selection outcomes. Setting the degree to three is optimal as higher degrees do not find important features. Lasso can be efficiently solved in less than 5 minutes. The optimization time of Harmonica is faster than Spearmint for a hierarchically bounded function. Harmonica's estimation error is linear in the function's noise level. The Chebyshev inequality is used to analyze a multidimensional random vector. A bounded function can be written as a polynomial with specific coefficients. The function g is the sum of the remaining terms not included in h, which is a sparse polynomial. By drawing random examples and forming a matrix A, we aim to recover the sparse coefficients of h. With a chosen number of examples, we can recover x with a small error bound. The function g is a linear combination of orthonormal monomials, and the entries e i = g(z i ) are independent random variables. The variance of g(z i ) is at most \u03b5/2, and the covariance matrix V of the vector e is calculated as E[ee ]. The covariance matrix V of the vector e is a diagonal matrix with every diagonal entry at most \u03b5/2. By applying Fact 10, we have that e^2 \u2264 \u221a\u03b5m with probability at least 1/2. Choosing m = \u00d5(s^2/\u03b5 \u00b7 log n^d) from Theorem 5 completes the proof. The probability can be boosted to any constant probability with a constant factor loss in sample complexity. There are at most N = nd polynomials \u03c8S with |S| \u2264 d, and an m \u00d7 N matrix A can be constructed from labeled examples drawn independently from D. This matrix can be used to find an s-sparse vector x such that Ax = y, where y is the output of the function f. The orthonormal polynomial basis for Boolean functions with respect to the uniform distribution on {\u22121, 1} n is the class of parity functions {\u03c7 S }. Corollary 8 follows by applying Lemma 7 and known structural facts about decision trees. A decision tree of size s is (\u03b5, log(s/\u03b5))-concentrated with L 1 norm bounded by s. For any function f with L 1 norm bounded by s, there exists an s 2 /\u03b5 sparse function g such that E[(f \u2212 g) 2 ] \u2264 \u03b5, leading to noise tolerance. Harmonica can find sparse functions efficiently with O(s log s) samples. In real-world applications like neural network tuning, assuming sparsity at each stage is reasonable. Harmonica's optimization time is fast due to using the Lasso algorithm after each stage. Hyperband and SH require more samples to cover optimal configurations even for sparse functions. Bayesian optimization is efficient but has cubic running time, limiting its applicability for large evaluations/high dimensionality. Harmonica, Hyperband, SH, and Random Search have parallel implementations, while Bayesian optimization is inherently serial and difficult to run in parallel. Previous works have explored parallel variants, but speed ups do not grow linearly with the number of machines. Harmonica can extract important features with weights in each stage, automatically sorting them by importance. Different hyperparameter options are used, with binary variables representing variables with multiple options. Selected important features and their weights are shown in the first 3 stages. In the first 3 stages of TAB1, monomials of variables with degree at most 3 are considered, excluding the 4th stage with no features of nonzero weights. Smart choices on important options are made based on BID18, with no dummy or irrelevant variables selected. Harmonica runs on a small network to extract important features before fine-tuning on a larger network, leading to significantly better solutions compared to other algorithms. Based on our simulation, random-search-based techniques can find configurations with low test error on small networks, but these configurations do not generalize well to large networks. In contrast, important features in the Fourier space do generalize. To test this, we spent 7 GPU days finding the top 10 configurations for an 8 layer network from 300 random selections, and applied these configurations to the Cifar-10 dataset. The study compared the performance of random search techniques on small and large networks. Despite finding low test error configurations on small networks, they did not generalize well to large networks. Harmonica was able to extract important features from small networks, which was not supported by Hyperband. Another experiment with Harmonica on a hierarchically bounded function showed optimization time comparison with Spearmint. Harmonica is significantly faster than Spearmint in estimating hidden functions with error proportional to noise levels. The synthetic function h(x) has three stages with sparse vectors containing weight pairs. Each stage leads to the next sparse function selection."
}