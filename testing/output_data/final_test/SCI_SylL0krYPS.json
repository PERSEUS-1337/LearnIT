{
    "title": "SylL0krYPS",
    "content": "Deep reinforcement learning has achieved success in difficult tasks, but recent studies show susceptibility to adversarial perturbations. This work focuses on continuous control agents in deep RL with adversarial attacks, proposing a two-step algorithm based on learned model dynamics. Experiments on various domains demonstrate the framework's effectiveness in degrading agent performance and driving them to unsafe states. Recent studies have shown that deep reinforcement learning agents are vulnerable to adversarial attacks, similar to deep neural network classifiers. Adversaries can manipulate the agent's observations to decrease its total accumulated reward. Pioneering work has demonstrated that using the FGSM attack can significantly decrease an agent's average reward in Atari games with small input perturbations. Recent studies have shown that deep reinforcement learning agents, like deep neural network classifiers, are susceptible to adversarial attacks. Adversaries can manipulate the agent's observations to reduce its total reward. The efficiency of attacking Atari agents has been improved by leveraging heuristics and Monte-Carlo planning on a generative video prediction model. The problem of attacking Atari agents often involves finding adversarial examples on image classifiers. However, strategies for discrete action agents may not directly apply to agents with continuous actions. Recent research has focused on adversarial testing for continuous control domains in a slightly different setting. In a challenging setting where agent training history is not available, this paper studies the robustness of deep RL agents against adversarial attacks. A two-step algorithmic framework is proposed to efficiently find adversarial attacks based on learned dynamics models. Experimental results demonstrate that the model-based attack can degrade agent performance effectively and efficiently compared to model-free attacks. The paper proposes a model-based attack on deep RL agents with continuous actions, showing its effectiveness compared to model-free attacks in various MuJoCo domains. Attacks in reinforcement learning are relatively understudied compared to adversarial examples in image classifications and other applications. Early works focused on deep RL agents in Atari games with pixel-based inputs and discrete actions, assuming accurate policies. Huang et al. (2017) used FGSM to find adversarial perturbations, while Lin et al. (2017) improved attack efficiency by timing the attacks strategically. The efficiency of adversarial attacks in reinforcement learning is improved by heuristics that identify optimal timing for attacks. Uesato et al. (2018) utilized rejection sampling and agent training histories to uncover bad initial states with fewer samples. Gleave et al. (2019) explored attacks from adversarial policies. Wang et al. (2019) studied verification of deep RL agents under attacks, which is not the focus here. Model-based RL methods involve acquiring predictive models. Model-based RL methods involve acquiring predictive models of environment dynamics to make decisions, which are more sample efficient than model-free methods. Various works have focused on learning and utilizing dynamics models for planning in RL. This section describes the problem setup, two threat models, and presents an algorithmic framework for designing adversarial attacks on deep RL agents with continuous actions. In model-based RL, a deterministic policy M interacts with a dynamics model f to predict the next state. An adversary's goal is to drive the agent to unsafe target states within budget constraints. Two threat models are considered: observation manipulation and action manipulation. In observation manipulation, the adversary can perturb the agent's perception within limits, while in action manipulation, the adversary can craft actions within limits. The text discusses an adversary's ability to manipulate an agent's actions within limits to drive it to unsafe target states. The objective is to minimize the total distance of each state to a pre-defined target state over a certain number of planning steps. The proposed algorithm involves learning a dynamics model of the environment and using optimization techniques to solve the optimization problems. Step 1: Learn a dynamics model f for developing a strong attack. Different forms of f can be applied based on the environment, such as a linear system or a more complex environment. Step 1: To develop a strong attack, learn a dynamics model f which can be linear or non-linear like neural networks. Model parameters can be learned via supervised learning with sample trajectories pairs.\n\nStep 2: Once the dynamical model is learned, solve Equations 3 and 4 to compute adversarial perturbations of observations/actions. When planning length T > 1, Equation 3 can be solved by incorporating equality constraints into the objective function. The objective is to solve Equations 3 and 4 using projected gradient descent (PGD) to obtain \u2206a i. The algorithm allows for a larger planning length T in solving the equations and applying the first n adversarial perturbations over n time steps. The attack is summarized in Algorithm 2 for Step 1 and Algorithm 3 for Step 2. In this section, experiments are conducted on standard reinforcement learning environments for continuous control. Results are demonstrated on 4 different environments in MuJoCo and corresponding tasks: Cartpole-balance/swingup, Fish-upright, Walkerstand/walk, and Humanoid-stand/walk. A state-of-the-art D4PG agent is trained for deep RL evaluations. The experiments involve training a state-of-the-art D4PG agent for deep RL evaluations. The agent is attacked for one episode with 1000 time steps, measuring total loss and total return reward. The attack algorithm's strength is determined by smaller total return reward and total loss. Comparison is made with model-free attack baselines using random searches and heuristics. The experiments involve training a state-of-the-art D4PG agent for deep RL evaluations. The agent is attacked for one episode with 1000 time steps, measuring total loss and total return reward. The attack algorithm's strength is determined by smaller total return reward and total loss. Comparison is made with model-free attack baselines using random searches and heuristics. A 4-layer feed-forward neural network with 1000 hidden neurons per layer is trained as the dynamics model for Cartpole, Fish, Walker, and Humanoid domains. Monte-Carlo sampling methods are used to generate sample trajectories from random noises for evaluation. The experiments involve training a D4PG agent for deep RL evaluations. A 4-layer feed-forward neural network is used to model dynamics for Cartpole, Fish, Walker, and Humanoid domains. Different training points are required for each domain, with larger planning lengths for Cartpole and Fish and smaller lengths for Walker and Humanoid. Projected gradient descent (PGD) is applied to solve equations, with Adam as the optimizer. The study uses Adam optimizer with 30 optimization steps and reports the best results for different tasks in Walker, Humanoid, Cartpole, and Fish domains. Unsafe states are defined for each domain, and results for observation and action manipulation are presented in Table 1a-d based on distance to the target state. Our proposed attack achieves lower loss compared to model-free baselines in various domains such as Walker, Cartpole, and Fish. It consistently outperforms baselines for observation and action perturbation threat models, except for the Fish domain where it loses to the flip baseline. The attack successfully makes the Walker fall down by minimizing its head height to zero. The average total loss of walker head height remains unaffected for the baselines. Our attack successfully lowers the head height of the walker and humanoid, causing them to fall down quickly, while the baselines remain unaffected. The total loss for the walker head height is 258 (468) and 1960 for the humanoid, demonstrating the effectiveness of our proposed attack. The reward function is often complex and its exact definition is not always available. Learning the reward function is an active research area not covered in this paper. However, attacking unsafe states can decrease the total reward of an agent. Results show that our proposed attack outperforms baselines, lowering the average total reward significantly. The efficiency of attack is evaluated in terms of sample complexity, with the neural network dynamical model trained with different numbers of episodes. Results show that our method outperforms baselines in reducing total head height loss of the walker. The efficiency of attack is evaluated in terms of sample complexity, with the neural network dynamical model trained with different numbers of episodes. By using more samples, the total losses can be significantly decreased, outperforming existing works in reducing total head height loss of the walker. In this paper, the problem of adversarial attacks in deep RL with continuous control is studied for two threat models. A model-based attack algorithm is proposed and shown to outperform model-free based attack baselines in extensive experiments on 4 MuJoCo domains. Future directions based on this work are detailed in the Appendix. The accuracy of learned models in the proposed technique is illustrated in Figure 3. By training models with different numbers of samples, it was found that a more accurate model is achieved with more training samples. The attack results show that a stronger attack is possible with a more accurate model, even though less accurate models still outperform the best baselines. The proposed method outperforms baselines by 1.3-2 times with 400-700 trajectories. A very accurate model is not necessary for effective attacks, but higher accuracy can further degrade agent performance. Adversaries generate 1000 trajectories with random noise, record total reward/loss, and report the best outcome. Our proposed attack method successfully uncovers vulnerabilities in deep RL agents, outperforming baseline adversaries. The baseline adversaries may be weaker without an \"unfair advantage\" of true reward access. The attack involves adding perturbations to the original state/action and projecting them within limits. The well-trained d4pg agents have a total reward close to 1000. Several interesting future directions are suggested. Future directions for research include learning reward functions for more effective attacks, extending the approach to develop black-box attacks, and incorporating the attack algorithm into adversarial training of deep RL agents. Three key challenges include balancing updates for the adversary and model, avoiding training cycles caused by agent overfitting, and ensuring the adversary does not hinder exploration or overly prioritize robust performance."
}