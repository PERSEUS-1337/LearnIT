{
    "title": "BJlrZyrKDB",
    "content": "The use of deep learning has increased the need for understanding and diagnosing models. Interpretation techniques are essential for data analysts, but most methods lack theoretical guarantees. A statistical framework for saliency estimation in black box computer vision models is proposed, which is model-agnostic and passes saliency checks. The method involves solving a linear program efficiently and establishes an upper bound on model evaluations needed for recovery with high probability. A new perturbation scheme for estimating local gradients is proposed, which is more efficient than random perturbation methods. The method's validity is demonstrated through sensitivity analysis. Several interpretation approaches have been proposed to address the lack of interpretability in deep learning models, especially in domains like medicine. These methods include visualizations and building saliency maps by attributing gradients of the neural network to input images. Interpretation approaches have been developed to enhance the interpretability of deep learning models, particularly in fields like medicine. Various methods, such as LIME, SHAP, and C-Shapley, aim to explain model predictions by perturbing inputs or using game theory concepts. Various interpretation methods, including LRP, LIME, and DeepLIFT, have been developed to enhance the interpretability of deep learning models. Chen et al. (2019) introduced L-and C-Shapley procedures to approximate Shapley values efficiently. However, many of these methods are heuristics with unclear estimands and efficiency in computation. Adebayo et al. (2018b) found that methods with good visual inspection may lack sensitivity to the model and data generating process. Nie et al. (2018) provided a theoretical explanation for image recovery using guided back-propagation and deconvolutional methods. Additionally, a statistically valid technique for model-agnostic saliency estimation has been proposed, showing consistency under reasonable assumptions and passing sanity checks. Our method introduces a new saliency estimation framework for CNNs based on input perturbation, efficiently computed through solving a linear program. In contrast to Burns et al. (2019), our approach does not require human input and has a lower computational load. Our method introduces a new saliency estimation framework for CNNs based on input perturbation, efficiently computed through solving a linear program. The optimization problem can be recast as a \"parametric simplex,\" allowing for the computation of the full solution path. Conditions are established for identifying significant pixels in the input with high probability, and finite-sample convergence rates are presented. A new perturbation scheme using a highly correlated Gaussian distribution is proposed to improve convergence rates. The linearly estimated gradient (LEG) is defined as the saliency parameter of interest, and a regularized estimation procedure for LEG is introduced in our statistical framework. In section 3, a regularized estimation procedure for LEG is proposed that penalizes anisotropic total-variation. Theoretical results are provided in Section 4, and numerical comparisons are presented in Section 5. Various mathematical notations and definitions are introduced for matrices, vectors, norms, and sets. In gradient based saliency approaches, the goal is to recover the deep learner's gradient with respect to the input. Local saliency is defined as the derivative of the deep learner's output with respect to the input at a specific point. The local saliency in gradient-based approaches aims to recover the deep learner's gradient at a specific point of interest x0 in X. The linearly estimated gradient (LEG) is defined as a proxy for the local gradient, based on a first-order Taylor series expansion of the function f(x) around x0. LEG is determined by a continuous distribution F and provides a best linear approximation in terms of squared error. The analyst visually demonstrates LEG on toy examples with a single pixel in Figure 1, comparing it to the gradient. LEG provides a more meaningful saliency score for highly varying functions. The variance of F affects LEG, with larger variance evaluating the input's effect on the output over a larger neighborhood. High variance in F can make LEG less useful for interpreting the model. The Local Explanation Gradient (LEG) provides a local interpretation at x0, which becomes more global as the variance of F increases. Lemma 1 shows that LEG can be expressed as an affine transformation of a high-dimensional integral. An empirical estimate for LEG can be obtained by replacing the expectation with the empirical mean through sampling x from F + x0. The empirical mean is obtained by sampling x from F + x0, calculating f(x), and applying Lemma 1. As n \u2192 \u221e, \u03b3 \u2192 \u03b3 due to the positive-definite covariance matrix of F. However, the slow convergence rate limits practicality. Regularization is proposed for faster convergence rates. Saliency scores are expected to be located in contiguous regions, leading to various procedures for estimation. The approach proposes estimating the LEG coefficient with an anisotropic L1 TV penalty, which helps produce reliable solutions with less model evaluations. The method is based on the \"high confidence set\" approach and utilizes a linear program to calculate the TV-penalized LEG estimate. The method utilizes the \"high confidence set\" approach to estimate the LEG coefficient with an anisotropic L1 TV penalty. It aims to find a solution that belongs to the confidence set and has sparse differences on the grid, effectively recovering \u03b3 with small total variation. The proposed method has low computational complexity, as the linear program in equation 4 can be solved in polynomial time using a primal-dual interior-point method. The method uses a primal-dual interior-point method with O(p1p2) 3.5 time complexity. Solutions can be obtained faster with simplex solvers like MOSEK, providing a solution in under 3 seconds for p1=p2=28. An alternative formulation can be solved using parametric simplex approaches, useful for tuning L criteria. The procedure is model-agnostic and does not require knowledge of the neural network. The method involves computing prediction values alongside a term, using a multivariate Gaussian distribution with a perturbation scheme. Saliency scores are obtained by summing absolute values of different channels. Theoretical analysis and convergence rates of the LEG-TV estimator are discussed, with insights on the ideal perturbation distribution. A condition similar to the restricted eigenvalue condition is presented, impacting the estimator's convergence rate. The LEG-TV estimator's convergence rate is impacted by a condition akin to the restricted eigenvalue condition. The main result is presented with a theorem, stating that under certain assumptions for the covariance matrix \u03a3, the estimator \u03b3 has a high probability of accuracy. The proof is based on the \"high confidence set\" approach by Fan (2013). Our theorem establishes that the TV penalized LEG model is statistically consistent, requiring a certain number of model evaluations to recover the true parameter \u03b3*. The convergence rate depends on the choice of \u03a3 for the perturbation scheme, offering faster rates with a carefully tuned selection. Our result shows that certain estimands require fewer samples, with the estimator's convergence depending on the spectral properties of \u03a3. The LEG-TV model yields solutions with no mean differences from the true LEG coefficient, and our estimator's convergence is influenced by the quantity \u03ba. The rate of convergence to the true LEG coefficient is inversely proportional to the term \u03ba. Perturbation schemes with large restricted eigenvalues result in saliency maps that require fewer samples to estimate the LEG. Most saliency estimation procedures use independent perturbations, leading to a covariance matrix equal to the identity matrix. However, this choice is not ideal for sparse solutions in the TV norm, as demonstrated by our theorem. Our proposed perturbation scheme aims to maximize the bound in equation 5 by choosing the covariance matrix \u03a3 based on the eigenvectors of D. This heuristic approach fixes many eigenvectors of \u03a3 and results in a maximal value for \u03ba without additional assumptions on S. The singular value decomposition of D is computed to determine \u03a3, leading to a reduction in the numerator of equation 5 to \u03c3^2 \u2206^T \u2206, where \u03ba = \u03c3^2. Figure 3 displays the selected eigenvectors of the proposed \u03a3. The proposed perturbation scheme selects eigenvectors of \u03a3 to maximize the bound in equation 5. These eigenvectors represent the principal directions of the distribution and are used for object detection. Samples drawn from the distribution show sharp contrasts at specific locations, aiding in boundary recovery. The perturbation scheme uses a multivariate Gaussian distribution for structure and a heuristic for correlation structure. The magnitude of perturbations, controlled by \u03c3 2, affects neural network predictions. In our implementations, we find that setting \u03c3 2 between 0.05 and 0.30 results in reasonable solutions for perturbations. The range is determined by computing perturbations on images using the VGG-19 classifier. Most results are shown for \u03c3 2 = 0.10. For the choice of L, two solutions are proposed: one based on theoretical suggestions and the other based on optimization problem quantities. In our demonstrations, we set L = K L L max, where K is a constant between 0 and 1. We use K L = 0.05 or K L = 0.10. It is possible to obtain the solution for all L using a parametric simplex solver or by starting with a large initial L and using the solution as a warm-start for a smaller choice of L. This approach returns the solution path for all L, demonstrating the robustness and validity of our procedure through numerical experiments. In a sensitivity analysis, various saliency methods are used to compute regions of importance and perturb these regions to observe their effect on predictions. The validity of saliency estimation procedures is tested by varying the weights of the neural network through a technique called \"cascading randomization\". This involves replacing the fitted weights of a CNN layer by layer and computing saliency scores with each change. Adebayo et al. (2018b) found that interpretability methods still provide saliency even after full randomization of weights, acting as edge detectors. When neural network weights are randomly perturbed, predictions change significantly, leading to flat local linear approximations and saliency scores of zero for all pixels. The LEG-TV method is robust and reliable for interpretation, as shown through a cascading randomization experiment on a VGG-19 network. The results indicate that the method fails to detect any signal after weights perturbation, leading to a zero estimate due to penalization. This highlights the method's dependency on the classifier used. The LEG-TV method's interpretation relies on the classifier used. Results from a sanity check show zero estimates after cascading randomization of network weights. Various interpretation models are compared, including GradCAM, LIME, SHAP, and C-Shapley, to assess the impact on predictions for the target class. The study compares different saliency map methods like GradCAM, LIME, SHAP, C-Shapley, and LEG-TV on a 28x28 grid. Saliency maps are downsized for fair comparison, improving estimator performance. LEG-TV offers sparse and noisy solutions based on penalty parameter choice. Results are based on 500 randomly chosen images from the ImageNet dataset. Results from sensitivity analysis on 500 randomly chosen images from the ImageNet dataset show that as perturbation size increases, predictions for the target class decrease for all methods. SHAP and LEG0 are most accurate in identifying crucial pixels, followed by LEG, GradCAM, C-Shapley, and LIME. Top 10% most salient pixels are plotted in Figure 7. In Figure 7, top 10% most salient pixels are plotted for three images in the dataset using different procedures like SHAP, LEG-TV, GradCAM, C-Shapley, and LIME. LEG-TV selects visually meaningful pixels, while SHAP identifies specific convolution patterns. LEG-TV relates misclassifications to relevant features like labels and barcodes. Our proposed statistical framework for saliency estimation relies on local linear approximations, leading to a computationally efficient saliency estimator with theoretical guarantees. Through empirical studies, we have shown that our method passes sanity checks and identifies highly relevant pixels for predictions, often outperforming its competitors. Additionally, our linear program can be recast by changing variables, improving sample complexity by altering the model evaluation scheme. The program involves a linear constraint on pixel differences and is derived from a theorem. The formulation can be expressed in an augmented form. The proof of Theorem 1 relies on a lemma, ensuring feasibility with a certain probability. The proof of Theorem 1 involves applying McDiarmid's inequality to bound the values of the function. By taking a union bound over all variables, the feasibility set for any L automatically includes \u03b3*. The technique is based on the Confidence Set approach by Fan (2013). The proof of Theorem 1 involves applying McDiarmid's inequality to bound the values of the function. By taking a union bound over all variables, the feasibility set for any L automatically includes \u03b3*. The technique is based on the Confidence Set approach by Fan (2013). Let \u2206 = D(\u03b3 - \u03b3*), where \u03b3 and \u03b3* are in the feasibility set. Define S = {j : D\u03b3* j = 0}, with |S| = s. Then, \u2206S1 \u2265 \u2206SC1, where Holder's inequality is used. The right-hand side is bounded by a previous result. By using previous results, we bound \u22062 and divide both sides by \u22062 as assumed in the Theorem."
}