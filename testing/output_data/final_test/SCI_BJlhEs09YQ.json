{
    "title": "BJlhEs09YQ",
    "content": "In this paper, the problem of learning the weights of a deep convolutional neural network is studied. A new algorithm called Deep Tensor Decomposition (DeepTD) is developed to learn all the kernels simultaneously from training data. The approach is based on a rank-1 tensor decomposition and is shown to be data-efficient, working effectively when the sample size exceeds the total number of convolutional weights in the network. Numerical experiments confirm the effectiveness of DeepTD in deep neural network architectures. Convolutional neural networks (CNNs) have significantly improved performance in various domains like image recognition. They construct higher level features from lower level ones and are efficient for large-scale applications. Convolutional neural networks (CNNs) are known for their remarkable efficiency, attributed to weight-sharing, convolutional nature, and efficient matrix/vector multiplication. Despite their empirical success, the reasons for their effectiveness remain a mystery. Recent interest focuses on developing rigorous foundations for neural networks, with an emphasis on depth for constructing higher-level features. State-of-the-art Resnet models typically have hundreds of layers. In this paper, an algorithm is proposed for learning deep CNN models with guarantees. The focus is on understanding the computational tractability of training deep CNN architectures and the data requirements. The model assumes inputs are i.i.d. from a Gaussian distribution and labels are generated based on convolutional kernels. The algorithm uses labels and features to construct a tensor that converges to a population tensor revealing kernel directions in the limit of infinite data. The algorithm proposed in the paper aims to learn deep CNN models with guarantees by using a tensor decomposition approach to reveal kernel directions from training data. The gap between population and empirical tensors decreases with larger training data, leading to approximate learning of kernels when data size is proportional to the model parameters. This work represents a step towards provable end-to-end learning of practical deep CNN models. In this work, tensor decomposition is used to learn deep CNN models despite nonlinearities and increasing depth. The focus is on utilizing this algorithm to enhance CNN training quality and speed. A CNN model consists of layers of neurons, including input, output, and hidden layers connected through convolutions. In this paper, the focus is on a CNN model with non-overlapping convolution operations defined by a stride length equal to the kernel length. The convolution is viewed as matrix/vector multiplications, with a kernel matrix defined for the operation. The paper focuses on a CNN model with non-overlapping convolution operations using a kernel matrix. The CNN input-output relationship and layer numbering are defined, with the input consisting of p features and the output being a one-dimensional label. The CNN model assumes hidden layers with p units and defines the kernel dimensions for relating layer outputs. Inputs are related to outputs via non-overlapping convolution using a kernel matrix. The CNN model assumes hidden layers with p units and defines the kernel dimensions for relating layer outputs. Inputs are related to outputs via non-overlapping convolution using a kernel matrix. The approach introduced in this paper, DeepTD, approximates convolutional kernels from training data through tensor decompositions. The DeepTD approach involves constructing a tensor from training data and performing a rank-1 tensor decomposition to approximate convolutional kernels efficiently. The DeepTD approach constructs a tensor from training data and performs a rank-1 tensor decomposition to approximate convolutional kernels efficiently. Anandkumar et al. (2014a); Ge et al. (2015) explain how the tensor converges to a population tensor T, revealing useful information about the kernels. The tensorized input is represented by x, a Gaussian random vector, with f CNN(x) as the output. The best rank-1 TD solution is NP-hard, but theoretical guarantees hold with an approximately optimal solution. The DeepTD approach constructs a tensor from training data to approximate convolutional kernels efficiently. The tensor converges to a population tensor T, revealing useful information about the kernels. In numerical simulations, popular software packages are used to solve for the tensor. The construction of the tensor involves centering the weights to ensure better concentration of the empirical tensor around its population counterpart. The centering procedure in constructing the tensor is crucial for the success of the approach, resembling batch-normalization in deep neural networks. Rank-1 tensor decomposition can recover convolutional kernels up to sign and scaling ambiguities, which may be challenging to overcome depending on the activation function used. Heuristics and theoretical guarantees are discussed in Appendix C for addressing these ambiguities. In Section 4, theoretical results for DeepTD are introduced. The empirical tensor concentrates around its population counterpart, which is well-approximated by a rank-1 tensor revealing convolutional kernels. DeepTD can learn these kernels up to sign/scale ambiguities. The quality of concentration is measured using the tensor spectral norm. The CNN model generates labels for tensorized input data. The empirical tensor approximates the population tensor with high probability, with the quality of approximation linked to the network's Lipschitz constant. Fluctuations in the CNN output affect the concentration of the empirical tensor. The quality of the empirical tensor approximation in a CNN model is influenced by fluctuations in the output, with the approximation guarantee worsening as the tensor becomes less concentrated. The approximation quality scales with the square root of model parameters and inversely with the square root of the number of samples. Additionally, the population tensor can be approximated by a rank one tensor, with the average slopes of activations playing a key role in quantifying this approximation. The average gain of a given input feature in a CNN model is quantified by the product of average slopes of activations along a path connecting input to output. This quantity reflects the amplification or attenuation due to nonlinear activations in the network, with a range from 0 (inactive network) to 1 (fully active network). The value of this quantity can be bounded from below by a constant in certain cases. The softplus activation function and the rank one CNN tensor are defined based on certain assumptions to ensure network activity. The rank one CNN tensor is the product of kernels scaled by the CNN gain. Activation smoothness is assumed for differentiable activations everywhere. Activation smoothness is crucial for quality rank one approximation as smoother activations lead to smoother variations in the population tensor entries. Kernel diffuseness parameter measures how diffused the kernels are, affecting the quality of the approximation. Theorem 4.7 discusses approximating a population tensor with a rank one tensor under certain assumptions. Theorem 4.7 discusses approximating the population tensor with a rank one tensor under assumptions about activation smoothness and kernel diffuseness. The quality of the approximation deteriorates with increased smoothness of activations and diffuseness of kernels, leading to more fluctuations in the population tensor. The relative error in the approximation is bounded, with smoothness often bounded by a constant for many activations like the softplus activation. The CNN gain \u03b1 CNN is bounded from below by a constant under certain assumptions on kernels and activations. When the length of convolutional patches scales with the square of the network depth by a constant factor, the rank one approximation is deemed sufficient. Future research may focus on achieving the correct scaling, and the results are expected to apply to non-differentiable activations like ReLU. The empirical tensor in DeepTD approximates the population tensor well and is guaranteed to estimate convolutional kernels accurately with high probability using few samples. This result applies to non-overlapping CNNs with a single kernel at each layer, assuming smooth activations and unit norm diffused kernels. The DeepTD method guarantees accurate estimation of convolutional kernels in non-overlapping CNNs with smooth activations and unit norm diffused kernels. Experimental validation was conducted using a CNN model with equal kernel lengths and ReLU activations, aligning with theoretical predictions. The DeepTD method uses the Tensorly library for tensor decomposition in Python. Experiments focus on depth D and width d values, with kernels generated randomly and normalized. Operational networks require at least 50% non-zero training labels for feasible learning. In experiments with DeepTD, correlation values above 75% are consistently achieved for N = 20, with kernels and estimates having unit norm. The method requires at least 50% non-zero training labels for feasible learning. The sample size grows proportionally to the total degrees of freedom, and experiments are carried out for N \u2208 {10, 20, 50, 100}. In experiments with DeepTD, correlation values above 75% are consistently achieved for N = 20, with kernels and estimates having unit norm. The effect of sample size becomes evident by comparing N = 20 and N = 50 for the input and output layers. Interestingly, correlation values are smallest in the middle layers, even when N is large. The NaiveTD algorithm is defined without centering in the empirical tensor, and the impact of the centering procedure of the DeepTD algorithm is assessed. The output shows a positive bias in expectation, highlighting the importance of centering. DeepTD outperforms NaiveTD for smaller oversampling factors of N = 10 or N = 20. The correlation difference persists across different layers and increases with kernel size d. Comparing ReLU and identity activations in the final layer, ReLU achieves significantly lower correlation due to an additional nonlinearity. This work is related to recent papers on neural networks and tensor decompositions. In this paper, a multilayer CNN model with depth D is studied, focusing on approximating CNN kernels using higher order tensor decompositions. An algorithm called Deep Tensor Decomposition (DeepTD) is proposed for learning all kernels simultaneously. Tensor Decomposition (DeepTD) algorithm builds a D-way tensor based on training data and applies rank one tensor factorization to estimate convolutional kernels. DeepTD can learn all kernels with minimal training data, as proven in theoretical findings and numerical experiments. The algorithm is able to approximate all kernels by utilizing a Gaussian model for input data and planted convolutional kernels for output generation. The Euclidean norm of a tensor is the sum of squares of its entries. Orlicz norms are defined for scalar random variables, with sub-exponential and sub-gaussian norms. Lemmas and definitions are provided for concentration arguments. Lemma A.3 states that the sum of sub-gaussian random variables is sub-exponential. It also connects Orlicz norms of random variables to the sum of their individual Orlicz norms. Additionally, the text introduces standard chaining definitions and provides an upper bound for the \u03b3\u03b1 functional with covering numbers of T. References to related works are also mentioned. Lemma A.7 introduces Dudley's entropy integral, relating the covering number of a set to a specific formula. The proof involves manipulating empirical and population tensors. Lemmas A.8 and A.9 provide bounds for certain terms, with their proofs deferred to later sections. Lemma A.9 states that DISPLAYFORM0 holds with a fixed numerical constant c2 > 0. The proof of Theorem 4.2 is concluded by combining Lemma A.9 with specific values for t1, t2, and c2. The lemma is proven by scaling a random process and showing a mixture of subgaussian and subexponential increments. The lemma in the previous section concludes that a fixed numerical constant c2 > 0 holds. Moving on, the current section discusses the sub-exponential nature of vector Yi in relation to tensor T. It shows that the difference between functions g(T) and g(H) is a sum of sub-exponential variables, leading to a tail bound analysis studied by Talagrand and others. The current section discusses the tail process studied by Talagrand and others, characterizing supremum of processes using \u03b31 and \u03b32 functionals. Distance metrics on tensors induced by the Frobenius norm are defined, leading to bounds with respect to d1 and d2 metrics. Theorem 3.5 of Dirksen FORMULA69 is applied, and a change of variable is used to update constants. Bounds on \u03b32 and \u03b31 terms are achieved by upper bounding the \u03b3\u03b1 functional in terms of Dudley's entropy integral. Dudley's entropy integral is used to find the \u03b5 covering number of RO, with coverings of unit 2 balls. The metrics d1 and d2 are scaled by a constant, leading to scaled \u03b3\u03b1 functions. The proof involves rewriting equations and applying bounds on empirical sums. Xi is a tensor with standard normal entries, and Theorem 1 by Tomioka & Suzuki is applied. The proof of the theorem involves defining non-overlapping CNN models as a tree structure, with path vectors satisfying specific conditions. The visualization in FIG10 illustrates this concept. The tree visualization in FIG10 represents path vectors connecting input features to the output in a non-overlapping CNN model. The kernel path gain is the multiplication of kernel weights along the path, while the activation path gain is the multiplication of derivatives of activations at hidden units along the path. The CNN model in Definition A.12 associates sets of hidden units with input data entries, resembling a tree structure. The population tensor can be approximated as a rank one tensor, as shown in FIG10. The tensorization operation in a CNN model is linear, with the population tensor being equal to a specific formula. The population gradient vector is defined as g CNN = E[\u2207fCNN(x)]. The activation path gain is denoted as \u03c6. A set of offsprings of the last hidden node in layer two is outlined. A vector k is defined such that ki = k i(i). The tensor K is equal to a certain value, and the corresponding tensor V is defined as well. The population tensor can be rewritten in vector form as g CNN = k \u2299 v, where \u2299 denotes the Hadamard product. The population tensor T is the outer product of convolutional kernels masked with tensor V. Approximating T with a rank one matrix involves replacing V with a scalar. The quality of this approximation is characterized by spectral and Frobenius norms, Euclidean norms, and the kronecker product of vectors. The Euclidean norm of individual vectors is used to prove Theorem 4.7, which is based on Lemma A.13 showing a stronger statement. The proof involves bounding the difference between vi and \u03b1CNN using a telescopic sum. The proof involves bounding the summands a - a -1 using Lemmas A.14 and A.15, which are deferred to later sections. The proof involves bounding the summands using Lemmas A.14 and A.15, with observations on functions and entries. Applying Lemma A.14 with Z = \u03b8 \u22121 and using 1-Lipschitzness of \u03c3 's, we conclude that S is the smoothness of \u03c3 and Lipschitz constant of \u03c6 \u2032. The E zm(Y ) term is assessed, with each entry of h (\u22121) obtained by inner products and activations \u03c3 (\u22c5). The tail bound for zm(Y ) is derived, leading to the upper-bound on each summand. Combining such upper bounds implies \u03ba i. This concludes the proof of Lemma A.13. The proof of Lemma A.13 concludes with the independence of X, Z and Lipschitzness of f. By using the weights and functions of the vector x set (i), the result is derived. The theorem follows by combining Theorems 4.2 and 4.7, providing a bound on the tensor spectral. The proof of Theorem 4.8 follows by combining Theorems 4.2 and 4.7. A perturbation argument for tensor decompositions is needed to translate a bound on the tensor spectral norm of Tn \u2212 LCNN to a bound on learning the kernels. Lemma A.16 is applied with specific parameters to complete the proof. The next section will focus on proving Lemma A.16, which involves rank one tensors and a maximization argument. Additionally, the ReLU model with a specific assumption on the kernels is considered. The ReLU model assumes kernels with mean larger than zero and modest diffusion. The proof involves applying inequalities and assumptions to arrive at a conclusion. The ReLU model assumes kernels with mean larger than zero and modest diffusion. In the above analysis, we conclude that using softplus activation instead of ReLU leads to bounding the expected value of hidden units. DeepTD approximates the rank one tensor from data, recovering convolutional kernels up to Sign/Scale Ambiguities. DeepTD approximates the rank one tensor from data, recovering convolutional kernels up to Sign/Scale Ambiguities (SSA). It may not be possible to recover ground truth kernels from training data, especially with ReLU activations. Centered Empirical Risk Minimization (CERM) is introduced to learn a good approximation of the network to minimize risk. The Centered Empirical Risk Minimizer (CERM) offers advantages over ERM by avoiding label or function bias in learning rates. CERM is used to address Sign/Scale Ambiguities (SSA) after estimating kernels with DeepTD, requiring only a few extra training samples. A greedy algorithm inspired by CERM is proposed to tackle SSA efficiently. The algorithm proposed in the next section is a greedy heuristic for finding the CERM estimate, which operates over a CNN function class. It aims to resolve Sign/Scale Ambiguities (SSA) by maximizing correlation between centered labels. The approach is tailored towards homogeneous activations, where a single global scaling is sufficient. In this section, a generalization bound for Centered Empirical Risk Minimization (CERM) is proven. The theorem guarantees that CERM will choose a function close to the population's minimizer using a finite sample size. By combining inequalities and setting parameters appropriately, it is shown that with high probability, the expected error of the function is less than a certain threshold. The text discusses how Theorem C.1 follows from Theorem C.2 by showing that Fk,B has a small Lipschitz covering number. A cover F' for the set Fk,B is constructed, providing a cover of F. By applying Theorem C.2, it is concluded that the expected error of the function is less than a certain threshold with high probability."
}