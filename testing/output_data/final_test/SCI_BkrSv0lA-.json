{
    "title": "BkrSv0lA-",
    "content": "The paper discusses compressing deep neural networks by weight quantization, specifically extending a loss-aware weight binarization scheme to ternarization with different scaling parameters for positive and negative weights. Experiments show that this scheme outperforms other quantization algorithms and maintains accuracy comparable to full-precision networks. The size of deep networks has hindered deployment on small computing devices, leading to efforts to reduce model size through approaches like pruning. Efforts to reduce model size include pruning dense networks and filters in convolutional neural networks based on their significance to the loss. Other approaches involve using more compact models like GoogleNet and ResNet, which replace fully-connected layers with global average pooling. SqueezeNet reduces model size by using 1x1 filters, while MobileNet compresses models using separable depth-wise convolution. ShuffleNet and Novikov et al. (2015) have proposed methods to compress models by utilizing group convolution and compact multilinear formats. CP and Tucker decompositions have also been used on kernel tensors in CNNs. Another approach is quantizing weights to a small number of bits, either with pre-trained models or training from scratch. Low-precision weights are also used but may suffer in performance. BinaryConnect algorithm pioneered by BID1 uses one bit for each weight while maintaining high accuracy. Rastegari et al. (2016) improved results by incorporating weight scaling. TernaryConnect quantizes weights to {-1, 0, 1} and adds scaling. DoReFa-Net extends quantization to more than three levels but lacks consideration of loss effects. Recent advancements focus on loss-aware low-bit quantized neural networks. In this paper, a loss-aware low-bit quantized neural network is proposed for efficient network compression. The ternarization scheme considers the effect on loss and is optimized using the proximal Newton algorithm. It allows for different scaling parameters and the use of more than two bits for weight quantization. Experimental results on various neural networks demonstrate the effectiveness of the proposed quantization scheme. The proposed quantization scheme for neural networks outperforms state-of-the-art algorithms. It involves binarizing weights and includes a scaling parameter for efficiency. The proposed quantization scheme for neural networks involves binarizing weights with a scaling parameter for efficiency. In weight ternarized networks, an additional quantized value of zero is used. In weight quantized networks, m bits are used to represent each weight, with a set of quantized values. A loss-aware quantized network called low-bit neural network (LBNN) was also proposed. The proposed low-bit neural network (LBNN) uses the alternating direction method of multipliers (ADMM) for optimization. Weight ternarization involves finding the closest ternary approximation of full precision weights, with quantization thresholds and scaling parameters obtained through an optimization problem. The weight is ternarized as \u0175 = \u03b1b, where \u03b1 > 0 and b \u2208 {-1, 0, 1}. The optimization problem involves finding desired quantized values using the proximal Newton method. The LBNN uses ADMM for optimization and weight ternarization to find the closest ternary approximation of full precision weights. The optimization problem involves finding quantized values using the proximal Newton method with a diagonal equilibration preconditioner. Each proximal Newton iteration consists of two steps: obtaining weights by gradient descent and minimizing the objective layer by layer. The LBNN uses ADMM for optimization and weight ternarization to find the closest ternary approximation of full precision weights. The optimization problem involves finding quantized values using the proximal Newton method with a diagonal equilibration preconditioner. The loss is not sensitive to weight when the loss surface is steep, ternarization needs to be more accurate. The constraint in (6) is more complicated than that in LAB, but a simple relationship can still be obtained for weight ternarization. When the curvature is the same for all dimensions at layer l, the solution reduces to that of TWN. The LBNN uses ADMM for optimization and weight ternarization with a diagonal equilibration preconditioner. LBNN's projection step is similar to (6) but lacks curvature information. LBNN updates weights with extra-gradient, doubling steps. LBNN uses full-precision weights in forward pass, unlike other quantization methods. The Loss-Aware Ternarization (LAT) algorithm, presented in Algorithm 3 of Appendix B, converges by ensuring the objective of the proximal Newton algorithm converges. It involves keeping full-precision weights during the update process and utilizes a different computation method for \u03b1 t l and b t l compared to LAB's Algorithm 1. Additionally, the algorithm can be extended to ternarize weights in recurrent networks. The Loss-Aware Ternarization (LAT) algorithm, discussed in Algorithm 3 of Appendix B, converges by ensuring the proximal Newton algorithm's objective converges. It involves maintaining full-precision weights during updates and uses a different computation method for \u03b1 t l and b t l compared to LAB's Algorithm 1. The algorithm can also extend to ternarize weights in recurrent networks. Interested readers can refer for more details. To simplify notations, superscripts and subscripts are dropped. The process of solving for \u03b1 involves introducing notations, sorting elements of |w| in descending order, and finding the optimal \u03b1 that yields the smallest value. The optimal \u03b1 in the range 1 to n \u2212 1 is determined by minimizing the objective in (6) using Algorithm 1. Different scaling parameters for positive and negative weights can be used, similar to TTQ. The optimization subproblem at the tth iteration involves finding exact and approximate solutions for \u03b1 t l and \u03b2 t l. For m-bit quantization, the set Q of quantized values is adjusted. The optimization process includes a gradient descent step with adaptive learning rates. In this section, experiments are conducted on feedforward and recurrent neural networks comparing various methods including weight-binarized networks like BinaryConnect, Binary-Weight-Network, and Loss-Aware Binarized network, as well as weight-ternarized networks like Ternary Weight Networks and Trained Ternary Quantization. The optimization process involves gradient descent with adaptive learning rates and a quantization step that can be efficiently solved by alternating minimization. The experiments compare weight-binarized and weight-ternarized networks on feedforward and recurrent neural networks. Weight-ternarized networks, including the proposed Loss-Aware Ternarization (LAT), perform better than weight-binarized networks on MNIST, CIFAR-100, and SVHN datasets. The setup for CIFAR-100 includes 45,000 training images, 5,000 validation images, and 10,000 testing images. The testing errors are shown in a table. The proposed LAT and its variants have the lowest errors among weight-ternarized networks. LATa performs similarly to full-precision networks on CIFAR-10 but is surpassed by BinaryConnect. Scaling parameters for CNN layers show higher values for input and output layers to maintain activation and gradient variances. LAT2 consistently outperforms TTQ in performance. The proposed LAT2 outperforms TTQ in performance, especially on CIFAR-100. Weight quantization is seen as a form of regularization that can positively contribute to performance. The scheme with logarithmic quantization performs the best among 3-bit quantization algorithms, showing better results on CIFAR-100 and SVHN. More quantization flexibility is beneficial when the weight-quantized network lacks capacity. In character-level language modeling experiments on LSTM BID8, different initializations for TTQ were tried, with LATe and LATa outperforming other weight ternarization schemes. They even surpassed the full-precision network on all three data sets. Convergence of training and validation losses on War and Peace is shown in FIG6. In character-level language modeling experiments, LAT and its variants converge faster than TWN and TTQ. LAT2e and LAT2a outperform TTQ on all data sets. LAQ outperforms DoReFa-Net with 3 or 4 bits. Ternarization yields the lowest validation loss on War and Peace and Linux Kernel. Logarithmic quantization is better than linear quantization for weight distributions. In this paper, a loss-aware weight quantization algorithm is proposed, which considers the effect of quantization on the loss. The algorithm uses the proximal Newton method, involving gradient descent and quantization steps to project full-precision weights onto quantized values. Ternarization solutions are provided, including an exact and an efficient approximate solution. The quantized networks often outperform full-precision networks, with deep networks benefiting from the regularization effect of low-bit quantization. The proposed quantization scheme, including ternarization solutions and experiments on feedforward and recurrent networks, outperforms the current state-of-the-art."
}