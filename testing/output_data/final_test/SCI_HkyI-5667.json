{
    "title": "HkyI-5667",
    "content": "Knowledge Bases (KBs) are growing larger, sparser, and more probabilistic, used for query inferences and rule mining. The challenge lies in efficiently utilizing incomplete KBs due to the uncertainty associated with each tuple and scalability issues with current completion techniques. SafeLearner is a scalable solution for probabilistic KB completion, using lifted probabilistic inference instead of grounding for faster probabilistic rule learning. SafeLearner was compared to ProbFOIL+ and AMIE+ on standard probabilistic KBs of NELL and Yago. Results show SafeLearner scales well with AMIE+ for simple rules and is faster than ProbFOIL+. Probabilistic databases are used to handle incomplete and uncertain knowledge bases. The SafeLearner system introduces lifted inference to avoid grounding steps and improve scalability in probabilistic databases. It enhances rule generation using AMIE+ to create deterministic candidate rules, which are then made probabilistic through lifted inference. The paper introduces SafeLearner, which utilizes lifted inference to generate probabilistic rules without grounding steps. It includes sections on background, defining the problem, outlining SafeLearner's working, proposing the algorithm, experimental evaluation, related work, and future research directions. The notation and definitions are adapted from previous work by Ceylan et al. In databases, a \u03c3-interpretation assigns truth values to \u03c3-atoms, which are ground atoms constructed from predicates and constants. Query answering involves finding substitutions that satisfy a first-order logic formula with free variables. The answer to a query is the set of substitutions that make the formula true in the database. The answer to a query in a database is denoted as Ans(Q, \u03c9). Queries may contain variables bound by quantifiers. A Boolean query has fully quantified variables, while a conjunctive query is a negation-free first-order logic conjunction. We mainly work with union of conjunctive queries (UCQ) in this paper. In this paper, we focus on union of conjunctive queries (UCQs). UCQs are represented as a collection of rules with the same relation in the head. The prediction set for a set of rules representing a UCQ is the union of its respective answer sets. A database is considered a Probabilistic Database (PDB) if every tuple has an assigned probability value. A Probabilistic Database (PDB) is a finite set of tuples with assigned probability values for a vocabulary \u03c3. Each PDB induces a unique probability distribution over interpretations. For Boolean queries, probabilities can be directly read from tables or require probabilistic inference. For example, the probability of coauthor(bob, carl) is 0.4, while the probability of \u2203x. coauthor(bob, x)\u2227 coauthor(carl, x) is 0.879. The text discusses query plans in probabilistic databases, specifically focusing on safe queries that can be evaluated in polynomial time. Safe queries have extensional query plans that allow for correct probability computation, while others may be #P-complete. The complexity of evaluating queries on a probabilistic database is either PTIME or #P-complete. The text discusses lifted inference algorithms for safe queries in probabilistic databases. The Lif t O R algorithm uses a divide and conquer approach to calculate query probabilities, allowing for the modeling of distributions with Prolog rules. The text discusses using Prolog rules to model distributions in probabilistic databases, illustrated through an example of introducing a new probabilistic relation. Syntactic sugar is used for probabilistic rules, allowing for the manipulation of probabilities of sigma-atoms. The text discusses using Prolog rules to model distributions in probabilistic databases, with a focus on introducing a new probabilistic relation. The auxiliary relation's arguments only range over variables in the head to keep rules simple for lifted inference. The goal is to avoid introducing probabilistic weights that could create unsafe queries. The intention is to fill missing probabilistic tuples in tuple-independent databases. In tuple-independent probabilistic databases, a new operation called Ind(R) creates a tuple-independent relation R by assigning marginal probabilities to individual tuples. When R is defined using probabilistic rules and deterministic relations, R = Ind(R). This process can be likened to materializing views into tuple-independent tables. The goal is to learn probabilistic rules from a given PDB D and target relation, using a set E of tuples from the target present in D as training examples. This set of training examples E defines a distribution over deterministic training sets, similar to how a probabilistic database represents a distribution over deterministic databases. During training, a collection of probabilistic rules is sought to achieve good results. During training, a collection of probabilistic rules is sought to obtain a good model for the data. The task involves finding rules that define a relation using various attributes. The definition of a good model depends on context, different loss functions, and assumptions about the training tuples. The problem statement is to find a set of probabilistic rules called H. SafeLearner addresses the problem of finding a set of probabilistic rules with minimum cross entropy loss by using a structure learning component to generate candidate rules and a parameter learning component to optimize weights. The focus is on learning deterministic candidate rules for binary relations in knowledge bases. The method of structure learning in SafeLearner focuses on generating deterministic candidate rules using AMIE+ for mining rules from knowledge bases. AMIE+ uses a sophisticated scoring function to output rules that meet specific criteria and improve over their parents' confidence scores. The approach involves tuning parameters after the structure is learned. The method in SafeLearner involves generating deterministic candidate rules using AMIE+ from probabilistic databases and training sets. This approach aims to find rules with high confidence levels, making it suitable for rule learning tasks. SafeLearner uses AMIE+ to find candidate rules with high confidence levels. The algorithm selects k rules with the highest confidence, checks if they form a safe UCQ, and removes any low-confidence rules to ensure safety. Additionally, a rule with an empty body is added for the target relation to avoid infinite cross entropy scores. The algorithm in SafeLearner uses AMIE+ to find high-confidence rules and ensures safety by selecting rules that form a safe UCQ. To prevent infinite cross entropy scores, a rule with an empty body is added for the target relation. The likelihood of the model's learned rules given the training data is measured using KL divergence. The expected log-likelihood of the model given training examples can be written as cross entropy, which needs to be minimized. Access to probabilities of all possible tuples is usually limited, leading to different settings for optimization. The text discusses two settings for treating tuples outside the training set when learning a probabilistic model for a sparse database. It categorizes target tuples into those in the training set, those outside but in the answer of a rule, and those with grounded proofs in the training data. The text discusses estimating the loss for target tuples outside the training data by categorizing them into different sets and sampling to speed up the estimation process. The text discusses sampling tuples for different categories to estimate probabilities, assigning weights based on subsampling ratios, and predicting the probability of generated tuples. It emphasizes the distinction between the distribution that generates tuples and the distribution that needs to be modeled. The text discusses probabilistic extensions of the PAC-learning framework, focusing on ignoring tuples not in the training set. It highlights the use of lifted inference engines for approximate probabilistic inference in constructing knowledge bases from nonstructured sources. Slimshot BID14 was the first engine developed for this purpose in 2016. In our implementation, we aim to perform lifted inference on queries by converting them into independent subqueries to ensure no variable is common across subqueries. SafeLearner initializes rule weights based on training data confidences for probabilistic inference. SafeLearner system tackles the problem statement by generating probabilistic rules for a target relation. It learns safe rules with polynomial-time data complexity through structure and parameter learning components. The rule weights are optimized using Stochastic Gradient Descent. SafeLearner system generates probabilistic rules for a target relation by optimizing rule weights. Algorithm 1 outlines the working of SafeLearner, which takes input a PDB D and a target relation as a Problog file. The algorithm parses D from the input file, separates tuples with probabilities, and uses AMIE+ to output deterministic rules. These rules are then checked for type consistency and initialized with probabilities to form a set of probabilistic candidate rules. The SafeLearner system generates probabilistic rules for a target relation by optimizing rule weights. The learned rules are made extensible to all possible target tuples by sampling more target tuples and formulating a UCQ. The QueryConstructor algorithm constructs queries from a hypothesis of rules by updating rule weights and removing insignificant rules. The SafeLearner system uses SGD to learn rule probabilities for a target relation. It prunes rules with low probabilities. SafeLearner is written in Python 2.7 and relies on Postgres 9.4 and Java. Algorithm 3 predicts the probability of a query for an example. The SafeLearner system uses SGD to learn rule probabilities for a target relation, prunes rules with low probabilities, and predicts the probability of a query for an example. Experimental Setup: Investigating the prediction of target tuples from other relations to address missing data in the database by learning rules from NELL iterations. The SafeLearner system uses the latest iteration (1110) for model validation by removing test data tuples present in training data. Baselines compared include ProbFOIL + with probabilistic rules from NELL, and deterministic rules obtained using AMIE+. Probabilistic rules with confidence scores as weights are used in SafeLearner system. The system compares ProbFOIL + with probabilistic rules from NELL and deterministic rules from AMIE+. The minimum confidence threshold is set for experiments. SafeLearner uses probabilistic rules with confidence scores as weights. The minimum confidence threshold for experiments is set at 10^-5. Results show that SafeLearner consistently provides the best estimates of probabilities on unseen test-set tuples. The differences in performance were smaller for precision-recall curves, indicating that ranking of tuples based on predicted probabilities can be reliably done by SafeLearner. SafeLearner can scale to over 14000 target tuples with fewer rules, balancing runtime and complexity. The approach relies on lifted inference, allowing for scalability in larger databases compared to previous works. Our approach, SafeLearner, utilizes lifted inference for scalability in larger databases. Previous methods do not consider model behavior on tuples outside the training set, leading to issues with false positives. Our work is related to learning from knowledge bases in statistical relational learning. The learned parameters of models in Markov logic networks are difficult to interpret, unlike probabilistic rules which have a clear probabilistic interpretation. Different loss functions, such as squared loss and probabilistic accuracy, can be used for parameter learning in probabilistic rules. The squared loss, a proper scoring rule, is used for learning probabilities in PDBs. Probabilistic accuracy is defined as 1 - (1/|E|) \u03a3 |p_i - q_i|. Probabilistic accuracy is a scoring rule used in probabilistic logic programming systems like Problog. It is not a proper scoring rule but has convenient properties. For example, maximizing probabilistic accuracy in a database with training examples can yield different results compared to optimizing cross entropy and squared loss. SafeLearner is a probabilistic rule learning system that supports lifted inference by mining independent deterministic candidate rules using AMIE+ and executing joint parameter learning over all the rule probabilities. It extends ProbFOIL+ by using lifted probabilistic inference, scaling better than ProbFOIL+. SafeLearner can jointly learn probabilistic rules over a probabilistic KB, outperforming ProbFOIL+ in learning complex rules. SafeLearner outperforms ProbFOIL+ in probabilistic rule learning by using lifted inference and scaling well on large databases like NELL Sports Database. It can learn complex rules, uses rules within the background theory, and is faster due to disintegrating complex queries and caching query structures. The work discusses how SafeLearner outperforms ProbFOIL+ in probabilistic rule learning by disintegrating complex queries, caching query structures, and using lifted inference. Future advancements could include extending probabilistic rule learning to an open-world setting."
}