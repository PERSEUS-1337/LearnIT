{
    "title": "HJx54i05tX",
    "content": "We analyze weight-tied multilayer vanilla autoencoders with random weights, revealing phase transition phenomena as depth increases. Our study provides insights on approximate inference, sensitivity to parameter perturbations, and pitfalls in training initialization practices for deep autoencoders. The deep autoencoder can be trained without layer-wise pre-training or batch normalization, even with tanh activation and up to 200 layers. It is a fundamental tool in machine learning for unsupervised learning, dimensionality reduction, and generative models. Its reconstruction power is valuable for anomaly detection and image recovery. The study focuses on analyzing the learning dynamics of a highly nonlinear structure in deep autoencoders with random weights and weight tying. The research explores the reconstruction capability of vanilla autoencoders in a high-dimensional setting, aiming to understand the impact of depth on the model's performance. The assumptions made in the study are justified by the analytical tractability gained from randomness and the enforced autoencoding through weight tying. The text discusses the properties of deep neural networks with random weights and weight tying, as well as the analysis of random multilayer feedforward networks through statistical inference. Weight tying in autoencoders is explored, showing comparable performance with and without it. Random connections and symmetry are also observed in other neural models. The high-dimensional setting is common in recent statistical learning advances. In the high-dimensional setting of statistical learning, the study explores the performance of random weight-tied autoencoders in approximate inference. Previous works have established upper bounds on input-output relationships in neural networks, with recent extensions to ReLU activation functions. The research aims to precisely define the concept of approximate inference in this context. In the study of random weight-tied autoencoders, the research focuses on defining approximate inference for deep autoencoders compared to shallow counterparts. The deep autoencoder shows higher sensitivity to parameter perturbations, indicating a different candidate function space and expressivity for feedforward networks. This work establishes a connection between random network study and trainability. In this work, insights on initializing deep weight-tied autoencoders are derived and experimentally verified without the need for layer-wise pretraining, drop-out, or batch normalization. The study explores 200-layer autoencoders, a task not attempted in prior works. The analysis reveals a significant quantitative difference between weight-tied and weight-untied networks, with the weight tying constraint posing a non-trivial challenge. The application of Gaussian conditioning technique is used to address this issue and obtain Theorem 1, drawing from studies in spin glass theory. The conditioning technique, initially seen in TAP equations in spin glass theory, is extensively used in the approximate message passing algorithm literature. Deep random weight-tied autoencoders present a rich and distinct picture compared to feedforward networks, with three fundamental equations governing the analysis in the limit of infinite depth. Multiple phase transition phenomena are observed. In the context of deep random weight-tied autoencoders, the encoder and decoder biases are defined along with activation functions. The weights and biases are generated independently, following specific scaling rules. The analysis considers the asymptotic high-dimensional regime with certain finite constants. All activations are assumed to be Lipschitz continuous. The text discusses the assumptions of Lipschitz continuity for all activations in the context of deep random weight-tied autoencoders. It also introduces scalar sequences related to the autoencoders in Theorem 1. The main theorem in the context of deep random weight-tied autoencoders is stated, describing the behavior of encoder and decoder outputs in the limit as n approaches infinity. The theorem also discusses the autoencoder's output. The theorem in the context of deep random weight-tied autoencoders states that x, x, and x have simple descriptions tracked by scalar sequences {\u03c4, \u03b3, \u03c1}. Analyzing {\u03c4, \u03b3, \u03c1} is simpler than studying x directly. Numerical simulations show good agreement for dimensions of a few hundreds. The theorem holds for a broader class of distributions beyond Gaussian biases. The theorem for deep random weight-tied autoencoders simplifies the analysis by focusing on scalar sequences {\u03c4, \u03b3, \u03c1}. Analytical simplifications are made with certain regularity conditions on activations and input x. The range of \u03c1 and \u03b3 is discussed, with conditions for non-decreasing and Lipschitz activations. Various parameters are introduced, including \u03c4 \u2265 0, for further clarity in the analysis. The identity parameters \u03c3 0 and \u03d5 0 are introduced along with the parameter \u03c4 \u2265 0. The mapping \u03b3 \u2192 G (\u03b3, \u03c1) is shown for different \u03c1, \u03b2, \u03d5, and \u03c3 for \u03c4 2 = 1. The existence of non-trivial and stable fixed points is of interest in the analysis. The mapping \u03b3 \u2192 G (\u03b3, \u03c1) is analyzed for various parameters, showing the presence of stable fixed points. The solution landscape changes significantly with \u03b2, indicating potential phase transition behaviors. Detailed analysis in Appendix C.2 explores properties of the equations. Specific pairs of \u03d5 and \u03c3, such as ReLU \u03d5 and \u03c3, are highlighted for their results. Results from the analysis show phase transitions for specific pairs of \u03d5 and \u03c3: ReLU \u03d5 and \u03c3 have two transitions at \u03b2 = 2 and \u03b2 = 4. For \u03b2 < 2, convergence to \u03b3 = 0 and \u03c1 = 0 occurs. For 2 < \u03b2 < 4, divergence to \u03c1 = +\u221e happens with certain initializations. For \u03b2 \u2265 4, divergence to \u03b3 = +\u221e and \u03c1 = +\u221e is observed. Another pair, ReLU \u03d5 and tanh \u03c3, has transitions at \u03b2 = 2 and \u03b2 = \u03b2 0 (\u03c4 ) \u2208 (2, \u221e). Convergence to \u03b3 = 0 and divergence to \u03c1 = +\u221e occur for 2 < \u03b2 < \u03b2 0. For \u03b2 > \u03b2 0, divergence to \u03b3 = +\u221e and \u03c1 = +\u221e is seen. Lastly, tanh \u03d5 and \u03c3 exhibit transitions at \u03b2 = 1 and \u03b2 = \u03b2 0 (\u03c4 ) > 1, with convergence to \u03b3 = \u03c1 for \u03b2 \u2264 1. Results from the analysis reveal phase transitions for different pairs of activations. Specifically, for tanh \u03d5 and ReLU \u03c3, there are phase transitions at \u03b2 = 1 and \u03b2 = \u03b2 0 (\u03c4 ) > 1. The convergence to \u03b3 = \u03c1 occurs when \u03b2 \u2264 1, while for 1 < \u03b2 < \u03b2 0, convergence to \u03b3 = 0 and \u03c1 \u2208 (0, 1) is observed. When \u03b2 > \u03b2 0, convergence to \u03b3 > 0 and \u03c1 \u2208 (0, 1) occurs. The behavior of \u03b3 and \u03c1 depends on the choice of activations, especially the decoder activation \u03d5, and can be trivialized with certain activations like ReLU. The analysis reveals phase transitions for different pairs of activations, with implications explored in the next sections. Theorem 1 quantitatively assesses the performance of random weight-tied autoencoders in \"approximate inference\". The concept of reversibility is discussed, focusing on how well the decoder can reproduce the input from hidden layer values. Theoretical understanding is sought for empirical findings in feedforward networks. The infinite depth simplification is considered under Interpretation 2, with a formalized notion of E for decoder performance evaluation. The discussion delves into the performance evaluation of random weight-tied autoencoders in \"approximate inference\" and the concept of reversibility in decoder output reproduction. It highlights the implications of phase transitions for different activation pairs and questions the signal recovery capability of the model. The discussion evaluates the performance of random weight-tied autoencoders in \"approximate inference\" and reversibility in decoder output reproduction. It questions the signal recovery capability of the model, emphasizing phase transitions for different activation pairs. The analysis provides a negative answer to certain conditions required for the model, offering a more precise understanding of its limitations as depth increases. The shallow autoencoder case is discussed, showing that \u03b3 and \u03c1 are independent of \u03b2, with no phase transition as \u03b2 changes. In certain regimes, \u03b3, \u03c1, and \u03b3/\u221a\u03c1 can grow sublinearly with \u03b2, leading to signal dominance regardless of parameter choices. The deep autoencoder is more sensitive to parameter changes compared to the shallow one. A slight perturbation in the parameters can result in a larger perturbation in the signal's strength. The use of larger \u03b1 may amplify signals more effectively. The ReLU activation function exhibits extreme sensitivity to parameter changes, with even slight perturbations in \u03b2 drastically affecting \u03b3 and \u03c1. Careful parameter selection is crucial, especially in infinite depth scenarios. The deep autoencoder is sensitive to parameter changes, with larger \u03b1 amplifying signals effectively. Careful parameter selection is crucial in infinite depth scenarios to avoid signal strength diminishing or boosting to extremes. Sensitivity to perturbations is common in deep neural networks, impacting trainability of weight-tied autoencoders. Appropriate values of \u03b3, \u03c1, and \u03c4 can improve trainability, with numerical errors expected if values are infinite. The deep autoencoder's trainability is affected by parameter choices, with pitfalls like numerical errors or quick saturation if values are too large. Careful selection of \u03c3 2 W and \u03c3 2 b is crucial for faster progress, especially when L is large. The ReLU activation function with \u03b1 = 1 suggests using \u03c3 FORMULA12 for feedforward networks. Edge of chaos (EOC) initializations are also considered for improved trainability. The EOC initialization, relevant to the encoder part with activation \u03c3, enables better signal propagation in deep feedforward networks. Different initialization schemes are listed for each pair of \u03d5 and \u03c3, with \"EOC\" indicating whether the scheme is an EOC initialization. \"Trainable\" indicates better trainability in the beginning, while \"Slowed\" and \"Inf\" indicate specific conditions. The schemes with \u03d5 = tanh should be compared against FIG3. The experiments involved testing different initialization schemes with \u03b1 = 1 on a weight-tied vanilla autoencoder trained on the MNIST dataset. Two settings were used: Setting 1 with tanh activation and Setting 2 with identity activation. Training was done using mini-batch gradient descent without regularizations for 5 \u00d7 10^5 iterations. The experiments involved testing different initialization schemes with \u03b1 = 1 on a weight-tied vanilla autoencoder trained on the MNIST dataset. Two settings were used: Setting 1 with tanh activation and Setting 2 with identity activation. The learning rates were fixed at 5 \u00d7 10 \u22123 for Setting 1 and 3 \u00d7 10 \u22123 for Setting 2. The results are plotted in FIG5, and the evolution over a broader range of parameters can be found in Appendix D.2. The results of testing different initialization schemes on weight-tied vanilla autoencoders trained on the MNIST dataset are in good agreement with the hypothesis. Scheme 3 and 6 in Setting 2 are trapped with numerical errors, while in Setting 1 they saturate quickly at a high loss. The chosen loss is slightly different from the traditional loss x - x^2, with the normalized loss yielding slight improvements. The experiments show that the normalized loss allows for ease of interpretation. The experiments on different initialization schemes for weight-tied vanilla autoencoders trained on the MNIST dataset show that certain schemes lead to slower progress in training, while others produce meaningful reconstructions. Scheme 5 is highlighted as a special initialization that saves training time. Good signal propagation through the encoder is not enough for trainability, as some schemes only output an \"average\" of the training set. The experiments on different initialization schemes for weight-tied vanilla autoencoders trained on the MNIST dataset show that certain schemes lead to slower progress in training, while others produce meaningful reconstructions. Scheme 7 with tanh activation performs best in terms of reconstruction loss, while Scheme 4, a hybrid of ReLU and tanh activations, shows slight improvements over Scheme 1. The study confirms the hypothesis on the trainability of weight-tied autoencoders with tanh activation. The experiments provide quantitative answers and are enabled by an exact analysis via Theorem 1. Questions remain on relaxing simplifications and exploring parameter variations across layers. The study confirms the trainability of weight-tied autoencoders with tanh activation. Questions remain on covariance structure between outputs of distinct inputs and the network's Jacobian matrix. More work is needed to understand training dynamics beyond initialization. The autoencoder's inner layers compute autoencoding mappings represented by g's. Bold-face letters denote vectors or matrices, with C as an arbitrary constant. Various mathematical notations are used to denote norms, singular values, and random variables. Uniformly pseudo-Lipschitz sequences of functions are defined in the context of R^n. The text discusses the concept of uniformly Lipschitz sequences of functions in R^n and the notation for convergence of random variables and vectors. Key results are stated to prove Theorem 1, including Proposition 5 in an asymptotic setting. In an asymptotic setting with a sequence of Lipschitz functions, a random weight-tied \"autoencoder\" with a single hidden layer is considered. The mapping function g is not separable, distinguishing it from a shallow autoencoder. This differs from previous investigations and techniques discussed in Pennington & Worah (2017) and Louart et al. The proposition and corollary highlight the unique characteristics of this setup. The random weight-tied multi-layer autoencoder is analyzed by understanding the structure and mapping function g. The study progresses from inner layers to outer layers, repeating the procedure L times. Information about inner layers is needed for calculations in outer layers, as shown in Corollary 6. The assumption of independent weights at different layers is important. The assumption of independent weights at different layers is crucial for the proof of Theorem 1 and Proposition 5 in the analysis of the random weight-tied multi-layer autoencoder. The main challenge lies in the correlation between the input and weight-tied structure, which requires conditioning on a linear constraint to achieve independence. This conditioning allows for the distribution of the weight component to be a conditional projection plus an independent Gaussian component. The assumption of independent weights is crucial for the proof. The Gaussian Poincar\u00e9 inequality is used multiple times. Conditioning on F is equivalent to conditioning on the linear constraint y = W \u03c3 (u). Bayati & Montanari (2011) discuss the projection onto \u03c3(u) and the corresponding orthogonal projection. The proof focuses on \u03a6n and shows that \u03c4, \u03b3, and \u03c1 are uniformly bounded as n \u2192 \u221e. The first term in \u03a6n is analyzed, where yd = \u03c4z for some z \u223c N(0, Im). The mapping y \u2192 y, \u03d5(g(y + b))/m is uniformly pseudo-Lipschitz. The proof focuses on the convergence of \u03a6n as n \u2192 \u221e. By analyzing the second term in \u03a6n, it is shown that \u03c1 is uniformly bounded. The proof concludes by showing that W is bounded with high probability. The proof shows that the mapping is uniformly pseudo-Lipschitz, and by Theorem 7, it is proven that the mapping is uniformly pseudo-Lipschitz. This is achieved by analyzing the convergence of \u03a6n as n \u2192 \u221e and showing that W is bounded with high probability. Lemma 8 states that g is uniformly Lipschitz with high probability. There exists a finite constant c > 0 such that the event E_N tends to 0 as N approaches infinity. This, combined with the union bound, proves the claim for a specific value of L. Lemma 8 proves that g is uniformly Lipschitz with high probability. The claim for a specific value of L is proven using the union bound. Lemma 9 shows that for each \u2265 0, \u03c4 +1 and \u03c4 +1 are finite and strictly positive. Theorem 1 is then ready for proof. Claim (a) of the theorem follows directly from Lemma 9. The forward pass through the encoder is the same as in random feedforward networks. Claims are proven by induction, with high probability for any bounded by the law of large numbers. Lemma 9 shows convergence to a non-zero constant. Theorem 1 is ready for proof, with Claim (a) following directly from Lemma 9. Assuming H +1 for some, we prove H. Recall that g +1 is independent of \u0398 = {W, b, v, x \u22121, z \u22121}. Consider N \u2208 N and let E N +1 denote the event as defined in Lemma 8. On the event \u00acE N +1, with respect to the randomness of \u0398, it is shown that DISPLAYFORM15. Due to Lemma 8 and Chebyshev's inequality, for any > 0, DISPLAYFORM18 where o n (1) \u2192 0 as n \u2192 \u221e. Invoking H +1, it follows that DISPLAYFORM19. Letting n \u2192 \u221e then N \u2192 \u221e, we have DISPLAYFORM20. After proving H +1, we deduce \u03b3 \u03b3 +1 and \u03c1 \u03c1 +1. The claim about \u03c4 \u22121 z 1 being replaced with x \u22121 in Claim (b) is easily recognized by examining the proof of Corollary 6. Simple simulations are performed to verify Theorem 1 at finite dimensions, including computing \u03b3 and \u03c1 values. Theorem 1 predicts that \u03b3 and \u03c1 values will be close for different dimensions. Results show good agreement with normality assumption for the variation component in x. Autoencoder simulations with different weight distributions also show good quantitative agreements. The simulations show good quantitative agreements between non-Gaussian distributions and the prediction in Theorem 1, even though the theorem is only proven for Gaussian weights. Figures 6 and 8 display the agreement among different parameters for various distributions of weights. Double integrals are used to compute \u03b3 and \u03c1, with simplifications possible for certain activation functions like ReLU. The text discusses the computation of \u03b3 and \u03c1 using double integrals, with simplifications for certain activation functions like ReLU. The procedure finds a stable fixed point by iteratively updating \u03b3 and \u03c1 until convergence. Properties of \u03b3 and \u03c1 are proven, and fixed point equations are \u03b3 = G (\u03b3, \u03c1) and \u03c1 = R (\u03b3, \u03c1). The equations are studied with Lipschitz continuous, non-decreasing functions \u03d5 and \u03c3. The text discusses implications of propositions related to ReLU and tanh activations, with proofs deferred to Section C.3. The propositions apply to broader classes of functions, and initialization for \u03b3 and \u03c1 is explained in different contexts. The text also defines fixed points for \u03b3 and \u03c1, and introduces notation for partial derivatives. Proposition 10 is specific to ReLU activations. Proposition 10 discusses the stability of fixed points for ReLU activation functions under certain conditions. It states that at \u03b3 = 0 and \u03b2 = 2, \u03c1 = 0 is a stable fixed point, and any \u03c1 is a stable fixed point. Additionally, it discusses the behavior of fixed points as \u03b3 approaches infinity and provides conditions for stability based on the value of \u03b2E {\u03c3 (\u03c4 z 1 )}. The proposition discusses the stability of fixed points for ReLU activation functions under certain conditions. It mentions that for \u03b2 < 2, convergence to \u03b3 = 0 and \u03c1 = 0 occurs with any initialization. For \u03b2 \u2208 (2, 4), convergence to \u03b3 = 0 and divergence to \u03c1 = +\u221e can happen with certain initializations. When \u03b2 \u2265 4, divergence to \u03b3 = +\u221e and \u03c1 = +\u221e occurs with any non-zero initialization. For \u03b2 < 2, convergence to \u03b3 = \u03c1 = 0 happens with any initialization. For \u03b2 \u2208 (2, \u03b2 0 ), convergence to \u03b3 = 0 and divergence to \u03c1 = +\u221e can occur with any non-zero initialization. For \u03b2 > \u03b2 0 , divergence to \u03b3 = +\u221e and \u03c1 = +\u221e happens with any non-zero initialization. When \u03b2 = \u03b2 0, \u03b3 remains unchanged from initialization, leading to divergence to \u03c1 = +\u221e. ReLU is unbounded at infinity with a derivative bounded away from zero, allowing \u03b3 and \u03c1 to grow infinitely. In contrast, for bounded functions like \u03d5 = tanh, \u03b3 and \u03c1 are limited. If \u03d5 is thrice-differentiable with specific conditions, \u03b3 = 0 and \u03c1 = 0 is a fixed point. If \u03d5 is bounded, 0 \u2264 \u03c1 \u2264 C and |\u03b3| \u2264 C/\u03c4. When \u03b2 = \u03b2 0, \u03b3 remains unchanged from initialization, leading to divergence to \u03c1 = +\u221e. ReLU is unbounded at infinity with a derivative bounded away from zero, allowing \u03b3 and \u03c1 to grow infinitely. In contrast, for bounded functions like \u03d5 = tanh, \u03b3 and \u03c1 are limited. If \u03d5 is thrice-differentiable with specific conditions, \u03b3 = 0 and \u03c1 = 0 is a fixed point. If \u03d5 is bounded, 0 \u2264 \u03c1 \u2264 C and |\u03b3| \u2264 C/\u03c4. Additionally, for any given \u03c1 > 0, there exists a finite \u03b2 * such that if \u03b2 \u2264 \u03b2 * , then \u03b3 = 0 is the only stable fixed point, while if \u03b2 > \u03b2 * , \u03b3 = 0 becomes unstable with an additional stable fixed point at \u03b3 > 0 finite. Lemma 12 establishes properties of the tanh function, where tanh (u) is between 0 and 1, and certain conditions apply. Proposition 11 applies when \u03b2 < 1, leading to specific outcomes. Proposition 11 suggests convergence to specific values based on different conditions for \u03b2, \u03c1, and \u03b3. The phase transitions for \u03c1 and \u03b3 occur at \u03b2 = 1 and some \u03b2 > 1 respectively. The proximity of these phase transitions is influenced by certain conditions. The phase transitions of \u03c1 and \u03b3 are influenced by specific conditions, with Claim (a) indicating that the phase transition of \u03c1 never occurs before that of \u03b3. Taking \u03d5 = \u03c3 = tanh and \u03c4 2 \u2248 0 can make the phase transitions close. If E {\u03c3 (\u03c4 z 1 )} > \u03d5 (0), \u03b3 and \u03c1 will share the same phase transition location. Proof of Proposition 10 shows G (0, \u03c1) = 0 and R (0, \u03c1) = \u03b2\u03c1/2. Study of \u03c1 when \u03b3 \u2192 \u221e is calculated in Table 1. In Scheme 1, 4, and 2, reconstructions are selected based on ranking reconstruction loss. Images from MNIST and EMNIST Letters test sets are used, showing recognizable digits and letters despite imperfect quality even after intensive training. The reconstructions from different schemes show recognizable digits for digit images but hardly recognizable letters for letter images. The trained networks capture low-dimensional structures of the data, except for Scheme 7 under Setting 2 which behaves differently due to its tanh network nature. The evolution of test reconstruction loss is shown on the plane \u03c3 2 W , \u03c3 2 b, with patterns emerging in good agreement with hypotheses. Evolution of neural networks is influenced by the values of \u03c3 2 W, with faster progress near \u03c3 2 W = 2 and slower evolution at higher values. The choice of activation function \u03d5=tanh also affects the speed of evolution, particularly when \u03c3 2 W is 1."
}