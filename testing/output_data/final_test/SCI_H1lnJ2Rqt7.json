{
    "title": "H1lnJ2Rqt7",
    "content": "Stochastic Gradient Descent (SGD) methods are commonly used to train neural network (NN) models with randomly selected batches. Large batch training can lead to accuracy degradation, poor generalization, and vulnerability to adversarial attacks. To address these issues, a novel large batch training method is proposed that combines adversarial training and second-order optimization techniques to adaptively change batch size during training. Our new method combines adversarial training and second-order optimization techniques to adaptively change batch size during training, outperforming existing solutions in accuracy and SGD iterations without requiring additional hyper-parameter tuning. Finding the right NN architecture for a specific application involves extensive hyper-parameter tuning and architecture search on large datasets. Using large distributed processor clusters can help address training delays, but large batch training can lead to accuracy degradation, poor generalization, and vulnerability to adversarial attacks. Various solutions have been proposed to mitigate these drawbacks. One solution to reduce the brittleness of SGD to hyper-parameter tuning is to use second-order methods like Newton/quasi-Newton methods, which have been shown to outperform SGD for training NNs. These methods use a second-order Taylor series approximation to the loss function to obtain curvature information, resulting in parameter-free optimization without the need for learning rates. Second-order methods like Newton/quasi-Newton have been shown to outperform SGD for training NNs, but they can exacerbate the large batch problem by being attracted to local minima. Early attempts at using second-order methods for training convolutional NNs have not been successful. Finding a regularization scheme to avoid local minima during training could resolve these issues. Adversarial learning methods are a special case of robust optimization methods. Recent studies have shown that robust optimization methods converge to flatter points in the optimization landscape, making models more robust to adversarial perturbations. To address the brittleness of SGD with large batch sizes, a new method combining second-order information regularization and robust optimization is proposed. This adaptive batch size approach utilizes curvature information from the Hessian matrix to regularize against sharp minima, leading to significantly improved results in large batch training of neural networks. The proposed method combines second-order information regularization with robust optimization to improve testing performance in large batch size training. It automatically adjusts batch size and learning rate based on Hessian information, leading to lower total SGD updates compared to state-of-the-art methods. This approach does not require additional hyper-parameter tuning and is proven to be convergent. The ABS method combines second-order information regularization with robust optimization for improved testing performance in deep learning. It adjusts batch size and learning rate based on Hessian information, achieving equal or better test performance compared to small batch SGD. Additionally, the ABSA method combines Hessian-based adaptive batch size with robust optimization, showing significantly better test performance with little computational overhead. Extensive testing on various datasets and neural networks validates the effectiveness of these strategies. The proposed algorithm achieves equal or better test accuracy compared to the state-of-the-art on various datasets and neural networks, using the same hyper-parameters for all experiments. It reduces computational overhead by approximating the Hessian operator and requires significantly fewer SGD updates. The paper introduces Hessian information and adversarial training in adaptive batch size training, with extensive testing on various datasets. Limitations include the overhead of backpropagating the Hessian and lack of support in existing frameworks for memory-efficient backpropagation. The algorithm achieves equal or better test accuracy compared to the state-of-the-art on different networks and datasets, with reduced computational overhead. Our method requires ten Hessian matvecs for power method iterations to reach a tolerance of 1e-2, resulting in additional overhead measured in wall clock time. The power iteration only needs to be done at the end of each epoch, reducing this overhead significantly. The theory holds for convex problems, with more analysis needed for non-convex settings. Future work could explore theoretical guarantees for non-convex settings similar to AdaGrad. Optimization methods based on SGD are effective for training NNs due to their ability to escape saddle-points and \"bad\" local minima. Asynchronous methods have been developed to break the sequential nature of weight updates in synchronous SGD, but reproducibility remains a challenge depending on the number of processes used. Large batch size training in synchronous SGD can increase parallelization opportunities and reduce total training time by distributing computations efficiently. However, it often leads to sub-optimal test performance due to attraction to local minima or sharp curvature directions. Scaling the learning rate has been proposed as a solution to this issue. In large batch size training, scaling the learning rate can achieve the same testing accuracy for large batches. ResNet-50 model on ImageNet dataset showed baseline accuracy recovery up to batch size of 8192. However, this approach does not generalize to other networks like AlexNet or tasks like NLP. Adaptive learning rate method (LARS) in BID37 allowed scaling training to a batch size of 32K with hyper-parameter tuning. Smith et al. proposed a hybrid increase of batch size and learning rate to accelerate training by selecting a strategy to \"anneal\" the batch size during training. Recent work has explored methods such as mix-precision training and anisotropic noise injection to escape sharp minima during large batch training. Adversarial training has also been shown to help \"regularize\" against sharp minima, with promising testing performance results. The link between robust optimization and regularization has been theoretically proven in previous studies. Ridge regression and Lasso have been used in robust optimization for model training, showing increased robustness to perturbations compared to normal SGD training. In a supervised learning framework, the goal is to minimize a loss function with model weight parameters. SGD is typically used with mini-batches and a learning rate. Large batch size training increases batch size to large values. Smith & Le (2018) consider learning rate and batch size important factors. Large batch size training increases noise injection during optimization, with a large learning rate and small batch size equivalent to high noise injection. The noise magnitude is proportional to the product of learning rate, time, and batch size. To benefit from small batch training, the learning rate should increase with batch size. Annealing behavior can be achieved by increasing batch size. The need for annealing in training can be understood by considering a convex problem. A method called Adaptive Batch Size (ABS) is proposed to utilize second order information to adaptively change the batch size. This method aims to regularize against local minima with poor generalization by adjusting the batch size based on the landscape of the loss function. Adaptive Batch Size (ABS) proposes adjusting batch size based on loss landscape to avoid local minima with poor generalization. Parameters include learning rate, batch size, eigenvalue ratio, and perturbation magnitude for adversarial training. The ABS algorithm adjusts batch size based on loss landscape to avoid poor generalization. The ABSA method combines ABS with adversarial training for regularization against sharp minima, leading to more stable models. The ABSA method combines ABS with adversarial training using the Fast Gradient Sign Method to solve a min-max problem for neural networks. The learning rate adaptively changes based on the Hessian eigenvalue to maintain the same noise level as in baseline training. This combined approach achieves better accuracy compared to traditional training strategies. The combined approach of ABS with adversarial training using the Fast Gradient Sign Method achieves better accuracy with fewer SGD updates compared to traditional training strategies. The ABS algorithm is proven to converge for strongly convex problems, with a theorem showing the expected optimality gap under certain conditions. The convergence rate of Alg. 1 can be improved with an adaptive b t, leading to faster convergence than basic SGD. Empirical results on logistic regression show the performance of ABS and ABSA methods on various datasets and NN models, compared to state-of-the-art methods for large batch training. Key metrics for comparison include final accuracy and total number of updates, aiming for higher accuracy with fewer SGD updates. Hyper-parameters remain unchanged in the algorithm, using the same parameters as the baseline model. The algorithm's performance is compared to baseline models using the same parameters. Results are shown for ABS (ABSA) compared to BaseLine, FB BID14, and GG BID29, as well as on TinyImageNet and ImageNet datasets. The method's superior performance requires backpropagating the Hessian, with discussion on using approximate Hessian information to reduce costs. Results on SVHN and Cifar-10/100 datasets are discussed, noting differences in batch sizes between GG and ABS (ABSA). The reported batch size represents the maximum during training, with a focus on the number of weight updates for comparison. The comparison of different models on various datasets shows that ABS achieves similar accuracy as BL, with a significantly lower number of parameter updates. ABSA generally outperforms other methods, confirming its superior results. ABSA method is tested on more challenging datasets like TinyImageNet and ImageNet, using the same hyper-parameters. Results show ABSA outperforms other methods, achieving better test accuracy with fewer SGD iterations. TinyImageNet has 200 classes with 500 images per class, making it easy to overfit. ABSA performance is about 1% higher than other methods on TinyImageNet. The ABSA method outperforms other methods on TinyImageNet, achieving about 1% higher test accuracy. The method does not tune hyper-parameters to avoid increasing training time, and explores using approximate second order information with a block Hessian approximation to reduce computational cost. The block Hessian approximation is tested for C1 and C4 models on Cifar-10 and Cifar-100 datasets. Results show that using a block Hessian can estimate the trend of the full Hessian with similar performance but lower computation cost. An adaptive batch size algorithm based on Hessian information is introduced to speed up the training process of neural networks. Our method combines a batch size algorithm based on Hessian information with adversarial training for NNs. Extensively tested on various datasets and models, it automatically adjusts batch size and learning rate, reducing parameter updates and improving generalization performance without additional hyper-parameter tuning. Block Hessian approximation can reduce computation cost while estimating the full Hessian trend. A block Hessian can approximate the full Hessian trend to reduce NN training time. The gradient function of L is Lipschitz continuous with Lipschitz constant L g. Each individual gradient is an unbiased estimation of the true gradient. The text discusses the proof of Theorem 1 in the context of batch size optimization for stochastic gradient descent. It includes lemmas and a toy example of binary logistic regression on a mushroom classification dataset. The gradient function of L is Lipschitz continuous with Lipschitz constant L g. The text discusses training losses of different optimization algorithms using gradient descent with varying step sizes. It outlines the datasets used, including SVHN, Cifar, and TinyImageNet, with details on the number of samples and classes in each dataset. The dataset includes 500 training and 50 validation images per class, with image sizes of 64 \u00d7 64. The ILSVRC 2012 dataset consists of 1000 image classes, with 1.2 million training images and 50,000 validation images. Different convolutional neural network models are implemented, such as an AlexNet-like model trained for 20 epochs and ResNet18 trained for 90 epochs on the Cifar-10 dataset. Data augmentation is not used in these models. Several models were trained on different datasets with varying epochs and learning rates. Data augmentation was implemented in some cases. For example, ResNet50 on TinyImageNet dataset was trained for 120 epochs with a learning rate of 0.1, while AlexNet on ImageNet dataset was trained for 90 epochs with a learning rate of 0.01. Data augmentation was used in both cases. The training strategy includes different approaches such as standard training procedure, linear scaling rule with warm-up stage, increasing batch size, and adaptive batch size strategy with or without adversarial training using Fast Gradient Sign Method. Hyper-parameters are set for all experiments with a slight adjustment for SVHN dataset. The training strategy includes various approaches such as standard training, linear scaling rule, increasing batch size, and adaptive batch size strategy with or without adversarial training. The computation of Hessian information involves approximating wall-clock time using parallel processes and measuring forward/backward calculations on a V100 GPU. The total time for computation on a V100 GPU includes forward and backward propagation, communication between machines, and Hessian computation overhead. Latency and bandwidth values are based on NERSC's Cori2 supercomputing platform. An example of simulated computation time cost for I3 on ImageNet is provided. Communication time formula is simplified for large processes and small latency terms. Simulation time for I3 on ImageNet with 512 processes is reported in Table 3. GG increases batch size by a factor of 10 at epoch 30, 60, and 80. Batch size per GPU core is set to 16 for SGD and 8 for Hessian computation due to memory limit. The batch size per GPU core is set to 16 for SGD and 8 for Hessian computation due to memory limit. The total batch size used for Hessian computation is 4096 images. The Hessian computation time for ABS/ABSA is 103.5 s, even though it is not efficiently implemented in existing frameworks. ABS/ABSA is still faster than BL, even with additional Hessian overhead. Additional computational overhead of adversarial computations is added to the ABSA method. In this section, additional empirical results are presented for the I3 model on ImageNet. Test performance drops from epoch 60 to 80 due to overfitting, but ABSA shows the best performance with less overfitting despite higher training loss."
}