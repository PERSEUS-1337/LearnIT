{
    "title": "BJ4AFsRcFQ",
    "content": "Recent image style transferring methods have achieved arbitrary stylization by using feed-forward networks with feature transformers. However, these methods did not consider multi-scaled style or the dependency between transformed feature statistics, leading to partially and inaccurately transferred styles in generated images. The proposed total style transferring method addresses the limitation of partial style transfer by transferring multi-scaled feature statistics through a single feed-forward process. It transforms feature maps of a content image into those of a target style image, considering inter-channel and inter-scale correlations. The transformed feature maps are inserted into the decoder layer using skip-connections, resulting in a stylized image. Previous methods improved image generation speed but did not consider multi-scaled styles or dependencies between feature statistics. Li et al. proposed a method using cascade networks to transfer multi-scaled style characteristics in a single feed-forward process. They also introduced a whitening and coloring transformer to transform feature map correlations. However, their approach requires multiple passes and does not guarantee style preservation through subsequent networks. The proposed method in this paper introduces an improved feed-forward network structure and a total style transfer technique to preserve multi-scaled style characteristics without interference between scales. It transforms both intra-scale and inter-scale statistics of feature maps efficiently through a single feed-forward pass, ensuring style preservation through subsequent networks. The proposed method introduces a feed-forward network structure for multi-scaled style transfer, considering correlations within and between scales. The feature transformer aligns the feature map with the target style, resulting in an output closer to the target style. The decoder learns with multi-scaled style loss, ensuring style preservation in the generated output. Our method introduces multi-scaled style transfer using skip-connections in a single feed-forward network, optimizing the merging of style patterns across different scales. This approach eliminates the need for multiple feed-forward passes in cascade networks, resulting in better style expression in the generated output image. In Sec. 3, a multi-scaled style transforming method is described, optimizing style patterns across different scales using skip-connections in a single feed-forward network. Gatys et al. utilized deep feature maps to represent image content and style features, improving image stylization through pixel-wise optimization and matching Gram matrix as a maximum mean discrepancy problem. Instance-based feed-forward neural networks were employed to enhance style quality and speed up image generation. The previous method BID2 was improved by using instance normalization (IN) BID19 to enhance style quality. Dumoulin et al. extended the single style network to transfer multiple styles by using conditional instance normalization (CIN) layers. Huang et al. proposed using adaptive instance normalization (AdaIN) layer for arbitrary style transfer, adjusting mean and standard deviation of content feature map to target style feature map. The target style feature map is decoded into an output image using covariance for improved stylization. Multi-scaled style transfer is achieved through intra-scale and inter-scale feature transform with a single feed-forward network. The single-scale correlation alignment of CORAL BID17 or WCT Li et al. (2017b) performs style normalization and stylization sequentially using covariance matrices and spatial features. Our intra-scale transform method applies single-scale transform independently to each scaled feature for different layers. Transformed features are inserted into the decoder through skip-connection to consider inter-channel and inter-scale correlations for style transfer. This approach differs from previous methods like CORAL BID17 or WCT Li et al. (2017b) by considering both types of correlations. The method involves performing a feature transform considering inter-channel and inter-scale correlations by applying the intra-scale feature transform to concatenated content and style image features instead of independently applying to each scaled feature. The transformed features are then split and downsampled before being inserted into the subsequent decoder through skip-connection for style transfer, considering both types of correlations. The method involves downsampling features into H c,i \u00d7 W c,i for subsequent decoder input through skip-connection. Skip-connected multi-scale features are merged in the decoding process using learnable convolution layers to improve image quality. This approach differs from previous cascade schemes by utilizing a single decoder network. Our method utilizes a single encoder/decoder network with parallel transformers for each scale feature, optimizing the merging of multi-scaled styles. In contrast to previous cascade schemes, we avoid degradation by transferring styles optimally. We use Mean-Std loss with modifications for generating stylized images, ensuring consistency with AdaIN transformer. Our method utilizes a single encoder/decoder network with parallel transformers for each scale feature, optimizing the merging of multi-scaled styles. We adopt Mean-Std loss BID5 with modifications to ensure consistency with AdaIN transformer. Instead of using Mean-Std loss as is, we incorporate Mean-Covariance loss to consider interchannel and inter-scale correlations. The style loss is calculated based on the square root of the Frobenius distance between covariance matrices of output and target style images. We use VGG16 feature extractor BID16 as the encoder and a mirror-structured network as the decoder in our style transfer network. Our decoder network for style transfer has larger channels in skip connections compared to previous methods. Style loss is calculated using specific layers, while content loss is calculated using another layer. Training data sets include MS-COCO and Painter By Numbers, each with around 80,000 images. Additional style images were used to test the proposed method. Images were resized to 256 pixels and randomly cropped during training. The network for style transfer was trained using random image pairs, with different batch sizes and epochs. Two networks were trained with varying numbers of style images, resulting in different stylized output images. The output images showed similar texture styles to the target images, demonstrating the effectiveness of the feature transform method. The network trained for style transfer showed improved texture style in the output images, especially with inter-scale transform. Skip-connections were tested in three different networks, with the one incorporating multiscale feature transformers and skip-connections showing the best results. The network for style transfer demonstrated enhanced texture style in the output images, particularly with inter-scale transform. Skip-connections were evaluated in three networks, with the one utilizing multiscale feature transformers and skip-connections yielding the most favorable outcomes. The stylized image output from the networks showcased improved color tone and small patterns as the number of skip-connections increased. The contributions of skip-connected features and decoded features from previous decoder layers were analyzed by observing loss gradients during the learning process. The skip-connected feature with target style dominantly affects the decoder learning at the start of training due to random initial weights in the previous decoder layer. As training progresses, gradient values for skip-connected and decoded features become similar, leading to their equal utilization in generating multi-scaled style images. Gradient values for skip-connected features are smaller than those for decoded features in the latter decoder layer. The skip-connected feature with target style has a significant impact on decoder learning at the beginning of training. As training progresses, gradient values for skip-connected and decoded features become similar, leading to their equal utilization in generating multi-scaled style images. The decoded feature of the latter decoder layer already accumulates multi-scaled styles from the previous skip-connection, resulting in less impact of the skip-connected feature. However, using skip-connection with the stylized feature of smaller scale has a certain effect on the result image in color tone matching. The amplitude of loss gradients with respect to convolution weights in skip-connected decoder layers during learning process is shown in Figure 6. The skip-connected (transformed) feature highly affects the decoder in initial iterations, but both decoded and transformed features have similar impact as training progresses. The latter decoder layer is less affected by skip-connected feature compared to the former layer. Our intra/inter-scale feature transform methods generate images that are somewhat degraded compared to the online optimization method BID2. Our multi-scaled style transfer method considers inter-channel and inter-scale correlation, resulting in generated images with texture detail and color tone more similar to the target style compared to single-scale methods. Our approach minimizes style loss between target style and output images, leading to styles closer to the target compared to methods that focus on reconstruction loss of content images. Additionally, our method outperforms Avatar-Net in performing multi-scale feature transform with a single decoder network. Our method, inspired by Sheng et al. (2018), utilizes VGG19 network for encoding and decoding, incorporating style loss for image-level reconstruction. In comparison to Avatar-Net, our approach generates stylized images with detailed content shapes and multi-scaled strokes, while Avatar-Net produces images with blurred content and color patterns. The selection of patch size is crucial in Avatar-Net, whereas our method does not require such adjustments. Our method, inspired by Sheng et al. (2018), utilizes VGG19 network for encoding and decoding, incorporating style loss for image-level reconstruction. In comparison to Avatar-Net, our approach generates stylized images with detailed content shapes and multi-scaled strokes. Our method achieved the lowest style loss with inter-scale feature transformer, making it extendable to arbitrary style transfer. Our method achieved the lowest style loss with inter-scale feature transformer, showing a trade-off between transferring style and preserving content in feedforward network methods. The results indicate that inter-scale correlation influences both the style and content of an image. Our method achieved 31% less encoder/decoder feed-forward time and 4% fewer parameters than the existing cascade network scheme. The total style transfer network utilizes multi-scale features to generate an image through a single feed-forward network, transferring style characteristics effectively. Our total style transfer network achieved the lowest style loss by utilizing multi-scale features and skip-connections in the feed-forward network structure. This modification allowed for the generation of multi-scaled style images without the need for multiple feedforward networks, reducing test time by 31% and memory consumption by 4% compared to cascade network schemes."
}