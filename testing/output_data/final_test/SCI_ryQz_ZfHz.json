{
    "title": "ryQz_ZfHz",
    "content": "Effectively inferring discriminative and coherent latent topics of short texts is a critical task for real-world applications. Traditional topic models face challenges due to data sparsity and complex inference algorithms. A novel model called Neural Variational Sparse Topic Model (NVSTM) is proposed, based on Sparse Topical Coding (STC) with auxiliary word embeddings for improved representation generation. The Variational Autoencoder (VAE) approach enhances model inference efficiency, allowing for easy exploration of extensions. Experimental results demonstrate the effectiveness and efficiency of NVSTM on various datasets. The effectiveness and efficiency of the Neural Variational Sparse Topic Model (NVSTM) are demonstrated on various datasets like onWeb Snippets, 20Newsgroups, BBC, and Biomedical datasets. Short texts are prevalent on the internet, posing challenges for traditional topic models due to data sparsity and complex inference algorithms. Previous works have introduced new techniques such as word embeddings and neural variational methods to address these challenges. Word embeddings and neural variational inference have been introduced to topic models to enhance topic modeling. However, these models often rely on computationally expensive inference procedures like Markov Chain Monte Carlo, making it difficult to explore extensions quickly. Neural variational inference, utilizing deep neural networks, has emerged as a powerful approach for unsupervised learning of complex distributions. The Neural Variational Sparse Topic Model (NVSTM) is proposed as a model for short texts, based on a sparsity-enhanced topic model STC and parameterized with neural networks. Trained with VAE, it combines the advantages of sparse topic models and deep neural networks. The Neural Variational Sparse Topic Model (NVSTM) combines sparse topic models and deep neural networks, utilizing auxiliary word embeddings to enhance short text representations. Experimental results show its superiority in topic coherence and text classification accuracy. The paper discusses related work, details of NVSTM, experimental results, and conclusions. Topic models like BID0, BID2, and BID12 are commonly used for tasks like information retrieval and document classification. While these models perform well on long texts with rich word co-occurrence data, they struggle with short texts. Efforts have been made to address data sparsity in short texts, with models like BID21 and BID10 introducing sparsity in document-topic and topic-term distributions. Inspired by Dirichlet priors, these models aim to extract focused topics and words by imposing sparsity constraints. Sparse topic models like BID6 and STC aim to extract focused topics and words by imposing sparsity constraints. However, the complicated inference procedures limit their applications. Topic models incorporating word embeddings, such as BID5, treat documents as collections of word embeddings and topics as multivariate Gaussian distributions in the embedding space. The assumption of topics being unimodal in the embedding space may not be appropriate. The BID7 proposed Latent Concept Topic Model (LCTM) and BID15 proposed Latent Feature Topic Modeling (LFTM) to model topics as distributions of concepts and incorporate word embeddings as latent features. BID22 introduced a correlated topic model using word embeddings to directly model topic correlation in the continuous word embedding space. However, these models face challenges in rapidly exploring extensions. Neural Variational Inference for topic models, utilizing a neural network to approximate the posterior of a generative model, has also been proposed by BID8 and BID17. The variational autoencoder (VAE) is a popular neural variational inference approach for building generative models with black-box inference processes. Efforts have been made to improve topic modeling with VAE, such as the auto-encoding variational Bayes (AEVB) method for latent Dirichlet allocation (LDA) and alternative neural approaches for parameterized distributions over topics. These methods aim to simplify the inference process and address issues like Dirichlet priors and component collapsing. The text discusses the development of a novel neural variational sparse topic model (NVSTM) based on VAE for short texts, combining neural networks and sparsity-enhanced topic models. It introduces Sparse Topical Coding (STC) and then proposes NVSTM, focusing on the inference process. The text introduces Sparse Topical Coding (STC) for representing documents and words in a low-dimensional topic space using a topic dictionary. The generative process in STC reconstructs word counts from a linear combination of topic bases, aiming for sparse word codes. The composite distribution in STC is super-Gaussian to achieve this sparsity. The text introduces Sparse Topical Coding (STC) for representing documents and words in a low-dimensional topic space using a topic dictionary. The composite distribution in STC is super-Gaussian, aiming for sparse word codes. To simplify calculations, the document code can be obtained via an aggregation of individual word codes. Black box inference methods are introduced to address complexity in STC, leading to the development of NVSTM based on VAE with word embeddings. The model NVSTM follows a generative process for each document, using a variational posterior Laplace to approximate word codes. It replaces super-Gaussian with a uniform distribution and incorporates word semantic information via word embeddings in a topic dictionary neural network. The topic dictionary neural network enriches feature space for short texts by incorporating word semantic information via word embeddings. It consists of a word embedding layer and topic dictionary layers that aim to convert word embeddings to a topic dictionary. The output is normalized through a simplex projection to conform to the framework of STC. The simplex projection normalizes each column of the topic dictionary, ensuring sparsity, non-negativity, and unity. Variational Autoencoder (VAE) is utilized for neural variational inference in the model. Variational Autoencoder (VAE) is a popular deep generative network that bridges the gap between neural networks and probability generative models. It consists of an encoder network, a decoder network, and a loss function. The encoder network parametrizes the approximate posterior, while the decoder network outputs the observed data with generative parameters. The VAE utilizes a multilayer perceptron network for encoding and decoding. The ELBO is rewritten with a regularizer for KL divergence and a reconstruction loss. Optimization is done with stochastic gradient descent, using a reparameterization trick to handle non-continuous variables. The VAE inference process for NVSTM involves using a function s derived from the encoder network to allow reconstruction error flow. The output s is constrained to be non-negative for interpretable word codes. The variational lower bound is obtained using the reparameterization trick, resulting in a closed-form objective function that can be efficiently solved with SGD. Algorithm 1 outlines the detailed algorithm, and experiments are conducted to evaluate the model's performance. The experiments conducted evaluate classification accuracy, sparse ratio of latent representations, quality of extracted topics, and document representations on four datasets: 20Newsgroups, Web Snippet, BBC, and Biomedical. The datasets vary in size and content, with preprocessing steps applied to clean the data before analysis. The Biomedical dataset consists of 20000 paper titles from 20 different MeSH categories. Words with a document frequency less than 3 are removed during preprocessing, resulting in 19989 documents with 20 classes. The model is compared with five topic models, including a classical probabilistic topic model and STC, a sparsity-enhanced topic model. In this study, three different topic models were utilized: STC with specific regularization constants, NTM with a learning rate and regularization factor, and DocNADE with various parameters such as hidden size and learning rate. These models were compared with classical probabilistic topic models on a Biomedical dataset. The study utilized different topic models with specific parameters such as hidden size, learning rate, and regularization factors. The GaussianLDA BID5 technique was implemented in Python using TensorFlow with pre-trained word embeddings from GloVe. The model showed stability with a hidden layer size of 500. The study evaluated the effectiveness of document representation learned by NVSTM through text classification tasks on various datasets using multi-class SVM. Different percentages of documents were used for training and testing across datasets. Classification accuracy under different methods and settings on the number of topics was reported in TAB2. The study compared the effectiveness of document representation learned by NVSTM with other models like NTM, DocNADE, and GLDA. NVSTM showed the highest accuracy across datasets, indicating the advantage of neural networks in distributed word representations. Sparse models like NVSTM were found to be superior in extracting topics from short documents. The average word codes learned by NVSTM were quantitatively investigated, showing better performance compared to LDA in web snippet categories. The study compared NVSTM with other models like NTM, DocNADE, and GLDA, showing NVSTM's superiority in accuracy and topic extraction from short documents. NVSTM's word codes were found to be sparser and more discriminative compared to LDA, with each non-zero element representing the topical meaning of words. Some words have narrow topical meanings, while others like \"hockey\" and \"marketing\" have broader meanings. The study introduced a neural sparsity-enhanced topic model NVSTM, utilizing VAE for inference in STC. The model's effectiveness was demonstrated through a t-SNE projection of document codes from various datasets, showcasing distinct clustering into categories. This highlights the semantic efficacy of the learned document codes by the model. The study introduced NVSTM, a neural sparsity-enhanced topic model using VAE for inference in STC. NVSTM utilizes word embeddings and a neural network framework to generate clear and semantic-enriched representations for short texts. Evaluation results show the model's effectiveness and efficiency. Future work may involve integrating other deep generative models like GAN."
}