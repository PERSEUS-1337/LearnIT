{
    "title": "By5ugjyCb",
    "content": "Deep learning algorithms achieve high classification accuracy with significant computation cost. A novel quantization scheme called PArameterized Clipping acTi-vation (PACT) optimizes activation clipping parameter \u03b1 during training to enable neural networks to work well with ultra low precision weights and activations without accuracy degradation. PACT allows quantizing activations to arbitrary bit precisions, achieving better accuracy compared to other quantization schemes. Both weights and activations can be quantized to 4-bits precision while maintaining accuracy. Both weights and activations can be quantized to 4-bits of precision without loss of accuracy, enabling improved inferencing performance in hardware. Deep Convolutional Neural Networks (CNNs) have achieved high accuracy in various application domains, but their computational and storage requirements pose challenges for widespread deployment. Reducing the bit-precision of CNN weights and activations has gained attention for minimizing memory and computation costs while preserving accuracy. Various techniques have been proposed to address computational challenges in deploying deep CNNs in resource-constrained edge environments. Weight quantization methods have shown significant reductions in bit-precision with limited accuracy degradation. The curr_chunk discusses a novel activation quantization technique called PArameterized Clipping acTivation function (PACT) that optimizes quantization scales during model training, leading to reduced bit-widths for weights and activations. This allows for a trade-off between hardware complexity and model accuracy. The novel activation quantization technique, PACT, introduces a new parameter \u03b1 to optimize quantization scales during training. It sets the quantization scale smaller than ReLU to reduce error but larger than conventional clipping functions for better gradient flow. Regularization is applied to \u03b1 for faster convergence. Empirical results show PACT achieves high model accuracy with extremely low bit-precision and 4-bit quantized CNNs perform similarly to single-precision floating point representations. The paper introduces PACT, a novel activation quantization technique that optimizes quantization scales during training using a new parameter \u03b1. It demonstrates high model accuracy with low bit-precision, showing that 4-bit quantized CNNs perform similarly to single-precision floating point representations. The paper also includes system performance analysis to show trade-offs in hardware complexity for different bit representations. Recently, various techniques have been proposed to minimize CNN computation and storage costs, including weight quantization schemes that enable DNN models to fit effectively in resource-constrained platforms. Ternarizing weights using statistical distribution or tuning quantization scales during training has further improved effectiveness. However, system performance gains are limited when only weights are quantized while activations remain in high precision, especially in convolutional neural networks. Neural networks have been studied for weight quantization to reduce computation and storage costs, with prior work proposing fully binarized networks. Recent research explores activation quantization using various bit-precision selections, but significant accuracy degradation occurs for ImageNet tasks with \u2264 2-bit precision. Improved logarithmic quantization schemes based on weighted entropy have also been investigated. Activations and weights quantization have been studied for neural networks to reduce computation and storage costs. While activation quantization has shown promise, challenges remain in effectively optimizing and adapting during training. Weight quantization can be compensated for during model training to mitigate errors. Traditional activation functions and their quantization challenges are discussed in detail. Activation quantization poses challenges, especially with ReLU as the activation function in CNNs. ReLU allows for superior accuracy but requires high dynamic range for quantization. Traditional activation functions do not have trainable parameters, making it difficult to compensate for errors during quantization. Activation quantization in CNNs, particularly with ReLU as the activation function, presents challenges due to the high dynamic range required for quantization. Clipping activation functions have been used to address this issue, but determining an optimal clipping value is challenging. Recent approaches like half-wave Gaussian quantization have shown promise in mitigating this challenge. PACT introduces a new activation quantization scheme with a parameterized clipping level, \u03b1, dynamically adjusted via gradient descent-based training to minimize accuracy degradation from quantization. It replaces the conventional ReLU activation function in CNNs and linearly quantizes the truncated activation output to k bits. The activation output is quantized to k bits using a parameterized clipping level \u03b1, optimized during training. The gradient \u2202yq \u2202\u03b1 is computed using the Straight-Through Estimator. A L2-regularizer is included in the loss function to limit dynamic range and minimize quantization errors. The value of \u03b1 converges to smaller values during training, reducing quantization loss. PACT works by limiting dynamic range of activations and minimizing quantization loss. Appendix A and B provide detailed analysis on its effectiveness. Activation quantization affects network parameters during training, with PACT finding a balance to minimize errors. Training loss is recorded over a range of clipping parameter \u03b1 to observe impact on network performance. The SVHN network is trained with a quantization scheme using a parameterized clipping ActFn instead of ReLU in its convolution layers. The cross-entropy converges to a small value as the clipping parameter \u03b1 increases, indicating the effectiveness of ReLU without quantization. However, training with a clipping parameter \u03b1 may reduce cross-entropy for certain layers even in the full-precision case. The impact of quantization on cross-entropy is shown in the forward-pass, highlighting the effectiveness of the proposed approach. The impact of quantization on cross-entropy is shown in the forward-pass, with the optimal \u03b1 varying for different layers. Plateaus of cross-entropy for certain \u03b1 ranges make gradient descent-based training challenging. Regularization helps eliminate plateaus, aiding convergence without affecting the global minimum point. The optimal \u03b1 extracted from the pre-trained model is crucial for minimizing training loss curves. The regularization coefficient, \u03bb \u03b1, is an additional hyper-parameter controlling regularization impact on \u03b1. The study focused on \u03b1 scope, initial values, and regularization effects. Sharing \u03b1 per layer was found to be the best scope, reducing hardware complexity. Initializing \u03b1 to a larger value and applying regularization during training proved advantageous. L2-regularization for \u03b1 was observed to be effective. The study focused on optimizing the regularization coefficient \u03b1 for minimizing training loss curves. L2-regularization for \u03b1 with the same parameter \u03bb used for weight was found to work well. The optimal value for \u03bb \u03b1 slightly decreases with higher bit-precision due to increased resolution for activation quantization. First and last layers were not quantized to maintain accuracy. PACT was implemented in Tensorflow using Tensorpack, and the effectiveness was demonstrated on various CNNs. More implementation details can be available in the Appendix. The baseline networks use the same hyper-parameters and ReLU activation functions. For PACT experiments, ReLU is replaced with PACT while keeping the same hyper-parameters. Different network architectures are used for CIFAR10-ResNet20, SVHN-SVHN, IMAGENET-AlexNet, IMAGENET-ResNet18, and IMAGENET-ResNet50. IMAGENET-ResNet50 (ResNet50, He et al.): a CONV layer followed by 16 ResNet \"bottleneck\" blocks and a final FC layer. Comparison with DoReFa (Zhou et al.) accuracy results. Evaluation of activation quantization scheme using various CNNs, showing training and validation error of PACT. Higher bit-precision leads to closer errors to full-precision reference. The study compares activation quantization performance using different CNNs, showing that higher bit-precision results in closer errors to full-precision reference. PACT achieves consistently lower accuracy degradation as activation bit-precision increases. PACT demonstrates robustness in activation quantization, achieving full-precision accuracy with 4-bit precision for both weights and activation. This is the lowest reported bit precision that can achieve near full-precision results. PACT demonstrates robustness in activation quantization, achieving full-precision accuracy with 4-bit precision for both weights and activation. Comparison with 7 previous quantization schemes shows PACT outperforms in accuracy degradation for AlexNet, ResNet18, and ResNet50. The accuracy degradation decreases as bit-precision increases for activation or weight. AlexNet even achieves better accuracy using PACT than full-precision. The gain in system performance due to reduction in bit-precision using PACT-CNN is demonstrated. The reduction in bit-precision achieved using PACT-CNN results in improved system performance. A DNN accelerator system with multiple cores and on-chip memory executes DNN layers using fixed-point MAC processing elements. Different versions of the accelerator show improved density with aggressive scaling of bit precision. By reducing bit precision from 16 bits to 2 bits for activations and weights, a 14\u00d7 improvement in density is achieved. A precision-configurable MAC unit allows dynamic modulation of bit precision while maintaining iso-area. Performance improvement for the ResNet50 DNN benchmark is shown in Fig. 6(c) using DeepMatrix for system performance estimation. Different external memory bandwidths are studied, including bandwidth unconstrained and constrained systems at 32 and 64. In bandwidth constrained systems, reducing data precision from 16 to 4 bits leads to a 4\u00d7 increase in peak FLOPs and a 4.5\u00d7 performance improvement due to data structures fitting within on-chip memory, reducing off-chip data transfers. In bandwidth constrained systems, reducing data precision from 16 to 4 bits leads to a 4\u00d7 increase in peak FLOPs and a 4.5\u00d7 performance improvement due to data structures fitting within on-chip memory, reducing off-chip data transfers. For 4 and 2 bit precision configurations, using 8 bit precision for the first and last layers of the DNN still. Quantizing the first and last layers to 4 or 2 bits could result in an additional 1.24\u00d7 performance improvement. A novel activation quantization scheme based on the Parameterized Clipping activation function (PACT) is proposed in this paper, replacing ReLU with an activation function optimized via gradient descent training. PACT outperforms ReLU when quantization is applied during training, as shown in empirical evaluations using popular convolutional neural networks like CIFAR10 and SVHN. Empirical evaluation of PACT activation function on popular convolutional neural networks like CIFAR10 and SVHN shows effective quantization of activations and weights down to 4-bits while maintaining near full-precision accuracy. PACT allows for increased accelerator cores in the same area, improving system performance. It is as expressive as ReLU due to the flexibility of the clipping parameter \u03b1. PACT activation function can achieve the same results as ReLU through SGD training. By adjusting the parameter \u03b1, PACT can mimic ReLU's behavior and converge to the desired output. This allows for effective quantization of activations and weights while maintaining accuracy in neural networks. PACT activation function can mimic ReLU's behavior through adjusting parameter \u03b1 to converge to the desired output, allowing for effective quantization of activations and weights while maintaining accuracy in neural networks. The network with PACT converges almost identical to the network with ReLU, demonstrating its effectiveness in achieving similar results. In activation quantization, there is a trade-off between errors due to clipping and quantization. The challenge lies in finding a proper clipping level to balance these errors, as shown in FIG7 for CIFAR10-ResNet20 training with different clipping levels. Activation functions with large dynamic range, like ReLU, suffer from quantization issues. Activation functions like ReLU suffer from quantization errors with decreasing bit-precision. PACT adjusts dynamic range based on output proximity to target to minimize clipping and quantization errors. CIFAR10-ResNet20 trained with varying clipping levels shows accuracy degradation with increasing \u03b1. Activation degradation occurs as \u03b1 increases, consistent with quantization error trend. PACT achieves optimal accuracy without exhaustive clipping level search, auto-tuning for efficiency in training large quantized neural networks. Validation error decreases with \u03b1 without quantization, with some cases even outperforming ReLU networks. In this section, details on hyper-parameters and design choices for PACT are presented. The optimal scope for \u03b1 is discussed, with options including individual \u03b1 for each neuron activation, shared \u03b1 among neurons within the same output channel, and shared \u03b1 within a layer. Empirical studies on CIFAR10-ResNet20 show that sharing \u03b1 per layer yields the best accuracy, which also reduces hardware complexity. The optimization behavior of \u03b1 in PACT can be explained by its initialization and regularization. Starting with a reasonably large value covers a wide dynamic range and avoids unstable adaptation, while applying L2-regularization helps alleviate quantization error. Sharing \u03b1 per layer is found to yield the best accuracy in empirical studies. The validation error for PACT-quantized CIFAR10-ResNet20 remains stable for a range of \u03bb \u03b1 values. Optimal \u03bb \u03b1 decreases slightly with higher bit-precision due to increased resolution. Quantizing first and last layers leads to accuracy degradation, as shown empirically for CIFAR10-ResNet20. The study found that quantizing the first and last layers with higher precision than the other layers can reduce accuracy degradation. The CNN implementation details and training settings are based on default networks provided by Tensorpack. ReLU following BatchNorm is used for the convolution. The study utilized the CIFAR10 dataset for image classification using a standard ResNet structure. ReLU activation functions were replaced with PACT for experiments, while Softmax was used for the fully-connected layer. Training was done from scratch with stochastic gradient descent. The study used the SVHN dataset for digit recognition with a CNN model and ADAM optimizer. The IMAGENET dataset with 1000 categories was also mentioned. The IMAGENET dataset consists of 1000 categories of objects with over 1.2M training and 50K validation images. Images are resized and cropped before input to the network. Modified AlexNet, ResNet18, and ResNet50 were used with specific configurations such as BatchNorm layer, ADAM optimizer, and L2-regularizer. ResNet18 has a CONV layer followed by 8 ResNet blocks, while ResNet50 has a CONV layer followed by 16 ResNet \"bottleneck\" blocks. ResNet50 consists of a CONV layer followed by 16 ResNet \"bottleneck\" blocks and a final FC layer. Stochastic gradient descent (SGD) with momentum of 0.9 and learning rate starting from 0.1 is used. L2-regularizer with decay of 10 \u22124 is applied to weight. Mini-batch size of 256 and maximum epochs of 110 are used. DoReFa-Net and Balanced Quantization are compared for CIFAR10, SVHN, AlexNet, and ResNet18 under the same experimental setting as PACT. Zhou et al. (2017) proposed a quantization scheme based on recursive partitioning of data into balanced bins. They compared the top-1/top-5 validation accuracy of their quantization scheme for AlexNet and ResNet18. Mishra et al. (2017) introduced WRPN, a scheme to increase the number of filter maps for activation quantization. Mellempudi et al. (2017) developed FGQ, a direct quantization scheme based on fine-grained grouping. Park et al. (2017) presented WEP, a quantization scheme considering statistics of weight/activation. In Park et al. (2017), quantization schemes were compared for accuracy with various bit-precision for AlexNet and batch normalization. HWGQ by Cai et al. (2017) utilized Lloyd search on Normal distribution for quantization accuracy comparison with different bit-precision for AlexNet, ResNet18, and ResNet50. The first and last layers were not quantized. PACT achieves the best accuracy for image classification on IMAGENET dataset compared to other quantization methods like DoReFa. It enables more aggressive weight quantization without sacrificing accuracy."
}