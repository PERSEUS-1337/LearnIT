{
    "title": "Sk0pHeZAW",
    "content": "Deep learning models often require large memory and energy consumption, limiting their applications on embedded platforms. This work proposes Weight Reduction Quantisation to compress the memory footprint by reducing the number of weights and bits. Additionally, it focuses on speeding up stochastic variance reduced gradients optimization on non-convex problems using mini-batch SVRG with $\\ell$1 regularization, showing faster convergence rates than SGD. Experimental evaluation on MNIST and CIFAR-10 datasets with LeNet models demonstrates the effectiveness of the approach. Our approach reduces memory requirements in convolutional and fully connected layers by up to 60$\\times$ without affecting test accuracy. Artificial intelligence applications vary in computational resources, with deep learning models like BID14 requiring significant memory and energy. Reducing model size to a compact form with small memory footprint is crucial for deploying on mobile devices. $\\ell$1 regularization helps prevent overfitting during training. Our research focuses on improving the method based on L1 regularization to reduce memory requirements in deep neural network optimization. We propose a compression method, Weight Reduction Quantisation, which reduces the number of weights and bits-depth of the model without sacrificing accuracy. This method employs sparsity-inducing L1 regularization to encourage many connections to be zero during training. The paper discusses an unconstrained minimization problem for estimating weights in all layers of a neural network using L1 regularization. The optimization method involves mini-batch SGD with regularization and weight update rules. The paper introduces two methods to address challenges in SGD optimization with L1 regularization: Cumulative L1 regularization clips penalties at zero for stable convergence, while Mini-batch SVRG reduces noise by decreasing gradient variance. The paper discusses methods to improve SGD optimization with L1 regularization. Mini-batch SVRG reduces gradient variance to enhance convergence rate, while highlighting the limitations of SGD with mini-batching due to complexity. SVRG is more efficient with batch size dependency, making it a better option for optimization. Global minimization of non-convex functions is NP-hard. SVRG can be applied in neural networks to accelerate the local convergence rate of SGD. Allen proved non-asymptotic rates of convergence of SVRG for non-convex optimization and proposed an improved SVRG that is faster than SGD. Mini-batch SVRG with cumulative 1 regularization reduces memory requirements by up to 25\u00d7 without affecting test accuracy. To reduce memory requirements, Weight Reduction Quantisation compresses model weights by reducing the number of bits stored per weight, achieving up to 60\u00d7 reduction without impacting accuracy. Modified SVRG shows faster convergence rates than ordinary SVRG and SGD, with sparse representation and pruning being effective methods to reduce parameters in deep learning models. Weight Reduction Quantisation compresses model weights by reducing the number of bits stored per weight, achieving up to 60\u00d7 reduction without impacting accuracy. Various methods like weight sharing, matrix factorization, and weight quantization are used to reduce memory requirements in deep learning models. HashedNets and k-means clustering are examples of techniques used to approximate weights and compress network connections. Various methods have been used to compress model weights in deep learning models. Gysel et al. successfully condensed CaffeNet and SqueezeNet to 8 bits with minimal accuracy loss. Han et al. quantized sparse weights matrices to 8-bit for convolutional layers and 5-bit for fully connected layers. Rastegari et al. utilized binary operations to approximate convolutions with 1-bit precision. Different approaches such as regularization, dropout, and shrinkage operators have been proposed to induce sparsity and reduce weights, with varying levels of success. The methods for stochastic iterative learning algorithms aim to find \u03b5-accurate solutions in finite iterations for non-convex problems. Gradients can get stuck in saddle points or local minima, so adding noise can help escape these points. There is no theoretical proof that SVRG converges faster than SGD. Studies have compared the complexity of SGD and SVRG on non-convex problems. In non-convex optimization, the efficiency of SVRG compared to SGD depends on the number of training samples. SVRG with fixed learning rate has a complexity of O(n + (n^(2/3)/\u03b5)), while mini-batch SGD with adaptive learning rate has a complexity of O(1/\u03b5^2). The convergence rates of SVRG are faster than SGD when n is small. Experimental results support this view, especially when SVRG is applied to sparse representation. Combining SVRG with cumulative 1 regularization can further enhance performance. The algorithm SVRG with cumulative 1 regularization faces two main issues: it may not be faster than SGD on sparse representation, and it has a trade-off between variance reduction and sparsity. The extension proposed by BID17 aims to address these challenges. The MSVRG extension of SVRG introduces adaptive learning rates to guarantee equal or better performance than SGD. Our method, Delicate-SVRGcumulative-1, also provides separate adaptive learning rates for faster convergence. Adaptive learning rates are crucial for optimization convergence, and our method aims to reduce the number of weights while maintaining stability. Our algorithm reduces variance for faster convergence rates in nonconvex optimization by adaptively updating the learning rate. It includes three parameters for controlling gradient convergence with L1 regularization implementation. The learning rate schedule emphasizes large gradient distances to avoid local minima, resulting in a fast convergence rate that decreases over time. The algorithm adapts the learning rate to reduce variance for faster convergence rates in nonconvex optimization. It includes parameters for gradient convergence control with L1 regularization. The learning rate schedule prioritizes large gradient distances to avoid local minima, resulting in a fast convergence rate that decreases over time. Additionally, bias-based pruning is used to further reduce the number of weights. The algorithm adapts the learning rate to reduce variance for faster convergence rates in nonconvex optimization, with parameters for gradient convergence control and L1 regularization. Bias-based pruning is used to further reduce the number of weights, with a rule based on connections' values compared to the network's minimal bias. Weight quantization can compress the model by reducing bit precision. Weight Reduction Quantisation is proposed as a final compression method, quantizing to 3-bit for convolutional layers and encoding 5 bits for fully connected layers. The compression method is evaluated on deep neural networks (DNNs) and convolutional neural networks (CNNs) like LeNet-300-100 and LeNet-5 using MNIST and CIFAR datasets. MNIST is a dataset of handwritten digits with 60,000 training examples and 10,000 test samples, while CIFAR-10 is a dataset commonly used in machine learning. The CIFAR-10 dataset consists of 50,000 training images and 10,000 test images with 32\u00d732 RGB pixels. Two types of error rates, top-1 and top-5, are used to measure model performance. A compression method using Weight Reduction Quantisation was applied to MNIST, achieving a 98% reduction in memory requirements with low test error rates on LeNet-300-100 and LeNet-5 models. The compression pipeline on the LeNet-5 model focuses on reducing the number of weights and further compressing the model through bit-depth reduction. The Weight Reduction Quantisation method achieves low error rates on MNIST and CIFAR-10 datasets, with compression rates up to 60\u00d7 in LeNet-300-100 and up to 57\u00d7 in LeNet-5 model. The Delicate-SVRG-cumulative-1 method is compared with other model compression techniques based on 1 regularization. It shows varying test error rates and weight sparsity as the regularization parameter \u03bb is adjusted. Compared to SGD-cumulative-1, SVRG-cumulative-1 has better compression ability but higher error rates due to variance. Replacing SGD with SVRG reduces test error but also reduces compression ability. Delicate-SVRG-cumulative-1 has the least number of weights and the best performance. The Delicate-SVRG-cumulative-1 method outperforms other compression techniques, showing lower test error rates and weight sparsity. It can be effectively applied in LeNet models without sacrificing accuracy, especially when bias-based pruning is added. The method achieves better performance than SGD-cumulative-1, with the potential for lower test error rates on complex datasets like CIFAR-10. Weight quantization and Delicate-SVRG-cumulative-1 can be effectively combined to achieve the best performance in LeNet models without accuracy loss on MNIST and CIFAR-10 datasets. The approach outperforms individual methods in reducing memory usage and test error rates. This combination shows no negative impact on convergence rates, achieving fast convergence similar to SGD-cumulative-1 or SVRG-cumulative-1. Delicate-SVRG-cumulative-1 with adaptive learning rates shows faster convergence and lower training loss compared to other methods, making it an efficient compression method for neural networks. Bias-based pruning in 1 regularization does not negatively affect convergence. Weight Reduction Quantisation is proposed as an effective compression technique for neural networks. Weight Reduction Quantisation efficiently compresses neural networks without sacrificing accuracy by reducing the number of weights and bits to store each weight. The method combines SVRG and cumulative 1 regularization to create Delicate-SVRG-cumulative-1, which significantly reduces parameters with separate adaptive learning rates. This approach improves SVRG for non-convex problems with fast convergence rates, reducing memory requirements by up to 60\u00d7 without accuracy loss on LeNet-300-100 and LeNet-5 models. After compression by Weight Reduction Quantisation, a compact deep neural network can be efficiently deployed on an embedded device with performance similar to the original model. The Delicate-SVRG-cumulative-1 algorithm combines SVRG and cumulative 1 regularization, showing better performance compared to SVRG and SGD. Multiple initializations were used to compare the methods, with consistent better results for the proposed method. The performance of our method, Delicate-SVRG-cumulative-1, surpasses SVRG and SGD with cumulative 1 regularization. Results demonstrate superior test loss on MNIST and CIFAR-10 datasets using LeNet-5 and LeNet-300-100 models. Comparison with other compression methods shows our method's effectiveness, especially when using different initial weights. In experiments comparing BiasPruning, SVRG, and SGD, BiasPruning is consistently better. The performance of SVRG compared to SGD depends on the dataset size, with SVRG outperforming on small datasets like MNIST but underperforming on larger datasets like CIFAR-10."
}