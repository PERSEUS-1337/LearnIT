{
    "title": "r1ledo0ctX",
    "content": "In one-class-learning tasks, generative models are used to model the normal case due to the lack of representative data for anomalies. A multi-hypotheses autoencoder is proposed to learn the data distribution more efficiently, with a discriminator critiquing the model to prevent artificial data modes. This consistency-based anomaly detection framework, ConAD, improves anomaly detection on CIFAR-10 by up to 3.9% points. Anomaly detection classifies samples as normal or abnormal, often treated as a one-class-learning problem. Abnormal samples can be rare or not fully represented. Examples include anomalies in autonomous driving, manufacturing defects, and medical data analysis. Anomaly detection is typically approached as a one-class learning problem using generative, reconstruction-based methods like variational autoencoders for high-dimensional inputs like images. These methods approximate the input distribution of normal cases to detect anomalies at test time. Autoencoders produce blurry reconstructions due to regressing the conditional mean and inability to model multi-modal distributions. Multiple-hypotheses networks, in conjunction with autoencoders, can provide more expressive power with a multi-headed decoder, resulting in clearer anomaly scores in the ConAD framework. Anomaly scores in the ConAD framework are clearer due to the challenges in training networks to produce multi-modal distributions. The winner-takes-all loss in loosely coupled hypotheses branches leads to over-fitting in dominant data modes and under-fitting in underrepresented regions, affecting anomaly estimation. Mixture density networks (MDNs) provide a strict coupling of hypotheses branches, learning a conditional Gaussian mixture distribution for anomaly detection in underrepresented data regions. MDNs can handle multi-modal distributions, but struggle with mode collapse in high-dimensional spaces, impacting anomaly estimation. In this work, multiple-hypotheses networks are used for anomaly detection to capture model uncertainty in data distribution. The approach combines multiple-hypotheses learning with a discriminator to address artificial data modes and ensure consistency with the real data distribution. Focus is placed on the local neighborhood and estimating sample fit based on distance to the closest cluster, avoiding issues with global distribution. Our approach uses multiple-hypotheses networks for anomaly detection, focusing on local density estimates to avoid global distribution issues. Evaluation on CIFAR-10 and the \"Metal Anomaly dataset\" shows improved performance compared to other methods like mixture density networks. Our model focuses on local neighborhood for anomaly score estimation, encouraging multiple hypotheses to cover different modes. The ConAD framework outperforms MDNs on CIFAR-10, highlighting the importance of considering local neighborhood over global distribution. Traditional one-class learning techniques struggle in high-dimensional domains, requiring careful feature selection. Recently, advances in generative modeling like Generative Adversarial Network (GAN) and Variational Autoencoder (VAE) are used for anomaly detection. However, GAN tends to assign less probability to real samples and VAE typically regresses to conditional means, leading to blurry reconstructions. To address model uncertainty in VAE, a solution is to give the decoder additional expressive power with multi-headed decoders to approximate multiple conditional modes. The approach involves training multiple networks in Multi-Choice-learning, estimating conditional Gaussian Mixture models in Mixture Density Network (MDN), and making multiple-hypotheses predictions. In MDN, mixtures are strictly coupled via coefficients, while in MHPs, mixtures act as loosely coupled local density estimators. Anomaly detection is addressed using MHP-training with VAE to directly handle model uncertainty. The anomaly score in MDN is based on weighted distances to all data modes, while in MHP, it is only based on the closest data mode, referred to as consistency-based learning. In MHP, consistency-based learning is emphasized, where samples close to one data mode have a small effect on loss. The learning dynamic is more efficient compared to MDN, with fewer samples having high loss. The outlier score in LOF BID4 is based on local neighborhood, not influenced by samples further away. The model focuses on learning many loosely decoupled local density estimates with MHP-learning, concentrating on the closest data mode for outlier detection. Our model utilizes the MHP-technique for anomaly detection, which approximates the data manifold before detecting anomalies in the input space. Unlike previous MHP-approaches, our framework is designed for distribution learning and avoids mode collapse among hypotheses. Our framework ConAD utilizes a discriminator D to assess the quality of generated hypotheses and prevent support of non-existent data modes. To avoid hypotheses mode collapse, our model employs hypotheses discrimination using pair-wise distances across a batch of hypotheses. Additionally, we propose to use MHP for the decoder to enhance expressive power. Shortcomings of multiple-hypotheses learning include support of artificial data mode and hypotheses mode collapse. These effects can be reduced with discriminator training and hypotheses discrimination. In one-to-many mapping tasks, unimodal models fail to capture data distribution, while MHP-networks represent Gaussian density functions with mean and variance. Increasing the number of mixture components in a mixture density network improves data distribution modeling. MHP-networks learn from the winnertakes-all loss, where only the best hypothesis receives the learning signal. BID23 proposed a smoothed loss to distribute a small ratio of the signal among non-optimal branches. However, this approach may lead to support of artificial data modes, causing inconsistency in the underlying distribution. In regions where the half-moon abruptly ends, the hypotheses in MHP and MHP-WTA support non-existing data regions, leading to fatal inconsistency for anomaly detection. Learning with the winner-takes-all loss allows non-optimal hypotheses to create artificial data regions without penalty, making it crucial to find a good parameter to reduce this effect. Choosing proper hyper-parameters in one-class-learning is challenging due to the lack of anomalies during training. Training Autoencoders with likelihood-metric can result in blurry reconstructions, impacting anomaly detection by falsifying the reconstruction error. This regression to the conditional mean leads to a one-to-many mapping from latent code to input space, where each point on the learned manifold represents multiple data points. In the optimal case, each point should represent a single input vector, requiring more data to achieve. The proposed multiple-hypotheses Variational Autoencoder (VAE) aims to express model uncertainty directly with multiple hypotheses, avoiding mode collapse and maximizing diversity across hypotheses for anomaly detection tasks. The proposed approach involves learning normal data distribution for anomaly detection tasks using multiple hypotheses in a Variational Autoencoder. Hypotheses act as clusters in the data conditional space, with anomalies detected based on distances to local clusters. A discriminator is used to prevent coverage of non-existing data regions by the hypotheses, promoting diversity among them. The method aims to express model uncertainty with multiple hypotheses predictions (MHP) in the conditional input space. The proposed approach involves using a Mixture Density Network (MDN) to predict a global density estimate in the conditional space. Different hypotheses are learned using a winner-takes-all objective, with shared layers among hypotheses networks to reduce free parameters. Multi-headed networks are used to implement MHP efficiently in neural networks. Our framework is based on the Variational Autoencoder (VAE) for effective manifold learning and efficient inference. To address mode collapse, we introduce a discriminator D and match density estimates with real data using Jensen-Shannon divergence (JSD). The D and G are in a mini-max game to ensure quality of generated hypotheses. The ConAD framework proposes multiple-hypotheses learning with a VAE and a discriminator to address mode collapse by employing hypotheses discrimination based on minibatch discrimination. The learning objective for the VAE generator is extended to assure the quality of generated hypotheses, ensuring diverse outputs to avoid detection easily. The ConAD framework utilizes a VAE with a discriminator to prevent non-existing data modes and enhance mode coverage. Anomaly detection is based on local likelihood estimates from the closest hypothesis. Evaluation compares the approach to recent deep learning and non-deep learning techniques for one-class learning tasks, where anomalies are rare and not present during training. The proposed framework, ConAD, employs a VAE with a Gaussian output distribution and a multiple-head-network decoder to support multiple hypotheses. Each hypothesis predicts a Gaussian density estimate. The ConAD framework utilizes a VAE with a discriminator to prevent non-existing data modes and enhance mode coverage. Anomaly detection is based on local likelihood estimates from the closest hypothesis. The framework can be extended to recent advances in deep generative modeling and is evaluated on CIFAR-10 and the Metal Anomaly dataset. Anomaly detection performance is measured using AUROC based on normalized negative log likelihood scores. The performance of traditional methods on the high-dimensional Metal anomaly dataset suffers due to the curse of dimensionality. Previous GAN-based techniques like AdGAN & AnoGAN are unstable and lead to random anomaly detection performance. MHP-based approaches are evaluated against uni-modal counterparts (VAE, VAEGAN) for anomaly detection on the Metal Anomaly dataset. Only 10% of maximally abnormal pixels are used to compute the total anomaly score to reduce noisy residuals. AUROC is computed on an unseen test set comprising normal and anomaly data. For detailed results, refer to attachment H. The performance of traditional methods on the Metal Anomaly dataset suffers due to the curse of dimensionality. Deep learning methods outperform traditional techniques, even without careful parameter tuning. Anomaly detection with MHP technique shows improved performance compared to plain MHP, especially in capturing classes with dominant backgrounds. Discriminative features from a pretrained AlexNet do not enhance anomaly detection performance. The MHP-technique improves anomaly detection performance compared to traditional methods on the Metal Anomaly dataset. Utilizing multiple hypotheses effectively in the ConAD-framework leads to significantly higher detection performance. Evaluation against density-learning methods shows that MDNs perform worse than the local density estimation provided by MHP. Our relaxation of density estimation into local estimation results in a significant improvement of up to 4.2% AUROC-score. Our approach, MHP, improves anomaly detection performance by utilizing local density estimation with multiple hypotheses. Unlike MDNs, which evaluate likelihood based on all data modes, MHP considers the closest data mode for computing local likelihood as the anomaly score. This approach suppresses over-estimation of anomaly degree compared to global likelihood, leading to better performance with just two hypotheses. However, without the discriminator D, increasing the number of hypotheses quickly degrades performance due to inconsistency. Our framework ConAD improves anomaly detection performance by remaining competitive or better with an increasing number of hypotheses. The discriminator D in our framework makes it adaptable to new datasets and less sensitive to the number of hypotheses used. However, when more hypotheses are utilized, the anomaly detection performance rapidly deteriorates, suggesting that noise is learned too easily. Our ConAD model is less sensitive to hyper-parameter choices and can better utilize the MHP-technique for anomaly detection tasks. By employing multiple-hypotheses networks, we can capture model uncertainty and provide a more detailed description of data distribution for improved anomaly detection. The use of a discriminator D helps reduce support for artificial data modes in hypotheses learning. The proposed model combines multiple-hypotheses learning with a discriminator D to ensure consistency and diversity in estimated data modes. It improves anomaly detection performance on CIFAR-10 and real-world tasks significantly. The Mixture Density networks predict a data conditional Gaussian mixture model in the data space, enhancing the model's ability to identify out-of-distribution samples. The proposed model combines multiple-hypotheses learning with a discriminator to improve anomaly detection performance. It extends GMM-learning by conditioning data and uses Mixture Density Networks to predict a data conditional Gaussian mixture model. The framework includes mixing coefficients and latent codes to form a likelihood function. The model addresses inconsistencies in the true underlying distribution by supporting artificial data regions. Mixture Density Networks (MDN) and ConADs approaches minimize inconsistencies in the underlying distribution. A simple toy problem with observable x and hidden y is predicted using the conditional distribution p true (y|x). The empirical mean y xi of p train (y|x i ) does not fully capture the multi-modal data conditional, leading to low-likelihood regions. The Negative-log-likelihood of MDN App. A under a Gaussian Mixture minimizes the energy for capturing data modes. The Negative-log-likelihood of the Mixture Density Network (MDN) App. A minimizes inconsistencies in the underlying distribution by centering each hypothesis at different empirical data points. The optimal solution for E W T A (\u0398 *) is not unique due to the multi-modal nature of the data conditional distribution. The winner-takes-all energy formulation allows for multiple solutions to the loss formulation, supporting arbitrary artificial data regions without penalty. Hypotheses generated by a neural network with parameters \u03b8 can lead to inconsistent approximations of the output distribution. The optimal least-squares solution for training data points and parameters is discussed. The winner-takes-all energy formulation supports multiple solutions without penalty, leading to inconsistent output distribution approximations by neural networks. The optimal least-squares solution for training data points is the mean, resulting in all hypotheses converging independently. The network architecture follows DCGAN but is scaled down for CIFAR-10, using deconvolutional layers and leaky-relu units. In the network, leaky-relu units are used with hypotheses branches represented as decoder networks heads. Each hypothesis predicts a Gaussian distribution with diagonal co-variance \u03a3 and mean. The winner-takes-all loss operates on pixel-level, with the best combined-reconstructions being the combination of winning hypotheses. Training involves feeding fake images to the discriminator D in 4 batches, with different types of fake images. The batch-size is set to 64 on CIFAR-10 and 32 on Metal Anomaly, with training done using Adam with a learning rate of 0.001. Our ConAD approach outperforms traditional methods and vanilla MHP-approaches significantly in CIFAR-10 anomaly detection. Mixture Density Networks perform similarly to uni-modal output distributions of VAEs. On the Metal Anomaly dataset, reconstructions from uni-modal models are blurry, but our ConAD approach provides clearer reconstructions capturing more details. The ConAD approach outperforms traditional methods and vanilla MHP-approaches in Metal Anomaly dataset anomaly detection. The likelihood maximizer in the hypotheses space is closer to the original image, capturing more details for differentiating between normal data noise and real anomalies. The residuals are clearer with ConAD, showing improved anomaly detection performance compared to other approaches. The ConAD approach outperforms traditional methods and vanilla MHP-approaches in Metal Anomaly dataset anomaly detection, capturing more details for differentiating between normal data noise and real anomalies. The task has become easily solvable for these methods."
}