{
    "title": "SkERSm-0-",
    "content": "This paper addresses the intrinsic dimension, real factor, disentanglement, and indicator issues of variational autoencoder (VAE) theoretically and practically through noise modeling. It discusses how VAE learns intrinsic factor dimension, encourages information sparsity, and clarifies disentanglement based on information conservation. The behavior of the disentanglement metric in VAE models is discussed, along with performance indicators for evaluating disentanglement and generating influence. Experiments under noise modeling and constraints support theoretical analysis and demonstrate unique characteristics in achieving disentanglement. VAEs have shown abilities in modeling causal relationships, extracting disentangled factors, and generating diverse signals in a controllable manner, enabling knowledge transfer through shared factors among tasks. VAEs have human advantages over machines and are used in various applications such as disentangled representations learning, few-shot learning, causal relationships modeling, and more. However, the lack of theoretical study on VAEs' generating and inference procedures hinders research progress. The information conservation theorem states that in Gaussian-VAE, the factor dimension must be equal to the number of independent Gaussian random variables generating the data. This model is scalable for unsupervised representation learning and can be applied to other continuous latent factors in VAEs. Gaussian-VAE assumes input x is generated by independent Gaussian factors z. The generating process is modeled as p \u03b8 (x|z) and the inference process as q \u03c6 (z|x). Both are parameterized by neural networks. In VAE, the approximate inference method maximizes the variational lower bound of p \u03b8 (x). An idealistic VAE perfectly encodes and decodes factors following a unit Gaussian distribution. The literature discusses the deterministic assumption on the generation process of x and factors z in VAE. It addresses the idealistic VAE's behavior and the information conservation theorem, showing that VAE learns the intrinsic factor dimension. The idealistic VAE learns the intrinsic factor dimension and promotes information sparsity in factors through a constraint induced by a Gaussian prior. The Information Conservation Theorem states that independent Gaussian random variables cannot be the generating factor of each other. The mutual information between factors learned by the encoder network and the signal x is crucial for evaluating generating influence. If the learned factors are conditionally independent given x, it can provide useful insights. The Mutual Information Separation theorem suggests that if the learned distributions can factorize, then considering each factor won't result in information loss. The theorem demonstrates that the second term in the variation lower bound is capable of controlling mutual information and similarity between the encoder network's output and the prior. It also has the capacity to reduce non-intrinsic factor dimensions, as shown in Fig. (2). The sparsity of mutual information indicates which factors are \"used\" by the encoder. The sparsity of mutual information in the disentangled VAE is determined by the encoder, which focuses on intrinsic factor dimensions. Noise learning in \u03b2-VAE shows qualitatively influential factor traversals, with factor equivalence class theorem 4 explaining multiple semantic changes induced by the same learnt factor. The influential factor traversals are visually presented, showcasing the encoding of similar semantics among different factors. The independent unit Gaussian factor assumption in VAE facilitates efficient coding, with the number of learnt factors ideally matching the true factors. Despite pre-specifying a larger number of latent factors, only a few unit Gaussian variables are used in practice. This discrepancy is explained by the information conservation theorem, clarifying the ambiguity of disentanglement terminology. The disentanglement of learnt factors in this paper refers to their independence and ability to generate and be inferred from an oracle signal. The estimation of D KL (q \u03c6 (z)||p \u03b8 (z)) can quantitatively assess the disentanglement of each extracted factor. The disentanglement of extracted factors is challenging to achieve in an unsupervised manner. Even in ideal cases, factors may vary due to being obtained within an equivalence class induced by pre-specified factors. Gaussian Factor Equivalence theorem and Linear Factor Equivalence Class are discussed, stating that idealistic VAE can learn any factors set in the equivalence class without a one-to-one correspondence expectation. The Gaussian Factor Equivalence theorem states that a set of independent unit Gaussian random variables can generate each other through a linear homeomorphism mapping. This implies that there is a class of unit Gaussian random variables that can generate each other and have equivalent conservation information. The Linear Gaussian Factor Equivalence Class theorem further clarifies that Gaussian-VAEs have a linear matrix multiplication freedom degree. The Gaussian Factor Equivalence theorem states that Gaussian-VAEs can learn factors in the equivalence class if they have a linear matrix multiplication freedom degree. Empirical results support this. To enhance pattern separation and completion ability, auxiliary constraints on mutual information need to be introduced. The Gaussian Factor Equivalence theorem suggests that VAEs struggle to achieve a \"disentangled representation\" due to multiple semantic factor changes when altering one factor. This challenges the idea of a \"one-to-one\" correspondence between factors and visual concepts. The VAE model tends to learn an \"entangled representation\" when presetting \"oracle generating factors\" in the equivalent class. Neurons in the hippocampus of animals have been found to possess multiple representation capabilities. For example, some hippocampal neurons in rats involved in spatial representation also represented sound frequencies after specific training tasks. BID6 proposed a \"simulated factor\" based \"disentanglement\" metric, but it may suffer instability when evaluating disentanglement. The VAE model may struggle with instability when evaluating disentanglement. Calculating the disentanglement metric in real datasets is challenging as it requires prior knowledge of the generating factors. Indicators for assessing latent factor disentanglement include Estimation for I encoder (x; z) and Estimation for I encoder (x; z h). Calculating q * (z) is essential for these indicators, and it can be done through minimization equivalence. Through the minimization equivalence, q * (z) can be obtained by gradient method from solving an optimization problem. To address implementation issues in real situations, noise modeling is integrated into the model, crucial for influencing disentanglement in experiments. The entangled representation can result from the over-large searching space of q \u03c6 (z|x). The entangled representation in VAE models can be caused by the large search space of q \u03c6 (z|x) diverging from p \u03b8 (z). To address this, an auxiliary upper bound is added to the objective, leading to the \u03b2-VAE approach where \u03b2 > 1. Noise is considered a generating factor not of interest, and q \u03c6 (z) aims to be similar to p \u03b8 (z). The \u03b2-VAE approach, introduced by BID6, involves setting \u03b2 > 1 to address entangled representations in VAE models. By pre-specifying \u03c3 2 as Gaussian noise, tuning \u03b2 is equivalent to tuning \u03c3 2, saving time on extra experiments. MNIST and CelebA datasets are used for comparisons between Gaussian-noise modeling \u03b2-VAE and noise-specified \u03b2-VAE. The experiments on MNIST and CelebA datasets compare the performance of \u03b2-VAE with and without prespecified noise. Noise modeling significantly influences disentanglement, as shown in FIG4. Noise specifications also impact model evidence quantitatively and reconstruction qualitatively. Noise specified and noise learning \u03b2-VAE achieve similar disentanglement quantitatively based on D KL (q). The use of auxiliary constraints in \u03b2-VAE influences disentanglement differently, with a stronger suppression on the encoder and number of influential factors. Increasing \u03b2 leads to lower KL divergence and better reconstruction. Noise learning in \u03b2-VAE enhances suppression on the encoder, as it minimizes auxiliary constraints. The encoder in VAEs determines the \"used\" factors for disentanglement, with the ability to learn intrinsic factor dimensions. VAEs automatically suppress auxiliary factors under suitable noise assumptions, leading to good reconstruction abilities. Factor equivalence holds in VAEs, allowing for learning any factor set in the equivalence class. Single factors can result in multiple semantic concepts, while the same semantic concept can be encoded in different factors. The topology properties of oracle signal are used to prove the information conservation theorem in representation learning. The discussion on q \u03c6 (z|x) not being deterministic is crucial for word disentanglement. Further studies may explore data with different dimension manifolds and VAEs' pattern separation capabilities. Acknowledgment to Haodong Sun for interactions related to the paper. The VAE objective involves optimizing noise variance parameters \u03c3 2 in a model for better noise adaptation. The SGVB estimator is used, with \u03c3 2 as an optimization variable. The empirical variational lower bound (EVLB) is constructed from minibatches of data points, serving as an important indicator for model evidence in experiments. The empirical variational lower bound (EVLB) is utilized in an experiment to optimize noise variance parameters. An alternative optimization strategy is employed to iteratively update parameters and ensure convergence. The algorithm involves optimizing parameters and implementing a direct gradient method for increased likelihood. The noise setting is improved by considering a mixture of noise types. In an effort to improve noise settings, a mixture of Gaussian noise is used, which has been proven effective in various applications. The posterior distribution is factorized, and a lower bound of log likelihood is reformulated. The reparameterization trick is implemented, and the SGVB estimator is utilized to construct an estimator for the mean marginal likelihood lower bound. An estimator for the mean marginal likelihood lower bound of the full dataset is constructed based on minibatches from input dataset X. The EM algorithm is used to solve the model, with steps including Expectation Step to calculate the latent variable expectation and Maximization Step to update parameters by alternative optimization strategy. The algorithm updates gradient methods with respect to \u03b8, \u03c6, \u03a0, \u03a3 by initializing coefficients and sampling mini batches from the data. Theorems are proven regarding inverse mappings and homeomorphism mappings between different topology structures. The mean and variance of y are tested, leading to the conclusion that y consists of independent unit Gaussian random variables. Theorems are proven regarding inverse mappings and homeomorphism mappings between different topology structures. y consists of independent unit Gaussian random variables, which can generate z with a linear homeomorphism mapping. Corollaries provide upper and lower bounds for the capacity of the encoder network based on KL divergence and mutual information. The estimation in definition 4 is a lower bound for D KL (q \u03c6 (z)||p \u03b8 (z)), with similar results on M-NIST. Definitions 6 and 7 provide alternative estimations for the encoder. The \"disentanglement\" metric in \u03b2-VAE aims to create statistic points for each factor and assess model learning. The VAE model may still receive a bad score under the disentanglement metric in some cases, even when following idealistic conditions. The learned factor z can be in the equivalence class of the true factors v, and the statistic point z diff(y) can be calculated using the mean and variance of (z1 - z2). The location of the statistic point is uniquely determined by the orthogonal transformation Q, which can affect the model learning process. The orthogonal transformation Q uniquely determines the location of the statistic point in the VAE model. However, there are issues with the calculation of q * (z) and the KL-divergence of two Gaussians, leading to unstable disentanglement metrics. All experiments on MNIST had to be redone to correct these errors. The new experiment results on MNIST corrected previous errors in calculating disentanglement metrics. The experiments also addressed questions about the correlation between factors and the efficiency of coding in VAE models. The experiment randomly splits datapoints into training, validation, and testing sets. Indicators and latent variables are evaluated on the testing set. Latent code traversal is shown for VAE-based models over a specified range. Seed images used for inference come from the testing set. The model is trained on the training set with assumed noise variance. The latent code traversal in VAE-based models is shown over a range of [-3, 3]. Seed images from the testing set are used for inference. The model is trained on the training set, and indicators are evaluated on a subset of the testing set. Noise learning in \u03b2-VAE is set from [1, 40, 80, 120, 160]. The latent code traversal in VAE-based models is demonstrated over a range of [-3, 3], using seed images from the testing set for inference. The \u03b2 setting for noise learning in \u03b2-VAE ranges from 1 to 40. BID9 and Rezende et al. (2014) introduced a network inference/recognition model for efficient learning and inference in directed probabilistic models with continuous latent variables and in scalable datasets. They utilized the reparameterization trick for joint optimization of a variational lower bound. Several variations of VAE have been proposed to enhance generation quality and disentanglement of learned representations. These variations include improvements in generative and inference network structures, such as convolution/de-convolution and ladder structures. Other advancements involve mechanisms like iterative attention generation/inference and normalizing flow. Efforts have also been made to combine GAN with VAE for better reconstruction. Efforts have been made to combine GAN with VAE for better reconstruction and high-level abstract visual features embedding. Different approaches have been taken to address issues like unstable training and mode collapsing in GANs and assumptions about noise and clean data in VAEs. Regularization techniques and adversarial losses have been introduced to improve factor distribution and disentanglement of learned representations. InfoGAN introduced the infomax principle to GAN by adding mutual information regularization for better disentangled representation. BID6 proposed the \u03b2-VAE framework, enhancing constraints on KL-divergence and showing improved disentanglement performance compared to conventional VAE methods. The \u03b2-VAE allows flexible tuning of a parameter beta for balancing KL-divergence and likelihood terms, leading to diverse factor traversals."
}