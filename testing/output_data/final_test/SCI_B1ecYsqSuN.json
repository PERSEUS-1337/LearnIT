{
    "title": "B1ecYsqSuN",
    "content": "We propose a data augmentation technique using bilingual dictionaries for semi-supervised neural machine translation. This method aims to address the limitations of back-translation models in low resource settings by generating synthetic sentences with word-by-word translations from bilingual dictionaries. This approach expands the model's vocabulary while maintaining high-quality content. The method proposed aims to improve NMT performance in low-resource scenarios by using bilingual dictionaries to generate synthetic sentences with word-by-word translations. This approach expands the model's vocabulary while maintaining high-quality content. The proposed method utilizes bilingual dictionaries to expand vocabulary on both source and target sides, reducing the probability of out-of-vocabulary words in NMT. The Word-on-Word (WoW) data augmentation technique outperforms previous methods in low-resource settings. The proposed method utilizes bilingual dictionaries to expand vocabulary on both source and target sides, reducing the probability of out-of-vocabulary words in NMT. It outperforms previous data augmentation methods in a low-resource setting by benefiting from in-domain and out-of-domain monolingual data, showing encouraging results for domain adaptation. Additionally, the method is effective in enhancing performance when applied over other augmentation techniques. Our approach incorporates monolingual data through null sentences and hidden states from pre-trained language models. We use bilingual dictionaries for data augmentation, focusing on rare words in low-resource settings for German-English and Spanish-English translation tasks using the TED Talks corpus. For low-resource translation tasks, bilingual dictionaries are used for data augmentation. A 1-layer 256 dimensional encoder-decoder model with attention is employed for training. The transportation commissioner's work involves more than just traffic signals and stop signs. Our approach utilizes bilingual dictionaries for data augmentation in low-resource translation tasks. We use word-on-word translations to create synthetic data, which is then added to the parallel corpus for training our model. This method improves the quality of synthetic data compared to back-translation. Using bilingual dictionaries for data augmentation in low-resource translation tasks improves synthetic data quality compared to back-translation. Word-on-word translation ensures accurate words in sentences, expanding vocabulary and increasing coverage on both source and target sides. In comparison to back-translation, word-on-word translation (WoW) improves synthetic data quality by directly correlating new target words with source words. WoW expands vocabulary and coverage on both source and target sides, outperforming baselines like COPY and BT in low-resource translation tasks. The low performance of BT compared to COPY can be attributed to the poor quality source sentences it generates. WoW outperforms all baselines, including COPY, for both language pairs, beating the best data augmentation baseline by 0.85 points for es-en and 0.79 points for de-en. Increasing the size of monolingual data used for augmentation leads to substantial improvements, with gains of up to 3.8 BLEU points for de-en and 4.3 points for es-en. Adding 40k synthetic sentences brings more benefit than adding 10k high quality parallel sentences. In a scenario where only a small parallel corpus is available for a specific domain, utilizing out-of-domain monolingual data can lead to significant improvements in translation quality. For instance, using out-of-domain monolingual corpora resulted in performance similar to adding in-domain parallel samples. This approach can be beneficial when translating from a source domain with parallel corpus to a target domain with only monolingual data available. Using out-of-domain monolingual data can significantly improve translation quality in target domains like news. Methods like BT may not work well in this scenario due to lack of exposure to target domain data. The out-of-vocabulary issue is exacerbated, making the approach more appealing. Results show that training on augmented datasets in the target domain outperforms training only on the source domain. Future work could explore using domain-specific bilingual lexicons for domains like medicine. Combining WoW with other augmentation methods like COPY and BT is possible, with the combination with COPY being the most promising. In this work, the combination of WoW with COPY is explored, showing better performance compared to WoW with BT. Utilizing bilingual dictionaries for data augmentation in low resource NMT is proposed, with ground truth dictionaries used in the experiments. The results indicate that combining methods like COPY with WoW can outperform using a single method alone, while combining with BT leads to a decrease in performance. This suggests that a low-quality BT model does not provide complementary benefits. Future work includes creating synthetic samples using induced dictionaries and incorporating phrase tables for low resource NMT."
}