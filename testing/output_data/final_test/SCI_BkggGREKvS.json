{
    "title": "BkggGREKvS",
    "content": "In this work, the focus is on promoting inter-agent coordination in multi-agent reinforcement learning using policy regularization. Two approaches are discussed: inter-agent modelling and synchronized sub-policy selection. Experiments on four challenging tasks show significant improvements in cooperative problems. The methods are compared against MADDPG and three baselines, with a thorough hyper-parameter selection and training methodology ensuring fair comparison. Hyper-parameter sensitivity, sample-efficiency, and asymptotic performance are evaluated for each learning method. The focus is on promoting inter-agent coordination in multi-agent reinforcement learning using policy regularization. Two approaches, inter-agent modelling and synchronized sub-policy selection, lead to significant improvements on cooperative problems. The effects of the proposed regularizations on learned behaviors are analyzed. Multi-Agent Reinforcement Learning (MARL) involves training agents to maximize expected return by interacting with other learning agents in the environment. Centralized Training and Decentralized Execution (CTDE) is a popular framework for MARL, implemented by training critics to approximate joint observations and actions' value. In promoting inter-agent coordination in multi-agent reinforcement learning, two priors, TeamReg and CoachReg, are explored to regularize learned policies. TeamReg focuses on predicting teammates' behavior for coordination, while CoachReg involves using different subpolicies synchronously. These approaches aim to efficiently discover coordinated behaviors and surpass task-specific reward shaping and curriculum learning. The text discusses the proposal of two novel approaches to promote coordination in multi-agent systems by augmenting CTDE MARL algorithms with additional multi-agent objectives. It also introduces new sparse-reward cooperative tasks and evaluates the approaches against three baselines, showing that the TeamReg objective provides a dense learning signal for policy guidance. The TeamReg objective provides a dense learning signal for policy guidance in multi-agent systems, leading to the discovery of high-performing team strategies. CoachReg fine-tunes sub-behaviors for improved overall performance in cooperative tasks within the framework of Markov Games. MADDPG is an adaptation of DDPG for multi-agent settings, allowing training of decentralized policies through a centralized procedure. Each agent has its own policy and critic for action selection and value estimation, trained off-policy from previous transitions. Each agent in MADDPG has its own policy and critic for action selection and value estimation, trained off-policy from previous transitions. Centralized critics estimate the expected return for each agent using Deep Q-Network (DQN) loss. Policies are updated to maximize the expected discounted return of the corresponding agent. Many works in MARL focus on explicit communication channels between agents to discover a common communication protocol for success. The agents in MARL must establish a common communication protocol for success. Explicit communication is not always necessary as physical communication can suffice. Various approaches have been explored to shape agents' behaviors towards coordination, such as using mutual information to influence behavior. In MARL, agents use agent modelling to predict the effects of their actions on others and encourage social empowerment. This approach extends to incentivizing team-predictable behaviors. Additionally, Barton et al. (2018) introduce convergent cross mapping (CCM) to measure coordination effectiveness between agents, but it lacks a tool for enforcing coordination in real-time. In this work, two coordination-driven multi-agent approaches are designed to promote coordinated behaviors without explicit communication channels. The methods rely on team-objectives as regularizers to enhance coordination, similar to General Value Functions and Auxiliary tasks in Deep RL. The work introduces coordination-driven multi-agent approaches to promote coordinated behaviors without explicit communication. The novelty lies in agents' policy bias towards predictability for teammates or synchronous sub-policy selection. Pseudocodes for implementations are provided. The decentralized framework trains agents to predict teammates' actions based on their own observations, aiming for more coordinated behaviors. Loss for continuous control is defined as Mean Squared Error between predicted and true actions of teammates. The proposed method aims to drive teammates' behaviors closer to predictions by leveraging a differentiable action selection mechanism. It introduces a novel objective called team-spirit, which focuses on predicting teammates' actions and regulating predictability. This dual regularization, called TeamReg, aims to foster structured interactions among agents. The method aims to teach agents to recognize situations and select sub-behaviors simultaneously using policy masks for explicit sub-behavior selection. Each agent generates its own policy mask from observations, utilizing a structured dropout approach to modulate the policy network. The method introduces a policy masking mechanism where agents can switch between sub-policies swiftly. To promote synchronization, a coach entity is introduced during training to drive agents towards coordinated behavior modulation. During training, a coach entity is used to guide agents in selecting behavior masks that lead to high returns and are predictable. Each agent is encouraged to match the coach's mask and perform efficiently. The coach is removed during evaluation, and agents rely on their own policy masks. Gradients are propagated using the Gumbel-softmax trick with a temperature of 1. In training, a coach guides agents in selecting behavior masks for high returns. Gradients are propagated using the Gumbel-softmax trick with a temperature of 1. Tasks are based on OpenAI multi-agent environments, with tasks like SPREAD and CHASE modified for cooperative coordination. Visualizations and descriptions of tasks are provided in Figure 3. Agents receive their own global position as observation in all tasks. In experiments, agents learn with continuous action spaces and sparse rewards, incorporating new biases in CTDE multi-agent policy search algorithms. The methods are evaluated by extending MADDPG and comparing against vanilla MADDPG and its variations in cooperative tasks. The study compares different variations of MADDPG in cooperative tasks, with one method sharing policy and value-function models across agents. A standardized hyper-parameter search routine is used for all algorithms and environments, with models trained for 15,000 episodes using 50 hyper-parameter configurations and 3 training seeds. The best configurations are selected for further training on 10 seeds for improved performance. CoachReg shows significant performance improvements on three specific environments. The study compares variations of MADDPG in cooperative tasks, with CoachReg showing significant performance improvements on three environments. TeamReg performs well except on COMPROMISE, where it underperforms. Parameter sharing is best on CHASE due to the optimal play strategy. Ablated versions of the methods are also discussed. The study compares variations of MADDPG in cooperative tasks, with CoachReg showing significant performance improvements on three environments. TeamReg performs well except on COMPROMISE. MADDPG + policy mask performs similarly or worse than MADDPG, never outperforming CoachReg. MADDPG + agent modelling does not drastically improve on MADDPG. TeamReg does not significantly outperform MADDPG except on the SPREAD environment. The full TeamReg approach shows improvement over its ablated version, except for the COMPROMISE task. The poor performance of TeamReg on COMPROMISE is due to a competitive component where agents do not share rewards, leading to domination cases. Using a predictability-based team-regularization in a competitive task can lead to domination cases, where one agent becomes dominant due to high values of \u03bb 2. The dominated agent eventually starts pulling in its own direction, causing the average return over agents to drop. This experiment highlights the potential harm of using predictability-based team-regularization in competitive scenarios. Team-regularization in competitive tasks can be harmful as it aims to make behavior predictable to opponents. On SPREAD and BOUNCE, TeamReg improves performance over baselines. The effects of \u03bb 2 on cooperative tasks are analyzed to see if it makes the agent modeling task more successful. Comparisons are made between the best performing hyper-parameter configuration for TeamReg on the SPREAD environment and its ablated versions. The regularization loss becomes more important as training progresses. The regularization loss becomes more important as training progresses. \u03bb 2 OF F |\u03bb 1 OF F leads to the highest team-spirit loss. Agents decrease team-spirit loss with \u03bb 1 ON but it never reaches values as low as with the full TeamReg objective. Pushing agents to be predictable (\u03bb 2 ON) improves performance. Team-spirit loss increases as agents master the task. Predictability may help agents explore in a structured manner without reward signals. In this section, CoachReg aims to verify agents' behavior by analyzing their policy masks on different episodes. The entropy of the masks indicates the diversity of sub-policies used by the agents. Results show that agents use multiple sub-policies in all environments. Agents in different environments use at least two distinct masks with non-zero entropy, showing diversity in sub-policies. They tend to switch between masks more uniformly on SPREAD compared to other environments. The Hamming proximity between agents' mask sequences indicates synchronous selection of policy masks at test time. Agents produce similar mask sequences, more so than randomly choosing between two masks. Some settings lead to agents generating interesting results. Some settings result in agents coming up with interesting strategies, such as alternating between sub-policies based on the target's position. CoachReg improves performance on coordination tasks and yields expected behaviors. Future work could explore using entropy regularization and mutual information to enhance sub-policy disentanglement. Stability across hyper-parameter configurations is a challenge in Deep RL, with hyper-parameter searches showing empirical evaluation of algorithm robustness. Full results of hyper-parameter searches are shared in the appendix. Our proposed coordination regularizers improve robustness to hyper-parameters in Deep RL algorithms, even with more parameters to search over. Additional experiments show that the performance benefits hold when the number of agents is increased in the SPREAD task. However, the performance of all methods drops quickly as the number of agents rises, making the coordination problem more difficult. In this work, two policy regularization methods, TeamReg and CoachReg, were introduced to enhance multi-agent coordination within the CTDE framework. Empirical evaluation demonstrated significant improvements in performance on cooperative multiagent tasks. The complexity of tasks increases with the number of agents, making coordination more challenging. Future research could explore these regularizations with other policy search methods. The current work introduces two policy regularization methods, TeamReg and CoachReg, to improve multi-agent coordination in the CTDE framework. Future research could explore these regularizations with other policy search methods and investigate the interaction between incentives. Additionally, there is a need to address the limitation of single-step metrics by exploring model-based planning approaches for long-term multi-agent interactions in cooperative tasks. In a BOUNCE environment, two agents are connected by a spring and must position themselves for a falling ball to bounce towards a target. Rewards are given based on the ball's trajectory. Termination occurs when the ball reaches the target. In the COMPROMISE environment, two agents are linked by a spring and must reach their assigned landmarks to receive a reward. Landmarks are randomly relocated, and termination occurs after a set number of timesteps. In the CHASE environment, two predators chase a prey following a scripted policy. In the CHASE environment, predators coordinate to trap the prey using repulsion forces and teamwork. The prey has higher speed and acceleration, requiring strategic coordination to corner it. The experiment uses Adam optimizer for parameter updates and feedforward networks for models. In experiments, a buffer-size of 10^6 entries and batch-size of 1024 are used. Agents are trained for 15,000 episodes of 100 steps, then re-trained for 30,000 episodes for convergence. Exploration noise decreases linearly to 0 during training. Discount factor \u03b3 is 0.95, gradient clipping threshold is 0.5. CoachReg uses K=4 for sub-policies with hidden layers of size 128 and C=32. Target weights are updated, and actor is updated with all target weights. During hyper-parameter search, various parameters such as learning rates, soft-update parameter, and exploration noise scale are optimized. Regularization weights are also tuned for TeamReg and CoachReg. The coach's learning rate is set equal to the actor's to simplify the search space. Values for hyper-parameters are drawn uniformly from specified ranges. During hyper-parameter search, parameters like learning rates and exploration noise scale are optimized. The best policy model is saved at the end of training and re-evaluated for final performance. Tables show the best hyper-parameters found for different environments and algorithms. Performance distribution is reported in Figure 10. The performance distribution across hyper-parameter configurations for each algorithm on each task is depicted in Figure 10. TeamReg and CoachReg boost the performance of the third quartile, increasing robustness. Learning curves for TeamReg and baselines on COMPROMISE are shown in Figure 14, indicating TeamReg tends to make one agent much stronger than the other. This domination is optimal until the dominated agent picks up the task, causing the dominant agent's return to decrease dramatically. In experiments with different masks, agents show synchronization, except on CHASE where they display dissimilarity and use fewer masks. This lack of synchronization on CHASE may explain why CoachReg does not improve performance in that environment. Agents in the CHASE environment do not synchronize well with different masks, which may explain why CoachReg does not enhance performance in that setting. The agents switch between policy masks during episodes, with the group selecting the same mask as suggested by the coach. The mean entropy and Hamming proximity of the mask distribution are compared for the \"MADDPG + policy mask\" approach. The \"MADDPG + policy mask\" approach results in agents displaying more diverse mask usage compared to CoachReg. However, the agents' mask selection is less synchronized with \"MADDPG + policy mask\" than with CoachReg. CoachReg tends to reduce diversity to have agents agree on a common mask, leading to more synchronized mask selection. The \"MADDPG + policy mask\" agents show more synchronization in mask selection compared to agents independently sampling masks from k-CUD. The entropy of policy mask distributions and Hamming Proximity were measured for each task and method. The task complexity increases as the number of agents in the SPREAD task increases from three to six. The best hyper-parameter configuration was used for each algorithm, and results are shown in Figure 18. The task complexity increases as the number of agents in the SPREAD task increases from three to six. Proposed methods outperform baselines even with increased regularization pressure from additional agents."
}