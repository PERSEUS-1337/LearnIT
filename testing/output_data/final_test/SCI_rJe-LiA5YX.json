{
    "title": "rJe-LiA5YX",
    "content": "The proposed method for mathematical optimization is based on flows along geodesics, utilizing Exponentially Decaying Flows (EDF) to converge on local solutions exponentially. Experimental results demonstrate high performance on optimization benchmarks and potential for good generalization in machine learning tasks. Developing sophisticated methods for solving hard optimization problems is crucial in the field of machine learning. In the field of machine learning, efficient optimization methods are crucial for achieving good generalization performance from statistical models. A new mathematical perspective on optimization is introduced to address the challenge of finding solutions that minimize objective functions and adjust parameters for optimal statistical estimator results. This method shows rapid convergence and compatibility with deep learning techniques, offering good statistical properties. Many optimization methods have been proposed and tailored to specific problems or models in this field. In machine learning, optimization methods like gradient descent, AdaGrad, and Adam are used to minimize objective functions and adjust parameters for optimal statistical estimator results. These methods offer efficient convergence rates and are tailored to specific problems or models. Second-order methods like Newton and Gauss-Newton are also studied in mathematical optimization theory. Second-order methods like the Newton and Gauss-Newton methods have strong convergence properties and can potentially overcome plateau problems. However, they are not widely used in machine learning due to high computational costs and lack of guaranteed properties in practical settings. K-FAC is a developing second-order method that shows high convergence rates with lower computational costs. In our approach, we introduce a Riemannian metric induced by non-linear functions to create dynamical systems for optimization. This allows us to design flows to control convergence rates, particularly with deep neural network models. The method shows promise in improving generalization performance and is applicable to mathematical optimization problems. In this section, we establish essential properties of dynamics for analyzing non-linear equations. We focus on a well-posed case with a unique solution \u03be, where a positive matrix induces a Riemannian metric on a closed subset \u2126. The time evolution of variable w on this manifold involves studying dynamical systems where w(t) moves on geodesics with respect to the metric g. The Euler-Lagrange equation for a Lagrangian L on a manifold \u2126 with respect to metric g is dp/dt = Gv, where v = dw/dt. By imposing boundary conditions, a geodesic between two points in \u2126 can be obtained as a solution. An initial condition leads to motion along the geodesic. Theorem 2.1 states that for a solution of a specific equation with an initial condition, w satisfies Fw(t) = (1 \u2212 t)Fw0. The paper discusses functions of w as those of t, deriving equations using the Beltrami identity and initial conditions. A closed form expression is obtained, leading to a differential equation with a different monotonically decreasing function. The motion described by this equation differs from the previous one but follows the same geodesic. Theorem 2.2 introduces a differential equation for an arbitrary point w0 in \u2126. The differential equation with initial condition w0 has a unique solution satisfying F(w(t1)) = 0, and the orbit under flow f coincides with the geodesic equation. The end point t1 can be set as \u221e under certain conditions, with an exponential convergence rate for F. An exponentially decaying flow (EDF) is defined for the non-linear equation F(w) = 0. An exponentially decaying flow (EDF) is introduced for the non-linear equation F(w) = 0, with a connection to the classical Newton method. A method based on EDF is developed for mathematical optimization problems in a compact subset \u2126 \u2282 R^N with no stationary points except for a minima. An example includes least squares problems where the optimization problem is equivalent to solving the non-linear equation F(w) = 0. The problem involves solving the non-linear equation F(w) = 0 by setting up an equation where the gradient of the loss function is zero. The gradient is expressed as \u2207 L(w) = J T \u03d5 F(w), with the Hessian H of the loss function L with respect to w. To simplify computations, alternative equations with similar properties to the original equation are sought. One such equation is considered, which deviates from the original motion along the geodesic. The text discusses the convergence properties of the geodesic related to a specific equation, showing that under certain conditions, the function converges to 0. It introduces two types of flows, EDF-H and EDF-G, and highlights the computational challenges of matrix inversion in EDF-based methods, especially in dealing with rank deficient matrices. In optimization problems, rank deficient matrices are common. To address this, pseudo-inverse matrices are used, which are computationally heavy. A projection method is considered for numerical computations, utilizing a differential equation approximated by a Krylov subspace of order k. This method interpolates between gradient methods and other approaches, with the coefficient c providing information about the matrices G or H. The Levenberg-Marquardt algorithm modifies equation FORMULA0 by adding a damping factor to G for stability. The damping factor \u03bb is a positive function of t, making the method well-behaved near singularities and compatible with stochastic approaches like mini-batch training. The Levenberg-Marquardt algorithm introduces a damping factor \u03bb to equation FORMULA0 for stability and compatibility with stochastic approaches like mini-batch training. The method can be accelerated by converting FORMULA0 into a second-order differential equation with \u03ba as a real-valued function of t, controlling convergence properties. Two ways of determining \u03ba are setting it as a constant \u03b1 or as \u03b1t^(-1). In deep learning optimization, the momentum gradient descent method can be enhanced by setting \u03ba(t) = \u03b1t^(-1) (W2), similar to Nesterov's acceleration scheme. The optimization problem involves training neural networks with input and output datasets x and y, dimensions dx and dy, and parameters w. The goal is to minimize a loss function for image classification tasks. In deep learning optimization, the loss functions are set in two typical cases. The first case sets the loss as (14) where F = \u03d5 and H L = I. In the second case, the loss is cross entropy with softmax function. The study conducted experiments on optimization performance of EDF-based methods and generalization performance in standard settings. In this study, experiments were conducted to test optimization and generalization performance using residual network models and CIFAR-10/100 datasets. Techniques of data augmentation and regularization were incorporated to measure effectiveness in practical settings. The focus was on exploring how statistical results relate to optimization when minimizing loss functions, with hyperparameters tuned accordingly. The conjecture on non-convex optimization problems in deep learning remains an open issue, not discussed in the experiments. In this paper, EDF-based methods (type G and type H) were evaluated for convergence performance on a CIFAR-10 data-fitting problem using a convolutional neural network. The experiments focused on the impact of hyperparameter k on the convergence performance of EDF-G. The performance of EDF-G depends on hyperparameter k, which interpolates between gradient and dynamics methods. Comparisons with EDF-H and other optimizers show EDF-G leads to faster loss decrease. The EDF-G optimizer outperformed standard optimizers by decreasing loss more rapidly, even when reduced to gradient methods at k = 1. EDF-G showed better overall convergence rates compared to EDF-H. Second-order methods on full-batch training converge quickly but result in worse generalization compared to stochastic approaches with mini-batches. EDF also suffers from poor generalization with full-batch training, but setting k small allows it to be compatible with stochastic approaches. In experiments comparing EDF-G with MSGD and Adam on CIFAR-10/100 classification tasks using Resnet models, data contamination was found in CIFAR-100. 9 pairs of data with the same image but different labels were excluded from training, and the remaining data was used. Training loss was computed at the end of each epoch using a specific formula. The training loss was defined using a specific formula based on the total number of data. Batch normalization statistics were calculated using an \"inference mode\" approach. Tests were conducted with different parameters for EDF and Momentum SGD, with the best convergence performance chosen for each optimizer. Adjustments were made to the learning rate for stability in convergence during optimization. In the 20-th epoch, EDF showed better optimization and generalization performance compared to Adam and Momentum SGD. Data augmentation and regularization techniques were employed to improve generalization performance. Rate decay scheme had a significant impact on the results. In the previous sections, rate decay schemes significantly impact optimization performance. Different rates were tested for MSGD, Adam, and EDF, with rates reset at specific epochs to improve performance. The comparison results are shown in Figures 4. The comparison results in Figures 4 and 5 show that EDF achieved better optimization performance with good generalization levels. EDF-based methods are more likely to achieve optimal solutions that generalize well compared to other standard optimizers. In comparison to other standard optimizers, EDF-based methods show better optimization performance and generalization levels. These methods have high potential for various tasks and can be optimized further through future studies. Computationally, EDF-based methods with GPU can perform Jacobian-vector-product at a similar cost as the gradient of the loss function. Pseudo-codes for update schemes of EDF-G with L/R-op are provided in Algorithm 1 and Algorithm 2 for specific cases. The convolutional network, similar to Section 6.1, was used on MNIST. Full-batch training per step is compared to stochastic training per epoch with a mini-batch of size 500. The dataset is divided into subsets for calculations. The computational cost of full-batch training is similar to that of mini-batch training. CIFAR-100 dataset includes 9 pairs of irregular data with the same image but different labels. In the CIFAR-100 dataset, there are 9 pairs of irregular data with the same image but different labels, which contaminate the training process and affect accuracy. When the network model was optimized with the full dataset, accuracy stagnated at 99.982%. Generalization also deteriorated with irregular data. The rate decay scheme was used to optimize and generalize performance, setting the learning rate to 0.2 times the initial value at the end of the 20th epoch."
}