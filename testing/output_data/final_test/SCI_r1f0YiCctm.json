{
    "title": "r1f0YiCctm",
    "content": "While deep neural networks are successful, reducing model size is crucial due to energy consumption and storage constraints. Variational weight distributions allow for more efficient coding schemes and higher compression rates compared to deterministic weights with techniques like pruning and quantization. Encoding network weights using a random sample requires fewer bits based on the Kullback-Leibler divergence. Our method, utilizing variational weight distributions, allows for explicit control of compression rates while optimizing expected loss. It outperforms previous approaches in neural network compression, achieving the best test performance for a fixed memory budget and highest compression rates for a fixed test performance on benchmarks like LeNet-5/MNIST and VGG-16/CIFAR-10. Increasing efficiency in deep learning models is a key challenge due to substantial memory requirements that can conflict with storage constraints. Memory requirements in neural networks can conflict with storage and communication constraints, especially in mobile applications. Compressing deep learning models through techniques like pruning, quantization, and coding has become a priority goal with economic and ecological benefits. Coding is the central routine, while pruning and quantization help reduce the entropy of weight distribution for shorter encoding lengths. Bayesian Compression also falls into this scheme. In the context of compressing deep learning models, a novel coding method is proposed in this paper to achieve higher compression rates. Unlike traditional approaches, this method encodes a random weight-set from the full variational posterior, departing from deterministic weight-set construction. This new approach aims to realize the efficiency postulated by the bits-back argument. The proposed coding method in this paper, called Minimal Random Code Learning (MIRACLE), selects a random weight-set from the variational posterior for efficient encoding. This approach, based on the bits-back argument, offers control over expected loss and compression size, providing higher compression rates for deep learning models. The MIRACLE compression algorithm offers explicit control over expected loss and compression size by using a penalty factor to adjust the KL-divergence. It outperforms previous techniques by achieving better performance on the test set for any desired compression rate. MIRACLE is an innovative compression algorithm that utilizes deep learning models' noise resistance. It efficiently encodes a random set of weights, offering explicit control over loss and compression size. The algorithm demonstrates superior performance in compressing neural networks compared to previous state-of-the-art methods. An early approach in network weight pruning is Optimal Brain Damage (LeCun et al., 1990), which uses the Hessian of weights to prune without affecting performance. A simpler approach in BID4 truncates small weights to zero, resulting in significantly smaller networks without performance loss. This approach evolved into Deep Compression, combining weight pruning, quantization, and Huffman coding for compression. The effectiveness of compression relies on encoding both network weights and training targets. HashNet proposed by BID1 uses hashing to enforce groups of weights to share the same value, achieving memory reductions of up to 64\u00d7 with gracefully degrading performance. Weightless encoding by BID14 shows neural networks are resilient to weight noise, leading to a lossy compression algorithm. Bayesian Compression by BID13 utilizes a Bayesian variational framework and the bits-back argument for compression. Bayesian Compression involves equipping network weights with a prior and approximating the posterior using a variational framework. The bits-back argument connects this framework to the Minimum Description Length principle, aiming to transmit targets with minimal message length using a neural network. The weight-set w* obtained from maximizing the framework is used to encode the targets' residuals efficiently. The weight-set w* encodes the residuals of the targets with a prior distribution, resulting in a message of length DISPLAYFORM1. This message allows for perfect reconstruction of the original targets and the variational distribution q \u03c6. By subtracting the length of the \"free message,\" the net cost of encoding the weights is KL(q \u03c6 ||p) = E q \u03c6 [log q \u03c6 p] nats, recovering the ELBO as negative MDL BID8. Bayesian Compression involves model compression by imposing a sparsity inducing prior distribution to aid pruning and quantization techniques. It produces a deterministic weight-set w* encoded similarly to previous works. The weight-set is interpreted as a sequence of i.i.d. variables with values from a finite alphabet. Bayesian Compression involves model compression by imposing a sparsity inducing prior distribution to aid pruning and quantization techniques. The weight-set w* is interpreted as a sequence of i.i.d. variables with values from a finite alphabet. According to Shannon's source coding theorem, w* can be coded with no less than NH[p] nats, achieved by Huffman coding. Shannon-style coding schemes are optimal when the variational family is restricted to point-measures. By extending the variational family to more general distributions q, the coding length KL(q||p) can be reduced. A method is developed to exploit the uncertainty represented by q to encode a random weight-set with a short coding length. Bayesian Compression involves model compression by imposing a sparsity inducing prior distribution to aid pruning and quantization techniques. A variational approach offers a solution for training a neural network within a memory budget constraint. The approach is based on the Minimum Description Length (MDL) principle and uses an encoding distribution rather than a prior. The goal is efficient coding by encoding parameters rather than data. The variational objective is related to the \u03b2-VAE, where parameters are encoded using a parameterized variational family. The variational objective related to the \u03b2-VAE aims to achieve good training performance and represent the model with a short code. By maximizing a weight-set drawn from the variational distribution, the model can be efficiently encoded. This communication problem involves training a variational distribution on a data set and sending a message efficiently. The message length needed to send a sample distributed according to q \u03c6 depends on the unknown data distribution p(D). The variational distribution q \u03c6 can be interpreted as a conditional distribution q(w|D), with an assumption that q(w|D)p(D) yields the encoding distribution p(w). This assumption is similar to a Bayesian setting, where data distribution is given as p(D) = p(D|w)p(w)dw. The message length cannot be smaller than KL(q \u03c6 ||p) due to the data processing inequality. The message length can approach the lower bound if Alice and Bob share a source of randomness. The results establish a characterization of mutual information in terms of minimal coding. Theorem 3.1 guarantees this principle. Theorem 3.1 guarantees near optimal efficiency in drawing a sample from a shared random source. An algorithm achieving this lower bound is proposed, utilizing a pseudo-random generator. An alternative method to produce an approximate sample from q \u03c6 is presented in Algorithm 1. Algorithm 1 presents an alternative method to generate an approximate sample from q \u03c6. The algorithm utilizes a discrete proxy distribution q with support on K samples drawn from p, where the probability mass for each sample is determined by importance weights. Decoding the sample is straightforward by drawing the k * th sample from the shared random generator. While simple and easy to implement, the correctness of this approach is a valid concern. The text discusses the use of Algorithm 1 to generate an approximate sample from q \u03c6 using a proxy distribution q based on importance sampling. The number of required samples grows exponentially, making it impractical for encoding a neural network. Despite potential bias due to limited samples, the algorithm converges to the correct distribution in the limit. Further practical considerations are addressed in Section 3.3. The text discusses the importance sampling estimator for unnormalized distributions, showing that E[q\u03c6[f]] is approximately equal to Eq[f] with high probability for any measurable function f. This result holds true for a specific function involving the variational objective. The text discusses the importance sampling estimator for unnormalized distributions, showing that E[q\u03c6[f]] is approximately equal to Eq[f] with high probability for any measurable function f. This result holds true for a specific function involving the variational objective. In this section, the application of Algorithm 1 within a practical learning algorithm Minimal Random Code Learning (MIRACLE) is described. In the context of the importance sampling estimator for unnormalized distributions, the text describes the practical learning algorithm Minimal Random Code Learning (MIRACLE) using Algorithm 2. Gaussian distributions with diagonal covariance matrices are used for q \u03c6 and p, with shared parameters for p within each layer of the encoded network. The encoding distribution is adapted to the task, allowing for effective variational training. The overall problem is divided into sub-problems with global and local coding goals, where the weight vector w is split into equally sized blocks with specific coding allowances. The text describes the practical learning algorithm Minimal Random Code Learning (MIRACLE) using Algorithm 2. It involves dividing the weight vector into equally sized blocks with specific coding allowances and imposing block-wise KL constraints using penalty factors. The encoding distribution is adapted to the task for effective variational training. Random splitting into blocks is efficiently coded via a shared random generator, with convergence ensured through a large number of training iterations. Variational updates alternate between encoding blocks and updating the variational distribution for not-yet coded weights. The text describes the practical learning algorithm Minimal Random Code Learning (MIRACLE) using Algorithm 2, which involves dividing the weight vector into blocks and imposing block-wise KL constraints. The encoding distribution is adapted for effective variational training, with a hashing trick further improving performance. Experiments were conducted on LeNet-5 and VGG-16, showing improved compression rates. The study conducted experiments on LeNet-5 and VGG-16 using state-of-the-art methods like Deep Compression and Weightless encoding. The hashing trick was applied to reduce layer sizes, with specific reductions for different layers. The local coding goal and number of variational updates varied for each model. The study compared the performance of MIRACLE with baseline methods on LeNet-5 and VGG-16 models. MIRACLE outperformed competitors by achieving better compression for a given test error rate and lower test error for a given model size. The construction of a Pareto frontier for MIRACLE was easier compared to other methods due to the direct reflection of hyper-parameters on compression size. The paper discusses the bits-back argument for coding model parameters in neural networks, emphasizing the importance of selecting a \"cheap\" weight set from the many \"good\" parameterizations. Their algorithm, MIRACLE, outperforms previous state-of-the-art methods in terms of compression and test error rates. Future work includes optimizing MIRACLE for memory efficiency, energy consumption, and inference time. The paper introduces MIRACLE, an algorithm for coding model parameters in neural networks to achieve compression and reduce test error rates. It aims to optimize for memory efficiency, energy consumption, and inference time by utilizing pseudo-random generators and compressed models. The method proposed by BID16 shows that the encoding length can be bounded, with an upper bound of |l(n)| = log n + 2 log log(n + 1) + O(1)."
}