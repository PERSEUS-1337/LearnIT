{
    "title": "S1epu2EtPB",
    "content": "In this paper, a fully convolutional model is proposed for generating video sequences directly in the pixel domain. The model uses a stochastic fusion mechanism to obtain a latent video representation from start and end frames, progressively increasing temporal resolution and decoding with 3D convolutions. It is trained end-to-end with an adversarial loss and can generate diverse and meaningful in-between video sequences. Recent advancements in computer vision and machine learning have made the goal of automatically turning comic books into animations more achievable. Video inbetweening, the process of creating intermediate frames between key frames, is at the core of this challenge. The development of neural network architectures and generative adversarial networks has led to significant progress in image and video synthesis. The problem of inbetweening in video generation has received less attention compared to unconditional video generation and video prediction tasks. Existing works typically use recurrent neural networks with LSTM or GRU cells to capture long-term dependencies. In video generation, recurrent models are often used to capture long-term dependencies. However, this paper introduces a stateless, fully convolutional model for video inbetweening. This approach offers simplicity, shorter gradient paths, deeper networks, and easier parallelization. The model enforces temporal consistency with start and end frames as inputs, making it effective for video inbetweening tasks. The proposed model for video inbetweening consists of a 2D-convolutional image encoder, a 3D-convolutional latent representation generator, and a video generator. It is crucial to separate the generation of the latent representation from video decoding for successful inbetweening. Stochastic fusion of key frame representations is used to progressively increase temporal resolution. The model for video inbetweening involves stochastically fusing key frame representations to increase temporal resolution and generate realistic video sequences. Diverse sequences can be generated by varying the input noise vector. The paper reviews related literature, describes the proposed model, presents experimental results, and discusses recent advances in video prediction and generation. Video prediction involves producing future frames based on past frames of a video sequence using deterministic methods trained to minimize L2 loss. Early works used recurrent neural networks, with later advancements like LSTM encoder-decoder and convolutional LSTM models. Video prediction methods have evolved from using recurrent neural networks to more advanced techniques like LSTM and 3D convolutional neural networks. Recent approaches focus on predicting future frames directly in the pixel domain or by estimating local and global transformations before predicting the next frame. Some methods also incorporate adversarial loss to ensure realistic predictions. Video generation aims to model future frames in a probabilistic manner using generative adversarial networks (GAN) and variational autoencoder networks (VAN). Different architectures have been proposed, such as one with two generators for foreground and static background pixels, along with a discriminator to distinguish between real and generated video sequences. Saito et al. (2017) propose a temporal generator and image generator based on CNNs to produce video sequences. Denton & Fergus (2018) suggest a variational approach to address mode collapse in GANs by recursively generating frames using LSTM. Babaeizadeh et al. (2018) use a variational approach to sample latent vectors for frame prediction network. In contrast to other methods, variational autoencoders aim to generate diverse and realistic video sequences by combining variational and adversarial loss functions. The latent vector is divided into content and motion components, leading to improved quality compared to previous approaches. In Villegas et al. (2017a), two encoders separate motion and content to generate hidden representations for video sequences, fused using a variational network. Adversarial loss enhances realism. Oh et al. (2015) predicted up to 100 frames in Atari games using CNN or LSTM encoding. Villegas et al. (2017b) extended this to real-world videos, estimating high-level features first. Video interpolation techniques are used to increase the temporal resolution of video sequences. Different approaches include optical flow-based interpolation, phase-based interpolation, and pixels motion transformation. These methods aim to predict high-level structures from past frames and generate future frames without requiring ground truth landmarks as supervision. Long-term video interpolation techniques focus on generating frames between existing ones with a large temporal gap. Various methods have been explored, including block-based motion estimation/compensation and convolutional LSTM models. Our work aligns with generative approaches, utilizing convolutional encoders to generate hidden representations of the first and last frames for interpolation. The proposed model aims to increase temporal resolution by interpolating frames between extended reference frames. It compares with existing methods and generates in-between frames using a video generator with three components: an image encoder, a latent representation generator, and a video generator. The model receives start and end frames along with a noise vector to produce a video sequence. The model consists of an image encoder, latent representation generator, and video generator, along with video and image discriminators for adversarial training. The image encoder processes input video frames to produce feature maps, while the latent representation generator fills in video content between start and end frames. The encoder architecture includes 4x4 convolutions and 3x3 convolutions to condense the feature map to the target depth. The model architecture includes a series of residual blocks that gradually fill in video content between start and end frames in the latent space defined by the image encoder. Each block learns a transformation to improve the generated video content. The representation length within each block is denoted as T (l). The model architecture consists of residual blocks that fill in video content between start and end frames in the latent space. Each block has a representation length denoted as T (l) and uses a layer-specific noise tensor for stochastic gating functions to fuse encoded representations. The model architecture involves broadcasting encoded representations along the time and spatial dimensions, followed by fusion and convolution steps using 3x3x3 kernels in a residual unit. The generation of the latent video representation is achieved through a coarse-to-fine scheme in the time dimension. The video generator uses a coarse-to-fine scheme in the time dimension, doubling the length of z every L/3 generator blocks. The generator alternates between regular and transposed convolutions for spatial up-sampling, producing the output video sequence. The model is trained end-to-end using an adversarial loss function with two discriminators: a 3D convolutional video discriminator and a 2D convolutional image discriminator. The architecture differs from previous work, with a single output for the entire video and a Resnet-based image discriminator. Training involves optimizing the non-saturating log-loss function. During optimization, the model adopts the non-saturating log-loss and replaces the average over intermediate frames with a single uniformly sampled frame to save computation. The discriminators are regularized by penalizing the derivatives of the pre-sigmoid logits to improve GAN stability. Batch normalization is used on all convolutional layers in the generator, while layer normalization is used in the discriminator. The architectural details of the encoder, decoder, and discriminators are provided in Appendix A. The approach was evaluated on three public datasets: BAIR robot pushing, KTH Action Database, and UCF101 Action Recognition Data Set. Videos were down-sampled and cropped to 64\u00d764, with subsequences of 16 frames used in experiments. Train/test splits were adopted for all datasets. The study used conventional train/test splits for datasets and a validation set for model checkpoint selection. No dataset-specific tuning was done, focusing on generating plausible transition sequences. The primary evaluation metric was the Fr\u00e9chet video distance (FVD), adapted from the Fr\u00e9chet Inception distance (FID) for videos. The study utilized the Fr\u00e9chet video distance (FVD) as a metric for evaluating video inbetweening, which compares the distributions of generated and ground-truth videos in an embedding space. The FVD is considered more suitable than the structural similarity index (SSIM) for this task. The model was tested by running it 100 times for each pair of key frames, generating different sequences with varying noise vectors u, and computing the FVD for each sequence. The study used the Fr\u00e9chet video distance (FVD) to evaluate video inbetweening by comparing distributions of generated and ground-truth videos. The model was tested by generating sequences with noise vectors and computing FVD. Training involved using the ADAM optimizer with specific parameters and took around 5 days on a Nvidia Tesla V100 GPU. A checkpoint was saved every 5000 steps, with evaluation based on the lowest FVD on the validation set. The study compared a full model with baselines that omit certain components, including a baseline without fusion and a naive approach that skips the latent video representation generator. The naive approach uses transposed 3D convolution directly on encoded representations to generate frames, incorporating spatially uniform noise for stochasticity. The results in Table 1 demonstrate the importance of the dedicated stochastic fusion mechanism and separate latent video representation generator. The study emphasized the significance of the dedicated latent video representation generator and stochastic fusion for improving video generation quality. Results showed statistically significant differences at a 95% confidence level across all datasets. The model's generated sequences aim to be similar in style and consistency with given start and end frames, as demonstrated in Figure 2. Good models for stochastic generation should produce high-quality and diverse samples, measured by average pairwise cosine distance in the FVD embedding space. Incorporating fusion increases sample diversity significantly in the FVD embedding space. A qualitative illustration in Figure 3 shows the diversity in generated videos. The average SSIM for the method is computed for each dataset to compare results with existing methods. Existing methods for video inbetweening include RNN-based video generation and optical flow-based interpolation. Our model generates 14 frames, twice as long as other methods, and is evaluated based on SSIM over 10 training runs. The diversity in generated videos is increased significantly with fusion, as shown in Figure 3. Our model generates 14 frames using direct 3D convolutions, outperforming existing methods on challenging datasets like UCF101. Optical flow-based methods achieve similar SSIM as RNN-based methods on BAIR and KTH, suggesting limitations in testing video inbetweening models. Our model for video inbetweening uses direct 3D convolutions and a stochastic gating mechanism to fuse information from key frames. Despite lacking recurrent components, it achieves good performance on benchmark datasets, offering a new perspective for future research on video generation."
}