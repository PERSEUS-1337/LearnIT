{
    "title": "SknC0bW0-",
    "content": "Bayesian optimization has been successful in optimizing expensive black-box functions, especially for tuning neural network hyperparameters. Methods like random search and multi-fidelity BO can outperform standard BO by using cheap approximations. A new algorithm, cfKG, is proposed for Bayesian optimization when fidelity is controlled by continuous settings. It focuses on sampling points with the highest value per unit cost. Bayesian optimization, including methods like random search and multi-fidelity BO, has been successful in optimizing expensive black-box functions, particularly for tuning neural network hyperparameters. A new algorithm, cfKG, is introduced for Bayesian optimization in settings where derivatives are available, outperforming state-of-art algorithms in various experiments. It focuses on sampling points with the highest value per unit cost, showing superior performance in optimizing synthetic functions, tuning CNNs, and large-scale kernel learning. In this paper, the focus is on optimizing expensive black-box functions using Bayesian optimization with evaluations of multiple fidelities and costs. The fidelity is controlled by continuous parameters, and a real-valued function g(x, s) is used to model these evaluations. The goal is to find ways to solve optimization problems more quickly by using computationally inexpensive approximations of the function f(x). The function g(x, s) is used to model evaluations of expensive black-box functions with multiple fidelities and costs. It can be evaluated with noise at a cost depending on x and fidelity-control parameters. This approach is applicable in hyperparameter tuning and other scenarios where cheap approximations of an expensive objective function can be observed. The algorithm BID3 uses the knowledge gradient approach to adaptively select hyperparameter configurations and fidelities for system evaluation. It supports parallel function evaluations and maximizes the value of information against its cost. Our approach, the continuous-fidelity knowledge gradient (cfKG) method, maximizes the value of information against its cost by choosing the point and fidelity to sample next. It differs from existing methods by considering the impact on the future posterior distribution over the full feasible domain. The cfKG method maximizes the value of information by selecting the next point and fidelity to sample, outperforming benchmark algorithms in various optimization tasks. The paper also reviews related work, presents the method, and tests it on synthetic functions and hyperparameter tuning for deep learning and kernel learning. In hyperparameter tuning, methods like Hyperband and modified expected improvement (EI) acquisition function are proposed for multi-fidelity settings. These methods aim to improve performance in training convolutional neural networks by adaptively allocating resources and selecting points to sample based on value vs. cost. In hyperparameter tuning, methods like Hyperband and modified EI acquisition function are proposed for multi-fidelity settings to improve performance in training convolutional neural networks by adaptively allocating resources and selecting points to sample based on value vs. cost. In the discrete-fidelity setting, BID1 proposes an early stopping criterion combined with SMAC, while BID7 generalizes the UCB criteria. Continuous-fidelity methods have shown to find good solutions faster than discrete-fidelity ones. In hyperparameter tuning, methods like Hyperband and modified EI acquisition function are proposed for multi-fidelity settings to improve performance in training convolutional neural networks. Continuous-fidelity methods have shown to find good solutions faster than discrete-fidelity ones, with cfKG providing improved performance in numerical experiments compared to other methods. This improvement may be due to cfKG selecting the point and fidelity jointly, considering the impact of fidelity choice on the best point to sample. This paper introduces a knowledge gradient method for continuous-fidelity settings, which is a novel approach compared to existing methods for discrete-fidelity settings. This paper presents the first multi-fidelity batch Bayesian optimization algorithm, extending the knowledge gradient method to continuous-fidelity settings and generalizing it to batch and derivative-enabled settings. The method builds upon existing work in single-fidelity knowledge gradient methods and computational techniques. In this section, the continuous-fidelity knowledge gradient (cfKG) algorithm is introduced for Bayesian optimization. It leverages low-fidelity approximations to improve performance in both derivative-free and derivative-enabled settings. The algorithm iteratively fits a statistical model to previous samples and selects the next sample based on maximizing an acquisition function. The continuous-fidelity knowledge gradient (cfKG) algorithm utilizes Gaussian process regression to model g(x, s) and its cost of evaluation. It introduces an acquisition function that values sampling based on information gained versus cost. A computational technique is generalized to maximize this function efficiently, with extensions discussed for the derivative-enabled setting. A Gaussian process prior is placed on the function g or its logarithm. The GP prior is defined by its mean and kernel functions with hyperparameters for inference. Evaluations of g(x, s) have additive normally distributed noise with variance \u03c3^2. The posterior distribution of g after n function evaluations is GP(\u00b5_n, K(n)) with hyperparameters including variance \u03c3^2. When optimizing hyperparameters, it is recommended to use a GP prior on the logarithm of the validation error instead of directly on the error itself. This approach allows for better modeling of the range of values assumed by the GP and makes it easier to handle steep climbs in error values. Additionally, a separate GP can be trained on the logarithm of the cost of evaluating the error. The cfKG method samples points and fidelities to maximize an acquisition function based on the knowledge gradient concept, which values the information gained from additional samples. When optimizing hyperparameters, it is recommended to use a GP prior on the logarithm of the validation error instead of directly on the error itself. This approach allows for better modeling of the range of values assumed by the GP and makes it easier to handle steep climbs in error values. Additionally, a separate GP can be trained on the logarithm of the cost of evaluating the error. The cfKG method samples points and fidelities to maximize an acquisition function based on the knowledge gradient concept, which values the information gained from additional samples by conditioning on specific outcomes. The cfKG algorithm selects the point and fidelity that jointly maximize the cfKG acquisition function, which considers the expected value of improvement due to sampling. It values the information gained from additional samples by conditioning on specific outcomes. The cfKG algorithm values joint evaluation of points and fidelities to maximize the expected improvement due to sampling. It allows for batch settings and efficient computational methods for optimization. In the next section, efficient computational methods for solving equations (3.3) and (3.5) are discussed. A method based on envelope-theorem for single-fidelity optimization is generalized, providing unbiased estimators for q-cfKG and its gradient. Stochastic gradient ascent is used to optimize the q-cfKG acquisition function, with support for multiple starts. The computation involves expressing results from chosen points and fidelities, utilizing normal distribution properties for posterior observations. To provide an unbiased Monte Carlo estimator for optimizing the q-cfKG acquisition function, we sample a standard q-dimensional normal random vector and calculate the minimum value. A second-order continuous optimization method is used to compute the gradient and Hessian of the function. The q-cfKG acquisition function is solved using a computational method based on the envelope theorem, providing an unbiased estimator of the gradient. The q-cfKG acquisition function is computed using a standard q-dimensional normal random vector and Cholesky factorization. The gradient can be calculated through infinitesimal perturbation analysis, allowing for unbiased estimation in hyperparameter tuning applications. The derivative-enabled cfKG algorithm utilizes unbiased gradient estimators within stochastic gradient ascent to solve optimization problems efficiently. It is developed for use in continuous-fidelity settings, reducing the number of function evaluations needed to find high-quality solutions. Additionally, a GP prior on g(x, s) implies a multi-output GP prior on (g, \u2207 x g). The derivative-enabled cfKG algorithm uses unbiased gradient estimators in stochastic gradient ascent for efficient optimization in continuous-fidelity settings. It leverages a multi-output GP prior on (g, \u2207 x g) and compares different versions on various functions and neural network tuning tasks. The study involves tuning convolutional neural networks on CIFAR-10 and SVHN datasets, large-scale kernel learning, and benchmarking against various Bayesian optimization algorithms. Synthetic test functions are modified to include fidelity control, mimicking the cost of training neural networks with more data and longer iterations. The study involves tuning convolutional neural networks on CIFAR-10 and SVHN datasets using sequential and batch cfKG to achieve lower simple regret than competitors. Standard data augmentation techniques are applied, and the CNN architecture includes 3 convolutional blocks and a softmax classification layer. The study involves tuning convolutional neural networks on CIFAR-10 and SVHN datasets using sequential and batch cfKG to minimize classification error. Hyperparameters optimized include learning rate, batch size, and number of filters. cfKG outperforms KG and Hyperband by exploiting cheap approximations, achieving a test data classification error of around 12% after training for 200 epochs. On the full training dataset for 200 epochs, test data classification error is \u223c 12% for CIFAR-10 and \u223c 5% for SVHN. Using derivative-enabled cfKG (cf-d-KG) in a large-scale kernel learning example, 3 hyperparameters of a GP with an RBF kernel are optimized on 1 million training points to maximize the log marginal likelihood. cf-d-KG successfully utilizes function and gradient evaluations to find a good solution. The novel continuous-fidelity BO algorithm, cfKG, efficiently finds good solutions to global optimization problems with less cost than existing algorithms, particularly in deep learning and kernel learning applications. Synthetic test functions are defined in detail for numerical experiments in Sect. 4.1, including augmented-Branin(x, s)."
}