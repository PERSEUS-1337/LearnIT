{
    "title": "BJxhijAcY7",
    "content": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines, with signSGD being a communication-efficient learning algorithm that uses 32x less communication per iteration than full-precision, distributed SGD. This algorithm transmits only the sign of gradient vectors to a server and makes update decisions based on a majority vote, ensuring convergence in both large and mini-batch settings. Unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The distributed training system in Pytorch led to a 25% reduction in training time for resnet50 on Imagenet using 15 AWS p3.2xlarge machines. Distributed algorithms for large-scale systems involve computation and communication among worker nodes. When devising new machine learning algorithms for distribution over networks, desiderata include fast convergence, good generalization performance, and robustness to network faults. Stochastic gradient descent (SGD) satisfies convergence and performance but can suffer communication overheads in large neural network models. A proposed algorithm involves workers sending the sign of their gradient to the parameter server. The algorithm SIGNSGD with majority vote involves workers sending gradient signs to the parameter server, which aggregates and sends back the majority decision. Communication is compressed to one bit, ensuring fast convergence (D3). Sign-based methods like SIGNSGD are known to perform well, inspiring popular optimizers like RMSPROP and ADAM (D1). Aggregating gradients by majority vote ensures robustness by preventing any worker from having too much power (D4). This work provides convergence guarantees for SIGNSGD in the mini-batch setting, showing how its behavior changes with signal-to-noise ratio. The theory of majority vote in SIGNSGD achieves Byzantine fault tolerance, with robust convergence even when up to 50% of workers behave adversarially. Empirical validation using Pytorch shows experimental evidence supporting the theory, speeding up Imagenet training compared to NCCL. Theoretical tools developed in BID13 show that the popular ADAM optimizer does not converge in the mini-batch setting. SIGNSGD, a special case of ADAM, has its convergence rate established for various objectives. These tools are expected to help understand the success modes of ADAM due to the unimodal and symmetric gradient noise distributions in practical problems. Neural network research has focused on training feedforward networks using gradient descent. Algorithms like RMSPROP and ADAM are popular optimizers in machine learning. Theoretical tools show that ADAM does not converge in the mini-batch setting, while SIGNSGD has established convergence rates for various objectives. The construction of BID13 relies on bimodal noise distributions, which is different from the usual unimodal and symmetric distributions. The proposed algorithm for distributed optimization, SIGNUM, utilizes neural net optimizers like SIGNSGD with default settings of \u03b7 = 0.0001 and \u03b2 = 0.9. Operations on vectors are element-wise, and tuning is recommended for optimal performance. SIGNSGD with component-wise adaptive learning rate is similar to RPROP optimiser BID14 for non-convex optimization. A sophisticated optimisation literature has developed alongside advances in deep learning practice. BID10 proposed cubic regularisation to escape saddle points and guarantee convergence to local minima of non-convex functions. Recent works like NATASHA BID2 also aim to escape saddle points using theoretical tricks. The relevance of these works to deep learning is uncertain, as the extent of saddle points as obstacles in practical problems is unclear. Prior work on gradient compression falls into two camps, with algorithms like QSGD, TERNGRAD, and ATOMO using stochastic quantisation schemes to compress gradients while maintaining unbiased approximations to the true gradient. The compressed stochastic gradient is an unbiased approximation to the true gradient, allowing for existing SGD convergence theory to be bootstrapped. Two camps exist in gradient compression: one focusing on theoretical guarantees and the other on practical performance. SIGNSGD with majority vote takes a different approach by directly using the sign of the stochastic gradient, despite its biased nature. BID8 and BID4 show that signed gradient schemes can converge well in large batch settings. Theoretical results on gradient compression may be less relevant to deep learning practice. BID4 showed promising results in the mini-batch setting with majority vote leading to compression in communication. Recent interest in making SGD Byzantine fault tolerant includes proposals like KRUM and BYZANTINESGD, which incur overheads. Theoretical results on gradient compression may be less relevant to deep learning practice. Majority vote can lead to compression in communication, but powerful adversaries may steer convergence to bad local minimisers. Gradient distributions for resnet18 on Cifar-10 show unimodal and symmetric behavior. The optimization theory for deep learning problems does not assume convexity due to the non-convex nature of neural network loss functions. Our objective function is non-convex, but we require a lower bound for meaningful convergence. Assumptions on Lipschitz smoothness and bounded variance are standard in stochastic optimization. We present them in a component-wise form to encode information about noise level and smoothness distribution. The stochastic gradient oracle provides independent, unbiased estimates with bounded coordinates. The stochastic gradient oracle provides independent, unbiased estimates with bounded coordinates. The gradient noise is assumed to be unimodal and symmetric, with distributions close to Gaussian due to the central limit theorem. This assumption is crucial for guaranteeing mini-batch convergence of SIGNSGD. Lemma 1 from 1823 is crucial for ensuring mini-batch convergence of SIGNSGD and also serves as a convergence proof for a parameter regime of ADAM. This implies that Assumption 4 could potentially fix the non-convergence proof of mini-batch ADAM without altering the algorithm itself. The theoretical results, proven in Appendix C, establish the mini-batch convergence behavior of SIGNSGD under certain assumptions. Theorem 1 outlines the non-convex convergence rate of mini-batch SIGNSGD algorithm for K iterations. The text discusses the gradient components with large signal-to-noise ratio and the critical SNR. It mentions the mixed norm of the gradient, dimension dependence, and assumptions for well-conditioned settings. The bound is simplified when all gradient components are in the low SNR regime. The text also refers to training a resnet18 model and the total variance bound. The text discusses the convergence analysis of SIGNSGD in relation to the probability of incorrect sign stochastic gradient vectors. It provides a bound on this probability under certain assumptions, such as unimodal symmetric gradient noise. The analysis considers the signal-to-noise ratio of the gradient components and the variance bound. The bound characterizes how the failure probability of a sign bit depends on the signal-to-noise ratio of the gradient component. Even at extremely low SNR, the sign stochastic gradient still provides useful information about the true gradient direction, ensuring convergence. Without certain assumptions, the mini-batch algorithm may not converge, as illustrated by an example with bimodal noise. The true gradient is positive, but the sign gradient is negative with high probability, causing SIGNSGD to move in the wrong direction. SIGNSGD is related to the ADAM algorithm, and Assumption 4 is crucial for mini-batch convergence guarantees. Without this assumption, SIGNSGD can still converge by using a large batch size that grows with iterations to stay in the high SNR regime. SIGNSGD stays in the high SNR regime with low failure probability for the sign bit. The robustness of SIGNSGD is studied when distributed by majority vote against blind multiplicative adversaries who can manipulate their stochastic gradient estimates. Rescaling the gradient can corrupt the entire model, showing that SGD is not robust to such attacks. Our algorithm is robust to adversaries who can corrupt the model by setting the gradient to infinity. The convergence rate of majority vote with adversarial workers is derived in terms of sample complexity N. Majority vote converges at a rate where honest gradients outnumber adversarial ones. The convergence rate of majority vote with adversarial workers is affected by the proportion of adversaries \u03b1. A large batch size is advantageous for variance reduction and faster convergence theoretically, but may pose practical challenges for workers. The result can potentially be extended to the mini-batch setting. For experiments, SIGNUM (Algorithm 1) was distributed by majority vote, serving as the momentum counterpart of SIGNSGD. Momentum was added to SIGNSGD in BID3 BID4. The Pytorch deep learning framework with Gloo communication library was used, along with custom compression code for 1-bit tensors. Performance was improved by fusing smaller tensors, training resnet50 on Imagenet across 7 to 15 AWS p3.2xlarge machines. Training resnet50 on Imagenet distributed over 7 to 15 AWS p3.2xlarge machines shows that majority vote training is 25% faster in terms of wall-clock time compared to distributed SGD. However, there is a slight degradation in generalisation accuracy with majority vote. Training QRNN on WikiText-103 with ADAM and SIGNUM distributed across machines reveals that SIGNUM completes an epoch three times faster than ADAM, achieving similar perplexity after 2 hours of training. Increasing the per-worker batch size improves SIGNUM's performance. Increasing the per-worker batch size beyond 240 may further improve SIGNUM's performance. Dropout was applied during training but not testing, leading to test perplexity beating training perplexity. Experiments with resnet18 on Cifar-10 and resnet50 on Imagenet show the impact of adversarial attacks on training stability. Learning instability was addressed by reducing the learning rate, resulting in stabilization. Resnet18 was trained on Cifar-10 with 7 workers at batch size 64. Various strategies were employed for handling negative and random adversaries during training. MULTI-KRUM and majority vote methods were compared, with MULTI-KRUM failing catastrophically when the number of adversaries exceeded the security level. Testing against SGD distributed using NCCL communication library showed that the framework was 4\u00d7 faster in communication than NCCL. The study compared majority vote to NCCL for distributed learning on the Imagenet dataset using AWS p3.2xlarge machines with Nvidia Tesla V100 GPUs. Majority vote showed potential for faster processing than NCCL, achieving a 25% faster training job completion. However, it exhibited slightly lower test set accuracy, suggesting the need for improved regularization or momentum tuning. In Figure 6, majority vote is compared to ADAM for training QRNN on WikiText-103, completing an epoch 3 times faster but with degraded accuracy. Figure 7 shows superior convergence of majority vote compared to QSGD. Robustness of SIGNUM with majority vote to Byzantine faults is tested on Imagenet dataset. In FIG8, experiments were conducted with hyperparameters tuned for the 0% adversarial case, showing learning tolerance up to 43% adversarial behavior. FIG9 compares majority vote to MULTI-KRUM with a security level of f = 2, highlighting the failure of MULTI-KRUM when the number of adversaries exceeds f. SIGNSGD is shown to fail more gracefully in such scenarios. Theoretical and empirical properties of a simple algorithm for distributed, stochastic optimization, SIGNSGD with majority vote aggregation, are analyzed. The study analyzed the robustness and efficiency of SIGNSGD with majority vote aggregation for training large-scale convolutional neural nets. It was found that increasing the mini-batch size can improve performance by making the gradient noise more Gaussian. Future work includes optimizing the implementation of majority vote by distributing the parameter server across machines to prevent communication bottlenecks. In experiments, the framework speeds up Imagenet training but still has a test set gap. Future work could focus on devising new regularization schemes for signed updates. Additionally, exploring the link between SIGNSGD and model compression shows promise. Theoretical calculations compare communication costs of SIGNSGD with majority vote aggregation to two forms of QSGD BID0. The communication cost of SIGNSGD with majority vote is 2M d bits per iteration, while there are two variants of QSGD given in BID0. In experiments, two variants of QSGD are compared in BID0. The first is L2 QSGD developed in theory, and the second is max QSGD used in experiments. The highest compression version, 1-bit QSGD, involves ternary quantization. 1-bit L2 QSGD snaps gradient components to {0, \u00b11} with a certain probability. The expected number of bits set to \u00b11 is bounded. Compressing a vector with 1-bit L2 QSGD requires at most log d bits per non-zero component. In experiments, two variants of QSGD are compared in BID0. The '2-way' version of 1-bit L2 QSGD converges poorly, so it is recommended to use the 1-way version where the aggregated compressed gradient is not recompressed. The final algorithm discussed is 1-bit max QSGD, which snaps gradient components to {0, \u00b11} with a certain probability. It requires no more than O(d) bits to compress a d-dimensional vector. 1-bit max QSGD compresses d-dimensional vectors to O(d) bits by setting non-zero entries to 1 and keeping the rest zero. This method is efficient for sparse vectors but may not be suitable for deep learning problems. In experiments, 1-bit max QSGD compresses vectors 5\u00d7 more than SIGNSGD, but the time cost of backpropagation limits its effectiveness. Lemma 1 discusses stochastic gradient approximation with bounded variance and symmetric noise distribution. The signal-to-noise ratio is always less than or equal to 1/2. The failure probability for the sign bit is analyzed under the assumption of negative g i. The algorithm for mini-batch SIGNSGD convergence rate is run for K iterations with specified learning rate and mini-batch size. The critical signal-to-noise ratio is defined as 'critical SNR', and the improvement of the objective during a single step is bounded. The algorithmic steps and improvements in stochasticity-induced error are analyzed under specific assumptions and bounds. The convergence rate of mini-batch SIGNSGD is examined over K iterations with defined parameters. The analysis includes the critical signal-to-noise ratio and the bounded improvement of the objective during each step. The convergence rate of mini-batch SIGNSGD is analyzed over K iterations with specific parameters. A fraction \u03b1 < 1/2 of workers behave adversarially, leading to a convergence rate based on the total number of stochastic gradient calls per worker. The failure probability of the vote is bounded, with a focus on the adversary manipulating their stochastic gradient estimate. The failure probability of the vote in mini-batch SIGNSGD is analyzed over K iterations with a fraction \u03b1 < 1/2 of adversarial workers. The adversary manipulates their stochastic gradient estimate by deciding to invert sign bits, affecting the convergence rate based on the total number of stochastic gradient calls per worker. The adversary in mini-batch SIGNSGD manipulates their stochastic gradient estimate by inverting sign bits, affecting convergence rate. The indicator function is analyzed as a Bernoulli random variable with success probabilities based on the sign of the gradient."
}