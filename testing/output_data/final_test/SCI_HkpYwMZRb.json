{
    "title": "HkpYwMZRb",
    "content": "Exploding gradients can still occur in popular MLP architectures despite techniques like Adam, batch normalization, and SeLU nonlinearities being used to address the issue. This limitation affects the effective training depth of networks, both theoretically and practically. The \"collapsing domain problem\" is highlighted as a potential issue in architectures that aim to avoid exploding gradients. ResNets address the exploding gradient problem by introducing skip connections, simplifying the network mathematically. The success of neural networks is attributed to their depth and ability to compose nonlinear functions. Previous research has emphasized the benefits of depth in training deep feedforward neural networks. The exploding gradient problem in training deep neural networks has been a major challenge since the introduction of gradient-based learning. The phenomenon occurs as the gradient backpropagates through the network, potentially growing exponentially from layer to layer. This can hinder the effectiveness of optimization algorithms like SGD. Despite the intuitive understanding of this issue, there are gaps in the foundational understanding. One key gap is the lack of a well-accepted metric for identifying exploding gradients. The exploding gradient problem in deep neural network training is a significant challenge due to the exponential growth of gradients as they backpropagate through the network. Different strategies, such as manipulating layer width, can impact gradient vector components but may not necessarily address the issue. While evidence suggests that exploding gradients can lead to poor results in certain scenarios, it remains unclear if this holds true for all algorithms and architectures. Algorithms like RMSprop, Adam, and vSGD are variations of SGD that rescale parts of the gradient vector to improve training outcomes. The question remains whether exploding gradients are a numerical quirk or a reflection of a challenging optimization problem that cannot be easily solved with simple modifications. Techniques such as normalization layers and careful weight scaling have been effective in stabilizing forward propagation and eliminating exploding gradients. Batch normalization and ResNet have been effective in stabilizing forward activations and addressing the issue of exploding gradients in deep networks. These techniques have helped in reducing gradient growth and improving convergence during training. While batch normalization and ResNet have been successful in stabilizing forward activations and mitigating exploding gradients, it is important to note that these techniques may not always be effective and can lead to other issues. Adding nuance to widely adopted ideas, such as the misconception that stabilizing forward activations alone can prevent gradient explosions, is crucial. Neural network architectures like ResNet, with skip connections, have shown continued performance improvement with increased depth compared to networks without skip connections. The introduction of skip connections has shown favorable changes to gradient properties in specific architectures. However, a general explanation for the power of skip connections has not been provided. The authors introduce the 'gradient scale coefficient' (GSC) to assess exploding gradients, demonstrating their presence in popular MLP architectures, even with techniques like normalization layers. This challenges the misconception that stabilizing forward activations alone can prevent gradient explosions. Introducing normalization layers can worsen the exploding gradient issue, limiting the effectiveness of training very deep MLP architectures. Exploding gradients are not just a numerical problem but indicate a complex optimization challenge, restricting the depth of trainable networks. The occurrence of exploding gradients in deep networks, even with stable forward activations, poses a fundamental obstacle to constructing deep trainable networks. The concept of the 'collapsing domain problem' is introduced for training deep feedforward networks. Introducing normalization layers can exacerbate the exploding gradient issue in training deep MLP architectures. Skip connections have a strong gradient-reducing effect on deep networks, leading to superior performance at great depths. ResNets simplify networks without skip connections and achieve an 'orthogonal initial state', providing a key criterion for neural network design. Recommendations for designing and training deep networks are derived in the conclusion. In the conclusion, practical recommendations for designing and training deep networks are derived, along with key implications for deep learning research. The appendix discusses related work on exploding gradients, network trainability measures, and the behavior of neural networks at great depth using mean field theory and dynamical systems theory. The implications of the work for the vanishing gradient problem are also discussed. In section B.2, the paper discusses the implications of the work for the vanishing gradient problem. It also compares the exploding gradient problem in feedforward networks to the issues in RNNs in section B.3. Additionally, open research questions and potential future work are highlighted in section B.4. The paper defines a neural network as a succession of layers and aims to minimize the error between the network's prediction and the true label. The network's layers are denoted from f L as the lowest to f 0 as the highest, focusing on gradient flow direction. Each layer has a dimensionality d l, with d 0 = 1, and the input x has dimensionality d. Layers have parameter sub-vectors \u03b8 l, with \u03b8 representing trainable elements. Layers can be 'unparametrized' or 'parametrized' based on trainable weights. A network with layers f 0 to f L has 'nominal depth' L, while 'compositional depth' is the number of parametrized layers. The 'compositional depth' of a network is the number of parametrized layers, not just the total number of layers. The 'quadratic expectation' and 'inverse quadratic expectation' of a random variable are defined in terms of the quadratic mean. However, this definition is insufficient as networks can be constructed with exponentially growing Jacobians by scaling each layer and parameter sub-vector. During training, correcting for changes in network parameters involves scaling gradient sub-vectors by a constant R > 1. A proposition states that neural networks with exponentially growing Jacobians can be trained to the same error level and predictions as networks without this property. This distinction is crucial in understanding the intrinsic difficulty of training neural networks. In the main body of the paper, propositions and theorems are informally stated for readability. In the appendix, they are rigorously restated with proofs and practical conditions discussed. The definition of 'exploding gradients' is outlined, showing the hardness of training without confounding effects. The 'quadratic mean norm' of a matrix A is defined as the quadratic mean of its singular values, measuring the expected impact on a vector with random orientation. The 'gradient scale coefficient (GSC)' is defined for network f(\u03b8) to determine exploding gradients. It relates to the norm of the Jacobian and the lengths of vectors and matrices. Exploding gradients occur when GSC can be approximated by an exponential function. The GSC measures the size of the gradient flowing backward relative to the size of the activations flowing forward. It also measures the sensitivity of layers to small random changes in parameters. The study focuses on the size of the gradient relative to activations in multi-layer perceptrons, specifically looking at exploding gradients during training and not considering generalization issues. In section 2.2, trainable networks can have exponentially growing Jacobians through simple multiplicative rescaling. The Gradient Scaling Coefficient (GSC) remains invariant to this rescaling, canceling out its effects. Exploding gradients are found in various popular MLP architectures, with the GSC showing exponential growth in certain scenarios. The curr_chunk discusses different activation functions and normalization techniques used in neural networks with compositional depth 50 and 100 neurons per layer. Weight matrix initialization methods for ReLU, tanh, and SeLU architectures are also mentioned. In neural networks with compositional depth, different activation functions and normalization techniques are discussed. The GSC(l, 0) grows linearly in log-space in certain architectures, leading to gradient explosion. Techniques like careful weight scaling and normalization layers may not effectively combat exploding gradients in these architectures. Normalization layers and activation functions like SeLU are used to scale activations and control the size of forward activations in neural networks. While these techniques can impact the network's mathematical properties, they do not necessarily reduce the exploding gradients. Other architectures like ReLU, layer-ReLU, and tanh do not exhibit exploding gradients but have their own drawbacks. All curves in the graph exhibit small jitters due to the plotting of GSC values at different layers. In neural networks, different types of layers affect the Gradient Scaling Coefficient (GSC) differently. Linear layers tend to shrink gradients, while normalization layers increase them. Despite these differences, individual layers have a small impact on GSC, with a relative invariance to the effects of each layer. For example, passing through a ReLU layer reduces both activation and gradient vectors by approximately \u221a2. The Gradient Scaling Coefficient (GSC) is relatively invariant to individual layers in neural networks, suggesting a deep property of the network. It is also robust to changes in width and depth, with the rate of gradient explosion decreasing slightly when width increases. Networks with oscillating widths still exhibit exponential growth in GSC. Results are summarized in table 1. The concept of 'effective depth' for ResNet architecture is introduced, where each layer is defined as the sum of an initial function and a residual function. The optimization problem for a residual net is defined, assuming identical layer dimensions. The initial function is typically chosen as the identity function, leading to the sum of terms involving identity matrices and residual Jacobians. If the operator norm of the residual Jacobians is less than a certain value, the norm of terms decreases exponentially. The concept of 'effective depth' in ResNet architecture is introduced, where layers co-adapt based on the sum of terms involving identity matrices and residual Jacobians. The size of terms containing many or all residual Jacobians decreases exponentially, leading to the determination of the 'effective depth' of the network. Veit et al. (2016) argue that only sets of layers with a non-negligible sum of terms co-adapt during training. The residual network behaves as an ensemble of shallow networks, supported by the success of stochastic depth training. Any neural network can be expressed as a residual network by choosing initial functions and defining residual functions. Training starts with all residual functions as zero functions. The 'residual trick' allows analysis devised for ResNet to be applied to arbitrary networks. The distinction between 'ResNet' and 'residual network' is made based on skip connections. An exploding gradient is caused by the GSC. The exploding gradient, caused by the GSC, limits the effective depth of deep MLPs due to the exponential training time. Gradient-based updates must be small to stay within the region of accurate linear approximation. Applying a random update to \u03b8 l changes the output f 0 with quadratic expectation f 0. The exploding gradient, caused by the GSC, limits the effective depth of deep MLPs due to the exponential training time. Gradient-based updates impact the function value more than random updates, leading to the error becoming negative with significant probability. If GSC(0,l) decreases exponentially in l, the relative size of updates must also decrease exponentially. This results in exponentially many updates required for a residual function to reach a certain size relative to the initial function. The exploding gradient caused by the GSC limits the effective depth of deep MLPs due to exponential training time. Theorem 1 states that training an MLP with exploding gradients to have effective depth \u03bb takes at least a certain number of updates, independent of the nominal depth. Corollary 1 suggests that if the number of updates to convergence is bounded, so is effective depth. The effective depth of deep MLPs is limited by exploding gradients, making it unreachable with certain algorithms. To validate this theory, four architectures were trained on CIFAR10 with a compositional depth of 51 and 100 neurons per layer. The best step size for SGD was determined for each linear layer by pre-training the highest layers with a small uniform step size. The best step size for each linear layer in deep MLPs was determined based on achieving the lowest training classification error. Step sizes inducing relative update sizes of 0.1 or less were considered to prevent weight instability. The algorithm for step size selection and justification is provided in section I.4. Lower layers require relatively smaller updates, especially if the gradient explodes rapidly. The trend of smaller updates for lower layers was observed, with 1 GSC(l,0) serving as an upper bound for useful updates. In deep MLPs, the best step size for each linear layer was determined to minimize training classification error. Lower layers need smaller updates to prevent weight instability, especially if the gradient explodes rapidly. The goal is to study pathological architectures rather than find optimal ones, so high errors are expected. GSC values across the network fluctuate during training, with architectures showing less gradient explosion achieving lower errors. The GSC values fluctuate during training in deep MLPs, with the gradient initially falling below 1 before resuming explosion. Nonstandard training procedures can significantly impact the GSC. Effective depth of each network is measured by estimating cumulative updates from \u03bb-residual terms. The effective depth of architectures in deep MLPs is measured by estimating cumulative updates from \u03bb-residual terms. Architectures with smaller updates tend to have a lower effective depth, with high-order co-adaption of layers occurring at the beginning of training. SeLU and tanh-batch architectures reach an effective depth close to their compositional depth. The operator norm of residual weight matrices after training is shown in FIG5. After training, the operator norm of residual weight matrices decreases for all architectures except SeLU, indicating a lack of high-order co-adaptation. To verify this, Taylor expansions were used to replace bottom layers in fully-trained networks, showing a limited effective depth. After training, Taylor expansion is used to reduce the compositional depth of the network, showing that layer-tanh and batch-ReLU networks can be greatly reduced without a significant increase in error. The resulting layer-tanh network outperforms the original batch-tanh and batch-ReLU networks, indicating a lack of high-order co-adaptations. Removing high-order co-adaptations among layers and groups of 3 or more layers leads to lower error, although the exact decrease cannot be computed. The study established a link between exploding gradients and training difficulties in MLPs. Gradient rescaling was unable to overcome these issues, even with techniques to stabilize forward activations. The gradient scale coefficient was identified as the best metric for studying exploding gradients. The results showed that using a single step size for each layer led to higher errors in the exploding architectures. The study identified the gradient scale coefficient (GSC) as the best metric for studying exploding gradients in MLPs. Using minibatches of size 1000 for training led to high error rates in architectures like batch-ReLU due to the interplay between exploding gradients and noise in batch normalization. The noise induced by batch normalization was found to have a relative size of approximately 1/sqrt(b), while the change in the error layer was approximately GSC * sqrt(b) according to proposition 2. The study highlighted the impact of gradient scale coefficient (GSC) on exploding gradients in MLPs, particularly in architectures like batch-ReLU. The introduction of batch normalization layers can lead to untrainable models due to the interplay between exploding gradients and noise. Techniques like using running averages can help reduce the dependence of batch normalization on the current minibatch. Other factors that can induce noise and cause issues with large gradients include dropout, stochastic nonlinearities, and network quantization. Exploding gradients occur when GSC values are not balanced, leading to a rate of explosion greater than 1. Theorem 2 states that under certain conditions, neural networks with random parameters can experience exploding gradients in expectation with a rate greater than 1. This occurs when the absolute singular values of the Jacobian of each layer are independent and identically distributed (IID) and differ by at least a certain probability threshold. Surjective endomorphisms on the hypersphere play a key role in this phenomenon. The absolute determinant of the product of absolute singular values in neural networks with random parameters is at least 1. Exploding gradients can occur even when forward activations are stable due to the need for unit absolute determinants in Jacobians, leading to exponential compounding of qm norm values. The qm norm values should be greater than 1 to avoid exploding gradients in neural networks. A sequence of layers including tanh/SeLU, linear, and length-only layer normalization can be a surjective endomorphism on the hypersphere. Both LOlayer-tanh and LOlayer-SeLU exhibit exploding gradients. Any endomorphism on the hypersphere with a specific structure is bijective. Theorem 2 outlines strategies to prevent exploding gradients in neural networks: using non-surjective layer functions and ensuring Jacobians approach orthogonal matrices. ReLU and ResNet employ these strategies. Surjective endomorphisms can lead to exploding gradients, suggesting that reducing the domain of forward activations can help mitigate this issue. Shrinking the co-domain of a layer function reduces eigenvalues of the Jacobian and the GSC, while keeping output length constant. In neural network design, strategies to prevent exploding gradients include using non-surjective layer functions and ensuring Jacobians approach orthogonal matrices. Shrinking the co-domain of a layer function reduces eigenvalues of the Jacobian and the GSC, while keeping output length constant. The standard deviation of activation values at each neuron across datapoints in different architectures shows a layer-to-layer shrinkage of the domain. The 'collapsing domain problem' arises when layers in a neural network become more similar with depth, leading to pseudo-linearity. This occurs when pre-activations fed into a nonlinearity are highly similar, causing the nonlinearity to be approximated by a linear function. In the case of a tanh architecture, if activation values become small, the tanh nonlinearity can be approximated by the identity function, resulting in the network becoming 'pseudo-linear' and unable to model nonlinear functions effectively. Based on the decrease in pre-activation standard deviation, a tanh network with compositional depth 50 has the capacity of a network with compositional depth 10. Similarly, for a ReLU nonlinearity, if pre-activations are mostly positive or negative, the nonlinearity can be approximated by a linear function. In the case of ReLU, if pre-activations are mostly positive, it can be approximated by the identity function, and if mostly negative, by the zero function. The concept of 'sign diversity' is explored in neural networks, showing a decrease in diversity as layers progress. The collapsing domain problem affects gradient updates by shrinking the useful update proportionally to the reduction in domain size. The collapsing domain problem reduces the largest useful update size of each layer, similar to exploding gradients. Training with layer-wise step sizes did not improve error values on CIFAR10. The difference between the 1 GSC bound and optimal update sizes is larger for collapsing domains compared to exploding architectures. In neural network design, there is a tension between avoiding exploding gradients and preserving forward activations. This tension arises from the discrepancy in layer-wise Jacobians, leading to hampered training. Collapsing domains reduce optimal update size, similar to exploding gradients, affecting network linearity during training. The discrepancy in layer-wise Jacobians is a foundational reason for the difficulty in constructing very deep trainable networks. Metrics like pre-activation standard deviation and sign diversity are discussed, along with the role of activation correlation in mean field theory. ResNet and architectures with skip connections have been successful. Collapsing domains are further discussed in future work. Skip connections in architectures like ResNet have been successful due to their ability to reduce the exploding gradient problem, allowing for training to greater depths. The concept of k-dilution shows that functions in these architectures become more linear as the value of k increases. This leads to a significant reduction in gradients, enabling ResNet to be trained to very deep depths successfully. In experiments with 5 ResNet architectures, gradient growth is lower compared to vanilla networks, mainly in lower layers. Domain collapse for layer-ReLU is slowed, and gradient reduction aligns with theorem 3. Dilution level and growth rate at each residual block are measured, showing reduced gradients in ResNet. The post-processing of the exploding architectures in the ResNet experiments confirms accurate gradient reduction estimates. ResNet enables higher update sizes, lower error, and higher effective depth compared to vanilla networks, except for layer-SeLU ResNet. The severe reduction of the gradient scaling coefficient persists throughout training. The severe reduction of the gradient scaling coefficient persists throughout training in ResNet, with higher effective depth values compared to vanilla networks. Gradient reduction is achieved not just by identity skip connections but also by skip connections that multiply the incoming value with a Gaussian random matrix. Results for these skip connections can be found in table 1. Deep ResNets behave like ensembles of shallow networks, while comparable vanilla networks behave like ensembles of even shallower networks. Deep ResNets are robust to lesioning, and vanilla networks are even more robust to depth reduction. K-dilution can lead to pseudo-linearity, damaging representational capacity. Dilution slowly disappears as functions are composed. If linear functions are used for dilution, it corresponds to feature refinement. The composition of B random functions that are k b -diluted in expectation is limited in reducing gradient signal cancellation without causing pseudo-linearity. Skip connections in general neural network architectures have a GSC-reducing effect. Users of ResNet can control the level of GSC reduction by adjusting the amount of dilution as they go deeper in the network. The ResNet architecture allows for control over the level of gradient signal cancellation reduction by adjusting the amount of dilution as the network goes deeper. This can be achieved by scaling skip and block functions with constants or inserting normalization layers between blocks/skip connections to maintain a constant dilution level. ResNets demonstrate reduced gradient via k-dilution, and the residual trick can be applied to generalize this concept to arbitrary networks. The ResNet architecture allows for gradient signal reduction by adjusting dilution levels as the network deepens. BID7 introduced 'looks-linear initialization' (LLI) for ReLU networks, outperforming ResNet. LLI reduces gradient growth in batch-ReLU throughout training, achieving lower training error on CIFAR10 than ResNet. DiracNet and LLI achieve orthogonal initial states, reducing gradient growth compared to Gaussian initialization. Non-orthogonal initial functions in deep networks lead to disadvantages like collapsing domains and exploding outputs. The purpose of not training a network from an orthogonal initial state is unclear. Networks with orthogonal initial functions are simpler mathematically and should be the default choice. In this paper, it is argued that networks with mathematically simpler functions should be the default choice. The study demonstrates that many MLP architectures exhibit issues like exploding gradients and collapsing domains, caused by discrepancies in layer-wise Jacobians. These pathologies are largely due to non-orthogonality in the initial functions, which can be made more orthogonal to mitigate these issues. Training from an orthogonal initial state can reduce issues like exploding gradients and collapsing domains in neural networks. It is beneficial to have initial functions that are more orthogonal, achieved through methods like skip connections or using architectures like ResNet. Avoiding low effective depth in networks can also help when not starting from an orthogonal initial state. To prevent issues like exploding gradients and collapsing domains in neural networks, it is important to have orthogonal initial functions. Nonlinear layers should be separated by nonlinearities to avoid pseudo-linearity. Techniques like normalization layers and careful weight initialization can help prevent activation pathologies. Skip connections are generally beneficial, while diluting nonlinear functions with linear functions can also be helpful. Adam, RMSprop, or vSGD can improve network performance. Optimization algorithms like Adam, RMSprop, and vSGD can improve network performance by addressing issues such as exploding gradients and collapsing domains. Adjusting the step size as the Gradient Scaling Coefficient (GSC) grows can benefit lower layers. Controlling the dilution level with skip connections, normalization layers, and scaling constants can trade off gradient growth and representational capacity. Theorems and propositions can be used for static estimates of gradient reduction. Deep neural networks with over 1000 layers have been trained, but excessive dilution is needed to prevent gradient explosion. It may be more beneficial to focus on network width rather than extreme depth to optimize learning outcomes. Exploding gradients indicate a challenging optimization problem that cannot be easily resolved by rescaling. The difficulty of optimizing deep neural networks is highlighted by the challenge of exploding gradients, which cannot be solved by simple modifications to algorithms. Using GSC as a benchmark, a link between exploding gradients and training difficulty was established. The residual trick in neural networks allows for the application of ResNet-specific tools, and the impact of different step sizes on training success was found to be significant. Table 1 presents key metrics for architectures in their randomly initialized state evaluated on Gaussian noise, detailing different normalization, matrix type, and skip connection types used in the study. In Table 2, training classification error for architectures on CIFAR10 is shown based on different normalization, matrix type, and skip connection types. 'Gaussian' matrices have entries drawn from a Gaussian distribution, 'looks-linear' refers to a specific initialization scheme, and 'identity' and 'none' are types of skip connections. In Table 2, training classification error for architectures on CIFAR10 is shown based on different normalization, matrix type, and skip connection types. 'Gaussian' matrices have entries drawn from a Gaussian distribution, 'looks-linear' refers to a specific initialization scheme, and 'identity' and 'none' are types of skip connections. The curr_chunk discusses gradient shattering in deep networks and how gradients become uncorrelated with depth, leading to exploding gradients. Further methodological details can be found in section I, and a detailed breakdown of results in figures 2, 4, 5, and 6. The curr_chunk discusses the exploding gradient problem and collapsing domain problem as a further specification of the shattering gradient problem. It extends previous work by showing that exploding gradients are still a challenge, especially in very deep batch-ReLU MLPs. The analysis includes a rigorous argument on why exploding gradients make training difficult and extends beyond ReLU networks. The analysis extends beyond ReLU networks and focuses on the exploding gradient problem, specifically distinguishing between two important cases. Saxe et al. (2014) also investigated the divergence of singular values in multi-layer Jacobians, leading to the direction of the gradient being determined by dominant eigenvectors. The multi-layer Jacobian's dominant eigenvectors determine the gradient direction, slowing down training. Batch normalization combats 'covariate shift' in intermediate representations, but the concept lacks a rigorous definition. Training deep networks involves layers adapting to each other's shifts to achieve optimal function computation. Deep networks achieve high accuracy by responding to shifts in different layers. Luo (2017) introduced an architecture using iterative numerical methods and matrix decomposition to combat deterioration in Hessian conditioning with depth. Matrix decomposition techniques, like those used by BID4 and BID1, could potentially reduce the divergence of singular values in layer-wise Jacobian during training. This work is similar to research on deep networks using mean field theory, which utilizes infinitely wide networks for analysis. In previous work, researchers used infinitely wide networks to analyze forward activations and gradients in initialized states, identifying two regimes - order and chaos. In the chaotic regime, gradient vector length explodes, while in the ordered regime, it vanishes. For tanh MLPs, correlation between forward activation vectors converges to 1 in the ordered regime and less than 1 in the chaotic regime. In a tanh MLP without biases, in the chaotic regime, the correlation converges to 0 ('zero limit correlation'). Mean field theory is used for static analysis of network architectures. The GSC is argued to be a better metric for detecting exploding or vanishing gradients compared to gradient vector length. In a tanh MLP without biases, the GSC is a better metric for detecting exploding or vanishing gradients compared to gradient vector length. For a tanh MLP with no biases, vanishing is achieved for \u03c3 w < 1, stability for \u03c3 w = 1, and explosion for \u03c3 w > 1. For a ReLU MLP with no biases, vanishing is achieved for \u03c3 w < \u221a 2, stability for \u03c3 w = \u221a 2, and explosion for \u03c3 w > \u221a 2. The advantage of considering GSC can be seen in the case of the ReLU network. The behavior of correlation and chaos in ReLU and tanh MLPs with no biases and different \u03c3 w values is analyzed. In ReLU MLP, infinitesimal noise leads to chaos but correlation converges to zero sub-exponentially, indicating the edge of chaos. In tanh MLP, correlation converges to zero, gradients explode, and the domain can collapse causing pseudolinearity. The concepts of unit limit correlation and collapsing domain problem are related, with the former being a special case of the latter. The tanh layer in deep networks can lead to domain collapse, while the ReLU and tanh architectures are on the edge of chaos. Varying step sizes between layers had a significant impact on the results, as shown in table 2. This contrasts with previous findings and highlights the importance of chaotic architectures in avoiding pseudo-linearity. In section 4, a rigorous argument is presented for the harmful nature of exploding gradients and chaos in deep networks. The impact of choosing appropriate step sizes when comparing neural architectures is emphasized. The importance of chaotic architectures in avoiding pseudo-linearity is highlighted, contrasting with previous findings. In section 6, collapsing domains can harm expressivity and trainability. ResNet architectures reduce gradient explosion, with BID3 proposing downscaled weights to combat gradient growth. However, limitations exist in reducing gradient growth without pseudo-linearity issues. Changing intermediate layer widths is not effective in reducing gradient growth. The GSC is proposed as a standard for assessing exploding gradients, with ResNet architectures designed for stability at any depth. Practical strategies for building ResNets include ensuring orthogonal vectors in residual and skip functions. ResNets ensure stability at any depth by using orthogonal vectors in residual and skip functions, along with various weight initialization strategies. In studying different weight initialization strategies for ResNets, it was found that normalizing weight matrices to have a unit qm norm is effective. The four initializations were analyzed for batch-ReLU and layer-tanh, with varying results. Initialization (ii) ensures orthogonality between skip and residual functions with probability 1, while initialization (iii) shows faster gradient growth. Initialization (iv) performs similarly to (i) but requires more dilution for reducing gradient growth. Introducing correlation between paths affects gradient growth and breaks certain conditions. Our study suggests that more evaluation is needed to determine if certain architectures can increase stability. Scaling residual and skip functions with constants is a common technique in ResNets. Our experiments did not encounter vanishing gradients as defined by the GSC. Our analysis suggests that strong domain collapse is necessary to reverse the gradient growth implied by theorem 2. Exploding gradients and vanishing gradients have been studied extensively in RNNs. The problem in RNNs is similar but different from the exploding gradient problem in feedforward networks. In classical RNN architecture, signals acquired early undergo a non-orthogonal transformation at every time step. In RNNs, signals undergo non-orthogonal transformation at each time step, leading to negative consequences. LSTMs and GRUs address exploding/vanishing gradients by leaving neurons unmodified unless new information is relevant. Managing exploding gradients in feedforward networks is challenging as each layer aims to modify the signal productively. LSTM eliminates non-orthogonality completely, similar to an orthogonal initial state. In feedforward networks, non-orthogonality is eliminated only from the initial function, while LSTM aims to eliminate non-orthogonality completely from each time step. The analysis in the paper focuses on MLPs without trainable bias and variance parameters, with potential applicability to other neural network architectures. The behavior of neural networks with trainable biases, convolutional layers, or recurrent layers is fundamentally the same, but additional complexity may arise. It is challenging to measure the degree of domain collapse in a network, with various metrics available. Different architectures exhibit collapsing domains in different ways, such as collapsing onto the origin in tanh networks or onto a line in linear MLPs. In deep learning, the behavior of neural networks can lead to domain collapse onto specific structures like the origin, a line, or a ray. The goal is to model a ground truth function that maps data inputs to labels, with the network's Gradient Scale Coefficient (GSC) indicating responsiveness to input changes. Matching GSCs between the network and ground truth function is crucial for accurate representation. The Gradient Scale Coefficient (GSC) of a network should match that of the ground truth for accurate representation. If the GSC is too low, there is underfitting, and if it is too high, there is overfitting. To achieve the right gradient, the network should have gradients of different magnitudes for different data inputs, enabling it to learn structured gradients. For example, in an image of a dog in a meadow, there should be a high gradient for facial features but a low gradient for the meadow pixels. The importance of gradient scale coefficients in neural networks is highlighted, with a focus on achieving accurate representation by adjusting gradients for different data inputs. Dilution can reduce gradient growth but may harm representation capacity, necessitating a balance in residual function size to avoid underfitting or overfitting. The presence of dilution in neural networks, including ResNet and vanilla MLPs, can impact the ability to model ground truth functions accurately. Assessing the level of \"linearity\" in a network remains an open question, with factors such as gradient behavior influencing the approximation of nonlinearities. The comparison between SeLU and batch-ReLU non-linearities is complex, as the behavior of gradients varies between the two. The impact of dilution on representational power in deep neural networks is an open question. An orthogonal initial state reduces gradients via dilution, allowing for larger updates and increased growth of residual functions, leading to greater effective depth. However, as residual functions grow, dilution decreases, causing the gradient to increase and updates to shrink, slowing the growth of effective depth. Training from an orthogonal initial state can increase effective depth, but additional techniques may be needed to learn functions effectively. The text discusses the impact of dilution on representational power in deep neural networks. It mentions that training from an orthogonal initial state can increase effective depth, but additional techniques may be required to learn functions effectively beyond a certain limit. It also explains the notation and terminology used in the context of data inputs, labels, vectors, and functions in neural networks. In the context of neural networks, random vectors and matrices can be radially symmetric, Gaussian initialized, or orthogonally initialized. Notation using parentheses for vector and matrix elements is used, assuming differentiability of neural networks. In neural networks, random vectors and matrices can be radially symmetric, Gaussian initialized, or orthogonally initialized. Theoretical results apply to arbitrary networks, with a focus on MLPs. A 'gradient-based algorithm' for training mutable parameter vectors is defined as a black box that queries gradients at arbitrary points. Successive states of the parameter vector are recursively defined. In a residual network, the gradient is calculated with respect to a parameter sub-vector. Terms containing \u03bb or more Jacobians of residual functions are called '\u03bb-residual'. Two scenarios are considered: one where the gradient includes all terms, and one where \u03bb-residual terms are removed. The difference between the parameter vectors in these scenarios is defined as the '\u03bb-contribution'. The 'effective depth at time t with threshold h' is also discussed. The effective depth measure in neural networks is influenced by the shift \u03b8 and the scale of \u03b8. The exact value of the threshold h is not crucial when comparing networks by effective depth. It may be necessary to set different thresholds for layers with varying values. Computing the effective depth measure is complex due to the exponential number of gradient terms required. The estimation of effective depth in experiments is explained in this section. In this paper, the effective depth measure in neural networks is discussed, influenced by the shift \u03b8 and scale of \u03b8. The algorithm for computing effective depth assumes training with stochastic gradient descent. The estimation of effective depth involves computing the \u03bb-contribution at each layer over all time points and query points. The core assumption is the application of the residual network model. The core assumption in this section is that the Jacobian of the initial function of a layer increases the lengths of all terms equally, regardless of the number of residual Jacobians. The impact of the Jacobian of the initial function is conservatively bounded by the impact of the Jacobian of the entire layer. The length increase of a term due to a residual Jacobian is estimated using ||r k || op. The sum of the lengths of all \u03bb-residual terms in a batch is used as a conservative bound on the length of the \u03bb-contribution of the batch. The core assumption is that the Jacobian of the initial function of a layer increases the lengths of all terms equally. The sum of the lengths of the \u03bb-residual terms in each update is used as an estimate of the total \u03bb-contribution. Second-order effects are ignored, and the effective depth estimate may be larger than its actual value. Some trained networks show robustness to Taylor expansion. For ResNet architectures, the estimate of effective depth needs to be adjusted. For ResNet architectures, the estimate of effective depth is adjusted to account for skip connections. The variable arr is modified as it crosses a skip connection or residual block. The impact of skip connections and initial functions of the residual block is approximated by the effect of the entire block. The effective depth measure in ResNet architectures accounts for skip connections and residual blocks. The impact of residual functions is unaffected by skip connections and is bounded by the operator norm. Training a linear MLP with a larger effective depth still results in a depth 1 network. Re-sampling weights and applying gradient descent is equivalent to just running gradient descent. The effective depth measure is influenced by the initial step size, with larger step sizes leading to worse outcomes in experiments. The effective depth measure in neural networks can be influenced by how layers are defined, such as considering a linear transformation and ReLU operation as part of the same layer in a ReLU MLP. Proposition 1 outlines the parameters and domains involved in neural network training. The algorithm can compute updates by querying gradients of the neural network at the current parameter value and query points, leading to a final error value on a given dataset. The algorithm computes updates by querying gradients of the neural network at the current parameter value and query points to achieve a final error value on a given dataset. The value\u03b8 where f attains error E final on D and makes the same predictions as f (\u03b8) on D is defined through layer functions and transformations. The algorithm trains f by submitting transformed gradient values and parameter updates. The algorithm applies updates to the parameter by maintaining invariances in the state of the system. The algorithm ensures that it makes the same predictions and attains the same error on the dataset under different states. This is shown through induction steps, where the algorithm queries gradients of the neural network to update the parameter value. The algorithm maintains invariances in the system state, ensuring consistent predictions and errors on the dataset. Induction steps show that the algorithm updates parameters by querying gradients of the neural network. The proof concludes with the condition that Jacobians of f have non-zero norms. The algorithm ensures consistent predictions and errors by maintaining invariances in the system state. It updates parameters by querying gradients of the neural network, with the condition that Jacobians of f have mostly non-zero norms. Claim (3) of the proposition excludes certain cases but can be extended for scaled errors and predictions. The algorithm achieves the same predictions and updates for both f and f, making f as \"easy to train\" as f. The algorithm ensures consistent predictions and errors by maintaining invariances in the system state. It updates parameters by querying gradients of the neural network, with the condition that Jacobians of f have mostly non-zero norms. The computation conducted by the algorithm is also identical, making f as \"easy to train\" as f regardless of the scaling transformation. There are no constraints on the explosion rate r, allowing for trainable networks with exploding Jacobians of any shape and depth. The proposition can be extended to non-deterministic algorithms using distributions and expectations, and directional derivatives can be used to cover nonlinearities like ReLU and SeLU. The GSC(k, l) measures the quadratic expectation of the relative size of the change in the value of f l in response to a change in f k. It is calculated using the singular value decomposition and applies to a fully connected linear layer without trainable bias parameters. If \u03b8 k is random and certain conditions are met, GSC(k, l) further measures the relative change in f l in response to a change in \u03b8 k. The quadratic expectation of \u03b8 k is independent of its length and all products of two different entries have an expectation of 0.\u03b8 k is random and can be expressed as the product of a random scalar length variable and an independent random vector orientation variable of unit length. The proposition covers initialization strategies for weight matrices, including Gaussian or orthogonal initialization. The network transformations must compensate for previous layer factors and parameter rescaling to maintain predictions and error values. The proposition covers weight matrix initialization strategies like Gaussian or orthogonal. It addresses network transformations that control activation scale, excluding certain changes like weight scaling in tanh or SeLU networks. Changing weights in ReLU networks is covered if error layer compensates. Linear layers followed by normalization are also covered. The proof shows that a combination of a nonlinearity, matrix multiplication, and layer normalization is bijective by demonstrating that the inverse image of any point under this transformation is a single point on the hypersphere. The proof demonstrates that the inverse image of any point under the transformation is a single point on the hypersphere. This is shown by contradicting the assumption of multiple inverse image points and proving the strict monotonicity of the function. Additionally, it is proven that the function assigning the length of the inverse image is strictly increasing away from the origin. The function assigning the length of the inverse image is strictly increasing away from the origin, proving that the function is continuous. The function assigning the length of the inverse image is strictly increasing away from the origin, proving continuity. To show left-continuity, consider a sequence approaching a point x from the left on the ray with inverse images. The lengths of the inverse images converge to the length of the inverse image of x. The function f assigns the length of the inverse image, proving continuity by showing left-continuity with converging lengths of inverse images. The co-domain of f is the length of the inverse image for each point on the ray. The function f assigns lengths of inverse images to points on the ray, proving continuity by showing left-continuity with converging lengths. The co-domain of f is the positive reals, argued by contradiction. If not, a non-empty set S of positive reals not attained exists. If s > 0 and s \u2208 S, a sequence x n with f(x n ) \u2192 s and strictly increasing f(x n ) is found. The inverse images of x n, denoted as y n, converge as |y n (i)| is bounded from above. The limit y lim is found as y n (i) converges, with all y n (i) having the same sign. The function f assigns lengths of inverse images to points on the ray, proving continuity by showing left-continuity with converging lengths. The forward image of y lim lies on the ray, leading to a contradiction. If s > 0 and s \u2208 S, a point x on the ray exists with f(x) = s. The inverse image y has certain properties based on the function \u03c3. Claim 6 states that the inverse image of the ray intersects the hypersphere in a single point, based on previous claims about the function f and its properties. By claim 5, there is a point on the ray with an inverse image of length r, and by claim 3, there is exactly one such point. This implies that the inverse image of the ray intersects the hypersphere in exactly one point. The proposition extends to different nonlinearities in the nonlinearity layer as long as certain conditions are met. A random function can be 'k-diluted in expectation' with respect to a random vector v under specific conditions. A function \u03c1(v) is 'scale-symmetric decomposable' (SSD) if it can be expressed in a certain form. Similarly, a random matrix S is 'scale-symmetric decomposable' (SSD) if it satisfies certain criteria. The proposition discusses random functions that are k-diluted in expectation with respect to a unit length vector u. It introduces matrices S and random functions \u03c1 that are either SSD or multiples of the identity, all independent and 2-diluted in expectation with respect to u. The proof involves induction over B, showing claims related to the symmetry of \u03c1 and S. The proposition discusses random functions that are k-diluted in expectation with respect to a unit length vector u. Matrices S and random functions \u03c1 are either SSD or multiples of the identity, all independent and 2-diluted in expectation with respect to u. The proof involves induction over B, showing claims related to the symmetry of \u03c1 and S. Claim (3) is proven by showing that f1(u) is radially symmetric. In the induction step, the radially symmetric property of S\u2022u is used to show the symmetry of S1S2..SBu. The proof involves induction over B, showing claims related to the symmetry of random functions. Claim (3) is proven by demonstrating the radial symmetry of f1(u) and S1S2..SBu. A Gaussian initialized matrix and an orthogonally initialized matrix are both SSD. The proof shows that popular skip connection types fulfill the condition of being either the identity or SSD. While many ResNets do not fully meet the SSD condition on \u03c1 b, they come close. The orientation of \u03c1 b can be governed by an independent, uniform unit length vector u \u03c1 if the last operation is multiplication with an SSD matrix. The loss of information on the length of the incoming vector should not significantly change the overall behavior due to random and high-dimensional inputs. The proposition applies in expectation over randomly initialized matrices, specifically for high-dimensional matrices. The compositional depth of an MLP with nominal depth L is defined by its linear layers, which are a combination of unparametrized initial functions and parametrized residual functions. See section D for the formal definition of effective depth and related concepts. The text discusses the gradient term in a neural network, specifically focusing on the update of parameter sub-vectors in an MLP. The theorem presented considers an MLP with all parameter sub-vectors initialized to zero and the update process with a sequence of updates. The text also mentions the number of ways to choose distinct positive integers that sum up to a given number. The text discusses the gradient-based algorithm for updating parameter sub-vectors in an MLP. It focuses on setting a small value for h to ensure effective depth \u039b and analyzes four conditions for the algorithm. The proof can be extended to algorithms using gradients of other layers and randomness. The batch size is assumed to be 1 but can be applied to larger batch sizes. The text discusses setting a small value for h to ensure effective depth \u039b in an MLP. It analyzes conditions for the algorithm, including bounding the gradient around the current parameter value. The update size is divided by the weight matrix in the initialized state for practicality. The strongest condition bounds all updates by the largest useful update size in the initialized state. The text discusses conditions for the algorithm, including bounding the gradient and ensuring effective depth \u039b in an MLP. It asserts two main points: bounding the sum of \u039b-residual terms and ensuring alg() is \"relatively Lipschitz\" over the gradient. This condition is met by SGD and custom layer-wise step sizes, but not by RMSprop or Adam. The text discusses conditions for the algorithm in training neural networks, focusing on bounding the gradient and ensuring effective depth \u039b in an MLP. It highlights limitations of RMSprop and Adam in scaling gradients and the exponential bound on training time. The loose bound suggests networks can be trained deeper than practical limits. The text discusses conditions for training neural networks, focusing on bounding the gradient and ensuring effective depth in an MLP. It highlights limitations of RMSprop and Adam in scaling gradients and the exponential bound on training time. The loose bound suggests networks can be trained deeper than practical limits. The hypersphere is a d \u2212 1-dimensional subspace in R d. Any endomorphism f l on that subspace will have certain properties. The text discusses conditions for training neural networks, focusing on bounding the gradient and ensuring effective depth in an MLP. It highlights limitations of RMSprop and Adam in scaling gradients and the exponential bound on training time. The hypersphere is a d \u2212 1-dimensional subspace in R d. Any endomorphism f l on that subspace will have Jacobians with at least one zero singular value. The text discusses conditions for training neural networks, focusing on bounding the gradient and ensuring effective depth in an MLP. It highlights limitations of RMSprop and Adam in scaling gradients and the exponential bound on training time. The hypersphere is a d \u2212 1-dimensional subspace in R d. Any endomorphism f l on that subspace will have Jacobians with at least one zero singular value. By integration by substitution, it is shown that nonzero singular values are assumed to be independent. The text then goes on to prove a claim by induction regarding certain identities and conditions. The text discusses conditions for training neural networks, focusing on bounding the gradient and ensuring effective depth in an MLP. It highlights limitations of RMSprop and Adam in scaling gradients and the exponential bound on training time. The hypersphere is a d \u2212 1-dimensional subspace in R d. Any endomorphism f l on that subspace will have Jacobians with at least one zero singular value. By integration by substitution, it is shown that nonzero singular values are assumed to be independent. The claim is proven by induction regarding certain identities and conditions, including the independence of u l+1 from \u03b8 l. Conditions for randomly initialized weight matrices are also discussed, with specific requirements for fulfilling conditions (4) and (5) related to weight matrix initialization and layer operations. The text discusses conditions for training neural networks, focusing on bounding the gradient and ensuring effective depth in an MLP. It highlights limitations of RMSprop and Adam in scaling gradients and the exponential bound on training time. The hypersphere is a d \u2212 1-dimensional subspace in R d. Any endomorphism f l on that subspace will have Jacobians with at least one zero singular value. By integration by substitution, it is shown that nonzero singular values are assumed to be independent. The claim is proven by induction regarding certain identities and conditions, including the independence of u l+1 from \u03b8 l. Conditions for randomly initialized weight matrices are also discussed, with specific requirements for fulfilling conditions (4) and (5) related to weight matrix initialization and layer operations. The orientations of the terms from each other up to preserving their angle. Finally, note that J l (\u03b8 l , F l+1 )u always lies in the left-null space of J l (\u03b8 l , F l+1 ). If the weight matrix is Gaussian initialized, the product of a Gaussian initialized matrix and an orthogonally initialized matrix is Gaussian initialized. Hence, we can insert an additional orthogonally initialized matrix and then proceed with the previous argument to show that conditions (4) and (5) are fulfilled. After applying a linear transformation with one of the two initializations, conditions (4) and (5) hold except for the length of f l is not 1. Hence, even if length-only layer normalization is not used as part of the endomorphism, we expect (4) and (5) to hold approximately in practice. Conditions (2) and (3) are not fulfilled in practice. The text discusses conditions for training neural networks, focusing on bounding the gradient and ensuring effective depth in an MLP. Conditions (2) and (3) are not fulfilled in practice, as they are used to derive a greater than unit qm norm from unit determinants. Theorem 3 introduces random vectors g and u, a k-diluted function f, and assumptions related to the Jacobian of \u03c1. Key assumptions include E(Su).(\u03c1(u)) = 0 and E(gR(u)).(gS) = 0. The growth of the GSC is represented by r, which is the expectation over the ratio ||\u03c1(u)||2||g||2. The key assumptions for training neural networks involve the skip connection being uncorrelated to the residual block function and gradient. This holds true for the forward direction under certain conditions, but not for the backward direction due to complex dependencies. However, mean field theory studies suggest that this dependence is immaterial. The key assumptions for training neural networks involve the skip connection being uncorrelated to the residual block function and gradient. This holds true for the forward direction under certain conditions, but not for the backward direction due to complex dependencies. The skip connection also indirectly affects the distribution of the incoming gradient. The skip connection in neural networks affects the gradient distribution and scale as it flows towards the input layer. The compositional depth of the Taylor approximation in ResNet architectures is determined by the number of parametrized residual functions. The skip connection in neural networks affects the gradient distribution and scale as it flows towards the input layer. A term s l (T m (\u03b8, X)) is added at each layer l with a skip connection in ReLU MLPs. The looks-linear initialization ('LLI') achieves an approximate orthogonal initial state. Normalization layers do not use trainable bias and variance parameters. A network of compositional depth N contains N linear layers and N \u2212 1 nonlinearity layers. In experiments with Gaussian noise and CIFAR10 datasets, different error layers and layer widths were used. The network had a compositional depth of 50 or 51, with data inputs and predictions of specific dimensions. Various initialization schemes for weight matrices were employed. The weight matrices in the network are initialized using different schemes such as Gaussian, orthogonal, and looks-linear. The first layer is always linear, followed by 25 skip connections, each bypassing a block of 6 layers. The final layers include a normalization layer, softmax (CIFAR10 only), and an error layer. In the final layers of the network, a normalization layer is added, followed by softmax (CIFAR10 only) and an error layer. Skip connections are used to transform the width from 3072 to 100 in the first linear layer, and from 100 to 10 in the last residual block. In some experiments, skip connections use random matrices with Gaussian noise. Inputs and labels are 100-dimensional vectors with entries drawn from a Gaussian distribution. In experiments, 100-dimensional input and label vectors were used, with entries from independent Gaussian distributions. Input vectors were normalized to length 10, and 100 datasets of size 10,000 were created. Forward activations and gradients were computed for each dataset and architecture studied. Metrics such as Expected GSC and pre-activation standard deviation were calculated for each layer without conducting training. We computed summary statistics for each layer in the architecture by averaging results over 100 datasets. For CIFAR10 experiments, features were preprocessed to have zero mean and unit variance. Different batch sizes were used for training different architectures with SGD. Trained various architectures with SGD using two methods: a single step size for all layers and a custom step size for each layer. Conducted a grid search for starting step sizes and adjusted step size based on training error. Training terminated after 500 epochs or 11 step size reductions. Representative step size selected based on lowest final training error. In the study, different starting step sizes were used for each layer in training a pre-trained network. Step sizes were periodically divided by 3 and training was terminated after 11 divisions or 500 epochs. Metrics computed included largest relative update size, effective depth throughout training, and training classification error at the end of each epoch. The study focused on the impact of reducing compositional depth via Taylor expansion after training, GSC, pre-activation standard deviation, and sign diversity. Operator norms of residual weight matrices were analyzed post-training, with optimal step sizes estimated for linear layers in CIFAR10 experiments. Pre-training involved selecting linear layers with similar step size requirements in exploding architectures. For pretraining in exploding architectures, linear layers were selected based on similar step size requirements. Different initialization methods were used for different types of layers, with a focus on maintaining relative update sizes below certain thresholds to prevent exploding architectures. During pre-training, small step sizes were chosen to prevent impacting effective depth. Training continued until the classification error reached 85%, with step sizes determined through grid search. In the selection phase, linear layers were trained one by one for one epoch each, with parameter changes undone between layers. Step sizes for each layer were chosen through grid search. After pre-training with small step sizes to prevent depth impact, linear layers were trained individually for one epoch each using grid search for step sizes. The selection phase involved searching over a grid with multiplicative spacing of 1.5 to find the step size with the lowest training error, avoiding relative update sizes of 0.1 or higher to prevent weight instability. Without pre-training, the selection phase resulted in noisy outcomes due to the need for large step sizes to jump between random points in parameter space, leading to spurious \"success\" with excessively large step sizes. Pre-training with small step sizes helps prevent depth impact. Training linear layers individually with grid search for step sizes, selecting the best step size with the lowest training error. Clipping is used to eliminate outliers and improve end-of-training error for specific architectures like vanilla tanh and ResNet layer-tanh. In the selection phase, linear layers are trained individually with grid search for step sizes, and the largest relative update size induced by the step size is shown in figures 2A, 4A, 5A, and 6A. Smoothing involves creating a mini-regression dataset to fit a line via least-squares regression in log scale to reduce noise among layer-wise step sizes. The error reduction process involves scaling all layer-wise step sizes with a single constant, chosen through a iterative process of training, rewinding, and adjusting the constant. Some architectures benefited from using clipping to find the best scaling constant. The study involved training multiple networks with different clipping techniques to find the best scaling constant. The network with the lowest end-of-training error was selected for presentation in the paper. Comparatively, 21 end-of-training error values were compared for single step size training."
}