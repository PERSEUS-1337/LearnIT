{
    "title": "HygPD4H22N",
    "content": "In this paper, three architectures are proposed to explore the intersection of network neuroscience and deep learning, aiming to bridge the gap between the two fields. By incorporating teachings from network neuroscience and connectomics, improvements over the ResNet architecture are demonstrated, along with a possible connection between early training and network spectral properties. Additionally, the trainability of a DNN based on the neuronal network of C.Elegans is shown, achieving 55.7% accuracy on the MNIST dataset by training only the edge-weights. The small-world topology is retained throughout the training phase. The small-world topology, with a small-world propensity metric > 0.53 for all epochs, is maintained during training for the MNIST dataset and C.Elegans neuronal network. Network neuroscience and connectomics focus on modeling brain networks with small-world configuration, supporting both segregated and distributed information processing. Human brain networks exhibit small-world properties, capturing individual cognition and physiological basis. The field of deep neural networks has mostly overlooked brain network topology, despite being neuro-biologically inspired. Some studies have shown that networks with small-world connectivity result in lower learning error and training time compared to regular or random networks. The authors experimented with small-world feedforward neural networks, achieving better results for a diabetes diagnosis dataset compared to fully connected networks. They aim to bridge the gap between connectomics/network-neuroscience and deep learning communities, exploring different neural network modules. The paper introduces RogueNet, a DNN model with small-world topological properties using ResNet modules as nodes. It also presents RamanujanNet and C.ElgansNet architectures based on BID14's node model. The RogueNet architecture expands on PreActivation ResNet by adding random additive connections between modules. The ResNet modules are structured in a global manner resembling a small-world network. Additive skip connections are combined with module activations using a convolutional gating mechanism inspired by the visual cortex hierarchy. High-level ConvNet units explain variance in monkey inferotemporal cortex neurons. The base architecture consists of Pre-Activation ResNet modules with random connections applied. The RogueNet architecture includes a 'bottleneck' module with three convolutional layers and skip connections from previous layers. The connectivity pattern is a small-world network inspired by Highway layers. The neuron architecture is shown in Figure 3. The RogueNet architecture uses a gating mechanism inspired by Highway layers, but with convolutional layers instead. The transformation function sums the output and activations from skip connections, described mathematically with a 1 by 1 convolution and sigmoid activation function. The RogueNet architecture incorporates a 1 by 1 convolution with a bias term and sigmoid activation function to determine the weighting of skip connection activations. Training was done on the CIFAR-10 dataset for 200 epochs using specific parameters, including a random re-computation of skip connection patterns based on a Watts Strogatz graph recipe. The RogueNet architecture explores various configurations to optimize performance, leveraging spectral properties of graph connectivity patterns. Results show that using pre-activations, trainable parameters in gating mechanisms, and shuffling skip connections once per epoch yield the best performance compared to ResNet34 on CIFAR-10. In their work, Xie et al. propose an architecture using expander graphs replicated across three modules. They experiment with various spectral gaps on 36 nodes and 64 edges graphs, training RamaujanNet on MNIST and ImageNette for 1 epoch each. The first epoch accuracy was positively correlated with spectral gap on MNIST and Imagenette datasets. Pearson correlation on MNIST was 0.55 and on Imagenette was 0.65. A new architecture based on a single graph module using a small world network topology from C.Elegans was proposed. Experiments were conducted on MNIST, KMNIST, and Fashion MNIST datasets. An experiment was conducted where parameters were frozen except for edge weights in the C.Elegans small world network. The C.ElegansNet architecture was trained on MNIST, KMNIST, and Fashion MNIST for 2 epochs each using the Adam optimizer. Another experiment involved training on MNIST for 20 epochs while tracking the small world propensity of the graph with learned edge weights. Parameters were standardized and a sigmoid activation function was applied before computing the small world propensity. The C.ElegansNet architecture achieved competitive results on MNIST, KMNIST, and Fashion MNIST, with a mean test accuracy of 99%, 93%, and 90% respectively. Freezing all parameters except edge weights resulted in 55.7% accuracy on MNIST. The small world propensity of the C.Elegans graph with learned edge weights remained stable during training. The experiments demonstrated improvements over ResNet by including skip connections with small world properties, exploring the impact of spectral gap and expander graphs, and studying the trainability of a DNN based on the neuronal network of C.Elegans. Future work will focus on examining other spectral properties of graph topologies and exploring parameter efficient connectivity patterns for improved performance. Performance of C.ElegansNet improved by freezing parameters except graph edge weights, achieving similar results to networks with more parameters."
}