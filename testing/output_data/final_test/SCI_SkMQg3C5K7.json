{
    "title": "SkMQg3C5K7",
    "content": "The study analyzes the speed of convergence to the global optimum for gradient descent training a deep linear neural network by minimizing the L2 loss over whitened data. Convergence at a linear rate is guaranteed under certain conditions on the dimensions of hidden layers, weight matrix initialization, and initial loss. These assumptions are necessary for convergence, especially in scalar regression cases. The results extend previous analyses of deep linear residual networks. Residual networks in deep learning rely on gradient-based optimization methods to solve non-convex problems. Efforts are being made to analyze the mathematical properties of critical points for convergence to the global optimum. Certain conditions, such as no poor local minima and the strict saddle property, have been shown to ensure convergence in various settings. However, these conditions may not always hold, as demonstrated in some cases. The landscape approach for deep networks faces limitations in proving convergence to global minimum due to conditions that may not hold for models with three or more layers. A potential solution is to focus on trajectory-based approaches, which have been explored in the context of linear neural networks. Linear neural networks, specifically fully-connected networks with linear activation, present challenges in optimization due to non-convex training problems with multiple minima and saddle points. BID1's research suggests that adding redundant linear layers to a linear prediction model can sometimes accelerate optimization, challenging the belief that convex problems are always preferable. Efficient convergence to global minimum remains elusive, with recent progress in analyzing linear residual networks. The current paper conducts a trajectory-based analysis of gradient descent for deep linear neural networks, exploring various settings beyond the previous research by BID3. The analysis considers convergence to a global minimum under specific conditions related to initialization and layer properties. The paper analyzes gradient descent for deep linear neural networks, showing convergence to the global minimum with specific conditions on layer dimensions, initialization, and initial loss. The results apply to networks of any depth and input/output dimensions. The paper presents an analysis of gradient descent for deep linear neural networks, demonstrating convergence to the global minimum under specific conditions on layer dimensions, initialization, and initial loss. The results apply to networks of any depth and input/output dimensions, with key assumptions necessary for successful convergence. The paper discusses gradient descent for deep linear neural networks, showing convergence to the global minimum under certain conditions. It includes a review of relevant literature and comparisons with other results. The training set is denoted by {(x (i) , y (i) )} m i=1 \u2282 R dx \u00d7 R dy, and the hypothesis is learned from a parametric family H := {h \u03b8 : R dx \u2192 R dy | \u03b8 \u2208 \u0398}. Whitened datasets are considered, with standard calculations showing convergence. The paper focuses on linear neural networks and their convergence to the global minimum using gradient descent. It discusses minimizing loss over whitened data and the parametric family of hypotheses for linear models. The paper discusses training a deep linear neural network with gradient descent, focusing on the optimization problem and convergence properties. For depth N = 1, gradient descent converges to the global minimum at a linear rate, while for depth greater than 1, it becomes a non-convex program. The paper discusses training a deep linear neural network with gradient descent, focusing on the optimization problem and convergence properties. For depth N = 1, gradient descent converges to the global minimum at a linear rate, while for depth greater than 1, it becomes a fundamentally non-convex program. The convergence properties of gradient descent are highly non-trivial, and a direct analysis of the trajectories taken by gradient descent can provide a guarantee for linear rate convergence to the global minimum. Additional notation is introduced for further analysis. In this section, the paper establishes the convergence of gradient descent for deep linear neural networks by analyzing the trajectories taken by the algorithm. It introduces concepts like approximate balancedness and deficiency margin to facilitate the main convergence theorem. The notion of approximate balancedness is formally defined, and a convergence guarantee is derived with constant probability over random initialization. The paper discusses the concept of \u03b4-balanced matrices in deep linear neural networks. It shows that if the weights are initialized to be approximately balanced, they will remain so during gradient descent iterations. This condition is easily met in linear residual networks. The paper introduces the concept of deficiency margin in deep linear neural networks. It defines deficiency margin as the distance a matrix is from containing low-rank matrices. If a matrix has deficiency margin c with respect to a target matrix, then any matrix with similar distance from the target has singular values bounded away from zero. The paper discusses the importance of initializing a linear neural network with weights that have a deficiency margin with respect to the target matrix \u03a6. This ensures convergence to the global minimum during gradient descent, with faster convergence rates as the deficiency margin increases. Claim 3 in the appendix provides insights on deficiency margins in single output models with Gaussian initialization. The paper emphasizes the significance of initializing a linear neural network with weights that have a deficiency margin concerning the target matrix \u03a6 for faster convergence during gradient descent. Claim 3 in the appendix delves into the impact of deficiency margins in single output models with Gaussian initialization, highlighting a delicate trade-off between standard deviation, balancedness, and meeting deficiency margins for optimal convergence. The deficiency margin in weight initialization for linear neural networks affects convergence during gradient descent. It is crucial for ensuring global minima in sublevel sets, but alone is not sufficient for proving convergence due to the non-convex and non-smooth nature of the loss landscape. Further research is needed to address rank-deficient target matrices and the lack of smoothness in the loss landscape. The deficiency margin in weight initialization for linear neural networks is crucial for convergence during gradient descent. Approximate balancedness and deficiency margin are key in ensuring efficient convergence, preventing issues like the vanishing or exploding gradient problems. Residual connections help maintain balancedness, leading to linear convergence to the global minimum. The assumptions in Theorem 1 regarding approximate balancedness and deficiency margin at initialization are crucial for efficient convergence in linear neural networks. Violating these assumptions can lead to convergence failure, as demonstrated in Appendix C. For linear residual networks, a target matrix with a Frobenius distance less than 0.5 from identity is a sufficient condition for meeting these assumptions. This strengthens a central result in BID3. Additionally, a scheme is presented for random near-zero initialization in scalar regression to ensure assumptions are met for efficient convergence to the global minimum. The assumptions in Theorem 1 are crucial for efficient convergence in linear neural networks. Violating these assumptions can lead to convergence failure. The dimension of each hidden layer must be greater than or equal to the minimum between those of the input and output layers. The likelihood of both assumptions being met simultaneously is left for future work. The initial end-to-end matrix W 1:N (0) must have full rank for deficiency margin c > 0. Theorem 1 relies on Lemma 1, which shows descent when \u03c3 min (W 1:N ) is bounded away from zero. The gradient of L 1 (\u00b7) is non-zero except at a global minimum. Lemma 1 is proven for perfect initial balancedness and small learning rate in an idealized setting. The objective value at each iteration of gradient descent is discussed, and a stronger version of the equation is shown. The weights remain balanced according to Theorem 1 and Claim 1 in BID1. The weights in BID1 remain balanced throughout optimization, leading to a differential equation for the matrix movement. The derivative of L 1 (W 1:N (\u03c4 )) yields Equation (10) without a 1/2 factor, with key equalities and inequalities derived. The proof of Theorem 1 follows from Lemma 1, showing non-negativity of coefficients and inequalities for every t. Deficiency margin c of W 1:N (0) implies \u03c3 min W 1:N (t) \u2265 c, leading to non-negative values no greater than 1. Incorporating the inequality FORMULA1. The proof of Theorem 1 follows from Lemma 1, ensuring non-negativity of coefficients and inequalities for every t. Balanced initialization assigns weights randomly while maintaining perfect balancedness. The concept, along with Theorem 1, guarantees linear convergence for scalar regression with constant probability over initialization randomness. With balanced initialization (Procedure 1), weights are randomly assigned with perfect balancedness, ensuring linear convergence for scalar regression with constant probability. The loss at iteration T of gradient descent is bounded by a certain value with high probability. The study presents empirical evidence supporting the benefits of initializing with balancedness in practice for linear neural networks trained via gradient descent. The experiment focused on a scalar regression task using the \"Ethanol\" dataset from the UCI Machine Learning Repository, showcasing promising results. The study tested different standard deviations for initialization in a three-layer network with hidden widths set to 32. A grid search over learning rates was conducted for each standard deviation to find the fastest convergence. Results showed a range of standard deviations leading to fast convergence, with optimization decelerating significantly below or above this range. The study evaluated the impact of initialization standard deviations on convergence time in three-layer and eight-layer neural networks with hidden widths set to 32. Results showed that overly small or large initialization hindered convergence, while an optimal range led to fast convergence. Theoretical findings were validated through gradient descent trajectories preserving weight balancedness. Transitioning from three layers to eight aggravated the instability with respect to initialization. There is now a narrow band of standard deviations that lead to convergence in reasonable time, and outside of this band convergence is extremely slow. The magnitude of the end-to-end matrix W 1:N depends on the standard deviation exponentially in depth, leading to a range of standard deviations required for moderately sized W 1:N for large depths. The balanced initialization procedure (Procedure 1) assigns weights W 1:N directly and distributes them in a perfectly balanced manner, ensuring approximate balancedness throughout optimization. This approach replaces the customary layer-wise scheme and results in consistent balancedness under gradient descent, as shown in the experimental results. The experimental results show that balanced initialization persists under gradient descent in training fully-connected non-linear neural networks. Different initializations based on Gaussian perturbations are evaluated, with balanced initialization leading to improved convergence. This effect is also observed in settings involving non-linear activation, softmax-cross-entropy loss, and stochastic optimization. The experiment involved evaluating different initializations for a fully-connected neural network with two hidden layers. The results, shown in FIG2, demonstrated that balanced initialization based on Gaussian perturbations led to improved convergence during training. Theoretical studies on gradient-based optimization in deep learning focus on proving convergence to global minimum through properties like no poor local minima and strict saddle. Various works have examined these properties in different deep learning settings, but the success of landscape-driven analyses in formally proving convergence remains a topic of active research. The success of landscape-driven analyses in proving convergence to global minimum for gradient-based algorithms has been limited to shallow models. An alternative approach is analyzing the trajectories taken by the optimizer, but current analyses only apply to shallow models. Deep models have also been studied in the context of linear neural networks, drawing technical ideas from previous works. However, existing treatments focus on gradient flow and do not address computational efficiency. BID3 proves convergence to global minimum for gradient-based algorithm training deep models, similar to our work on linear neural networks. BID3 is limited to linear residual networks with uniform width, while we allow for more flexibility in network configurations. Our analysis focuses on scenarios where \u03a6 is symmetric and positive definite, or within a certain distance from identity. We provide a strict generalization for the latter scenario, showing convergence of gradient descent to global minima for deep linear neural networks with balanced initial weight matrices. This result applies to networks of arbitrary depth and configurations supporting full rank. Our analysis focuses on deep linear neural networks with balanced initial weight matrices supporting full rank. Efficient convergence with significant probability remains an open problem, as exponential iterations may be required for convergence in some settings. The analysis focuses on deep linear neural networks with balanced initial weight matrices, addressing the \"vanishing gradient problem\" through balanced initialization. This approach shows favorable convergence in theory and practice, with potential for further research on variants for convolutional layers. The optimization landscape near gradient descent trajectories reveals insights applicable to non-convex objectives in deep neural network training. The analysis focuses on deep linear neural networks with balanced initial weight matrices to address the \"vanishing gradient problem.\" Claim 2 states that with small standard deviation of initialization, the probability of matrices being \u03b4-balanced is high. Claim 3 discusses deficiency margin in scalar regression models, where a small standard deviation leads to meeting a deficiency margin with high probability. Claim 3 discusses the probability of meeting a deficiency margin in scalar regression models with small standard deviation. The requirement d0 \u2265 20 simplifies expressions in the claim, and the probability can be increased by adjusting constants. The sought-after standard deviations always exist. In this appendix, it is shown that violating the assumptions on initialization necessary for convergence can lead to failure. Successful optimization in deep learning depends on careful initialization. Claim 4 demonstrates that without approximate balancedness at initialization, no learning rate can guarantee convergence in certain cases. Claim 5 shows that even with perfectly balanced initializations, convergence can fail in deep learning networks with certain target matrices, regardless of the learning rate used. The text discusses the failure of convergence in deep learning networks, regardless of the learning rate used, due to certain target matrices. Additional notation and terminology are introduced for clarity in understanding the loss associated with linear neural networks during gradient descent training. The text introduces notation for the loss in linear neural networks during gradient descent training, with a focus on updating weights and preventing convergence issues. It also addresses the impact of initialization on convergence and extends the discussion to accommodate different network depths and dimensions. The text discusses the tensor product of matrices and proves a lemma related to weight matrix properties in neural networks during gradient descent training. Lemma 1 is a consequence of Lemma 2, which states that matrices have bounded spectral norms. The proof involves establishing conditions at each time step and using matrix inequalities. The proof involves showing that the singular values of matrices D1 and OD2OT are bounded by a certain value, leading to constraints on the singular values of other matrices Wj. This is done by utilizing the singular value decomposition of the matrices. The proof involves orthogonal matrices Uj, Vj, and diagonal matrix \u03a3j. The singular values of Wj are non-increasing. Using the Frobenius norm and Lemma 4, we derive inequalities for matrices A, B. By a series of transformations, we verify certain equations. The proof involves orthogonal matrices Uj, Vj, and diagonal matrix \u03a3j. By Lemma 4, we derive inequalities for matrices A, B using the Frobenius norm. We verify certain equations by writing the eigendecomposition of W 1:N. The proof involves orthogonal matrices Uj, Vj, and diagonal matrix \u03a3j. By Lemma 4, inequalities for matrices A, B are derived using the Frobenius norm. The singular value decomposition of W 1:N is written to verify certain equations. The nonzero eigenvalues of W j are decreasing along the main diagonal of \u03a3 j. If there is some j such that W j W j \u03c3 > 2 1/N C 2/N, then a contradiction arises. The proof involves orthogonal matrices Uj, Vj, and diagonal matrix \u03a3j. By Lemma 4, inequalities for matrices A, B are derived using the Frobenius norm. The singular value decomposition of W 1:N is written to verify certain equations. If W j W j \u03c3 > 2 1/N C 2/N, then a contradiction arises. Lemma 7 states that under certain conditions, the descent will occur at iteration t, with the required conditions holding for every t. The proof involves orthogonal matrices Uj, Vj, and diagonal matrix \u03a3j. Inequalities for matrices A, B are derived using the Frobenius norm. The singular value decomposition of W 1:N is used to verify equations. If certain conditions are met, descent will occur at iteration t. The Frobenius norm of higher order terms is bounded. The proof involves orthogonal matrices Uj, Vj, and diagonal matrix \u03a3j. Inequalities for matrices A, B are derived using the Frobenius norm. The singular value decomposition of W 1:N is used to verify equations. If certain conditions are met, descent will occur at iteration t. The Frobenius norm of higher order terms is bounded. Matrices A1, . . . , AK are defined using inequalities and eigenvalue decompositions. The minimum diagonal elements of D, E, and \u039b are compared, leading to further inequalities. The proof involves orthogonal matrices Uj, Vj, and diagonal matrix \u03a3j. Inequalities for matrices A, B are derived using the Frobenius norm. The singular value decomposition of W 1:N is used to verify equations. If certain conditions are met, descent will occur at iteration t. The Frobenius norm of higher order terms is bounded. Matrices A1, . . . , AK are defined using inequalities and eigenvalue decompositions. The minimum diagonal elements of D, E, and \u039b are compared, leading to further inequalities. With the base case t = 0, weights W 1 (0), . . . , W N (0) being \u03b4-balanced, A(0) holds automatically. B(0) is established with deficiency margin c > 0. C(0) is implied using conditions A(0) and Lemma 6. Lemma 7 conditions are established by A(t) and B(t), leading to the proof of A(t + 1). Using matrices A, B, and inequalities, A(t + 1) is further verified. The deficiency margin of weights W(i) is compared, ensuring descent at iteration t. By summing B(0) to B(t), the proof for A(t + 1) is completed. The proof of Lemma 2 follows by induction on t, combining Lemma 8 and Theorem 1 to establish convergence. Lemma 7 conditions lead to verifying A(t + 1) by summing B(0) to B(t). The deficiency margin of weights W(i) ensures descent at iteration t. The proof of Lemma 8 is postponed to Appendix D.5, where it will be restated as Lemma 16. An additional technique used in the proof of Theorem 2 improves the convergence rate by considering the deficiency margin of W 1:N (0). This small deficiency margin is crucial for maintaining a constant probability in the theorem. Initially, the optimization process will progress slowly due to the small deficiency margin, but it will eventually speed up after a certain period of time. After a slow start, the deficiency margin of W 1:N (t) will reach a constant, leading to faster optimization. The matrices W 1 (0), . . . , W N (0) are close to 0 initially, forming a saddle point. The increase in (t) \u2212 (t + 1) shows the iterates escaping the saddle point. The distribution of W 1:N (0) is rotation-invariant, and Lemma 8 is applied to it. The distribution of the norm W 1:N (0) 2 is not too spread out, as shown by Lemma 9. This result is crucial for the balanced initialization of W 1 (0), . . . , W N (0) to reach faster optimization. The distribution of the norm W 1:N (0) 2 is crucial for balanced initialization of W 1 (0), . . . , W N (0) for faster optimization. Assuming \u03b7 gives Equation FORMULA12 with c = DISPLAYFORM5. The conditions of Theorem 1 hold with probability at least given in Equation (38). If we choose DISPLAYFORM7, then DISPLAYFORM8. Iteration t 0 satisfies the conditions for optimization. The distribution of the norm W 1:N (0) 2 is crucial for balanced initialization of W 1 (0), W N (0) for optimization. Iteration t 0 satisfies the conditions of Theorem 1 with deficiency margin \u03a6 2 /2. By Equations FORMULA4 and FORMULA2, we can ensure (T ) \u2264 by taking DISPLAYFORM13. The probability of \u03b4-balancedness holding is at least p for d 0 \u2265 d 0. The probability of \u03b4-balancedness between layers is considered using matrices with Gaussian distributed entries. Markov's inequality is used to prove a claim, and a union bound is applied to establish the claim with a specific \u03b4 value. Notation and multilinear polynomial properties are introduced with an absolute constant for the following conditions. The lemma characterizes the norm of the end-to-end matrix following Gaussian initialization. Matrices with Gaussian entries are considered, and multilinear polynomial properties are discussed. The lemma characterizes the norm of the end-to-end matrix following Gaussian initialization, discussing multilinear polynomial properties. The proof involves choosing constants and utilizing Gaussian distributions. The lemma characterizes the norm of the end-to-end matrix following Gaussian initialization, discussing multilinear polynomial properties. The proof involves choosing constants and utilizing Gaussian distributions. D is equivalent to DISPLAYFORM2 DISPLAYFORM3, by the structure of C and D. The probability that V lands in \u2202C is at DISPLAYFORM4, which also applies to V landing in \u2202D. For any V \u2208 \u2202D, V has deficiency margin \u03a6 2 /(ad) with respect to \u03a6 with probability of at least DISPLAYFORM5. The deficiency margin of V is equal to 1 \u2212 V \u2212 \u03a6 2. \u00b5 has a well-defined density, so we can set \u00b5 to be the probability density function of V 2. Integrating over spherical coordinates gives DISPLAYFORM8. Lemma 15 is used along with the fact that the distribution of V conditioned on V 2 = r is uniform on S d (r). Proof of Claim 3 involves random vector W 1:N denoted by W \u2208 R 1\u00d7d0 R d0, with \u00b5 being rotation-invariant. The standard deviation of the entries of each W j is given by DISPLAYFORM0. The deficiency margin is at least DISPLAYFORM3. The deficiency margin in the proof is at least DISPLAYFORM3, with the target matrices \u03a6 satisfying \u03c3 min (\u03a6) = 1. Assuming c \u2265 3/4, the matrices have deficiency margin c with respect to \u03a6. For d = 1, the matrices are real numbers, and the lemma proves that the real numbers W 1 (t), ..., W N (t) have alternating signs for each integer t. The lemma proves that real numbers W j (t) have alternating signs for each integer t, with specific values for B(t) and C(t) for t \u2265 1. The proof involves setting B(1) and C(1) based on \u03b7 and A, and then iteratively updating B(t + 1) and C(t + 1) using specific formulas. The inductive step is established for the case where all W j (t) are negative, leading to the completion of Lemma 17. The lemma establishes that real numbers W j (t) have alternating signs for each integer t, with specific values for B(t) and C(t) for t \u2265 1. The proof involves setting initial values based on \u03b7 and A, and iteratively updating them using specific formulas. The loss in the d-dimensional case is equal to the loss in the 1-dimensional case, which grows exponentially for the chosen initialization. If not chosen carefully, gradients and weights can explode in deep learning practice. The lemma establishes that real numbers W j (t) have alternating signs for each integer t, with specific values for B(t) and C(t) for t \u2265 1. The proof involves setting initial values based on \u03b7 and A, and iteratively updating them using specific formulas. The loss in the d-dimensional case is equal to the loss in the 1-dimensional case, which grows exponentially for the chosen initialization. If not chosen carefully, gradients and weights can explode in deep learning practice. In addition, implementation details for running experiments in PyTorch are provided, ensuring compliance with the analysis. In experiments with linear and non-linear neural networks, PCA whitening was applied to the dataset for compliance. Labels were rescaled to ensure a unit Frobenius norm. The global optimum was computed using a specific formula. Balanced initialization was implemented for linear networks, while random elements were added for non-linear networks. In the non-linear network experiment, random orthogonal matrices were added to the network layers to maintain balanced initialization. Grid search was conducted over learning rates ranging from 10^-4 to 1."
}