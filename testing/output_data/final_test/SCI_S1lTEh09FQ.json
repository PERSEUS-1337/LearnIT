{
    "title": "S1lTEh09FQ",
    "content": "Binarized Neural Networks (BNNs) are gaining interest for their computational efficiency. However, neural networks are vulnerable to attacks, which can be harmful in safety-critical applications. Developing effective attack algorithms is crucial for building robust neural networks. In this work, the challenge of attacking Binarized Neural Networks (BNNs) is studied through combinatorial and integer optimization. A Mixed Integer Linear Programming (MILP) formulation is proposed, but it becomes intractable as the network and perturbation space grow. To address this, the IProp algorithm is introduced, which outperforms the standard gradient-based attack (PGD) on MNIST and Fashion-MNIST datasets. The success of neural networks in various tasks has led to their widespread deployment, but they can be vulnerable to attacks. Recent research focuses on enhancing the robustness of neural networks against adversarial attacks by using adversarially generated examples. This approach involves training models with adversarial examples to improve their resistance to potential attacks. By formulating adversarial learning as a robust optimization problem, researchers aim to find the best model parameters that can withstand worst-case adversarial perturbations. Attack algorithms are utilized to augment the training dataset with adversarial examples, resulting in more resilient models. These advancements highlight the importance of developing effective methods for generating adversarial examples. In this work, the focus is on designing effective attacks against Binarized Neural Networks (BNNs) BID8, which are neural networks with weights in {\u22121, +1} and the sign function non-linearity. BNNs are crucial in low-power or hardware-constrained settings, making attacking and robustifying them a significant task. The discrete, non-differentiable structure of BNNs hinders typical attack algorithms that rely on gradient information. Strong attacks are essential for effective adversarial training, motivating the need to develop better attacks to modify inputs slightly and change the neural network's predicted class. The objective of generating an optimal adversarial example is to modify input slightly so that the neural network predicts a different class. This is achieved by maximizing the difference between the target class and the predicted class, within a specified perturbation limit. Targeted attacks are guided by this objective function in the adversarial learning literature. The formulation and algorithm for generating adversarial perturbations in BNNs can be modeled as a Mixed Integer Linear Program (MILP). MILP is a powerful tool for optimization problems and is conceptually useful due to the piecewise constant nature of BNNs. Mixed Integer Linear Programming (MILP) is used to generate adversarial perturbations in Binary Neural Networks (BNNs). MILP allows for various constraints and objectives, but comes with a computational cost. While MILP can compute globally optimal perturbations for small networks, scalability is limited. Experimental results show a performance gap between gradient-based attacks and optimal perturbations in BNNs. IProp (Integer Propagation) is a novel attack algorithm for Binary Neural Networks (BNNs) that is more scalable and efficient than MILP. It utilizes target propagation to tune the perturbation vector iteratively, starting from the last hidden layer and working towards the input layer. The use of MILP formulations enables layer-to-layer target propagation in IProp. IProp is a novel target propagation algorithm for adversarial machine learning, utilizing MILP formulations for layer-to-layer propagation. It outperforms PGD on BNN models pre-trained on MNIST and Fashion-MNIST datasets, showing promise for integer optimization methods in adversarial learning. The paper discusses the use of integer optimization methods in adversarial learning and discrete neural networks, with a focus on Binarized Neural Networks (BNNs) and their applications in various domains. The MILP formulation and a heuristic algorithm called IProp are presented, along with experimental results. BNNs are highlighted for their computational efficiency and suitability for fast hardware implementations. Adversarial attacks on neural networks have been extensively studied, with methods like L-BFGS and Fast Gradient Sign Method (FGSM) being developed. Projected Gradient Descent (PGD) has shown to be more effective, especially with random restarts. Comparison with SPSA is also included in the study. Various attacks have been devised for different perturbation constraints. Adversarial attacks on neural networks have been extensively studied, with methods like L-BFGS and Fast Gradient Sign Method (FGSM) being developed. Projected Gradient Descent (PGD) has shown to be more effective, especially with random restarts. Different attacks have been created for various perturbation constraints, including MILP attacks against ReLU networks and an evaluation of attack methods against BNNs. This has led to the search for more powerful attacks exploiting the discrete nature of BNNs. Verification in neural networks involves proving that a network will not misclassify a given point, with SAT solvers being more effective than MILP solvers for Binary Neural Network (BNN) verification. The IProp algorithm complements exact verification methods by quickly finding counterexample perturbations. A Mixed Integer Linear Programming formulation is used for BNN attack problems, offering insights for designing better algorithms like IProp. The BNN attack operates on a fully-connected, feed-forward BNN with specific variables for perturbations. The BNN attack involves perturbations in feature j, pre-activation sums, activation values, and constraints for a forward pass in the BNN. The MILP formulation implements the BNN attack from the perturbed input to the output layer. The MILP attack model involves perturbations in pre-activation values and constraints for a forward pass in the BNN, accommodating batch normalization for effective training. Solving the MILP attack model becomes challenging quickly, while gradient-based attacks like PGD are not suitable for BNNs. The MILP attack model involves perturbations in pre-activation values and constraints for a forward pass in the BNN, accommodating batch normalization for effective training. However, gradient-based attacks like PGD are not suitable for BNNs due to issues with the sign function activation. BID8 propose using a differentiable surrogate function g to address this issue, but the gradient used by PGD may not indicate the correct ascent direction during backpropagation. IProp is a BNN attack algorithm that operates directly on the original BNN, rather than an approximation of it. The objective function can be expanded to show that when the weights out of a neuron into the output neurons are equal, the activation value of that neuron does not contribute to the objective function. IProp is a BNN attack algorithm that aims to achieve ideal target activation values in hidden layers by maximizing the objective function. The algorithm operates directly on the original BNN and updates the activation values based on the solution that improves the objective. The algorithm IProp aims to achieve ideal target activation values in hidden layers by updating activation values based on a linear optimization problem with constraints. The variables to optimize over are activation vectors, and the optimization problem models a satisfaction problem. The IProp algorithm aims to achieve ideal target activation values in hidden layers by solving a sequence of optimization problems. Starting with the last layer and moving backwards, each solution provides the target for the subsequent layer. After obtaining the final solution, a perturbation of x is computed to produce the target activation values. This process is a single iteration of the algorithm, with both optimization problems being NP-Hard. The IProp algorithm decomposes the full-network MILP problem into smaller subproblems by solving optimization problems that are NP-Hard. These layer-to-layer problems are easier to solve than the MILP problem, making the algorithm efficient for networks with 2-5 hidden layers and 100-500 neurons. However, there are critical questions raised regarding the ambitious nature of setting target activation values and the potential failure of target propagation. In solving the sequence of problems FORMULA7, a layer's problem may have multiple optimal solutions that achieve the same number of targets in the next layer. The perturbation budget is crucial as it can make many targets impossible to achieve. Solutions are proposed to make the algorithm IProp-aware and effective, inspired by gradient optimization methods to avoid overshooting good solutions. IProp aims to limit target satisfaction to prevent overly optimistic solutions. The current incumbent perturbation, denoted as p*, is updated based on the binary activation vector of each layer. Target propagation is performed to obtain a potential perturbation pt, which is run through the BNN. If the objective function improves, p* is updated, and the process continues until a local optimum is reached or time runs out. IProp algorithm prioritizes equally good solutions based on the L0 distance between binary activation vectors of layers. This metric is used as a tie-breaker in the optimization process. The cost metric is used as a tie-breaker in the optimization process, incorporated into the objective with a small multiplier. Binarized neural networks are trained using BNN code 1 by BID8 on a machine with a GeForce GTX 1080 Ti GPU. Networks of various depths and widths are trained to minimize cross-entropy loss on MNIST and Fashion-MNIST images, achieving 90-95% test accuracy. These networks are larger than those used in recent papers leveraging integer programming or SAT solving for adversarial attacks or verification. In experiments, 60,000 MNIST and Fashion-MNIST training images were used to achieve 90-95% test accuracy on MNIST and 80-90% on Fashion-MNIST. Attack generation utilized the Gurobi Python API and iterated PGD in PyTorch, with a 3-minute time cutoff on test points. MILP problems were solved within IProp with a 10-second cutoff. Attacks were run on a cluster of 5 compute nodes, each with 64 cores and 256GB of memory. The target class was specified as the second-highest activation, and the fraction of flipped test points was shown in FIG3. The MILP approach finds optimal attacks within the time cutoff for small perturbation budgets and networks, but becomes computationally intensive as network size grows. The average runtime for the solver is 27 seconds for a 2x100 network with a budget of 0.01, and 777 seconds for a 2x200 network. Most runs timeout at the MILP time limit of 1800 seconds as the network size increases. This is attributed to the weakness of the linear programming relaxation and heuristics implemented by Gurobi. IProp (in red bars) achieves a success rate close to optimal MILP performance on small networks and scales better than MILP. It outperforms PGD for most network architectures, especially for small perturbations. IProp is more computationally expensive than PGD for larger perturbation values. IProp achieves higher values than PGD, indicating its effectiveness in modifying network activations. IProp constructs stronger attacks that surpass PGD attacks after a few seconds. The effect of step size in IProp is investigated, showing that smaller step sizes may lead to easier target activations. In the context of investigating the effectiveness of IProp in modifying network activations and constructing stronger attacks compared to PGD, an adaptive step size strategy is proposed. The strategy involves initializing the step size at 5% of the network width and halving it every 5 iterations if no better incumbent is found. This approach allows for wider network architectures to use larger step sizes, leading to improved performance in achieving target activations. The IProp attack method demonstrates flexibility by allowing warmstarting with initial perturbations obtained from running PGD. This approach significantly improves attack success rates, showcasing the value of finding good initial solutions in combinatorial local search methods for generating adversarial examples. The MILP model and IProp algorithm are used for attacking BNNs, aiming to generate perturbations for adversarial training. The MILP model shows limitations in speed for training, while IProp algorithm scales up solving process effectively compared to PGD attack. This approach lays the foundation for improving BNN robustness to perturbations. The work presents contributions to improve attacks and robust training of BNNs by utilizing ideas from discrete optimization and machine learning. Target propagation ideas like IProp can potentially enhance BNN training, along with exploring hard-threshold networks. The method of \"simultaneous perturbation stochastic approximation\" (SPSA) is implemented for gradient-free attacks. SPSA follows BID33 using Adam optimization with learning rate 0.01, batch size of 100, and iteration limit of 100. SPSA performs worse than IProp and PGD in flip prediction rates."
}