{
    "title": "rJgDT04twH",
    "content": "In the field of Deep Reinforcement Learning (DRL), the study explores using implicit human feedback to enhance training. Instead of explicit feedback, non-expert humans silently observe the agent's interactions, with their reactions sensed through electrodes on the scalp. This implicit feedback, such as error-related event potentials, is used to improve the agent's learning in RL tasks, specifically in an Atari-type environment. The study explores using error-related event potentials to enhance training in Deep Reinforcement Learning (DRL) by capturing human observer signals through an EEG cap and using them as an auxiliary reward function for a DRL algorithm in Atari-type games. The study demonstrates the generalizability of error-potentials across different environments in Deep Reinforcement Learning (DRL). Two frameworks are proposed to integrate recent DRL advances with error-potential feedback efficiently. The approach is scaled to complex environments, showing its significance through experiments with synthetic and real users. Deep Reinforcement Learning (DRL) algorithms have achieved remarkable feats, but are complex and time-intensive to train, especially in environments with sparse rewards. Human participation can accelerate training and reduce costs without compromising performance, inspiring research efforts to incorporate human feedback. In this paper, an alternative paradigm is proposed to enhance reward functions in Deep Reinforcement Learning (DRL) without burdening humans. The use of electroencephalogram (EEG) brain waves to generate reward functions for DRL algorithms is explored, inspired by human error-processing systems. This approach aims to leverage the natural activity of the human brain to inform agents about sub-optimality without placing a heavy burden on humans. The feasibility of capturing error-potentials through EEG to enhance reward functions in Deep Reinforcement Learning is demonstrated. Utilizing human brain signals as auxiliary rewards can speed up training convergence, but challenges such as time-intensive tasks, noisy signals, and stochasticity in inferring error-potentials exist. The definition of ErrPs is generalizable across different environments, allowing for transferability without re-learning. Two frameworks combine DRL with human feedback (ErrP) efficiently, reducing the need for human supervision in training DRL systems. Active Learning methods are relied upon for this purpose. Our work combines DRL with human feedback efficiently, using Active Learning methods to accelerate learning and improve sample efficiency. The first framework involves humans providing implicit feedback during training, while the second framework allows for implicit feedback before training begins. Quality (Q) function is learned from human feedback to provide supplementary rewards to the RL agent. Results from real ErrP experiments demonstrate the generalizability of error-potentials across various Atari-like environments. The study combines Deep Reinforcement Learning (DRL) with error-potentials (ErrP) to estimate implicit human feedback in new environments. Two frameworks are proposed to integrate DRL advancements with ErrP feedback efficiently. The first framework involves humans providing feedback during training, while the second framework obtains feedback before training. The approach is scaled to complex environments and validated through synthetic and real user experiments. Previous studies focused on RL from human rankings or ratings, relying on explicit feedback. In this work, implicit feedback from non-expert humans via ErrPs is utilized, which is inherently noisy. Previous studies have shown the benefits of ErrPs in simple settings, using ErrP-based feedback as the sole reward. Unlike previous works, the ErrP decoder in this study is trained on labeled examples from simple environments to improve training efficiency. In this work, labeled ErrPs examples from simple environments are used to train the ErrP decoder for complex environments. The study combines recent advances in DRL with MDP problems and model-free RL methods to learn the Q-function. The transition tuple (x, a, r, x) represents consecutive experiences under behavior policy \u03c0. Modern DRL techniques like DQN and target networks are utilized. Human intrinsic reactions to agent behavior are sensed through electrodes on the scalp, monitoring event-related electric potentials. Riemannian Geometry framework is used for classifying error-related potentials as binary variables. Implicit human feedback accelerates training of DRL algorithms. The evaluation result of a method to accelerate DRL algorithms by adding a negative penalty when ErrP is detected is discussed. While obtaining implicit human feedback through EEG is less burdensome, it still poses challenges due to noisy signals and stochasticity. Three practical approaches to integrating ErrP with DRL advancements are presented. Recent advances in DRL involve learning ErrPs for specific games and using them in different games without re-learning. Two frameworks combine DRL advancements with human feedback to speed up agent learning efficiently. One framework allows implicit feedback during training without prior game knowledge, while the other uses human feedback before training by learning a reward function from initial trajectories with ErrP labels. Non-expert demonstrations can improve Q function generalization in state-space. The study explores error-potentials in EEG signals under different paradigms of human-machine interaction tasks, showing similarities in shape and timing across various scenarios. This prompts an investigation into the consistency of error-potentials in different environments, such as games. In Fig.5, grand average waveforms across three game environments (Maze, Catch, Wobble) show consistent error-potentials. Experimental evaluation confirms generalizability across environments, informing deep reinforcement learning algorithms. Active Learning frameworks optimize learning with minimal labeled examples. Acquisition function efficiently guides AL processes. In active learning, an acquisition function is used to select data points for labeling. A framework for training RL agents with non-expert human feedback is introduced, utilizing recent advances in active learning methods. The framework uses an uncertainty-based acquisition function to select state-action pairs for human labeling. The Deep-Q-Network is modeled using Bayesian learning methods for uncertainty estimation. The framework presented utilizes active learning methods to train RL agents with non-expert human feedback. It employs an uncertainty-based acquisition function to select state-action pairs for labeling, reducing the number of inquiries significantly. The negativity of state-action pairs is queried from stored ErrP labels, adding a negative penalty to the environmental reward. This approach aims to prevent the agent from deviating from optimal paths in the game. The framework utilizes active learning methods for training RL agents with human feedback. It selects state-action pairs for labeling based on uncertainty, reducing inquiries. RL algorithms with sparse rewards require heavy exploration initially. Imitation learning followed by RL fine-tuning improves sample efficiency. The framework utilizes active learning methods for training RL agents with human feedback, selecting state-action pairs for labeling based on uncertainty. The RL agent plays the game until the end of the episode, updating the DQN Q with experiences from the replay buffer. An alternative reward is derived from a learned quality function, augmenting the RL algorithm. This approach differs from the first framework by making queries for ErrP labeling on trajectories initially given in the demonstration. The proposed framework utilizes active learning methods for training RL agents with human feedback, selecting state-action pairs for labeling based on uncertainty. The human subject provides implicit feedback via ErrP on state-action pairs along trajectories, labeling each pair as positive or negative. The framework then learns the reward function using maximum entropy RL methods based on the decoded ErrP labels and initially given trajectories. The framework uses maximum entropy RL methods to learn the reward function from imperfect demonstrations, improving generalization in the state-space. Three navigation games in OpenAI Gym are developed, including Wobble, Catch, and Maze. The games are based on the Atari framework with default dimensions. Wobble is a 1-D cursor-target game with 20 blocks on a horizontal plane. The cursor starts in the center, and the target is within three blocks. The agent can move left or right to reach the target. Catch is a simplified version of Eggomania 2, with an egg falling in a 10x10 grid space. The agent can choose to move left, right, or do nothing. Maze is a 2-D navigational game. The maze in the 2-D navigational game consists of a 10x10 grid where the agent and target move within blocks. An experimental protocol involves a machine agent playing the game while human reactions are captured through EEG electrodes placed on the scalp. The study involved recording human EEG data during a 2-D navigational game using OpenViBE software. Five human subjects participated in three separate sessions, with each session lasting less than 15 minutes. The agent took action every 1.5 seconds, and research protocols were approved by the Institutional Review Board. The study involved recording EEG data during a 2-D navigational game with five human subjects. The method discussed in section 3 is used to augment RL algorithms with ErrP labels, showing significant improvement in training convergence. The \"No ErrP\" method refers to regular RL algorithms without human feedback, with success rate defined as the ratio of successful plays in previous episodes. Training completes when the success rate reaches 1. In the evaluations, the Q network is modeled using Bayesian deep learning methods. Three approaches are evaluated for integrating DRL with human feedback via ErrPs. ErrP signals are decoded using cross-validation for each game. Maze game shows the highest AUC score, followed by Catch and Wobble. The study evaluates the generalization capability of error potential signals and the decoding algorithm by training on Catch and testing on Maze game. Catch captures over 80% of the variability in ErrPs for Maze. The AUC score of generalizability performance is presented, showing a 2.25x improvement in training time. Preliminary experiments show insights into generalizability across all three games with different action spaces. The study evaluates the generalization capability of error potential signals by training on Wobble and testing on Catch game. AUC scores of 0.7359 and 0.6423 were obtained for different agent actions. Similarly, for the Catch game, AUC scores of 0.71 and 0.84 were found for different egg positions. The difference in mean was statistically significant for both tests. The study evaluates the generalization capability of error potential signals by training on Wobble and testing on Catch game, with statistically significant results. In the active RL framework, three acquisition functions are explored: entropy, mutual information, and confidence interval. Benchmark performance of full access method is shown, along with evaluation of the first framework on Maze game using real ErrP experimental data. Bayesian DQN is used for the Q network, showing that the first framework can achieve similar performance with less feedback inquiries compared to full access method. The second framework, evaluated in Maze game with Bayesian DQN, shows improved performance with fewer feedback inquiries compared to the full access method. The trajectories are initially randomly generated and corrupted by wrong actions, leading to 372.1(\u00b158.2) ErrP inquiries. The reward function speeds up training convergence significantly. The second framework outperforms the full access method in simulations, requiring only 20 trajectories for ErrP inquiries. It relies on human input for specifying initial trajectories. Error-potentials of a human observer watching an agent play Atari games are used as an auxiliary reward function for a DRL algorithm. The augmentation effect of ErrP labels on RL algorithms is validated, and two augmentation frameworks for RL agents are proposed for different situations. The future work includes integrating human feedback through error-potentials in RL agents, testing generalization capabilities in various environments, and exploring EEG-based cooperation between humans and machines. The EEG signals were filtered using xDAWN spatial filtering to project them from sensor space to source space for feature extraction and classification using a regularized regression model. Dimensionality reduction was achieved through relevant channel selection and backward elimination. The EEG signals were filtered using xDAWN spatial filtering for feature extraction and classification with a regularized regression model. A threshold value was selected for decision-making by maximizing accuracy on the training set. The algorithm for decoding ErrP signals involves preprocessing, spatial filtering, electrode selection, tangent space projection, regression, and decision threshold selection. Additionally, two DQN models were introduced, including a Bayesian DQN model with a linear Q-function approximation. The Bayesian DQN model utilizes the DQN architecture and Gaussian distributions to calculate the posterior of \u03c9 a. It constructs a replay buffer D a and calculates the posterior of Q value as a Gaussian distribution. Another Bayesian DQN model, the bootstrapped DQN, uses a bootstrapped neural network to approximate a posterior sample for the value. This model is implemented with bootstrapping and neural networks instead of linear value functions and Gaussian sampling. In this work, three acquisition functions are explored in selecting trajectories for ErrP labeling. The trajectory set is denoted as D, the learned Q network as Q, and the acquisition function as a(\u03c4, Q). The selected trajectory maximizes the acquisition function given Q, with three acquisition functions being explored: Entropy, which selects the trajectory with the maximum entropy, measuring uncertainty along the trajectory. The reward learning is modeled as a probabilistic maximum due to noisy ErrP feedback. In this work, reward learning is modeled as a probabilistic maximum entropy RL problem. The Q function is trained using positive and negative state-action pairs from demonstrations. A baseline function t(s) is introduced to refine reward shape and reduce learning update variance. The Q function Q B (s, a) := Q(s, a) - t(s) is shown to induce the same optimal policy as Q(\u00b7, \u00b7). The baseline function t*(\u00b7) can be learned by optimizing t*. The baseline function t*(\u00b7) is learned by optimizing t* to minimize J(t). The loss function l(\u00b7) is chosen to be l 1 -norm through empirical evaluations. Another set of demonstrations D R is incorporated to help learn the state dynamics efficiently. After reward learning, the reward function Q B (s, a) is represented as Q B (s, a) - \u03b3 max a \u2208A Q B (s', a). This reward function is used to augment the RL agent in the Box World game environment. The Box World game environment features an 8x8 room with keys and boxes scattered randomly. An agent, represented by a black pixel, can move in four directions. Keys and boxes are represented by colored pixels, with keys matching box locks. The game utilizes synthetic human feedback with noisy labels on state-action pairs. The Q network is implemented using bootstrapped DQN for this complex environment. The Box World game environment uses bootstrapped DQN for a complex environment. The simulation results show the performance of RL algorithms with and without human feedback. Three acquisition functions are compared, with mutual information showing similar performance to full access method in convergence speed with fewer inquiries. Confidence interval performs the worst due to not considering model properties, while mutual information outperforms entropy but requires more human feedback inquiries."
}