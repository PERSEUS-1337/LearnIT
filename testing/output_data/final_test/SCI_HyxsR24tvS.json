{
    "title": "HyxsR24tvS",
    "content": "To address mode collapse in GANs, a new training method is proposed where fake samples can be treated as real ones to reduce gradient exploding. The equilibrium between generators and discriminators is rarely achieved in practice, leading to unbalanced generated distributions. By penalizing differences in discriminator outputs and treating certain fake samples as real, gradient exploding can be mitigated for a more stable training process and better generalization. Generative Adversarial Networks (GANs) have been popular for generating high-quality images. They involve a zero-sum game between a generator and discriminator network. A new training method is proposed to stabilize the process and improve generalization by treating fake samples as real to reduce gradient exploding. Numerous researchers have tried to improve the performance of GANs due to issues like unstability and mode collapse. Theoretical equilibrium is often not achieved with discrete finite samples during training. The original GAN objective lacks a theoretical generalization guarantee, leading to poor generation capacity in GANs. Poor generation capacity in GANs is attributed to discriminators overfitting to real data samples and gradient exploding. Thanh-Tung et al. proposed a zero-centered gradient penalty to address this issue. Recent work by Wu et al. focused on privacy protection. This paper addresses mode collapse resulting from gradient exploding, achieving better generalization and stable training. The contributions include removing the sigmoid function in the discriminator's last layer and improving generalization capability. During training, achieving a theoretical equilibrium where the discriminator outputs a constant for both real and generated data is unachievable. This can lead to gradient exploding when fake data points approach real ones, causing an unbalanced generated distribution. Penalizing the difference between discriminator outputs and treating fake as real can help prevent overfitting to certain real data points. The text discusses a novel GAN training method that prevents mode collapse by treating some fake samples as real ones based on discriminator outputs. This approach stabilizes the training process and improves the generated distribution. The method aims to address overfitting and unbalanced distributions in GANs. Various methods have been proposed to improve the stability of GAN training, including well-designed structures and better objectives. Gradient penalty to enforce Lipschitz continuity is also a popular direction. Theoretical studies have shown local stability and convergence for GAN optimization. Mode collapse is a persistent problem in GAN training, leading to a lack of diversity in generated samples. Recent studies have shown that GANs may miss significant modes in the data distribution. Various approaches, such as using multiple generators and mixed samples, have been proposed to address mode collapse. Recent studies have addressed mode collapse in GAN training by exploring diverse approaches such as using multiple generators and mixed samples. Hoang et al. (2018) and Lucas et al. (2018) focus on conveying diversity information, while He et al. (2019) and Yamaguchi & Koyama (2019) study mode collapse from probabilistic treatment and entropy of distribution perspectives. Goodfellow et al. (2014) showed that with sufficient capacity, a global equilibrium is reached when the real and generated distributions match, leading to optimal discriminator output. Theoretical explanations for mode collapse in GAN training involve gradient exploding in the discriminator, leading to multiple fake datapoints being moved towards real datapoints, resulting in mode collapse. This phenomenon occurs when the absolute value of the directional derivative of the discriminator approaches infinity, causing gradient exploding. During GAN training, mode collapse can occur when the discriminator easily distinguishes between real and fake samples, preventing a global equilibrium. The unknown distribution of real data causes the discriminator to always classify real samples as real and generated samples as fake. This leads to an unbalanced distribution and pushes the generated distribution towards real samples instead of the target distribution. During GAN training, an unbalanced distribution can deviate from the target distribution, affecting the discriminator's output. The optimal discriminator in original GAN is not constant unless specific conditions are met, leading to a theoretical equilibrium that may not be achievable until all samples are covered uniformly. This can result in real data points having higher discriminator outputs than generated ones. During GAN training, an unbalanced distribution can lead to gradient exploding and overfitting, causing mode collapse. The discriminator's output may be higher for real data points than generated ones. Stabilizing the discriminator's output is crucial to achieve a faithful generated distribution. During GAN training, an unbalanced distribution can lead to gradient exploding and overfitting, causing mode collapse. The discriminator's output may be higher for real data points than generated ones. Stabilizing the discriminator's output is crucial to achieve a faithful generated distribution. In a simplified scenario, when a fake datapoint is close to a real datapoint, the generator updates the fake datapoint according to the gradient received from the discriminator. If the fake datapoint is very close to the real one, the norm of the gradient generator receives tends to explode, leading to multiple fake datapoints. During GAN training, stabilizing the discriminator's output is crucial to prevent mode collapse. In a scenario with real and generated datapoints, the discriminator's output influences the target value. When the discriminator's output for fake datapoints increases and the difference between real and fake outputs decreases, the target value decreases. Conversely, when the discriminator's output for fake datapoints decreases and the difference between real and fake outputs increases, the target value increases. In a more general scenario, the optimal discriminator output for real datapoints and nearby generated datapoints is of interest. During GAN training, stabilizing the discriminator's output is crucial to prevent mode collapse. An extra constraint is needed to alleviate the difference between discriminator outputs on real and fake datapoints. Penalizing the difference between discriminator outputs on close real and fake pairs can reduce the norm of \u2207 y i L G (y i), making it possible to move fake datapoints to other real datapoints instead of being trapped at x 0. During GAN training, stabilizing the discriminator's output is crucial to prevent mode collapse. To alleviate the difference between discriminator outputs on real and fake datapoints, a zero-centered gradient penalty can be enforced to stabilize the discriminator output. This penalty helps in moving fake datapoints to other real datapoints instead of being trapped at x 0. During GAN training, a zero-centered gradient penalty can stabilize the discriminator's output and prevent mode collapse by filling the gap between real and fake datapoints. This penalty makes it harder for the discriminator to distinguish between real and fake data, preventing overfitting. The method proposed by Thanh-Tung et al. (2019) using linear interpolation may not efficiently fill this gap. The author discusses how linear interpolation on latent codes may not effectively fill the gap between real and fake datapoints in GAN training. The probability of gradient exploding is low when a fake datapoint approaches a real one, making it hard to fill the gap. As more fake datapoints move towards a real datapoint, the gradient for fake datapoints increases, leading to instability in the training process. During GAN training, similar generated samples are observed in later stages, indicating instability. To address this, the objective is to reduce the gradient magnitude for optimal discriminator by treating some fake as real data. This involves considering fake data as real with a certain probability during training, aiming to improve generalization. During GAN training, to reduce gradient magnitude and improve generalization, fake data can be considered as real with increasing probability. This helps prevent overfitting to single data points and allows fake samples to be moved to other real data points more easily. During GAN training, fake samples can be trained as real samples in the discriminator to prevent overfitting. Generated samples with low discriminator output are chosen to replace real samples, reducing large gradients. A zero-centered gradient penalty is also added on real samples. During GAN training, a zero-centered gradient penalty is added on real datapoints to prevent overfitting. The penalty is enforced on a mixture of real and fake datapoints, with the goal of reducing large gradients. Sampling more generated datapoints for training as real ones can increase the probability of finding overfitting regions with large gradients. Increasing the number of fake datapoints considered as real for training can slow down the speed of covering real samples at the beginning. Our method aims to stabilize discriminator output and prevent mode collapse efficiently during GAN training. By using a finite dataset with real samples from Gaussian distributions, we tested the effectiveness of our approach in preventing an unbalanced distribution caused by overfitting. The weight \u03bb for gradient penalties in all GANs was set to 10, with a training batch size of 64 and a quarter of the real training batch consisting of generated samples. Our method prevents mode collapse and stabilizes discriminator output during GAN training. Results show that our approach generates better samples with good generalization compared to other GAN methods. Additionally, our method can gradually cover different modes in a dataset with multiple Gaussian distributions, alleviating gradient exploding issues. Our method prevents mode collapse and stabilizes discriminator output during GAN training, generating better samples with good generalization. It can cover different modes in a dataset with multiple Gaussian distributions, alleviating gradient exploding issues. The method moves fake datapoints to other Gaussian modes when attraction to other modes is larger than to overfitted datapoints, achieving a faithful distribution even when samples in high dimensional space are far apart. Comparison with GAN-0GP-sample on CIFAR-10, CIFAR-100, and ImageNet datasets using ResNet-architectures shows improved generation with higher inception score and lower FID value. The study uses different optimization techniques for CIFAR and ImageNet experiments, achieving higher Inception score and lower FID value. The results show that their method outperforms GAN-0GP-sample by a large margin. Our method outperforms GAN-0GP-sample by a large margin in terms of covering more modes and having a better balanced generation. The losses of discriminator and generator during CIFAR-10 training show that our method has a much more stable training process compared to GAN-0GP-sample. Our method stabilizes discriminator output on real and fake datapoints, preventing overfitting. It outperforms GAN-0GP-sample on ImageNet, producing high-quality samples without using category labels. Our method stabilizes the training process of GANs by considering some fake samples as real ones based on discriminator outputs. Experiments on diverse datasets confirm the effectiveness of our approach in improving training stability. Our method stabilizes GAN training by improving performance on diverse datasets. The discriminator maximizes an objective to distinguish between real and fake samples, with an optimal discriminator having fewer parameters than previously thought. The discriminator in this case has fewer parameters than O(d x (m + n)), with a real datapoint x 0 and a generated datapoint y 0 satisfying specific conditions. The discriminator is constructed as a MLP with two hidden layers and outputs are computed using a sigmoid function. The discriminator objective also has a more optimal value than the theoretical optimal. The discriminator objective in this case has a more optimal value than the theoretical optimal version. To achieve this optimal value, certain equations are rewritten and solved, leading to the conclusion that certain variables increase with a specific parameter increasing. The text discusses the increase of certain variables with a specific parameter increasing in the context of GAN experiments using Pytorch."
}