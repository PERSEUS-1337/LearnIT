{
    "title": "Bke0rjR5F7",
    "content": "In this paper, the authors propose modifying linear penalties to second-order ones for more practical training in non-convex, large-data settings. This approach allows for training with a fixed penalty coefficient, avoiding instability and lack of convergence. They also develop a method for efficiently computing gradients with second-order penalties in stochastic mini-batch settings, resulting in a well-performing algorithm empirically. Machine learning systems are increasingly used in real-world applications, impacting decisions like parole conditions, loan applications, and airport screening. Biases in these models can unfairly disadvantage certain groups, such as gender-specific biases in word embeddings and unfair predictions in convict recidivism. Previous work has shown that ignoring protected traits during training does not alleviate these biases. Previous work has shown that enforcing blindness to protected traits during training is ineffective due to inherent biases in the data, leading to models that perpetuate undesirable biases. Research on machine learning fairness is of great interest, with precise notions like demographic parity and equal opportunity being expressed mathematically as constraints on machine learning models. Recent works propose incorporating fairness during training through constrained optimization. In this paper, a general method for imposing fairness conditions during training in non-convex, large data settings is presented. The success of deep neural networks highlights the need for methods that are practical in such settings, as existing work often focuses on convex, small data-set scenarios or requires complex training methods. In non-convex, large data settings, a method is proposed to optimize the dual objective with respect to model parameters and penalty coefficients concurrently. Linear penalties for fairness criteria are re-expressed as second-order penalties, allowing for easier optimization with a fixed penalty coefficient \u03bb. This approach simplifies optimization in potentially non-convergent scenarios, avoiding the need for complex methods. In non-convex, large data settings, a method is proposed to optimize the dual objective with respect to model parameters and penalty coefficients concurrently. Linear penalties for fairness criteria are re-expressed as second-order penalties, simplifying optimization with a fixed penalty coefficient. This approach improves generalization performance and is easily optimized in large-data settings. The algorithm optimizes constraints like feature orthonormality and predictive fairness in various settings. It defines a machine learning model as a function in a set of real-valued functions. Fairness criteria are re-expressed as second-order penalties for optimization. The algorithm optimizes constraints for feature orthonormality and predictive fairness in machine learning models. Fairness criteria are expressed as linear constraints on the probability function d(x), with popular notions like demographic parity. The algorithm optimizes constraints for feature orthonormality and predictive fairness in machine learning models. Fairness criteria are expressed as linear constraints on the probability function d(x), with notions like equal opportunity, equalized odds, and disparate impact. The algorithm optimizes constraints for feature orthonormality and predictive fairness in machine learning models by approximating linear constraints as quadratic constraints. It introduces optimization based on second-order penalties to address issues associated with linear constraints. If the loss function is convex, solving the dual problem leads to the optimal solution, but in the non-convex case, a saddle point may not exist. When optimizing jointly over model parameters and multipliers, the Lagrangian may not converge to a saddle point, leading to randomized classifiers. This requires sophisticated procedures with many parameters and hyperparameters, making training difficult. The Lagrangian approach may lead to randomized classifiers due to difficulties in convergence, requiring complex procedures with numerous parameters and hyperparameters for training. To address this, an objective function is proposed with a hyperparameter \u03bb to balance fairness and accuracy tradeoff, allowing for optimization with constraints. The penalty coefficient \u03bb is a fixed hyperparameter that can be optimized using standard methods for a desired fairness-accuracy trade-off. Second-order penalties serve as a proxy for fairness metrics, while methods like Lagrangian with linear penalties require relaxation of constraints and hyperparameter tuning. The alternative way of solving for fairness-constrained classifiers through hyperparameter tuning overcomes drawbacks associated with Lagrangian methods. Tuning \u03bb directly for desired fairness metrics decouples difficulty of hyperparameter tuning in Lagrangian approaches, leading to better generalization performance and less overfitting. Introduction of second-order penalties may complicate stochastic optimization of the objective. The introduction of second-order penalties in stochastic optimization may complicate the objective. Despite this obstacle, it is possible to express the second-order penalty as an expectation of individual loss functions over pairs of data points sampled from the dataset. This derivation is crucial for modern machine learning applications with large datasets optimized using stochastic mini-batches. The second-order penalty is expressed as a double integral. The introduction of second-order penalties in stochastic optimization may complicate the objective. Despite this obstacle, the sum of double integrals can be expressed as a double integral of a sum. The gradients of this double integral can be approximated via Monte Carlo estimation using two independent samples. Algorithm 1 outlines a fairness-aware training algorithm for a machine learning model parameterized as d \u03b8. The training loss is an expectation of individual loss functions, and the algorithm performs stochastic gradient descent with fixed penalty coefficient \u03bb by sampling batches. Our work introduces second-order penalties in a general setting for fairness in machine learning. The optimal choice of \u03bb is determined through hyperparameter tuning. Unlike previous works in the convex setting, we propose a deterministic classifier by avoiding the two-player game formulation in the non-convex case. This approach allows us to train for fairness without the need for a randomized distribution over classifiers. Our approach introduces second-order penalties for fairness in machine learning, applicable to highly non-convex models. Unlike previous works, we avoid the two-player game formulation and convergence issues by using a fixed penalty coefficient in training. This method is distinct from adversarial training and allows for optimization of penalties stochastically. The proposed quadratic penalties for fairness in machine learning are similar to previous notions of orthogonality. The penalties penalize the Frobenius norm of the Gram matrix of model outputs and can be optimized stochastically. This approach avoids two-player game formulation and convergence issues, allowing for efficient optimization of penalties. The text discusses the use of quadratic penalties for unbiased minimization of the Bellman error in machine learning models. The penalties encourage orthogonality in learned features, as demonstrated on the Iris dataset with a simple model. Results show that as the penalty weight increases, the features become decorrelated. The technique is then applied to a convolutional autoencoder on MNIST images. The text discusses using quadratic penalties to encourage orthogonality in a convolutional autoencoder model applied to MNIST images. By increasing the penalty coefficient, feature correlation decreases while maintaining reconstruction error. The dataset includes examples for predicting income levels and bank product subscriptions, with fairness constraints based on protected groups like gender and age. The model uses linear regression and demographic parity for fairness. The dataset includes examples for predicting income levels and bank product subscriptions, with fairness constraints based on protected groups like gender and age. The 5 fairness constraints are demographic parity. The task is to predict whether a community has high or low crime rate based on race attributes. The ProPublicas COMPAS recidivism data is used to predict recidivism based on various factors. The dataset includes examples for predicting income levels and bank product subscriptions, with fairness constraints based on protected groups like gender and age. The 4 fairness constraints for predicting recidivism are based on race attributes, using a 2-layer Neural Network with ReLU activations. Two methods, Deterministic Lagrangian and Stochastic Lagrangian, are used to ensure accuracy/fairness trade-off in the predictions. The Stochastic Lagrangian method returns a stochastic solution to a two-player game with the Lagrangian as pay-off function, approximating a Nash equilibrium. Hyperparameters are optimized through a grid search, selecting the model with the highest accuracy on the validation set. Final evaluation is done on an unseen test set, with hyperparameter search for Lagrangian baselines focusing on fixed learning rate. The method aims to achieve a balance between accuracy and constraints. Our method achieves the best testing error and fairness constraints compared to Lagrangian baselines, with lower constraint violations and almost no trade-off in accuracy. It also has the lowest overall false positive rates across the dataset. Our method achieves the best testing error and fairness constraints compared to Lagrangian baselines, with lower constraint violations and almost no trade-off in accuracy. It also has the lowest overall false positive rates across the dataset. The method is able to learn a classifier closer to satisfying fairness constraints while maintaining a reasonable level of accuracy. Pareto curves in Figure 3 illustrate the trade-off between testing error and constraint violation for our method on two datasets."
}