{
    "title": "SJxsV2R5FQ",
    "content": "The representation for describing transition models in complex uncertain domains using relational rules is demonstrated to be more versatile and sample efficient than learning a monolithic transition model. An iterative greedy algorithm constructs deictic references to determine relevant objects, and feed-forward neural networks learn the transition distribution on these objects' properties. This strategy is showcased in a simulated domain where a robot pushes stacks of objects on a cluttered table. In this paper, a strategy for learning state-transition models in complex systems is presented. The model is structured in terms of rules that depend on a small number of objects in the domain. The actions taken have localized effects on the objects being operated on and related objects, without affecting the majority of other objects. The focus is on learning the kernel of a rule, which depends on and affects a set of objects. Actions by intentional systems are directly parametrized by objects being operated on. Deictic reference is used to determine which objects are relevant for prediction or likely to be affected. Deictic reference is a way of naming an object in relation to the current situation. It was introduced as a representation strategy for AI systems by BID0. This style of reference helps describe policies for a video-game agent and characterizes relevant objects for each rule. It simplifies describing transition models on variable-size domains by using fixed-length vectors. The text describes a transition model using fixed-length vectors to represent objects and their relations, learned through neural networks. It introduces an algorithm for learning sparse relational transition models with deictic references and neural network parameters. The approach combines rule learning with flexible neural network models, demonstrated in a robot-manipulation domain. The text discusses learning rules in noisy domains using Boolean combinations of binary input features to predict action effects. The approach involves schema networks and lifted, probabilistic rules that can be applied generally. It is inspired by previous work on deictic expressions and object-oriented reinforcement learning. Our representation and learning algorithm improves on previous strategies by using feed-forward neural networks as a transition model for domains with real-valued properties. The EM-based learning algorithm creates a smoother optimization space, making learning faster and more robust. The approach does not construct new functional terms during learning, leaving that for future work. The rule-based structure can be interpreted as a graph network, with objects as nodes and deictic functions as labeled directed hyper-edges. Our learning method focuses on determining which neighbors to condition on dynamically, based on edge labels. This dynamic graph network is suitable for modeling an agent's interactions in a complex environment with actions having local effects. The approach assumes a class of problems where most objects do not change state on any given step. The learning method focuses on dynamic graph networks for modeling agent interactions in a complex environment with local effects. The method assumes a class of problems where objects do not change state often. The problem domain is formalized with a new rule structure for probabilistic transition models and defining primitive actions for making changes in the world state. The problem domain is defined by a tuple D = (\u03a5, P, F, A) with a countably infinite universe of objects, properties, deictic reference functions, and action templates. Functions in F map objects based on properties in P, with an example function F i mapping objects within a certain distance. A consists of action templates parameterized by continuous parameters. In this work, problem instances are characterized by a domain D and a universe of objects U. States are defined by the values of properties on objects. The action space is constructed by applying action templates to object tuples and continuous parameters. Modeling uncertainty is crucial for robust behavior in many domains. The key to robust behavior lies in modeling uncertainty and creating plans that respect it. A sparse relational transition model (SPARE) for domain D defines a probability density function on resulting states when taking actions. This model is specified in terms of domain elements P, R, and F to apply to any problem instance. Transition rules and a score function are used to predict the distribution of the next state. The final prediction of SPARE is based on maximizing the score function. The final prediction of SPARE is based on maximizing the score function, where transition rules are learned from past experience with a loss function. Transition rules are characterized by action templates, deictic references, predictor, and default variances for each property. The final prediction of SPARE is based on maximizing the score function through transition rules characterized by action templates, deictic references, predictors, and default variances for each property. The action template operates on a tuple of object variables, using deictic references to find subsets of objects for prediction. Predictors are neural networks mapping fixed-length inputs. The predictors in SPARE are neural networks that map inputs to outputs, applied to objects in a relational graph. A \"de-aggregator\" function assigns predictions to designated objects based on their properties. Functions generate lists of objects affecting predictions and affected by actions. The model extends to cases of single or multiple objects. The function F in the domain determines objects in the object list by applying functions to them. References can only refer to named objects in the action. When a function returns a set of objects, an aggregator is needed to operate on the set. The function F in the domain determines objects by applying functions to them, with references to named objects in the action. Aggregators are used when a function returns a set of objects, providing a single value for each property P i \u2208 P aggregated over the set. The vector x contains action parameters \u03b1 for action A and property P i for selected objects O t. Aggregators are used for computing properties of objects in sets. The output construction involves adding objects gradually to form the output set \u00d4. The output vector y is a mean aggregator applied to properties of objects in sets. The predictor \u03c6 \u03b8, like a neural network, predicts the distribution for the output vector y based on input values x. Representing complex output distributions is challenging. In this work, Gaussian distributions are used to represent property values in y with mean and variance. A transition rule maps a state and action to a distribution over the new state. The rule applies if a is an instance of A and input/output object lists are not empty. Objects are assigned to variables in the action instance, and references are computed based on selected objects. The deictic reference F is applied to actual property values in the state. The transition rule applies if input/output object lists are not empty. Predictions are made on the mean and variance values of property P i of output objects. The predicted distribution of the resulting state is computed using default variance values. Multiple predicted distributions can appear for the same object in the output vector. When making predictions for output objects, a mixture of predicted distributions with uniform weights is used. If an element in the output list is a set, the same property distribution is predicted for all elements in the set. Transition rules are used to build a SPARE, with a score assigned based on the number of references in the rule. The learning problem involves finding a transition model that minimizes loss based on a set of experience tuples in a specific problem domain. Good generalization performance requires varying aspects across training instances. The learning algorithm is described in three parts. The learning algorithm involves three parts: learning \u03c6 \u03b8 to predict a Gaussian distribution on y given x, learning reference lists \u0393 and \u2206 for a single transition rule, and using an EM method for learning multiple rules. The predictor \u03c6 \u03b8 minimizes Eq. (2) by predicting both the mean \u00b5 \u03b8 (x) and the diagonal variance \u03a3 \u03b8 (x) using a neural network. The negative data-likelihood loss function is directly optimized for the set of experience tuples E T to which rule T applies. The learning algorithm involves optimizing the default variance of a rule by computing loss-minimizing values for predicted variances. A greedy procedure is used to construct input and output reference lists for a single transition rule in the domain. Our approach involves incrementally building up the input reference list \u0393 by adding tuples one at a time via a greedy selection procedure. The list is initialized to be empty and for each iteration, a new reference is selected based on minimizing a loss function. If the loss decreases, the reference is added to the list, otherwise, the process terminates. In the selection process, the input reference list \u0393 is incrementally built by adding tuples one at a time through a greedy procedure. The process terminates when i exceeds the maximum allowed number of input references, N \u0393. The algorithm simplifies by setting \u2206 = \u0393 and constructing the lists of deictic references using a single pass of the greedy algorithm. Training data in robotic manipulation tasks may require multiple rules due to different object relations in various states. In robotic manipulation tasks, training data may involve multiple rules for different object relations in various states. To learn K rules from a single experience set E, initial clustering separates samples into K clusters, followed by an EM-like approach to refine samples and learn rule parameters. Membership probabilities Z are learned to assign samples to transition rules, initialized via clustering and refined through EM. To learn K rules from a single experience set E in robotic manipulation tasks, a transition rule is first learned from E using a specific algorithm. The transition rule is then used to transform the experience set into x and y values, which are concatenated with the loss function values for each sample. K-means clustering is applied to these concatenated values to compute squared distances from each sample to cluster centers. Membership probabilities for each sample to each of the K transition rules are initialized based on these distances. An EM-like algorithm is introduced to improve sample assignment to transition rules and learn rule details. To learn K rules from a single experience set E in robotic manipulation tasks, a transition rule is first learned from E using a specific algorithm. The transition rule is then used to transform the experience set into x and y values, which are concatenated with the loss function values for each sample. K-means clustering is applied to these concatenated values to compute squared distances from each sample to cluster centers. Membership probabilities for each sample to each of the K transition rules are initialized based on these distances. An EM-like algorithm is introduced to improve sample assignment to transition rules and learn rule details. In the next step, details of the rules themselves are learned, and a minor modification to transition rules is made to obtain mixture rules. A mixture rule is defined as T = (A, \u03c0 \u0393 , \u03c0 \u2206 , \u03a6), where \u03c0 \u0393 represents a distribution over all possible lists of input references \u0393, of which there are a finite number. Each possible list of references \u0393 is referred to as a shell, and \u03c0 \u0393 is a distribution over possible shells. DISPLAYFORM2 is a collection of \u03ba transition rules, each with an associated predictor and default values. Predictions for a sample using a mixture rule are made by combining predictions from each of the mixture rule's \u03ba transition rules according to the probabilities defined by \u03c0 \u0393. The EM-like algorithm is used to learn K mixture rules in robotic manipulation tasks. Each mixture rule combines \u03ba transition rules based on probabilities assigned by \u03c0 \u0393 and \u03c0 \u2206. This approach allows for smoother sorting of experience samples into clusters corresponding to different rules. The algorithm initializes distributions \u03c0 \u0393 = \u03c0 \u2206 for each mixture rule and learns transition rules on weighted experience samples. Data likelihood loss function values are computed for multiple explored shells during the process. The algorithm initializes distributions for each mixture rule and learns transition rules on weighted experience samples. Data likelihood loss function values are computed for multiple explored shells, with weight redistribution based on a voting procedure among the top shells. The algorithm initializes distributions for each mixture rule and learns transition rules on weighted experience samples. Data likelihood loss function values are computed for multiple explored shells, with weight redistribution based on a voting procedure among the top shells. The predictor minimizes validation loss for each sample, assigning shell weights proportional to the sum of sample weights that voted for each shell. Membership probabilities are updated by scaling with data likelihoods from using each of the rules to make predictions. In the domain of predicting pushing stacks of blocks on a cluttered table top, the approach SPARE is applied. The object universe consists of blocks with different sizes and weights, with properties including shapes and positions. The action template push(\u03b1, o) is used to push towards a target object with specific parameters. The approach SPARE is applied in predicting pushing stacks of blocks on a cluttered table top. The 3D domain is simulated using PyBullet. Gaussian noise is added to action parameters to imitate real-world inaccuracies. Deictic references in the reference collection F are considered. The neural network function is compared in the study. The study compares a neural network function approximator and a fully connected graph neural network in predicting the next state of objects in a scene. The graph neural network consists of nodes representing objects, with bidirectional edges connecting them. Encoders, propagation networks, and a node decoder are used for message passing and prediction. The comparison is conducted in a simulated 3D domain with added noise to action parameters. In a problem scenario involving a gripper pushing a stack of blocks, 1250 instances were randomly sampled with stable block configurations. Training data was collected with state-action-next state tuples, where the target object for pushing was the bottom block. Deictic references were used to construct inputs and outputs, showing improved performance as references were added. Training performance is illustrated in FIG3. Performance improves noticeably with the addition of deictic references during the experiment. The default standard deviations decrease as references are added until leveling off after the third reference captures all moving objects in the scene. Sensitivity analysis compares the approach to baselines in terms of performance with varying numbers of objects in the problem. In the experiment, performance improves with deictic references. Adding extra blocks affects NN's performance, while SPARE outperforms graph NN by making good predictions for extra blocks. NN's performance drops as more objects are added to the table. Our SPARE approach outperforms NN by making accurate predictions for extra blocks on a cluttered table. The data likelihood is evaluated with varying training samples, showing improved performance with SPARE compared to NN. Our SPARE approach outperforms NN by accurately predicting extra blocks on a cluttered table. The approach benefits from more training samples and achieves good performance with only 500 samples. In a more general setting, our approach learns multiple transition rules for predicting the next state using an EM-like procedure. The EM approach concentrates on the 4-block case, showing average weight assigned to the \"target\" rule among samples of different stack heights. The SPARE approach outperforms NN by accurately predicting extra blocks on a cluttered table with only 500 samples. It learns multiple transition rules for predicting the next state using an EM-like procedure, showing the average weight assigned to the \"target\" rule among samples of different stack heights. The results demonstrate the power of combining relational abstraction with neural networks to learn probabilistic state transition models for domains with little training data. The structural nature of the learned models enables efficient planning methods for robotic actions. The purpose of obtaining a transition model for robotic actions is to enable planning to achieve high-level goals. The experiments in this paper used neural network predictors for mean and variance predictions. The predictors for the templates approach were trained for 1000 epochs each. The predictors for the templates approach and baseline NN predictor were trained for 1000 epochs each with a decaying learning rate. The GNN used node and edge encoders to map to latent spaces of 16 dimensions, with propagation networks consisting of 2 fully connected layers. The GNN was trained for 900 epochs with a decaying learning rate. States were parameterized by the (x, y, z) pose of objects in the scene, and action parameters included the starting pose of the robotic gripper and a \"push distance\". The robotic gripper's starting pose and a \"push distance\" parameter were included in the actions, with added noise for stochastic behavior. Clustering-based approaches were used to initialize membership probabilities, evaluating the clustering approach's performance on push dataset separation achieved by discrete clustering. TAB0 displays sample separation for stacks of varying height, showing the proportion of samples assigned to each cluster. The clusters were ordered by block sample size, with good separation observed. Some samples were misassigned to different clusters. Templates trained on the clusters reliably select deictic references considering the correct number of blocks. The proposed clustering-based approaches for initializing membership probabilities in cluster analysis show better sample separation results when initialized based on the square of the inverse distance to cluster centers compared to non-squared distances. The importance of the log data likelihood feature for the success of these approaches is highlighted. The importance of log data likelihood in clustering-based initialization approaches is crucial for better data separation. Scaling data likelihood can result in improved performance, suggesting the need to tune the relative importance of input features. Additionally, the order in which objects are presented in the baseline model can impact performance. In the interest of fairness, the target object always appears first in the ordering. Objects can be sorted by position (x-coordinate, y, z) for better predictions. Sample separation from clustering-based initialization shows probabilities assigned based on distance to cluster centers. Object ordering affects baseline performance in experiments. The effect of object ordering on baseline performance is tested by applying a push action to a stack of three blocks on a table. Different object orderings (random, xtheny, stack) are compared, showing that predicted log likelihood decreases as more extra blocks are added, with the ideal ordering performing the best."
}