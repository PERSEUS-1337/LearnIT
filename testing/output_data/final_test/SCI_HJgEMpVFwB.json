{
    "title": "HJgEMpVFwB",
    "content": "Deep reinforcement learning (RL) policies can be vulnerable to adversarial perturbations in observations. Adversarial policies in multi-agent environments can attack RL agents, even generating seemingly random behavior. These adversarial policies are successful in high-dimensional environments and induce different activations in victim policy networks. Recent research has shown that deep RL policies can be vulnerable to adversarial attacks, inducing different activations in victim policy networks. Adversarial examples for image classifiers have led to a new field of research into adversarial attacks and defenses. It is possible to attack a victim policy by building an adversarial policy that takes actions in a shared environment, inducing natural observations with adversarial effects on the victim. RL has been applied in various settings such as autonomous driving, negotiation, and automated trading. In domains like autonomous driving, negotiation, and automated trading, attackers cannot directly modify the victim's input. Adversarial opponents can induce natural observations with adversarial effects on the victim, making environments vulnerable to adversarial policies. Despite never standing up, the adversarial opponent wins 86% of episodes, far above the normal opponent's 47% win rate. Adversarial policies are more successful in higher-dimensional environments compared to lower-dimensional ones. Analyzing the victim's policy network activations reveals that adversarial policies induce different and widely dispersed activations. Fine-tuning the victim against the adversary provides temporary protection, but repeating the attack method exposes vulnerabilities to new adversaries. Repeated fine-tuning may offer defense against a range of adversaries. Our paper introduces a novel threat model for adversarial examples in RL, demonstrating the existence of adversarial policies in simulated robotics games. These policies outperform the victim despite minimal training, creating natural observations that disrupt the victim's policy network. The analysis shows that high-dimensional environments are more vulnerable to attacks, highlighting the importance of recognizing this threat model in deep RL deployments. Adversarial policies can uncover unexpected policy failure modes in benign settings. Adversarial training using adversarial policies could enhance robustness by training against undiscovered weaknesses. Previous studies focused on small perturbations to images causing mispredictions. Attackers can construct new images or find misclassified ones. The p model is seen as a local approximation. The p model is considered a local approximation for the true worst-case risk. Adversarial examples are intentionally designed inputs to cause machine learning models to make mistakes. Previous work in RL focused on p-norm threat models, showing vulnerabilities in deep RL policies to small perturbations in image observations. Our work introduces a physically realistic threat model that prevents direct modification of victim's observations, leading to unexpected policy failures against new opponents. Our defense strategy involves fine-tuning the victim against the adversary, inspired by prior work on adversarial training in image classification and deep RL policies. This approach follows a tradition of worst-case analysis in RL, where transition functions are chosen adversarially from an uncertainty set. Doyle et al. (1996) addressed the converse problem of finding optimal policies for a given set of transition functions. Various methods exist to verify the robustness of policies in robust MDPs. Our method can test policies in complex multi-agent tasks and scales with improvements in RL algorithms. We model the victim playing against an opponent in a two-player Markov game, where the opponent can be controlled by an adversary. The game involves transition functions for optimal policies and synthesizing controllers robust to adversarial disturbances. The game involves state sets, action sets, state transitions, and reward functions for two players. The adversary has access to the victim's actions but not internal information. The victim follows a fixed stochastic policy, typical in pre-trained models. In safety critical systems, models are validated and frozen to prevent new developments. In a two-player Markov game, the attacker aims to find an adversarial policy that maximizes rewards by solving a single-player MDP with the victim policy held fixed. The attacker must solve an RL problem as the victim policy is a black-box, making the MDP's dynamics unknown. The attacker aims to find adversarial policies in zero-sum simulated robotics games by solving an RL problem. They demonstrate the existence of adversarial policies and describe the attack method used in these environments. The victim policies were trained in pairs through self-play against random old versions of their opponent. The pre-trained policy weights released in the \"agent zoo\" of Bansal et al. (2018b) were used for the attacks. In asymmetric environments, agents labeled ZooVN and ZooON play two-player games in the MuJoCo robotics simulator. They observe body joints and opponent positions. Episodes end with a win or time limit, resulting in a draw. Environments from Bansal et al. (2018a) are evaluated, excluding Run to Goal. Sumo Ants task investigates dimensionality importance with 4 LSTM agents. Adversarial policies are trained to maximize Equation 1. In Section 3, an adversarial policy is trained using Proximal Policy Optimization (PPO) to maximize Equation 1. Sparse rewards are given at the end of episodes, with positive outcomes for the adversary winning and negative for losing or tying. Training is done for 20 million time steps using Stable Baselines's PPO implementation. Hyperparameters were selected through manual tuning and random search. The adversarial policies consistently outperform victim policies and the pre-trained Zoo baseline. The adversarial policies, trained using Proximal Policy Optimization (PPO), consistently outperform victim policies and the pre-trained Zoo baseline. The win rates against all victims are summarized in Figure 4, with qualitative evaluation showing that the adversarial policies beat the victim by not performing the intended task. Adversarial policies exploit weaknesses in the victim's policy to win, as demonstrated in videos at https://attackingrl.github.io/. In some scenarios, the adversarial policy induces poor actions from the victim's policy. In Sumo Humans, the adversarial policy adopts a high-level strategy to win by maintaining a stable position, which is effective against some victims but less so against others. The adversarial policies are effective against some victims but less so against others, achieving win rates of 62% and 45%. Testing against off-distribution baselines shows that victim policies are robust to typical observations, and the performance of adversarial policies cannot be solely explained by off-distribution observations. In this section, we focus on understanding why adversarial policies exist and how they manipulate victims through body position in high-dimensional environments. Adversarial policies differ in activations of the victim's policy network and are shown to reliably win against victims by inducing natural actions. Adversarial policies manipulate victims through body position in high-dimensional environments to win by inducing natural actions against victims. A 'masked' victim performs worse than a normal victim when playing normal opponents but fares better against an adversary. For example, in You Shall Not Pass, the normal opponent wins 78% of the time against the masked victim but only 47% against the normal victim. Conversely, the normal victim loses 86% of the time to the adversary. The study found that in high-dimensional environments, adversarial policies can manipulate victims to win by inducing natural actions. A 'masked' victim performs worse against normal opponents but fares better against an adversary. The non-transitive relationships between policies in games like You Shall Not Pass suggest caution is needed when assuming transitivity in methods like self-play. The study explores the vulnerability of RL policies to attacks in high-dimensional environments. It suggests a trade-off between observation space size and performance, with more observation making agents more vulnerable to adversaries. The hypothesis is that higher dimensionality of the observation space controlled by the adversary increases vulnerability to attacks, which is tested in the Sumo environment. The study tests the hypothesis that higher observation space dimensionality makes agents more vulnerable to attacks in the Sumo environment. Results show a lower win-rate for the adversary in low-dimensional Sumo Ants compared to high-dimensional Sumo Humans. The analysis involves recording activations from victim's policy network and using Gaussian Mixture Model (GMM) and t-SNE representation for better understanding adversarial observations. The study demonstrates that higher observation space dimensionality increases vulnerability to attacks in the Sumo environment. Adversarial policy induces activations with the lowest log-likelihood, while normal opponents induce activations with high likelihood. A t-SNE visualization shows clear separation between adversarial, random, and normal opponent activations. The ease of attacking policies emphasizes the importance of effective defenses. The study highlights the vulnerability of policies in the Sumo environment to attacks. Two training methods, single and dual training, are explored to defend against adversaries. Results show that fine-tuned victims are robust against adversary Adv1 but struggle against normal opponent ZooO1. ZooDV1 performs better than ZooSV1 but still faces challenges. This suggests that features used by ZooV1 may be easily manipulated. The study introduces a novel threat model of natural adversarial observations in zero-sum simulated robotics games. New adversaries AdvS1 and AdvD1 outperform previous adversary Adv1, demonstrating vulnerabilities in state-of-the-art victims trained via self-play. The study introduces a novel threat model of natural adversarial observations in zero-sum simulated robotics games. State-of-the-art victims trained via self-play are shown to be vulnerable to adversaries inducing off-distribution activations. Adversarial policies win by confusing the victim, not by learning a generally strong policy. This observation is significant as even apparently strong self-play policies can harbor serious vulnerabilities. The study demonstrates that strong self-play policies can have hidden vulnerabilities, as shown by an attack that lowers the victim policy's performance. The attack can succeed even against policies in a local Nash equilibrium. A simple defense of fine-tuning the victim against the adversary can be beaten, indicating the difficulty in eliminating adversarial policies. The study highlights the challenges in eliminating adversarial policies, with a defense that forces the adversary to trip up the victim instead of confusing them. Scaling up this defense through population-based training with continual addition of new agents is suggested for future work. Hyperparameters for training were determined through random search on two environments, Kick and Defend and Sumo Humans. The study determined hyperparameters for training on two environments, Kick and Defend and Sumo Humans. The total time steps were chosen based on diminishing returns. A mixture of in-house and cloud infrastructure was used for experiments, taking 8 hours to train an adversary for a single victim. Activations from all feed forward layers of the victim's policy network were collected and analyzed using Gaussian Mixture Model and t-SNE representation. The study determined hyperparameters for training on two environments, Kick and Defend and Sumo Humans. A Gaussian Mixture Model and t-SNE representation were used to analyze activations from the victim's policy network. Models with different perplexity values were fitted, with 20 components and a full covariance matrix achieving the best results. Supplementary figures are provided for further analysis. The study analyzed activations from the victim's policy network using a Gaussian Mixture Model and t-SNE representation. Models were fitted with a perplexity of 250 to activations from 5000 timesteps against individual opponents for clarity. Opponent Adv is the best adversary trained against the victim, while Opponent Zoo is either Zoo1 (Sumo) or ZooO1 (other environments). Results for other victims can be seen in Figure 8."
}