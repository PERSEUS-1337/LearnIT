{
    "title": "ryefmpEYPr",
    "content": "Deep neural networks have shown great success in knowledge management applications, but they are often complex with many trainable edges. Many of these edges are redundant and have little impact on network performance. The iSparse framework proposed in this paper can sparsify the network by 30-50% without affecting performance. It uses an edge significance score to determine edge importance and can be applied during training or on pre-trained models, with minimal computational overhead. Comparisons with other methods like PFEC, NISP, and DropConnect are also discussed. Deep neural networks, particularly convolutional neural networks, have been successful in various applications. However, their large number of trainable edges result in high computational costs. The iSparse framework offers effective network sparsifications with minimal computational overhead compared to other methods like PFEC and NISP. Deep neural networks have high computational costs due to their non-linearity and large number of trainable edges. Hardware innovations have helped but are reaching their limits, leading to interest in network-specific optimization techniques like compression, pruning, and regularization to reduce edges. However, these techniques often require retraining the pruned network, resulting in computational waste. Many successful networks often have redundant edges, leading to computational waste. Sparsification techniques can predict 95% of network parameters while only learning 5%. These techniques include neuron/kernel and edge/weight sparcification methods. Weight distribution and model accuracies for the MNIST dataset are shown in Figure 1. Network sparsification techniques, such as eliminating neurons with low l2-norms of weights or using neuron importance score propagation, can effectively reduce the size and complexity of DNNs and CNNs without significant loss in accuracy. However, it is important to note that edge weights alone cannot be used for pruning the network. The significance of edge weights in network pruning is crucial. A new iSparse framework is proposed to quantify the importance of each edge in the network, reducing redundancy by sparsifying insignificant weights. Experimental results show a 50% reduction in network size without affecting performance. The iSparse framework quantifies edge importance in network pruning, enabling retraining-free sparsification on pre-trained networks or during training. It achieves 30-50% sparsification with minimal impact on model accuracy. iSparse framework enables effective network sparsifications, outperforming PFEC, NISP, Retraining-Free, and DropConnect. Deep neural networks, especially CNNs, have shown significant success in various data analysis and machine learning applications. A typical CNN consists of feature extraction layers responsible for learning complex patterns, activation layers capturing non-linear patterns, and pooling layers for sampling data. The training process involves forward-propagation mapping input data to an output variable. The number of trainable parameters in deep networks can range from tens of thousands to hundreds of millions. Advancements in network regularization involve parameter pruning and regularization techniques to compress the network by eliminating redundant or insignificant parameters. Various methods such as pruning parameters with near-zero weights, filtering out convolutional kernels with minimum weight values, and minimizing changes in the final network have been proposed. Recent research has focused on minimizing changes in network performance by eliminating neurons with minimal impact using neuron importance scores. More complex approaches involve weight quantization, such as quantizing inputs and output activations using binary weights. Additionally, leveraging redundancy in the network through hashing functions has been proposed for compressing weights in low-level mobile hardware. In recent research, minimizing changes in network performance by eliminating neurons with minimal impact has been a focus. The proposed iSparse framework quantifies the significance of individual connections in the network to identify and remove insignificant parameters without updating edge weights or retraining the network. This approach aims to address the issue of large numbers of insignificant edges in deep neural networks. The iSparse framework aims to determine the significance of edges in a neural network to sparsify the network and eliminate redundant and insignificant edges. It involves generating binary mask matrices to identify and remove insignificant parameters without updating edge weights or retraining the network. The iSparse framework involves generating binary mask matrices to determine the significance of edges in a neural network for sparsification, without updating edge weights or retraining the network. The goal is to assign a non-negative real value to each edge, based on an edge significant score measure, to set the binary value of the mask matrix for sparsification. The iSparse framework focuses on generating binary mask matrices to assess edge significance in neural networks for sparsification, without altering edge weights or retraining the network. It emphasizes the importance of considering each edge's contribution to the network output, rather than solely relying on edge weights for sparsification. iSparse focuses on generating binary mask matrices to evaluate edge significance in neural networks for sparsification without changing edge weights or retraining the network. It considers each edge's contribution to the network output, using neuron significance scores to compute edge significance scores. The edge significance in neural networks for sparsification is determined by considering the neuron scores of the final output layer and using infinite feature selection. The significance scores of edges in a layer take into account the weights of the edges and downstream edges leading to the final output layer. The masking matrix in layer l depends on the lowest significance of the most significant edges, based on a target sparsification rate. The iSparse method sparsifies edges based on their significance scores, keeping only the highest scoring edges. The mask matrix is integrated into the layer to introduce informed sparsity by eliminating insignificant edges. This process does not propagate changes back to earlier layers in the network. The iSparse method integrates edge significance scores into the training process by updating mask matrices for trainable layers. The back-propagation rule is modified to exclude edges that do not contribute to the final model output. This approach introduces informed sparsity without propagating changes to earlier network layers. In the iSparse framework, error is masked as Err l * M l during back-propagation. Experimental evaluation was done using LeNet and VGG architectures, comparing against PFEC, NISP, and DropConnect approaches. iSparse was implemented in Python using Keras and TensorFlow. Experiments were conducted on Intel Xeon E5-2670 with Nvidia Tesla P100 GPU. LeNet-5 and VGG-16 were used as baseline architectures for sparsification performance evaluation on image classification datasets. LeNet-5 and VGG-16 are baseline architectures used for image classification datasets. LeNet-5 is a simple network with 5 trainable layers, while VGG-16 is a 16 layer network with interleaved max-pooling layers. VGG uses ReLU activation to learn complex patterns in real-world datasets like CIFAR10/20/100. In experiments, iSparse was compared against various network sparsification techniques like DropConnect, Retraining-free, and PFEC, using benchmark datasets such as CIFAR10/20/100, SVHN, GTSRB, and ImageNet. The number of trainable parameters for each model/data set pair is reported in Table 1. PFEC aims to eliminate neurons with low impact on model accuracy by computing l2-norms of neuron weights. iSparse is compared to other network sparsification techniques like DropConnect, Retraining-free, and PFEC. It computes l2-norms of neuron weights to eliminate low-impact neurons. NISP proposes a neuron importance score propagation technique. iSparse shows high robustness to sparsification levels in ImageNet dataset for VGG-16 network. iSparse demonstrates superior robustness to sparsification levels in the network, with only a \u2264 6% drop in accuracy for top-1 and \u2264 2% drop in accuracy for top-5 classification when sparsified by 50%. In comparison, competitors like Retrain-free experience larger accuracy drops, with \u223c 16% and \u223c 6% loss for top-1 and top-5 classification. Other models also suffer significant accuracy drops at lower sparsification levels, highlighting iSparse's effectiveness in maintaining accuracy. The iSparse algorithm demonstrates superior robustness to sparsification levels in the network compared to competitors like Retrain-free. iSparse can introduce significant additional sparsification with minimal impact on accuracy, unlike other techniques such as PFEC and NISP which sparsify input neurons or DropConnect which selects edges randomly. Figure 7 visually illustrates the differences in mask matrices created by the algorithms. The iSparse algorithm, unlike Retrain-free, selects edges in a fine-grained manner using edge significance measures. This approach results in significant accuracy improvements, as shown in Table 2. The algorithm demonstrates superior robustness to sparsification levels and outperforms alternatives like DropConnect. In this section, the performance of iSparse (iS) is compared against DropConnect (DC) and Retraining-Free (RF) using different activation functions and network optimizers. iSparse consistently provides the best classification accuracy across various configurations. The study also explores the impact of different sparsification orders on iSparse's performance. The performance of iSparse is compared against DropConnect and Retraining-Free using various configurations, with iSparse consistently providing the best classification accuracy. Different sparsification orders are explored, showing that iSparse is not sensitive to the order of sparsification. Edge sparsification rate directly impacts classification time, with iSparse allowing for 30-50% sparsification without major accuracy impact. The proposed iSparse approach for edge sparsification in deep neural networks (DNNs) shows potential for significant performance gains with minimal impact on classification accuracies. iSparse utilizes an output-informed framework and edge significance scores to achieve efficient computations in the sparsified space, leading to the least execution times compared to DropConnect and Retraining-Free methods. The iSparse framework minimizes network redundancy by sparsifying edges with low significance scores. Experiments show 30-50% network sparsification with minimal impact on classification accuracy. iSparse is robust to variations in network elements and offers a better accuracy/classification-time trade-off compared to competitors."
}