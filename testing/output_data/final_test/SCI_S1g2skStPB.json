{
    "title": "S1g2skStPB",
    "content": "Discovering causal structure among variables is a key problem in empirical sciences. Traditional score-based methods use heuristics to search for a Directed Acyclic Graph (DAG). However, these methods may not perform well with finite data. To address this, a new approach using Reinforcement Learning (RL) is proposed to search for the best scoring DAG. The model generates graph adjacency matrices from input data and incorporates penalty terms for acyclicity. Unlike typical RL applications, the goal here is to find the best DAG structure. The proposed approach uses Reinforcement Learning as a search strategy to find the best scoring Directed Acyclic Graph (DAG) from generated graphs. It allows for a flexible score function under the acyclicity constraint and has shown improved search ability in experiments on synthetic and real datasets. Causal discovery methods that infer causal relationships from passively observable data are attractive in fields where controlled randomized experiments are expensive or impossible. Causal discovery methods involve score-based approaches to find the best Directed Acyclic Graph (DAG) from a set of graphs. These methods face challenges due to the NP-hard nature of the problem, primarily caused by the acyclicity constraint. Existing solutions often rely on local search strategies to tackle this issue. Existing causal discovery methods face challenges due to the acyclicity constraint in finding the best Directed Acyclic Graph (DAG). Approaches like Greedy Equivalence Search (GES) enforce acyclicity one edge at a time, while hybrid methods combine constraint-based and score-based approaches. Zheng et al. (2018) introduced a smooth characterization for the acyclicity constraint, formulating the problem as a continuous optimization task. Existing causal discovery methods face challenges due to the acyclicity constraint in finding the best Directed Acyclic Graph (DAG). Approaches like Greedy Equivalence Search (GES) enforce acyclicity one edge at a time, while hybrid methods combine constraint-based and score-based approaches. Zheng et al. (2018) introduced a smooth characterization for the acyclicity constraint, formulating the problem as a continuous optimization task. Problem (1) can be formulated as a continuous optimization problem w.r.t. the weighted graph adjacency matrix by picking a proper loss function. Subsequent works have adopted different loss functions and used Neural Networks (NNs) to model causal relationships. Reinforcement Learning (RL) is proposed to search for the DAG with the best score according to a predefined score function. Reinforcement Learning (RL) is utilized to automatically search for the best Directed Acyclic Graph (DAG) based on a predefined score function. An encoder-decoder NN model generates directed graphs from observed data, with rewards calculated using the score function and penalty terms for acyclicity. Policy gradient and stochastic optimization methods train the NN weights to output the graph with the highest reward. Experimental results demonstrate improved search ability without compromising flexibility in selecting score functions. The proposed approach using BIC as a score function outperforms GES on linear non-Gaussian acyclic model and linear-Gaussian datasets. Constraint-based causal discovery methods involve conditional independence tests to find causal skeleton and determine edge orientations. Multiple testing problem arises with conflicting results that are difficult to handle. Causal discovery methods involve handling conflicts in testing results, with some methods being less robust due to errors in building the graph skeleton. Functional causal models can distinguish between different DAGs in the same equivalence class, such as LiNGAM and nonlinear additive noise model. Recent NN-based approaches to causal discovery include works by Yu et al. and Lachapelle et al. Recent NN-based approaches to causal discovery, such as causal generative NNs and adversarial causal generative models, have been proposed. Sequence-to-sequence learning advancements have led to the use of NNs for optimization in various domains, including solving the traveling salesman problem using pointer networks. Additionally, using reinforcement learning has been suggested to address combinatorial problems with simple reward mechanisms. Our work introduces a novel approach to causal discovery using a reinforcement learning paradigm with an encoder-decoder model for generating graph adjacency matrices. The reward function is tailored for causal discovery by incorporating a score function and an acyclicity constraint. Each variable in the data generating procedure is associated with a node in a directed acyclic graph. The model for causal discovery involves variables associated with nodes in a directed acyclic graph, where observed values are determined by parents in the graph and independent noise. Causal minimality is assumed, and the model can only be identified up to Markov equivalence class without further assumptions. Synthetic datasets are used in experiments to evaluate the estimated graph against the true DAG. The linear-Gaussian model studied in previous works can be extended to LiNGAM when noises are non-Gaussian. Causal discovery aims to infer the causal DAG from observed data X, sampled independently on an unknown DAG with fixed functions and noise distributions. The text discusses inferring the causal graph from observed data using neural networks. The approach involves designing a neural network-based graph generator to output a graph adjacency matrix based on the observed data. Previous attempts using feed-forward neural networks failed to capture causal relations effectively, leading to exploration of pointer networks for better results. In this work, the focus is on generating a binary adjacency matrix for an acyclic graph using encoder-decoder models. The encoder utilizes an attention-based structure to identify causal relations among variables. Future work may explore other attention-based models like the graph attention network. The decoder in this work generates a binary adjacency matrix by building relationships between encoder outputs. Different decoder choices include the neural tensor network model and the bilinear model. The Transformer structure is also an option for generating the adjacency matrix. The Transformer structure is used to generate an adjacency matrix in a row-wise manner. The single layer decoder performs the best, possibly due to fewer parameters and easier training for better DAGs. RL is proposed as a search strategy for finding the best DAG, improving search ability over traditional methods. Score functions are constructed for maximizing rewards by an RL agent. The BIC score is a key component in maximizing rewards for an RL agent when constructing a directed graph. It is consistent and locally consistent, with parameters \u03b8 and dimensionality d \u03b8 playing a crucial role. Gaussian additive noises are assumed, and linear models are applied to each causal relationship to calculate the BIC score. The BIC score is crucial in maximizing rewards for an RL agent in constructing a directed graph. It includes a log-likelihood objective and penalty on the number of edges. Different regression methods can be used to estimate causal relationships, such as quadratic regression and Gaussian Process Regression. Acyclicity is enforced through penalty terms in the score function to allow for changes in multiple edges. In constructing a directed graph, the BIC score is important for maximizing rewards for an RL agent. Acyclicity is enforced through penalty terms in the score function to allow for changes in multiple edges. The penalty weight needed to obtain exact DAGs can be large if only h(A) is used, so an additional penalty term is added to induce exact DAGs. Other functions, such as the total length of all cyclic paths in the graph, can also be used to compute 'distance' from a directed graph to DAGs. The reward function combines the score function and the acyclicity constraint, with penalty parameters \u03bb1 and \u03bb2. The penalty parameters \u03bb1 and \u03bb2 in the reward function aim to maximize rewards for directed graphs while enforcing acyclicity. Problems (1) and (6) are shown to be equivalent with properly chosen \u03bb1 and \u03bb2, ensuring that a minimizer of one is a solution to the other. Proposition 1 guarantees equivalence with a lower bound S L on the score function over all directed graphs. The lower bound S L on the score function over all directed graphs can be found with different methods depending on the score function used. For example, with the BIC score, variables can be fitted against each other without considering the penalty on the number of edges. On the other hand, the independence-based score function may simply set S L = 0. The equivalence of Problems (1) and (6) can be guaranteed by setting \u03bb 1 = S U \u2212 S L and any \u03bb 2 \u2265 0. The proposed RL approach to score-based causal discovery involves setting penalty weights for generating directed graphs closer to DAGs. Starting with small penalty weights and gradually increasing them ensures the satisfaction of certain conditions. Different score functions may have varying ranges, while acyclicity penalty terms remain independent. The proposed RL approach involves setting penalty weights for generating directed graphs closer to DAGs. The algorithm adjusts scores to a certain range using predefined scores. The weight \u03bb2 is updated similarly to the Lagrangian multiplier rule used by NOTEARS. In the proposed RL approach, penalty weights are set to generate directed graphs closer to DAGs. The algorithm adjusts scores using predefined values, with \u03bb2 updated akin to NOTEARS' Lagrangian multiplier rule. Parameter choices such as S0, t0, \u03bb1, \u22061, \u03bb2, \u22062, and \u039b2 can be further optimized for specific applications. The RL paradigm's exploitation and exploration scheme guides the search for the best acyclic graph with the highest score among all final outputs. The policy \u03c0(\u00b7 | s) and NN parameters \u03c8 are used for graph generation, with the training objective being the expected reward. The training objective is to optimize parameters using policy gradient methods and stochastic optimization. The critic, a 2-layer feed-forward NN, is trained with Adam on mean squared error. An entropy regularization term is also used for training. The training objective is to optimize parameters using policy gradient methods and stochastic optimization. An entropy regularization term is added to encourage exploration of the RL agent. The inferred graphs from the actor-critic algorithm are all DAGs in experiments. Computing rewards for generated graphs is more time-consuming than training NNs, so rewards for different graph structures are recorded. The BIC score can be decomposed according to single causal relationships, and corresponding RSS i is recorded to avoid repeated computations. All generated graphs during training are recorded, and the one with the best reward is outputted. In practice, pruning estimated edges in a greedy way is necessary to refine the inferred causal relationships. This involves removing parental variables and evaluating the resulting graph's performance. For linear models, pruning is achieved by thresholding estimated coefficients. Additionally, a penalty weight on the number of edges is added to the reward, with a preference for a penalty weight of log n as included in the BIC score. In this work, a penalty weight of log n is used in the BIC score for pruning inferred graphs to reduce false discoveries. Empirical results comparing various approaches, including GES, PC algorithm, ICA-LiNGAM, CAM, NOTEARS, DAG-GNN, and GraN-DAG, are reported on synthetic and real datasets. Default hyper-parameters are used for implementations unless specified. The authors use a thresholding method for pruning in various algorithms. They apply significance testing based on p-values for CAM and GraN-DAG. An RL based approach is implemented using Tensorflow. The decoder is modified, and hyper-parameters are unchanged. Batch size is set to 64, hidden dimension to 16. The approach is combined with BIC scores under Gaussianity assumption, denoted as RL-BIC and RL-BIC2. Estimated graphs are evaluated. The authors evaluate estimated graphs using three metrics: False Discovery Rate (FDR), True Positive Rate (TPR), and Structural Hamming Distance (SHD). SHD measures the smallest number of changes needed to convert the estimated graph into the true DAG. They treat unoriented edges as true positives and generate a binary adjacency matrix for the graph. Edge weights are assigned independently to obtain a weight matrix. The authors use a weight matrix W to sample data with Gaussian and non-Gaussian noise models. They generate datasets with m = 5,000 samples and consider graphs with d = 12 nodes. The learning process of the proposed method RL-BIC2 on a linear-Gaussian dataset is shown in Figure 2. The proposed method RL-BIC2 generates 683,784 different graphs on a linear-Gaussian dataset, much lower than the total number of possible DAGs. The pruned DAG matches the underlying causal graph. Empirical results show that PC and GES perform poorly on relatively dense graphs, while ICA-LiNGAM recovers true causal graphs for LiNGAM data but struggles with linear-Gaussian data. NOTEARS and DAG-GNN show good causal discovery results. CAM and GraN-DAG do not perform well due to assumptions about causal relationships. In contrast to GraN-DAG's poor performance, NOTEARS and DAG-GNN show good causal discovery results. Modifying feed-forward NNs to linear functions improves performance similar to NOTEARS. RL-BIC2 outperforms RL-BIC in recovering true causal graphs, even with randomly sampled noise variances. The RL-BIC method outperforms GES on datasets, showing improved search ability. The approach is tested on larger graphs with lower average degree, incorporating prior information with a common bias term. More observed samples and training iterations are chosen for the increased search space. On LiNGAM datasets, RL-BIC2 shows good performance with FDR, TPR, and SHD values. On LiNGAM datasets, RL-BIC2 performs well with FDR, TPR, and SHD values comparable to NOTEARS. Nonlinear causal relationships with quadratic functions are considered, expanding features to include first-and second-order terms. Coefficients are sampled from a uniform distribution, and edges in the causal graph are removed if parent variables have zero coefficients. The non-Gaussian noise model is used with 10-node graphs and 5,000 samples, making the true causal graph identifiable. In quadratic regression, outliers are treated by applying thresholding to estimated coefficients. RL-BIC2 outperforms PC and GES in 10-node graph experiments. In quadratic regression, outliers are handled by applying thresholding to estimated coefficients. RL-BIC2 performs better than PC and GES in 10-node graph experiments. For fair comparison, the same quadratic regression based pruning method is applied to NOTEARS outputs, resulting in NOTEARS-2. This pruning reduces false discovery rate (FDR) by removing spurious edges without affecting true positive rate (TPR). Since pruning does not improve TPR significantly for other methods, it is not applied to them. NOTEARS can be modified to model causal relationships using quadratic functions, creating a nonconvex optimization problem with a certain number of parameters. Despite not outperforming RL-BIC2 overall, NOTEARS-3 discovered almost correct causal graphs on more than half of the datasets. The increased number of optimization parameters and complex adjacency matrix may have made the problem harder to solve. There is potential for NOTEARS-3 to improve causal discovery performance with different optimization algorithms. Additionally, a nonlinear model with Gaussian process sampled functions and uniformly distributed noise was considered for causal graph generation. In a setting with identifiable causal relationships, empirical results show that ICA-LiNGAM, NOTEARS, and DAG-GNN perform poorly on a 10-node and 40-edge graph model. The methods operate on weighted adjacency matrices, while a new approach uses Gaussian Process Regression with RBF kernel to model causal relationships, avoiding overfitting issues. To avoid overfitting, different methods are used to discover a protein signaling network from observational data. The RL-BIC method outperforms others, with a dataset containing both observational and interventional data. The ground truth causal graph has 11 nodes and 17 edges, showing sparsity. The true graph is sparse with 11 nodes and 17 edges. Detailed results on the estimated graph are provided, including total edges, correct edges, and the SHD. GPR with RBF kernel is used for modeling causal relationships, along with CAM pruning. RL-BIC and RL-BIC2 show promising results compared to other methods, utilizing RL to search for the optimal DAG. The actor-critic algorithm is employed for RL, incorporating a predefined score function and penalty terms for acyclicity. Experiments using the actor-critic algorithm with encoder-decoder models show advantages in causal discovery. Challenges remain with large graphs, but smaller problems can be decomposed. Future work includes improving computational efficiency and leveraging prior knowledge for reducing search space. Developing an efficient score function is crucial for the proposed approach, rather than relying solely on computing resources to accelerate training. Various RL algorithms, such as the asynchronous advantage actor-critic algorithm, have shown effectiveness. Different NN-based decoders are described for generating binary adjacency matrices, including single layer, bilinear, Neural Tensor Network (NTN), and Transformer decoders utilizing multi-head attention. Proper early stopping criteria are recommended to prevent unnecessary total iteration numbers in experiments. The Transformer decoder utilizes multi-head attention to generate decoder outputs, followed by a shared feed-forward NN. Empirical results on linear-Gaussian data models with 12-node graphs are provided. In the implementation, the encoder output dimension is set to 64, single layer decoder dimension to 16, and NTN decoder parameter K to 2. The single layer decoder performs the best, benefiting from fewer parameters and ease of training, while the Transformer encoder captures causal relationships effectively. The encoder effectively captures causal relationships among variables. Linear-Gaussian FDR, TPR, and SHD values are provided. The equivalence of two problems is proven, showing that a directed acyclic graph (DAG) must be a solution. Contradictions arise when the graph is not a DAG or achieves a lower score. The text discusses contradictions in the context of directed acyclic graphs (DAGs) and their minimum score. It highlights the challenges of using only the indicator function for acyclicity in the RL approach, as it may not guide the agent effectively in generating DAGs. The text discusses contradictions in using the indicator function for acyclicity in directed acyclic graphs (DAGs) in the RL approach. It highlights challenges in generating DAGs effectively. The text discusses challenges in generating directed acyclic graphs (DAGs) effectively using the RL approach. It explores the impact of penalty weights on acyclicity terms in training the agent to produce DAGs. Starting with a DAG initialization did not lead to good performance, as early iterations generated graphs with many missing true edges. The text discusses the impact of penalty weights on training RL agents to generate directed acyclic graphs (DAGs). Existing implementations of causal discovery algorithms like ICA-LiNGAM and GES/PC are compared for recovering weighted adjacency matrices. The text discusses causal discovery algorithms like CAM, NOTEARS, and max-min hill climbing for recovering weighted adjacency matrices in DAGs. Implementations are available in Java and R packages. The text discusses causal discovery algorithms like DAG-GNN and GraN-DAG, which use neural networks to model causal relationships and optimize weighted adjacency matrices. Implementation codes are available on Github repositories. GraN-DAG (2019) utilizes feed-forward NNs to model causal relationships and optimize weighted adjacency matrices. The implementation codes can be found on the first author's Github repository. An entropy regularization term is added, and modifications are made to the reward and decoder. Linear functions are used in GraN-DAG, yielding similar results to NOTEARS in experimental evaluations. Results from empirical experiments comparing LiNGAM and linear-Gaussian data models with 12-node graphs and different noise variances are presented in Table 6. The FDR, TPR, and SHD values for NOTEARS and linear-Gaussian models are shown, with additional experiments using uniformly sampled noise variances. In experiments with 30-node graphs, a biased term is added to each decoder output to incorporate information from a sparse true causal graph. The biased term is trainable and initialized to be -10 to aid in generating graph adjacency matrices efficiently."
}