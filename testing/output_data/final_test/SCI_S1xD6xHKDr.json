{
    "title": "S1xD6xHKDr",
    "content": "The interpretability of neural networks is crucial for real-world applications, focusing on detecting interactions between features. A novel approach is proposed to build a hierarchy of explanations based on feature interactions, evaluated with LSTM, CNN, and BERT classifiers on text classification datasets. The proposed method aims to provide explanations for deep neural networks in NLP tasks like text classification, question answering, and machine translation. Lack of understanding in their decision-making process makes them black-box models, increasing the risk in real-world applications. Producing interpretable decisions is crucial for building trust in neural network models. The proposed method aims to provide hierarchical explanations for deep neural networks in NLP tasks, addressing the limitations of word-level explanations. This approach captures the interaction between features to enhance understanding of model predictions. The proposed method aims to provide hierarchical explanations for deep neural networks in NLP tasks, capturing feature interactions to enhance model prediction understanding. The method produces a hierarchical structure illustrating how different features interact for prediction, with examples showing contributions of words and phrases. The interacted Shapley value is used to capture feature interactions. The proposed INTERSHAPLEY method uses interacted Shapley values to measure feature interactions in text classification tasks with neural network models. It segments text into phrases and words recursively, providing hierarchical explanations for deep neural networks in NLP tasks. The method is evaluated against competitive baselines for explanation generation. The INTERSHAPLEY method introduces a novel approach to measure feature interactions in text classification tasks using interacted Shapley values. It offers hierarchical explanations for deep neural networks in NLP tasks by segmenting text into phrases and words recursively. The method outperforms existing techniques in both automatic and human evaluations. Explanation methods for neural networks include decoding interpretable knowledge, modifying network architectures, and developing model-agnostic methods. These approaches aim to explain model predictions and behaviors, but may have limitations in real-world applications. Model-agnostic methods focus on generating explanations based on model predictions, applicable for any black-box models. Various techniques like Leave-one-out and LIME have been proposed to interpret black-box models by observing changes in predicted probabilities. SHAP, LShapley, and C-Shapley are Shapley-based methods that estimate individual word contributions locally. The curr_chunk discusses the use of SHAP, LShapley, and C-Shapley methods to evaluate feature importance and reduce computational complexity. It also mentions hierarchical interpretations in neural networks, such as agglomerative contextual decomposition (ACD) and the extension of SHAP values by Lundberg et al. (2018). The curr_chunk introduces a new method for detecting feature interactions in text classification without relying on external hierarchical structures. It defines a feature importance score and discusses a novel approach for building a hierarchy of explanations based on optimizing feature interaction scores in a given text. The feature importance score is crucial in building a hierarchy of explanations for text classification. It measures the contribution of a subset of features to each model prediction, helping to optimize feature interaction scores. The score is defined based on the model output with respect to labels and the predicted label obtained by the model. In this work, features are represented by a sequence of words, and the importance score quantifies the difference between the probabilities of the predicted class and the second most probable. The feature importance score in text classification measures the contribution of a subset of features to each model prediction. It quantifies the difference between the probabilities of the predicted class and the second most probable class, showing the confidence in classifying the text. Words outside the subset are replaced with a special token to maintain the same length for interpretability. This approach considers the prediction process as a coalition game to measure feature interactions consistently. The feature interactions in text classification are measured consistently using Shapley values. The interaction between features is computed based on the contributions from other features in the set. The total interaction between two features is the sum of their individual interactions. The hierarchical interpretation approach involves recursively splitting a feature set into subsets with strong intra-interactions and weak inter-interactions. Each level of the hierarchy consists of feature subsets, with candidate sets for the next layer obtained by splitting one element at a time. The number of potential partitions depends on the size of the feature set. The hierarchical interpretation approach involves recursively splitting a feature set into subsets with strong intra-interactions and weak inter-interactions. The number of potential partitions depends on the size of z. The best feature set at level + 1 satisfies Equation 4 to find the best partition between z\u0302 with minimum interaction score. The method scales well with input text size n and can combine words into phrases using the bottom-up method. Detailed comparisons will be shown in Appendix A. The proposed method is evaluated on text classification tasks with three neural network models: LSTM, CNN, and BERT, on SST and IMDB datasets. Evaluation includes word-level and phrase-level explanations using INTERSHAPLEY, with metrics like AOPC and log-odds scores. Word-level explanations provide a sequence of important words. The AOPC and log-odds scores are metrics used to evaluate word-level explanations in text classification tasks. AOPC measures the average change in prediction probability by deleting top k% words, with higher scores indicating the importance of deleted words. On the other hand, log-odds scores calculate the difference in negative logarithmic probabilities before and after masking the top r% features, with lower scores being better. IN-TERSHAPLEY achieves the best performance on both evaluation metrics for extracting keywords via feature importance scores, laying a solid foundation for feature interaction detection. The performance of L-and C-Shapley is worse on the IMDB dataset due to a small window size causing errors in approximating Shapley values for long sentences. LIME's performance drops significantly on BERT compared to other models, indicating its limitations with deep neural network models. The evaluation section presents results on phrase-level explanations extracted using proposed methods. It demonstrates successful capture of strong interactions through quantitative and qualitative analysis, showcasing how different features interact for final predictions. The evaluation section showcases successful capture of strong interactions in phrase-level explanations. It involves extracting important phrases and randomly inserting them back into sentences to evaluate the impact on predicted labels. This process helps in understanding how different features interact for final predictions. The evaluation section demonstrates the successful capture of strong interactions in phrase-level explanations by extracting important phrases and evaluating their impact on predicted labels. INTERSHAPLEY outperforms ACD in cohesion-loss and average explanation length on SST and IMDB datasets with LSTM models, indicating its ability to capture important and concise phrases effectively. Results of INTERSHAPLEY show that LSTM model improves cohesion-loss on IMDB dataset, while BERT performs poorly due to length constraint. Visualizations of INTERSHAPLEY and ACD for LSTM on SST dataset reveal how bravura and emptiness interact, leading to wrong prediction by LSTM. INTERSHAPLEY captures this interaction accurately, while ACD fails to do so. The study employs human annotators to evaluate the explanation of model predictions. An interface is used where annotators choose a label based on their understanding of the explanation. The coherence score is calculated as the ratio of human annotations coherent with the model predictions. The study evaluates the coherence of model explanations by human annotators. Coherence score is defined as the ratio of annotations coherent with model predictions. 60 movie reviews from IMDB dataset are randomly selected for experiments comparing INTERSHAPLEY and ACD with LSTM model. INTERSHAPLEY outperforms ACD with higher coherence scores, indicating better human understandability of explanations. INTERSHAPLEY achieves high coherence scores on neural networks, validating its ability to interpret black-box models. BERT has higher prediction accuracy but lower coherence score, indicating poor interpretability. Hierarchical interpretations for the LSTM model show differences between top-down and bottom-up approaches in capturing key phrases. In this work, various models like RNN, CNN, and BERT are used for text classification tasks. The models are initialized with pretrained word embeddings and fine-tuned for optimal performance. Two benchmark datasets are utilized for evaluation. The study compares different methods for text classification using benchmark datasets like SST-2 and IMDB. Competitive baselines such as Leave-one-out, CD, L-and C-Shapley, LIME, and ACD are evaluated for word and phrase-level explanation generation."
}