{
    "title": "S1EzRgb0W",
    "content": "Neural networks often make mistakes, and the reasons behind these errors are often unclear, leading to them being considered black boxes. This paper introduces a method to visually explain misclassifications by showing what needs to be added to an image for correct classification. By combining adversarial examples, generative modeling, and difference target propagation, this technique aims to demystify neural networks for users. Given the widespread use of deep learning in real-world applications, explaining misclassifications in images has become crucial. One approach is to use Gradient Descent to perturb misclassified images for correct classification. This method helps understand errors, like a self-driving car stopping for a tree misidentified as a pedestrian. The \"Gradient Descent on the input\" approach by BID8 introduces adversarial examples, showing that small perturbations can change an image's classification. These perturbations, resembling white noise, are often imperceptible but can alter the output class. The goal is to make perturbations interpretable and relevant to the misclassification. The paper introduces a method using generative models to create explanations for misclassified images. It includes a classifier and a generative model trained on the same dataset. The method perturbs features of misclassified images to achieve correct classification. The paper is structured with sections on the proposed method, experiments, related work, and conclusion. The method involves an encoder and a generator to create perturbed images for correct classification. The goal is to generate a perturbed version of a misclassified image that is correctly classified. The perturbed reconstruction is defined mathematically, inspired by Difference Target Propagation. Our method, inspired by Difference Target Propagation, aims to minimize Cross Entropy and limit the added features to the original image by using gradient descent to learn the perturbation. The perturbation stops once the condition for correct classification is met, generating Latent Sympathetic Examples (LSE) using a generative model. Our method aims to explain classifier decisions by perturbing an image to achieve correct classification. The perturbation is kept minimal and interpretable, using a combination of generative models for MNIST and CelebA datasets. The goal is to add only relevant parts to the image for classification, avoiding projection of prototypical class examples. The method involves training two Variational Auto Encoders on MNIST with different dimensional latent spaces to analyze the impact on image classification. A classifier is attached to the VAEs without updating their weights to understand the method's effectiveness. Results from the 2-D VAE show differences in predictions between the generated images and original images. The predictions of c(g(z)) and c(x) for various values of z are plotted in FIG1 and FIG2 respectively, showing significant differences. The method may not always perturb to the target class, as illustrated in FIG2 where class '1' is missing. Sometimes the method gets stuck in a local minimum, hindering the generation of LSE. A successfully perturbed image's latent space is shown in figure 4, highlighting the visible perturbation in the image space with coarse image quality. The VAE with a 10-D latent space produces better image quality compared to a coarse image space. Research aims to explore the relationship between latent space dimensionality and the effectiveness of generating LSE's. Two experiments are conducted: perturbing misclassified examples to calculate success rate, and generating latent adversarial examples from correctly classified images. Some classes may not be perturbed successfully, such as class '1'. The study explores the impact of latent space dimensionality on generating latent adversarial examples (LAE's) using a VAE. Two experiments were conducted, perturbing misclassified examples to their correct class. The 10D VAE successfully generated LSE's in 95.40% of cases with an average euclidean distance of 0.217, while the 2D VAE perturbed 78.16% of examples. The experiment aimed to compare the success rates of generating latent adversarial examples (LAE's) and latent space embeddings (LSE's) using VAEs of different dimensionalities. The 10-D VAE had a higher success rate of 91.11% in creating LSE's compared to the 2-D VAE with 22.34%. The average L-2 distances for the 10-D and 2-D VAEs were 1.53 and 0.52, respectively. The success rates for generating latent adversarial examples differ significantly between 2-D and 10-D VAEs. The 10-D latent space is less likely to get stuck, making LSE generation more effective than LAE generation. The distance traveled in the latent space affects the chances of getting stuck in a local minimum. Contours in the latent space indicate cross entropy errors, with specific points annotated for each step. The algorithm stopped at the gold star, with annotated steps corresponding to images on the left. Perturbations must be meaningful and close to the original point for the method to succeed. The quality of reconstructions and latent space dimensions influence the size of perturbations. Perturbed images from 2-D and 10-D VAEs show coarser perturbations in the 2-D latent space. The perturbations made using a 10-D latent space in a latent variable model are more precise compared to a 2-D latent space. For example, specific details like dimming parts of strokes or circles were more accurately perturbed in the 10-D model. However, perturbations in the 2-D model were sometimes less precise, such as projecting another image over the original or filling in parts of the image. Increasing the latent space led to more subtle perturbations overall. When increasing the latent space, perturbations become more subtle. A VAEGAN with a 1024-D latent space was trained on the CelebA dataset, with classifiers for traits like \"Male\" and \"Smiling\". Successful explanations for misclassifications were provided, showing potential biases in the dataset. The success of finding biases in a dataset using perturbations depends on specific traits like \"Mustache\" and \"Long hair\". The method's effectiveness is shown through success rates varying per trait, with \"Mustache\" performing significantly worse. The search for biases is conducted in the 1024D latent space of a VAEGAN trained on the CelebA dataset. Examples of using the LSE technique to perturb images for correct classification. Original image, perturbation, and perturbed image shown. Misclassifications corrected by adjusting brightness of earrings, hair below nose, and darkening hair under the nose. The perturbation in the image with a mustache corrected misclassification by making the adam's apple lighter. Adversarial examples exploit low probability areas in the image manifold to deceive neural networks. Various optimizers can be used to generate adversarial attacks in an untargeted manner. Our work extends prior research by using gradient descent to create adversarial examples that help the trained classifier minimize prediction errors. This approach is different from traditional adversarial attacks, as it aims to be sympathetic to the classifier rather than deceive it. Our method uses a trained generative model to constrain the search for adversarial examples, making them more interpretable to users. Inspired by Difference Target Propagation, which sends targets instead of gradients backwards through the network during training. The authors propose a method to minimize loss by using invertible layers and applying a linear correction to adjust backpropagating targets. This correction ensures that perturbations in image space decrease as latent perturbations decrease. The technique is related to heatmapping methods, specifically the image-specific saliency map technique. However, a downside is that heatmaps can be noisy and change irrelevant background parts of an image to increase classification scores. The authors introduced a new method to explain misclassifications in neural networks using localized perturbations. The method shows promising results on various datasets but may have limitations in real-world applications due to varying success rates depending on annotated traits in images. An image's trait interpretation can be ambiguous for annotators, making it difficult to classify extreme cases with certainty. The method can be used to identify class imbalances in datasets and improve classifier performance by generating edge cases for training. The method discussed can help improve classifier performance by generating edge cases for training. It aims to make neural network decisions more explainable by annotating edge cases. The perturbation process is shown in figures, demonstrating how the method perturbs images to be correctly classified. The image is stuck in a local minimum, misclassified as a zero instead of an eight due to a bias in the dataset. Perturbations target the hair, indicating a lack of men with long hair in the dataset. Perturbations are normalized using min-max normalization."
}