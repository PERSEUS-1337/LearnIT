{
    "title": "rJePwgSYwB",
    "content": "Generative adversarial networks (GANs) are a widely used framework for learning generative models, with Wasserstein GANs (WGANs) being a successful variant that can be trained with stochastic gradient descent-ascent. This paper demonstrates that when the generator is a one-layer network, stochastic gradient descent-ascent can converge to a global solution in polynomial time and sample complexity. GANs have been applied to various tasks such as image-to-image translation, image super-resolution, domain adaptation, probabilistic inference, and compressed sensing. Recent advances in generative adversarial networks (GANs) have led to the success of Wasserstein GANs (WGANs), which leverage neural networks to better measure the difference between target and generated distributions. Theoretical studies have shown that with the right design, WGANs can identify the target distribution with low sample complexity. Prior work has focused on the stability and convergence properties of gradient descent-ascent (GDA) in GAN training and min-max optimization problems. Recent studies have explored the use of gradient descent-ascent (GDA) and its variants in GAN training and min-max optimization problems. These works have investigated conditions for GDA to converge to globally optimal solutions under convex-concave objectives or locally optimal solutions under nonconvex objectives. Variants of GDA with improved stability properties, including negative momentum, have been identified. In the context of GAN training, it has been shown that WGANs with linear features can benefit from these advancements. In the context of GAN training, the study explores the global convergence properties of stochastic GDA for WGANs with non-linear generators. It focuses on learning a single-layer generative model with various activation functions. The research shows that stochastic gradient descent-ascent can learn a target distribution using polynomial time and samples, assuming the target distribution is realizable in the generator's architecture. The study focuses on the global convergence properties of stochastic gradient-descent for WGANs with non-linear generators. It analyzes the dynamics to show global optimum attainment and designs the discriminator for statistical rate. Previous works establish conditions for local convergence and stability in GAN training. The study explores global convergence properties of stochastic gradient-descent for WGANs with non-linear generators. Previous research has focused on local convergence and stability in GAN training, but global convergence remains a challenge. The work extends beyond linear generators to consider WGANs with diverse activation functions, demonstrating recovery of ground truth distribution parameters with proper gradient-based algorithms. WGANs present nonconvex-nonconcave optimization challenges, with various notions of local min-max solutions. GDA can find stationary points of the max objective in nonconvex-concave scenarios, while variants with negative momentum show global convergence to the min-max solution. Several works have studied mode collapse in GANs, questioning their ability to learn the distribution rather than just memorize data. Proposed solutions include specific generator designs and parametric discriminator setups to achieve statistical rates and reduce sample complexity. Wu et al. (2019) introduced an algorithm for learning the distribution of a single-layer ReLU generator network. Our paper focuses on understanding when a WGAN formulation can learn in polynomial time and sample complexity, using GAN formulations for learning a generator with specific parameters and activation functions. The latent variables are sampled from a normal distribution, and the real distribution outputs samples based on ground truth parameters. The Wasserstain GAN formulation involves choosing k for expressivity, using a linear discriminator for learning marginal distributions, and defining directional derivatives. The Wasserstain GAN formulation involves choosing k for expressivity, using a linear discriminator for learning marginal distributions, and defining directional derivatives. With a linear discriminator Dv(x) = vx, the minimax game is adjusted for odd activation functions by using an adjusted rectified linear discriminator to enforce bias. The loss function is modified accordingly to learn each marginal of D based on specific activation function criteria. The discriminator's capacity is regularized similar to the Lipschitz constraint in WGAN. Theorem 1 states that alternating gradient descent-ascent with proper learning rates leads to global min-max points. The correct norm of a * i is recovered, and the marginal distribution for each entry i is learned. A (rectified) linear discriminator is used in the previous section to interact with the i-th random variable, enabling WGAN to learn the correct a i for all i. However, there is no interaction between different coordinates of the random vector. To improve learning the joint distribution, a quadratic discriminator is proposed to enforce component interactions. The regularized version is studied, focusing on second-order stationary points and convergence with gradient descent. The computation of g(A) relies on Hermite polynomials, extending from unit row vectors to the general case. The activation function \u03c6(x) can be rewritten using normalized Hermite polynomials as \u03c6(x) = \u221e i=0 \u03c3 i h i. By reparametrizing with Z = AA, the function g(Z) = g(A) is defined. Assumptions are made that \u03c6 is an odd function plus constant, and \u03c3 1 = 0. Common activations like tanh and sigmoid satisfy these assumptions. Additionally, g(Z) has a unique stationary point for activations like leaky ReLU. Lemma 2 states that for odd activation functions like leaky ReLU, the only zero point for the polynomial scalar function g is at z = z*. This property is extended to Problem 1, where optimization is done over function g when a*i = 1 for all i. Theorem 2 concludes that for leaky ReLU activations and functions meeting Assumption 2, when k = d, all second-order KKT points are achieved. Theorem 2 states that for leaky ReLU activations and functions satisfying Assumption 2, all second-order KKT points are global minimums when k = d. This leads to convergence of alternating projected gradient descent-ascent on Eqn. (3) to A : AA = A * (A * ). The extension for non-unit vectors is straightforward and discussed in the Appendix. Algorithm 1 involves online stochastic gradient descent ascent on WGAN, where gradient ascent with step-size 1 optimizes V in each iteration. The min-max optimization problem is solved using gradient descent over g F with a batch of samples. The algorithm in Algorithm 1 optimizes the empirical risk to yield the ground truth covariance matrix with high probability, with large enough sample complexity and mini-batch size. The estimator g(\u00b7) approximates the population risk well on both gradient and Hessian. Any -SOSP for g(A) leads to an O( )-FOSP for the semi-definite programming on g(Z). An O( )-FOSP of g(Z) induces at most O( ) absolute error compared to the ground truth covariance matrix Z*. The activation and its gradient are assumed to be Lipschitz continuous. Sample complexity is estimated by bounding the gradient and Hessian for population risk and empirical risk on observation samples. Lemma 3 and Lemma 4 discuss the activation assumptions and the gradient estimation in the WGAN setting. The gradient error between the function f(A) and g n (A) is analyzed, with a concentration bound derived to determine the error bound. Lemma 5 addresses the empirical risk g m,n. In the previous section, the error bound between f(A) and g(A) was discussed. The empirical risk g m,n is shown to be an unbiased estimator of g n. Concentration bounds over observation samples and mini-batch sizes are conducted to show the relationship between f(A) and g(A), leading to the establishment of Problem 1. Problem 1 involves finding a solution for Tr(A X i A) = y i, where X i \u2208 S, y i \u2208 R, and i = 1 to n. The re-parametrized version defines an approximate second-order stationary point (-SOSP) and an approximate first-order stationary point (-FOSP) for optimization. The -FOSP condition involves a symmetric matrix Z, a vector \u03c3, and a symmetric matrix S. The goal is to show that an -FOSP of g(Z) provides a good approximation for the true parameter. In this section, we present experimental results validating stochastic gradient descent ascent's performance in recovering the parameter matrix. We analyze the impact of sample size, activation function choice, and output dimension on parameter estimation accuracy. The results support our theory and connect recovery guarantees with sample complexity and batch size. In Figure 1, the relative error for parameter estimation decreases with increasing sample complexity. The recovery error decreases with higher sample complexity and smaller output dimension. A comparison for different activation functions is also included. The activation function is modified as \u03c6 := R(\u03c6 \u2212 C), where C is the constant bias term of \u03c6. The optimization objective can be rewritten for cases where \u03c6 is positive and increasing on its support [0, +\u221e). The stationary points satisfy the necessary condition \u2207 v f 1 (A, v) = 0, with \u03c6 defined differently for odd or non-odd cases. At stationary points, the condition is that \u03c6 is positive and monotone increasing on its support [0, \u221e). The only solution for this condition is a j = a * j , \u2200j, as shown by Claim 3. The function h(\u03b1) := E x\u223cN (0,\u03b1 2 ) f (x) is monotone increasing if f is positive and monotone increasing on its support [0, \u221e). Therefore, at the stationary point where \u2207f 1 (A, v) = 0, we have \u2200i, a * i = a i. At stationary points, the function h(A) is optimized by taking gradient ascent steps on the discriminator side v. The symmetry of the Gaussian allows for simplification in verifying global minima. The function g(Z) is studied for stationary points by looking at individual components. At stationary points, the function h(A) is optimized by taking gradient ascent steps on the discriminator side v. To study the stationary point of g(Z) = jkg jk (z jk), we simplify the notation and focus on odd-plus-constant activations. The polynomial f(a) is factorized to a - b and two always nonnegative factors I and II. For ReLU activation, h(-1) = 0 for any z * jk, preventing the same conclusion as for other activations. For leaky ReLU with coefficient \u03b1 \u2208 (0, 1), the function \u03c6(x) = max{x, \u03b1x} can be expressed as (1 \u2212 \u03b1)ReLU(x) + \u03b1x. The stationary point Z of g(Z) = jkg jk also satisfies certain conditions. By reparametrizing the problem, we can analyze the first-order stationary point for Problem 2. This involves finding a vector \u03c3 such that certain conditions are met. The text discusses the conditions for second-order stationary points in Problem 1, concluding that they are global minimums. It also mentions the use of Hermite functions in analyzing the loss function landscape for matrix A. The text discusses the conditions for second-order stationary points in Problem 1, concluding that they are global minimums. It also mentions the use of Hermite functions in analyzing the loss function landscape for matrix A. Here, the variables are normalized and coefficients for each term are explicitly written out. The conditions for -FOSP of Eqn. (6) are satisfied, and it remains to show S A \u2212 I. If A is rank-deficient, there exists a vector v such that Av = 0. The text discusses conditions for second-order stationary points in Problem 1, concluding they are global minimums. It mentions using Hermite functions to analyze the loss function landscape for matrix A. The conditions for -FOSP of Eqn. (6) are satisfied, and it remains to show S A \u2212 I. If A is rank-deficient, there exists a vector v such that Av = 0. From (10), we have b S A b \u2265 \u2212 /2 b 2 for any b, i.e. S A \u2212 /2I d\u00d7d. When A is full rank, the column space of A is the entire R d vector space, and therefore S A \u2212 I d\u00d7d directly follows from the -SOSP definition. The empirical risk on observations is written as X = E x\u223cD [xx]. The directional derivative of \u2207g(A) \u2212 \u2207g n (A) with direction B is given by Claim 5. By matrix concentration inequality, we have with probability 1\u2212\u03b4. The target function is E Sgm,n (A) \u2212 g n (A). The polynomial is always positive for z = z * and k to be odd. Comparing equations (12) and (13), we have Z. The text discusses conditions for second-order stationary points in Problem 1, concluding they are global minimums. Using Hermite functions to analyze the loss function landscape for matrix A. Conditions for -FOSP of Eqn. (6) are satisfied, and it remains to show S A \u2212 I. When A is rank-deficient, there exists a vector v such that Av = 0. The polynomial is always positive for z = z * and k to be odd. By comparing equations (12) and (13), we have Z \u2212 Z * Proof of Theorem 3. From Theorem 31 from Ge et al. (2015), for small enough learning rate \u03b7, and arbitrary small, there exists large enough T, such that Algorithm 1 generates an output A (T) that is sufficiently close to the second order stationary point for f. Finally with Lemma 7, we get Z \u2212 Z * F \u2264 O()."
}