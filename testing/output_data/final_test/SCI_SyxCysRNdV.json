{
    "title": "SyxCysRNdV",
    "content": "In this paper, a method for rumor detection on social media is proposed to address limited labeled data and class imbalance issues. An offline data augmentation technique based on semantic relatedness is utilized, using unlabeled social media data to augment labeled data. A context-aware neural language model and a credibility-focused Twitter corpus are employed to learn effective representations of rumor tweets. Experiments on real-world events demonstrate the effectiveness of the method in generating larger training data with reasonable quality via weak supervision. In this study, a method for rumor detection on social media addresses the challenge of limited labeled data by using offline data augmentation with semantic relatedness. The research presents preliminary results using a neural network model with augmented data for rumor detection, highlighting the importance of Machine Learning and Natural Language Processing in this field. The scarcity of labeled data remains a major obstacle in studying rumors on social media. In this work, a novel data augmentation method for rumor detection is proposed based on semantic relatedness. The approach aims to address the challenge of imbalanced class distributions in publicly available data sets for rumor-related tasks. Previous studies suggest that rumors can evolve into variants with similar propagation patterns, making enriching labeled data with unlabeled source tweets a promising strategy for improving rumor detection methods. Our study proposes a data augmentation method for rumor detection using semantic relatedness. By leveraging a paraphrase identification corpus and context-sensitive embeddings, we assign pseudolabels to unlabeled tweets based on pairwise similarity. The ELMo BID18 neural language model is fine-tuned on a credibility-focused social media corpus to encode tweets, showing improved detection with increased training data size. This approach has potential for further enhancements with deeper neural networks. We present results for three events and the performance of a deep neural network model for rumor detection with augmented data. Four publicly available datasets and a Twitter paraphrase corpus are utilized in this project. The project utilizes various datasets for data augmentation in rumor detection, including a Twitter paraphrase corpus, SemEval-2015 task 1 data, PHEME data set, CrisisLexT26 data, and Twitter event data from 2012-2016. These datasets cover paraphrase identification, semantic similarity measurement, and tweets associated with real-world events for enhancing the detection of rumors. The project utilizes various datasets for data augmentation in rumor detection, including a Twitter paraphrase corpus, SemEval-2015 task 1 data, PHEME data set, CrisisLexT26 data, and Twitter event data from 2012-2016. Among the events selected are 'Ferguson unrest', 'Sydney siege', 'Ottawa shooting', 'Charliehebdo attacks', 'Germanwings plane crash', and 'Boston marathon bombings'. The large corpus comprises more than 60M tweets grouped into 1049 events, each manually annotated with credibility ratings, used to fine-tune ELMo for rumor-related tasks. References are generated separately for PHEME5 and 'Boston bombings'. The text discusses the process of generating references for rumor detection using weak supervision. It involves computing semantic similarity between reference and candidate tweets using a contextual embedding model (ELMo) that is fine-tuned with domain-specific data. Language-based filtering and linguistic preprocessing are applied to pairs of tweets to compute ELMo embeddings for measuring semantic relatedness. The text discusses computing ELMo embeddings of tweets for semantic relatedness measurement. Cosine similarity is used as a relatedness measure, with thresholds fine-tuned using SemEval-2015 data. Source tweets are selected based on these thresholds, and social-temporal context data is collected for them. Source tweets without context are filtered out, and data is collected for events from Twitter and CREDBANK datasets. The train corpus contains 6,157,180 tweets with 146,340,647 tokens and 2,235,075 vocabularies. Fine-tuning a pretrained ELMo model using CREDBANK data set. Training corpus split into batches for encoding tweets. Training time exceeded 800 hours on a NVIDIA Kepler. The training process for fine-tuning a pretrained ELMo model on the CREDBANK dataset took over 800 hours on a NVIDIA Kepler GPU with limited memory. The results showed significant improvements in perplexity on both hold-out and test sets. Additionally, the effectiveness of the fine-tuned ELMo model was demonstrated in comparison to other word embedding models on the SemEval-2015 data. Data augmentation procedures were followed to enhance the quality of the model. Data augmentation procedures were followed to enhance the quality of the model, including selecting rumor source tweets based on a relatedness threshold of 0.8 and sampling non-rumor source tweets. Computational performance for ELMo embedding and semantic relatedness measurement was done on CPU, with tweet encoding at 10 per second and pairwise comparison at 869 pairs per second on average. After data augmentation, a limited amount of augmented data was obtained for \"bostonbombings\", with experiments showing promising results for \"sydneysiege\" and \"ottawashooting\". Additional rumors and non-rumors were collected for both events. Rumor detection experiments were conducted using two different datasets, PHEME5 and PHEME5 with the \"bostonbombings\" data (\"PHEME5+Boston\"), employing a baseline model for rumor detection with slight modifications. The augmented data is publicly available. We modified the \"MTL2 Veracity+Detection\" implementation for rumor detection only, using a source tweet and the top 24 replies. Leave-one-out cross-validation was performed on PHEME5 and augmented datasets, showing decreased performance with augmented data. The \"fergusonunrest\" event posed the most difficulty, but data augmentation improved performance. Augmentation for events like \"bostonbombings\" has potential to boost overall rumor detection performance. The study presents a methodology for data augmentation in rumor detection using semantic relatedness between limited labeled and unlabeled data. The research aims to utilize a large amount of unlabeled Twitter data and DNNs for various tasks related to social media rumors. The effectiveness of semantically augmented data in addressing labeled data scarcity and class imbalance issues in rumor datasets has been demonstrated. Future plans include augmenting data for more events, conducting experiments on rumor detection using deep learning, and evaluating the effectiveness of augmented data in preventing overfitting and enabling deeper neural networks for rumor detection. Further experiments will examine the generalization of rumor detection models on unseen data. Experiments will be conducted to examine the generalization of rumor detection models on unseen rumors."
}