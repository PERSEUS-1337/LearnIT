{
    "title": "rkLyJl-0-",
    "content": "Progress in deep learning is hindered by the time it takes to train large models. Using more hardware is not always effective. A new algorithm is introduced in this paper that is faster and scales better with more computational resources. The algorithm computes the inverse Hessian of each mini-batch for descent directions without explicit approximation. It has been successfully used to train large ImageNet models with mini-batch sizes up to 32000 without loss in validation error or increase in total number of steps. Our optimizer improves validation error in models by 0.8-0.9\\% at smaller mini-batch sizes. It can reduce the number of training steps needed by 10-30\\% while maintaining practicality and ease of use. The algorithm is computationally cheap and requires tuning only one hyperparameter (learning rate). Approaches that decrease total wall-time without sacrificing model generalization are valuable. Mini-batch SGD computes the average gradient over a small set of examples and takes a step in the direction of the negative gradient. Decreasing variance by increasing batch size may result in sublinear speedups and degraded generalization performance. Recent work suggests training ResNets and AlexNet on Imagenet with large mini-batches up to 8192 without loss of accuracy, shortening training time to hours instead of days. In an effort to reduce training time, recent work has shown success in training ResNets and AlexNet on Imagenet with large mini-batches of up to 8192 without sacrificing accuracy. This approach shortens training time significantly, from days to hours. Additionally, a novel stochastic optimization algorithm is introduced in this work that utilizes limited second-order information without explicit approximations of Hessian matrices or Hessian-vector products, aiming to further reduce wall-time during training. Our algorithm computes a descent direction on mini-batches by inverting the Hessian without explicit computations, using Neumann series expansion. Large-scale experiments on real models show linear speedup up to batch size of 32000, maintaining or improving model quality compared to baseline. Our algorithm improves model quality compared to the baseline and can reduce validation error by 0.8-0.9% with smaller mini-batches. It also offers a 10-30% decrease in the number of steps while maintaining baseline model quality. The algorithm is easy to use with the learning rate as the only hyperparameter. Despite various research efforts on faster stochastic optimization algorithms, none have gained popularity due to added computational cost and implementation complexity. Recent research has focused on using very large batches to scale up mini-batch sizes without degradation in evaluation metrics. The goal is for a neural net to learn to predict a target, either discrete or continuous, by minimizing the loss function. If the true data distribution is unknown, the optimization problem needs to be solved. Our algorithm involves solving a linear system on each mini-batch by forming a separate quadratic subproblem. Our algorithm forms separate quadratic subproblems on each mini-batch and solves them using an iteration scheme. In Section 2.2, we show how to avoid the need for a Hessian matrix. Additionally, in Section 3, we make the algorithm practical by discussing different ways to solve the linear system. The method described in the curr_chunk utilizes a power series expansion of the approximate inverse for solving linear systems, specifically using the Neumann power series for matrix inverses. This approach is based on the Richardson iteration and is equivalent to gradient descent on the quadratic objective. The algorithm avoids the need for a Hessian matrix and makes solving linear systems practical for large datasets. The curr_chunk discusses the use of mini-batches in optimization, forming a stochastic quadratic approximation, and solving linear systems using the Neumann series. The method aims to optimize a function without the need for a Hessian matrix, making it practical for large datasets. The curr_chunk discusses the justification for using a first-order approximation to incorporate curvature information in a matrix-free fashion. It introduces the idealized Neumann algorithm for optimization and updating weights. The practical solution of the algorithm is further elaborated in the rest of the paper. The curr_chunk discusses the difference between their technique and a typical stochastic quasi-Newton algorithm in approximating the Hessian of the total loss. Their algorithm approximates the Hessian only on the mini-batch to obtain the descent direction, while stochastic quasi-Newton algorithms use second-order information about the total objective. The curr_chunk justifies using curvature information from gradient evaluations in a mini-batch setting, highlighting better concentration properties of gradients over Hessians. The two-loop structure of Algorithm 1 differs from traditional optimization approaches by solving a simpler linear system in the inner loop. This divergence from typical optimization papers for machine learning focuses on deriving convergence rates without standard assumptions on smoothness and strong. The text discusses the challenges of building an optimizer for large-scale deep neural networks, focusing on the impracticality of the idealized Neumann optimizer algorithm due to assumptions about the positive definiteness of the Hessian. Techniques for convexifying the problem and reducing hyperparameters are introduced, with a mention of cubic regularization for dealing with non-convexity in the objective. The cubic regularization method, studied in BID5, adds a regularization term to the objective function to ensure convergence to a critical point. Inspired by this, two regularization terms - a cubic regularizer and a repulsive regularizer - are added to the objective to prevent large updates to parameters during optimization. The regularized objective function includes a repulsive potential term that becomes dominant at the end of training. The Hessian matrix is positive definite, but individual batch Hessians may not be. To address this, a positive definite matrix is defined using eigenvalues of the Hessian. This matrix is used in the inner loop for updates to the descent direction. The descent direction updates involve computing extremal mini-batch Hessian eigenvalues to adjust the repulsive regularizer and moving average parameters over the training trajectory. Initially, there are large negative eigenvalues that decrease towards zero during optimization. The number of SGD warm-up steps and reset steps also play a role in the optimization process. The largest positive eigenvalues increase linearly during optimization, validating the mini-batch convexification routine. The cubic regularizer ensures convexity without distorting the Hessian excessively. Adjustments to the Neumann algorithm include a short phase of vanilla SGD at the start for improved performance and stability in training. The text discusses the challenges of determining the number of inner loop iterations in optimization algorithms for deep neural networks. It highlights the trade-off between efficiency and the risk of the algorithm becoming similar to stochastic gradient descent. The solution proposed involves computing stochastic gradients at every iteration instead of freezing a mini-batch. The text discusses solving a stochastic optimization subproblem in the inner loop instead of a deterministic optimization problem. This change is effective in practice and makes the optimizer insensitive to the number of inner loop iterations. Additionally, a single learning rate is applied instead of two, and a doubling schedule for experiments is recommended for setting the mini-batch size. The Neumann optimizer is recommended for large batch settings, with a focus on selecting a large mini-batch size. Experimental evidence supports this hypothesis. The algorithm simplifies implementation by using displaced parameters. A list of hyperparameters that work across various models is provided, with the learning rate being the only user-selected parameter. The Neumann optimizer utilizes cubic regularizer \u03b1, repulsive regularizer \u03b2, momentum \u00b5(t), and moving average parameter \u03b3. Initial weights are set, followed by running vanilla SGD for a few iterations. The optimizer is evaluated on large convolutional neural networks for image classification, showing success on smaller datasets without hyperparameter adjustments. The Neumann optimizer, evaluated on large convolutional neural networks for image classification, shows stability in final evaluation metrics across runs. Experiments were conducted on the ImageNet dataset using Tensorflow on Tesla P100 GPUs. Training epochs were used as a measure of time to abstract away system variability. Standard Inception data augmentation was applied, with specific input image sizes for different models. Code will be open-sourced in the future. The Neumann optimizer was compared to standard optimization algorithms using various models like Inception-V3, Resnet-50, and Resnet-101. Different optimizers and batch sizes were used for training these models on 50 GPUs. The Neumann optimizer was evaluated using 50 GPUs, with a linear learning rate scaling and decay schedule. Adam was used as a baseline algorithm for comparison. Evaluation was based on final test accuracy and the number of epochs required to reach a specific accuracy level. Training curves showed oscillations early on, possibly due to hyperparameter mis-specification. The Neumann optimizer showed improved generalization and faster training compared to Adam and RMSProp on large scale image classification models. The models trained were robust to hyperparameter mis-specification, with a consistent 0.8-0.9% improvement in evaluation error across all models. This improvement can be traded-off for faster training by running the optimizer for fewer steps, resulting in a 10-30% speedup while maintaining baseline accuracy. Our Neumann optimizer outperforms both Neumann optimizer and RMSProp, showing improved generalization and faster training on large scale image classification models. It efficiently uses large batches, scaling up to mini-batches of size 32000 while maintaining model accuracy. The Neumann optimizer is a state-of-the-art method for utilizing large mini-batch sizes while maintaining model quality. It outperforms previous methods and shows improved generalization and faster training on large-scale image classification models. Regularization improves validation performance, and even without it, there is a performance improvement from running the Neumann optimizer. However, it did not show any speedup or quality improvements on a large-scale sequence-to-sequence speech-synthesis model called Tacotron. In this paper, a large batch optimization algorithm for training deep neural nets is presented. The algorithm implicitly inverts the Hessian of individual mini-batches and only requires tuning the learning rate as a hyperparameter. Experimental results show that the optimizer can handle mini-batch sizes up to 32000 without quality degradation. Interestingly, smaller mini-batch sizes lead to models that generalize better, improving validation error by 0.8-0.9% across various architectures without a drop in classification loss. The Neumann optimizer does not improve training loss, suggesting further investigation into this phenomenon. The Neumann optimizer does not improve training loss, indicating a different local optimum. Optimization and generalization cannot be decoupled in deep neural nets. Various Krylov subspace methods, like conjugate gradient, were not as effective as SGD in preliminary experiments. The Lanczos algorithm provides information about the eigenvalues of the mini-batch Hessian, using matrix-vector products to compute extremal eigenvalues efficiently. The Lanczos algorithm efficiently computes extremal eigenvalues of the mini-batch Hessian. By applying a shift operation to the Hessian, the most negative eigenvalue can be computed when there is an upper bound on the most positive eigenvalue. An experiment on a CIFAR-10 model showed that the maximum eigenvalue increases linearly during optimization, while the most negative eigenvalue decays towards 0. These observations are used to specify a parametric form for the \u00b5 parameter. In this section, the performance of the Neumann optimizer is compared with other stochastic optimizers on a convex problem involving synthetic binary classification. The input features were 100-dimensional vectors with a small weight decay of 10^-6. SGD, Adam, and Neumann optimizers were compared for batch sizes of 10 and 200 with different learning rates. The Neumann optimizer had \u03b1 and \u03b2 set to 0 since the problem is convex. Additionally, a true second-order Newton algorithm was studied with hyperparameter tuning. The study compared the performance of the Neumann optimizer with other stochastic optimizers on a convex problem. A true second-order Newton algorithm was used with hyperparameter tuning, setting a higher learning rate and allowing special access to Hessian estimates from a separate mini-batch. The Newton algorithm outperformed SGD and Adam in reducing the cost, especially at larger batch sizes. Removing regularizer terms resulted in lower quality output models, but still better evaluation metrics than a baseline RMSProp. In comparing different initializations and trajectories of the Neumann optimizer, the final output model quality is substantially better than RMSProp, despite variations in intermediate training losses and evaluation metrics."
}