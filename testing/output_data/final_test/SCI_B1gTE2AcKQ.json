{
    "title": "B1gTE2AcKQ",
    "content": "In this work, a novel learning algorithm called Wake-Sleep-Remember (WSR) is introduced for hierarchical Bayesian program induction. The algorithm infers concepts as short, generative, stochastic programs, learns a global prior over programs for generalization, and uses a recognition network for efficient inference. WSR successfully learns latent programs in symbolic domains like cellular automata and Gaussian process kernels, as well as on a new dataset called Text-Concepts for structured patterns in natural text data. The goal is to develop learning algorithms that can quickly grasp a concept from minimal examples. Few-shot learning research focuses on developing algorithms that can quickly learn a concept from minimal examples. Two main approaches are model-driven, which incorporates domain knowledge directly into the learner, and data-driven, which starts with minimal assumptions. Model-driven approaches use explicit models that are interpretable and natural to compose, while data-driven approaches rely on learning from data without pre-specifying biases. Data-driven approaches in few-shot learning acquire inductive biases from a large background dataset, utilizing models like Neural Statistian, MAML, and Prototypical Networks. These models rely on stochastic gradient descent for scalability. While model-driven approaches incorporate domain knowledge directly, data-driven approaches start with minimal assumptions and learn from data without pre-specifying biases. In this work, the authors introduce a learning algorithm called Wake-Sleep-Remember (WSR) that combines SGD with neurally-guided search to learn concepts represented as stochastic programs. They aim to bridge the gap between explicit models and meta-learning from large datasets. Additionally, a new dataset for few-shot concept learning in a natural domain of short text patterns is released, containing concepts like phone numbers, dates, email addresses, and serial numbers. The authors introduce a learning algorithm called Wake-Sleep-Remember (WSR) that combines SGD with neurally-guided search to learn concepts represented as stochastic programs. They aim to bridge the gap between explicit models and meta-learning from large datasets. Additionally, a new dataset for few-shot concept learning in a natural domain of short text patterns is released, containing concepts like phone numbers, dates, email addresses, and serial numbers. This work focuses on structured meta-learning and explainable AI in the domain of generative models of spoken words unsupervised. The goal is to infer the latent phoneme sequence that generated each recording, which can be a challenging computational problem. Training a Helmholtz Machine involves two models: a generative model describing the joint distribution of latent phonemes and sounds, and a recognition model inferring phonemes quickly from data. Algorithms for this are motivated by Variational Bayes, with the marginal likelihood of observations bounded by a KL divergence term. The Helmholtz machine learning process involves maximizing the evidence lower bound (ELBo) from the true posterior p(z|x) to the recognition model's approximate posterior q(z; x). Hinton et al. proposed an alternating optimization algorithm for updating the generative model p and recognition model q. The Variational Autoencoder (VAE) by BID12 offers an alternative solution for training q without relying on KL-divergence approximation. The authors propose a method to construct an unbiased approximation to the ELBo using a single sample from q, leading to a low variance estimate for the gradient. This gradient estimate can be used in SGD to train both q and p simultaneously towards the ELBo. When z is discrete, VAEs rely on the policy gradient estimator from reinforcement learning, which has high variance. This challenge has spurred research on variance reduction techniques. Training VAEs with discrete latent variables is a challenging research problem due to a bias-variance tension. The wake-sleep algorithm and VAEs have different approaches, with the former relying on approximate updates for q and the latter requiring stronger assumptions on the model. Both methods depend on accurate posterior inference by the recognition model, which can be unrealistic on difficult problems. Recent work aims to address the challenges of training VAEs with discrete latent variables by using importance weighting over many samples from q. This approach is bottlenecked by the practicality of evaluating samples per gradient step. An alternative approach involves introducing a 'memory' into the Helmholtz Machine to keep track of the best latent explanations for each observation. This method mitigates the difficulties of discrete inference and improves training compared to Wake-Sleep algorithms. WSR introduces a memory module to optimize the finite support of the recognition model q, allowing for better handling of discrete latent variables. Unlike VAEs and Wake-sleep, WSR learns a separate recognition model r and variational posterior q. This approach jointly trains p and q using an unbiased estimate of the variational objective, while also incorporating self-supervised training for the recognition model. The text discusses a new approach called WSR that uses a memory module to optimize the recognition model for discrete latent variables. It aims to learn a posterior distribution over latent programs for each instance and a prior capturing a global bias. The challenge lies in inferring programs from data and designing a learning algorithm that can handle weaker recognition models. The text introduces a new approach called WSR that utilizes a memory module to optimize the recognition model for discrete latent variables. It aims to learn a posterior distribution over latent programs for each instance and a prior capturing a global bias. The challenge is inferring programs from data and designing a learning algorithm that can handle weaker recognition models. The approach departs from the usual Helmholtz machines formulation by separating the recognition model from the variational distribution. The recognition model is trained self-supervised using samples from the prior, and a Memory module maintains the best values of z found for each x. Unlike Wake-sleep, samples from the recognition model are routed through the Memory module to train the generative model. The WSR approach utilizes a Memory module to optimize the recognition model for discrete latent variables. It maintains the best values of z found for each x, weighted by their joint probability. Resampling from the Memory module is used to train the generative model, ensuring updates decrease the KL divergence between the Memory and the true posterior. The training procedure is detailed in Algorithm 1, where the recognition network samples are used to update the Memory and train the generative model. The training procedure involves updating memory with recognition network samples, training the generative model with memory samples, training the recognition network with generative model samples, and updating the prior with a reference distribution. This allows the model to adjust its prior distribution as it learns from data. The algorithm involves placing a hyperprior over p(z) with a concentration parameter \u03b1 controlling confidence in the reference distribution. This hyperprior can be integrated into the training objective by adding an extra term. The algorithm is tested on learning rules for noisy 1-dimensional cellular automata from generated images. The algorithm involves choosing a binary vector to represent the update rule for cellular automaton and sampling the first generation randomly. Subsequent rows and cells are updated based on neighboring cells and a sampling process. Different datasets with varying complexity levels are created for cellular automaton rules. The algorithm aims to discover latent rules for images in datasets by learning a generative process and using a CNN for recognition. It compares performance with three baseline algorithms: VAE, Wake-Sleep, and No Recognition. The algorithm proposes updates to M i using samples from the prior z p \u223c p(z). Results show differences in performance between VAE, Wake-Sleep, and WSR on various datasets. WSR consistently learns accurate programs for image classification with > 99% accuracy, even on challenging datasets. The algorithm evaluates performance on different datasets, with WSR achieving high accuracy in image classification tasks. It then transitions to the task of finding explainable models for time-series data, drawing inspiration from BID4's Gaussian process kernel learning approach. The algorithm learns a hierarchical model over multiple time-series to produce interpretable and generalizable descriptions of the data structure. In this work, a hierarchical model is learned over time-series data from the UCR Time Series Classification Archive. The dataset consists of 1-dimensional time series data from various sources, normalized to zero mean and unit variance. A simple grammar over kernels is defined, aiming to learn a prior distribution over symbolic kernel structures and continuous variables. The latent program is defined as a symbolic kernel expression over characters. An LSTM prior is defined over these kernel expressions, along with prior distributions over continuous latent variables. Approximation of the marginal likelihood is done using variational inference. A hyperprior is placed on the distribution over kernel expressions. Examples of latent programs discovered are shown in Figure 5, revealing compositional structure. The model displays meaningful compositional structure in time series data and can make plausible extrapolations. The latent representation allows concepts to be easily translated into natural language. The model is tested on learning short text concepts from a new dataset called Text-Concepts. The dataset comprises 1500 concepts with 10 examples each, collected from spreadsheets on GitHub. The dataset Text-Concepts was created by selecting a subset of 1500 columns from online repositories on GitHub. Columns were filtered to remove certain types of data to promote diversity. The dataset aims to model concepts using Regular Expressions. The dataset Text-Concepts aims to model concepts using Regular Expressions. Each class in the dataset can be described by a latent regular expression with probabilistic, generative semantics. The likelihood of a regex generating a set of strings is evaluated using dynamic programming. The probability distributions are placed over random choices involved in generating a string from regex. The dataset Text-Concepts models concepts using Regular Expressions with probabilistic semantics. A network based on the RobustFill architecture generates a regex for a set of strings. The goal is to learn a regex for each set of strings while also learning a global distribution and a recognition model for inference on new sets. The regex expressions have probabilities for different operations. The WSR algorithm learns regular expressions with probabilistic semantics for concepts, achieving over 75% accuracy in a 100-way classification task. It crucially utilizes both recognition model and memory, outperforming other baselines. The VAE algorithm was ineffective in learning in this regex model. The VAE algorithm struggled to learn effectively in the regex model, even with control variates to reduce variance. Results from training a VAE using a different model architecture, VAE-LSTM, showed better optimization but lacked domain knowledge in its structure. The latent representations were less explicit and generalized less effectively to new examples. Investigating whether WSR learns a realistic inductive bias over concepts by sampling new concepts from the learned prior. In this paper, the Wake-Sleep-Remember algorithm is proposed to learn interpretable concepts from one or a few examples. The algorithm aims to achieve strong generalization by starting with rich domain knowledge and using a neural recognition model to guide search for high-probability programs. The algorithm includes a Helmholtz machine augmented with a persistent memory of discovered latent programs. The Wake-Sleep-Remember algorithm utilizes a neural recognition model and persistent memory to learn generalizable concepts across different domains. Comparison with baseline models shows that it effectively searches for programs and makes strong generalizations by leveraging domain knowledge and extensive background data."
}