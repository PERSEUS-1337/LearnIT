{
    "title": "rJxRJeStvB",
    "content": "Reinforcement learning methods for combinatorial optimization problems are being explored for multi-robot sequential assignment planning. The challenges include achieving near-optimal performance in large problems and transferability to varying numbers of robots and tasks. A method proposed in this paper shows success in addressing these challenges for robot/machine scheduling problems. The method proposed in the paper addresses challenges in robot scheduling problems by expressing them as random probabilistic graphical models, developing a mean-field inference method for Q-function inference, achieving transferability through sequential encoding of problem state, and resolving computational scalability with a heuristic auction-based Q-iteration fitting method. The method proposed in the paper addresses challenges in robot scheduling problems by expressing them as random probabilistic graphical models. It achieves 97% optimality with transferability in discrete-time, discrete space problems (MRRC) and maintains this optimality under stochastic contexts. The method is extended to continuous time, continuous space formulation, making it the first learning-based method with scalable performance in multi-machine scheduling problems. It achieves comparable performance to popular metaheuristics in Identical parallel machine scheduling (IPMS) problems. In this paper, the focus is on orchestration problems in multi-robot planning, where the task completion time is a key factor. The complexity of robot planning problems, even for single-robot scheduling like the Traveling Salesman Problem, raises scalability concerns. Scalable heuristic methods have been developed to address these challenges. The focus is on orchestration problems in multi-robot planning, where task completion time is crucial. Scalable heuristic methods have been developed for deterministic multi-robot planning problems, such as the Traveling Salesman Problem, to address scalability concerns. The authors observed that combinatorial optimization problems can be formulated as sequential decision-making problems, relying on estimates of future costs for task sequences. Their solution framework is based on three key assumptions. The authors propose a solution framework for multi-robot planning that utilizes a probabilistic graphical model (PGM) and mean-field inference method called structure2vec. They introduce a theory for random PGM and develop a new structure2vec iteration for state representation and mean-field inference. Additionally, they focus on sequential encoding of information for transferability in combinatorial optimization problems. The authors introduce a new structure2vec iteration for state representation and mean-field inference in multi-robot planning. They focus on transferability in combinatorial optimization problems through a two-step hierarchical mean-field inference process. The first step infers graphical distances between tasks, while the second step infers joint assignments of robots. Transferability in the second step is achieved through the scale-free characteristic of fitted Q-iteration. Additionally, an auction-based assignment approach is proposed to address the computational complexity of action selection. The authors propose a heuristic auction for action selection based on Q(s, a) inference transferability, reducing computational complexity while improving performance. They also consider processing and setup times for tasks, enhancing task completion efficiency. The authors solve the IPMS problem for make-span minimization by minimizing total time from start to completion of last task. They formulate multi-robot/machine planning as sequential joint assignment decision problems and use a Q-function based policy for joint assignment. New results in random PGM-based mean-field inference methods are provided. In Section 3.1, new results are presented on random PGM-based mean-field inference methods and an extension of the graph-neural network based inference method called structure2vec. Section 3.2 discusses how precise and transferable Q(s t , a t ) inference is achieved through careful encoding of information using the extended structure2vec. The transferability of the Q(s t , a t ) inference method in Section 3.3 enables a good action choice heuristic with polynomial computational complexity. In mean-field inference, a surrogate distribution is found with the smallest Kullback-Leibler distance to the original joint distribution. This surrogate distribution is used to solve the inference problem, and can be computed analytically with PGM information. However, it is unrealistic to assume knowledge of PGM probability distributions in most inference problems. Dai et al. (2016) introduced structure2vec to address this limitation, suggesting that inference problems with graph-structured data can be viewed as a specific PGM structure with two types of random variables. The structure2vec method by Dai et al. (2016) embeds mean-field inference into neural networks for PGMs with graph-structured data, overcoming the need for explicit probability distributions. The structure2vec method by Dai et al. (2016) utilizes fixed point iterations of neural networks on vectors {\u03bc k} for robot scheduling as random PGM-based mean-field inference. In this context, inference problems in robot scheduling induce a 'random' PGM structure, specifically a 'random' Bayesian Network. The experiment involves sequential decision making using policy \u03c6 to determine how robots serve all remaining tasks in a given sequence, defining each task as a random variable X i. In robot scheduling, a random PGM is defined for inference problems, where tasks are represented as random variables X i. The relationships among these variables form a Bayesian Network, with semi-cliques denoting conditional dependencies between variables. The semi-clique D ij acts as a clique when a robot choosing task t i leads to task t j next. Mean-field inference with random PGM simplifies inference by focusing on presence probabilities of semi-cliques. This method allows for the development of structure2vec inference for random PGM, particularly when semi-cliques involve two random variables. The semi-clique D ij represents a clique when a robot selects task t i followed by task t j. Mean-field inference simplifies by focusing on semi-clique presence probabilities. Structure2vec inference for random PGM is developed for semi-cliques involving two random variables. The semi-clique D ij represents a clique when a robot selects task t i followed by task t j. Mean-field inference simplifies by focusing on semi-clique presence probabilities. Structure2vec inference for random PGM is developed for semi-cliques involving two random variables. ij as {e is an unbiased and consistent estimator of E[f ( ij , age i , age j )]. For each sample k, for each task i and task j, we form a vector of u k ij = (e k ij , age i , age j ) and compute We obtain {p ij } from {g ij } using softmax. Algorithm details are in Appendix F. In this section, we show how Q(s t , a t ) can be precisely and transeferably inferred using a two-step structure2vec inference method (For theoretical justifications on hierarchical variational inference, see Ranganath et al. (2015) ). We here assume that we are given (T t , E T T t ) and inferred {p ij } so that Corollary 1 can be applied. For brevity, we illustrate the inference procedure for the special case when task completion time is deterministic (Appendix G illustrates how we can combine random sampling to inference procedure to deal with task completion times as a random variable). Step 1. Distance Embedding. The output vectors {\u03bc 1 k } of structure2vec embeds a local graph information around that. The first step of the structure2vec algorithm focuses on embedding information about robot locations around task nodes to infer the relative graphical distance from robots to each task. The input for this step only includes robot assignment information. The output vectors from this step contain sufficient information about the relative graphical distance from all robots to each task. The second step involves inferring the value likely present in the local graph around each task based on the output vectors from the first step. The structure2vec algorithm embeds information about robot locations around task nodes to infer the relative graphical distance from robots to each task. The output vectors from the first step contain information about the 'age' of tasks. In the second step, the algorithm embeds a local graph structure around each node to determine the value likely present in the local graph. Finally, by aggregating the embedding vectors for all nodes, the algorithm infers the 'value likeliness' of the global graph and maps it into Q(s t , a t) using a neural network layer. The inference steps in the algorithm involve determining the value likelihood in a local graph based on the ratio of robots to tasks. Overestimation or underestimation occurs depending on this ratio, but it does not affect the Q-function based action decision as long as the order of Q-function values remains the same. This order invariance is crucial for action choice at each time-step. The Q-function based action choice involves finding the action with the largest Q-value at each time-step. However, in a multi-robot setting, the computation complexity grows exponentially with the number of robots and tasks. To address this issue, an efficient heuristic auction-based policy (ADP) is proposed, denoted as \u03c6 Q \u03b8. This policy iterates between a bidding phase and a consensus phase to find the optimal action. The auction-based policy \u03c6 Q \u03b8 involves a bidding phase where robots bid on tasks based on Q-values, and a consensus phase where the auctioneer updates the matching. The bidding process considers unassigned robots and tasks, with the highest value bid winning. This iterative process continues until no more edges can be added to the matching. The auction-based policy involves bidding and consensus phases iterated until no more edges can be added. The central decision maker selects the winning bids. The computational complexity is polynomial, and empirical results show near-optimal performance. In fitted Q-iteration, \u03b8 is updated using auction-based policy instead of max-operation to minimize errors. In the Auction-fitted Q-iteration framework, exploration in parameter space using neural network parameters is suggested to avoid catastrophic results in combinatorial problems. This method, originally for policy gradient RL, can be useful in auction-fitted Q-iteration. In auction-fitted Q-iteration, exploration involves applying random perturbations to neural network parameters \u03b8 in structure2vec. The resulting perturbation affects the Q-function used for decision making via the auction-based policy. Updates for the surrogate Q-function occur after each problem is complete. Simulation experiments are conducted in a discrete time, discrete state environment using maze generators. Movement success rates vary based on deterministic or stochastic environments. The robot's movement success rates vary in different environments. Routing problems are solved using Dijkstra's algorithm for deterministic environments and dynamic programming for stochastic environments. The central decision maker uses task completion time samples to determine optimal routing policies. Two reward rules are considered: linearly decaying rewards and nonlinearly decaying rewards. Performance tests were conducted under four environments: deterministic/linear. Performance tests were conducted under four environments: deterministic/linear rewards, deterministic/nonlinear rewards, stochastic/linear rewards, and stochastic/nonlinear rewards. Three baselines were used: exact baseline, heuristic baseline, and indirect baseline. An exact optimal solution exists for deterministic with linearly decaying rewards, while for other cases, a conservative approach is taken. An indirect baseline was constructed using the Sequential greedy algorithm (SGA). The Sequential greedy algorithm (SGA) is used as a baseline for task allocation in both linear and non-linear reward scenarios. Mean task completion time is used for task allocation in stochastic environments. An indirect baseline is constructed by comparing the performance of our method to SGA in experiments with deterministic-linearly decaying rewards. Experimental results show near-optimal performance for our proposed method in various robot and task configurations. The scalability and transferability of the learning algorithm are tested by comparing training requirements for different problem sizes and evaluating performance across varying robot and task configurations. The algorithm shows consistent performance across different problem sizes and demonstrates transferability when tested on larger problem instances. The experiment tested the transferability of the learning algorithm across different problem sizes. Results showed a small performance loss in lower-direction transfer tests and up to 4 percent loss in upper-direction transfer tests. An ablation study removed three components from the method to evaluate their impact. The experiment tested the transferability of the learning algorithm across different problem sizes, showing a small performance loss in lower-direction transfer tests and up to 4 percent loss in upper-direction transfer tests. An ablation study removed components from the method to evaluate their impact, with the full method achieving near-optimal performance in a deterministic/linearly decaying rewards setting. The method claims scalable performance among machine scheduling problems, focusing on comparable performance for large problems rather than superiority over heuristics specifically designed for IPMS. The experiment tested the transferability of the learning algorithm across different problem sizes, showing a small performance loss in lower-direction transfer tests and up to 4 percent loss in upper-direction transfer tests. An ablation study removed components from the method to evaluate their impact, with the full method achieving near-optimal performance in a deterministic/linearly decaying rewards setting. The method claims scalable performance among machine scheduling problems, focusing on comparable performance for large problems rather than superiority over heuristics specifically designed for IPMS. Specifically designed for IPMS, processing times and setup times are determined using uniform distributions. Metaheuristics like Greedy Descent, Guided Local Search, Simulated Annealing, and Tabu Search are used for comparison with the algorithm's results. Results for different machine and job combinations are provided in Table 4, showing the makespan obtained by the algorithm divided by the baseline makespan. Our method achieves scalable performance for multi-robot/machine scheduling problems, demonstrating success in both challenges of scalability and transferability. By identifying robot scheduling problems as random PGMs, we developed a meanfield inference theory and extended the structure2vec method. A heuristic auction enabled by transferability overcomes limitations of fitted Q-iteration. Experimental evaluation shows success for MRRC problems in deterministic/stochastic environments, making our method the first learning-based algorithm to achieve scalable performance. Our method achieves scalable performance for multi-robot/machine scheduling problems, demonstrating success in both challenges of scalability and transferability. It can be easily extended to ride-sharing or package delivery problems, where the objective is to maximize total collected user utility. The completion time distribution of each move is crucial in determining task completion time. In multi-robot/machine scheduling problems, task completion time is the sum of two random variables. Ride-sharing and package delivery problems can be formulated as such tasks. In a continuous state/continuous time space formulation, robots finish tasks and assignment decisions are made at 'decision epochs'. Task completion time in multi-robot scheduling consists of travel time, setup time, and processing time. Robots can be reassigned during travel or setup phases but not during processing. Each robot is given a set of tasks it can assign itself at each decision epoch. This problem is modeled as a Markov Decision Problem with states, actions, and rewards defined based on robots and tasks. In multi-robot scheduling, task completion time includes travel, setup, and processing time. Robots can be reassigned during travel or setup phases but not during processing. Each robot has a set of tasks it can assign itself at each decision epoch. The problem is modeled as a Markov Decision Problem with states, actions, and rewards based on robots and tasks. The action at each decision epoch is the joint assignment of robots satisfying constraints. An action is defined as a maximal bipartite matching in a bipartite sub-graph of the graph. In multi-robot scheduling, task completion time includes travel, setup, and processing time. Robots can be reassigned during travel or setup phases but not during processing. Each robot has a set of tasks it can assign itself at each decision epoch. The problem is modeled as a Markov Decision Problem with states, actions, and rewards based on robots and tasks. The action at each decision epoch is the joint assignment of robots satisfying constraints in a bipartite sub-graph of the graph. Reward in MRRC is determined by the age of a task when serviced, denoted as R(k). The objective is to find an optimal assignment policy \u03c6 * given an initial state s 0. In this paper, the IPMS problem with 'sequence-dependent setup time' is discussed, where task completion time is the sum of setup time and processing time. The objective is to minimize the total time spent from start to finish all tasks by making assignment decisions for free machines at decision epochs. The problem discussed involves task completion time for machines, consisting of processing and setup time components. Machines can be reassigned during setup phase but not during processing phase. Each epoch, a machine is given a set of tasks it can assign based on its phase. This is formulated as a Markov Decision Problem with states, actions, and rewards defined. The problem involves task completion time for machines, with processing and setup time components. Machines can be reassigned during setup but not processing. Markov Decision Problem formulated with states, actions, and rewards defined. Defined assignment policy as a function mapping state to action. The problem involves finding an optimal assignment policy \u03c6 for a robot scheduling problem, where robots serve tasks in a specific sequence. Different scenarios can be defined based on how tasks are serviced by robots. Random variables are used to characterize tasks, such as the time they are serviced. The goal is to determine the relationship between scenarios and task characteristics. In a robot scheduling problem, the relationship between scenarios and task characteristics is determined using random variables to characterize task completion time. Conditional independence is observed, where the completion time of a task is only dependent on the previous task and the assigned task. This relationship is represented in a Bayesian Network, which is a graph showing the special relationship among random variables. Given all possible PGMs on {X k} and semi-cliques C, the set of factorizations is denoted as F = {S 1, S 2, ..., S N}. A factorization with index k is denoted as S k \u2286 C. If we are provided with P({S = S m}), we can analyze the relationships within the Bayesian Network."
}