{
    "title": "rkZzY-lCb",
    "content": "Methods for calculating dense vector representations for features in unstructured data have been successful for knowledge representation. Feat2Vec is a method that calculates embeddings for data with multiple feature types, ensuring they exist in a common space. In supervised learning, it offers advantages over other methods by enabling higher prediction accuracy and avoiding the cold-start problem. Feat2Vec outperforms existing algorithms in unsupervised learning by leveraging the structure of multiple feature types to create dense embeddings with lower dimensionality. These embeddings have advantages such as enabling more efficient training and unsupervised learning, and can be used for downstream prediction tasks. Word2Vec is another algorithm that provides useful embeddings for various prediction tasks. The loss function of the continuous bag of words (CBOW) algorithm of Word2Vec is tuned to predict the next word of a sequence. The embeddings produced are used for tasks like analogy solving and sentiment analysis. Supervised methods like matrix factorization produce task-specific embeddings that are highly tuned to a prediction task. Matrix factorization and Word2Vec have limitations in calculating embeddings for items not available during training. Feat2Vec is a novel method proposed in this paper that allows calculating embeddings of arbitrary feature types from both supervised and unsupervised data. It is the first algorithm that can calculate general-purpose embeddings for arbitrary feature types, not tuned for a specific prediction task. Supervised Feat2Vec is a flexible method for calculating embeddings of unseen items using alternative textual descriptions. Factorization Machine BID22 is a successful method for general-purpose factorization, formulated as an extension to polynomial regression. Factorization Machine replaces individual pairwise parameters with a vector of latent factors for each feature, reducing the number of parameters to estimate. The dot product measures the similarity between features' latent factors, allowing for a more expressive model with shared latent structures. Feat2Vec extends the Factorization Machine model by allowing grouping of features and enabling arbitrary feature extraction functions. It introduces a supervised method for learning Feat2Vec and a novel unsupervised training procedure. The framework proposes extending factorization machine with neural methods by defining feature groups to introduce structure into feature interactions. Feat2Vec extends the Factorization Machine model by introducing feature groups and a feature extraction function \u03c6 i for each group. It builds r latent factors from these features, where interactions only occur between different feature groups. This approach allows reasoning on a higher level of abstraction by grouping features together, enabling interactions between feature group embeddings. Feat2Vec extends the Factorization Machine model by introducing feature groups and a feature extraction function \u03c6 i for each group. Entities interact with each other via the output of \u03c6 only, allowing for arbitrary processing of subfeatures. For example, individual words in a document can be grouped into a \"document\" feature group, with the extraction function \u03c6 capturing attributes of the document as a whole. This novel model is compared to existing factorization methods in Figure 1. Feat2Vec extends the Factorization Machine model by introducing feature groups and a feature extraction function \u03c6 i for each group. The model allows for arbitrary processing of subfeatures, such as grouping individual words in a document into a \"document\" feature group. Feat2Vec can be used to address the cold-start problem by treating words as indexed features within a structured feature group \u03ba w. Feat2Vec extends the Factorization Machine model by introducing feature groups and a feature extraction function \u03c6 for each group. It allows for arbitrary processing of subfeatures, like grouping words in a document into a \"document\" feature group. This approach enables fast inference for cold-start documents using neural networks within factorization machines. The text discusses the limitations of using neural functions for fast inference in cold-start documents and the inability to interpret embeddings as containing latent factors. It also mentions the potential to combine this approach with Feat2Vec. The parameters of a deep factorization model can be learned using training data to minimize a loss function for labeling and classification tasks. The text discusses regularization control, optimization for different tasks, mini-batch updates in neural models using Keras, and the use of ADAM optimization algorithm. It also mentions the use of binary classifiers for large label numbers and multi-label classification tasks. Feat2Vec is used for multi-label classification tasks with a binary output. Negative examples are sampled for training, and different sampling strategies can be used to improve validation error. Feat2Vec is utilized for multi-label classification tasks with a binary output. In an unsupervised setting, embeddings are learned using Feat2Vec with only observed data as the training dataset. Implicit sampling is used to provide negative examples for the model, particularly in scenarios with high-dimensional feature groups. Feat2Vec is used for multi-label classification tasks with binary output. Negative Sampling in Word2Vec lacks theoretical guarantees. A new implicit sampling method is introduced for learning unsupervised embeddings for structured feature sets. Unlike Word2Vec, this method does not constrain feature types to be words and can reason on more abstract entities in the data. The implicit sampling algorithm for unsupervised Feat2Vec involves generating negative labels for positive records by iterating over feature groups in the dataset. This method allows reasoning on abstract entities in the data beyond word constraints. The sampling method for unsupervised learning involves iterating over observations in the training dataset. It randomly selects feature groups from noise distributions to create negative observations. The complexity of a feature extraction function is calculated to sample feature groups based on their complexity. The complexity of a feature group is determined by the number of parameters associated with it, placing more weight on features with more parameters. The sampling probabilities for each feature group are influenced by a hyperparameter \u03b1 1, which can flatten the distribution or sample proportionally to complexity. Parameters from intermediate layers, such as convolutional layers, also contribute to a feature group's complexity. The appendix visualizes how feature sampling rate varies with hyperparameters for features of different complexities. Sampling values within feature groups is done using a strategy similar to Word2Vec, utilizing the empirical distribution of values. The method may generate negatively labeled samples by chance, which can be addressed through Negative Sampling or Noise Contrastive Estimation (NCE). NCE loss is optimized for unsupervised learning of embeddings. For unsupervised learning of embeddings, a NCE loss function is optimized to adjust the statistical model for random negative labels. Parameters learned during training are represented by \u03b8. The burden of NCE includes calculating a partition function Zx for each unique record type x to transform the probability of positive or negative labels into a well-behaved distribution. This process requires a significant amount of computation. The burden of calculating a partition function Zx for each unique record type x in unsupervised learning of embeddings can be avoided by setting Zx = 1 in advance, as shown by BID18. This approach does not significantly impact the model's performance, as the model can effectively learn the probabilities itself. The new structural probability model includes a score function s(.) for record x and a total probability function P Q (.) for drawing negative samples. The loss function L optimizes parameter values \u03b8. Feat2Vec optimizes parameters \u03b8 for feature extraction functions \u03c6 while considering negative sample probabilities. It is equivalent to optimizing a convex combination of loss functions from n Factorization Machines, resulting in n multi-label classifiers for different target feature groups. The text discusses the optimization of parameters for feature extraction functions in Feat2Vec, showing proof in Appendix 1. It addresses working hypotheses for evaluating supervised embeddings and describes the experimental setup with development and test sets. Results are reported on a single training-test split for a multi-label classification task, using Area Under the Curve (AUC) as an evaluation metric. Sampling of negative labels is done based on label frequency to prevent bias towards popular labels. The evaluation strategy ensures unbiased performance measurement using AUC as a metric for classification and ranking tasks. Regularization is avoided to prevent overfitting, with early stopping used instead. Code for experiments is shared online for reproducibility, and a CNN is used for text feature extraction. Featuring a Convolutional Neural Network (CNN) for text extraction, the study compares Feat2Vec with Collaborative Topic Regression (CTR) on the CiteULike dataset. Hyper-parameters are set based on previous guidelines, with a focus on r values of {5, 10, 15} for CTR's training time optimization. Feat2Vec outperforms CTR in terms of training time and performance on unseen documents, achieving an AUC of 0.9401 on warm-start condition and degrading to 0.9124 on unseen documents. Feat2Vec can also be trained over ten times faster using GPUs. Further optimization of hyper-parameters could lead to greater improvements. Comparison with DeepCoNN, a deep network for text incorporation, is also discussed. Feat2Vec is a state-of-the-art method for predicting customer ratings using textual reviews. It outperforms DeepCoNN in terms of performance and training time, achieving a significant improvement in mean squared error over Matrix Factorization. Feat2Vec builds feature groups for item identifiers, users, and review text, providing a more efficient and general approach compared to DeepCoNN. Feat2Vec is more efficient than DeepCoNN as it duplicates text less frequently during training epochs. The performance of unsupervised embedding algorithms can be evaluated through a ranking task using unseen data. Feat2Vec's performance is compared to Word2Vec's CBOW algorithm in this task. In the evaluation approach, cosine similarity of embeddings of associated entities is compared using a movie dataset. Feat2Vec and MF were trained on GPU, while CTR was trained on CPU. Similarity of movie directors and actors in the same film is compared. Textbook and user embeddings are evaluated for rankings based on mean percentile rank. The Internet Movie Database (IMDB) dataset contains information on 465,136 movies, focusing on writers, directors, and cast members. An anonymized dataset from a leading technology company with 57 million observations and 9 categorical variables is used for educational services. See the appendix for experimental setup details. In a dataset with 57 million observations and 9 categorical feature types, each observation represents a user's interaction with a textbook. The model uses cast member embeddings to predict the actual director of a film in the test set, ranking directors based on cosine similarity. Feat2Vec outperforms CBOW in predicting directors, as shown in the evaluation results presented in TAB1. Feat2Vec significantly outperforms CBOW in predicting directors, with a Top-1 Precision metric of 2.43% compared to CBOW's 1.26%. The distribution of rankings is explored in detail in the appendix. Feat2Vec excels in predicting real-valued features with complex extraction functions, showcasing its advantage over token-based algorithms like Word2Vec. The model evaluates movie ratings by comparing IMDB rating embeddings to director embeddings, calculating Root Mean Squared Error (RMSE). In an experiment, the flattening hyperparameter \u03b11 is varied to analyze its impact on predicting movie ratings based on director embeddings. A low \u03b11 improves rating quality but may result in lower quality director embeddings. Results are compared to Word2Vec's CBOW algorithm and a random uniform variable baseline. Feat2Vec outperforms CBOW in predicting movie ratings across all hyper-parameter settings, with the algorithm's performance showing little sensitivity to hyperparameter choice. Field-Aware Factorization Machine BID10 allows different weights for feature interactions but lacks feature groups and extraction functions like Feat2Vec. Other algorithms have been proposed for generating continuous representations of entities such as biological sequences, network graphs, and machine translation. Generative Adversarial Networks (GANs) have been used for unsupervised image embeddings effective for classification. Generative Adversarial Networks (GANs) have been used for unsupervised image embeddings effective for classification and generating natural language. A new algorithm called StarSpace is being developed with similar goals, but limited to bag of words at the moment. Feat2Vec can learn embeddings for all feature values, while StarSpace samples a single arbitrary feature which may not generalize well. Future work may compare with StarSpace. Feat2Vec is a general-purpose method that decouples feature extraction from prediction for datasets with multiple feature types. In supervised settings, it outperforms algorithms designed specifically for text, even using the same feature extraction CNN. In unsupervised settings, Feat2Vec's embeddings are effective. Feat2Vec's unsupervised embeddings capture relationships across features better than Word2Vec's CBOW algorithm. It exploits dataset structure for more sensible embeddings and is the first method to calculate continuous representations of data with arbitrary feature types. Future work could focus on reducing human knowledge requirements by automatically grouping features or choosing feature extraction functions. The method is evaluated on 2 datasets each in supervised and unsupervised settings. The evaluation of supervised and unsupervised Feat2Vec on 2 datasets each shows promising results towards general-purpose embedding models. Testing sets were defined separately for the IMDB and educational datasets to ensure unbiased evaluation. In the evaluation of Feat2Vec on IMDB and educational datasets, cross-validation was performed on the loss function. Regularization did not significantly impact results. Entity pairs in the test dataset were ranked based on cosine similarity. Different target and input embeddings were used for each dataset. For training Feat2Vec on the IMDB dataset, \u03b1 values were set to 3/4, while for the educational dataset, \u03b1 values were set differently. The embeddings were learned with a dimensionality of r = 50 under both algorithms. CBOW was implemented for unsupervised experiments, and extraction functions were used to represent features in the IMDB dataset. Multiple values for features were constrained to a maximum of 10 levels for model input. We truncate features to no more than 10 levels, pad sequences with a \"null\" category, and use CBOW Word2Vec algorithm for training. Unique r-dimensional embeddings are learned for categorical variables without one-hot encodings, allowing multiple categories to be active. The text discusses the process of preprocessing text data by removing non alpha-numeric characters, stopwords, and stemming words. It also explains how real-valued features are passed through a neural network to generate embeddings. The approach is ordering-invariant and aims to extract information efficiently. The Feat2Vec algorithm utilizes real-valued features with relu activation functions to map numeric values to a high-dimensional embedding space. It outperforms Word2Vec in capturing nonlinear relations, as shown in a ranking comparison with CBOW using the IMDB dataset. Feat2Vec generally yields lower rankings, except in the upper tail of the ranking space where CBOW excels. Feat2Vec outperforms CBOW in the upper region of rankings (1 to 25), especially up to rank 8. The algorithm excels in extracting information for entities that are sparse in the training data. The gradient for learning embeddings with Feat2Vec is a convex combination of gradients from targeted Factorization Machines for each feature in the dataset. The loss function for the Feat2Vec model is a convex combination of targeted classifiers for each feature, with weights proportional to feature group sampling probabilities. The feature extraction function \u03c6 used in experiments for supervised tasks involves using 1000 convolutional filters with a width of 3 words. The network architecture includes building a vocabulary of common words, converting text to sequences of one-hot encodings, and padding or truncating as needed. The input passage of text is processed through layers including an embedding layer assigning a vector to each word, convolutional filters extracting features, and applying ReLU activation. The input passage of text is processed through layers including an embedding layer assigning a vector to each word, convolutional filters extracting features, and applying ReLU activation. To enforce learning from the features of the text, 1-max pooling is applied to the output of the filters, yielding a vector of length F independent of the text's length. Higher-level features are learned using a fully connected layer with p units and ReLU activation, with dropout regularization during training to prevent co-adaptation. The final embedding for x j is computed for factorization. The final embedding for x j is computed by a dense layer with r output units and an activation function, where r is the embedding size of indexable items. The maximum vocabulary size is set to 100,000 words, and input embedding size to 50 for all experiments. The number of convolutional filters is set to 1,000, and the dropout rate to 0.1. The maximum sequence length is chosen based on typical document lengths. The CNN architecture used for DeepCoNN is similar to the previous section, with PReLU activations used for the final layer to avoid 'dying' ReLU units. Hyper-parameters include 100 convolution filters, 50 units for the fully connected layer, word embedding size of 100, vocabulary size of 100,000, and maximum document length of 250. Feat2Vec is compared with Collaborative Topic Regression using embedding sizes of 5, 10, and 15."
}