{
    "title": "ryUlhzWCZ",
    "content": "In this paper, the authors propose combining imitation and reinforcement learning using reward shaping with an oracle. They show that the cost-to-go oracle can shorten the planning horizon based on its accuracy, bridging the gap between imitation and reinforcement learning. They introduce Truncated HORizon Policy Search (THOR) to find policies that maximize reshaped rewards over a finite planning horizon when the oracle is sub-optimal. The authors introduce Truncated HORizon Policy Search (THOR) to maximize reshaped rewards over a planning horizon when the oracle is sub-optimal. THOR outperforms RL and IL baselines, showing superior performance even with sub-optimal oracles. Imitation learning reduces sample complexity by leveraging expert demonstrations, improving learning efficiency in RL tasks. However, learned policies may be limited to the expert's performance. Previous approaches like DAgger and Aggregation with Values provide theoretical guarantees. Combining imitation learning (IL) and reinforcement learning (RL) can overcome sample inefficiencies and surpass sub-optimal expert performance. Previous approaches like DAgger and Aggregation with Values offer theoretical guarantees but may result in sub-optimal policies with a sub-optimal expert. BID5 attempted to combine IL and RL by interleaving incremental updates to achieve better performance than the expert policy. In this work, a novel approach is proposed to combine imitation learning (IL) and reinforcement learning (RL) through Reward Shaping BID16. By using a cost-to-go oracle for cost shaping, a new MDP with an optimal policy can be generated, potentially improving over the expert policy. Cost shaping with the cost-to-go oracle in imitation learning (IL) and reinforcement learning (RL) combines to generate a new MDP with an optimal policy, improving over the expert policy. The effectiveness of cost shaping with an imperfect estimator of the cost-to-go is studied, showing that it shortens the learner's planning horizon based on the accuracy of the oracle. The proposed strategy, named Truncated HORizon Policy Search with cost-to-go oracle (THOR), combines imitation learning (IL) and reinforcement learning (RL) by shaping the cost using an expert's cost-to-go oracleV e and then optimizing over a truncated planning horizon. The proposed strategy, Truncated HORizon Policy Search with cost-to-go oracle (THOR), combines imitation learning (IL) and reinforcement learning (RL) by optimizing over a truncated planning horizon. A gradient-based algorithm is proposed for discrete MDPs, guaranteeing a policy that outperforms the expert with quantifiable gap. THOR is shown to be more sample efficient than TRPO-GAE and learn better policies than AGGREVATE with access to an imperfect cost-to-go oracle. The BID8 algorithm focuses on a model-based RL approach for maximizing k-step rewards with a termination value approximation. Truncating the planning horizon can trade off bias and variance in estimating the reward-to-go when using an imperfect cost-to-go oracle. This bias-variance tradeoff has been extensively studied in Temporal Difference Learning literature. The bias-variance tradeoff in Temporal Difference Learning literature and policy iteration literature has been extensively studied. Ng's dissertation discusses how reward shaping can decrease the discount factor of the original MDP without losing optimality. This work considers trading off between the hardness of the reshaped MDP and the optimality of the learned policy, suggesting a path towards understanding previous imitation learning approaches through reward shaping. This work aims to unify imitation learning and reinforcement learning by adjusting the planning horizon based on the expert oracle's proximity to the optimal value function. It also includes a lower bound analysis on the performance limitation of AGGREVATE with an imperfect oracle. Additionally, a model-free, actor-critic algorithm for continuous state and action spaces is proposed for optimizing Markov Decision Processes. The paper introduces a model-free actor-critic algorithm for optimizing Markov Decision Processes by adjusting the planning horizon based on the expert oracle's proximity to the optimal value function. The objective is to find the optimal policy \u03c0* that minimizes the value function V\u03c0(s) for all states s in S. Access to a cost-to-go oracle V_e(s) is assumed, which may not be equal to the optimal value function V*M0. In the experiment, the focus is on learning a cost-to-go oracle V_e(s) using TD methods from expert demonstrations. By reshaping the cost function in the MDP, the optimal policy on the new MDP is shown to be the same as the optimal policy on the original MDP. This allows for using V_e(s) as a potential function for cost shaping in Inverse Reinforcement Learning. Using the cost-to-go oracle V_e as a potential function for cost shaping does not change the optimal policy. The alternative objective for policy search provided by Eq. 2 is as difficult as the original problem. Previous IL algorithms like AGGREVATE ignore temporal correlations and directly perform policy iteration over the expert policy, guaranteeing that a greedy policy performs at least as well as the expert. The greedy policy performs as well as the expert when the expert is optimal. However, if the cost-to-go oracle is imperfect, the resulting policy may not be close to the optimal one. The dependency of the planning horizon on the oracle is analyzed in this section. The imperfect oracle in IL algorithms like AGGREVATE and AGGREVATE D BID19 BID25 may only learn a policy that is \u03b3 /(1\u2212\u03b3) away from optimal. An example is provided where the induced policy from the imperfect oracle can be significantly different from the optimal policy, leading to mistakes. The proof and details can be found in Appendix A. AGGREVATE aims to optimize the one-step cost of a reshaped MDP using a cost-to-go oracle. When the oracle is imperfect, AGGREVATE may suffer from lower bounds due to its greedy nature. However, with a perfect oracle, being greedy on one-step cost is justified. AGGREVATE aims to optimize the one-step cost of a reshaped MDP using a cost-to-go oracle. When the oracle is imperfect, lower bounds may be encountered due to its greedy nature. However, with a perfect oracle, being greedy on one-step cost makes perfect sense. The optimal policy on the reshaped MDP only optimizes the one-step cost, shortening the planning horizon to one. The policy search problem aims to minimize the total cost of a policy over a finite number of steps. By defining cost shaping and using a telescoping sum trick, the k-step disadvantage with respect to the cost-to-go oracle can be minimized. The policy class is assumed to be rich enough to always have a policy that minimizes the disadvantage at every state. Optimizing the disadvantage with k > 1 can help outperform the expert, as shown in Theorem 3.2. Theorem 3.2 states that optimizing a k-step disadvantage with k > 1 can lead to a policy that outperforms the expert, as compared to the policy induced by the oracle. This highlights the importance of considering a multi-step disadvantage in policy optimization. Our theorem shows a performance gap between optimizing Eq. 6 for k > 1 and the oracle-induced policy, which was not addressed in previous work. Setting k = 1 aligns with AGGREVATE's goal of optimizing the expert's disadvantage. Gradient computation involves reshaping costs in trajectories and updating the policy parameter. Finite horizon optimization is favored over infinite horizon due to lower variance. The main theorem provides a tradeoff between the optimality of the solution and the difficulty of the optimization problem. Using a cost-to-go oracle, a reshaped MDP's cost function is obtained, leading to the proposal of THOR: Truncated Horizon Policy Search. THOR focuses on minimizing total cost over a truncated horizon instead of optimizing over an infinite long horizon. THOR is derived from the insight obtained in the previous section, focusing on minimizing total cost over a k-step time window. To find an optimal policy, a k-step truncated value function and state-action value function are defined on the reshaped MDP. Instead of enumerating all states, an approximate policy iteration approach is used to minimize the weighted cost over the state space for MDPs with large or continuous state spaces. In the context of minimizing total cost over a k-step time window, parameterized policy \u03c0 can be optimized using gradient-based update procedures like Stochastic Gradient Descent. The exploration policy can be the currently learned policy mixed with a random process to encourage further exploration. The policy gradient is computed by replacing the exploration distribution with the average state distribution induced by executing the current policy on the MDP. The gradient used in Alg. 1 involves replacing the expectation with empirical samples from \u03c0 n and Q \u03c0,k M with a critic approximated by Generalized Advantage Estimator (GAE) \u03c0,k M. The proposed gradient formulation in Alg. 1 is similar to Truncated Back-Propagation Through Time and back-propagates the effectiveness of the cost through time. The proposed gradient formulation in Alg. 1 is a truncated version of the classic policy gradient, back-propagating the cost through time at most k-steps. This approach eliminates long temporal correlations between costs and old actions, providing a middle ground between IL and RL. When k = 1 and V e = V * M0, it can be seen as No Back-Propagation Through Time. THOR performs gradient descent with a specific format to discourage low advantage actions over the optimal policy. It is equivalent to a pure RL approach when the oracle information is not useful. THOR was evaluated on robotics simulators from OpenAI Gym, reporting rewards instead of costs. In this section, OpenAI Gym BID4 reports rewards instead of costs and compares against TRPO-GAE and AGGREVATED BID25. Oracles are simulated by training TRPO-GAE to obtain an expert policy \u03c0 e, collecting trajectories, and training a value function V e. The experiments focus on using pre-trained V e for reward shaping without access to experts, making it a challenging setting. The goal is to demonstrate the effectiveness of V e as an estimator of V * M0. The study compares THOR with different k values to AGGREVATED and TRPO-GAE in terms of performance. No pre-training of policy or critic using demonstration data is done. Parameters are set based on TRPO-GAE recommendations. Two discrete action control tasks with sparse rewards are considered: Mountain-Car, Acrobot, and a modified sparse reward version of CartPole. All simulations have sparse rewards. In sparse reward environments like Mountain-Car and Acrobot, pure RL approaches struggle due to reward sparsity. THOR with k > 1 outperforms AGGREVATED in Acrobot, while in Mountain Car, THOR with k > 1 (especially k = 10) shows higher performance once the reward signal is received. THOR with k > 1 also outperforms TRPO-GAE, with better performance as k increases. Making the Acrobot setting harder by reducing the chance of a random policy further highlights THOR's effectiveness. THOR with different settings of k outperforms TRPO-GAE in simulators with continuous state and actions from MuJoCo simulators. Hopper and Swimmer, which do not have reward sparsity, show great results with policy gradient methods. The larger and more complex state and control space in these simulations compared to previous ones highlight the effectiveness of THOR. The value function estimator V e is less accurate in estimating V * M0 due to limited expert trajectories covering only a small part of the state and control space. Results show that a planning horizon of k around 20% \u223c 30% is needed for good performance. AGGREVATED (k = 1) learned little with imperfect V e. Reward shaping with V e outperformed TRPO-GAE when k = H, consistent with previous observations. Policy gradient methods can still benefit from V e even when not close to V * . The approach significantly reduces the need for expert demonstrations. Our approach combines imitation learning and reinforcement learning through cost shaping with an expert oracle. Cost shaping with the oracle shortens the learner's planning horizon based on the oracle's accuracy compared to the optimal policy's value function. Setting k = 1 reveals previous imitation learning algorithm AGGREVATED, while using k > 1 with an imperfect oracle can outperform AGGREVATE and AGGREVATED. THOR (Truncated HORizon policy search) is a gradient-based algorithm that minimizes total cost over a finite planning horizon, combining imitation learning and reinforcement learning. It outperforms RL and IL baselines with an accurate oracle, and can be combined with other RL techniques like DDPG. THOR is a gradient-based algorithm that combines imitation learning and reinforcement learning to minimize total cost over a finite planning horizon. It outperforms RL and IL baselines with an accurate oracle and can be combined with other RL techniques like DDPG. In experiments, expert demonstrations were used to pre-train V e using TD learning, and there are other ways to improve accuracy, such as online updating V e with expert feedback during training. The special MDP constructed for theorem 3.1 has deterministic transitions, 2H + 2 states, and two actions per state. The theorem is proven by constructing this MDP with specific characteristics. The text discusses an oracle V e with specific values, and the calculation of costs for optimal policies using Q e. It shows that the induced policy from oracle Q e makes mistakes, leading to a cost of \u03b3/(1-\u03b3). The proof of Theorem 3.2 involves simplifying notation and defining value functions. The proof simplifies notation and defines the value function V \u03c0 for any state s 1 \u2208 S."
}