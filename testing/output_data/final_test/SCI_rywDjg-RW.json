{
    "title": "rywDjg-RW",
    "content": "Neural Guided Deductive Search (NGDS) is a hybrid synthesis technique that combines symbolic logic techniques and statistical models to generate programs from input-output examples. It aims to provide real-time synthesis on challenging benchmarks by reducing the learning problem of the neural component to a simple supervised learning setup. This approach produces programs that satisfy specifications and generalize well on unseen examples. Neural Guided Deductive Search (NGDS) combines symbolic logic techniques and statistical models to generate programs from input-output examples. It reduces the learning problem of the neural component to a simple supervised learning setup, allowing training on limited real-world data while leveraging powerful recurrent neural network encoders. The method's effectiveness is demonstrated by evaluating on real-world customer scenarios. Automatic synthesis of programs with up to 12\u00d7 speed-up compared to state-of-the-art systems is a classical problem in AI, particularly in the area of Programming by Examples (PBE). PBE systems synthesize programs based on example inputs and outputs, making tasks accessible to users without programming skills. These systems are evaluated based on correctness, efficiency, and generalization capabilities. Figure 1 shows an input-output spec for learning a program that maps inputs to outputs and generalizes well. Two programs are provided as examples, with one clearly demonstrating better generalization. State-of-the-art PBE systems use either symbolic or statistical approaches for program synthesis. Program synthesis can be achieved through logical reasoning and domain-specific knowledge, or statistical methods. While logical approaches produce correct programs with minimal examples, they require significant engineering effort and struggle with real-time performance. Statistical systems, on the other hand, are easier to implement but lack in training data and may generate unnatural programs that do not generalize well. Additionally, purely statistical systems do not guarantee that the generated program meets the specifications, requiring multiple programs to be generated and evaluated. Neural-Guided Deductive Search (NGDS) is a hybrid synthesis technique that combines the desirable aspects of logical reasoning and statistical methods. It uses deductive search with a domain-specific language to decompose synthesis problems into smaller sub-problems. By predicting the usefulness of pursuing specific sub-problems, NGDS streamlines the search process, saving time. NGDS streamlines the search process by integrating symbolic and statistical approaches through an intelligent controller. It uses a statistical model to predict the generalization score of the best program from a branch, allowing for efficient exploration of sub-problems. The algorithm combines deductive search with a controller that selects sub-problems based on the model's predictions, optimizing the program synthesis process. NGDS integrates symbolic and statistical approaches through an intelligent controller to streamline the search process. Deductive search is Markovian, generating independent sub-problems at every level, enabling a supervised learning formulation. This allows for a dataset of search decisions over PBE tasks, promoting generalization and offline training. NGDS is evaluated in the string transformation domain, showing promising results. NGDS is evaluated on the string transformation domain, leveraging PROSE for PBE. It achieves a 68% success rate with one example, outperforming neural synthesis techniques by 4\u00d7 in speed. NGDS matches PROSE accuracy with a 12\u00d7 speed-up on challenging tasks. NGDS achieves significant speed-up of up to 12\u00d7 on challenging tasks by utilizing a branch-and-bound optimization controller and a program synthesis algorithm. The evaluation against state-of-the-art baselines shows substantial gains in real customer tasks. The use of a domain-specific language (DSL) enhances the efficiency of the program synthesis process. A DSL is a restricted programming language suitable for expressing tasks in a specific domain. It includes operators like conditionals, loops, string concatenation, and date/time formatting. DSLs are specified as context-free grammars with strongly typed symbols and operators. Inductive program synthesis involves a spec \u03d5 with input-output constraints. The program has an input state \u03c3 and output constraint \u03c8. The goal is to synthesize a program that satisfies the spec \u03d5. Program synthesis aims to find a program in a DSL that satisfies a given spec \u03d5 by generating desired outputs. For instance, formatting a phone number involves extracting specific parts and concatenating them using hyphens. This process is represented in the DSL as DISPLAYFORM1 with specific operations like SubStr and regex. In the DSL, programs aim to satisfy a spec \u03d5 by generating desired outputs. Programs take inputs and return a concatenated string of atoms, where each atom is a constant or a substring of the inputs extracted using position logic. The RegexOccurrence position logic finds the k th occurrence of a regex r in x and returns its boundaries. Other position logics include AbsolutePosition for selecting start and end positions independently. The main challenge in DSL programming is finding a program that satisfies input-output examples and generalizes to unseen inputs. The synthesis process involves searching for spec-satisfying programs and ranking them using a domain-specific function. The candidate program P in language L, along with input states \u03c3 in \u03a3, is scored for generalization by the implementation of h. This process balances program generality, complexity, and behavior on available inputs. In PBE systems like PROSE, h is learned from customer tasks. The goal of inductive program synthesis is to find a program set S that satisfies input-output examples and generalizes to unseen inputs. The goal of inductive program synthesis is to find a program set S that satisfies input-output examples and generalizes to unseen inputs. PROSE employs a deductive search strategy, exploring the grammar of L top-down to find programs that satisfy a given specification \u03d5. It iteratively unrolls productions into partial programs and reduces synthesis problems to smaller subproblems. PROSE computes the top programs using guiding principles and spawns smaller synthesis problems for each production. PROSE breaks down a production N := F (N 1 , . . . , N k ) into k smaller synthesis problems Learn(N j , \u03d5 j ), deducing necessary and sufficient specs \u03d5 j for each N j. The deduction logic is domain-specific for each operator F. Recursive solving of subproblems and uniting results is done. For example, a spec \u03d5 = {\"Yann\" \"Y.L\"} on a transf orm program yields a necessary & sufficient spec for the atom sub-program. The witness function for the Concat operator produces a disjunctive spec \u03d5 a = {\"Yann\" DISPLAYFORM5 inducing corresponding necessary conditions. The spec \u03d5 a = {\"Yann\" DISPLAYFORM5 induces necessary and sufficient suffix spec on the second parameter. The disjuncts in \u03d5 a will be satisfied by different program sets, creating logical non-determinism and exponential search tree size. Our main contribution is a neural-guided search algorithm that predicts the best program scores from each branch, allowing PROSE to omit branches unlikely to produce the desired program. This algorithm selects the top k programs rooted at a nonterminal symbol N based on a spec \u03d5, defined through a set of productions. Our proposed data-driven method aims to select the most promising production rule N := F i (N 1 , . . . , N k ) based on the spec \u03d5, without exhaustively exploring all branches in the search tree. By understanding the relationship between \u03d5 and the ranking function h, we can predict the optimal rule, even without fully exploring it. This approach helps in efficiently identifying the top k programs for each production, ultimately leading to the selection of the top k programs overall. Our data-driven method selects the best production rule based on the spec \u03d5, reducing the search space by considering specific transformations. Decisions depend on the ranking function h, with some choices being subtle. For example, selecting transf orm := atom or transf orm := Concat(. . .) depends on the spec's characteristics. The ranking function h plays a crucial role in determining the best program branch based on the spec \u03d5. The goal is to predict production rules that will lead to a top-ranked program, with decisions influenced by the relationship between \u03d5, h, and the candidate branch. The goal is to learn a function f to predict the score of the top-ranked program synthesized from a production rule \u0393 that satisfies a given specification \u03d5. The production selection problem is treated as a supervised learning task, simplifying the process compared to reinforcement learning. Two models for learning f have been evaluated using a loss function. The models evaluated for learning a function to predict program scores involve a common structure based on LSTM architecture. One model attends over input when encoding the output, while the other does not. Using the predictions of the score model alone is insufficient for perfect prediction, as it may lead to incorrect decisions during the search process for top programs satisfying a given specification. The branch selection strategy in program synthesis must balance performance and generalization by selecting the optimal number of branches to avoid committing to incorrect paths early in the search process. The controller for branch selection in program synthesis aims to predict the expected score of the best program from each program set to guide the search process in constructing a most generalizable program set of size k. The controller utilizes a ranking function, a symbolic search algorithm, and a score model based on the depth of the program set. Neural-guided deductive search in program synthesis involves using predicted scores to narrow down the set of productions and select a subset of generated programs. Two controllers are proposed and evaluated, one based on a threshold score and the other on a branch & bound technique. The unified algorithm for program synthesis combines deductive search with symbolic PL insights and a ranking function to select the most generalizable program from the set of spec-satisfying ones. The neural-guided deductive search (NGDS) accelerates the search process by using a learned score model and branch selection controller. It retains symbolic insights for correctness but focuses on branches likely to yield the desired generalizable program, reducing unproductive search time. Different score prediction models can be trained for various aspects of the search process, enhancing evaluation and simplifying supervision. The NGDS algorithm accelerates the search process using a learned score model and branch selection controller. It evaluates different strategies for production fitness and simplifies supervised learning problems. Evaluation includes accuracy and speed-up compared to baseline methods on a test set of tasks. The algorithm is tested on string manipulation tasks with a DSL, assessing generalization accuracy and synthesis time. The dataset consists of 375 tasks from real-world customer string manipulation problems, split into training, validation, and test data. Tasks include date/time formatting, address manipulation, name modification, and email ID generation. Only one input is provided as the spec to the synthesis system, with the rest used for evaluation. The synthesis process with PROSE resulted in \u2248 400,000 intermediate search decisions. The method is compared against two state-of-the-art neural synthesis algorithms: RobustFill BID6. We compare our method against two state-of-the-art neural synthesis algorithms: RobustFill BID6 and DeepCoder BID1. RobustFill uses Attention-C model with DP-Beam Search and DeepCoder's predictions are combined with PROSE for fair comparison. Both algorithms are trained on randomly sampled programs, with variants trained with 2 or 3 examples for fairness. Our approach involves training models with 2 or 3 examples for fairness, although 1 example is most important in real-life industrial usage. We conduct ablations with specialized prediction models for different stages of the synthesis process. LSTM-based models are trained using CNTK BID25 with Adam BID15, and controllers include threshold-based and branch-and-bound methods. The study evaluates different controllers for algorithm selection based on accuracy-performance trade-off. The best performing algorithm on the test set is NGDS(T 1 + P OS, BB), while NGDS(T 1, BB) performs slightly better on the validation set. Generalization accuracy and synthesis time are key evaluation metrics. The study compares the performance of different methods for real-time synthesis, highlighting the accuracy and speed advantages of NGDS and PROSE over RobustFill and DeepCoder. The integration with a symbolic system like PROSE leads to significantly improved generalization accuracy. NGDS also saves over 50% of synthesis time compared to PROSE on average. However, DeepCoder sacrifices accuracy by eliminating branches with correct programs in 65% of tasks. The study shows that DeepCoder sacrifices accuracy by eliminating branches with correct programs in 65% of tasks. Our method achieves impressive speed-up of > 1.5\u00d7 in 22 cases, such as a 12\u00d7 speedup in a simple extraction case and a 2.7\u00d7 speed-up in a test case involving learning Concat and Substring operators. The study compares DeepCoder and NGDS models, with NGDS showing slower performance on some tasks compared to PROSE. Attention-based models in NGDS achieve higher accuracy but are computationally expensive, leading to a decision to forgo attention in initial NGDS and explore model compression in future work. NGDS evaluates LSTM for branches that were previously pruned, leading to reevaluation of neural network multiple times. This results in considerable slowdown, especially when performance of PROSE is already fast. Two examples illustrate failure modes, such as exploring only Concat branch instead of Atom branch due to higher predicted score. The NGDS system explores only the Concat branch, leading to incorrect programs due to absolute position indexes. The model struggles with punctuation signals and experiences a slowdown when evaluating different extraction logics. The B&B controller explores multiple branches, causing a relative slowdown in network evaluation time. Neural Program Induction systems synthesize programs by training neural networks to map example inputs to outputs. Different models can learn simple or complex programs, but they need to be re-trained for each problem type. In contrast, Neural Program Synthesis systems use pre-learned neural networks to synthesize programs in a given language. Seminal works propose producing a high-level sketch of the program and then synthesizing it using a neural or enumerative synthesis engine. Our hybrid synthesis approach significantly outperforms the purely statistical approach of RobustFill and DeepCoder by guiding the search process in a top-down goal-oriented enumeration and being trained on real-world data. In this work, the deductive search approach is built upon for program synthesis from formal logical specifications. While deductive search ensures program correctness and generalization, it requires engineering to speed up the process. NGDS integrates neural-driven predictions into deductive search for real-time program synthesis with a small number of input-output examples. It builds upon PROSE, a symbolic logic-based system, to avoid top-down enumerative grammar exploration and achieve impressive synthesis performance while maintaining the advantages of a deductive system. Compared to existing neural synthesis techniques, NGDS guides the search towards generalizable programs. Our system combines symbolic deductive inference with statistical techniques to generate programs that guarantee correctness, generalize well with minimal examples, and solve most test cases quickly. By integrating these approaches, we achieve the best of both worlds - avoiding extensive engineering efforts of symbolic systems while maintaining high-quality program generation and performance gains over neural and symbolic systems. Our system combines symbolic deductive inference with statistical techniques to generate high-quality programs quickly. Future research directions include exploring better learning models for production rule selection and applying the technique to diverse grammars."
}