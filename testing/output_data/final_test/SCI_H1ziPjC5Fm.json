{
    "title": "H1ziPjC5Fm",
    "content": "In this paper, a novel scheme for interpreting and explaining deep models without additional annotations is proposed. The method identifies internal features relevant to the model's classes and visualizes them for interpretation. At test time, the network prediction is explained with supporting visualizations. A method to address artifacts in visualizations and a new dataset for evaluation are also introduced. Experiments on various datasets demonstrate the effectiveness of the proposed method. Experiments on MNIST, ILSVRC 12, Fashion 144k, and an8Flower datasets show detailed explanations with good coverage of relevant features. DNN-based methods have achieved impressive results in computer vision tasks. This work aims to improve visual feedback capabilities of DNN-based methods for more visually-descriptive predictions. The goal is to bridge the gap between model interpretation and model explanation. Model interpretation of DNNs involves understanding what a trained model has learned and justifying its decisions. This can be done by manually inspecting visualizations of filters in each layer or comparing internal activations with annotated concepts in a dataset. Both methods have limitations, such as subjective bias in manual inspection and cognitive expense for deeper models. Model interpretation of DNNs involves understanding what a trained model has learned and justifying its decisions. This can be done by manually inspecting visualizations of filters in each layer or comparing internal activations with annotated concepts in a dataset. Limitations include subjective bias in manual inspection and cognitive expense for deeper models. Interpretation capabilities over the network are limited by available annotations, with high costs for adding new concepts due to pixel-wise nature. Weaknesses include the generation of spatial filter-wise responses through deconvolution-based heatmaps or up-scaling activation maps to the image space. The visualization comparison shows how heatmaps generated by the method attenuate artifacts and provide more detailed feedback compared to deconvnet-based methods. Up-scaled activation maps lose details and are only computable for convolutional layers, highlighting the need to alleviate these issues. The method proposed automatically identifies relevant internal filters in a trained DNN model to predict the class of interest. This selection is formulated as a \u00b5-lasso optimization problem, combining filter-wise responses to make predictions. At test time, the identified filters and class prediction provide an explanation for the predicted class. The proposed method automatically identifies relevant internal filters in a trained DNN model to predict the class of interest. It removes the need for expensive pixel-wise annotation and provides visual explanations using a deconvolution-based method. The method considers spatial responses from any filter at any layer, offering visually pleasant feedback and interpretive explanations. The proposed method offers an objective evaluation of explanation methods using a synthetic dataset called an8Flower. It allows for the identification of important network-encoded features for predicting a given class and quantitatively measuring the performance of model explanation methods. The proposed method aims to identify important network-encoded features for predicting a class without manual inspection or expensive annotations. It offers detailed visual feedback and can be applied to any network type. Additionally, a dataset and protocol for evaluating model explanation methods are introduced. The proposed method aims to visualize properties of the function modelled by a network by systematically covering the input image and measuring the difference in activations. Different experiments are conducted to evaluate the method, and conclusions are drawn based on the results. Other works focus on linking internal activations with semantic concepts, such as feature selection methods and exhaustively matching filter activations. The proposed method visualizes network properties by covering input images and measuring activation differences. It links internal activations to semantic concepts without the need for additional annotations, unlike other methods. Visualizations of mid-level elements reveal relevant features encoded by a DNN. The Deconvnet method and other techniques visualize network properties by revealing relevant features encoded by a DNN through pixel-level precision visualizations. These methods use activations from different layers to identify image regions responsible for observed activations. BID23 introduced guided back-propagation to improve heatmap visualizations by removing negative contributions. BID32 proposed Global Average Pooling for class activation maps, while BID20 improved efficiency in computing activation map weights. BID3 further enhanced object localization with neuron-specific weights. DeconvNet with guided-backpropagation is chosen for its precision in producing visual feedback. In the context of improving visual feedback precision, BID32 proposed changes in the backward pass to reduce visual artifacts while maintaining network structure. Different evaluation protocols were suggested, including a saliency-based approach and crowd-sourced user studies, each with their own limitations and biases. The proposed protocol aims to reduce subjective bias in evaluating deep models by predefining regions to be highlighted in explanations. It involves identifying relevant layer/filter pairs for each class during training, and using internal responses and relevance weights for predictions at test time. The protocol reduces bias in evaluating deep models by predefining regions for explanations. It identifies relevant layer/filter pairs for each class during training and uses internal responses and relevance weights for predictions at test time. Visualizations show image regions contributing to predictions based on abstract concepts learned by deep models. The protocol reduces bias in evaluating deep models by predefining regions for explanations and identifying relevant layer/filter pairs for each class during training. Internal responses and relevance weights are used for predictions at test time, with visualizations showing image regions contributing to predictions based on abstract concepts learned by deep models. The process involves extracting image-wise responses by computing the L2 norm of each channel within each layer and concatenating the responses to obtain a layer-specific descriptor that is L1-normalized. The matrix X is constructed by passing training images through the network F to store internal responses, representing each image with a vector xi. The last layer output directly related to classes of interest is not considered in this process. The i th image of the dataset is represented by a vector xi\u2208R m defined by filter-wise responses at different layers. The possible classes are organized in a binary vector li\u2208{0, 1} C. The binary label matrix L=[l1, l2, ..., lN ] is produced by putting annotations from all images together. The \u00b5-lasso problem is solved with a parameter \u00b5 to control sparsity, using the Spectral Gradient Projection method. The matrix W =[w1, w2, ..., wC ] is obtained after solving the \u00b5-lasso problem, with sparsity enforced on W. During training, sparsity is imposed on matrix W by constraining the L1 norm of each element. Relevant features are identified for classes of interest, indicated by W. At test time, feedback visualizations are generated by considering the response of these features on tested images. The response r\u0135 i is computed as the element-wise product of the sparse vector w\u0135 and the filter-wise response vector x i. This sparse vector w\u0135 adds minimal cost at test time. During training, sparsity is imposed on matrix W by constraining the L1 norm of each element. Relevant features are identified for classes of interest, indicated by W. At test time, feedback visualizations are generated by considering the response of these features on tested images. The sparse vector w\u0135 adds minimal cost at test time, with the w\u0135 vector being highly sparse. Features are visualized using a Deconvnet-based method with guided backpropagation to highlight important layer/filter pairs. During training, sparsity is imposed on matrix W by constraining the L1 norm of each element to identify relevant features for classes of interest. At test time, feedback visualizations are generated by considering the response of these features on tested images, using a Deconvnet-based method with guided backpropagation. The input image is processed through a network, storing activations from each filter at each layer until reaching a specific layer p. Activations from filter q at layer p are then backpropagated with inverse operations to the input image space, resulting in a set of heatmaps indicating the influence of pixels contributing to the prediction. During interpretation/explanation methods, heatmaps are generated by moving backwards from an internal point in the network to the input space. Grid-like artifacts in heatmaps are caused by internal resampling due to network operations with stride larger than one (S>1). To address this, the stride is set to 1 in the backwards pass, maintaining the network structure. The relationship between input and output sizes of a network operation block is defined by a specific equation. Our method addresses grid-like artifacts in heatmaps caused by internal resampling with stride larger than one. By setting the stride to 1 in the backwards pass, the network structure is maintained. The input size of the operation block is resampled to match the feature map size produced by the forward pass. This resampling is done using nearest-neighbor interpolation for fast computation. The improvements introduced by our method can be seen in Fig. 2. The method addresses grid-like artifacts in heatmaps caused by internal resampling with a larger stride. Four sets of experiments are conducted to verify the importance of identified features, evaluate visual quality improvements, quantify the capability of visual explanations, and assess method sensitivity. Experiments are performed on MNIST, Fashion144k, imageNet, and a subset of cat images from imageNet. The curr_chunk discusses datasets used for experiments, including MNIST, imageNet, Fashion144k, and imageNet-cats subsets. It mentions the number of images, classes, and validation sets for each dataset. The importance of identified features in training is evaluated by measuring changes in classification performance. The curr_chunk evaluates the importance of identified features in the network by measuring changes in classification performance through feature removal. Different sets of features are tested, including All, OnlyConv, Random selection, and the original network performance. The OnlyConv method assumes relevant features are only present in convolutional layers. The curr_chunk discusses the impact of feature removal on classification performance, highlighting the importance of identified features in the network. It shows that removing the identified features, All and OnlyConv, results in a drop in classification accuracy, emphasizing their relevance for the classes of interest. Additionally, random feature removal has a lower effect on classification accuracy, demonstrating the significance of the identified features. The method that considers the complete internal structure, All, shows a stronger drop in performance compared to OnlyConv, which only considers features from convolutional layers. Increasing the sparsity value \u00b5 in the \u00b5-lasso formulation leads to more specialized features that can better handle rare instances of the classes of interest. Starting with a low value of \u00b5=10 helps focus on relevant features while keeping computational costs low. Qualitative analysis was conducted to gain insight into the information encoded by features with high responses. Average visualizations were computed using top 100 image patches, showcasing rich semantic representations for different classes. Examples from imageNet-Cats demonstrated the descriptive characteristics captured by the identified features, such as fur patterns and specific features of different cat breeds. The identified features in the imageNet dataset capture body shapes and fur patterns of various animals, scenes, and objects. The Fashion144k dataset shows classes responding to different colors and body parts, with a focus on hair and clothing styles. Some classes also exhibit a high response to specific visual patterns like black-white gradients. The model effectively exploits human-related features and background-related features to make decisions, as shown in visual explanations. Examples in FIG2 demonstrate how the model uses relevant features from objects and their context to predict class labels. The visual explanations generated by the method assess the visual quality by comparing with activation maps from internal layers and DeconvNet output. Various visualizations are shown for different layers/filters throughout the network. The proposed method improves visualizations by attenuating grid-like artifacts in lower layers and providing more precise visualizations in higher layers compared to upsampled activation maps. Previous works suggested the network focuses on \"semantic\" parts, which is discussed further in BID8. The method improves visualizations by reducing grid-like artifacts in lower layers and providing more precise visualizations in higher layers. A box-occlusion study is conducted to measure heatmap quality, showing a 2% mean difference in prediction confidence compared to previous methods. Synthetic datasets with 6 and 12 classes are generated for evaluation. The method improves visualizations by reducing grid-like artifacts and providing more precise visualizations. Synthetic datasets with 6 and 12 classes are generated for evaluation, defining features and generating masks for discriminative regions. Performance is evaluated using different feature selection methods and state-of-the-art techniques for visual explanations. Our method effectively identifies discriminative regions in visual explanations, highlighting features with better detail and coverage compared to existing methods. Quantitative results show higher mean IoU of discriminative features. However, our method involves an additional feature selection process. Our method involves an additional feature selection process using \u00b5-lasso at training time, requiring the definition of an additional parameter \u00b5. The evaluation protocol allows for objective quantitative comparison of visual explanation methods, complemented by simpler user studies to ensure meaningful explanations for users. Recent works have emphasized the importance of verifying that generated visual explanations are relevant to the model and classes being explained. An experiment was conducted to compare visual explanations for different predicted classes, showing that a good explanation should be sensible to the class and generate different visualizations. Results from models trained on ILSVRC'12 and ImageNet-cats datasets are presented in Figure 9. The explanation generated for the predicted class 'cat'/'tabby' in the imageNet-cats model focuses on different regions compared to randomly selected classes. Previous methods like DeconvNet and Guided-Backpropagation did not perform well in highlighting class-specific features, instead emphasizing first layer filters and edge-like structures in input images. The method proposed enriches DNN predictions by highlighting visual features contributing to the prediction, going beyond regions with prominent gradients. It focuses on relevant features for the task, allowing interpretation through average feature-wise visualizations. If a better visualization method emerges, it could be utilized. The method proposed enhances DNN predictions by highlighting relevant visual features without the need for additional annotations. A novel dataset for evaluating DNN explanation methods is also introduced in this section. The document is organized into six parts, including implementation details, quantitative analysis, and examples of average images used for evaluation. In Section 6.4, extended examples of average images for interpretation are provided. Section 6.5 showcases additional visual explanations generated by the method. The document concludes in Section 6.6 with a qualitative comparison highlighting the advantages of the proposed method over existing work. The experiments utilize pre-trained models from the MatconvNet framework for the MNIST and ImageNet datasets, and a VGG-F-based model for the Fashion144k dataset. The network architecture includes 8 layers for MNIST and a VGG-F model with 21 layers for ImageNet. The VGG-F model used in the experiments has 21 layers, with 15 convolutional and 5 fully connected layers. The model is fine-tuned on the ImageNet-Cats subset. The relevance of identified features is verified by measuring their ability to encode information from the classes of interest. Performance is reported based on different \u00b5 values, showing that even with a low amount of selected features, visual characteristics of the classes of interest can be properly encoded. Identified features hold strong potential for visual explanation. The mean area under the ROC curve (mean-AUC) is presented for all classes in a dataset, with a small number of features needed for good reconstruction. An8Flower dataset is introduced for evaluating model explanation methods, created from the eggplant model with added discriminative features for each class. Images are rotated 360 degrees and 40 frames are rendered at different viewpoints for each class, with data augmentation applied to increase variation. The data augmentation procedure involves cropping and rotating images to create 1000 example images per class in the an8Flower dataset. Three variants of the dataset are shown in Figure 11: an8Flower-single-6c, an8Flower-double-12c, and an8Flower-part-2c. In this section, extended examples of average images used for visual interpretation are provided. The images are compared with those generated by existing methods, showing up-scaled activation maps and heatmaps. Average images for imageNet-Cats and Fashion144k datasets are displayed, sorted based on relevance for each class. In FIG1 and FIG2, average visualizations for classes from the imageNet dataset are shown. Semantic concepts start to appear within a few top relevant features per class. Features encode class-specific properties like nose shape for imageNet-Cats and hair color for Fashion144k. Some features also relate to background/context. The proposed method produces sharper visualizations than those generated from up-scaled activation maps and state-of-the-art deconvnet-based methods. The visual explanations generated by the method enhance interpretation capabilities. Additional visual explanation results are presented in FIG4, 19, 13, 12. In the visual explanations generated by our method, heatmaps indicate pixel locations contributing to the prediction, along with the layer number. Extended results compare visualizations with upsampled activation maps and deconvnet output. In Figures 23-27, our method improves visualization by reducing grid-like artifacts in lower layers and providing more precise visualizations in higher layers compared to upsampled activation maps. The heatmaps in Figure 17 and FIG4 show pixel locations contributing to predictions, with layer numbers and types color-coded. In Figures 23-27, the method enhances visualization by reducing grid-like artifacts in lower layers and providing more precise visualizations in higher layers. The heatmaps in Figure 17 and FIG4 display pixel locations contributing to predictions, with layer numbers and types color-coded. Generated visual explanations from different datasets are shown in Figures 20 and 21, with heatmaps indicating pixel locations associated with features contributing to predictions. Figure 22 and Figure 23 show visual explanations with heatmaps indicating pixel locations contributing to predictions, color-coded by layer type. Our method reduces grid-like artifacts in lower layers and provides precise visualizations in higher layers. Our method reduces grid-like artifacts in lower layers and provides detailed visual feedback compared to upsampled activation maps. Visual quality comparisons from imageNet-Cats and Fashion114k datasets show the effectiveness of our heatmaps in attenuating artifacts introduced by deconvnet-based methods. Our method, demonstrated in visual quality comparisons on the Fashion114k dataset, effectively reduces grid-like artifacts in lower layers and provides detailed visual feedback compared to upsampled activation maps. The heatmaps show higher coverage and stronger response in the ground truth mask area, outperforming deconvnet-based methods."
}