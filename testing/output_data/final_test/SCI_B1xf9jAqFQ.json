{
    "title": "B1xf9jAqFQ",
    "content": "Recurrent neural networks (RNNs) model natural language by sequentially processing input tokens. Structural-Jump-LSTM is a neural speed reading model that can skip and jump text during inference, improving efficiency. Experimental evaluation shows its superiority over other state-of-the-art reading models. Structural-Jump-LSTM is a faster model than vanilla LSTM, achieving better FLOP reduction while maintaining or improving accuracy. GRU and LSTM are RNN units designed to handle long term dependencies but are computationally expensive. Attention mechanisms help the network focus on important parts of the input sequence. The attention mechanism in neural networks, such as hard attention, allows the model to focus on important parts of the input sequence, reducing the number of floating point operations needed for inference. This is known as speed reading, where the model achieves the same accuracy with fewer FLOPs. Prior work on speed reading processes text in chunks of words or blocks, optimizing the reading process. The Structural-Jump-LSTM 1 model is inspired by human speed reading and aims to skip unimportant words in important sections while jumping over unimportant sections of text. It can skip individual words and use punctuation to define jumps to the next sub-sentence separator or end of sentence symbol. Extensively evaluated against state-of-the-art speed reading models BID19 and BID22. The Structural-Jump-LSTM model aims to skip unimportant words and sections of text, using dynamically spaced jumps and word level skipping. It outperforms other speed reading models like BID19 and BID22 in terms of FLOP reductions and reading accuracy. The model is trained using reinforcement learning with the REINFORCE algorithm. The REINFORCE algorithm is used in the BID21 model, where the reward is based on correct predictions. BID5 allows for jumping back and forth while reading important parts. BID23 utilizes a CNN-RNN network for reading blocks of words and making decisions on re-reading, jumping ahead, or jumping to the end using reinforcement learning. The actor-critic method leads to more stable training with rewards based on prediction loss and FLOPs used. BID8 proposes an early-stopping method using a RNN to determine when to stop reading, tied to FLOP reduction and the reward signal. BID19 introduces a model with two RNNs, choosing between a \"big\" and \"small\" RNN for text skimming. BID1 trains a LSTM to potentially skip word updates, contrasting with BID19's approach of skimming words. Gumbel softmax is used for non-differentiable choices instead of REINFORCE algorithm. BID19 introduces a network for text skimming using a straight-through estimator for non-differentiable action choice. This approach is applied to image classification. Other models include a speed reading model for sentiment classification by BID9 and a method for question answering by BID3 using CNN-based sentence classification. BID16 addresses issues with long input sequences in gated units like GRU and LSTM. Speed reading reduces input sequence length, while BID16 updates only part of the LSTM at a time. The proposed model handles LSTM updates by controlling which part of the state can be updated at a current time point using an oscillating function. It utilizes memory networks to store states and applies an attention mechanism to prevent older states from being overwritten. The model considers previous skip and jump agent actions, as well as word embedded tokens for the next word prediction. Overall, existing models are either jump or skip/skim based, while this model introduces the first speed reading approach. The proposed speed reading model combines skipping and jumping based on text structure, improving efficiency without sacrificing accuracy. The Structural-Jump-LSTM model includes an RNN with LSTM units, a skip agent, and a jump agent. The skip agent decides whether to skip a word or update the LSTM, while the jump agent moves forward in the text based on punctuation structure. The Structural-Jump-LSTM model utilizes a skip agent and a jump agent to improve efficiency by avoiding unnecessary LSTM state updates. The jump agent moves forward in the text based on punctuation structure, contributing to a reduction in FLOPs during inference. The Structural-Jump-LSTM model uses a skip agent and a jump agent to make decisions on skipping words and moving forward in the text based on punctuation structure, reducing unnecessary LSTM state updates and FLOPs during inference. The Structural-Jump-LSTM model utilizes a skip agent and a jump agent to decide whether to skip words or move forward in the text based on punctuation structure. The network samples actions from jump-action and skip distributions to determine the next step in processing the text. The Structural-Jump-LSTM model uses a skip agent and jump agent to decide whether to skip words or move forward in the text based on punctuation structure. The probability distribution for sampling actions is computed, with objectives including producing an output for classification and learning when to skip and jump for maximum accuracy. The model is optimized with cross entropy loss for classification tasks. The model uses a reinforcement learning approach with a skip agent and jump agent to decide whether to skip words or move forward in the text. A reward function is defined based on reading amount and prediction accuracy. An advantage actor-critic approach is used to train the agents and reduce variance. The loss for the skip agent is calculated using a value estimate of the state, while the jump agent follows a similar process. The model uses a reinforcement learning approach with skip and jump agents to decide whether to skip words or move forward in the text. The value estimate of a state corresponds to how much reward is collected when acting from that state. The loss for the agent is calculated using advantage instead of reward, ensuring the sign of the loss depends on the achieved reward compared to the expected reward. The model uses reinforcement learning with skip and jump agents to decide whether to skip words or move forward in the text. The value estimate is trained with squared difference using observed values for each state. An entropy loss is added to provoke exploration in the network. The total loss for the network is controlled by trade-offs between components. Rewards are given for each action, with the reward for skipping a word scaling with the document length. The jump action gives no reward. The model uses reinforcement learning with skip and jump agents to decide whether to skip words or move forward in the text. Rewards are given for each action, with the reward for skipping a word scaling with the document length. An additional reward is given based on correct predictions, with a trade-off between rolling reward and model performance. Experimental evaluation is presented using tasks and datasets from state-of-the-art speed reading models. The model uses reinforcement learning with skip and jump agents to decide whether to skip words or move forward in the text. Rewards are given for each action, with a trade-off between rolling reward and model performance. Experimental evaluation is presented against state-of-the-art models for sentiment and topic classification, as well as Question Answering datasets. The approach involves applying a fully connected layer on the LSTM output for classification tasks and initializing word embeddings with GloVe embeddings. The model uses reinforcement learning with skip and jump agents to decide whether to skip words or move forward in the text. Rewards are given for each action, with a trade-off between rolling reward and model performance. Experimental evaluation is presented against state-of-the-art models for sentiment and topic classification, as well as Question Answering datasets. The approach involves applying a fully connected layer on the LSTM output for classification tasks and initializing word embeddings with GloVe embeddings. Training the model involves using RMSprop with a chosen learning rate, batch sizes, dropout for reducing overfitting, LSTM cell size, and gradient clipping. The model uses reinforcement learning with skip and jump agents to decide whether to skip words or move forward in the text. During training, the agents' fully connected layer is fixed to 25 neurons, and the entropy loss is included in the total loss to promote full read behavior. The word embedding is fixed for Question Answering datasets and trainable on other datasets. The trade-off between correct prediction and speed reading is controlled by w rolling, chosen via cross-validation. The cost of skipping a word is fixed at 0.5 to encourage reading over skipping. During the speed reading phase, the total loss includes prediction, actor, critic, and entropy losses. The actor loss is scaled by a factor of 10 for comparability. Entropy loss is chosen through cross-validation, with most datasets performing best at 0.1. Actions are chosen greedily for QA and via sampling for others. Non-QA datasets use uniform action target distributions for increased exploration, while CBT-CB and CBT-CN have a distribution with 95% probability mass on the \"read\" choice to reduce skipping and jumping exploration. The objective of speed reading involves maximizing model accuracy while reading as few words as possible. Two ways the model can skip a word are jumping over it or skimming it. Percentage of words jumped over and total reading percentage are reported to capture speed reading aspects. Total FLOPs used by the models are calculated. The evaluation includes calculating total FLOPs used by models and presenting results of the Structural-Jump-LSTM from DBPedia. The model skips words by jumping over them or skimming them, with accuracy and reading percentages compared to a full reading baseline shown in TAB4. Our approach, Structural-Jump-LSTM, achieves similar accuracies to vanilla LSTM but reads significantly less text, ranging from 17.5% to 68.8% compared to a full reading baseline. The speed reading behavior varies across datasets, with some showing no skipping or jumping. In 7 out of 8 datasets, our model improves accuracy, indicating that the jumps and skips are meaningful. An example of this behavior can be seen in Figure 2 from DBPedia. The model learns to skip uninformative words, reads important ones for predicting the target, and jumps to the end of sentences. It inspects the first two words of the last sentence before making a prediction. Table 3 shows our approach's scores compared to five state-of-the-art speed reading models, with reported FLOP reductions or speed increases. State-of-the-art models use different network configurations and training schemes. Our approach provides the best or shared best FLOP reduction on all datasets compared to other speed reading models. The second best method for FLOP reduction is Skim-LSTM, while the worst is the Adaptive-LSTM model. Skim-LSTM has an advantage in FLOP reduction due to the difference between the small and large LSTM used by the model. Skim-LSTM uses a LSTM size of 200 for Question Answering tasks and a default size of 100 for the rest, with the small size tested between 5 to 20. Large skimming percentage suggests reducing the size of the large LSTM without affecting performance. Jumping based models are less prone to evaluation flaws as they cannot carry over information from skipped words. LSTM-Shuttle provides consistent accuracy improvements with noticeable FLOP reduction compared to Skim-LSTM. The FLOP reduction in comparison to a full read model is denoted by FLOP-r. LSTM-Jump shows significant accuracy improvements on CBT-CN and CBT-NE tasks. Models like LSTM-Jump and LSTM-Shuttle optimize by limiting the number of jumps allowed, giving them an advantage over other methods. This approach helps in reducing the number of words read between important sections of the text and the final prediction. Structural-Jump-LSTM is a recurrent neural network for speed reading that benefits from prior knowledge encoded in the budget. Budgets can be advantageous if prior information about the document is available, but they can be rigid as every document has the same budget. Structural-Jump-LSTM is a neural speed reading model that skips and jumps over text based on punctuation structure, reducing floating point operations while maintaining accuracy compared to a vanilla LSTM model. The Structural-Jump-LSTM model skips and jumps over text based on punctuation structure, improving efficiency without losing effectiveness. Future work involves exploring different reward functions to enhance agent training."
}