{
    "title": "SyeniynqTE",
    "content": "The eigenvalues in converged deep networks exhibit fast decay, resembling a Heavy-tail distribution with mostly small eigenvalues and a few spikes of large eigenvalues. We use a stochastic approximator to generate histograms of eigenvalues in deep networks. The distributions converge to a heavy-tail spectrum across all layers during ImageNet training. The study of generalization in deep networks focuses on properties of linear operators in network layers, such as matrix norms and eigenvalues. Visual inspection of eigenvalue spectra is facilitated by computing histograms. Histograms are useful for estimating eigenvalue distribution in deep networks. An example in a squeeze_net network shows a heavy-tail distribution with spikes of large eigenvalues. Computing singular values is costly due to high dimensions in convolutional neural networks. In deep networks, estimating histograms of eigenvalues can be done using ARPACK for high eigenvalues and Chebyshev approximations for the rest. This approach is based on the decay property seen in deep networks and is useful for understanding eigenvalue distribution in networks like squeeze_net. The highest-dimensional linear operators in deep networks, such as convolution layers, transform feature maps efficiently using specialized convolution routines. Estimating eigenvalues of network layers can be done using these routines, ensuring no bias term is added. This approach is useful for understanding eigenvalue distribution in networks. The eigenvalues of the network layer are computed efficiently using the implicitly restarted Lanczos method. The largest eigenvalue is estimated and used to derive an equidistant histogram-binning. Stochastic estimation of eigenvalue counts is performed within the range [-1, 1]. The eigenvalues of the network layer are efficiently computed using the implicitly restarted Lanczos method. The matrix is transformed to ensure eigenvalues fall within the range [-1, 1]. The number of eigenvalues in a specific range is approximated using Chebyshev polynomials. The count can be rewritten as the trace of a polynomial matrix function, which can be estimated using stochastic trace estimators. The quantity can be approximated using stochastic trace estimators, such as Hutchinson estimators. By drawing independent components from a zero-mean distribution, we can estimate the count without explicitly computing \u03a6 k (A). An algorithm is derived based on Chebyshev polynomials to estimate the count. Experimental results are obtained using a pyTorch implementation on imagenet data with a squeeze_net architecture. Training involves reducing the learning rate after 30 epochs and using mini-batches of size 128 for plain stochastic gradient descent. The text discusses using gradient descent with mini-batches of size 128 and computing histograms for convolution layers. Eigenvalues are computed with a budget of 1000 and approximated using a stochastic estimator. Matrix functions for real-valued functions are explained, with examples shown in histograms. The text also references Highham's book for a comprehensive introduction to matrix functions. The text discusses using gradient descent with mini-batches of size 128 and computing histograms for convolution layers. Eigenvalues are computed with a budget of 1000 and approximated using a stochastic estimator. Matrix functions for real-valued functions are explained, with examples shown in histograms. Highham's book is referenced for a comprehensive introduction to matrix functions. Additionally, different phases in the spectograms are identified, with observations of random matrix theory behavior and unexpected bumps in histograms possibly due to padding in convolutions. Heavytail behavior is observed as optimization progresses, with separation of largest eigenvalues from the bulk after one epoch of training. The eigenvalues in the convolution layers show changes over training epochs, with the largest eigenvalues growing steadily initially and then decreasing when the learning rate is reduced. This suggests regularization is occurring. The last layer maintains its shape, with the largest bar decreasing in size and the difference moving to other bars in the histogram. The text discusses the evolution of eigenvalue spectrum during imagenet training using convolutional networks, specifically squeeze_net networks. The authors aim to estimate the covariance structure of intermediate feature representations and explore the relationship between covariance matrices and parameter matrices. The differentiable estimator used may have potential for regularization."
}