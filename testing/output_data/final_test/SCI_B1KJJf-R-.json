{
    "title": "B1KJJf-R-",
    "content": "Neural Program Search algorithm generates programs from natural language descriptions and input/output examples by combining Deep Learning and Program Synthesis methods. It uses a domain-specific language and a Seq2Tree model for efficient search. The approach outperforms sequence-to-sequence models and addresses the central problem of synthesizing programs from user intent in artificial intelligence. Progress has been made in program synthesis from examples and descriptions. Neural Program Search algorithm combines Deep Learning and Program Synthesis to generate programs from natural language descriptions and examples. It overcomes limitations of existing techniques by learning from both description and examples, offering high accuracy and speed for practical application. The proposed approach combines a LISP-inspired DSL with deep learning to synthesize programs from descriptions and input/output pairs. The Seq2Tree model BID0 is used for natural language understanding and program search guided by a deep learning model. The proposed model combines a tree decoder with attention to compute probabilities for each symbol in an AST tree node. A tree beam search is then used to find the most likely tree consistent with input/output pairs. The model outperforms traditional search techniques and heuristics, incorporating deep learning for better results. The related work includes programming by example, latent program induction, and semantic parsing. Recent work has integrated deep learning models into traditional search techniques, such as DeepCoder and Deep API Programmer. Program synthesis from natural language descriptions has shown progress, but is limited by small datasets and existing deep learning model constraints. Additional computational modules like Neural Turing Machines and Neural GPUs have been used to teach neural networks program behavior. Semantic parsing is a related field to program synthesis from description, where programs are limited to structured forms. Using a domain-specific language for code generation allows for converting programs to multiple target languages. The DSL, inspired by LISP, is designed for practical applications in multiple target languages like SQL, Python, and Java. It includes a type system where each constant, argument, or function has a type. The program in the DSL consists of arguments and a program tree with symbol types like constant, argument, function call, function, or lambda. Additionally, the DSL has a library of standard functions. The DSL includes a library of standard functions with return types and a fixed number of arguments. The type system reduces possible combinations in the program tree. The neural network model uses an attentional encoder-decoder architecture with RNN for encoding arguments and textual descriptions. The decoder is a doubly-recurrent neural network for generating tree structured output. Attention is used to incorporate relevant information from the encoder during decoding. The neural network model utilizes an attentional encoder-decoder architecture with RNN for encoding arguments and textual descriptions. Functions g p and g s update ancestral and fraternal hidden states for nodes in the tree structure. Context is computed using a general attention mechanism to align with encoder presentations. The hidden states are combined to obtain a full hidden state containing information from parent and siblings. The model utilizes an attentional encoder-decoder architecture with RNN for encoding arguments and textual descriptions. The hidden states are combined to obtain a full hidden state containing information from parent and siblings. The model is trained using back-propagation with teacher forcing to feed target labels for parent/sibling nodes. Error is calculated using cross-entropy loss for each decoded node. The model uses Tree-Beam search in the program space with a deep learning model to score symbols in each AST node. The search algorithm starts with a priority queue and continues until a complete program is found. The top N most probable trees are kept in the queue. The Tree-Beam search algorithm starts with an empty tree on the far left. When a tree is popped from the priority queue, new trees are generated by considering possible symbols for the first empty node in the pre-order traversal. The search continues until a complete program is found and evaluated with sample input/output examples. The Tree-Beam search algorithm generates new trees by considering possible symbols for empty nodes in the pre-order traversal. Each program in the priority queue is represented as an incomplete tree, and the search stops if the expected output is matched or if the maximum number of programs has been evaluated. The Seq2Tree model computes probabilities for each symbol to fill empty nodes, and new trees are constructed by replacing empty nodes with symbols of the correct type. Unlikely trees are also pushed into the priority queue, and the least probable trees are removed until the queue size is maintained. In experiments evaluating the Seq2Tree model, optimizing cloning trees and pushing them to the queue contributes to search performance. Optimization techniques include using persistent trees to save memory and time on cloning trees. When creating a new tree from an existing tree by introducing a new node, only the nodes on the path from root to the new node need to be cloned. During training, dynamic batching is used to read trees of different shapes efficiently. For batched search, Seq2Tree is invoked on a single node at a time, allowing for multiple invocations to be batched. This speeds up evaluation when running the search on multiple tasks simultaneously. During training, dynamic batching is used to efficiently read trees of different shapes. However, when evaluating a single task, batching across tasks is not applicable. An alternative approach of popping multiple incomplete trees from the priority queue did not speed up the search and sometimes even slowed it down. This could be because the top incomplete tree is usually the correct one, minimizing the benefit of batched execution. AlgoLisp is a new dataset for training models to synthesize data processing programs in a Lisp-inspired language. Each problem in the dataset has 10 unique tests with input-output pairs. Existing datasets for code synthesis tasks include description to bash command and description to SQL, but none are suitable for this specific problem. See TAB1 for dataset statistics. The AlgoLisp dataset provides problems for training models to synthesize data processing programs in a Lisp-inspired language. The dataset includes 10 unique tests for each problem. Two specific approaches are suggested for using tests during inference. The dataset was synthesized by selecting tasks from computer science and algorithms courses and parameterizing assignments. The AlgoLisp dataset includes problems for training models in a Lisp-inspired language. It provides a variety of tasks such as finding elements in an array and sorting them. The dataset is designed for learning basic composition and simple concepts in the DSL. However, due to the limited number of homework assignments used, models trained on this dataset may not generalize to new algorithm types. One specific task involves finding the smallest element in an array that is greater than the minimum element. The task involves computing the largest element in an array that is strictly smaller than the maximum element in the array. It also includes finding the largest element in an array that is present in another array and is strictly less than the maximum element in the second array. Additionally, there is a task to find the largest element in an array that is divisible by two and is strictly less than the maximum element in the array that is also divisible by two. The dataset is split into train, dev, and test sets by code form to ensure models learn to compose simpler concepts for novel problems. Accuracy is evaluated by comparing output on a holdout set of tests for each task. Models are implemented using PyTorch, Dynamic Batching, ADAM, and placeholders for out-of-vocabulary words. The model uses placeholders to handle out-of-vocabulary tokens in neural networks, increasing the vocabulary size. It is compared to Attentional Sequence to Sequence models, showing results on the AlgoLisp dataset for Seq2Seq+Att and Seq2Tree models. The performance of the search algorithm is also evaluated separately. The Seq2Tree model improves upon the attentional sequence to sequence model by 11% by explicitly modeling the tree structure of code. The Search algorithm alone finds a limited number of programs, while the Seq2Tree + Search model combines both approaches and achieves the best result of 90.1%. The quality of the neural network is reflected in how accuracy changes with the number of trees visited during search. The Search algorithm explores the entire program space when there is no limit on the maximum number of trees visited. Comparing the improvement that the neural network model brings to the search algorithm is also discussed. The neural network model improves search performance by predicting correct symbols with high accuracy, leading to quicker tree discovery. Program complexity is reflected in tree depth, with accuracy decreasing as depth increases. The algorithm presented focuses on program synthesis from textual specifications. The algorithm presented combines deep learning for understanding language and programming patterns with a search technique to find the correct program. Empirical results show improvement in program composition learning. Limitations include semi-generated training data and the need to learn from few examples per problem type. Learning from few examples per problem type is crucial, especially when there are no input/output examples, requiring user interaction and improved techniques for output decoding in neural networks."
}