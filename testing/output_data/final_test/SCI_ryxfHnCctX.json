{
    "title": "ryxfHnCctX",
    "content": "In this paper, a novel approach is introduced to address the filter-level pruning problem for binary neural networks. The method involves a main/subsidiary network framework where the main network learns representative features for prediction performance optimization, while the subsidiary component acts as a filter selector. A layer-wise and bottom-up training scheme is proposed to avoid gradient mismatch. The study includes theoretical and experimental comparisons between the learning-based and greedy rule-based methods. Deep neural networks have evolved significantly over the past decade, becoming deeper and more complex. To address the computational cost associated with these networks, research on network acceleration and compression has become prominent. A novel approach is introduced in this paper for filter-level pruning in binary neural networks, involving a main/subsidiary network framework. The method includes theoretical and experimental comparisons between learning-based and greedy rule-based methods, demonstrating its effectiveness on various image classification datasets. Research on network acceleration and compression is active, with DNN pruning algorithms being popular for their efficiency in memory and speed. Redundancy exists in human brains and deep models, motivating this line of research. Existing research categorizes pruning methods based on the level of the object, such as connection-level, unit/channel/filter-level, and layer-level pruning. Connection-level pruning is widely studied, producing sparse networks that reduce memory footprint and I/O consumption. However, these methods may not accelerate computation unless hardware is specifically designed for them. Structural pruning techniques like unit/channel/filter-level pruning are more hardware-friendly for computation acceleration and memory compression in deep learning frameworks. Specifically, binary neural networks use quantization to compress general deep neural networks. The quantization strategy in binary neural networks compresses deep neural networks by using low-precision fixed-point numbers for weights and activation functions. This approach can lead to significant computation and memory savings, but may also introduce noise and reduce the representation capacity of the networks. As a result, binary neural networks require larger model sizes to compensate for this loss. Binary neural networks aim to reduce redundancy through quantization and pruning strategies, which compress network topology by reducing precision and trimming connections. While previous studies focused on full-precision models, these strategies are essential for low-power embedded systems like smartphones and wearable devices in virtual reality applications. The text discusses the challenges of applying traditional pruning methods to binary neural networks with 1-bit weights and activations. It introduces a new approach combining pruning and quantization to simplify binary neural networks. Experimental results show that learning-based pruning outperforms rule-based criteria. The proposed method uses a main-subsidiary framework to optimize prediction performance in binary neural networks. The paper proposes a learning-based structural pruning method for binary neural networks to reduce filters/channels while maintaining prediction performance. It demonstrates the superiority of the non-greedy learning-based method over rule-based methods in selecting objects to prune. A main-subsidiary framework is designed to iteratively learn and prune feature maps, showcasing the limitations of rule-based methods and the advantages of learning-based methods through theoretical and experimental results. Additionally, a mathematical analysis for L1-norm based methods is provided to address gradient mismatch in the subsidiary component training. The paper introduces a learning-based structural pruning method for binary neural networks to reduce filters/channels while maintaining prediction performance. It utilizes a main-subsidiary framework to iteratively learn and prune feature maps, addressing gradient mismatch in training the subsidiary component. Various pruning methods like Optimal Brain Surgeon and Deep Compression are discussed, with a focus on group pruning. Researchers have proposed methods to increase group sparsity in connection-level pruning by applying sparse constraints to channels, filters, and layers. Various techniques such as LASSO constraints, L1-Norm rank for filter pruning, and leveraging scaling factors from batch normalization for channel pruning have been introduced. These methods are designed for full-precision models and cannot be easily transferred to binary networks. Binary neural networks, like XNOR-Net, do not include scaling and shifting parameters to maintain Boolean operations. Classical tricks such as L1-Norm ranking filters and LASSO constraints are not applicable due to binary values. Recent research suggests that full precision computation is not essential for training and inference in DNNs, leading to investigations into weights quantization with 16-bit and 8-bit fixed-point numbers. Low-bit models with binary and ternary weights are also explored for higher compression and acceleration ratios. Weight quantization in neural networks aims to replace all MAC operations with Boolean operations by quantizing both activation and weights. The activation function in quantized networks is a step function, which poses challenges for gradient flow during backpropagation. The straight-through estimator (STE) is commonly used to approximate the gradient of the step function as 1 in a certain range. The Half-wave Gaussian Quantization (HWGQ) proposed by BID4 aims to reduce mismatch between quantized activation function and backward ReLU. Binary Neural Networks (BNN) from BID6 and BID3 use 1 bit for both activation functions and weights, resulting in a faster network. Pruning filters in BNNs involves removing certain filters to improve accuracy. The goal of the method is to remove filters in binary neural networks to improve efficiency. By using a new binary network called subsidiary component as learnable masks, redundant features can be screened out. This approach simplifies binary networks and adapts optimization techniques to solve the pruning problem. Each update of the subsidiary component acts as exploration in the mask. The subsidiary component serves as a learnable mask to explore the mask search space and find optimal masks for pruning filters in binary neural networks. The weights of the subsidiary component are initialized using a uniform distribution, and elements with the same first index share the same value. The subsidiary component output tensor is used to screen the main network's weights through element-wise product, resulting in transformed weights for convolution with input feature maps. The subsidiary component acts as a fixed mask to prune filters in binary neural networks. After training the main network, a binary operator is used to select features in a layer-wise manner, with the subsidiary component's weights at the current layer being trainable. The subsidiary component's transformation function for the filter mask is changed to binarized numbers, with elements indicating whether filters are kept or removed. Regularization terms are added to prevent degeneration to zero, and the loss function includes cross entropy and distillation loss. Layers M j are fixed after training. The subsidiary component's transformation function for the filter mask is changed to binarized numbers, with elements indicating whether filters are kept or removed. Regularization terms are added to prevent degeneration to zero, and the loss function includes cross entropy and distillation loss. Layers M j are fixed after training. In the main network, layers before i are fixed, and retraining occurs for layers after Layer i. A layer-wise and bottom-up training scheme is proposed for the subsidiary component, prioritizing training of layers closer to the input. During training, previous layers are fixed, and subsequent layers start with near-zero values. This approach helps alleviate gradient mismatch and avoid trivial solutions. The text discusses the challenges in optimizing binary neural networks, specifically focusing on the weight gradient mismatch and activation gradient mismatch issues caused by quantization functions like Sign(\u00b7). A layer-wise and bottom-up training scheme is proposed to address these issues, prioritizing training of layers closer to the input and fixing layers that are harder to train early on. The proposed bottom-up layer-wise training scheme addresses weight and activation gradient mismatch in binary neural networks by prioritizing training of layers closer to the input and fixing harder-to-train layers early on. This approach results in more stable training curves, better adaptation to feature distribution shifts, and higher pruning ratios. The proposed layer-wise training scheme prioritizes training layers closer to the input, achieving higher pruning ratios. The method involves initializing subsidiary component weights, fixing them, and training the main network from scratch. Training starts with the first binary kernel, adjusting activation functions and parameters for each layer accordingly. The proposed method involves fixing subsidiary layers and main layers before the i-th layer, and retraining main layers after the i-th layer to guide the thin network to learn similar output distributions with the original network. A distillation loss is added to stabilize the training subsidiary component. The distillation loss is calculated using the final output of the pruned and original networks before the softmax layer. The proposed method involves retraining main layers after the i-th layer to guide the thin network to learn similar output distributions with the original network. A distillation loss is added to stabilize the training subsidiary component, calculated using the final output of the pruned and original networks before the softmax layer. Previous methods use rules to rank and remove the least important filters based on weight magnitude or other criteria, assuming filters behave independently, which may cause problems. Pruning filters independently may lead to issues when filters are highly correlated, as rule-based methods may prune out filters that have learned the same features. Most criteria for pruning are not suitable for scenarios with only two discrete values. One possible method is to exhaustively search for the optimal pruning set, but this is impractical for modern DNNs with thousands of filters. Our method uses a soft \"search\" strategy that is more efficient, utilizing a gradient-based approach. If the main network is full-precision, the L1-Norm based pruning technique is relevant to our method. Our method utilizes a soft \"search\" strategy for efficient pruning of filters in deep neural networks. Unlike the L1-Norm based method which controls perturbation in the next layer, we focus on optimizing the final network output. The L1-Norm approach minimizes the upper bound of a specific problem, but it is not directly applicable to binary networks due to identical norms for {\u22121, +1} tensors. Previous work has used LASSO regression to minimize reconstruction errors in each layer. The method focuses on optimizing the final network output by directly optimizing the discrete variables of masks without relaxation. Pruning experiments were conducted on VGG-11, Net-InNet (NIN), and ResNet-18 on CIFAR-10 and ImageNet using 1-bit binary neural networks inherited from XNOR-Net BID26. The study proposed a rule-based method for filter-level pruning in binary neural networks inherited from XNOR-Net BID26. Different pruning schemes were tested, and the learning curve for the subsidiary component was analyzed with varying learning rates. The weights of the first and last layers were kept full-precision, and pruning was only done for intermediate layers. The study focused on filter-level pruning in binary neural networks, with the first and last layers kept full-precision. Different pruning schemes were tested, measuring effectiveness in terms of PFR and error rate before and after retraining. Learning rates were adjusted for the main network and subsidiary component, with a constant learning rate for ImageNet. The experiments compared \"prune once and retrain\" (MSF-Layerwise) and \"prune and retrain iteratively\" (MSF-Cascade) schemes. The study compared different pruning schemes in binary neural networks, focusing on filter-level pruning. NIN and VGG-11 were tested on CIFAR-10, with batch normalization used for stability. Pruning filters by 30% to 40% only increased error rates by 1% to 2%. Training subsidiary components with different learning rates showed interesting results. In experiments with subsidiary components for different models, varying learning rates significantly affect convergence. A smaller learning rate results in lower accuracy and higher pruning, while a larger rate yields the opposite. The high-dimensional solution space of binary neural networks makes it challenging for components to move to better points. Using a larger learning rate allows for more aggressive \"searching\" by subsidiary components. In experiments with subsidiary components, different initialization ratios (SP) impact convergence. SP values of 0.4, 0.6, and 1.0 lead to similar outcomes, while 0.2 results in poor performance due to excessive filter removal and limited network adjustment ability. The network's self-adjustment ability is limited and cannot converge to a good state. It is recommended to initialize the SP to greater than 0.4 for better performance. ResNet, with identity connections within residual blocks and more layers than NIN and VGG-11, increases network capacity and redundancy as depth increases. Experimental results show increased sensitivity in residual blocks with downsampling layers. Results for ResNet on CIFAR-10 are in table (1), with layer statistics in the Appendix. ResNet-18 on ImageNet is further validated, with \u03b1 set between 10^-7 to 10^-9 for balancing accuracy and pruning ratio. After 20 epochs of retraining for each layer, the final PFR is 21.4%, with retrained error decreasing from 50.02% to 49.87%. Using STE, weights gradient is utilized. In this study, after 20 epochs of retraining, the final Pruning Factor Rate (PFR) is 21.4%, with the retrained error decreasing from 50.02% to 49.87%. The use of Straight-Through Estimator (STE) introduces gradient mismatches in weights and activations. The experiments were conducted on the CIFAR-10 dataset and ImageNet, comparing different pruning and retraining schemes."
}