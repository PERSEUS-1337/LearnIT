{
    "title": "ByetGn0cYX",
    "content": "In this work, a novel formulation of planning is proposed as a probabilistic inference problem over future optimal trajectories, using sampling methods to tackle planning in continuous domains with a fixed computational budget. A new algorithm, Sequential Monte Carlo Planning, is designed by leveraging classical methods in Sequential Monte Carlo and Bayesian smoothing for control as inference. It is shown that this approach can capture multimodal policies and quickly learn continuous control tasks, essential for intelligent behavior in machine learning agents. Algorithms like Alpha Go and Cross entropy methods have achieved impressive results in various tasks such as playing Tetris and enabling robots to perform complex manipulations. However, these algorithms have limitations due to strong assumptions about the environment, such as requiring a discrete setting or assuming Gaussian distributions. Planning in environments with continuous actions and complex dynamics remains a challenge due to the need to approximate distributions over future trajectories. This paper addresses these limitations by framing planning as a density estimation problem using deep learning and probabilistic inference methods. By modeling any distribution over future trajectories, this approach leverages tools from the inference research community to handle the complexity of sequential states and actions. Sequential Monte Carlo (SMC) methods model states and actions efficiently in reinforcement learning. The goal is to find the optimal policy by framing RL as an inference problem within a probabilistic graphical framework. An auxiliary binary random variable O t denotes the \"optimality\" of a state-action pair at time t. Sequential Monte Carlo (SMC) methods model states and actions efficiently in reinforcement learning. The goal is to find the optimal policy by framing RL as an inference problem within a probabilistic graphical framework. By considering variables as latent and observed, a Hidden Markov Model (HMM) can be constructed. The optimal policy is expressed as p(a t |s t , O t:T ). Finding optimal trajectories is equivalent to finding plausible trajectories yielding a high return. If rewards are bounded above, a constant can be removed to ensure well-defined probabilities. Many control as inference methods approximate density by optimizing variational lower bound. Soft Actor-Critic estimates optimal policy using soft Q function. Drawing samples from complex sequential distributions can be done using SMC methods. In control as inference methods, density is approximated by optimizing variational lower bound. Soft Actor-Critic uses soft Q function to estimate optimal policy. Drawing samples from complex sequential distributions can be achieved with SMC methods. In the context of planning, samples are drawn from proposal distribution q to estimate target distribution p using importance sampling. Sequential Importance Sampling (SIS) and Sequential Importance Resampling (SIR) are methods used to approximate density by sampling from a proposal distribution q. SIS updates unnormalized weights iteratively, while SIR is useful for long horizons where samples from q have low likelihood under p. In the context of control as inference, planning involves approximating optimal future trajectories through simulation. Weight impoverishment can occur in sequential importance resampling methods, where one weight dominates while others converge to zero. To address this, a resampling step can redistribute particles to higher likelihood regions, reducing variance growth. An accurate model of the world is essential for planning when multiple optimal trajectories are possible. Planning in the control-as-inference framework involves approximating optimal trajectories with a finite horizon. Bayesian smoothing is used to estimate the distribution of a latent variable based on past and future observations. Smoothing can be achieved through a forward-backward messages approach. The two-filter formula BID4 BID26 corresponds to a forward-backward messages factorization in a Hidden Markov Model. Filtering estimates the latent variable based on past observations, while smoothing estimates it based on past and future measurements. Belief propagation in HMMs involves computing forward and backward messages recursively. The backward message in a Hidden Markov Model is computed recursively and can be estimated using SMC methods. It represents the probability of following an optimal trajectory from the next time step until the end of the episode, given the current state. This concept is related to the value function in Reinforcement Learning. The backward message in a Hidden Markov Model is estimated using SMC methods and represents the probability of following an optimal trajectory from the next time step until the end of the episode. This is related to the value function in Reinforcement Learning. The equation presented differs from the traditional Bellman equation and is equivalent to learning a value function used in Maximum Entropy RL. The sequential weight update formula corrects the expectation of the return of a trajectory. Our algorithm, similar to the Auxilliary Particle Filter, updates weights using a one look ahead simulation step. Assumptions about the environment model's perfection are common in planning algorithms like LQR and CEM. To address model errors, Model Predictive Control (MPC) involves re-planning at each time step. The root state for planning is the current state, aiming to build a set of particles and weights representative of the planning density. Our algorithm updates weights using a one look ahead simulation step, sampling from a proposal distribution or model-free agent and using a learned model to sample the next state and reward. Only one sample is used to estimate expectations, potentially incurring a small bias. After updating weights with a one look ahead simulation step, our algorithm performs resampling of trajectories based on their weight. A trajectory is then sampled and its first action executed in the environment. Observations are collected and added to a buffer for training the model-free agent. An alternative algorithm without resampling is also discussed. The black nodes represent actions in the tree, while white nodes represent states. The objective is biased towards trajectories with high likelihood under a defined posterior, which may lead to seeking risky trajectories with potentially very low outcomes. The bias in the objective towards high likelihood trajectories may lead to seeking risky outcomes, but this bias is less detrimental in deterministic environments. Another issue arises when training the policy and model together, leading to an optimism bias in the learned transitions. SMCP can handle multimodal policies by training the model separately from the policy on transition data. It allows for exploration by keeping a distribution over promising trajectories and adapting to environmental changes. Two versions of SMCP were applied and compared to CEM in a 2D point mass environment. The agent can control displacement within a square with a starting position and goal specified. The goal is at g = (x = 1, y = 0.5) with the reward based on the agent's closeness to the goal. Different methods like SIR, SIS, and CEM were compared in finding solutions. Only SIR was able to effectively explore and find solutions in the planning distribution. The agent using Sequential Importance Resampling found good trajectories without collapsing on a single mode. There are two optimal trajectories due to a partial wall in the square. Planning algorithms use an isotropic normal distribution, with no need for learning as the environment's dynamics are known. The experiments with 1500 particles show how resampling can focus on promising trajectories in dealing with multimodality. The experiments on Mujoco benchmark suite BID5 focus on planning to increase RL agents' learning speed. The environment model used is a probabilistic neural network minimizing a gaussian negative log-likelihood model. Two planning algorithms, CEM and Random Shooting, are used as baselines along with SAC, a model-free RL algorithm with high performance. Additional details on architecture and hyperparameters are provided in the appendix. The SAC BID17 algorithm, a model-free RL approach, shows strong performance on Mujoco tasks and is used as a proposal distribution in planning. Results indicate that SMCP initially learns slower than CEM and RS but eventually outperforms them. SMCP also learns faster than SAC by leveraging model information early on. Challenges arise from estimating transitions and reward functions in unmodified environments. The SAC algorithm shows strong performance on Mujoco tasks and is used in planning. SMCP learns slower than CEM and RS initially but outperforms them eventually. Challenges come from estimating transitions and rewards in unmodified environments. Planning as inference has been explored in cognitive neuroscience but doesn't lead to practical algorithms. In reinforcement learning literature, planning is framed as an inference problem, but existing works simplify dynamics and do not capture full posterior distribution. Control theory uses particle filters for inferring system state for control. Recent work in model-based RL focuses on improving environment modeling. Recent work in model-based RL has focused on improving environment modeling and accounting for different types of uncertainties. BID8 compared models that consider aleatoric and epistemic uncertainties using an ensemble of probabilistic models. BID16 combined a variational autoencoder and LSTM to model the world, while BID6 used a model to enhance target for temporal difference learning. Other works have explored learning how to use a model directly. BID14 learned a good proposal distribution for SMC methods by minimizing KL divergence with the optimal proposal, similar to how SAC is used but minimizing reverse KL instead. In recent work, the connection between planning and inference has been explored, utilizing deep learning and probabilistic inference to create an efficient planning algorithm. The method combines model-free and model-based reinforcement learning, showing promising results on Mujoco tasks. However, the particle-based inference method faces challenges due to the need for many particles to approximate the posterior distribution accurately, leading to computational expenses. The method combines model-free and model-based reinforcement learning, showing promising results on Mujoco tasks. However, the particle-based inference method faces challenges due to the need for many particles to approximate the posterior distribution accurately, leading to computational expenses. Resampling can have adverse effects, such as particle degeneracy. More advanced SMC methods like Particle Gibbs with Ancestor Sampling have been proposed to address this issue. Using models of the environment learned from data can result in compounding errors for prediction over long sequences. Re-planning at each time step (Model Predictive Control) is chosen to be more robust to model errors. More powerful models or uncertainty modeling techniques can improve the accuracy of the planning algorithm. Uncertainty modeling techniques can enhance planning algorithm accuracy. SMCP showed fast learning on complex control tasks. The planning as inference framework is versatile and can be a foundation for combining probabilistic inference and deep reinforcement learning. The contribution from the action prior can be seen as a gaussian prior. The optimal value function in BID30 is defined using the forward-backward equation. In experiments, 1000 transitions are randomly collected to pretrain the model, normalize observations, and predict planning distributions for N particles. The maximum number of particles for each method is fixed at 2500. We set the maximum number of particles for each method to 2500. A custom implementation with a Gaussian policy was used for both the SAC baseline and the proposal distribution in SMCP. Adam with a learning rate of 0.001 was utilized. The reward scaling recommended by BID17 was applied, and a two hidden layer architecture with 256 hidden units was used for the value function, policy, and soft Q functions. The model was trained to minimize the negative log likelihood of the state transition distribution and predict the change in states. Additional penalty on action magnitude was manually added. The model is trained to minimize the negative log likelihood of the state transition distribution and predict the change in state. A penalty on action magnitude is manually added to simplify learning. The significance of the results is evaluated using guidelines from BID9, comparing the mean return of the method to SAC using 20 random seeds on each environment. A Welch's t-test is conducted on the average return from steps 150k to 250k for SIR-SAC and SAC, with p-values reported for each environment tested on Mujoco. HalfCheetah-v2 shows a p-value of 0.003, indicating strong evidence that the method outperforms SAC. The study presents evidence that their method outperforms SAC in various environments such as HalfCheetah-v2, Hopper-v2, and Walker2d-v2. The Effective Sample Size (ESS) improves as the proposal distribution gets better. The ESS values are around 15%, indicating no heavy weight degeneracy. Additionally, the negative log likelihood loss of the environment's model during training is reported."
}