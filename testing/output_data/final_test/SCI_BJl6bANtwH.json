{
    "title": "BJl6bANtwH",
    "content": "We present local ensembles, a method for detecting extrapolation at test time in a pre-trained model by focusing on underdetermination. The method uses local second-order information to approximate the variance of predictions across an ensemble of models from the same class. It can detect when a pre-trained model is extrapolating on test data, with applications to out-of-distribution detection, detecting spurious correlates, and active learning in vital areas where machine learning is deployed. In this paper, a method for detecting extrapolation in trained models is developed, focusing on underdetermination. Underdetermination occurs when multiple predictions are equally consistent with the training data and model specifications. This method can detect when a model is extrapolating on test data, with applications in various areas of machine learning. Local ensembles are introduced as a method to measure underdetermination in pre-trained models, specifically in the context of deep learning. This approach aims to detect unreliable predictions by assessing the variability of predictions across a set of local perturbations of the model. Local ensembles are a post-hoc alternative to fully trained ensembles, providing a computationally cheap method to measure prediction uncertainty in pre-trained models. They assess the variability of predictions across local perturbations of model parameters, addressing the limitations of approximate methods for estimating uncertainty. In this paper, the authors introduce local ensembles as a test-time method for detecting underdetermination-based extrapolation in overparameterized models. They demonstrate that this method approximates the variance of a trained ensemble with local second-order information and provide a practical way to approximate this quantity. Through experiments, they show that their method can detect extrapolation in various scenarios. The authors introduce a local ensemble extrapolation score for auditing trained models, assessing extrapolation on a point-by-point basis. The score measures variability induced by randomly choosing predictions from models with similar training loss. It is based on the norm of the prediction gradient. The score is proportional to the standard deviation of predictions across a local ensemble of models with near-identical training loss. The derivation involves defining a local ensemble of models with similar training loss and the relationship between the extrapolation score and prediction variability within this ensemble. The spectral decomposition of the Hessian matrix plays a key role in the derivation. To construct a local ensemble of loss-preserving models, we utilize eigenvectors with large eigenvalues for high curvature directions and eigenvectors with small eigenvalues for low curvature directions. Perturbations in flat directions do not significantly alter the training loss in a local minimum or saddle point scenario. This subspace is characterized by the span of eigenvectors with small eigenvalues. The ensemble subspace is defined by eigenvectors with small eigenvalues, allowing for parameter perturbations that generate models with similar training loss. The extrapolation score E m (x) quantifies the variance of test predictions due to random parameter perturbations in this subspace. It is proportional to the standard deviation of predictions at a point x. The distribution of loss-preserving parameter perturbations is characterized by the matrix U m. The linearized change in prediction variance is derived from this. The local ensemble extrapolation score E m (x) is tested on various tabular datasets by training an ensemble of neural networks with different random seeds. The comparison of mean E m (x) values to standard deviation \u0177(x) is plotted in Fig. 2 for one dataset. In Fig. 2, a linear relationship is observed between two quantities in the datasets. The relationship is weaker in the Diabetes dataset due to noisier data. Results for the standard deviation of the loss at test points can be obtained by redefining g as the loss gradient. The method for computing extrapolation scores involves constructing a set of eigenvectors that span a loss-preserving ensemble subspace. The method involves constructing a subspace using the top m eigenvectors and defining an ensemble subspace as their orthogonal complement. Perturbations in this subspace contribute negligibly to the sum, and eigenvectors are used to project gradients. The method involves constructing a subspace using the top m eigenvectors to create a \"large eigenvalue\" subspace. The projection matrix U m \u22a5 projects vectors into this subspace, while I \u2212 U m \u22a5 projects into its complement: the ensemble subspace. The success of this approach depends on choosing the right value for m, which affects the sensitivity of the extrapolation score E m (x) to underdetermination in the trained parameters. If m is set too low, E m (x) will over-estimate the prediction's sensitivity, while setting m too high will make it less sensitive. The Lanczos iteration is used to estimate the top m eigenvectors, providing practical advantages for usage in the scenario. It performs well under early stopping and returns good estimates of the top m eigenvectors after m iterations. The Lanczos iteration is efficient for estimating the top m eigenvectors. It allows for caching intermediate steps, requires implicit access to the Hessian, and is simple to implement with only one hyperparameter. Tuning the stopping value m is efficient, and the main constraint is space rather than time. In this paper, the choice of the number of eigenvectors (m) was informed by the tradeoff between memory usage and model size. Increasing m further could potentially improve performance. Techniques like online learning of sparse representations could help mitigate this tradeoff. Comparing the extrapolation score to other reliability quantification methods reveals similarities in using local information in the Hessian for inference. See Appendix B for more details on the Lanczos iteration. The Laplace approximation of the posterior predictive variance is derived by interpreting the loss function as a Bayesian log-posterior distribution over model parameters. RUE (Resampling Under Uncertainty) and influence functions are used to approximate the variability of predictions by resampling training data. Schulam & Saria combine influence functions with a random distribution of weights to estimate prediction variance under bootstrap resampling. Our method focuses on prediction variability induced by underconstrained parameters, incorporating terms with small eigenvalues to address ill-conditioned Hessians. This approach differs from other methods that struggle with underdetermination, offering a solution to the issue of extrapolation. Our method addresses prediction variability caused by underconstrained parameters by removing inverse eigenvalue weights that cause issues with inverse-Hessian methods. It offers computational advantages over methods that rely on inverting the Hessian, simplifying the process and allowing for reuse of important components for future test inputs. This caching feature is not possible with methods that require calculating the inverse Hessian-vector product for each new test input. Some methods for uncertainty detection and out-of-distribution (OOD) detection include using distances in a learned embedding space, activation space of a neural network, and nearest-neighbor baselines. Other related tasks like OOD detection and calibration have various methods such as generative models and ensemble methods. These methods have applications in uncertainty detection, active learning, and OOD detection. In this section, the text discusses various methods for uncertainty detection and out-of-distribution (OOD) detection, including approximate ensembling methods like MC-Dropout and Bayes by Backprop. It also mentions the concept of \"Rashomon sets\" exploring loss-preserving ensembles formally. The text presents evidence that local ensembles can detect extrapolation due to underdetermination in trained models through a range of experiments. The text discusses methods for uncertainty and out-of-distribution detection, including ensembling techniques like MC-Dropout and Bayes by Backprop. It explores detecting extrapolation in blind spots through various experiments, showing high AUC for OOD performance despite some inaccuracies in estimates. The text also mentions using a toy experiment to visualize the data. The text discusses detecting extrapolation in blind spots using ensembling techniques with neural networks. It highlights mistrusting predictions on extreme values and OOD tasks, showing a relationship between extrapolation score and AUC performance. The extrapolation score is related to the standard deviation of ensemble predictions, indicating whether the input is out-of-distribution. By estimating only 2 eigenvectors, >90% AUC is achieved, with 10 eigenvectors performing best. As more iterations are completed, smaller eigenvalues are found, improving AUC even with low cosine similarity to ground truth eigenvectors. This robustness is attributed to the low-dimensional ensemble subspace of the model class. In an experiment, a blind spot is created in a dataset by inducing collinearity in feature space. New features are generated as linear combinations of existing features, leading to underdetermination in relationships with the target. At test time, simulated features are sometimes sampled from their marginal distribution, breaking linear dependence and requiring extrapolation. This task can be made easier by detecting extrapolation. In an experiment, a blind spot is created in a dataset by inducing collinearity in feature space. New features are generated as linear combinations of existing features, leading to underdetermination in relationships with the target. At test time, simulated features are sometimes sampled from their marginal distribution, breaking linear dependence and requiring extrapolation. This task can be made easier by detecting extrapolation. The new data is trivially out-of-distribution. The experiment involves training neural networks on tabular datasets and comparing to nearest-neighbour baselines using different spaces for distance calculation. In an experiment, a blind spot is induced in latent space to break correlations between factors in a pre-trained model. Local ensembles (LE) outperform baselines in assigning higher scores to inputs that break collinearity. The experiment uses the CelebA dataset of celebrity faces with 40 latent binary attributes. The experiment induces a blind spot in latent space to disrupt correlations in a pre-trained model. A convolutional neural network is trained to predict a specific attribute, but fails to classify inputs where the attribute is in the blind spot. This failure is compared to baseline extrapolation scores using different methods. In Table 2, results for difficult-to-predict values of L (Male and Attractive) and C (Eyeglasses and WearingHat) are presented, showing performance of local ensembles. The method achieves strong performance compared to baselines, with notable variation between tasks. The loss gradient cannot be calculated at test time, so a separate extrapolation score is used. The performance of MaxProb and the loss gradient variant are correlated, possibly related to \u2207\u0176. The effect of increasing m is inconsistent, with discussions on relationships to eigenspectrum. Table 2 shows AUC for Latent Factors OOD detection task, with in-distribution definitions and use of local ensembles. Active learning is used to probe blind spots in the model. In active learning experiments using MNIST and FashionM-NIST datasets, a small CNN model is trained with a training set of twenty labeled data points. Local ensembles are compared to a random baseline selection mechanism, showing that the method outperforms the baseline on both datasets. In active learning experiments with MNIST and FashionM-NIST datasets, a small CNN model trained with twenty labeled data points showed improved performance over the baseline. The method's flexibility was highlighted by using only 10 eigenvectors in the Lanczos approximation, which proved to be effective. A post-hoc method called local ensembles was introduced to detect extrapolation due to underdetermination in a trained model, showcasing practical flexibility and utility. In future work, the method aims to scale to larger models and explore different stopping points. Applications in fairness and interpretability are also of interest. The Lanczos iteration is a method for tridiagonalizing a Hermitian matrix, similar to power iteration. The Lanczos iteration is a method for tridiagonalizing a Hermitian matrix by ensuring orthogonality at each step. Specialized algorithms exist for computing the final eigendecomposition efficiently. However, numerical instability can be a challenge, leading to inaccuracies. Two-step classical Gram-Schmidt orthogonalization can help produce an orthogonal basis with minimal errors. The Lanczos algorithm is simple to implement and can handle stochastic minibatch computation. Despite potential inaccuracies, even noisy estimates can be useful. Experimental results on detecting broken collinearities in feature space are presented, along with results on three tabular datasets. More details on the experiments can be found in the main text. Experimental details on datasets and models used include preprocessing steps such as mean subtraction and standard deviation division for each feature. Datasets like Boston, Diabetes, Abalone, and WineQuality were utilized, with specific data sources mentioned. Conversion of sex to a three-dimensional vector and loading datasets using Tensorflow Datasets were also highlighted. Pixel values were adjusted for MNIST and FashionMNIST datasets. For the first experiment, a two-layer neural network with 3 hidden units in each layer and tanh units was trained on data generated from y = sin(4x) + N(0, 1/4). The training set consisted of 200 points, with 100 test points and 200 out-of-distribution points. The data was aggregated over a grid of 10 points from -1 to 1 using the min function. The Lanczos algorithm was run until convergence. In the second experiment, a two-layer neural network with 5 hidden units in each layer and ReLU units was trained on data generated from y = \u03b2x^2 + N(0, 1). The error rates for in and out of distribution test sets with correlated latent factors were analyzed. The experiment involves training a two-layer MLP with ReLU activations on data with spurious correlates E (Eyeglasses) and H (Wearing Hat). The dataset includes 200 training points, 100 test points, and 200 out-of-distribution points. The model is trained with mean squared error loss and Lanczos iteration is used with specific parameters. Random features are chosen for pre-processing the data. We split 30% of the dataset as OOD and generate new features with random noise. A CNN with ReLU activations is used, consisting of two convolutional layers with 50 and 20 filters each. The model is trained with cross entropy loss and includes an extra dense layer with 30 units. Validation set is sampled randomly as 20% of the training set, and Lanczos iteration runs for 3000 iterations. In the Lanczos iteration, 3000 iterations are run using a CNN with ReLU activations, two convolutional layers, and a dense layer with 64 units. Models are trained with mean squared error loss, batch size 32, and a 100-step running average window for performance estimation. An experiment correlating latent label L and confounder C attribute is discussed in Section 5.3, showing drastically different in-distribution and out-of-distribution test errors. Tasks exhibit differing behaviors as shown in Fig. 13 and 14. In Fig. 13 and 14, tasks show varying behaviors with more eigenvectors estimated. Improved performance is seen for Male/Eyeglasses and Attractive/WearingHat tasks, but not for others. The method achieves significant improvements in these cases, indicating correct functionality. The relationship between loss gradient and MaxProb method is discussed, showing an inverted V-shape pattern. In Fig. 16, the eigenspectrums of tasks in the correlated latent factors experiment are examined. Tasks labeled Attractive have larger eigenvalues, where the lossgradient-based variant of local ensembles failed. Tasks where local ensembles performed best had a smaller prominent negative eigenvalue compared to the positive eigenvalue. The eigenspectrum of four CNNs trained on correlated latent factors is analyzed. The local ensembles method may have been less successful in other tasks due to significant eigenvalues. The method can be applied post-hoc and compared to baselines that can also be applied. The MC-Dropout method constructs ensembles at test time by averaging random dropout inferences. It is not post-hoc and works best for models with dropout on each layer. Both MC-Dropout and local ensembles estimate ensemble variance through model perturbations, but local ensembles use Gaussian perturbations projected into the ensemble subspace. Our method differs from MCDropout by creating a \"flatter\" ensemble, directly measuring underdetermination by comparing nearby models justified by training data. Using four tabular datasets, we train neural networks with dropout on each weight, creating an MC-dropout ensemble of size 50 with a dropout parameter of 0.1. Training loss is calculated for each model in the ensemble to assess its \"flatness\" on the training set. Our method creates a \"flatter\" ensemble by directly measuring underdetermination through comparing nearby models justified by training data. For each dataset, a neural network with the same architecture as in the dropout experiment is trained, and a standard deviation \u03c3 is chosen to ensure a fair comparison with perturbations on the same scale. The Lanczos iteration is run to convergence to compute the ensemble subspace for each iteration. In the experiment, Gaussian noise is sampled and added to the model's trained parameters in ensemble subspaces of varying dimensions. This creates an ensemble of size 50 for each dimension, with training loss calculated and compared to the original model. The results show that as more eigenvectors are projected out, the ensemble subspace becomes more refined. As more eigenvectors are projected out, the ensemble subspace becomes flatter, leading to smaller training loss perturbations. Comparing our method to MC-dropout ensembles, we find that our method directly measures underdetermination in the model and outperforms MC-dropout on a simulated features task. Our method outperforms MC-dropout on a simulated features task, as shown in Fig. 17."
}