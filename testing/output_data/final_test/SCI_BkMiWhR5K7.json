{
    "title": "BkMiWhR5K7",
    "content": "We introduce a framework for generating adversarial examples in a black-box setting using gradient priors. Our bandit optimization-based algorithm improves black-box attacks, requiring fewer queries and achieving higher success rates than current methods. Neural networks are vulnerable to adversarial examples, which are perturbed inputs designed to deceive network predictions. Neural networks are vulnerable to adversarial examples, perturbed inputs designed to deceive network predictions. Researchers have developed methods to construct such attacks, mostly using gradient-based techniques. However, these attacks often rely on the white-box threat model, requiring direct access to the network's classification loss gradient. In real-world scenarios, this level of access is not always feasible, limiting the attacker to issuing classification queries. In real-world scenarios, attackers are limited to issuing classification queries to targeted networks, which is a more restrictive black box threat model. Recent work has shown how zeroth-order optimization can be used to estimate gradients from classification queries for mounting gradient-based attacks, but this method requires a significant number of queries, even in improved versions. We propose a new approach for generating black-box adversarial examples using bandit optimization to exploit prior information about the gradient, breaking through the optimality of current methods. Our attack uses fewer queries and is more successful compared to previous state-of-the-art methods. Our attack is successful and uses fewer queries compared to previous methods. Adversarial examples are inputs that are perturbed to induce misbehavior in a machine learning system. We focus on untargeted attacks and consider a standard threat model with perturbations constrained by a fixed norm. Our algorithms can be adapted to targeted attacks easily. The text discusses generating adversarial examples by maximizing the classification loss function while staying close to the original input. The projected gradient descent method is commonly used for white-box attacks on bounded adversaries. The projected gradient descent (PGD) method perturbs the input in the direction that locally increases the loss the most, ensuring x is always a valid perturbation. It is designed for white-box attacks on bounded adversaries. In black-box settings, where the adversary only has access to the loss function value, methods like finite difference can still estimate gradients for attacks. This method uses value queries to estimate the directional derivative, with the step size governing the quality of the gradient estimate. In black-box settings, finite difference methods can estimate gradients for attacks using value queries. The step size \u03b4 > 0 determines the quality of the gradient estimate, with smaller \u03b4 providing more accuracy but also decreasing reliability due to precision and noise issues. This method allows for the construction of an estimate of the gradient by finding the d components through inner products with standard basis vectors. The PGD attack can be implemented using this estimator, as demonstrated by BID4 in the black-box setting. The Inception v3 BID24 classifier on the ImageNet dataset has dimensionality d=268,203, requiring 268,204 queries. The algorithm can potentially be made more query-efficient by relying on imperfect gradient estimators for successful PGD attacks. The effectiveness of the single-step PGD attack on the Inception v3 BID24 classifier varies with gradient estimator accuracy. Even estimating only 20% of coordinates correctly can lead to misclassification for over 60% of images. The adversariality rate is the portion of 1,000 random ImageNet images misclassified after one FGSM step. Our experiments show that even with imperfect gradient estimation, an adversary can cause misclassification using the iterated PGD attack. Successful attacks do not require perfect gradient estimation, but rather an estimator with a large inner product with the actual gradient. The key challenge is efficiently finding this type of estimator. The gradient estimation problem involves finding a unit vector maximizing the inner product with the actual gradient, using a limited number of function value queries. This task can be seen as an underdetermined vector estimation problem, where executing multiple queries provides information on the inner product of the gradient and chosen direction vectors. The gradient estimation problem involves finding a unit vector maximizing the inner product with the actual gradient using limited function value queries. Executing multiple queries provides information on the inner product of the gradient and chosen direction vectors, leading to an underdetermined linear regression problem. The least squares method is considered as compressive sensing requires sparsity structure not present in loss gradients. Norm-minimizing least squares estimation is a signal-processing method that approaches the estimation problem by casting it as an undetermined linear regression problem. The algorithm yields solutions similar to Natural Evolution Strategies (NES) and is proven to be equivalent to NES in certain cases. The k-query NES estimator and LSQ estimator provide non-vacuous equivalence bounds in underdetermined settings. The least-squares estimate is optimal for k = d and minimizes errors for k << d in linear regression problems. The least-squares estimator is efficient for linear regression problems with isotropic Gaussian errors. It is the minimum-variance unbiased estimator for the latent vector g. Despite the optimality of least squares, there is still room for improvement in black-box adversarial attacks. In the context of black-box adversarial attacks, prior knowledge about the gradient can be leveraged to enhance gradient estimation methods. This prior information can help improve the efficiency and effectiveness of iterative gradient attacks like PGD. In the context of black-box adversarial attacks, prior knowledge about the gradient can be leveraged to enhance gradient estimation methods. This includes considering time-dependent priors, such as the \"multi-step prior,\" which shows that successive gradients along the optimization path are heavily correlated. The normalized inner product between successive gradients shows a non-trivial correlation, indicating a potential gain from incorporating this correlation into iterative optimization. Using the gradients at time t-1 as a prior for the gradient at time t can lead to improved optimization. Data-dependent priors can be utilized to reduce query complexity in machine learning. In image classification, the spatial similarity of pixels can serve as a prior, extending to gradients as well. Comparing gradients with an average-pooled version can quantify this phenomenon. Appendix B discusses the application of mean pooling to gradients, measuring cosine similarity between average-blurred gradients and actual gradients. Results show that gradients of images are locally similar, allowing for dimensionality reduction by a factor of k^2. This reduction leads to improved black-box adversarial attack performance. The availability of informative gradient priors requires a framework for easy incorporation into black-box construction. Our proposed method utilizes a bandit optimization framework to minimize average loss in a sequence of rounds where the agent chooses actions with unknown loss functions. The goal is to compare the agent's total loss to that of the best expert in hindsight. The bandit optimization framework involves keeping track of a latent vector v t within a specified set K to incorporate prior information into gradient prediction. The gradient estimation problem is cast as a bandit optimization problem, where the action at each round t is a gradient estimate g t based on v t, and the loss t corresponds to the negative inner product between the prediction and the actual gradient. The bandit optimization framework involves using a latent vector v t to incorporate prior information into gradient prediction. The loss function t is evaluated on a prediction vector g t using the finite differences method. Different priors, such as time-dependent and data-dependent, can be incorporated by carrying over the latent vector between gradient estimations or enforcing a specific structure on it. For data-dependent priors, the latent vector can be reduced in dimensionality via average-pooling. The bandit framework for adversarial example generation incorporates a latent vector v t to predict gradients. The algorithm is general and can be applied to construct black-box adversarial examples with perturbation constraints. The loss function is defined based on the gradient estimate g, with the latent vector serving as a prior for the prediction. The algorithm for adversarial example generation involves updating the latent vector v t using a gradient estimator \u2206 t and a first-order update step. The estimator \u2206 is based on the standard spherical gradient estimator, with antithetic sampling used for estimation. The algorithm involves updating the latent vector v t using a gradient estimator \u2206 t with antithetic sampling. The gradient estimator \u2206 is a general mechanism for updating v t in bandit optimization, not directly related to the gradient estimation problem. The update rule for the convex set K is determined by the choice of A. Gradient ascent is used for K = R^n, exponentiated gradients for K = [-1, 1]^n. The algorithm combines gradient estimation with image update using boundary projection, resulting in an efficient method for creating black-box adversarial examples. Algorithm 3 shows the 2-constrained case. Evaluation is done on bandit approach and natural evolutionary strategies for generating untargeted adversarial examples. In the context of generating untargeted adversarial examples, the effectiveness of different methods is evaluated on the ImageNet BID21 dataset. The comparison includes different threat models, optimization trajectories, and classifier models like Inception-v3, Resnet-50, and VGG16. Additionally, the evaluation involves bandit approaches with time and data priors, as well as comparisons to NES in the CIFAR-\u221e threat model. The bandits framework with data-dependent and time prior (Bandits T D) outperforms the previous state of the art (NES) in generating adversarial examples on ImageNet, with six and three times less failure rates in the \u221e and 2 perturbation constraints, respectively. Our Bandits T D method outperforms NES in generating adversarial examples on ImageNet with significantly fewer queries, especially in the \u221e and 2 settings. Additionally, our method is over 6 times more query-efficient than AutoZOOM at a 100% success rate. We also have similar results for CIFAR-10 under the \u221e threat model. Our methods show superior performance in black-box attacks and gradient estimation. Our methods outperform NES in generating adversarial examples with fewer queries, especially in the \u221e and 2 settings. The loss of the classifier over time and performance on gradient estimation are plotted, showing our methods dominate NES in performance metrics. In the first line of work, algorithms use queries to perturb input for misclassification. BID19 introduced an iterative attack on binary classifiers. BID26 showed that their methods require fewer queries per successful image compared to NES. They also demonstrated success in fooling a PDF document malware classifier using genetic algorithms. In a separate line of work, researchers have developed various black-box attack strategies on deep neural networks using genetic algorithms, greedy search algorithms, finite-differences, gradient-based optimization, and natural evolution strategies. These methods aim to decrease sample complexity and mimic target network decisions by training substitute networks. The curr_chunk discusses training a substitute network to mimic a target network's decisions using black-box queries and generating adversarial examples to attack the original model. It also mentions the limitations of substitute models in creating targeted black-box adversarial examples. The perspective on black-box adversarial attacks is presented as a gradient estimation problem, with a least-squares estimator being an optimal solution. The curr_chunk discusses the development of a bandit optimization approach for black-box adversarial attacks, utilizing extra prior information about the gradient to improve success rate and query efficiency. This framework outperforms existing methods by a factor of two to six, paving the way for more efficient adversarial attacks. The curr_chunk discusses the connection between LSQ and NES in the context of constructing more efficient black-box adversarial attacks. Theorems are presented regarding Gaussian k-query NES estimators and least-squares estimators of gradients, with associated proofs. The work is supported by various grants and scholarships. The curr_chunk discusses bounding the inner products of \u03b4 i s with the gradient in the context of constructing efficient black-box adversarial attacks. The matrix A is defined as a k \u00d7 d matrix with the \u03b4 i s as its rows. The goal is to bound AA T \u2212 I = \u03bb max (AA T \u2212 I), where \u03bb max (\u00b7) denotes the largest eigenvalue. Observe that AA T and \u2212I commute and are simultaneously diagonalizable. The text discusses bounding the inner products of \u03b4 i vectors with the gradient in constructing efficient black-box adversarial attacks. The matrix A is a k \u00d7 d matrix with \u03b4 i vectors as its rows. The goal is to bound the i-th largest eigenvalue of AA T \u2212 I. By using covariance estimation theorem, it is shown that the probability of inner products exceeding a threshold is at most a certain value. Each \u03b4 i vector is randomly sampled from a distribution, contributing to the overall analysis. The text discusses efficient black-box adversarial attacks by bounding inner products of \u03b4 i vectors with the gradient. The least-squares estimator x LSQ is proven to be a minimum-variance unbiased estimator. The Cramer-Rao Lower Bound theorem is applied to show that the variance of the estimator is the inverse of the Fisher matrix. The text demonstrates that the least-squares estimator x LSQ is a minimum-variance unbiased estimator by showing that the variance of the estimator is the inverse of the Fisher matrix. This implies finite-sample efficiency, as proven through the computation of the Fisher matrix from the distribution of the samples y. Compressed sensing approaches can solve optimization problems on large datasets, but require sparsity to outperform least squares methods. Lack of sparsity in image gradients is shown through analysis of canonical bases. The lack of structural sparsity in gradients is highlighted in a convolutional neural network. Results show that successive gradients on the NES trajectory are significantly correlated, motivating the use of a time-dependent prior. Comparisons between NES, Bandits T, and Bandits T D methods on the CIFAR-10 dataset demonstrate that our methods require fewer queries per successful image for any desired success rate. Our study compares the effectiveness of attacks on CIFAR-10 using Inception-v3, ResNet-50, and VGG16 classifiers with NES and Bandits T D methods. The Bandits T D method outperforms NES, requiring fewer queries and achieving higher success rates. Our study compares the effectiveness of attacks on Inception-v3, ResNet-50, and VGG16 classifiers using NES and Bandits T D methods. The best method consistently outperforms NES on black-box attacks, requiring fewer queries and achieving higher success rates. The study compares attacks on Inception-v3, ResNet-50, and VGG16 classifiers using NES and Bandits T D methods. The best method achieves a 100% success rate and reduces queries by over 6-fold compared to NES. Table 7 shows a comparison of query-efficient attacks on ImageNet dataset using different methods. Bandits T and Bandits T D achieve a 100% success rate with significantly fewer queries compared to AutoZOOM-BiLin and AutoZOOM-AE from BID25."
}