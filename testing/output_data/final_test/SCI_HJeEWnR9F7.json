{
    "title": "HJeEWnR9F7",
    "content": "Forecasting in various applications relies on numerical solvers for partial differential equations (PDEs). Deep-learning techniques have been proposed but are limited by the fact that training data are obtained using PDE solvers, restricting their use to domains where the PDE solver is applicable. Methods are presented for training on small domains and applying the models on larger domains with consistency constraints for physically meaningful solutions. Results are demonstrated on an air-pollution forecasting model for Dublin, Ireland. Solving partial differential equations (PDEs) is crucial in various fields like computer graphics, financial pricing, civil engineering, and weather prediction. Conventional prediction approaches in PDE models rely on numerical solvers, which can be computationally intensive. The ability to generate large volumes of data is beneficial in applications where rapid prediction is essential, such as weather forecasting. The ability to use deep artificial neural networks for physical models is a key trend in applied mathematics and data science. However, their applications have not reached the same level of success as in image classification or speech recognition due to scalability issues. Recent research on deep learning for physical models still relies on PDE solvers to obtain outputs. Deep learning is used as non-linear regression for PDE solver outputs, resulting in significant computational speedup. Training DNNs on small domains with consistency constraints allows for application on larger domains. The approach involves using consistency constraints within the training of a DNN to increase the spatial domain by concatenating outputs of PDE-based models. A numerical study on pollution forecasting shows a slight decrease in accuracy but eliminates boundary artifacts. This method can be applied to patch multiple meshes or zoom in multi-resolution approaches. It is the first attempt to apply domain decomposition techniques to deep learning. The approach involves using consistency constraints within the training of a DNN to increase the spatial domain by concatenating outputs of PDE-based models. It promises the ability to ensure physical viability across multiple meshes for a physical phenomenon governed by a PDE. The method includes training a deep learning model for each sub-grid, ensuring consistency across sub-grids, and scaling up for increased prediction accuracy. Communication between meshes enables individual domain prediction with external information. The output of PDE-based simulations on a mesh consists of receptors R(M m ) and hidden points H(M m ), partitioning the mesh points. Boundaries B mn link pairs of points from two meshes with associated importance constants. Each mesh M m has simulations indexed with time t \u2208 Z, aiming to minimize residuals subject to consistency constraints for physical \"sanity\" of results. The physical \"sanity\" of results is ensured by using projection and proximity operators to map outputs from a nonlinear regression between inputs and PDE-based simulation outputs on mesh points. The mapping function f(m) must be smooth across mesh boundaries, requiring dimensions to be the same. Smoothness at boundary points is defined by the norm of the difference of arguments. The norm of a difference of arguments being \"smooth\" at a boundary of two meshes means that predicted values within the meshes are numerically close. Adding the norm of their gradient differences extends this to first derivatives. Solving equation 1 involves Lagrangian relaxation techniques, implying the existence of \u03bb (m) t such that the infimum over f (m) coincides with r * for each m \u2208 M. If some boundaries are infinite, the optimization problem becomes infinite-dimensional. The optimization problem becomes infinite-dimensional when some boundaries are infinite. Techniques from iterative solution schemes can be used, with the first term in equation 2 being finite-dimensional and separable across meshes. An iterative scheme is proposed for separable approximations, where values from the trained model at boundary points can be obtained. The iterative scheme proposed for separable approximations involves constructing vectors as bounds on model outputs at different points in meshes, forming interval constraints for elements of the model at those points. This approach simplifies computations by replacing a variable with a constant, making it easier to solve. The iterative scheme proposed for separable approximations involves constructing vectors as bounds on model outputs at different points in meshes, forming interval constraints for elements of the model at those points. In deep learning, this scheme should be seen as a recurrent neural network (RNN) with parameter sharing across different parts of the model. Training the RNN involves an optimization problem separable in m \u2208 M, providing an over-approximation for any \u03bb. Equation 4 allows for efficient training of the RNN. Equation 4 enables fast convergence in training the RNN, even for non-linear maps. The subgradient of the max function is well understood and easily implemented in deep-learning frameworks. The approach can be viewed as iterative model-order reduction in numerical analysis, particularly in multi-fidelity methods. The original PDEs serve as the full-order model to reduce, while equations 1 and 4 act as high and low-fidelity data-fit reduced-order models, respectively. The approach involves learning the non-linear mapping between inputs and predictions on each independent mesh, ensuring consistency of the solution across meshes. This draws on a history of work on setting boundary conditions as consistency constraints in solving PDEs. It can be applied not only to simple patching of tiles but also to more complex scenarios. The BID24 framework involves patching and zooming in mesh resolution changes. It utilizes a Recurrent Neural Network for city-scale pollution monitoring, incorporating atmospheric data, pollution measurements, and traffic data. The approach ensures consistency across meshes by learning the non-linear mapping between inputs and predictions. Our test case in Dublin, Ireland, lacks large-scale air pollution models despite real-time traffic, pollution, and weather data availability. Traffic contributes significantly to nitrogen oxides (NOx) and Particulate Matter (PM) levels in cities, with diesel engines being a major factor. We aim to estimate and predict traffic-induced air pollution levels of NOx, PM2.5, and PM10 across the city using an air pollution dispersion model. The air pollution dispersion model used in Dublin is based on the Gaussian Plume model, which estimates pollution levels from roadway links. Inputs include traffic volumes and atmospheric data, while parameters like emission factors and pollution dispersion coefficients are considered. The model provides periodic estimates of pollution levels. The study uses Caline 4, an open-source dispersion modeling suite, to solve the Gaussian Plume model for different cities with heterogeneous parameters. Caline is limited to 20 line sources and receptors per solve, motivating the use of a deep learning approach for inhomogeneous discretization of the road network. The deep learning approach is implemented for the use case of Dublin, Ireland, where the area is divided into 12 domains with pollution sources. Inputs to the PDE solver include traffic volume data from SCATS deployment and weather data from The Weather Company. Training data covers almost a year of hourly data. Outputs consist of NO2, PM2.5, and PM10 concentrations at predefined receptors per domain. The RNN model is implemented in Tensorflow to obtain non-linear regression between inputs and outputs, with consistency constraints applied iteratively. Domain knowledge is used to pick specific parameters for each mesh based on expected accuracy of the PDE-based model. The study implemented a RNN model in Tensorflow with consistency constraints for non-linear regression. Parameters were chosen based on domain knowledge for accuracy. Validation was done using hourly NOx concentrations at 6 sites in the city. The deep-learning forecaster showed good performance with a mean absolute percentage error of about 1. The deep-learning forecast error was reduced from 1% to 7% using consistency constraints. Boundary artefacts disappeared after training iterations. Consistency constraints allow training on small domains and applying to larger ones, ensuring physically meaningful solutions. Promising results were shown in an air-pollution forecasting model. The work demonstrates promising results on an air-pollution forecasting model for Dublin, Ireland, using DNN. Possible extensions include exploring other network topologies like LSTM units, over-fitting control with improved architectures, and interpretation of the trained model. The work also showcases Geometric Deep Learning in conjunction with mesh-free methods for 3D point clouds and non-uniform meshing. The curr_chunk discusses the potential applications of non-uniform meshing and choice of receptors within meshes in 3D or higher dimensions. It also mentions the integration of solving PDEs with geometric modeling in isogeometric analysis. Additionally, it suggests generalizing methods in multi-fidelity modeling by combining reduced-order and full-order models using adaptation, fusion, or filtering. The scaling up of deep learning for PDE-based models is highlighted as a fruitful area for further research, particularly in air pollution forecasting. Consider pollutants like ground-level ozone concentrations and ensemble or multi-fidelity methods. A joint model combining traffic forecasting, weather forecasting, and air pollution forecasting using LSTM units could be explored."
}