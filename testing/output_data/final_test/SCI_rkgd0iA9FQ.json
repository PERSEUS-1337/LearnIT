{
    "title": "rkgd0iA9FQ",
    "content": "In this work, the authors delve into the theoretical convergence properties of popular algorithms RMSProp and ADAM for training neural nets. Recent research suggests that these algorithms may have worse generalization properties compared to stochastic gradient descent. The authors provide proofs that ADAM and RMSProp are guaranteed to reach criticality for smooth non-convex objectives, along with bounds on the running time. In experiments comparing RMSProp, ADAM, and Nesterov's Accelerated Gradient method on various autoencoder setups and VGG-9 with CIFAR-10, ADAM shows sensitivity to its momentum parameter \\beta_1. At high values (\\beta_1 = 0.99), ADAM outperforms NAG in terms of lower training and test losses. However, NAG can perform better when ADAM's \\beta_1 is set to the commonly used value of 0.9, highlighting the importance of hyperparameter tuning for better generalization performance. The experiments demonstrate that NAG outperforms other autoencoders in reducing gradient norms and exhibits an increasing trend for the minimum eigenvalue of the Hessian. Optimization problems in machine learning often involve a finite sum optimization structure, with neural network problems falling under a similar non-convex function setup. Stochastic Gradient Descent (SGD) is a common algorithm used to solve such problems, often with momentum added for improved performance. The Heavy-Ball (HB) method and Nesterov's Accelerated Gradient (NAG) are popular variants of Stochastic Gradient Descent (SGD) with momentum. These methods have shown superior convergence properties compared to gradient descent for both convex and non-convex functions. In the stochastic case, the benefits of NAG and HB over regular SGD are not theoretically justified in general. However, in practice, NAG has shown good convergence and generalization on neural net problems. The performance of NAG, HB, and SGD is sensitive to hyper-parameter selection. Adaptive gradient algorithms like RMSProp and ADAM have become popular for optimizing deep neural networks. Adaptive gradient methods like RMSProp and ADAM are popular for optimizing deep neural networks due to their ease of tuning compared to SGD, NAG, or HB. These methods use a linear transformation of the history of gradients, making them less sensitive to hyperparameter selection. However, they lack theoretical justifications in the non-convex setting. Adaptive gradient methods like RMSProp and ADAM lack theoretical justifications in the non-convex setting, even with exact gradients. Studies have shown that SGD and HB generalize better than RMSProp and ADAM with stochastic gradients. Additionally, ADAM generalizes poorly for large neural networks, while RMSProp performs better in certain tasks like character-level language modeling. This work provides the first convergence guarantees for adaptive gradient methods in neural-net training, specifically for deterministic RMSProp and ADAM. It sheds light on the relative performances of different algorithms and their generalization abilities in the full-batch setting. Our findings show that ADAM and RMSprop may not converge to zero average regret in certain sequences of convex functions. We also conduct empirical experiments comparing their performance with NAG on autoencoder tasks using MNIST data in full and mini-batch settings. In autoencoder experiments on MNIST data, ADAM with high momentum (\u03b2 1 = 0.99) outperforms NAG and RMSProp in full-batch settings. As autoencoder size increases, RMSProp fails to generalize. Similar behavior is observed in mini-batch experiments. Results on CIFAR-10 using VGG-9 CNN are presented. Tuning ADAM's \u03b2 1 closer to 1 helps close the generalization gap with NAG and HB. Tuning ADAM's \u03b2 1 closer to 1 helps close the generalization gap with NAG and HB. Definitions of smoothness property and square-root of diagonal matrices are provided for non-convex objectives. The pseudocodes for NAG, RMSProp, and ADAM algorithms are listed, along with the initialization steps and oracle access for gradients of f. Mini-batch RMSProp's effectiveness has been demonstrated in previous research. The text discusses the effectiveness of mini-batch RMSProp for autoencoding on depth 2 autoencoders trained on MNIST data. It also introduces the concept of convergence to criticality for stochastic RMSProp under certain technical assumptions. The gradient oracle is defined, and the algorithm for convergence is outlined. The text discusses the effectiveness of mini-batch RMSProp for autoencoding on depth 2 autoencoders trained on MNIST data. It introduces the concept of convergence to criticality for stochastic RMSProp under certain technical assumptions, defining a gradient oracle and outlining the algorithm for convergence. The theorem guarantees finding a critical point in a certain number of steps, even if sign constraints on gradients only hold at points visited by stochastic RMSProp. This constraint ensures gradient options lie in the same orthant of Rd, although it may change between iterations. The text discusses the convergence of deterministic RMSProp without sign conditions, proving convergence with a constant step length and no need for the \u03be parameter. The text discusses a variant of deterministic RMSProp without the \u03be parameter, which weakens convergence rates. Theorem 3.3 proves convergence of this version with no \u03be shift. In experiments with full-batch ADAM, deterministic ADAM is analyzed in the small \u03b2 1 regime. Theorem 3.4 states that Deterministic ADAM converges to criticality for a smooth function with a minimizer. The algorithm requires specific step sizes and a natural number T for convergence. If certain parameters are set, T can be determined based on the second iterate of the algorithm. Theorem 3.4 shows that Deterministic ADAM converges for smooth functions with a minimizer, requiring specific step sizes and a natural number T for convergence. The analysis aims to understand when ADAM can converge, with potential for faster rates than currently shown. The \"bias correction term\" in ADAM is linked to its step length, with the theorem highlighting the theoretical basis for this term. Experimental tests on ADAM and RMSProp performance are conducted on autoencoders and CIFAR-10 using VGG-9. The autoencoder model uses ReLU activations and shared weights on CIFAR-10 with VGG-9. The loss function is defined for input z, and comparisons of ADAM and RMSProp in a regression setting are limited. In a study inspired by Rangamani et al. (2017), experiments were conducted on autoencoders using RMSProp for MNIST with 2 layer ReLU networks. All network layers were kept at the same width (denoted as h) to analyze the impact of increasing depth or width without additional variables. Weight matrices were initialized using Glorot initialization, sampling from a uniform distribution. The study conducted experiments on autoencoders using different optimization algorithms (ADAM, RMSProp with NAG) on the MNIST dataset. Hyperparameters were tuned using grid search, and the best set was chosen based on the lowest loss on the training set after 10^5 iterations. The MNIST images were cropped from 28x28 to 22x22 to reduce computation time for testing various neural net architectures. The study compared different neural net architectures by cropping MNIST images to 22x22 size. A subset called mini-MNIST was created with 5500 training images and 1000 test images, representing 10% of the full dataset. This subset closely mirrors the distribution of labels in the full MNIST dataset. In a mini-batch setup with a fixed size of 100, experiments were conducted on the full training set of MNIST and CIFAR-10 using VGG-9. The parameter \u03be in RMSProp and ADAM implementations plays a crucial role in lowering gradient norms and test losses, as shown in experimental evidence. The parameter \u03be in RMSProp and ADAM implementations plays a crucial role in lowering gradient norms and test losses, as shown in experimental evidence. To track movement from \"bad\" to \"good\" saddle points, the most negative eigenvalue of the Hessian is monitored. Using scipy.sparse.linalg.eigsh, eigenvalues of the Hessian are computed efficiently through matrix-vector products. This method allows for accurate computation without storing the full Hessian matrix in memory. The NAG algorithm shows a gradual increase in the minimum eigenvalue of the Hessian and a decrease in the gradient norm. However, RMSProp and ADAM perform better in actual numbers. The training loss, test loss, and gradient norms vary for different algorithms on a 3 hidden layer autoencoder with 1000 nodes in each hidden layer. More comparisons for various neural net architectures can be found in the appendices. Increasing momentum helps NAG achieve lower gradient norms, although it may negatively impact training or test performance on larger networks. NAG consistently shows the lowest gradient norms compared to other algorithms, except for single hidden layer networks. On the other hand, pushing \u03b2 1 closer to 1 significantly improves ADAM's performance, resulting in lower training and test losses compared to other algorithms. In mini-batch experiments on training autoencoders with the full MNIST dataset, NAG performs better than ADAM on small networks. However, for larger networks, ADAM with a momentum parameter closer to 1 shows better generalization with significantly lower test losses. Overall, ADAM outperforms NAG in terms of test loss, training loss, and gradient norm reduction metrics. In general, both ADAM and NAG show improved performance when their momentum parameter is closer to 1. NAG excels in reducing gradient norms, while ADAM with high momentum performs best for training error on larger networks. Theoretical guarantees of convergence to criticality are presented for RMSProp and ADAM in optimizing non-convex objectives. The interplay between adaptivity and momentum in training networks is explored through experiments on autoencoder architectures. The value of the gradient shifting hyperparameter \u03be significantly influences the performance of ADAM and RMSProp. ADAM performs well when its momentum parameter \u03b21 is close to 1. Experimental results on VGG-9 with CIFAR-10 and training autoencoders on MNIST support these conclusions across different network widths and depths, batch settings, and image size compression. The regime of \u03b21 close to 1 poses challenges for proving convergence in ADAM, suggesting a need for further theoretical advancements. The proof of Theorem 3.1 is presented, involving the definition of \u03c3 t, bounds on v t, and the use of eigenvalue bounds. The update step of stochastic RMSProp is discussed, along with the set of random variables H t corresponding to the first t iterates. The assumptions about the stochastic oracle are also considered. The proof of Theorem 3.1 involves defining \u03c3 t, bounds on v t, and eigenvalue bounds. Stochastic RMSProp update step is discussed with random variables H t for first t iterates. Assumptions about the stochastic oracle are considered, leading to analysis and substitutions in equations. Stochastic RMSProp reaches -criticality in T iterations. New variables are introduced in the proof. In the proof, new variables are introduced to simplify the analysis. The expectation over the oracle call at the t th update step is equivalent to random sampling. Substituting variables back into the equation leads to the explicit form of the needed expectation. The lemma is proved by showing that the claim holds for all cases. The lemma is proved by showing that the claim holds for all cases. Variables are introduced to simplify the analysis in the proof. The expectation over the oracle call at the t th update step is equivalent to random sampling. Substituting variables back into the equation leads to the explicit form of the needed expectation. The proof involves bounding terms and showing relationships between them. The lemma is proven by introducing variables to simplify the analysis. The recursion for v t is solved, leading to bounds and inequalities. A parameter \u03b2 t is defined to satisfy conditions, allowing for a constant step length \u03b1 t to be determined. The lemma introduces variables to simplify the analysis and solves the recursion for v t, leading to bounds and inequalities. A parameter \u03b2 t is defined to satisfy conditions for determining a constant step length \u03b1 t. The lemma introduces variables to simplify the analysis and solves the recursion for v t, leading to bounds and inequalities. A function P (T ) is defined, and by substituting bounds into an inequality, a point x result is obtained. For T = O( 1 4 ), the algorithm is guaranteed to find at least one point. Proof by contradiction shows a relationship between values at consecutive updates, leading to the determination of a step length \u03b1 * at the midpoint of an interval. The lemma introduces variables to simplify the analysis and solves the recursion for v t, leading to bounds and inequalities. The recursion of v t can be solved, and a sequence of functions is defined for each i = 0, 1, 2.., t. By substituting the update rule for m t, we can analyze the sequence of functions. After introducing variables to simplify the analysis and solving the recursion for v t, bounds and inequalities are derived. By substituting the update rule for m t, the sequence of functions is analyzed, leading to lower bounds and inequalities. The assumption is that g t > 0 and 0 < \u03b2 1 < 1. This leads to defining constants and substituting equations to prove a lower bound for Q t. The final step length \u03b1 t guarantees a decrease in the function value. The hyper-parameters of optimization algorithms like NAG, RMSProp, and ADAM are tuned by adjusting parameters such as step size, momentum, decay, and perturbation. Step sizes for ADAM are varied conventionally. The tuning method follows a logarithmically-spaced grid approach. The hyper-parameters of optimization algorithms like NAG, RMSProp, and ADAM are tuned by adjusting parameters such as step size, momentum, decay, and perturbation. The tuning process involves varying step sizes and momentum values over specific ranges. The perturbation value is set to a default value, except for specific experiments where its effect on convergence and generalization properties is analyzed. In the context of tuning hyper-parameters for optimization algorithms like NAG, RMSProp, and ADAM, the text discusses the initialization of the gradient v0 and the tuning of step sizes and beta values for ADAM. It mentions using a modified version of RMSProp with v0 set to 0 for faster convergence. The experiments involved varying step sizes and beta values, with a perturbation value of \u03be = 10^-8 for ADAM. Setting \u03b21 = 0.99 instead of the usual 0.9 yielded better results for the autoencoder problem. In experiments testing hyper-parameters for optimization algorithms like NAG, RMSProp, and ADAM, the study focused on changing \u03be values while keeping other parameters fixed. Resizing input images to 17 \u00d7 17 and 12 \u00d7 12 dimensions was also explored. Results showed that ADAM with \u03b21 = 0.99 converged fastest and generalized the best, while NAG outperformed ADAM with \u03b21 = 0.9 on a 3 layer network with 1000 hidden nodes in each layer. In experiments with various neural net architectures on mini-batches of size 100 and input dimension of 22 \u00d7 22, results showed that most full-batch results extended to the mini-batch case. The experiments included networks with 1, 3, and 9 hidden layers, each with 1000 nodes. The results were presented in FIG1, showing the loss on the training set, test set, and the norm of the gradient on the training set."
}