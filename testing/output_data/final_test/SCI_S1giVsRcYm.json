{
    "title": "S1giVsRcYm",
    "content": "The paper introduces a simple approach for exploration in reinforcement learning, focusing on tasks with large state-spaces requiring generalization. It is based on the substochastic successor representation, allowing for theoretically justified algorithms in the tabular case and providing insights for settings with function approximation. The substochastic successor representation can implicitly count the number of times each state has been observed, connecting two disjoint areas of research. Empirical results show its performance in traditional tabular domains and a deep reinforcement learning algorithm inspired by these ideas matches recent pseudo-count-based methods in hard exploration Atari 2600 games. Reinforcement learning involves an agent learning to act optimally through trial and error interactions with the environment to maximize the sum of rewards. In reinforcement learning, agents must balance immediate and delayed rewards to explore the state space efficiently. The number of samples needed for learning is influenced by how quickly the agent explores the state-space. Despite success stories, the common approach is to select exploratory actions randomly. In reinforcement learning, random exploration is common but often fails in environments with sparse rewards. Model-based approaches involve agents learning the environment dynamics to plan future actions effectively. Sparse rewards refer to environments where the agent receives a reward signal of zero for most states. Model-free methods are popular in large environments where it is difficult to enumerate all states. These methods estimate state values directly from transition samples, making systematic exploration more challenging. Despite this, model-free approaches are scalable and commonly used in practice. In this paper, an algorithm for exploration based on the successor representation (SR) is introduced. The SR generalizes between states using the similarity between their successors, learned through temporal-difference learning. Inspired by the substochastic successor representation (SSR), this approach aims to address exploration challenges in large domains. The SSR is a concept introduced to implicitly count state visitation for exploration in model-based RL. It connects representation learning and exploration, leading to sample-efficient algorithms. Additionally, it aids in designing a model-free deep RL algorithm that performs well in hard exploration Atari 2600 games. The value of a state in reinforcement learning is defined as the expected sum of discounted rewards when following a policy. This can be computed recursively using transition probabilities and reward functions. Traditional model-based algorithms learn estimates of these functions to make predictions. In reinforcement learning, estimates of transition probabilities and rewards are used to estimate the value of a state. Model-free RL directly estimates the value function from samples using temporal-difference learning. Generalization is necessary for problems with large state spaces by parametrizing the value function with weights. Our algorithm utilizes the successor representation (SR) for estimating value functions in reinforcement learning. The SR, denoted as \u03a8 \u03c0 , is defined with respect to a policy \u03c0 and can be estimated through TD learning. Successor features extend the SR to function approximation settings, providing a generalized solution for large state spaces. Successor features generalize the SR to the function approximation setting. The successor features can be learned with TD learning. The successor features for a state s are defined for a given policy \u03c0 and feature representation \u03c6(s) \u2208 R d. The concept of the substochastic successor representation (SSR) is introduced, derived from an empirical transition matrix with a small probability of terminating at each state. The SSR can be used to recover visit counts n(s) through algebraic manipulation. The SSR is used as inspiration for a new deep reinforcement learning algorithm for exploration. It approximates learning the SR from an uninformative initialization and uses a stochastic update rule. The learned SR guides exploration and shows comparable performance to theoretical algorithms in synthetic settings. Definition 3.1 defines the Substochastic Successor Representation as a matrix induced by the environment's dynamics and policy. Theorem 1 formalizes the idea that the 1 norm of the SSR implicitly counts state visitation. It involves the empirical transition matrix and the top eigenvector of a stochastic matrix. The top eigenvector of the successor representation (SR) has an eigenvalue. By using a bound on the quadratic term, it is possible to recover state visitation counts from the SR. The SSR, obtained by modifying the SR, can be converted into a reward function by defining an exploration bonus. This bonus is added to the reward being maximized by the agent, with a scaling parameter \u03b2. The proposed exploration bonus incentivizes agents to visit less common states quickly by penalizing visits to commonly visited states. Performance of the algorithm is evaluated over 100 runs with a 95% confidence interval reported. Results show effectiveness in a standard model-based algorithm with updates to transition and reward models. The algorithm's performance was evaluated using RiverSwim and SixArms domains, showing comparable results to R-MAX and outperforming on SixArms. Learning the SSR is challenging in large environments, especially when the representation is non-stationary. In the function approximation setting, an algorithm is designed for learning the successor representation directly using TD learning. By pessimistically initializing estimates, TD tends to underestimate values, capturing the SSR accurately. This approach is effective in environments where transition probabilities are not explicitly estimated. The algorithm combines ideas from recent work to learn successor features jointly with the feature representation. A neural network is used to learn the agent's value function and successor representation. The architecture is based on DQN with adjustments to match a known successful architecture. The part of the architecture that predicts state-action values is referred to as DQN e and is trained to minimize a certain function. The architecture, based on DQN, predicts state-action values and is trained to minimize a loss function. Successor features are computed by the network's bottom layers, ensuring all features are normalized and non-negative. Successor features are computed by the two bottom layers of the network, minimizing loss. To address the issue of zero loss, we do not propagate gradients and introduce an auxiliary task to encourage learning before non-zero rewards. Additionally, an intrinsic reward is defined to promote exploration. Successor features are computed by the network to minimize loss and an auxiliary task is introduced to encourage learning before non-zero rewards. An exploration bonus is determined based on the inverse of the 2-norm of the vector of successor features of the current state, with positive rewards being preferred in DQN experiments. The network is initialized similarly to BID18, using Xavier initialization in most layers except for the fully connected layers around element-wise multiplication. The evaluation protocol for training agents in MONTEZUMA'S REVENGE involved using a frame skip of 5 with a full action set. The agent learns from raw pixels and was trained with RMSprop using standard parameters for DQN. Exploration was annealed in DQN's -greedy strategy over the first million steps. The evaluation protocol for training agents in MONTEZUMA'S REVENGE involved using a frame skip of 5 with a full action set. The agent learns from raw pixels and was trained with standard parameters for DQN. The discount factor, \u03b3, is set to 0.99 and weights w TD = 1, w SR = 1000, w Recons = 0.001 were adjusted for loss function scaling. Results after 100 million frames are summarized, comparing the performance of different algorithms including DQN MMC e +SR, DQN MMC +CTS BID2, and DQN MMC +PixelCNN BID20 for exploration. The algorithm compared its performance to a traditional DQN network without additional modules or intrinsic reward bonus, showing significant improvement. Learning curves and results analyzing the impact of auxiliary tasks are available in the Appendix, demonstrating higher scores than DQN in challenging games. The algorithm achieves higher scores than DQN in challenging games, especially in MONTEZUMA'S REVENGE. The exploration bonus has a significant impact, with improvements in games like GRAVITAR and VENTURE. Comparisons to DQN MMC +CTS and DQN MMC +PixelCNN show similar performance without requiring a density model. In this paper, evidence is provided that the algorithm performs as well as others with theoretical guarantees in tabular, model-based cases. The algorithm augments the state-space with an imaginary state to reduce uncertainty, unlike R-MAX which deletes transitions after a certain number of visits. The algorithm discussed in the current text chunk performs well in large domains without requiring a density model, using pseudo-counts through density models as inspiration. It shows comparable performance to other model-free approaches with proven sample-complexity bounds. The SSR captures transition dynamics and has been used for exploration. Previous work has shown promise in tabular cases but scalability in large domains like Atari games is uncertain. Neural network architecture is motivated by prior research on predicting next screens and learning successor representations from raw pixels in RL algorithms. In this paper, a general method for exploration in RL is introduced, which implicitly counts state visitation to guide the exploration process. This approach is compatible with representation learning and can be adapted for large domains, offering new possibilities for future work. The supplementary material discusses potential future work based on the results presented in Section 3, including using the substochastic successor representation for generating algorithms with PAC-MDP bounds. It also suggests studying the impact of different auxiliary tasks on algorithm performance and exploring the connection between representation learning and exploration. The material includes details such as pseudo-code of the model-based algorithm, descriptions of tabular domains used in evaluation, and learning curves of DQN and DQN MMC. The supplementary material includes details on the evaluation of DQN and DQN MMC in Atari 2600 games, additional experiments on the role of auxiliary tasks in ESSR, and pseudo-code for the model-based algorithm. The ESSR algorithm relies on a network to estimate state-action values, successor representations, and next observations. Experiments show the benefit of using an exploration bonus from the successor representation, but the impact of the auxiliary task is not clear. The experiments focus on Montezumas Revenge to address the issue of exploration. Dropping the auxiliary task resulted in lower performance, indicating its necessity for the algorithm to perform well. The experiments showed that auxiliary tasks are necessary for the algorithm to perform well. Dropping the exploration bonus and successor representation module resulted in lower performance in Montezuma's Revenge. The auxiliary task alone was not sufficient for the reported performance. Figure 4 shows learning curves for algorithms in Atari 2600 games with different amounts of experience. Lighter lines represent individual runs, while solid lines show the average over multiple runs. The curves are smoothed using a window of size 100."
}