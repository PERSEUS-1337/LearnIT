{
    "title": "H1gsz30cKX",
    "content": "Normalization layers are commonly used in deep neural network architectures to stabilize training, enable higher learning rates, accelerate convergence, and improve generalization. However, a new approach called fixed-update initialization (Fixup) challenges the belief that these benefits are unique to normalization. Fixup addresses the exploding and vanishing gradient problem at the start of training by properly rescaling a standard initialization. It has been shown to stabilize training for networks with thousands of layers and can achieve state-of-the-art performance in image classification and machine translation without the need for normalization. The development of novel neural network models and training techniques has led to major advances in artificial intelligence applications, particularly in image classification and machine translation. State-of-the-art image recognition systems now rely on deep network blocks with convolutional layers, skip connections, and normalization mechanisms like batch normalization. Various normalization techniques are also crucial for achieving good performance in tasks such as machine translation and generative modeling. These techniques help stabilize learning and enable the training of very deep neural networks. Fixup is a method proposed for training very deep neural networks without normalization. It rescales the standard initialization of residual branches to stabilize learning, enable higher learning rates, accelerate convergence, and improve generalization. The method addresses the exploding gradient problem of residual networks at initialization and outperforms state-of-the-art normalization methods on real-world benchmarks. The Fixup method proposed for training deep neural networks without normalization addresses the exploding gradient problem in residual networks at initialization. It rescales the standard initialization of residual branches to stabilize learning, enable higher learning rates, accelerate convergence, and improve generalization. The method does not properly account for the effect of residual connections, causing exploding gradients in ReLU networks. The ResNet output variance grows exponentially with depth due to the lack of normalization techniques. The analysis focuses on the variance of each layer in a plain ResNet with residual blocks, assuming zero initialization for simplicity. The ResNet structure prevents the output variance from vanishing by forcing it to grow with depth, leading to exponential explosion without normalization. This can cause gradient explosion, as the gradient norm of certain activations and weight tensors is lower bounded by the cross-entropy loss at initialization. This applies to convolutional and linear weights in neural networks with ReLU nonlinearity. The analysis focuses on positively homogeneous functions in neural networks without normalization, such as ResNet and DenseNet with ReLU nonlinearity and skip connections. Positively homogeneous sets of first degree involve parameters that exhibit specific properties under scaling. Examples of positively homogeneous functions are common in neural networks, including linear functions. In neural networks, positively homogeneous functions like linear operations without bias and ReLU nonlinearity are common. A function composed of positively homogeneous functions is also positively homogeneous. The study focuses on classification with cross-entropy loss, using a neural network function denoted as f. The cross-entropy loss is defined with logits and the average loss, with specific assumptions about the network. The study focuses on classification with cross-entropy loss using a neural network function denoted as f. Theorems are presented regarding the gradient norm and softmax probabilities in the network without normalization layers. Examples of positively homogeneous sets in a ResNet are illustrated. In Figure 2, examples of positively homogeneous sets in a ResNet without normalization are shown. The study highlights the failure mode of standard initializations for training deep residual networks, emphasizing the importance of proper update scale to the network function. A new initialization approach is proposed for successful training. The study emphasizes the importance of proper update scale to the network function in training deep residual networks. The goal is to design an initialization for SGD updates that are in the right scale and independent of depth. Residual branches update the network in sync, with each branch changing the network output in correlated directions. An SGD step to each branch should change the network output by \u0398(\u03b7/L) on average to achieve an overall \u0398(\u03b7) update. The study focuses on designing an initialization for SGD updates that are in the right scale and independent of depth. It explores how to initialize a residual branch with m layers so that its SGD update changes the network output by \u0398(\u03b7/L). The study also discusses the constraints for F(x) to make \u0398(\u03b7/L) updates and how to rescale the weight layers of a standard network. The study explores how to initialize a residual branch with m layers so that its SGD update changes the network output by \u0398(\u03b7/L). It discusses constraints for F(x) to make \u0398(\u03b7/L) updates and how to rescale weight layers of a standard network. Additionally, new methods for initializing a residual branch through rescaling the standard initialization of each layer are suggested. To match the training performance of a network with normalization, biases and multipliers need to be considered. Biases in linear and convolution layers help restore representation power after normalization. Inserting scalar biases before each weight layer and activation layer in a residual network without normalization improves training performance. Multipliers scale the output of a residual branch. To train residual networks without normalization, biases and multipliers are essential. Biases restore representation power, while multipliers scale the output of a residual branch. Inserting scalar biases and one scalar multiplier per residual branch can mimic the weight norm dynamics of a network with normalization, eliminating the need for a new learning rate schedule. This method, known as Fixup initialization, allows for training deep residual networks effectively. Initialization for training deep residual networks without normalization involves setting the classification layer and last layer of each residual branch to 0, scaling weight layers inside branches, and adding scalar multipliers and biases. Rule 2 of Fixup is crucial for training deep networks, while Rules 1 and 3 further enhance performance. Ablation experiments support these findings. Our approach suggests delicate control of initialization scale is beneficial, aiming to match BatchNorm's fast training for deep models. We evaluate methods on test accuracy with increasing depth using wide residual network architecture. In our study, we compare Fixup to three baseline methods for deep models, using default weight decay and learning rate settings. We limit the number of epochs due to computational constraints and demonstrate the applicability of Fixup in Transformer for machine translation. In this study, Fixup is compared to baseline methods for deep models in machine translation using the fairseq library. Dropout probabilities are optimized for different datasets, and training with Fixup is found to be stable and fast. Results show that models do not overfit when LayerNorm is replaced by Fixup. Fixup matches or surpasses state-of-the-art results using Transformer models on various datasets without suffering from overfitting. Unlike traditional normalization methods, Fixup does not compute statistics during training, making it a unique approach for training deep neural networks. Theoretical analysis of deep networks focuses on training very deep neural networks. Recent studies on dynamical isometry enable training 10,000-layer CNNs from scratch. For residual networks, properties like activation scale, gradient variance, and dynamical isometry have been explored. The standard initialization for residual networks is found to be problematic, but a novel approach using positive homogeneity for bounding the gradient norm is proposed, applicable to various neural network architectures. The paper provides a novel initialization scheme for training deep residual networks without batch normalization successfully. It discusses the optimization landscape of linearized residual nets and challenges the effectiveness of Xavier or He initialization schemes. The analysis aims to understand the impact of batch normalization on neural network training. The paper proposes a new initialization method for training deep residual networks without batch normalization. It challenges the effectiveness of standard initialization methods like Xavier or He's by showing that they lead to steep loss surfaces. The study suggests using smaller initializations for residual networks to alleviate this issue. Additionally, it incorporates a multiplier in each residual branch inspired by previous research on the impact of (batch) normalization and weight decay on learning rates. Other approaches, such as data-dependent initialization and recurrence, have also been proposed to address the initialization problem in residual nets. The paper introduces a new Fixup initialization method for training deep residual networks without batch normalization. It addresses the exploding gradient problem at initialization and suggests initializing residual branches close to zero. The study highlights the importance of choosing a good initialization scheme for reliable training of deep residual networks. In Section 3, the Fixup initialization method is developed to ensure proper updates for the entire network and each residual branch. Extensive experiments show that Fixup matches normalization techniques in training deep networks and achieves state-of-the-art test performance with regularization. The work opens up new possibilities for theory and applications, including analyzing training dynamics, extending the initialization scheme, understanding regularization benefits, and improving test performance. The proof idea for Theorem 2 involves showing that the gradient of the cross-entropy loss with respect to the scaling factor can be computed. By invoking the directional derivative argument, it is demonstrated that the FC layer weights have a symmetric probability density function with mean 0. This leads to an estimation of the lower bound for the scale, utilizing the fact that logsumexp(z) is greater than or equal to the maximum value of z. The analysis shows that the scale of actual change to the network function made by a gradient descent step is crucial. If updates to different layers align, the network may undergo a drastic change in one step, even if each layer changes minimally. This scenario is more reflective of reality. The analysis reveals that the network function can undergo significant changes in one step if updates to different layers align, reflecting reality. The assumptions include a specific network architecture with fully-connected weight layers, ReLU activation functions, and no bias parameter. The gradient update to the network is discussed in Theorem 3 under these assumptions. Theorem 3 discusses the gradient update to the network parameters, emphasizing the importance of each residual branch contributing only a \u0398(\u03b7/L) update on average to allow the whole network to be updated by \u0398(\u03b7) per step independent of depth. The proof involves replacing ReLU activation layers with diagonal matrices and ensuring all positive preactivations remain positive with a sufficiently small learning rate \u03b7. The gradient update to network parameters is discussed in Theorem 3, emphasizing the importance of each residual branch contributing only a \u0398(\u03b7/L) update on average. The gradient w.r.t. the i-th weight layer in the l-th block is calculated, and the proper initialization of a scalar branch F(x) is focused on in this section. Theorem 4 states the result assuming certain conditions are met. The text discusses the importance of proper initialization for network parameters, focusing on achieving a desired update scale for the network function. Theorem 4 provides guidance on rescaling the standard initialization. Additionally, the section presents training curves for different architecture designs and initialization schemes, comparing batch normalization, Fixup, and various ablated options. The experiments conducted validate the hypothesis that the gap in test error between Fixup and batch normalization is mainly due to overfitting. To address overfitting, Mixup and Cutout BID4 with default hyperparameters are used as additional regularization techniques. The experiments are performed on CIFAR-10 dataset with WideResNet-40-10 and on SVHN with WideResNet-16-12, showing that models trained with Fixup and strong regularization are effective. Models trained with Fixup and strong regularization are competitive with state-of-the-art methods on CIFAR-10 and SVHN, as well as our baseline with batch normalization. Without additional regularization, Fixup fits the training set well but overfits significantly. Fixup is competitive with networks trained with normalization when the Mixup regularizer is used.Normalization in neural networks dates back to biological visual system modeling and includes methods like local response normalization, batch normalization, and layer normalization. Normalization methods in neural networks, such as weight normalization and activation normalization, play a key role in improving model performance."
}