{
    "title": "rkevSgrtPr",
    "content": "The universal approximation theorem states that a feedforward neural network with one hidden layer can approximate any continuous multivariate function to a given threshold, as long as the activation function is non-polynomial. The paper provides an algebraic proof of the theorem and quantifies the number of hidden units needed for approximation. It shows that for a compact input space, a neural network with a single hidden layer can uniformly approximate any polynomial function with a specified degree. Additionally, for any continuous function, a certain number of hidden units can suffice for approximation. The paper discusses the universal approximation theorem for feedforward neural networks with one hidden layer, showing that a certain number of hidden units can uniformly approximate any polynomial function with a specified degree. It also highlights conditions under which this approximation property holds, such as restricting weights in the last or first layer. The paper demonstrates the existence of a certain number of hidden units that can uniformly approximate functions in feedforward neural networks, even under specific weight restrictions. The universal approximation theorem states that a neural network with one hidden layer using a sigmoidal activation function can approximate any continuous function on a compact domain. This result was first proven by Cybenko, Hornik, and Funahashi in 1989. Hornik later extended the theorem to include any continuous bounded non-constant activation function. A single hidden layer neural network can uniformly approximate any continuous function on a compact domain with a non-polynomial activation function. The number of hidden units required for this approximation could be arbitrarily large. The minimum number of hidden units needed for approximation within a threshold \u03b5 is at least \u2126(1/\u221a\u03b5) when using the ReLU activation function for certain classes of functions. The paper discusses the approximation capabilities of single hidden layer neural networks with non-polynomial activation functions. It shows that for continuous functions, the number of hidden units needed for approximation is at most O(\u03b5^-n), regardless of the function class or dimensionality. The algebraic proof of the universal approximation theorem by Leshno et al. (1993) shows that n+d d hidden units suffice for polynomial functions with total degree at most d. The bound N \u2264 n+d d is independent of error threshold \u03b5 or output dimension m. The UAP holds under strong weight conditions, with non-bias weights in the last layer having small magnitudes. Additionally, there exists \u03bb > 0 for approximation depending only on \u03c3 and the function f. Our main results refine the universal approximation theorem by showing that non-bias weights in the first layer can always be chosen to have magnitudes greater than \u03bb, despite restrictions. Sections 2-6 cover preliminaries, results, consequences, algebraic approach, and proofs. The text discusses vector spaces, functions, monomials, polynomials, and continuous functions in the context of real numbers and variables. It also introduces the concepts of C(X) for continuous functions and P(X) for polynomial functions. The text introduces vector spaces for polynomial functions and continuous functions in the context of real numbers and variables. It discusses the computation capabilities of neural networks with one hidden layer, focusing on bias weights and non-bias weights. The activation function is not applied to the output layer in this context. The text discusses the application of activation functions in neural networks, emphasizing the use of dense subsets of metric spaces. It also touches on the importance of bias weights in the output layer for neural network approximation results. The text discusses the importance of dense subsets in metric spaces, particularly in the context of neural networks and activation functions. It also mentions the significance of bias weights in the output layer for neural network approximation results. The text discusses the Stone-Weierstrass theorem, which states that for any compact set X in R^n, any function in C(X) can be uniformly approximated by a sequence of polynomial functions. The text discusses polynomial functions in P(X) for a compact set X in R^n. Chebyshev's result states that the best polynomial approximant to a function f of degree d is unique. Jackson's theorem deals with approximating continuous functions on closed intervals. Theorems are presented for uniformly continuous functions and best polynomial approximants in different contexts. Theorem 3.1 states that for a polynomial function f in P(X) with certain conditions on weights, there exists a choice of weights that satisfy specific conditions. Theorem 3.2 discusses approximating continuous functions on closed intervals using a certain integer N and weights W, with conditions on the weights depending on f and \u03c3. The universal approximation theorem (Leshno et al., 1993) states that for any \u03b5 > 0, there exists an integer N such that with probability 1, bias weights can be chosen to approximate any function f \u2208 P on a compact set X using a fixed number N of hidden units. This result follows from Theorem 3.2 and the requirement that \u03c3 must be non-polynomial for the theorem to hold. The universal approximation theorem states that with a fixed number of hidden units, any function can be approximated on a compact set X. Previous research has shown that neural networks with specific hidden layer sizes can achieve this, with fewer hidden units and a single hidden layer compared to other approaches. The proof of Theorem 3.2 utilizes Jackson's theorem to provide an explicit bound based on the modulus of continuity of the function being approximated. The moduli of continuity for different classes of continuous functions are characterized explicitly. For instance, a function is \u03b1-H\u00f6lder if it satisfies a specific inequality. The freezing of lower layers in a neural network has interesting consequences, as shown in Theorem 3.3. The freezing of lower layers in a neural network, as discussed in Theorem 3.3, does not necessarily reduce the model's representability. Pre-training frozen layers and randomly initializing them may have quantitatively different outcomes. Theorem 3.3 can be seen as a result on random features in relation to kernel methods. In relation to kernel methods, random features were studied by Rahimi & Recht (2007). Sun et al. (2019) proved an analog of Theorem 3.3 for functions in a reproducing kernel Hilbert space using ReLU activation functions. Yehudai & Shamir (2019) discuss the role of random features in neural network representability. The Universal Approximation Property (UAP) has been studied in various contexts, including the depth and width of neural networks. Lu et al. (2017) and Hanin (2017) proved UAP results for neural networks with bounded width and ReLU activation functions. The role of depth in neural network expressive power has also been a topic of interest. The curr_chunk discusses the linear independence of polynomial functions based on distinct real numbers. The results can potentially be applied iteratively to deeper neural networks for approximating compositional functions. Theorem 5.1 presents a key result, with subsequent results being multivariate extensions using similar ideas. The curr_chunk discusses the linear independence of polynomial functions based on distinct real numbers. It follows that a sequence of linearly independent functions has a non-zero Wronskian if the evaluation at x=1 gives a non-zero value. The monomial functions form a basis for P\u2264d(Rn). The set of monomial functions forms a basis for P\u2264d(Rn). The n-tuples in \u039bn\u2264d are sorted in colexicographic order. Theorem 5.6 states that if a polynomial p in Pd(Rn) has all non-zero coefficients, and fixed polynomial functions p1,...,pk are defined on non-bias weights of the first layer, then pUind is dense in U if the non-bias Vandermonde matrix of W is non-singular. The proof involves the use of generalized Wronskians and Schur polynomials. In algebraic combinatorics, the determinant of Q[W] is a Schur polynomial. Choosing positive pairwise distinct values for w ensures non-singularity. A Schur polynomial can be expressed as a sum of certain monomials. Lemma 5.9 discusses polynomial approximants and continuity. The dimension of P\u2264d(Rn) as a real vector space has hidden units representing continuous functions. Every hidden unit in algebraic combinatorics represents a continuous function determined by weights and activation function. Linearly independent polynomial functions can approximate these functions, allowing for approximation of all coordinate functions. Polynomial approximations to the activation function and varying weights help in this process. Functions in P\u2264d(Rn) can be identified geometrically. The minimum number of hidden units required for the Universal Approximation Property is independent of the approximation error threshold. The minimum number of hidden units needed for the Universal Approximation Property is independent of the approximation error threshold \u03b5. The proof involves fixing a point x0 \u2208 X and defining a function f0 \u2208 C(X, Rm). By choosing the best polynomial approximant \u03c3r to \u03c3|Yr of degree d, it is possible to perturb \u03c3 if necessary to ensure \u03c3r \u2208 Pd(Yr) with all-non-zero coefficients. For every h in Pd(Rn), \u03bd(h) is the coefficient vector with respect to the basis, and \u03bd(h) is the truncation of \u03bd(h) by removing the first coordinate. Define C = 8N(N-1)cf > 0. Let Wr be the set of vectors in Wn,mN with second coordinate less than r. There exists a sequence {\u03bbk} of positive real numbers diverging strictly, such that for each k, there is yk in Y\u03bbk with \u03c3(yk) = \u03c3\u03bbk(yk). Assuming \u03c3(\u03bbk) = 0 for all k, each \u03bbk is well-defined due to the compactness of X. The bias weights of W can be chosen without restrictions. A W exists with linearly independent bias weights that span P\u2264d(X). For every t, there exist unique N,k in R such that fN,k gW N,k. As \u03bbk \u2192 \u221e, when k is sufficiently large. When k is sufficiently large, the convex hull contains points \u03bd(f[t]) and 0N-1. The barycentric coordinate vectors approach the barycenter with coordinates (1/N,...,1/N) as \u03bbk \u2192 \u221e. For every \u03b4 > 0, there exists a sufficiently large k such that the normalized distance of the barycentric coordinates is bounded by \u03b4. This implies that for any r > 0, B r is contained in the convex hull. When k is sufficiently large, the convex hull contains points \u03bd(f[t]) and 0N-1. The barycentric coordinate vectors approach the barycenter with coordinates (1/N,...,1/N) as \u03bbk \u2192 \u221e. For every \u03b4 > 0, there exists a sufficiently large k such that the normalized distance of the barycentric coordinates is bounded by \u03b4. This implies that for any r > 0, B r is contained in the convex hull. The proof of the theorem involves defining g based on certain conditions and showing that specific weights are contained within intervals, leading to the verification of two assertions. The proof of Theorem 3.2 involves finding the best polynomial approximant to f[t] for each integer d. Theorem 5.6 is general and can be used to prove universal approximation theorems for various neural network classes. A single set of weights with a non-singular \"non-bias Vandermonde matrix\" can serve as a criterion for showing the UAP for neural networks with weight constraints. Our criterion aims to be general for future \"neural-like\" networks, with an algebraic approach to understand how depth, width, weight constraints, and architecture choices affect neural network approximation capabilities. The role of non-bias and bias weights in the UAP is explored, with a focus on generalized Wronskians and differential operators. The text discusses the colexicographic order of n-tuples, defining matrices and generalized Wronskians for neural networks. The criterion aims to understand how network properties affect approximation capabilities, focusing on bias weights and differential operators. The evaluation of the generalized Wronskian at x = 1n gives a non-zero value, leading to the inference that the matrix M is upper triangular. The diagonal entries of M are determined by factorials of n-tuple entries, resulting in det(M) = 0. If det(M) = det(Q[W]) = 0, then W \u2208 pUind. The conjecture by Mhaskar (1996) suggests that a smooth non-polynomial function \u03c3 may require at least \u2126(\u03b5\u2212n) hidden units to uniformly approximate functions in the class S of C1 functions with bounded Sobolev norm. Evidence supporting this conjecture was provided using a heuristic argument based on a result by DeVore et al. (1989). The conjecture remains open. If the conjecture by Mhaskar (1996) is true, then the upper bound O(\u03b5\u2212n) in Theorem 3.2 is optimal for general continuous non-polynomial activation functions. For specific activation functions like the logistic sigmoid function or polynomial spline functions with finitely many knots, the minimum number N of hidden units needed to approximate every function in S must satisfy (N log N) \u2208 \u2126(\u03b5\u2212n). There is still a gap between the lower and upper bounds for N in these cases, and finding optimal bounds would be interesting."
}