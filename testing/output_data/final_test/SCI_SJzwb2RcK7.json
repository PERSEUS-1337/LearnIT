{
    "title": "SJzwb2RcK7",
    "content": "The paper presents a method for adversarial decomposition of text representation, allowing for the fine-grained controlled change of different aspects of an input sentence. The method uses adversarial-motivational training with a special motivational loss to encourage better decomposition. The obtained meaning embeddings are evaluated on a downstream task of paraphrase detection, showing their effectiveness. The paper introduces a method for adversarial decomposition of text representation to improve input sentence meaning embeddings for paraphrase detection. This approach aims to overcome the limitations of single vector representations by encoding the input sentence into multiple vectors. Our method encodes input sentences into multiple vectors, each responsible for a specific aspect. This approach is related to style transfer work, focusing on separating meaning and form for controllable changes in text representation. Our method focuses on separating meaning and form in text representation by encoding sentences into multiple vectors. We propose a novel model based on adversarial motivational training, inspired by GANs and adversarial autoencoders. We use a motivator to encourage better decomposition of meaning and form, and make our code publicly available on GitHub. The proposed methods are evaluated for learning separate aspects of input representation. The text discusses learning to separate language representations, including diachronic slices and social registers. It mentions previous work on style transfer research and different approaches to generating text in a specific form. The text discusses approaches to inducing a change in the form or meaning of an existing utterance, such as generating verses in the style of a target poet or producing product reviews with a specific rating. Various methods, including training separate networks on verses by different hip-hop artists or using language models conditioned on additional inputs, are explored. The text discusses different approaches to controlling both \"content\" and \"style\" simultaneously in language models. It includes using special context vectors, \"speaker\" vectors for dialog consistency, and evaluating models based on \"content\" and \"style\". The training data can be divided into aligned corpora for paraphrasing or non-aligned datasets for form shift problems. Recent advancements in natural language processing have introduced new approaches that do not rely on parallel corpora for tasks such as changing tense and sentiment in sentences. One method involves training a variational auto-encoder (VAE) followed by a discriminator and generator trained alternately to classify target sentence attributes. This approach has shown success in altering the content and style of text. Recent advancements in natural language processing have introduced new approaches that do not rely on parallel corpora for tasks such as changing tense and sentiment in sentences. A special loss component forces the hidden representation of the encoded sentence to not have any information about the target sentence attributes. Different models have been proposed, including using a VAE to modify the hidden representation of a sentence to match the desired form, applying a GAN to align hidden representations of sentences from two corpora, and using \"style\" vectors during decoding to produce sentences with desired properties. The models are trained using algorithms like Professor-Forcing and adversarial loss constraints on hidden space vectors. Recent advancements in natural language processing have introduced new approaches that do not rely on parallel corpora for tasks such as changing tense and sentiment in sentences. BID13 proposed models for generating sentences with target properties using an adversarial loss, unlike previous proposals. Our model works directly on sentence representations in hidden space, treating form as a continuous variable rather than a binary feature like in previous works. Our work introduces a model that considers linguistic form as a continuous variable for text generation tasks, allowing for a large number of forms to be utilized. This approach is more aligned with the reality of language use, where there are varying degrees of overlap between different registers and language changes gradually over time. The model introduced considers linguistic form as a continuous variable for text generation tasks, allowing for a large number of forms to be utilized. This approach can be applied to authorship attribution and text representation decomposition, as shown in preliminary experiments. The model introduces linguistic form as a continuous variable for text generation tasks, enabling fine-grained controllable changes in form. It aims to learn two encoders for meaning and form, along with a generator, to generate samples with different forms from different domains. The model introduces linguistic form as a continuous variable for text generation tasks, enabling fine-grained controllable changes in form. It encodes input sentences into latent vectors for meaning and form, allowing for the generation of output with different forms from different domains. The approach is limited to cases where distributions of meaning are similar in parallel or comparable corpora, making it challenging to transform content between registers like children's books and scholarly papers. The model encodes input sentences into latent vectors for meaning and form, enabling fine-grained controllable changes in text generation. It consists of four main parts: encoder E, generator G, discriminator, and motivator. The encoder captures the meaning and form of the sentence, while the generator reconstructs the original input sequence. The approach is inspired by Generative Adversarial Networks (GANs) and aims to dissociate meaning and form in text generation tasks. The model incorporates encoder E, generator G, discriminator, and motivator to enforce specific distribution and characteristics on the output. It introduces two additional components, discriminator D and motivator M, to encourage learning the dissociation of meaning and form. The discriminator classifies form based on latent meaning vector, while the motivator classifies form based on latent form vector to simplify the task for the encoder. The encoder E and generator G, both modeled with neural networks, use a GRU to encode input sentences and produce latent vectors for form and meaning. Parameters for the encoder include W m , b m , W f , b f , and for the GRU unit. The generator also uses a GRU unit, taking meaning and form vectors as input to generate output. The encoder and generator, implemented with neural networks, utilize a GRU to encode input sentences and generate latent vectors for form and meaning. The generator then uses these vectors to produce an output sentence through a fully-connected layer. The encoder and generator are trained using standard reconstruction loss, with an adversarial approach used to ensure the meaning vector does not contain form information. The text discusses using a discriminator to dissociate the form and meaning of sentences by assigning scores based on different domains. An adversarial loss function is used to train the encoder, with the strength of the adversarial loss controlled by a hyperparameter. Additionally, a motivator is proposed to further improve the dissociation between form and meaning. The text proposes using a motivator M with a motivational loss to improve dissociation between form and meaning in sentences. The motivator learns to classify the form of input sentences, encouraging the encoder to encode form information in the form vector. The training procedure involves two stages: training the discriminator D and motivator M, and training the encoder E and generator G. Unlike previous methods, the D and M are not trained more than the E and G. The encoder and generator are trained with a loss function that combines reconstruction loss with losses from the discriminator and motivator. Evaluation of style transfer in this task is challenging, with metrics like \"transfer strength\" and \"content preservation\" used to assess success in changing the form successfully. In experiments, a classifier is trained on two corpora to recognize linguistic form, with a focus on changing form/meaning accuracy. A GRU unit with fully-connected layers is used as the classifier. Content preservation is measured using cosine similarity with pretrained word embeddings to assess how much meaning is preserved during form changes. In experiments, a classifier is trained on two corpora to recognize linguistic form, with a focus on changing form/meaning accuracy. A sentence embedding is computed using max, mean, and average pooling over timesteps, followed by calculating cosine similarity scores between original and changed form sentences. The form is treated as a continuous variable, requiring a different approach for fair comparison. For each sentence in the test set, random sentences from the opposite form corpus are sampled for comparison. The experiment involves sampling random sentences from two corpora, encoding them into meaning and form vectors, and generating new sentences for evaluation. The method is evaluated on datasets reflecting changes in meaning, form, and sentiment polarity. One experiment focuses on changing form using titles of scientific papers and news articles from online libraries. The dataset used for exploring diachronic language change includes texts of 17 plays by William Shakespeare in Early Modern English and their translations into contemporary English. Previous work on style transfer for text included experiments with changing sentiment polarity, but this study focuses on changes in form rather than meaning. The model proposed by BID13 is considered the most recent and similar to this work. The \"style-embedding\" model proposed by BID13 was implemented as a baseline for comparison, achieving high accuracy in transfer strength metrics for Shakespeare and Headlines datasets. Results show significant differences in forms in the corpora. Different configurations of form and meaning vectors were tested, with larger form vectors leading to higher transfer strength but lower content preservation. The proposed method outperforms previously proposed methods in transfer strength. The proposed method outperforms the baseline model in transfer strength, with lower content preservation but better overall performance. It excels in transferring both form and meaning, especially on the Shakespeare dataset. However, there is no guarantee of fluency in generated sentences after switching the form vector. The proposed method excels in transferring both form and meaning, as shown by the perplexity of generated sentences and visualized embeddings. The model can produce form embeddings clustered into two groups even without the motivator, indicating successful dissociation of form and meaning. The model can produce form embeddings clustered into two groups even without the motivator, but the separation effect is more pronounced in its presence. The motivator consistently improves transfer strength of ADNet, as shown in qualitative evaluation examples. The model can produce form embeddings clustered into two groups even without the motivator, but the separation effect is more pronounced in its presence. The motivator consistently improves transfer strength of ADNet, as shown in qualitative evaluation examples. English translation back into the original. The same was done in the opposite direction. Table 1 : Decoding of the source sentence from Early Modern English (EME) into contemporary English (CE), and vice versa. Table 2 illustrates the possibilities of ADNet on fine-grained transfer applied to the change of register. The model correctly captures the meaning of sentences and decodes them using the form of the source sentences. Note how the model preserves specific words and the structure of the source sentence. The model successfully captures the meaning of sentences and decodes them using the form of the source sentences, as shown in Table 2. It separates form embeddings into two groups, with the motivator enhancing transfer strength in ADNet. The model preserves specific words and sentence structures, such as using colons and \"to\"-constructions. The ADNet model was evaluated on paraphrase detection using the SentEval toolkit and Microsoft Research Paraphrase Corpus. It outperformed other unsupervised systems and had the highest F1 score, showing improvement in understanding sentence meaning regardless of form. ADNet is a new model that performs adversarial decomposition of text representation without requiring a parallel training corpus. It allows for fine-grained changes in sentence form and meaning, achieving superior results in language register shift and diachronic language change tasks. The proposed motivational loss leads to better separation of form embeddings."
}