{
    "title": "rylejExC-",
    "content": "Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. Previous attempts to reduce the receptive field size by subsampling neighbors lacked convergence guarantees. This paper introduces preprocessing strategies and control variate based algorithms to further reduce the receptive field size. Empirical results show similar convergence speed with fewer neighbors per node, significantly reducing time consumption on the Reddit dataset. Graph convolution networks (GCNs) generalize convolutional neural networks to graph structured data by applying the same linear transformation to all neighbors of a node, followed by mean pooling. GCNs have been successful in various tasks such as semi-supervised node classification, inductive node embedding, link prediction, and knowledge graphs. However, training GCNs efficiently is challenging due to the graph convolution operation. The graph convolution operation in GCNs poses challenges for efficient training. Neighbor sampling reduces the receptive field size by randomly subsampling neighbors at each layer, improving scalability compared to batch algorithms. In this paper, novel stochastic training algorithms for GCNs are developed to reduce the receptive field size to l D (l). The time complexity of training GCN is made comparable to training MLPs by allowing D (l) to be as low as two. This is achieved through preprocessing the first graph convolution layer and using control variate techniques. Our novel control variate-based stochastic training algorithms for GCNs reduce bias and variance compared to the neighbor sampling algorithm. Empirical tests on six graph datasets show improved performance with a shorter training time, achieving comparable results in fewer epochs. In the node classification task, an undirected graph is used with vertices, edges, feature vectors, and labels. The goal is to predict labels for vertices where the label is not observed. Graph convolution layers are defined with activation matrices and weight matrices to process the input features. Graph Convolutional Networks (GCN) utilize neighbor information for node classification by incorporating information from all L-neighbors in an L-layer GCN. This makes GCN more powerful than a multi-layer perceptron (MLP) model, but also complicates stochastic training due to the large receptive field size per minibatch. The large receptive field size per minibatch in training data leads to high time complexity, space complexity, and IO. Alternative notations are introduced to compare algorithms, focusing on computing u v based on node v's neighbors. The propagation rule u = D v=1 p v h v is used to reduce the receptive field size. Neighbor sampling (NS) algorithm proposed by Hamilton et al. (2017a) reduces the receptive field size by randomly choosing D (l) neighbors for each node on the l-th layer. The NS estimator u N S is unbiased but has a large variance, leading to biased prediction and gradients. Training with NS may have a regularization effect similar to dropout when D (l) is moderate. The Neighbor Sampling (NS) algorithm by Hamilton et al. (2017a) reduces the receptive field size by randomly selecting D (l) neighbors for each node on the l-th layer. This can lead to biased predictions and gradients due to high variance. To address this, a technique is presented to preprocess the first graph convolution layer by approximating ADropout p (X) with Dropout p (AX), maintaining prediction performance. The modification allows preprocessing the first graph convolution layer by using U as the new input, reducing the number of graph convolution layers. Two novel control variate based estimators with smaller variance and stronger guarantees are presented. The model assumes no dropout for now and addresses it later. The text discusses a stochastic approximation algorithm called CV, which uses historical activations to reduce variance in predictions. It applies Monte-Carlo approximation and analyzes the convergence of the training algorithm using CV for stochastic gradient. The algorithm computes approximate predictions in matrix form, aiming to reduce variance and improve accuracy. The text introduces a stochastic approximation algorithm called CVD, which utilizes historical activations to reduce variance in predictions. It incorporates a weight scaling procedure to approximate the mean and works effectively with dropout configurations. The algorithm aims to compute approximate predictions and improve accuracy in training. The CVD algorithm utilizes historical activations to reduce variance in predictions by separating the mean and variance. It incorporates a weight scaling procedure and works effectively with dropout configurations. The algorithm aims to compute approximate predictions and improve accuracy in training. The activations h v are one dimensional and independent. Variance is decomposed into VMCA and VD terms. VMCA should be small, with different values for exact, NS, CV, and CVD estimators. CV estimator is preferred for models without dropout due to simplicity. The CV estimator is favored for models without dropout due to its simplicity. It overestimates the variance, with NS overestimating by D times and CV having even larger variance. The CVD estimator is the best for models with dropout, showing the same variance as the exact estimator. CV also offers stronger theoretical guarantees than NS, with predictions becoming exact after a few testing epochs. Training with CV's stochastic gradients converges to GCN's local optimum for models without dropout. The CV estimator is preferred for models without dropout due to its simplicity and stronger theoretical guarantees. It ensures exact predictions after a few testing epochs, outperforming NS. By running forward propagation with CV for L epochs, exact predictions can be obtained, surpassing NS which cannot recover the exact prediction. The CV estimator is more scalable than batch algorithms as it does not require loading the entire graph into memory. Training with CV's approximated gradients converges to a local optimum of GCN, regardless of neighbor sampling size. The algorithm converges to a local optimum for models without dropout, with strong theoretical guarantees. Our algorithm converges to a local optimum and we discuss the time complexity of different algorithms, decomposing it into sparse and dense time complexity for matrix multiplication. Our implementation is slightly slower than others due to storing node features in main memory. We examine the variance and convergence of our algorithms on various datasets, reporting Micro-F1 for predictive performance. The study compares the original model (M0) with an approximated model (M1) by switching the order of dropout and aggregating neighbors. Three settings are compared, showing similar performance. The experiments were repeated multiple times on different datasets using the same model architectures with slightly different hyperparameters. The study compares the original model (M0) with an approximated model (M1) by switching the order of dropout and aggregating neighbors. Three settings were compared, showing similar performance. The convergence experiments used M1+PP, D(l) = 20 as the fastest baseline. The algorithms were compared with a small neighbor sampling size of D(l) = 2. CV+PP was validated to converge to a local optimum of Exact, while NS and NS+PP had higher training loss due to biased gradients. Predictive accuracy was compared among models trained by different algorithms. The study compared different algorithms for training models with dropout turned on. CVD+PP showed comparable validation accuracy with Exact on all datasets, while CV+PP and NS+PP worked acceptably on most datasets. CV+PP reached comparable accuracy with Exact on all datasets except PPI, and NS+PP had slightly lower accuracy but still within 2%. These algorithms can be used when predictive performance is not a top priority. The study compared different algorithms for training models with dropout turned on, emphasizing the use of exact algorithms for predictions. The algorithm NS without preprocessing performed much worse than others, highlighting the importance of preprocessing. CVD+PP was found to be about 5 times faster than Exact due to reduced receptive field size. Different algorithms were compared for prediction quality, with CV reaching the same testing accuracy as Exact, while NS and NS+PP performed worse. In this study, different algorithms were compared for training models with dropout. CV achieved the same testing accuracy as Exact, while NS and NS+PP performed poorly. The study also introduced preprocessing strategies and control variate based algorithms to reduce the receptive field size of GCN for faster training. The algorithms showed comparable convergence speed with the exact algorithm, even with a neighbor sampling size of D(l) = 2. The study compared algorithms for training models with dropout, where CV achieved the same testing accuracy as Exact. Preprocessing strategies and control variate based algorithms were introduced to reduce the receptive field size of GCN for faster training. The per-epoch cost of training GCN is comparable to training MLPs, with strong theoretical guarantees for the control variate based algorithm. The algorithm showed comparable convergence speed with the exact algorithm, even with a neighbor sampling size of D(l) = 2. The study introduced preprocessing strategies and control variate algorithms to reduce GCN's receptive field size for faster training. The algorithm showed comparable convergence speed with the exact algorithm, even with a neighbor sampling size of D(l) = 2. Theorem 2 was proven in 3 steps, showing that an SGD algorithm converges as the gradient bias approaches zero. The study introduced preprocessing strategies and control variate algorithms to reduce GCN's receptive field size for faster training. The algorithm showed comparable convergence speed with the exact algorithm, even with a neighbor sampling size of D(l) = 2. Theorem 2 was proven in 3 steps, showing that an SGD algorithm converges as the gradient bias approaches zero. Some K > 0, s.t. , H CV,i \u2212 H CV,j \u221e < K and H CV,i \u2212 H i \u221e < K for all I < i, j \u2264 T , where I is the number of iterations per epoch. Proof. Because for all i > I, the elements of X CV,i are all taken from previous epochs, i.e., X CV,1 , . . . , X CV,i\u22121 , we know that... The proof presented in the curr_chunk demonstrates the existence of a K value for a sequence of model weights under certain conditions. The proof involves Lipschitz continuity, smoothness, and the use of biased stochastic gradients. The algorithm is assumed to be warmed-up for a certain number of steps with initial weights. The proof concludes by showing the existence of K for a sequence of weights. The proof in the current chunk shows the existence of a K value for model weights under specific conditions, involving Lipschitz continuity and biased stochastic gradients. Testing 3-layer GCNs on the Reddit dataset with similar settings as 2-layer GCNs, the algorithm is subsampled for efficiency. The convergence results and time consumption for reaching a certain accuracy are presented. Additionally, the independent Gaussian assumption is justified for a 2-layer GCN. The neighbor's activations are independent. Assumption 1 is not GCN-specific and is discussed in Wang & Manning (2013). Lemma 3 states that if a and b are independent random variables, their transformations f1(a) and f2(b) are also independent. This result can be generalized to deeper models. The average correlation of all pairs of neighbors is computed, and the average neighbor correlation on layer l is defined as AvgCorr(l,v,d). The average feature correlation and neighbor correlation per layer are reported on various datasets. A GCN with 10 layers is trained on each dataset, and correlations are computed for layers 1 to 9. The average neighbor correlation is close to zero on the first layer, with no strong tendency of increased correlation as the number of layers increases after the third layer. The average neighbor correlation and feature correlation tend to increase as the number of layers in a GCN increases, but both remain on the same order of magnitude. The activations do not become much more correlated than in the MLP case, and both correlations are much smaller than one. (Wang & Manning, 2013)"
}