{
    "title": "rkeRMT9cLH",
    "content": "Fine-tuning BERT on legal documents provides valuable improvements for NLP tasks in the legal domain. Access to large legal corpora is a competitive advantage for commercial applications and academic research on analyzing contracts. Businesses rely on contracts for critical obligations like scope of work and cancellation policies. Automatic extraction and classification of contract terms have been modeled as classification, entity, and relation extraction tasks. This paper focuses on classification tasks in legal document analysis. In this paper, the impact of utilizing a large domain-specific corpus of legal agreements to improve classification models by fine-tuning BERT is investigated. The study assesses the performance of using pre-trained BERT, the impact of further fine-tuning BERT, and how this changes with training on larger corpora. The findings show valuable improvements in accuracy as the size of the legal corpus used for fine-tuning BERT increases. Lexion is a commercial venture building an \"intelligent repository\" for legal agreements that automatically classifies documents and fills metadata values using entity extraction. The application organizes documents, links related documents, calculates date milestones, identifies outlier terms, and offers features for reports, alerts, sharing, and integration with other systems. To enhance our legal application, we have developed a pipeline and user-interface for document processing, OCR, annotation, and model training. Leveraging our legal corpus, we fine-tuned BERT with hundreds of thousands of legal agreements to maximize accuracy and gain a competitive edge. We fine-tuned the BERT-Base uncased model using a proprietary dataset of legal agreements. The focus was on annotating the \"Term\" attribute, specifically the \"fixed\" term that expires after a set time. The study focused on annotating the \"Term\" attribute in legal agreements, distinguishing between the \"fixed\" term that expires after a set time and the \"auto-renewing\" term. The system achieved high accuracy through pre-and post-processing, splitting the dataset for training and validation, and avoiding over-fitting during training. The study trained a neural network with a Bag-of-Word input and BERT classifier, fine-tuning the BERT layers for the end task. Results showed the benefits of domain-specific fine-tuning over pre-trained models, as seen in Table 1. The study compared performance metrics of pre-trained BERT and fine-tuned BERT on a domain-specific corpus, showing significant improvement with fine-tuning. Results in Table 2 demonstrate the effectiveness of fine-tuning, with faster learning rates and lower train loss shown in Figure 3. The BERT-based architecture used was kept simplistic for fair comparison purposes. In contrast to the simplistic BERT-based architecture used for fair comparison, a more advanced architecture with fine-tuned BERT was employed in the system, showcasing impressive results without complex feature engineering or hyper-parameter tuning."
}