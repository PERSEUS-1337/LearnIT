{
    "title": "SklibJBFDB",
    "content": "The paper introduces IdBench, a benchmark for evaluating word embeddings of identifiers in source code for semantic relatedness and similarity. It aims to address the lack of a standardized evaluation method for identifier embeddings. The benchmark is based on ratings from 500 software developers and compares different embedding techniques. The paper introduces IdBench, a benchmark for evaluating word embeddings of identifiers in source code for semantic relatedness. It highlights the varying effectiveness of embeddings across different techniques and the need for improved representations to avoid fatal mistakes in developer tools. IdBench serves as a gold standard for guiding the development of novel embeddings. Reasoning about source code based on learned representations has various applications, such as predicting method names, detecting bugs and vulnerabilities, predicting types, detecting similar code, inferring specifications, code de-obfuscation, and program repair. These techniques are based on embeddings of source code, which map code into a continuous vector representation encoding semantics. Identifiers, like words in natural languages, are crucial in source code as they account for the majority of it. Identifiers are essential in source code as they convey important information about the meaning of the code. Code analysis techniques rely on embeddings of identifiers to represent semantic relationships. The effectiveness of identifier embeddings can be measured using gold standards, similar to word embeddings in natural language. The paper addresses the problem of measuring and comparing the effectiveness of embeddings of identifiers in source code. It introduces IdBench, a tool for evaluating how well an embedding reflects human ratings of word similarity. Source code vocabularies differ from natural language, with unique terms and abbreviations, making it challenging to reuse existing gold standards for evaluation. Developers constantly create new identifiers, and even common words may have different meanings in computer science. IdBench is a benchmark for evaluating semantic similarities of identifiers based on developer opinions. It gathers ratings from 500 developers on pairs of identifiers to measure how well embeddings agree with human ratings. This dataset helps understand the strengths and weaknesses of current embeddings. The raw dataset is compiled into a benchmark with hundreds of identifier pairs for evaluating similarities. The approach is applied to JavaScript code, with comparisons to state-of-the-art embeddings showing variations in representing identifier relatedness and similarity. The CBOW variant of FastText is found to be the most accurate in representing relatedness, but none accurately represent identifier similarities due to confusion with identifiers of opposite meanings. This paper evaluates embeddings of identifiers, highlighting confusion with opposite meaning identifiers and lack of synonyms. Simple string distance functions are surprisingly effective for similarity tasks. Contributions include a novel methodology based on developer opinions and a benchmark for evaluating embeddings. The paper evaluates existing embeddings and string similarity functions for identifiers, distinguishing between relatedness and similarity. Relatedness refers to the association between identifiers, while similarity refers to their interchangeable meanings. The paper evaluates embeddings and string similarity functions for identifiers, distinguishing between relatedness and similarity. Similarity is a stronger semantic relationship than relatedness. IdBench includes benchmark tasks to measure identifier similarity. Developer opinions are gathered through a direct survey to rate identifier relatedness and similarity. Developers are shown pairs of identifiers and asked to rate their relatedness and similarity on a five-point Likert scale. Each developer rates 18 pairs of identifiers, randomly sampled from a larger pool. The ratings from the direct survey form the basis for the relatedness and similarity tasks in IdBench. The survey aims to assess how well identifiers fit in a given code context, as identifier names alone may not provide enough information to judge their similarity accurately. The survey in IdBench addresses the challenge of judging identifier similarity by showing code context and asking developers to choose the best fitting identifier. Ratings from the indirect survey are used for contextual similarity tasks, selecting pairs from a corpus of JavaScript files. The study selects 300 pairs of identifiers from a corpus of 50,000 JavaScript files based on word embeddings to cover various degrees of similarity. 17,000 identifiers are extracted and their cosine distances are computed using two different word embeddings, resulting in two sorted lists of pairs from which 150 pairs are sampled. The study selects 150 pairs of identifiers from a corpus of 50,000 JavaScript files based on word embeddings to cover various degrees of similarity. The pairs are selected by picking 75 of the most similar pairs from the first 1,000 pairs and randomly sampling pairs from different similarity ranges. Code contexts for the survey are gathered by searching the code corpus for occurrences of the selected identifiers, with five different contexts randomly selected for each identifier. Participants are paid via Amazon Mechanical Turk to complete surveys on a specific pair of identifiers. 500 developers participate, providing at least 10 ratings for each pair. Outlier participants are removed based on Inter-Rater Agreement using Krippendorf's alpha coefficient. The study involves calculating the agreement level between participants' ratings for pairs of identifiers. Participants with ratings that significantly differ from the average are removed. Additionally, participants who decrease the overall agreement level are eliminated. Participants' ratings are filtered based on IRA increases of at least 10%. Direct survey ratings are used for IRA-based filtering. Pairs with confusing contexts are eliminated as a third filter. After filtering ratings based on IRA increases and eliminating pairs with confusing contexts, a threshold is used to remove pairs with significant differences in similarity ratings. The remaining identifier pairs are categorized into small, medium, and large benchmarks based on different thresholds. Ratings from developer surveys are converted into similarity scores for evaluation. The conversion of ratings for identifier pairs from (Miller & Charles, 1991) into similarity scores is illustrated in Table 3, showing examples of relatedness and similarity. The examples demonstrate how some pairs are lexically similar or synonyms, while others have distinct properties. Some pairs are weakly or not related at all. To assess the relatedness and similarity of identifiers, five vector representations are evaluated against IdBench: Word2vec variants (w2v-cbow and w2v-sg), FastText (FT-cbow and FT-sg), and a path-based embedding technique. All embeddings are trained on a code corpus of 50,000 JavaScript files. The study evaluates various embeddings and string distance functions for identifier-related code analysis, including Word2vec variants and FastText trained on a JavaScript code corpus. Additionally, string similarity functions like Levenshtein's edit distance are considered for semantic relatedness assessment. Spearman's rank correlation is used to measure agreement with the IdBench benchmark. The study evaluates embeddings and string distance functions for identifier-related code analysis, showing that FastText-cbow consistently outperforms all techniques in terms of relatedness and similarity. Neurally learned embeddings surpass string distance-based functions, with correlations ranging from 46% to 73%. The study compares different embedding techniques for code analysis, with FastText variants outperforming others. Path-based embeddings score similarly to Word2vec. FastText-cbow shows the strongest agreement in similarity tasks. String distance functions surprisingly outperform some embeddings in certain tasks. The contextual similarity task results confirm that path-based embeddings perform better than other techniques for representing code snippets in a vector space. While embeddings are effective for relatedness, they fall short in achieving high agreement for similarity tasks. There is significant room for improvement in semantic similarity applications of identifiers. Semantic similarity is crucial for tools that suggest variable or method names, as well as for identifier name-based tools for finding programming errors or variable misuses. The lack of accurate embeddings for representing semantic similarities of identifiers motivates further work on embedding techniques. FastText tends to cluster identifiers based on n-grams, which can impact similarity representation. Path-based embeddings cluster words based on structural and syntactical contexts, helping identify synonyms despite lexical differences. However, they may also group unrelated identifiers together, leading to potential confusion in code analysis. The use of string distance functions in identifying semantic similarities in identifiers is highlighted, showing comparable results to learned embeddings. However, lexical approaches may overlook synonymous identifiers. Various gold standards for evaluating word embeddings based on human judgments have been proposed, focusing on relatedness or similarity of words. This work addresses the need for a gold standard for identifiers in source code, as existing standards for natural language words are insufficient. Data gathering involves asking human raters about the relatedness or similarity of words, following a methodology proposed by previous studies. The methodology for gathering judgments about contextual similarity involves asking participants to choose words to fill in a blank. Prior work uses manual selection, free association databases, and cosine similarities from pre-existing models to determine relatedness and similarity. Inter-rater agreement for NL words reaches high levels, showing genuine human intuition. The IRA provides an upper bound for the correlation between models and the gold standard. Current models have room for improvement, especially in terms of similarity. Embeddings of identifiers are crucial for code analysis tools, with approaches like Word2vec being popular for bug detection, type prediction, and vulnerability detection. String similarity functions are also used in name-based tools for bug detection. IdBench aims to improve the quality of embeddings used in code analysis tools. Various methods like log-bilinear neural language models, graph neural networks, and sequence-based neural networks are employed to embed code for different purposes. Code2seq generates NL word sequences from code embeddings. This paper introduces IdBench, the first benchmark for evaluating vector space embeddings of identifier names in source code. It complements the COSET benchmark for evaluating embeddings of entire programs by focusing on identifiers. IdBench gathers ratings from 500 developers to provide gold standard similarity scores for identifiers, aiming to improve learning-based code analysis tools. IdBench is a benchmark for evaluating vector space embeddings of identifier names in source code. It compares different embedding techniques and string distance functions, showing significant differences in their performance. The best embedding is effective at representing related identifiers, but all techniques have room for improvement in representing similar identifiers. IdBench aims to guide future efforts in improving identifier embeddings for better machine learning models of source code."
}