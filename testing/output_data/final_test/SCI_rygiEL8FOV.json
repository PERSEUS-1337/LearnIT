{
    "title": "rygiEL8FOV",
    "content": "We present a framework for building unsupervised representations of entities as probability distributions over co-occurring contexts in a low-dimensional space. This approach leverages Optimal Transport tools like Wasserstein distance and barycenters. The method is applied to obtain unsupervised text representations, showing significant gains in tasks like sentence similarity and word entailment compared to existing models. The proposed approach captures uncertainty and polysemy by modeling entities as distributions, utilizing the underlying geometry of the task, providing interpretability through optimal transport between contexts, and is easily applicable on existing point embedding methods. The framework is useful for unsupervised or supervised problems in text or other modalities, requiring only a co-occurrence structure. The code and pre-built histograms are available at https://github.com/context-mover. The curr_chunk discusses various methods for representing language and images, including vector representations, convolutional neural networks, and LSTM hidden states. These methods aim to map input entities to dense vector embeddings in a low-dimensional space to preserve semantics. In contrast to traditional methods that represent entities as single points in space, we focus on co-occurrence information between entities and their contexts. Each entity is represented as a probability distribution over its contexts, with contexts embedded in a low-dimensional space. This approach allows us to compare the cost of moving contexts between entities, termed Context Mover's Distance (CMD). The distributional estimate of the entity is the distribution over contexts embeddings, while individual context embeddings are referred to as point estimates. The proposed approach focuses on representing entities as probability distributions over their contexts, with contexts embedded in a low-dimensional space. This method aims to capture the inherent uncertainty and polysemy of entities in natural language by utilizing distributional estimates, which are more informative than point-wise embedding vectors alone. The co-occurrence information needed for building distributions is already obtained in point-wise embedding methods like GloVe, but has been largely overlooked. The co-occurrence information crucial for the approach is inherent in embedding methods like GloVe. The framework can represent various entities in different problems. It is recommended to use distributional estimates for entity representation alongside point-wise embedding methods. The framework utilizes optimal transport of contexts as a distance measure, offering benefits for tasks such as word and sentence representations, sentence similarity, and hypernymy. The entropic regularization introduced allows for efficient computations on GPUs. The method described in the curr_chunk utilizes Context Mover's Distance (CMD) to measure distances between words, enabling state-of-the-art word entailment metrics. The resulting representations capture various senses of entities and can be extended from words to groups of entities like sentences. The curr_chunk introduces the use of Wasserstein barycenters to improve performance on datasets, surpassing supervised methods like InferSent and GenSen. It builds upon the concept of vector space models for natural language, popularized by Word2vec and GloVe, with suggestions to enrich embeddings for capturing additional information. The challenge remains in capturing various semantics and uncertainties within a single point embedding. The curr_chunk discusses the representation of words with distributions, including Gaussian distributions and mixtures of Gaussians. It also mentions the use of elliptical and Gaussian distributions with a Wasserstein metric. The proposal involves estimating distributions over context embeddings based on co-occurrence information. The curr_chunk discusses utilizing optimal transport in NLP based on co-occurrence information for unsupervised representation learning. It highlights the use of Wasserstein space for embedding entities and mentions the limitations of other metric spaces like Hyperbolic space. The focus of optimal transport in NLP has been on transporting words directly for downstream applications. This includes document distances, topic modeling, document clustering, and others. The Word Mover's Distance computes the distance between documents as an optimal transport between their bag-of-words. In this approach, transport is considered over contexts instead of words, enabling the development of representations for words and compositions of words like sentences and documents. Optimal Transport (OT) allows comparing probability distributions over a ground space G by lifting distances between points to distances between distributions. Unlike other divergences like KL, OT considers the geometry of the ground space. KL(\u00b5||\u03bd) requires absolute continuity of \u00b5 with respect to \u03bd. Linear Program Formulation is used in the discrete case of OT. Optimal Transport (OT) in the discrete case involves a linear program formulation where the goal is to minimize transportation costs when moving goods from factories to shops. The Optimal Transport distance between two probability measures is determined by finding the optimal transportation matrix. OT considers the geometry of the ground space and does not require absolute continuity between distributions. The p-Wasserstein distance is defined for empirical probability distributions, with regularization and Sinkhorn iterations used for efficient solution. The regularized problem can be solved using Sinkhorn iterations, albeit with some approximation. The regularized p-Wasserstein distance can be efficiently solved using Sinkhorn iterations, with the approximation error controlled by the regularization strength \u03bb. The convergence to an \u03b5-accurate solution is independent of n, resulting in a complexity of O(n^2/3). The Wasserstein barycenter minimizes the sum of Wasserstein distances to given measures, with practical applications using the regularized barycenter Bp,\u03bb. In this section, the distributional estimate used to represent each entity, such as words, is defined. It involves building a histogram over contexts and embedding them into a space. The histogram measures the likelihood of a word occurring in a context, which is generally intractable to formulate in closed form. The text discusses the empirical estimation of word occurrences in context relative to the total frequency in the corpus. It also mentions the computation of CMD using a transportation matrix and building a histogram based on co-occurrence matrix entries. The text discusses embedding contexts into a low-dimensional space to capture semantics, using techniques like SVD or deep neural networks. It combines histograms and context embeddings to represent words with an empirical distribution. The text discusses defining a distance between words using distributional estimates and a meaningful metric on ground space. This distance, known as Context Mover's Distance (CMD), measures the ease of transporting contexts between words. The interpretation of CMD involves a transportation map, providing insight into why two words are considered similar in meaning. The interpretation of CMD involves understanding why two words are considered similar in meaning by examining the movement of contexts. CMD combines neural and count-based models by using histogram information characteristic of count-based models and point estimates reflective of embeddings in neural models. Adding a point estimate to the distributional estimate with a mixing weight allows for the best of both worlds in terms of information integration. The framework discussed involves reweighting contexts to sum to 1 \u2212 m, with a focus on single word contexts. It can be extended to represent entity composition via Wasserstein barycenter. The flexibility of the framework allows for defining an asymmetric cost measuring entailment. Co-occurrence counts alone may not indicate a strong association between words and context words. The Positive Pointwise Mutual Information (PPMI) matrix addresses the issue of weak associations between words and context words. Variants of PPMI, such as the shifted and smoothed SPPMI matrix, enable better extraction of semantic associations from co-occurrence matrices. This allows for improved formulation of bin values for word histograms. The histogram of word w in Eq. (4) can be formulated as SPPMI\u03b1,s(w,c) for context c. To address computational considerations, representative contexts are used to form histogram bin values. Efficiency is improved by using batched implementations on Nvidia TitanX. The goal of developing a representation for sentences that captures semantics efficiently using batched implementations on Nvidia TitanX GPUs. This involves computing Wasserstein-distances and barycenters at high speeds, while also focusing on distributional estimates to capture uncertainty and polysemy in sentence representations. Various unsupervised representation methods have been proposed in the past, aiming to better capture the inherent meaning of sentences through vector embeddings. The Wasserstein barycenter efficiently represents a sentence by capturing the simultaneous occurrence of words. It considers the geometry of ground space, unlike simple Euclidean averaging. Figure 3 illustrates this comparison based on distributional estimates of words. The Wasserstein barycenter, known as Context Mover's Barycenters (CoMB), is more suitable than naive averaging for applications with inherent geometry. Averaging point-estimates has been effective in NLP tasks, including sentence similarity. CoMB represents each sentence as a distribution over contexts, utilizing Context Mover's Distance (CMD) to measure the distance between sentences. The Context Mover's Distance (CMD) is used to measure the distance between sentences by defining it under a given ground metric. To evaluate CoMB as an effective sentence representation, 24 datasets from SemEval STS tasks are considered, with sentences from various domains. The objective is to assign a similarity score to each sentence pair and rank them, evaluated against the ground truth ranking via Pearson correlation. Euclidean or angular distance is used as the ground metric, with point estimates obtained using GloVe on the Book Corpus. The representative contexts for distributional estimate are obtained through K-means clustering of point estimates with respect to angular distance. Various unsupervised methods like NBoW, SIF, Sent2vec, Skip-thought, and WME are benchmarked against each other. Additionally, recent supervised methods like InferSent and GenSen are compared, with GloVe as the ground metric. The best results are highlighted, with CoMB showing superiority over SIF. Using CoMB Mix + PC removed improves average test performance by 10% compared to SIF PC removed. When using Sent2Vec as the ground metric, CoMB Mix + PC removed achieves a performance of 66.4, a 4% improvement over Sent2vec. Sent2vec is an unsupervised method that outperforms WME and popular supervised sentence embedding methods like GenSen and InferSent. Ablation studies show that CoMB and SIF are complementary in nature, with CoMB performing better with differences in predicates and SIF excelling with differences in subjects. Increasing the number of clusters improves performance around K = 300 to 500. By around K = 300 to 500, increasing the number of clusters plateaus in performance. CoMB outperforms SIF on datasets with longer sentences. Using CoMB with GloVe or Sent2vec boosts performance beyond popular supervised methods like GenSen and InferSent. Future prospects include utilizing non-associativity for Wasserstein barycenters to consider word order in sentence representation. Hypernymy is a relation between words where one word's semantics are contained within another word's. It is relevant for tasks like Question Answering. Methods for hypernymy detection capture semantics and uncertainty about word contexts, making it a good testbed for evaluating entity representation effectiveness. The effectiveness of representing entities by the distribution of their contexts is tested using the Distributional Inclusion Hypothesis. The Context Mover's Distance is utilized with a ground cost to measure entailment relations accurately. The Distributional Inclusion Hypothesis is tested by representing entities through their context distribution. A method by Henderson & Popa (2016; Henderson, 2017) models known information about words using entailment vectors. Evaluation on 10 datasets shows the flexibility of the framework with an arbitrary cost function. Using CMD along with entailment embeddings shows a significant performance boost on most datasets, competing with state-of-the-art methods like Gaussian embeddings. On certain datasets, we even outperform or match state-of-the-art performance by simply using CMD with ground cost D based on entailment embeddings. This approach is not limited to specific entailment vectors and using CMD with specialized embedding vectors could further enhance results. Using specialized entailment vectors can enhance the performance of methods like CMD. Representing entities with distributional estimates on top of co-occurrence structures allows for optimal transport over context distributions. This framework provides an efficient and interpretable metric for comparing entities and groups, improving tasks like sentence similarity and word entailment detection. The key takeaway is to utilize co-occurrence information instead of discarding it, leading to better results in NLP tasks. In the appendices, supplementary details on experiments, mathematical framework, and results are provided. Computational aspects and clustering importance are discussed. Detailed results of sentence representation and hypernymy detection experiments are listed. Qualitative analysis of sentence similarity and hypernymy detection is described. Further details on the experimental framework, PPMI formulation, and Optimal Transport are given. References are also provided. In Section S1.5, references for software release are provided. Sentence representations are obtained using the Toronto Book Corpus, with errors removed and sentences longer than 300 words filtered out. The vocabulary size is 205513, with a co-occurrence matrix built using code from GloVe. Vectors have 300 dimensions and were trained for 75 iterations at a learning rate of 0.005. Performance was verified on word similarity tasks. Hypernymy detection is also discussed. The entailment vector training was done on a Wikipedia dump with 1.7B tokens using Stanford NLP library. Experiments used a vocabulary of 80,000 and 200-dimensional word embeddings. Scores on hypernymy detection task were reproduced following a similar training procedure. Hyperparameters were tuned using the HypeNet training set. Probabilities in PMI are estimated from cooccurrence counts in the corpus. The PPMI matrix is biased towards infrequent words and can be smoothed by raising context probabilities to an exponent. Shifted PPMI (SPPMI) matrix with a log shift can improve semantic associations. Computational aspects involve using Scipy for efficient PPMI computations and considering fraction of SPPMI contributions towards word partitions. The PPMI matrix can be normalized using column normalization (\u03b2) to balance the effect of non-uniform spread in total PPMI across clusters. Setting \u03b2 to 0.5 or 1 boosts performance on STS tasks. Another option is to interpolate between normalization and normalization with respect to cluster size by introducing a parameter \u03b2. Implementation involves using Python Optimal for this process. The implementation involves using Python Optimal Transport (POT) S5 for computing Wasserstein distances and barycenters on CPU. A custom GPU implementation using PyTorch is also built for efficiency. A batched version for barycenter computation is implemented, allowing for the computation of around 200 barycenters in 0.09 seconds with a speedup of about 10x. Scalability can be further improved by considering stochastic optimal transport techniques. When computing optimal transport using POT, a \u03bb value around 0.1 is typically used with log or median normalization to stabilize Sinkhorn iterations. Clipping the ground metric matrix can also improve performance. The value of p is less critical in regularized Wasserstein barycenter problems, with empirical results showing p = 1 performing better than p = 2 in sentence similarity experiments. In sentence similarity experiments, p = 1 outperformed p = 2 by 2-3 points. It takes less than 11 minutes to get results on all STS tasks with 25,000 sentences, including clustering, converting co-occurrences, and STS computation. Code and data for the experiments are available on Github and precomputed resources can be downloaded. A standard evaluation suite for Hypernymy is also provided for assessing model performance. The HypEval Python package for hypernymy detection handles OOV pairs and allows for efficient evaluation on GPU. Computational aspects and clustering make the problem scalable. The distributional estimate and the impact of the number of clusters on performance are discussed. Optimal transport between histograms of contexts offers a pleasing interpretation but may be computationally intractable. The cardinality of support histograms varies significantly, making the Sinkhorn algorithm computationally challenging. One solution is to use representative contexts through clustering to reduce the number of contexts needed. Dense low-dimensional embeddings with a meaningful metric could also help decrease the required number of contexts. Clustering contexts based on metric D G can lead to more abstract transport between contexts, but may reduce interpretability. Obtaining K representative contexts can help in writing histograms for words. The impact of the number of clusters on performance is analyzed using kmcuda's efficient K-Means algorithm on GPUs for Context Mover's Barycenters (CoMB) in sentence similarity experiments. The impact of the number of clusters on performance in sentence similarity experiments using Context Mover's Barycenters (CoMB) is analyzed. The performance improves with an increase in clusters until around K = 300, plateauing beyond that. Variants B and C show continued performance improvement until K = 500, possibly due to PPMI column normalization. The choice between K = 300 or 500 clusters depends on validation results. The text discusses the performance of Context Mover's Barycenters (CoMB) on sentence similarity tasks, suggesting that alternative methods may further improve results. Detailed results on STS-12, 13, 14, and STS-15 tasks are provided, along with validation set performance on STS16 using Toronto Book Corpus. The numbers represent average Pearson correlation x100. The text discusses the importance of the PPMI smoothing parameter \u03b1 in CoMB for sentence similarity tasks. The ideal value varies per task, but settings in Table S4 work well across tasks. Comparison of hyperparameters for CoMB is provided in Table S4. In this section, we analyze examples where CoMB performs better or worse than SIF. Comparing by rank is more meaningful than comparing raw distance values. Evaluation is done through Pearson/Spearman rank correlation. The better method is determined by comparing similarity scores relative to rank based on ground-truth scores. The evaluation is based on comparing similarity scores relative to rank using ground-truth scores, which range from 0 to 5. The ground-truth scores indicate the level of equivalence between sentence pairs. The evaluation procedure ranks sentence pairs based on similarity scores and ground-truth scores ranging from 0 to 5. Examples are selected where CoMB and SIF show the maximum rank difference for qualitative analysis. Three datasets are used to understand overall behavior. CoMB outperforms SIF in aggregate quantitative performance on Images and News datasets, while SIF performs better on WordNet. Qualitative analysis on News dataset from STS 2014 shows CoMB's better overall performance with a Pearson correlation of 64.9 compared to 43.0 for SIF. Examples of equivalent sentence pairs with additional details show CoMB's superiority over SIF. CoMB is better than SIF at ranking sentence pairs closer to the ground-truth ranking, especially when additional details are present in the sentences. This suggests that CoMB's approach of averaging word embeddings is more resilient to details compared to SIF. However, in some cases where sentence pairs have equivalent meanings with a few word substitutions, SIF outperforms CoMB. Observation 3 highlights how CoMB ranks sentence pairs closer to the ground-truth compared to SIF, especially when differences lie in the predicate. Observation 4 focuses on sentences differing in proper nouns, where CoMB excels in identifying these distinctions. Observation 3 highlights CoMB's strength in ranking sentence pairs closer to ground-truth when differences are in the predicate. Observation 4 emphasizes CoMB's ability to excel in identifying distinctions in sentences with proper nouns. In scenarios where the subject makes the most difference, SIF seems to be more effective. Observation 1 and 3 show that CoMB excels in ranking sentence pairs with distinct predicates. SIF is more effective when the subject or object is the distinguishing factor, as seen in observations 2 and 4. Results from the News dataset in STS14 also support these findings. The ranking of sentence pairs is done based on similarity, with CoMB and SIF methods compared for accuracy. The text discusses the comparison between CoMB and SIF ranking methods, highlighting their complementary nature and the potential benefits of combining them. It also suggests exploring different ground metrics for these methods to improve performance. Additionally, it mentions analyzing sentence lengths in datasets for STS tasks to understand the complexity of the data. The analysis of sentence lengths in datasets for STS tasks reveals that CoMB performs better than SIF on datasets with longer effective sentence lengths. The comparison shows that CoMB outperforms SIF on average, indicating potential factors at play in the performance difference. CoMB outperforms SIF on average, with a higher effective sentence length of 7.48 compared to 5.03. This aligns with the observation that more details in CoMB can refine the implied meaning. The difference in performance on datasets like STS13-FNWN with longer sentence lengths suggests the impact of corpora. CoMB outperforms SIF on the Images dataset in the STS15 task, with a Pearson correlation of 61.8 versus 51.7. The ranking of sentence pairs is done in descending order of similarity, with CoMB ranking closer to the ground-truth rank highlighted in blue. Observation A highlights the effectiveness of CoMB with CMD in ranking sentence pairs with varying degrees of equivalence. For example, CoMB ranks sentences with different descriptions accurately. Observation B focuses on sentence pairs with commonalities but not equivalence, where SIF may not perform as well as CoMB. Observation C discusses how CoMB is better at ranking completely dissimilar sentence pairs compared to ground-truth rankings. For example, in sentences like \"a little girl and a little boy hold hands on a shiny slide\" & \"a little girl in a paisley dress runs across a sandy playground\", CoMB outperforms in ranking. Observation C discusses how CoMB is better at ranking completely dissimilar sentence pairs compared to ground-truth rankings. For example, in sentences like \"a little girl and a little boy hold hands on a shiny slide\" & \"a little girl in a paisley dress runs across a sandy playground\", CoMB outperforms in ranking. In another example, SIF also ranks higher than ground-truth in a sentence pair about a person moving with something blue in the background. CoMB ranks it at 310 while SIF ranks it at 591, both closer to ground-truth than CoMB. The CoMB ranking is based on representing sentences with CoMB and comparing them with CMD. SIF ranking involves representing sentences with SIF and using cosine similarity. Qualitative analysis on the WordNet dataset from STS14 is discussed, showing examples of equivalent sentences ranked by CoMB and SIF. CoMB performs better in ranking dissimilar sentence pairs compared to ground-truth rankings. CoMB outperforms SIF in ranking sentence similarity by considering additional definitions and subtle differences in sentence objects. This is evident in examples where CoMB ranks higher due to its ability to capture context variations more effectively. When computing Wasserstein barycenter of distributional estimates using CoMB to represent sentences, the closest word in the vocabulary is found and compared to SIF with cosine distance. Observations are made on the top 10 closest neighbors for CoMB and SIF, highlighting differences in their approaches to sentence representation. CoMB and SIF do not consider word order. CoMB captures a diverse set of words fitting well in the context of a sentence. For example, \"i love her\" can generate contexts like \"i actually love her\" or \"i doubt her love\". CoMB's closest neighbors include words like 'decades' and 'commerce', while SIF's are similar to query words. CoMB's closest neighbor can act as a good next word for the query, suggesting potential for sentence completion tasks. In this section, detailed results for hypernymy detection are provided along with the corresponding hyperparameters. The average performance gain across 10 datasets using CMD and entailment vectors is indicated. The listed variants of CMD with the best validation performance on HypeNet-Train are discussed, along with common hyperparameters for both. Out of vocabulary details for entailment experiments are also included. The previous section provides detailed results for hypernymy detection with corresponding hyperparameters and performance gains using CMD and entailment vectors. The current section focuses on analyzing the performance of the method on different PPMI hyper-parameter configurations and comparing the effectiveness of using CMD with embeddings from Henderson (2017) versus using entailment embeddings alone through qualitative analysis by rank. Ground-truth details are compared using ranks assigned by both methods. In this analysis, binary ground truth data is used to evaluate the performance of methods on the BIBLESS dataset. Out of 1635 examples, 814 are labeled as 'True' and 821 as 'False'. The goal is to rank 'True' examples from 1 to 814 and 'False' examples from 815 to 1635. The selection criteria focus on examples with the largest rank differences between methods and how each method ranks hypernyms and hyponyms. The analysis evaluates the performance of methods on the BIBLESS dataset using binary ground truth data. The focus is on ranking hypernyms and hyponyms, with CMD outperforming Henderson embeddings quantitatively. Examples show where CMD and Henderson differ in their entailment predictions, with observations on co-hyponym pairs like ('banjo', 'flute'). The Henderson method struggles with assessing directionality in certain word pairs, while CMD performs better overall. Examples like ('box', 'mortality') show CMD's limitations, suggesting a need to consider word pair similarity in the ground metric."
}