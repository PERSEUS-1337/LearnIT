{
    "title": "HZEg3S-Gppr",
    "content": "Given the rapid advancement in NLP and speech processing techniques, there is a lack of systematic studies comparing different methods. To address this gap, a study was conducted on neural network models of spoken language to analyze how phonology is represented. Two analytical techniques were used to measure the encoding of phonemes and sequences in neural activation patterns. Factors such as learning and temporal scope of activations were manipulated. The study found that neural activation patterns encode phonological information to a certain extent. The study on neural network models of spoken language analyzed how phonology is represented in neural activation patterns. Global-scope methods are recommended for reporting analysis results with randomly initialized models. In the study of neural network models of spoken language, there is a focus on analyzing and interpreting the representations that emerge in these models. Various analytical techniques have been proposed and applied to different tasks and architectures. However, there is a lack of systematic studies comparing the strengths and weaknesses of these techniques. As a case study, three different models processing speech signals are examined to analyze their learned neural representations using two commonly applied techniques. In the study, neural representations in models of spoken language are analyzed using diagnostic models and representational similarity analysis. Two important factors manipulated in the experiments are the role of learning and the scope of extracted neural activations. The study aims to properly control for these factors in analyzing phonology representations. The study analyzes neural representations in models of spoken language using diagnostic models and representational similarity analysis. The scope of extracted neural activations is controlled for, with a focus on local and global activations. Results show differences between trained and randomly initialized target models, emphasizing the importance of reporting results with randomly initialized models as a baseline. Neural representations in models of spoken language are analyzed using various techniques to understand the linguistic information encoded in the learned representations. Different approaches are used to track the response of neurons and determine the type of linguistic information captured in the representations. The paper focuses on analyzing neural representations in models of spoken language using Probing techniques and Representational Similarity Analysis (RSA). These techniques are used to compare the output systematically and decode information from activation patterns of untrained recurrent networks. The paper analyzes neural representations in models of spoken language using Probing techniques and Representational Similarity Analysis (RSA). It discusses how the dynamics of the network and input signal characteristics can lead to non-random activation patterns. Techniques like Echo State Networks (ESN) and Liquid State Machines (LSM) use randomly initialized networks to generate nonlinear response signals for training models at a reduced cost. The focus is on how training a model changes the information encoded in representations from randomly initialized neural models. The majority of neural models focus on text rather than speech for representation analysis. Some studies analyze neural representations of phonology learned from speech input, showing selectivity to phonetic features. Phonemes are learned as an intermediate representation for predicting graphemes in deep layers. In experiments, diagnostic classifiers are used to study phoneme encoding in neural representations of speech. Results show substantial phonological knowledge encoded in all layers, with the strongest in lower recurrent layers. Similarly, another study examines phoneme encoding in an end-to-end ASR system, finding phonological information best represented in lower input and convolutional layers. This research extends to multiple languages like Arabic and English. The study extends previous work on phoneme encoding in neural representations of speech to multiple languages and datasets. The analysis focuses on transformer models trained on automatic speech recognition tasks, with a release of source code for replicating the analyses. The study utilized a pretrained joint CTC-Attention transformer model from the ESPNet toolkit, trained on the Librispeech dataset. The architecture includes 2D convolutional layers, followed by 12 transformer layers in the encoder and 6 in the decoder. The convolutional layers have 512 channels, and the flattened output dimensions are 20922 and 10240. The model used in the study is trained on a spectrogram with 80 coefficients and 3 pitch features, augmented with SpecAugment. It has 5000 SentencePiece subword tokens in the output and is trained for 120 epochs using Noam optimization. Decoding is done with a beam size of 60, resulting in word error rates of 2.6% and 5.7% on the test set. The Visually Grounded Speech (VGS) model matches images with spoken captions and is trained on the Flickr8K Audio Caption Corpus using an improved architecture by Merkx et al. The model in the study is trained on a spectrogram with 80 coefficients and 3 pitch features, augmented with SpecAugment. It uses 5000 SentencePiece subword tokens in the output and is trained for 120 epochs using Noam optimization. The speech encoder includes a 1D convolutional layer, bidirectional GRU layers, and a self-attention-based pooling layer. The image encoder utilizes features from a pre-trained ResNet-152 model followed by a linear projection. The model is trained using the Adam optimizer with a cyclical learning rate schedule and input features are MFCC with total energy and delta coefficients. The model architecture includes a transformer encoder with one-directional recurrent layers, a 1024 GRU unit attention-based decoder, and is trained on the Flickr8k dataset for speech recognition. The model is not optimized for speech recognition but aims to be similar to the RNN-VGS model. Two analytical approaches are considered: a diagnostic model and a regressor trained on neural activation patterns. The regressor is trained to predict information from neural activation patterns, indicating the presence of information in the neural representations. Representational similarity analysis (RSA) measures similarities between stimuli in neural activation pattern space and symbolic linguistic representations. Diagnostic models have trainable parameters, while RSA-based models do not, except with a trainable pooling operation. Activation patterns in hidden layers can be viewed as local or global representations. The text discusses different analysis models for neural representations, including local diagnostic classifiers, local RSA, and global diagnostic classifiers. Local diagnostic classifiers use single frames or time-steps as input to predict phonemes, while local RSA measures similarities between neural activations. Global diagnostic classifiers predict phoneme presence in an utterance based on global features. The text discusses using a diagnostic classifier to predict phoneme presence in utterances based on global neural activations. Pairwise similarity scores are computed between global representations using Pearson's r and string similarities. Two approaches for pooling representations are evaluated: mean pooling and attention-based pooling. The text discusses attention-based pooling for computing weighted averages over sequence positions to optimize the RSA score or diagnostic classifier error. Pearson's r is used for RSA, while relative error reduction measures phoneme decoding accuracy. The text discusses the importance of comparing scores on trained and randomly initialized models to assess sensitivity to learning effects. Analytical methods should show clear separation in scores. Global RSA values in RNN-VGS model layers may be influenced by correlations between string and visual similarities. The text discusses the use of RSA to control for confounding similarity spaces in visually grounded speech models. It mentions the use of scalar weights and a linear model for simplicity in training. In visually grounded speech models, X+Z is the sum squared error of the model with all variables, and Y \u223cZ is the sum squared error of the model with X removed. Y represents pairwise similarities in phoneme string space, X represents similarities in neural activation space, and Z represents similarities in visual space. Visual similarities are computed using cosine similarity on image feature vectors. Analytical methods are implemented in Pytorch. Diagnostic classifiers are trained using Adam with a learning rate schedule scaled by 0.1 after 10 epochs with no accuracy improvement, terminating training after 50 epochs. Global RSA with attention-based pooling is trained using Adam for 60 epochs with a fixed learning rate of 0.001. Model parameters are snapshot after every epoch for reporting results. In visually grounded speech models, model parameters are snapshot after every epoch for reporting results. When computing RSA scores, it is common practice to use the whole upper triangular part of matrices containing pairwise similarity scores between stimuli. However, due to a large number of stimuli, an alternative procedure is used where each stimulus is sampled without replacement for a single similarity calculation. Figures 1-3 display the results of analyzing target models using diagnostic and RSA methods. The data points are organized in a matrix of panels, showing local and global scope with different pooling methods. Diagnostic classifiers use relative error reduction, while RSA uses Pearson's correlation coefficient. Figure 4 shows global RSA results with mean pooling on the RNN-VGS model, controlling for visual similarity. The patterns of results for each model will be discussed separately in the following sections. Most experiments suggest phonemes are best encoded in pre-final layers of deep networks. Learning impacts analytical method predictions, with trained models outperforming randomly initialized ones. RNN layers 3 and 4 in VGS model show strong phoneme encoding. Trained model representations are stronger than random ones. Global learning impact is more significant than local scope. The global impact on trained representations diverges from random representations in the first RNN layer onward. Activation similarities contribute significantly to predicting phoneme string similarities in top RNN layers. Global diagnostic variants have higher scores and steeper curves, aligning more closely with encoding objectives. The choice of method, RSA versus diagnostic classifier, interacts with scope and impacts neural activations and phoneme labels. Local RSA shows weak correlations due to range restriction. Both analytical methods are sensitive to learning, with divergence in results between random and trained representations in early recurrent layers. Separation for local diagnostic classifiers is weaker in RNN models. Our experimental findings suggest that the temporal scale of extracted representations is an important choice, with global representations being more sensitive to learning. Results show that attention-based learned pooling is more erratic than mean pooling, requiring more careful tuning. Recommendations include running analyses on randomly initialized target models, as scores on such models are often close to trained models. In a systematic study of analysis methods for neural models of spoken language, global methods applied to pooled representations are recommended as a complement to standard local diagnostic methods. Results suggest that learning has a significant effect on the encoding of phonology in RNN models. However, limitations remain in controlling for various factors of variation in target models. In future studies, sampling target models with a larger number of combinations of factors is planned to address the need for architectural modifications. Analytical method choices may lead to changes in other aspects of the analysis, such as global RSA capturing phoneme sequential order. Further disentangling these differences is a goal for future research."
}