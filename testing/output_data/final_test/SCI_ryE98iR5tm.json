{
    "title": "ryE98iR5tm",
    "content": "Deep latent variable models have been successful in various data domains. Lossless compression using these models has not been practically implemented yet. A new scheme called 'Bits Back with ANS' (BB-ANS) aims to achieve near-optimal lossless compression rates with latent variable models. By applying this scheme to compress the MNIST dataset using a variational auto-encoder model (VAE), superior compression rates were achieved compared to standard methods. The scheme is highly parallelizable, suggesting that with a high-quality generative model, significant improvements in compression rate could be achieved with acceptable running time. The implementation is available open source at https://github.com/bits-back/bits-back. The relationship between information theory and machine learning is well-established. The connection between information theory and machine learning is strong, with probabilistic models of data being equivalent to lossless compression methods. Algorithms like Huffman coding, arithmetic coding, and asymmetric numeral systems implement this idea. Recent research has focused on applying deep learning methods to lossy compression, such as using deep latent Gaussian models for compression. Recent research has focused on applying deep learning methods to lossy compression, with an emphasis on using deep latent Gaussian models. While lossy compression has been implemented using various models like auto-encoders and GAN-like objectives, there is a lack of coverage on lossless compression. This study aims to advance in the direction of lossless compression using latent variable models, specifically focusing on the method of 'bits back coding' to extend algorithms to cope with latent variables efficiently. The study introduces a new coding scheme called 'Bits Back with ANS' (BB-ANS) to improve compression rate and code complexity for lossless compression of large datasets with deep latent variable models. BB-ANS is demonstrated to outperform generic compression algorithms when applied to the MNIST dataset using a variational auto-encoder (VAE) with continuous latent variables. Bits Back with ANS (BB-ANS) using a VAE shows superior performance over generic compression algorithms for binarized and raw MNIST data. The method utilizes latent variables for lossless data compression, with potential for even better results with larger models. The approach involves encoding data with a fully observed model, known as 'range coding' or 'entropy coding', with message lengths measured in bits. Arithmetic coding (AC) and asymmetric numeral systems (ANS) are algorithms that encode a sequence of symbols into a message with minimal bits. The message length is determined by the information content of the sequence and a small constant overhead. Shannon's Source Coding Theorem is used to calculate the expected message length. Arithmetic coding (AC) and asymmetric numeral systems (ANS) are algorithms that encode sequences with minimal bits. AC and ANS are close to optimal according to Shannon's Source Coding Theorem. They differ in the order of message decoding - AC is FIFO while ANS is LIFO. The decoder in these algorithms maps i.i.d. bits to a sample from a distribution. ANS/AC can be seen as invertible samplers, mapping random bits to samples via the decoder. Arithmetic coding (AC) and asymmetric numeral systems (ANS) are algorithms that encode sequences with minimal bits. They differ in the order of message decoding - AC is FIFO while ANS is LIFO. ANS/AC act as invertible samplers, mapping random bits to samples via the decoder. The sender and receiver can communicate symbols using a generative model with a latent variable, y. Bits back coding allows efficient encoding and decoding of symbols using a generative model with a latent variable. The sender communicates an extra sample y 0 to the receiver, who then recovers the latent sample and symbol. The expected increase in message length is equal to the negative of the evidence lower bound (ELBO), a key focus of recent research on inference and learning. Recent research has focused on inference and learning with approximate posteriors using the ELBO as an objective function. Maximizing the ELBO for a model implicitly minimizes the message length for bits back coding. The 'chaining' scheme involves using encoded data points as extra information for subsequent points, creating a daisychain-like effect. The 'chaining' scheme, also known as 'bits-back with feedback', involves implementing a stack-like wrapper around arithmetic coding (AC) to overcome decoding order issues. However, this incurs a compression rate cost due to flushing AC between iterations. A novel method using Asymmetric Numeral Systems (ANS) eliminates this overhead by leveraging its stack-like nature, making it more efficient for implementing chaining without compression rate penalties. The novel method 'Bits Back with ANS' (BB-ANS) involves using the stack-like state of an ANS coder for efficient encoding and decoding. The process is invertible and repeatable, allowing for encoding and decoding of symbols without compression rate penalties. The BB-ANS algorithm allows for efficient encoding and decoding using the stack-like state of an ANS coder. It is compatible with various models and can be repeated without compression rate penalties. The efficiency of compression with BB-ANS may be affected by factors such as finite precision approximation of probabilities. A Python implementation of the encoder and decoder is provided in the appendix. In the worst case scenario, a batch with only one datapoint will have a message length equal to the log joint, log p(s 0 , y 0). Optimization is equivalent to maximum a posteriori (MAP) estimation. For batches with more than one image, this effect is amortized. BB-ANS performs well with 30 samples, as shown in FIG0. Two specific issues related to BB-ANS are discussed, and their effects are experimentally investigated in Section 3.2. When compressing the MNIST test set, these issues do not significantly impact the compression rate, which is typically close to the negative ELBO in experiments. Bits back coding has been implemented for models with discrete latent variables, but successful models with continuous latents, like the VAE used in experiments, are not excluded. A derivation based on previous work is presented. In our experiments, we derive the surprising fact that continuous latents can be coded with bits back without affecting the coding rate. Our implementation is the first to support continuous latents. We approximate a continuous probability distribution with a discrete distribution by partitioning the real line into buckets. During bits back coding, we discretize both the prior and the approximate posterior using the same set of buckets. Sampling from the discrete approximation uses approximately. The text discusses the encoding of continuous latents using bits back without affecting the coding rate. It explains how discrete approximations are used to sample from the distribution, with the precision of the latent sample scaling with the logarithm of the precision. The impact of discretization is minimal if the density functions are smooth, and increasing precision beyond a certain point has negligible benefits in most machine learning models. In experiments, performance gains were minimal beyond 16 bits per latent dimension. Latent space is divided into buckets with equal mass under the prior. Discretization is simple, computationally efficient, and shows good performance. Further research is needed to optimize the trade-off between compression rate and computation. Random bits are needed for bits back coding to produce true samples from the distribution. During chaining, compressed data points are used as seeds for the next iteration. The encoding of latent variables may not always produce clean bits due to discrepancies between the true sampling distribution and the prior. The average encoding distribution is crucial in this process. The discrepancy between the true sampling distribution and the prior can impact the encoding of latent variables. BID11 measured this using 'marginal KL divergence' and found it significantly affects the ELBO for VAE models on MNIST. The BB-ANS coding scheme was demonstrated using a VAE with a multidimensional latent and standard Gaussian prior. The usual VAE training objective is the ELBO, which can be integrated into the BB-ANS framework for training. The text discusses training a VAE on the MNIST dataset and compressing it using the BB-ANS framework. VAEs with fully connected networks are used, with different dimensions for hidden layers and latent variables. The output distributions are parameterized for both raw and binarized MNIST data. The BB-ANS framework utilizes a layer of dimension 200 with a stochastic latent of dimension 50 to model output distributions on pixels using a beta-binomial distribution. The generative network outputs two beta-binomial parameters for each pixel and initializes the BB-ANS chain with 'clean' bits instead of random sampling. Around 400 bits are needed for initialization, with the exact number depending on the entropy of the approximate posterior. The BB-ANS scheme shows superior compression performance compared to standard schemes, even with small network sizes and simple architectures. The BB-ANS framework, using a small latent variable model, outperforms standard compression techniques. Compression rates on binarized MNIST and full MNIST test sets are close to negative ELBO values. Effects of finite precision and 'clean' bits are minimal. State-of-the-art latent variable model implementation is not the focus, but BB-ANS shows promising results in experiments. BB-ANS can achieve compression rates close to the negative ELBO values of state-of-the-art latent variable models like PixelVAE BID8. Predictions suggest that BB-ANS with PixelVAE may offer significantly better compression rates than existing schemes, assuming small discrepancies between compression rate and ELBO for larger models. There are no fundamental differences between complex hierarchical VAEs like PixelVAE and simple VAEs used in experiments. Potential extensions of BB-ANS include time series latent variable models, but implementing them may lead to sub-optimal compression rates. Further exploration is needed on interleaving bits with time steps in models. Predicted compression of BB-ANS with PixelVAE against other schemes is measured in bits. Table 3 shows the predicted compression of BB-ANS with PixelVAE compared to other schemes in bits per dimension. The current implementation of BB-ANS is in pure Python and runs on CPU, but could potentially be parallelized on GPU hardware to improve speed. The encoder computes CDF and inverse CDF of distributions, which could benefit from GPU acceleration. The ANS algorithm, although less trivial, is amenable to parallelization and has been implemented on GPU in open source toolboxes. Future work includes optimizing BB-ANS for parallel architectures to achieve high performance with large datasets. Considerations for communication and storage costs of model parameters may be necessary when using BB-ANS with a neural net model like VAE. When using BB-ANS with a large scale model, communication costs of parameters should be considered. Training a latent variable model to effectively compress a wide range of images could make the cost of communicating model weights worthwhile. Efforts to train such models are of interest in the machine learning community, especially on expansive datasets like ImageNet. This direction is seen as the most promising for practical applications of BB-ANS. Recent developments aim to reduce the space needed for neural network weights. Recent developments in methods to decrease the space required for neural network weights, such as quantizing weights to low precision, show promise in reducing communication and storage costs. Probabilistic modeling in machine learning is an active research area, with potential applications in lossless compression. A scheme called BB-ANS has been demonstrated for lossless compression using latent variable models, achieving superior compression rates on the MNIST dataset. BB-ANS was demonstrated for compressing the MNIST dataset with superior rates. It addresses latent discretization and achieves close to negative ELBO compression sizes. This achievement with latent variable models suggests improved lossless compression rates. BB-ANS is parallelizable and can run on GPU hardware for fast compression. The derivation of bits back coding is presented for communication using generative models with latent variables. The sender and receiver can communicate a sample from a model with a latent variable by encoding and decoding according to forward probabilities. By utilizing additional random bits, the sender can decrease the encoded message length significantly. This method allows for efficient communication and improved lossless compression rates. The sender can generate a sample y 0 using some bits and encode it with the forward model. The receiver can recover the information by decoding and encoding y 0. Different distributions can be used for sampling y 0, leading to varying message lengths. The expected message length on s 0 is equal to the negative of the evidence lower bound (ELBO), also known as the 'free energy' of the model. The optimal setting of q is the posterior p(y | s 0 ), with this setting the message length is the information content of the sample s 0. Bits back can achieve an optimal compression rate with access to the posterior, otherwise, an approximate posterior must be used. BID1 and BID19 approach lossless compression with latent variables by generating a latent from an approximate posterior and encoding according to the prior and likelihood. The cost of coding the hierarchical distribution is a small fraction of the total coding cost. Using BB-ANS for encoding multiple data points would result in a better compression rate. Discretizing the continuous latent space is necessary for encoding continuous variables with ANS. The discretization must be appropriate for the densities used for coding. Discretizing the continuous latent space is crucial for efficient coding. The discretization should align with the densities used for coding. To achieve this, the maximum entropy discretization of the prior is proposed, ensuring efficient coding and decoding processes. The discretization of the prior is crucial for efficient coding and decoding processes. Minimizing the objective function encourages minimizing the KL divergence between the posterior and the prior, indicating that the maximum entropy discretization of the prior may also be suitable for coding according to the posterior. The maximum entropy discretization of the prior may be suitable for coding according to the posterior. Python implementation of BB-ANS encoding and decoding is shown in FIG4. The functions for encoding and decoding use LIFO algorithms, such as ANS coding. Each pop function must precisely invert the corresponding append function. More details, including an example implementation with a VAE model, can be found in the repository. The repository https://github.com/bits-back/bits-back includes an example implementation with a variational auto-encoder model (VAE)."
}