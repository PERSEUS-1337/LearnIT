{
    "title": "HJlRFlHFPS",
    "content": "In this work, the authors aim to learn a transformation of contextualized word representations that retains structural information while discarding lexical semantics. They generate groups of structurally similar but semantically different sentences and use a metric-learning approach to emphasize structural components in the vectors. The transformed vectors cluster in space based on structural properties rather than lexical semantics, outperforming original representations in few-shot parsing tasks. The authors aim to extract structural information from contextualized word representations while disregarding lexical semantics. They use a metric-learning approach to emphasize structural components in the vectors, resulting in transformed vectors that cluster based on structural properties rather than lexical semantics, outperforming original representations in few-shot parsing tasks. The authors aim to disentangle syntax from lexical semantics in word representations by learning a transformation that removes lexical-semantic information while preserving structural properties. This disentanglement allows for better control of confounding factors in model analysis and prediction attribution. The authors aim to disentangle syntax from lexical semantics in word representations to enable controlled generation and search-based applications. They generate structurally-similar sentences without presupposing their formal structure, focusing on preserving the structural component regardless of the lexical semantics. The authors aim to disentangle syntax from lexical semantics in word representations by generating structurally-similar sentences and learning representations that are invariant to changes in lexical semantics. This approach is demonstrated through experiments on identifying structurally-similar words and few-shot parsing. In the field of NLP, disentanglement is less researched compared to other areas like style transfer and image recognition. Several works have attempted to disentangle factors of variation such as sentiment or age of the writer in controlled natural language generation. Conditional generative models have been trained to generate new sentences with specific properties or transfer existing sentences to similar ones by explicitly conditioning a decoder network with a vector of attributes. In NLP, disentanglement is a less researched area compared to style transfer and image recognition. Previous works have focused on disentangling sentence-level attributes, while this study aims to disentangle components in the representations of individual words. Other works have examined how semantic and syntactic information is distributed across neural models of text. Recent works have focused on extracting syntax-related representations from neural models of text, using diagnostic classifiers to predict syntactic properties and demonstrating that different parts of the model encode information at various levels of abstraction. Additionally, studies have probed attention patterns in models like BERT, showing how individual attention-heads focus on syntactically-meaningful relations in the input. Artetxe et al. (2018) and Hewitt & Manning (2019) utilized linear transformations to adapt word representations for syntactic and semantic tasks. Li & Eisner (2019) employed a variational estimation method to extract word embeddings for parsing tasks, assuming a specific formal syntactic structure. In contrast to previous works that assume a given structure and use supervision to learn structural information, we aim to expose structural information in neural networks in an unsupervised manner. Our goal is to learn a function that operates on contextualized word representations and extracts vectors that highlight structural information while minimizing lexical information. The text discusses learning a function that operates on contextualized word representations to extract vectors highlighting structural information while minimizing lexical information. This approach is inspired by works showing semantic and syntactic relations between words can be approximated by simple vector arithmetic. The method involves representing pairs of words by the difference between their transformations and learning from groups of structurally equivalent sentences. The text discusses generating sets of structurally equivalent sentences using a language model and learning a function to map contextualized word vectors to neighboring points in space. This approach aims to highlight structural information while minimizing lexical details, inspired by the idea that semantic and syntactic relations can be approximated through vector arithmetic. The text discusses training a function to map word vectors to neighboring points in space, distinguishing between positive and negative pairs using Triplet loss. The network uses vector arithmetic to highlight structural information in sentences. The text discusses using a pre-trained language model - BERT - to replace content words in sentences while maintaining grammatical structure and respecting various restrictions. This method aims to generate sentences with similar structure by replacing words with words of the same part of speech. The text discusses using a pre-trained language model - BERT - to replace content words in sentences while maintaining grammatical structure. The approach aims to generate grammatical sentences with similar structures, although there are limitations in more nuanced cases. The text discusses using a BERT masked LM model to generate equivalent sentences by replacing words with top-30 predictions, aiming to expose structural similarity in neural language models. The approach involves iteratively masking and replacing words in a sentence to increase semantic variability. The text discusses using a BERT masked LM model to generate equivalent sentences by replacing words with top-30 predictions, aiming to expose structural similarity in neural language models. The suggestions from BERT aim to be semantically and structurally correct, maintaining POS and encouraging semantic diversity through an auto-regressive replacement process. Despite some generated sentences violating requirements, the average sentence is grammatical and maintains the original structure. Additional generated groups are shown in the appendix, highlighting recurring errors. Sets in Figure 1 were generated using this method, resulting in 1,500,000 training pairs and 200,000 evaluation pairs for the training process. The training process for mapping function f involves using 1,500,000 training pairs and 200,000 evaluation pairs. ELMo and BERT-based contextualized representations are experimented with, sampling 11 pairs from each group of equivalent sentences on average. ELMo representations are of dimension 2048, while BERT representations involve concatenating the mean of words' representation across all 23 layers with layer 16. Triplet loss is used to learn the mapping function f, where equivalent sentences are randomly chosen as anchor and positive sentences for training. The training process involves using 1,500,000 training pairs and 200,000 evaluation pairs. Triplet loss is used to learn the mapping function f, where equivalent sentences are randomly chosen as anchor and positive sentences for training. The transformation model f is a single linear layer mapping from dimensionality 2048 to 75, chosen based on development set performance. The goal is to move the anchor vector V A closer to the positive vector V P and farther from the negative vector V N using triplet loss. The training process involves using triplet loss to optimize the mapping function f for sentence representations. The softmax version of the triplet loss is calculated using cosine-distance between vectors. The model is trained using the Adam optimizer for 5 epochs with a mini-batch size of 500. Hard negative sampling is used to obtain negative vectors. The gradient backpropagates through pair vectors to learn individual word representations. Contextualized vectors are kept intact during training. The training process involves using triplet loss to optimize the mapping function for sentence representations. Negative sampling is used to obtain negative instances for each mini-batch. The model is trained to retain structural information encoded in contextualized vectors while discarding other information. The representations acquired by the model are evaluated in an unsupervised manner. The training process involves using triplet loss to optimize the mapping function for sentence representations, with negative sampling for obtaining negative instances. The model retains structural information in contextualized vectors while discarding irrelevant details. The transformed vectors focus on syntactic properties, outperforming original ELMO representations in a few-shot setting. Training data includes 150,000 sentences from Wikipedia, tokenized and POS-tagged by spaCy. No syntactic annotation is used during training. The evaluation sentences for the experiments are sampled from a collection of 1,000,000 original and unmodified Wikipedia sentences. A 2-dimensional t-SNE projection of 15,000 random content words is shown, comparing original ELMo states to syntactically transformed ones. The transformed vectors are more neatly clustered, showing improved clustering of specific dependency labels. After syntactic transformation, clusters like direct objects and prepositional-objects are separated. Functions that were grouped together in ELMo, such as adjectives and noun-compounds, are now split into different clusters based on their sentence positions. A large enough mini-batch is needed to find challenging negative examples, assuming pairs from different sentence groups are valid negatives. The t-SNE projection of ELMO states before and after transformation shows improved clustering of syntactic functions. The text discusses how clusters are split based on sentence positions after syntactic transformation. K-means clustering is used to quantify the difference, with different K values tested. The average cluster purity score reflects the division into grammatical functions. After syntactic transformation, K-means clustering was used to analyze clusters split based on sentence positions. Different K values were tested, resulting in increased class purity scores. Examples show structural similarity and word function changes before and after transformation. The director's anger towards Crazy Loop leads to an unsuccessful attempt to remove him from the show. Jetley's mother was the daughter of a high court advocate. The city offers a route-finding website for personalized bike routes. The transformed vectors aim to capture more structural similarities. The study focuses on comparing structural properties of word vectors before and after syntactic transformation. Results show differences in closest-word queries between unmodified ELMo vectors, ELMo vectors after transformation, and ELMo vectors after training on a random initialization. Structural properties include dependency-tree edges and higher-level structures like subjects within relative clauses. The study compares structural properties of word vectors before and after syntactic transformation. It analyzes dependency-tree edges, higher-level structures like subjects in relative clauses, and constituency-parse paths. The evaluation involves parsing 400,000 random sentences from a Wikipedia sample, using ELMo and BERT for contextualized word representations, and querying 400,000 word vectors. The study evaluates structural properties of word vectors before and after syntactic transformation by analyzing dependency-tree edges, higher-level structures, and constituency-parse paths. It involves parsing random sentences from a Wikipedia sample using ELMo and BERT for contextualized word representations and querying word vectors. The baseline models tend to retrieve lexically similar words, leading to \"right for the wrong reason\" results, which are controlled for to ensure grammatical generalization of the model. The study evaluates the structural properties of word vectors by analyzing dependency-tree edges, higher-level structures, and constituency-parse paths. The evaluation shows that the model's performance in capturing structural information is superior to lexical information, as evidenced by the drop in performance of baseline models like ELMo and BERT on words with high entropy POS tags. The study focuses on evaluating the structural properties of word vectors, showing significant improvement with ELMo over the baseline. The model emphasizes encoding the depth of words in the tree structure, resulting in better representation. There is a notable increase in full constituency-path to the root, and a decrease in lexical match pairs, indicating the removal of irrelevant lexical information for structure. The study evaluates the impact of ELMo on word vector structural properties, showing significant improvement over the baseline. The model emphasizes encoding word depth in tree structure, leading to better representation. The model trained on randomly-initialized ELMo performs substantially worse than the baseline. The model acquires different generalization and emphasizes different similarities in its representations. The study compares dependency parsers trained on different representations, expecting the 75-dimensional representations to capture structural information. They conduct few-shot dependency parsing tests to evaluate the performance of their hypothesis. The model uses ELMo contextualized embeddings and compares them with their smaller representation. The study compares dependency parsers using ELMo contextualized embeddings with a smaller 75-dimensional representation called elmo-pca. The elmo-pca experiment applies PCA to reduce dimensionality and tests if structural information is preserved. Another experiment involves learning a matrix during training to reduce embedding dimension. In the study, a matrix is learned during training to extract structural information from representations. The experiment elmo-reduced is compared to syntax, where structural extraction is applied on top of ELMo representation. Results show that in lower training sizes, syntax outperforms elmo and elmo-pca, indicating that structural information is crucial. In this study, a matrix is learned to extract structural information from representations. The syntax representation outperforms elmo-reduced in labeled attachment score. The proposed unsupervised method distills structural information from neural contextualized word representations. The method presented in this study extracts structural information from representations, demonstrating improved performance in parsing tasks compared to ELMo. The approach aims to disentangle different types of information in neural sequence models and can be beneficial for tasks like augmenting parse-tree banks. Future work will focus on enhancing the alignment between structural elements. In future work, the method aims to extend soft alignment between sentences. Results show improvements in closest-word queries after syntactic transformation. \"Baseline\" refers to unmodified BERT vectors, while \"Transformed\" refers to vectors post-transformation. \"Difficult\" evaluation focuses on diverse POS tags."
}