{
    "title": "S1xzyhR9Y7",
    "content": "Multi-view learning provides self-supervision with different views of the same data. The distributional hypothesis offers self-supervision from adjacent sentences in unlabelled corpora. Two multi-view frameworks for learning sentence representations in an unsupervised manner are presented, one using a generative objective and the other a discriminative one. The final representation is an ensemble of two views, one encoded with an RNN and the other with a linear model. The vectors produced by these frameworks offer improved representations compared to single-view models. Multi-view learning methods improve representations by combining different views of data, enabling self-supervised feature learning for future prediction. Minimizing disagreement among views helps learn rich features, and ensemble of feature vectors enhances generalization ability. Distributional hypothesis suggests words in similar contexts have similar meanings, and distributional similarity confirms this idea. The distributional hypothesis is widely used in machine learning to learn vector representations of languages without human-annotated data. Unsupervised and self-supervised learning are important due to the difficulty and cost of obtaining annotated data. Algorithms based on multi-view learning and distributional hypothesis aim to learn from unlabeled data, drawing inspiration from the lateralization and asymmetry in information processing of the human brain. Our proposed multi-view frameworks combine RNN-based models and linear/loglinear models for learning sentence representations. Previous work on unsupervised sentence representation learning can be categorized into generative objective models, which follow an encoder-decoder structure. These models aim to produce vector representations for input and generate sentences in the adjacent context. However, scalability for large corpora is hindered by slow decoding. Our first multi-view framework combines an RNN encoder with an invertible linear projection decoder, reducing training time and utilizing the decoder post-learning. Regularization enforces invertibility on the linear decoder, allowing it to be used as a linear encoder after training. A discriminative objective involves training a classifier on top of the encoders to distinguish adjacent sentences. Our second multi-view framework utilizes a discriminative objective with RNN and linear encoders to maximize agreement among adjacent sentences. It processes the same data in two distinctive ways, encouraging better representation alignment between views for future use of learned representations. Two frameworks for learning sentence representations are proposed, one using a generative objective and the other adopting a discriminative objective. Both frameworks involve learning two encoding functions, an RNN and a linear model. Aligning representations from two views improves performance on evaluation tasks compared to single-view trained counterparts. Models trained under these frameworks achieve good performance on unsupervised tasks and outperform existing models. Our model combines RNN-based sentence encoder and avg-on-word-vectors encoder into multi-view frameworks to achieve good results on both supervised and unsupervised tasks without labeled information. The motivation behind this idea is that RNN-based encoders capture complex syntactic interactions, while avg-on-word-vectors encoder is effective at processing sentences. The multi-view frameworks combine RNN-based sentence encoder and avg-on-word-vectors encoder to capture complex syntactic interactions and coarse sentence meaning. Two different sentence encoders are used to compose the sentence representation, including a bi-directional GRU and a linear avg-on-word-vectors model. The learning framework described in the curr_chunk utilizes a model that transforms word vectors in a sentence using a learnable weight matrix and outputs an averaged vector. It focuses on predicting words in the next sentence without the need for an autoregressive or RNN decoder. The framework includes an RNN encoder and a linear decoder to generate sentence representations that excel on downstream tasks. Negative sampling is applied to calculate the likelihood of generating words in the next sentence. The learning framework described utilizes a model that transforms word vectors in a sentence using a learnable weight matrix and outputs an averaged vector. Negative sampling is applied to calculate the likelihood of generating words in the next sentence. To enforce invertibility on the weight matrix, a row-wise orthonormal regularisation is applied during training. The regularisation formula ensures that the inverse function is easily computed. The learning framework described utilizes a model that transforms word vectors in a sentence using a learnable weight matrix and outputs an averaged vector. Negative sampling is applied to calculate the likelihood of generating words in the next sentence. To enforce invertibility on the weight matrix, a row-wise orthonormal regularisation is applied during training. The regularisation formula ensures that the inverse function is easily computed. In the subsequent step, the framework sets \u03b2 to 0.01 and utilizes the decoder as the encoder after learning, maximizing agreement between sentence representations across two views. The training objective is to minimize the loss function with a trainable temperature term \u03c4 for exaggeration. The DISPLAYFORM2 model uses a trainable temperature term \u03c4 to emphasize differences between adjacent sentences. The choice of cosine similarity based loss is influenced by the correlation between word vector length and frequency. The model is unsupervised/self-supervised, learning important meaning from similarities between neighboring sentences. Postprocessing involves removing the top principal component and applying l2 normalization. In the multi-view framework, a discriminative objective aims to reduce discrepancies. Our multi-view framework with discriminative objective aims to reduce the training-testing discrepancy by removing the top principal component during learning. Three unlabelled corpora from different genres are used in experiments, with models trained separately on each corpus using Adam optimizer and gradient clipping for stable training. Pretrained word vectors (fastText) are fixed during learning. Summary statistics of the corpora can be found in TAB6. Representation pooling methods in the testing phase are also discussed in Table 2. In the testing phase, different pooling methods such as \"max(\u00b7)\", \"mean(\u00b7)\", and \"min(\u00b7)\" are used to calculate a single sentence representation. Experiments are conducted in PyTorch, and models are evaluated using the modified SentEval package. Hyperparameters are tuned based on the averaged performance on STS14. Batch size and dimension are also considered in the framework. In the testing phase, different pooling methods are used to calculate a single sentence representation. Experiments are conducted in PyTorch, and models are evaluated using the modified SentEval package. Hyperparameters are tuned based on the averaged performance on STS14. Batch size and dimension are considered in the framework. The representation of a sentence input is calculated as z = \u1e91 f + \u1e91 g /2, where \u1e91 is the post-processed and normalized vector. Unsupervised tasks include tasks from SemEval STS 2012-2016 and SemEval2014 SICK-R. Models are compared with fastText and fastText+WR in unsupervised learning, and word vectors are pretrained on each task without label information in semi-supervised learning. The supervised learning methods used in the study include ParaNMT, InferSent 3 trained on SNLI and MultiNLI. Results show that models trained with their learning frameworks outperform other unsupervised and semisupervised methods. The model trained on the UMBC News Corpus with discriminative objective performs the best, likely due to the domain match with the STS tasks. Evaluation involves learning a linear model on top of the learnt sentence representations. The study evaluates supervised learning methods like ParaNMT and InferSent 3, showing models trained on UMBC News Corpus perform the best. Learning a linear model on top of sentence representations is preferred for selecting relevant dimensions. The models perform similarly or better than existing methods with higher training efficiency. The study evaluates supervised learning methods like ParaNMT and InferSent 3, showing models trained on UMBC News Corpus perform the best. Learning a linear model on top of sentence representations is preferred for selecting relevant dimensions. The models perform similarly or better than existing methods with higher training efficiency. On top of hidden states H, the last hidden state, and z g are calculated with three pooling functions. Post-processing and normalization steps are applied individually, and the representations are concatenated to form a final sentence representation. Tasks include Semantic relatedness (SICK), paraphrase detection (MRPC), question-type classification (TREC), movie review sentiment (MR), Stanford Sentiment Treebank (SST), customer product reviews (CR), subjectivity/objectivity classification (SUBJ), and opinion polarity (MPQA). Results are presented in Table 5, along with comparisons to supervised task-dependent training models, supervised learning models, and unsupervised learning models. Note that the results of the best single model of MC-QT (BID33) trained on BookCorpus are collected for fair comparison. Six models trained with different methods are also discussed. The best single model of MC-QT (BID33) trained on BookCorpus outperforms other methods on some tasks. The model trained on Amazon Book Review performs best on sentiment analysis due to strong sentiment information. RNN and linear encoders perform well in both frameworks, with generative and discriminative objectives showing similar performance. Orthonormal regularization on the linear decoder enforces invertibility in the multiview framework. Comparisons are made between training with and without the invertible constraint. The ensemble method of two views, f and g, benefits from aligning representations by applying invertible constraint. RNN encoder f improves on unsupervised tasks by aligning with g. On supervised tasks, the linear classifier picks relevant features from both views for good predictions in the multi-view framework. No significant difference is observed with or without the invertible constraint. The multi-view framework with two different views/encoding functions is compared to other variants, showing that linear/log-linear models produce better representations for unsupervised tasks. The results in TAB4 demonstrate that g consistently outperforms f on unsupervised tasks, and multi-view learning with both f and g improves performance on both unsupervised and supervised tasks. In multi-view frameworks, two encoding functions f and g improve each other's view, leading to better performance on supervised tasks. However, simply averaging representations from two views without alignment results in poor performance on unsupervised tasks. The ensemble of two views provides better performance on both supervised and unsupervised evaluation tasks. Our multi-view framework with f and g encoding functions outperforms ensembles of two multi-view models on unsupervised tasks and matches results on supervised tasks. The model also shows higher training efficiency. The proposed sentence representation learning frameworks combine RNN-based and linear encoders, can be trained efficiently on large unlabelled corpora, and demonstrate generalization ability through experiments on three corpora. The study demonstrates the effectiveness of the multi-view framework with f and g encoding functions, showing superior performance on unsupervised tasks and matching results on supervised tasks. The experimental results highlight the advantages of using linear/log-linear models for unsupervised tasks and RNN-based models for supervised tasks. Multi-view learning aligns f and g to produce better representations, while the ensemble of both views provides rich semantic information. Future research should explore the impact of various encoding methods. Future work should explore the impact of different encoding architectures and learning under the multi-view framework, inspired by the asymmetric information processing in the human brain. The Power Iteration algorithm is used to estimate the top principal component from representations produced by f and g. The Power Iteration algorithm is used to estimate the top principal component from representations produced by f and g. The algorithm applies to a batch of representations Z from either f or g to estimate the top eigenvector of the covariance matrix. Hyperparameters to tune include batch size, GRU encoder dimension, context window, and number of negative samples. Results are based on a model trained with specific parameters. The model presented in this paper is based on training with N = 512, d = 1024. The context window is set to c = 3 for the discriminative objective and K = 5 for the generative objective. It requires 8GB on a GTX 1080Ti GPU. Initial learning rate is 5 \u00d7 10 \u22124, weights are initialized using BID23 method, and word vectors are fixed from FastText. Temperature term is initialized as 1 and tuned during training. The model presented in the paper is trained with N = 512, d = 1024, and uses a context window of c = 3 for the discriminative objective and K = 5 for the generative objective. The model requires 8GB on a GTX 1080Ti GPU, with an initial learning rate of 5 \u00d7 10 \u22124. The temperature term \u03c4 is a trainable parameter initialized to 1 and decreases consistently through training. A post-processing step 'WR' improves model performance on unsupervised tasks and supervised sentence similarity tasks, but has no significant impact on single sentence classification tasks. The model in the paper is trained with specific parameters and utilizes a post-processing step 'WR' to enhance performance on various tasks. The generative objective in the model, when combined with the discriminative objective, does not show significant improvement compared to using either objective alone."
}