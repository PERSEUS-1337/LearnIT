{
    "title": "Sygx4305KQ",
    "content": "We propose a fast second-order method for deep learning that requires only two additional operations per iteration compared to stochastic gradient descent. This method addresses issues with current second-order solvers by keeping a single estimate of the gradient projected by the inverse Hessian matrix, updating it once per iteration with just two passes over the network. No estimate of the Hessian is maintained. CurveBall is a method validated on small problems with known solutions, demonstrating faster convergence on large models like ResNet and VGG-f networks without hyperparameter tuning. It shows generality by testing on randomly-generated architectures, highlighting the limitations of Stochastic Gradient Descent (SGD) in terms of convergence speed. Second-order methods like Newton's method rescale the gradient based on the local curvature of the objective function, allowing for faster progress in poorly scaled regions of the parameter space. This rescaling can achieve local scale-invariance and better convergence compared to SGD. In this paper, a new algorithm is proposed to make second-order optimization practical for deep learning by avoiding the storage of Hessian estimates. The algorithm treats the computation of the Newton update as solving a linear system that can be solved via gradient descent, overcoming obstacles in applying second-order methods to deep models. Our proposed method, CURVEBALL, makes second-order optimization practical for deep learning by avoiding the storage of Hessian estimates. It treats the Newton update computation as solving a linear system via gradient descent, with little overhead. The method is equivalent to momentum SGD with an additional term for curvature. Memory footprint is small, similar to momentum SGD. The paper introduces technical background, presents the method, evaluates it with experimental results, discusses related work, and summarizes findings. The method described is focused on optimizing the parameters of a model, such as a neural network, using gradient descent. It introduces a variant of gradient descent with momentum to improve convergence. The approach aims to find the optimal parameters by minimizing a loss function, with the goal of achieving better performance in deep learning tasks. Momentum GD, with a momentum parameter \u03c1, shows faster convergence than GD for convex functions, remains stable under higher learning rates, and has better resistance to poor scaling of the objective function. It requires minimal additional computation and memory, making it widely used in practice. In neural networks, GD is often replaced by stochastic gradient descent (SGD), where gradients are computed on small random batches. The Newton method, similar to GD, uses the inverse Hessian matrix for gradient direction but may face numerical challenges. The Levenberg method regularizes the Hessian matrix to address numerical instability issues in computing descent directions. Unlike momentum GD, the new step is independent of the previous step. The regularization rescales the step appropriately for different directions based on curvature, but loses the scale-invariance of the original Newton method. The LevenbergMarquardt method addresses numerical instability by regularizing the Hessian matrix. It replaces I in eq. 4 with diag(H) to improve convergence for non-convex functions like deep networks. Jacobians are computed using back-propagation, with Jacobian-vector products implemented for fast computations involving the Hessian. Forward-mode automatic differentiation (FMAD) computes a vector-Jacobian product in the forward direction, which is relevant for calculations involving the Hessian in deep learning. FMAD is suitable for vector-valued functions of a scalar argument, while backpropagation (RMAD) is used for derivatives of scalar-valued functions like the learning objective. The main difference between RMAD and FMAD is the direction of gradient propagation. The Hessian in deep learning can be computed using forward-mode automatic differentiation (FMAD) or back-propagation (RMAD). Both methods have similar computational overhead and can be implemented similarly. To prevent second-order methods from being attracted to saddle-points, a surrogate matrix like the Gauss-Newton approximation is commonly used. When the Hessian is positive semidefinite (PSD), only computing Hessian-vector products is required for methods that implicitly invert the Hessian. The cost of computing the Hessian in deep learning can be reduced by only computing Hessian-vector products. This approach is similar to classic results but written in terms of common automatic differentiation operations. The Hessian-free method is the most memory-efficient method for finding a step by minimizing a specific equation. The Hessian-free method in deep learning reduces the cost by computing Hessian-vector products. Algorithm 2 illustrates the process, where conjugate-gradient steps are used to find a search direction. Changes proposed aim to eliminate the costly inner loop by reusing the previous search direction and reducing inner loop iterations to just one. The Hessian-free method in deep learning aims to reduce costs by computing Hessian-vector products. Algorithm 2 uses conjugate-gradient steps to find a search direction. However, changes were made to Algorithm 1 to interleave updates of the search direction and parameters, replacing CG with gradient descent to improve stability. This new algorithm is equivalent to momentum GD, with the addition of an extra curvature term. The algorithm CURVEBALL adds curvature to momentum gradient descent, known as the heavy-ball method. It includes a regularization term and minimizes automatic differentiation steps for efficiency. The proposed method introduces hyper-parameters for tuning, but the optimal values can be obtained through a linear system. The elements of the matrix can be computed with one additional forward pass, allowing for automatic hyper-parameter rescaling. The regularization term \u03bbI can be interpreted as a trust-region. A small \u03bb corresponds to an unregularized Hessian and a large trust-region, while a large \u03bb indicates a poor fit. The difference between predicted and real objective changes can be measured by computing \u03b3. This estimate of the trust region is evaluated every 5 iterations, adjusting \u03bb accordingly. Our algorithm is not very sensitive to the initial \u03bb, which is initialized to one in batch-normalization experiments. The method is a variant of the heavy-ball method, with proofs presented for more tractable cases due to the recursive nature of the algorithm. Global convergence rates have been challenging to establish, especially for non-convex problems like deep neural networks. The method presented in this study demonstrates global linear convergence for convex quadratic functions and ensures descent direction for convex non-quadratic functions. The effectiveness of the method is supported by empirical evidence from extensive experiments on large-scale problems without hyper-parameter tuning. The study presents an optimizer for large-scale deep learning architecture, starting with simpler problems to evaluate its strengths and weaknesses. A comparison is made with popular solvers like SGD with momentum and Adam, as well as traditional methods like Levenberg-Marquardt and Newton's method. The first problem tackled is finding the minimum of the two-dimensional Rosenbrock test function, allowing visualization of each optimizer's trajectories. The study evaluates optimizers for deep learning on deterministic and stochastic problems, including fitting a deep network with ill-conditioned matrix A. Results show poor performance of first-order methods in all cases. Hyper-parameters for SGD and Adam are determined through grid-search. The study evaluates optimizers for deep learning, finding that first-order methods perform poorly with high variance. Newton method and Levenberg-Marquardt show better performance but are impractical for larger problems. BFGS performance correlates negatively with noise levels. Second-order methods like CURVEBALL, Newton, and LM converge quickly. The study evaluates optimizers for deep learning, finding that first-order methods perform poorly with high variance. Newton method and Levenberg-Marquardt show better performance but are impractical for larger problems. BFGS performance correlates negatively with noise levels. Second-order methods like CURVEBALL, Newton, and LM converge quickly. Methods are typically not used in such scenarios due to the large number of parameters and stochastic sampling. A basic 5-layer CNN is trained on CIFAR-10 with and without batch-normalization. A larger ResNet-18 model is also trained. SGD and Adam are used as baselines, with CURVEBALL outperforming competitors in each setting. In each setting, CURVEBALL outperforms competitors in a robust manner, even with normalization and different model types. The method shows compelling performance on the ImageNet dataset, with results on both medium and large-scale settings using VGG-f architecture. The margin of improvement increases with the dataset scale. When compared to other second-order methods on MNIST, our method shows promising results. Our method performs comparably to first-order solvers on the MNIST dataset, while KFAC shows slower progress until stabilizing its Fisher matrix estimation. Standard architectures may be biased towards favoring SGD, as shown in random architecture results. Further assessment is needed to test the optimizers' ability to generalize across architectures. The study compares optimizers on 50 randomly generated deep CNN architectures to assess their generalization ability. CURVEBALL outperforms first-order methods on CIFAR10, with errors consistently lower than SGD and Adam. The method is competitive in efficiency with first-order solvers, as shown in wall-clock time comparisons. Our method is competitive with first-order solvers in efficiency and does not require tuning. The prototype includes custom FMAD operations that could benefit from further optimization. We also experimented with a Hessian-free optimizer but found it to be slower due to costly operations. Validation errors of trained models are compared, showing our method's effectiveness. The validation errors of trained models are reported, showing that models trained with the proposed method generally perform better, except for ResNet due to overfitting. Second-order methods are effective for optimizing deterministic functions, but their application to stochastic optimization, especially in deep neural networks, is still an active research area. Various methods have been developed to improve stochastic optimization with curvature information while avoiding the high cost of storing and inverting a Hessian matrix. Various methods have been developed to improve stochastic optimization with curvature information while avoiding the high cost of storing and inverting a Hessian matrix. Popular approaches include constructing updates from parameter gradients and their first-and-second-order moments at previous iterates, such as AdaGrad, AdaDelta, RMSProp, or Adam. These solvers set adaptive learning rates using empirical estimates of curvature with diagonal or rescaled diagonal Gauss-Newton approximations. While these methods decrease computational cost, their overall efficiency can often be matched by a well-tuned SGD solver. Trust-region methods and cubic regularization are second-order solvers that invest more computation per iteration for higher quality updates. Trust-region methods BID6 and cubic regularization BID25 BID4 are examples of second-order solvers that aim for higher quality updates by inverting the Hessian matrix or using approximations like the Gauss-Newton method. Another approach in the trust-region family involves introducing second-order information with natural gradients, derived from a loss function based on Kullback-Leibler divergence. The natural gradient method replaces the Hessian with the Fisher matrix to follow the curvature in the Riemannian manifold. The Fisher matrix is used to optimize the path in the metric space induced by KL-divergence. Various methods like TONGA BID31, BID27, Martens, and KFAC optimizer have been introduced to improve efficiency in solving deep network problems. These methods often involve system inversion, which can be computationally expensive. Several works have explored using cheaper computation of Hessian-vector products through automatic differentiation to perform system inversions with conjugate gradients. Some methods have used rank-1 approximations of the Hessian for efficiency, but these have only been tested on smaller models compared to deep learning standards. These approaches require multiple steps per parameter update, putting them at a disadvantage compared to first-order methods. Orr (1995) utilizes automatic differentiation for computing Hessian-vector products to create adaptive learning rates per parameter. LiSSA BID0 approximates the Hessian inverse using a Taylor series expansion. The algorithm refines the Newton step estimate iteratively with a different update rule compared to our gradient-descent-based update. They reset the step estimate state for every minibatch, unlike our approach. Our algorithm aggregates the implicit Hessian across all past mini-batches, unlike BID0 who invert the Hessian independently for each mini-batch. This difference is crucial for deep neural networks due to the small batch sizes compared to the number of parameters. The Hessian matrix for a mini-batch is not a good substitute for the full dataset's Hessian and is severely ill-conditioned. Setting the parameter correctly can help achieve convergence on ill-conditioned problems. The gradient descent interpretation in our work provides an additional degree of freedom compared to BID0's Taylor series recursion. Our focus is on training deep networks on large datasets, where no previous Newton method has outperforms first-order methods commonly used in deep learning. In this work, a practical second-order solver tailored for deep-learning-scale stochastic optimization problems is proposed. The optimizer can reach better training error than first-order methods with no hyper-parameter tuning. Future work aims to improve the wall-clock time by optimizing the FMAD operation and studying optimal trust-region strategies. The Hessian matrix is diagonalized, and the optimal solution is obtained by minimizing the function. The text discusses the diagonalization of the Hessian matrix and the transition matrix R i that characterizes the iteration in a second-order solver for deep-learning-scale stochastic optimization problems. It mentions the use of eigendecomposition and vector equations to model multiple iterations in closed-form. The text discusses the diagonalization of the Hessian matrix and the transition matrix R i for second-order solvers in deep learning optimization. It models multiple iterations in closed-form using eigenvalues and convergence conditions. The convergence rate is determined by the eigenvalues of R i, with specific conditions for convergence illustrated graphically. The regions of convergence for different eigenvalues in deep learning optimization are graphically illustrated, showing a scaling factor along the \u03b2h i axis. The largest range of \u03b2h i values occurs when \u03c1 = 1, with 0 < \u03b2h i < 4 3. The update z t+1 in Algorithm 1 is a descent direction when \u03b2 and \u03c1 are chosen according to specific conditions, ensuring convergence. The update z t+1 = \u03c1z t \u2212 \u03b2\u2206 zt+1 satisfies z is positive definite (PD) by construction. Hyper-parameter evolution during training is shown in Figure 5, including average momentum \u03c1, learning rate \u03b2, and trust region \u03bb for each epoch of the basic CNN on CIFAR10, with and without batch normalisation. The scales are made comparable by plotting \u03bb divided by its initial value."
}