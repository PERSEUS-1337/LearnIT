{
    "title": "Skj8Kag0Z",
    "content": "Adversarial neural networks are challenging to train due to the fact that optimal weights correspond to saddle points, not minimizers. A modification of stochastic gradient descent stabilizes these networks, reliably converging to saddle points. This prevents collapse and allows for faster training with larger learning rates. Adversarial networks are crucial in various applications such as image generation and style transfer. Adversarial networks, such as style transfer and domain adaptation, have the ability to tackle various unsolved problems but are difficult to train due to the challenge of balancing two objectives simultaneously. This difficulty arises from the need to find a saddle point in the loss function, where weights are adjusted to maximize performance on one task while minimizing performance on another. Adversarial training methods search for saddle points in neural networks, introducing the risk of the training path sliding off objective functions and causing loss collapse. To address this, authors recommend early stopping, gradients/weight clipping, or specialized objective functions. A proposed prediction step aims to maintain stability in adversarial networks by being asymptotically stable for saddle point problems. Prediction enables faster training of adversarial networks using large learning rates without instability problems. Saddle-point optimization problems involve variables u and v, typically solved using alternating stochastic gradient method. Learning rate schedules {\u03b1 k } and {\u03b2 k } are used for minimization and maximization steps in this method. In practice, gradient updates are often performed by an automated solver like the Adam optimizer, including momentum updates. To stabilize training of adversarial networks, a prediction step is added where a prediction, \u016b k+1, is used to obtain v k+1 instead of calculating v k+1 using u k+1. The Prediction step estimates where u will be in the future assuming its trajectory remains the same. Common adversarial network problems and their saddle-point formulations are discussed, including Generative Adversarial Networks (GANs) fitting a generative model to a dataset through a competitive game. Generative Adversarial Networks (GANs) involve a generator competing against a discriminator to fit a model to a dataset. The generator maps random noise vectors to points in the data distribution, while the discriminator determines if a point is real or generated. Training involves adjusting network weights to solve a saddle point problem. Domain Adversarial Networks (DANs) are also discussed. Domain Adversarial Networks (DANs) extract feature representations from a \"source\" domain to train models for generalization to a \"target\" domain. In DANN, feature layers map data points to an embedded space for training a classifier, while an adversarial discriminator distinguishes between source and target domain data using only embedded features. The goal is to optimize a task-specific objective on the target domain while fooling the discriminator by solving an adversarial loss function. The text discusses stabilizing GANs by modifying the objective function, using regularization terms, training \"hacks,\" re-engineering network architectures, and designing different solvers. The Wasserstein GAN approach replaces the original objective with a new function and introduces weight clipping for the discriminator. However, WGAN training is unstable at high learning rates and with certain solvers. The unrolled GAN is a new solver that can stabilize training. The unrolled GAN is a new solver that stabilizes training by requiring more expensive gradient computations. It involves multiple extra discriminator updates for each generator update, leading to increased computation and memory usage. The method is related to the primal-dual hybrid gradient method, which achieves stability using a similar prediction step but is only applicable to bi-linear problems. Stochastic methods for convex saddle-point problems are categorized into stochastic coordinate descent and stochastic gradient descent. A \"doubly\" stochastic method was proposed for strongly convex bilinear saddle point problems. For general saddle point problems, \"doubly\" stochastic gradient descent methods are discussed. Prediction methods in stochastic methods for convex saddle-point problems help prevent the minimization step from overpowering the maximization step, ensuring stability in the algorithm. Three different perspectives are presented to explain the effect of prediction: intuitive, analytical involving dynamical systems, and a rigorous proof-based approach. The standard alternating SGD switches between minimization and maximization steps, with the risk of instability if one step dominates the other. Prediction methods in stochastic methods for convex saddle-point problems help balance the minimization and maximization steps, ensuring stability in the algorithm. By utilizing prediction, the maximization step can adjust based on the strength of the previous minimization step, leading to more effective optimization. When exact gradient updates are used, Algorithm (3) follows a simple dynamical system with closed-form solutions on a bi-linear saddle. The resulting iterations behave like a discretization of a system of differential equations, resembling a simple harmonic oscillator for small step sizes. The closed-form solution for u depends on the initialization parameters. The algorithm's behavior is influenced by the initialization, with small \u03b1 and \u03b2 values leading to non-convergence around a saddle point. The prediction step improves convergence by creating damped harmonic motion. Linearizing the problem results in a dynamical system with solutions that converge into the saddle point. A rigorous convergence analysis is provided, assuming convexity in u and concavity in v. The function L(u, v) is convex in u and concave in v, leading to convergence measured using the \"primal-dual\" gap. The SGD method with prediction converges in expectation with an error bound. Experiments show the benefits of the prediction step for adversarial nets, including a saddle point problem on a toy dataset and state-of-the-art models for GANs, domain adaptation, and fair classifiers. Additional results and experiments are available in the Appendix. The code can be found at https://github.com/jaiabhayk/stableGAN. The code for the task of classifying MNIST digits as even or odd, with noise corruption, is available at https://github.com/jaiabhayk/stableGAN. A LeNet network trained on this problem encodes noise information, achieving 100% accuracy in a noise vs no-noise classifier. An adversarial model is created to make LeNet ignore the noise, with a softmax loss for even vs odd classification and discriminating noisy samples. The classifier and discriminator are pre-trained using Caffe BID18's default LeNet implementation before being combined into an adversarial net. The combined adversarial net was trained with and without prediction, aiming to maintain good classification accuracy while preventing the discriminator from outperforming a simple strategy. Prediction had little impact at a low learning rate but significantly improved stability at higher rates, as shown in Figure 1. The default learning rate of the Adam solver was found to be unstable. The default learning rate of the Adam solver is unstable unless prediction is used. Our proposed predictive step is tested on generative adversarial networks (GAN) for image modeling tasks using CIFAR-10 on DCGAN architecture. Comparison is made with DCGAN and unrolled GAN using different training protocols. The Adam optimizer with learning rate=0.0002 and \u03b2 1 =0.5 is used in the solver for DCGAN. Training loss curves show that DCGAN and unrolled GAN collapse at higher learning rates, but remain stable when a predictive step is used. Images generated with prediction are more diverse. The training procedure for DCGAN and unrolled GAN collapsed on all five random seeds, with DCGAN collapsing more frequently. The quality of images generated using DCGAN is inferior to that of the predictive and unrolled methods. The predictive step method collapsed only once, adding trivial cost to the training algorithm. Training DCGAN takes 35 secs/epoch, with prediction it takes 38 secs/epoch, and unrolled GAN takes 139 secs/epoch. Inception scores show Stacked GAN trained with prediction generates good samples at different learning rates. The inception score on real images of CIFAR-10 dataset is 11.51 \u00b1 0.17. Domain adaptation task involves altering learned representations to generalize to target distribution. Comparisons on target domain accuracy are made on six source-target domain tasks in TAB1. In domain adaptation tasks, the prediction step shows benefits on easy adaptation tasks with similar data samples. However, for tasks with distinct data samples like AMAZON-to-WEBCAM, an extra prediction step improves target domain label prediction by 1.3 \u2212 6.9%. Fair feature representations are also considered to prevent discrimination based on sensitive variables. The task involves learning classifiers for sensitive classes using a minimax problem formulation. Model selection is done by maximizing the difference between accuracy and discrimination values. The \"Adult\" dataset is used to classify income levels based on gender. The advantage of using prediction for model selection is demonstrated using a restricted class of models. In the experiment, 100 models are randomly selected for training with an autoencoder and adversarial discriminator. A prediction step is introduced to improve model selection by evaluating on the validation set. The AFLR approach with prediction shows better generalization on the test set compared to without prediction. The modification to the alternating SGD method enhances the training process. The prediction step improves the stability of adversarial networks by preventing network collapse and enabling training with a wider range of learning rates. Theoretical results show that the prediction step is asymptotically stable for solving saddle point problems. The derivation of the harmonic oscillator behavior of Algorithm (3) on a simple bi-linear saddle is provided. Smooth weakly convex objective functions behave similarly within a small neighborhood of a saddle point. The Taylor approximation for stability on saddles is crucial for the convergence of the prediction method in solving saddle point problems. As the learning rate decreases, the prediction method converges while the non-prediction method does not. This analysis describes the behavior of the prediction method on smooth problems where convergence occurs. The prediction method converges on saddles and harmonic oscillators when the gradient method is applied to linear problems. The differential equations describe undamped harmonic motion, which can be decomposed into single-variable problems using eigenvalue decomposition. The solution involves cosine functions and diagonal matrices. The solution to the problem involves cosine functions and diagonal matrices. The predictive algorithm improves convergence by producing damped harmonic motion that sinks into the saddle point. The iterates of the predictive method satisfy a damped harmonic motion equation. The solutions involve damped harmonic motion, with iterates converging to a saddle point. The stochastic primal-dual gradients show O(1/ \u221a k) convergence rate, proven using Lemmas 1 and 2. The proof involves convexity and concavity properties of L(u, v) and the use of Lipschitz gradients. The primal-dual gap is analyzed, showing convergence to a saddle point with O(1/ \u221a k) rate. The primal-dual gap convergence to a saddle point with O(1/ \u221a k) rate is analyzed in the Lemmas. Experimental details involve evaluating a domain adaptation task on the OFFICE dataset consisting of images from three domains: AMAZON, DSLR, and WEBCAM. Adam solver outperforms SGD in performance and convergence, suggesting default hyper-parameters can be retained for training without further tuning. The text discusses the use of domain adaptation to improve cross-domain accuracy in image classification tasks. The protocol from a previous study is followed, using Caffe BID18 for implementation. Experimental details involve using the \"Adult\" dataset to classify whether a person earns \u2265 $50k/year based on census data. Gender is considered as the sensitive variable in the classification task. The study focuses on using gender as the sensitive variable in classification tasks. Data is split into training, validation, and testing sets. A toy dataset is used with a GAN architecture inspired by a previous study, sampling two-dimensional data from a mixture of eight Gaussians. The study utilizes a GAN with two fully connected hidden layers, each with 128 units and tanh activations. The final layers have different activations - linear for the generator and sigmoid for the discriminator. The adam solver is used with default parameters and a batch size of 512. Results show that the GAN is able to capture all modes of the dataset, including 100 Gaussian modes equally spaced around a circle. The study trained a GAN with two different input batch sizes, small (64) and large (6144), using various training algorithms. For the small batch size, the GAN failed to capture all modes of the dataset due to the limited number of input modes sampled at each iteration, leading to training instability. This highlights the challenge of training GANs on datasets with a high number of modes compared to the batch size. The advantage of prediction methods for generating higher resolution images of size 128 x 128 using the AC-GAN architecture is demonstrated. The Inception score measured during training shows that the prediction method remains stable and speeds up the training process even with a large number of classes."
}