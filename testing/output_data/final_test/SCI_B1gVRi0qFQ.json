{
    "title": "B1gVRi0qFQ",
    "content": "Abstraction of Markov Decision Processes simplifies learning optimal policies by ignoring unimportant details. A new algorithm based on MDP homomorphisms for abstract MDPs in continuous state spaces is proposed. The algorithm can learn abstractions from experience and guide exploration in new tasks. Task transfer method outperforms a deep Q-network baseline. Abstraction is effective for control in complex environments, offering a simpler policy compared to deep neural networks. In this paper, the focus is on state abstraction in Markov Decision Processes. The goal is to find the smallest MDP homomorphic to simplify learning optimal policies by ignoring unimportant details. Abstraction of the state space groups together states with similar behaviors, leading to simpler policies. Temporal abstraction complements state abstraction, creating temporally-extended actions for tasks that take hundreds or thousands of time steps to solve. The goal is to find the smallest MDP homomorphic to simplify learning optimal policies by ignoring unimportant details. This abstract MDP removes unnecessary distinctions between states, focusing on end-to-end manipulation tasks in robotics with high-dimensional state and action spaces. The approach leverages the theoretical foundations of BID5 and utilizes convolutional neural networks for learning from high-dimensional inputs. In this paper, a new algorithm is proposed for creating abstract MDPs from experience in both discrete and continuous state spaces. The algorithm explores the environment with either a random uniform policy or a deep Q-network, generates an abstract MDP homomorphic to the underlying MDP, and plans optimal actions in the abstract MDP. Additionally, a classifier based on a convolutional network is developed to sort state-action pairs based on their behavior. The paper proposes methods for speeding up learning and dealing with class imbalance, as well as guiding exploration in new tasks using abstract MDPs. It beats a baseline in some tasks and performs equally well in others. Related work includes MDP homomorphism algorithms for finding minimal MDPs from experience. The paper discusses the use of Controlled Markov Processes (CMP) for supervised learning, including methods for finding approximate homomorphisms and transferring optimal policies to similar tasks. The paper explores the use of Controlled Markov Processes (CMP) for supervised learning, focusing on finding approximate homomorphisms and transferring optimal policies to similar tasks. An MDP is defined as a tuple S, A, \u03a6, P, R, where S is the set of states, A is the set of actions, \u03a6 is the state-action space, P is the transition function, and R is the reward function. The goal is to identify minimal MDPs that maintain the structure of the original MDP, with MDP homomorphisms capturing this concept. The optimal state-action value function in the minimized MDP is still optimal in the underlying MDP according to Theorem 1. During MDP minimization, the partition of \u03a6 induces the abstract MDP, and the state-action partition leads to the quotient MDP. The induced quotient MDP is an abstract MDP that ignores unimportant distinctions from the underlying MDP. The framework of options is used for transferring learned homomorphisms between similar tasks. The algorithm abstracts MDP with sparse reward functions and deterministic transition and reward functions. The algorithm proposed in Subsections 5.1 to 5.5 focuses on partitioning a continuous state space, adapting the Partition Iteration algorithm for finding minimal MDPs homomorphic to the underlying MDP. The approach involves modifying the base algorithm for speed and robustness, guiding exploration with an abstract model, and separating transitions based on positive rewards (SplitRewards). The algorithm focuses on partitioning a continuous state space by refining a reward-respecting partition with the Split operation. A classifier is trained to predict state-action blocks, and during state projection, it evaluates a state under a set of actions to predict possible state-action blocks. The algorithm aims to partition a continuous state space by refining a reward-respecting partition with the Split operation. Modifications to the classifier increase speed and robustness, including weight reuse for neural networks. This helps avoid retraining the classifier from scratch at every step, saving time and reducing errors during state projection. The algorithm partitions a continuous state space by refining a reward-respecting partition with the Split operation. The neural network is trained from scratch, with weights retained for subsequent re-trainings. Early stopping of training is implemented to select the best-performing snapshot. Class balancing is done by oversampling minority classes to equalize sample sizes. The algorithm partitions a continuous state space by refining a reward-respecting partition with the Split operation. Decision trees do not require oversampling, so they are used only with a convolutional network. State-action block size threshold is set to prevent over-segmentation due to misclassifications. The state-action partition provides enough information for the agent to act in the environment without evaluating abstract states. A directed graph of state-action blocks is used to decide the next action. The algorithm partitions a continuous state space using a reward-respecting partition with the Split operation. A directed graph of state-action blocks is constructed to determine the next action. The abstract graph connects state-action blocks, with the goal node marked as the one leading to the goal set of states. A breadth-first search is used to find the shortest path to the goal from each node. The agent selects the best state-action block based on the shortest path to the goal. Multiple actions may be available in the current state, and the agent chooses the state-action block with the shortest path to the goal. The algorithm selects the state-action block with the shortest path to the goal, sampling actions based on classifier confidence. Initial partitioning requires diverse experience. Random exploration may work for simple environments, but not for complex tasks. The algorithm uses a deep Q-network to address the issue of random policy in tasks. It leverages prior knowledge from previous tasks to guide exploration and start solving new tasks efficiently. Our algorithm creates options to guide the agent to specific state blocks in the abstract MDP, using q-values updated with a fixed learning rate. It aims to reach the goal of new tasks efficiently without prior information. Our algorithm creates options for the agent in continuous state space environments. A version for discrete environments was also developed for comparison. The algorithm's performance is tested on a single task and compared with prior work. Task transfer experiments are conducted as well. Neural networks were implemented in Tensorflow 1. Experiments were repeated 20 times and results were averaged. In the continuous task, the agent stacks pucks in a grid world environment with end-to-end actions. The source code is available online. The agent in the continuous task stacks pucks in a grid world environment using a depth image. The task is episodic, with the goal of stacking a target number of pucks. The agent is rewarded upon reaching the goal and uses a convolutional network for classification. Experience is collected using a random uniform policy or a deep Q-network. The deep Q-network settings are listed in Appendix B, with a limit of 10 state-action blocks to speed up experiments. The replay buffers are limited to 10000 transitions for both the partitioning algorithm and DQN due to memory constraints. The blocks world environment consists of three blocks in four positions, with tasks of increasing difficulty. The agent is penalized for actions not leading to the goal and rewarded for reaching the goal state. The decision tree from the scikit-learn package is used as the classifier, training faster and often performing better than a neural network. Modifications specific to a neural network are omitted. Despite high accuracy, the number of state-action blocks is capped at 100 to avoid creating too many pairs. The abstract MDP is recreated every 3000 time steps, and the task ends after 15000 time steps. The algorithm can find the minimal MDP homomorphic to the problem and act optimally. The agent is taught to solve the task of stacking 3 pucks in a 3x3 grid world environment using a random uniform policy. The algorithm's cumulative reward over 2000 episodes with different state-action block size thresholds is shown in FIG2. Setting the threshold too high leads to under-segmentation, while setting it too low results in misclassification and performance drops. The algorithm aims to find the minimal MDP homomorphic to the problem and act optimally. The algorithm aims to find the minimal MDP with 6 state-action blocks. A comparison with the decision tree version is made, highlighting differences in supervision and task transfer. Wolfe's version includes a 0.2 chance of action failure, which is omitted in our approach. Our algorithm tests on a sequence of two tasks in a grid world environment. The tasks involve stacking pucks, with the second task requiring the agent to stack three pucks. Comparisons are made with a vanilla deep Q-network and two baselines. The algorithm aims to find the minimal MDP with 6 state-action blocks. Our algorithm tests on a sequence of two tasks in a grid world environment involving stacking pucks. Comparisons are made with a vanilla deep Q-network and two baselines. The agent augmented with the goal option reaches a similar cumulative reward to the baseline with weight sharing. The experiment involved a sequence of tasks in a grid world environment with stacking pucks. The agent, augmented with options for reaching all state blocks of the abstract MDP, learns to select the correct option for the second task, achieving a significantly higher cumulative reward than the baselines. The baseline with weight sharing unexpectedly performs better, even when the first task's goal is not as beneficial. The deep Q-network can learn the second policy easier due to pre-trained convolutional layers. An algorithm for finding abstract MDPs in continuous state spaces was developed and demonstrated to work with medium-sized abstract MDPs. A method for guiding exploration with an abstract MDP learned in a previous task based on the options framework was devised. The transfer method outperforms a deep Q-network baseline in robotic manipulation tasks. Future work aims to address deterministic MDPs with failure modes in robotic manipulation tasks. In future work, the algorithm aims to handle non-determinism in the MDP and prevent over-segmentation by merging state-action blocks when necessary."
}