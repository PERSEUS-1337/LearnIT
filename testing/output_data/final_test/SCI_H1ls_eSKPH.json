{
    "title": "H1ls_eSKPH",
    "content": "Learning neural networks with gradient descent over a long sequence of tasks can lead to catastrophic forgetting, where fine-tuning for new tasks overrides important network weights for previous tasks. Approaches like task rehearsal and growing networks limit scalability, while regularization based on the Fisher information matrix penalizes changes to parameters relevant to old tasks. Introducing Hessian-free curvature estimates as an alternative method avoids the need to calculate the Hessian, taking advantage of flat regions in the loss surface. Our experiments show that we outperform previous work by exploiting flat regions in the loss surface to calculate a Hessian-vector-product relevant to the current task. Prior knowledge serves as an inductive bias, limiting the hypothesis space and improving generalization in machine learning. Limiting the hypothesis space and introducing bias can improve generalization in machine learning. This can be achieved by restricting the functions expressible by the learning algorithm or by adding regularization terms. Multitask learning, training on different tasks simultaneously, can also enhance generalization and efficiency. Multitask learning improves generalization and reduces sample requirements, but scalability is limited. Learning tasks sequentially is not feasible due to data storage limitations. Catastrophic forgetting is a common issue in neural networks when optimizing over multiple datasets sequentially. Retaining important parameters for previous tasks is crucial for maximizing generalization and sample-efficiency in multitask learning and continual learning frameworks. The scalability of learning in neural networks can be enhanced by preserving important parameters for previous tasks. The Bayesian framework and Elastic Weight Consolidation (EWC) are promising approaches to address catastrophic forgetting. EWC approximates the prior with a Gaussian centered around optimized network parameters for previous tasks, reducing catastrophic forgetting efficiently. In contrast to EWC, Ritter et al. (2018) propose a block-diagonal approximation for the prior from older tasks by defining a quadratic approximation that requires calculating the Hessian. Our work suggests an alternative method for calculating the Hessian using Hessian-free methods to estimate curvature information of network parameters. Our method utilizes a subset of the Hessian to incorporate the importance of individual weights and dependencies between network parameters when training over multiple tasks. Results show outperformance of EWC and comparable performance to Ritter et al. (2018) on disjoint tasks, with lower space complexity. In the continual learning framework, neural network parameters are optimized over multiple datasets, with the goal of achieving high accuracy on the current task while preserving performance on previously visited tasks. The paper discusses background on continual learning, EWC, and Kronecker-factored Laplace approximation, presents a new method, demonstrates its efficiency, and compares it against state-of-the-art approaches. Related work is also discussed before concluding. To prevent catastrophic forgetting in neural networks during continual learning, it is crucial to retain important parameters for previous tasks while allowing the network to learn new tasks without increasing the network's complexity. This ensures high performance on all tasks without the need for additional parameters or regularization terms. EWC (Kirkpatrick et al., 2017) is an efficient approach to prevent catastrophic forgetting in neural networks during continual learning. It adds a penalty to important parameters for previous tasks to retain them while allowing the network to learn new tasks without increasing complexity. This approach fosters parameter sharing and positive transfer effects, improving sample efficiency by reusing past experience. To address the challenges of maintaining the full posterior over all previous datasets in continual learning, EWC approximates the posterior with a Gaussian. This approach involves storing previous weights and a diagonal approximation of the FIM for the previous task, allowing for efficient learning of new tasks while retaining important parameters for previous tasks. In addressing the challenges of continual learning, EWC approximates the posterior with a Gaussian by storing previous weights and a diagonal approximation of the FIM for the previous task. Ritter et al. (2018) improve upon this by adopting a Bayesian online learning approach to find a MAP-estimate \u03b8 * sequentially. This involves introducing a parametric approximate posterior and updating it in two steps. The posterior update involves two steps: an update step using the Bayesian rule and a projection step where the new posterior is projected onto the same parametric family. The core improvement is in the projection step, where \u03a3 \u22121 is projected to \u03a3 \u22121 t+1 to maintain information about all tasks up to task t + 1. In the context of updating posterior information for multiple tasks, et al. (2018) propose a method that computes the Hessian around the most recent solution and combines it with Hessians from previous solutions to preserve information about previous tasks. This approach requires storing a set of parameters per task, unlike previous methods that identify important parameters for each task. EWC uses the diagonal of the FIM, while Ritter et al. (2018) use a block-diagonal Kronecker-factored approximation of the Hessian. The authors build upon the metalearning intuition, particularly from the MAML algorithm, to address the same problem differently. The method identifies model parameters that lead to faster learning for all tasks in a task distribution by penalizing large changes to them. It uses a meta-learning objective and the Hessian matrix to find important network parameters that affect the loss significantly. The method utilizes the Hessian matrix to identify crucial network parameters affecting the loss significantly. By exploiting flat regions in the loss surface, a subset of the Hessian containing relevant information is used to estimate directions with high curvature, crucial for determining important weights. This approach provides a good approximation of the Hessian while preserving enough curvature information for larger networks. Instead of computing the full Hessian, a Hessian-vector-product is calculated efficiently by sampling the curvature in a given direction. This method, initially presented in Pearlmutter (1994), avoids explicit calculation of the Hessian and computes the product directly using finite differences. This approach has been used in Hessian-free optimization methods, such as truncated-Newton, to estimate directions with high curvature for important weights. The Hessian-vector-product is calculated efficiently by sampling curvature in a given direction, inspired by Stochastic Meta-Descent. Momentum is used to select the vector v, which holds information about parameters changed the most during training. The eigenvector corresponding to the largest eigenvalue is an alternative to momentum in optimizing important parameters for a task. It represents the direction of highest curvature and includes critical information for the task. The power method is used to compute this eigenvector, resulting in a vector that maintains second-order interactions. The curvature estimate is calculated using the Hessian-vector-product, constructing a positive semidefinite matrix with the absolute values of the vector as entries. The curvature estimate for task t is calculated using the Hessian-vector-product, with a low storage requirement compared to other approaches. A hyperparameter \u03bb controls the regularization term's influence on the loss function. The approach only needs to store two vectors independently of the task sequence size. In experiments, Hessian-free curvature estimations are compared to EWC and Kronecker-factored approximation methods using the permutedMNIST dataset. Source code will be released upon publication. Hyperparameter search includes network structure with 1 or 2 layers and 200 hidden units. In experiments, different methods are compared using the permutedMNIST dataset. Hyperparameter search includes network structure options and various values for \u03bb. The best hyperparameters lead to improved accuracy for Kronecker-factor approximation compared to other methods like EWC. Farquhar & Gal (2019) suggest using a specific version of disjointMNIST for evaluating continual learning approaches. Farquhar & Gal (2019) recommend using a specific version of disjointMNIST for evaluating continual learning approaches. In this experiment, MNIST is split into two tasks: letters '0' to '4' and '5' to '9'. A ten-way classifier is used, making the problem more challenging than before. Training on the second split can overwrite the parameters of the ten-way classifiers for the classes of the first split. The network architecture consists of 2 layers with 100 hidden units each, a batch size of 250, and 10 epochs. The same Adam parameters as in the PermutedMNIST experiment are used for comparison. In a hyperparameter search over \u03bb, our approach using momentum outperforms EWC with 91.01% accuracy vs 86.11%. The results show the balance between retaining information on old tasks and learning accuracy on new tasks. The different scales in \u03bb between our results and Ritter et al. (2018) are due to implementation details. Our approach using momentum outperforms EWC with 91.01% accuracy, comparable to the Kronecker-factored approximation. The Single-Headed-Split-MNIST task involves splitting digits into five groups, each with two classes. The classifier shares a head for all tasks, making predictions for all possible outputs. Regularization methods like Elastic Weight Consolidation (Kirkpatrick et al., 2017) are commonly used in the field of catastrophic forgetting. Our Hessian-free curvature estimations consistently outperform EWC, with the momentum-based variant achieving 57.54% accuracy and the eigenvector approach reaching 55.36%. The results show that our approach using momentum is close to the Kronecker-factored approximation. The approach proposed by Ritter et al. (2018) applies a Kronecker-factored approximation of the Hessian to improve over Elastic Weight Consolidation (EWC) in addressing catastrophic forgetting. This method aims to capture interactions between parameters for better performance. The approach proposed by Ritter et al. (2018) uses a Kronecker factorization to improve over Elastic Weight Consolidation (EWC) by capturing second-order parameter interactions. In contrast, Ghorbani et al. (2019) exploit flat regions in the loss surface to use a small subset of the Hessian for relevant information, sampling from it using a Hessian-vector-product. Rehearsal methods aim to reduce catastrophic forgetting by replaying examples from previous tasks and learning the distribution of training data. Variational Autoencoders are used to add artificial samples from the learned distribution when a new task is learned. Recent approaches in addressing catastrophic forgetting involve using generative adversarial networks and episodic memory to preserve subsets of training data for each task. However, these methods face scalability issues as the number of tasks increases, requiring replaying of samples or storing a large dataset. Progressive Neural Networks incrementally increase architecture capacity by adding a new network for each task connected to old ones to allow for transfer. Other approaches add capacity only when necessary, like Self-Organizing Map based on a similarity metric. This paper addresses catastrophic forgetting in a continual learning framework by using a similarity metric to determine when to add new nodes to the network. Unlike other methods, it dynamically adds capacity based on the error signal and features learned, but has stringent architectural constraints. This paper introduces a novel approach to address catastrophic forgetting in continual learning by utilizing second-order parameter dependencies with constant space complexity. By sampling from a small subset of the Hessian using a Hessian-vector-product, the network can incorporate the importance of individual weights and dependencies between parameters when training over a long task sequence. Our algorithm addresses catastrophic forgetting in continual learning by utilizing second-order parameter dependencies with constant space complexity. It outperforms EWC and is comparable to Kronecker-factor approximation while requiring less memory."
}