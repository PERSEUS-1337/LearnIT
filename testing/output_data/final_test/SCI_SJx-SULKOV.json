{
    "title": "SJx-SULKOV",
    "content": "Recent developments in generating images from text descriptions have focused on static descriptions, lacking the ability to generate images interactively based on incremental text input. Our proposed method generates images incrementally based on scene-graphs using a recurrent network architecture with Graph Convolutional Networks and Generative Adversarial image translation networks. The model outperforms other approaches in generating visually consistent images for growing scene graphs on the Coco-Stuff dataset. Generative Adversarial Networks (GANs) are effective in generating real world images by training a generator to produce samples that fool a discriminator. The next frontier is creating customized images and videos based on user preferences. Recent advancements include generating images from natural language descriptions using conditional-GANs. This has broad implications in Robotics, AI, Design, and image retrieval. Conditional-GANs can generate images related to text descriptions, but struggle with complex sentences. Scene graphs are used to represent complex sentences more explicitly, improving image generation and captioning. Leveraging scene graphs can help generate more complex structures and corresponding images. Our approach overcomes limitations in generating images incrementally by conditioning the process over cumulative image steps and unseen parts of the scene graph. Unlike previous methods, our approach can generate high-quality real-world scenes with multiple objects without losing context. Our framework allows for interactive modification of images generated from structured scene graphs, preserving context and contents over previous steps. It does not require intermediate supervision, making it suitable for generating real-world images like MS-COCO. Since the advent of Generative Adversarial Networks (GANs), there have been many efforts to generate images using frameworks like LSTM and conditional GAN. Previous works focused on single objects like faces or flowers, but generating complex scenes with multiple objects and specific relationships remains a challenging research problem. BID15 proposed an architecture with multiple GANs stacked together to address this issue. BID15 proposed a multi-GAN architecture for generating images in a coarse-to-fine manner, later arranging generators in a tree-like structure for better results. BID4 introduced an end-to-end pipeline for inferring scene structure and generating images from text descriptions. AttnGANs start with low-resolution images and improve them incrementally, lacking consistency mechanisms during image generation. BID7 proposed using scene-graphs for image synthesis, processed with a graph convolution network to predict objects' bounding boxes and segmentation masks. A cascaded refinement network generates output images, trained adversarially for realism but lacks object saliency and temporal consistency. The previous work used scene-graphs for image synthesis but lacked object saliency and temporal consistency. Another recent approach by El-Nouby et al. (2018) introduced a conditional text-to-image generation method that iteratively generates images based on ongoing context and history. They use GRU to process text instructions, conditioning augmentation, and GAN for image generation. The method filters out latent embeddings of already generated objects for better textural consistency. However, it requires supervision at every generation stage for training. Our approach utilizes perceptual similarity based regularization and graph-based embeddings to generate images from scene graphs without the need for ground truth at intermediate steps. We employ a generative-adversarial approach where the network penalizes based on image realism and object presence. Preserving object relations specified in the scene graph is crucial. The architecture for generating images from scene graphs includes a Graph Convolution Network (GCN), Layout Prediction Network (LN), and Cascade Refinement Network (CRN). GCN operates on graphs, computing new vectors for nodes and edges. LN predicts layouts based on GCN outputs. Refer to the original paper for detailed descriptions. The Layout Prediction Network uses object embeddings from the Graph Convolution Network to predict scene layouts with segmentation masks and bounding boxes. The Cascade Refinement Network then generates images based on the scene layout using convolutional refinement modules. The Scene Layout Network processes object embeddings from the scene graph to predict bounding boxes and segmentation masks. This information is then used by the Cascaded Refinement Network to generate the final image in a coarse-to-fine manner. The layout is downsampled and passed through convolution layers before being upsampled and iteratively refined to produce the output image. This method allows for context preservation during image generation. Our method extends BID7 with a recurrent architecture that generates images using incrementally growing scene graphs. To preserve visual context, we replace noise channels with RGB channels from the previous image. The SLN generates layouts for newly added objects by removing representations from previous steps. The method extends BID7 with a recurrent architecture for generating images using incrementally growing scene graphs. Perceptual loss is used for intermediate images to enforce similarity to the final image. Various losses such as box loss, mask loss, and L1 pixel loss are employed to ensure realism and accuracy in the generated images. The method introduces perceptual similarity loss to ensure contextual similarity between images generated at different steps. This additional loss helps the model 'hallucinate' intermediate steps with content similar to the ground truth. Additionally, supervision is provided on the predicted bounding box coordinates to preserve relations. The 2017 COCO-Stuff dataset BID1 augments the COCO dataset BID8 with additional stuff categories. It annotates images with bounding boxes and segmentation masks for 'thing' and 'stuff' categories. Synthetic scene graphs are constructed based on object coordinates, with six geometric relationships. Three splits are created based on the number of objects in each image, with incremental object addition. Incremental generation is trained for three steps, with the potential for extension. To enable comparison against BID7, the incremental generation process is trained for three steps, with the possibility of extension. Real datasets like COCO present challenges as there are no \"ground-truth\" images for interim scene graphs. The loss formulation is crucial for generating realistic images not limited to synthetic datasets. The dataset preprocessing steps follow those of BID7, which excludes objects covering less than 2% of the image. Our model improves on existing methods by incrementally adding objects according to the scene graph and generating high-quality images. Performance comparison with baselines BID7 and BID14 is shown in FIG0. The study compares the performance of the model with baselines BID7 and BID14 by incrementally adding objects based on the scene graph to generate high-quality images. The first baseline captures semantic context but lacks consistency over multiple passes, generating new images for each scene graph. The method involves creating splits for each image based on the number of objects, generating synthetic scene graphs for incremental image generation. Our model incrementally adds new objects to images based on the scene graph, improving image quality by avoiding cluttered scenes. It generates objects as described by the scene graph and outputs noise in the background when necessary. The model generates objects based on the scene graph and adds background details like grass and sky. However, it can also hallucinate background objects due to dataset biases. Inception Score is used to evaluate image quality. The model generates clear objects and diverse images from different classes in ImageNet. Inception Scores are compared between images generated by the baseline model sg2im and the sequential generation model over three steps. The sequential model outperforms due to modified loss formulation and incremental generation. Our model outperforms in Inception Scores over three steps due to modified loss formulation and incremental generation. It successfully retains context and previously generated content in subsequent steps, as shown in Table 2 and qualitatively in FIG0. Our approach demonstrates the ability to sequentially generate images using growing scene graphs with context preservation. In future work, the approach aims to generate image sequences that maintain context and object consistency over time. Plans include exploring end-to-end generation with text descriptions and improving generation quality by using attention on GCN embeddings during the process. This could simplify the task of modifying specific regions in the image. In future work, the approach aims to generate image sequences that maintain context and object consistency over time. Plans include exploring better architectures for image generation through layouts for higher resolution image generation, potentially making the task of modifying specific regions in the image easier."
}