{
    "title": "SJgzLkBKPB",
    "content": "Our approach aims to generate more focused saliency maps by balancing specificity and relevance, highlighting features of the input state that are most relevant for the agent in taking an action. This is achieved by considering the impact of perturbation on the expected reward of the action to be explained and downweighting irrelevant features that alter the expected rewards of other actions. The approach is compared with existing methods on agents trained in various games like Chess, Go, Breakout, Pong, and Space Invaders. Our approach aims to generate more focused saliency maps for RL-based agents playing board games and Atari games. Through illustrative examples and human studies, we show that our approach produces more interpretable saliency maps compared to existing methods. Interpreting the strategies learned by RL agents can help users understand the problem the agent is trained to solve before deploying them for real-world applications. Existing approaches in computer vision propose methods for interpreting reinforcement learning-based agents by generating saliency maps. These maps explain agent behavior by applying Gaussian blur to different parts of input images and using differences in value functions and policy vectors. Promising results are achieved on agents trained to play Atari games. Limitations include highlighting features that affect actions beyond the one being explained. In this work, a perturbation-based approach is proposed for generating saliency maps for black-box agents. Existing approaches highlight features that are not relevant to the action being explained, leading to limitations in interpreting reinforcement learning-based agents. The proposed method aims to address these limitations by focusing on perturbing the state to generate more accurate saliency maps. The proposed perturbation-based approach aims to generate saliency maps for black-box agents with a focus on specificity and relevance. It downweights irrelevant features and highlights only those relevant to the action being explained, resulting in a more accurate saliency map. This method addresses limitations in interpreting reinforcement learning-based agents by emphasizing perturbation of the state. The proposed approach generates accurate saliency maps for black-box agents by downweighting irrelevant features and highlighting only relevant ones. It improves interpretations for board games and Atari games compared to existing methods, aiding skilled players in solving puzzles more effectively. The importance of state features for an agent in taking action is determined by a saliency score denoted by S[f]. This score helps in identifying important features for the agent during the exploitation phase, leading to actions with the highest expected reward. The saliency of each feature f is denoted by S[f] and represents its importance for the agent when taking action. Perturbation based saliency approaches involve perturbing the state s, querying M to get Q(s', a) for all legal actions, and computing S[f] based on the differences in Q(s, a). The saliency map is computed based on the differences in Q(s, a) for legal actions in states s and s'. The saliency of each feature f, denoted by S[f], should focus on the effect of perturbation specifically on the action being explained. The saliency map is computed based on the differences in Q(s, a) for legal actions in states s and s'. It should focus on the effect of perturbation specifically on the action being explained. Features irrelevant to the action should not be salient. Existing saliency approaches do not capture these action-specific effects. The saliency computation in Iyer et al. (2018) focuses on the change in Q-values for a specific action, but it does not consider the effects on other actions. To address this, we use softmax over Q-values to compute the difference in relative expected rewards between the original and perturbed states (\u2206p). This helps identify features important for the specific choice of action by the agent. Identifying Relevant Changes: Apart from focusing on the change in Q-values for a specific action, we also compute the relative returns of all other actions using softmax normalization. The KL-Divergence is used to measure the difference in relative expected rewards between states, indicating the spread of effects over other actions. The saliency of a feature is determined by combining the change in expected rewards and the distribution similarity using KL-Divergence. The saliency is low when the perturbation affects many actions uniformly, reflecting the property of specificity. The saliency measure is defined as the harmonic mean of the change in expected rewards and distribution similarity. The approach discussed focuses on producing meaningful saliency maps by considering the specificity and relevance of perturbations. Human studies on chess puzzles were conducted to validate the usefulness of the generated saliency maps. Additionally, a Chess saliency dataset was introduced to compare different perturbation-based approaches, showing the superiority of the proposed method in identifying relevant chess pieces. In Section 3.4, the approach demonstrates understanding tactical ideas in chess by interpreting a trained agent's actions. The method works for black-box agents, using various agents like Stockfish for chess and MiniGo RL for Go. Saliency maps are generated using different agents and methods, with code and detailed results available on Github. Examples of saliency maps highlight qualitative differences in the approach. Our action-focused approach generates more meaningful saliency maps compared to existing methods, as demonstrated in chess with Stockfish. Existing approaches highlight irrelevant pieces, while our approach only highlights pieces relevant to the move. This is shown through comparisons on Atari games like Breakout, Pong, and Space Invaders. In comparing approaches on Atari games like Breakout, Pong, and Space Invaders, our method highlights relevant regions of the input image more precisely. In contrast, the other approach highlights irrelevant regions. Additionally, in a Go game scenario, black strategically moves to capture white stones threatening its own, demonstrating a tactical gameplay strategy. Our approach generates saliency maps that highlight key pieces on the chess board, providing useful information for problem-solving in chess puzzles. Human studies with chess players show the effectiveness of our method in highlighting important structures and pieces for strategic gameplay. Our approach generates saliency maps that are more helpful for humans when solving chess puzzles compared to other approaches. The saliency maps highlight key pieces and structures, improving accuracy and speed in puzzle-solving. A Chess saliency dataset was introduced to automatically compare different perturbation-based approaches in generating saliency maps. The dataset consists of 100 chess puzzles with a single correct move marked by human experts. The experts identify important pieces for each move, and the results are compared to existing approaches. The saliency maps generated by the new approach outperform other methods in identifying relevant chess pieces. Our approach computes saliency maps and ROC curves using different components individually, comparing the harmonic mean to other combination methods. The saliency maps generated by our approach demonstrate common tactical motifs in chess more accurately than alternative approaches. By interpreting the moves played by the Stockfish agent, we can understand common tactical ideas in chess, as shown in Figure 7 with corresponding saliency maps. Stockfish plays strategic moves like Rook x d6 and Queen x h7, sacrificing pieces to set up checkmate opportunities. Saliency maps highlight key pieces and tactics in the game. The saliency map in Figure 7f shows key aspects of the position in a chess game. Black's queen and bishop threaten mate on g2, while the white queen defends it. After white recaptures the sacrificed rook with the pawn on c3, black can attack both the white rook and queen with bishop to b4. The white queen is \"overworked\" on d2, guarding both the g2-pawn and a5-rook. The robustness of the saliency maps is evaluated by making irrelevant changes to the positions in the chess saliency dataset. To evaluate the robustness of the saliency maps, perturbations were made to the chess positions in the dataset by removing specific pieces. The AUC values for the perturbed datasets were found to be 0.92, indicating that the saliency maps remained consistent even with non-salient pieces removed. This confirms the reliability of the saliency maps in understanding RL agents and tasks. Different approaches have been introduced to provide natural language explanations for RL agents, including analyzing execution traces and clustering states of behavior using t-SNE. These methods aim to explain policies without relying on hand-crafted state representations, making them suitable for games where agents learn from raw input. Zahavy et al. use Semi-Aggregated Markov Decision Processes to approximate black box RL policies. The use of Semi-Aggregated Markov Decision Processes (SAMDPs) to approximate black box RL policies is discussed. The importance of providing interpretable insights into agent behavior is highlighted, with a focus on visual interpretable explanations using saliency maps. Gradient-based methods are used to identify input features that are most salient to the trained DNN. Gradient-based methods like guided backpropagation, excitation backpropagation, DeepLIFT, GradCAM, GradCAM++, and Integrate gradients have been used to address the shortcomings of gradient magnitude heatmaps. However, these approaches still rely on the shape in the immediate neighborhood of a few points and use perturbations that lack physical meaning, making them vulnerable to adversarial attacks. They are not suitable for scenarios with black-box access to the agent and struggle with white-box access to the model. Perturbation-based methods are preferred for black-box agents, focusing on computing the importance of input features by removing or altering them in a domain-aware manner. Various perturbations have been explored to remove information without introducing new data, such as those by Fong & Vedaldi (2017) and Zeiler & Fergus (2014); Ribeiro et al. (2016). Existing perturbation-based approaches for RL focus on the complete Q (or V) and may not produce specific saliency maps for the action of interest. Our proposed approach measures the impact only on the selected action, resulting in more focused and useful saliency maps. Saliency maps visualize the dependence between input and output, providing situation-specific explanations for decisions. These local explanations are valuable for understanding, debugging, and building trust in machine learning systems. In developing trust with machine learning systems, explanations may not provide general insights or guarantee applicability to different scenarios. Limitations include the inability to highlight the importance of the absence of certain attributes in saliency maps. Future exploration aims to provide precise and actionable explanations for the RL setting. Future work will explore perturbation functions to calculate the importance of empty squares in games like Chess and Go. Strategies will be developed to consider the valid state space when computing saliency, addressing the issue of exploring states outside the manifold. Additionally, the saliency of each feature will be estimated independently, with a focus on ignoring feature dependencies. In future work, strategies will be developed to calculate the importance of empty squares in Chess and Go, considering the valid state space and exploring feature dependencies. The approach presented generates focused saliency maps for black-box policy-based agents, showing more interpretability for humans than existing methods. The study provides insights into a black-box RL agent's behavior using pre-trained MiniGo and Atari agents. Saliency maps are generated using different algorithms, and code is available on Github for further details. In chess and Go experiments, board positions are perturbed by removing one piece at a time. The saliency maps for Atari agents are generated using a perturbation technique that adds a Gaussian blur to the input image. Chess puzzles are taken from Lichess and Go puzzles from OnlineGo for illustrative examples. Saliency maps highlight different pieces for each move in chess positions. The saliency maps in Chess highlight key pieces for each move, showing the importance of certain pieces like the pawn on g7 for move Qd4. Results from the LeelaZero Deep RL agent demonstrate the meaningful generation of saliency maps for RL agents in Chess."
}