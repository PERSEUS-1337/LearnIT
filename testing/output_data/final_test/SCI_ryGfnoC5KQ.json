{
    "title": "ryGfnoC5KQ",
    "content": "Kernel RNN Learning (KeRNL) is a reduced-rank approximation to backpropagation through time (BPTT) for training recurrent neural networks (RNNs). It replaces a rank-4 gradient learning tensor with a simple reduced-rank product of sensitivity weight and temporal eligibility trace. This structured approximation, inspired by node perturbation, allows for learning sensitivity weights and eligibility kernel time scales through perturbations. It aims for biologically plausible machine learning with lower complexity and memory demand, and faster feedback time. The current standard for training recurrent architectures is Backpropagation Through Time (BPTT), which assigns temporal credit by unfolding a recurrent neural network in time and backpropagating the error. However, from a biological perspective, BPTT-like backpropagation in feedforward neural networks is implausible for many reasons. BPTT-like backpropagation in feedforward neural networks is implausible due to the heavy memory demand and time complexity involved in transmitting errors backwards in time and assigning credit for past activity affecting present performance. This method requires two-way synapses or a symmetric copy of feedforward weights, leading to alternating gating of the network's dynamical process and switching between nonlinear and linear dynamics. The complete network states going T timesteps back in time must be stored, making each iteration slow when training tasks with long time scale dependencies. KeRNL addresses the inefficiency and lack of biological plausibility in backpropagation for recurrent networks by using a forward-flowing temporal eligibility trace and spatial sensitivity weight instead of a lengthy backward-flowing backpropagation phase. This approach reduces the need to store all past states in memory and avoids recomputing the entire gradient at each update. In recent years, efforts have been made to implement backpropagation algorithms in a more biologically plausible way, aiming for simpler implementations. The symmetry requirement between forwards and backwards weights can be addressed using random return weights, but learning still involves a separate backward pass through a network with linearized dynamics. Some research has explored using STDP-like learning rules for neurons to extract error information, with error backpropagation as a relaxation to equilibrium for learning fixed points. Other work has looked into replacing batch learning with online learning, with BPTT typically implemented in batch settings. Backpropagation Through Time (BPTT) is commonly used in batch learning settings with fixed sequence lengths. However, it struggles with online learning due to the need to repeatedly backpropagate errors through an expanding graph. Real Time Recurrent Learning (RTRL) and Unbiased Online Gradient Optimization (UORO) address this issue by tracking synaptic weights' impact on the hidden state in a feedforward manner. Decoupled Neural Interfaces (DNI) estimate gradients to improve efficiency. KeRNL simplifies gradient estimation by using only rank-2 tensors, unlike RTRL which requires a rank-3 tensor. UORO also simplifies computations but still involves non-local operations. DNI necessitates a separate network for synthetic gradient tracking. KeRNL's simplicity lies in local computations and integration over a few relevant quantities in a single-layer RNN. The dynamics of recurrently connected hidden units in a single-layer RNN are governed by weights, biases, non-linearity, and summed inputs. The readout includes target output, error feedback, cost gradient, sensitivity weights, and eligibility trace. The simplicity of KeRNL lies in local computations and integration over relevant quantities. The eligibility trace in KeRNL specifies the responsibility of a synapse for errors in a neuron based on past activity, using a learnable temporal filter. KeRNL does not require backpropagation through time and only uses rank-2 tensors for computation, contrasting with BPTT. KeRNL uses sensitivity tensor to compute gradients without backpropagation through time, utilizing a rank-2 sensitivity weight matrix and a temporal kernel for learning interactions between neurons. The activity of a neuron affects other neurons for learning. Learning parameters (\u03b2, \u03b3) are described in the next section. KeRNL uses sensitivity tensor for computing gradients without backpropagation through time. The full gradient rule for a recurrent network is written down, treating parameters as functions that can vary over time. The derivative is expressed as a functional derivative, using the \"unfolding-in-time\" trick. The sensitivity lemma is applied to express gradients with respect to weights. The learning rule KeRNL(2) is derived by applying the sensitivity lemma to express gradients with respect to weights. The time-dependent computation involves a leaky integral of presynaptic activity multiplied by the change in postsynaptic activity. Parameters \u03b2 and \u03b3 are learned by tracking the effect of hidden perturbations during the forward pass. The effect of previous noise on the current hidden state is computed using sensitivity. Parameters \u03b3 and \u03b2 are trained to predict the network's response to noisy perturbations by taking gradients with respect to the objective function. The update rule for sensitivity weights and inverse-timescales is derived from an Ansatz substitution. The error in reconstructing perturbation effects is represented by sensitivity weights and integrals over applied perturbations. Parameters are updated before computing gradients, as described in the pseudocode table. KeRNL is tested on various tasks requiring memory and computation over time, showing competitiveness with BPTT. Batch learning is implemented on tasks like the adding problem and pixel-by-pixel MNIST. An online version of KeRNL with an LSTM network is compared with results from the UORO algorithm on the A n, B n task. Hyperparameters for BPTT and KeRNL include the learning rate and gradient clipping parameter. In practice, the same hyperparameter settings for learning rate and gradient clipping tended to work well for both BPTT and KeRNL. KeRNL also had an additional hyperparameter for sensitivity weights and kernels, which did not need fine-tuning and often worked well across a broad range. Both RMSprop and Adam optimizers were implemented, with the best results reported. The adding problem task required the network to sum input from one stream whenever there was a non-zero entry in the second stream, testing the network's ability to remember sparse information. The study compared the performance of two networks, an IRNN and an RNN with tanh nonlinearity, on a task involving long sequences of sparse information. Untruncated BPTT performed well on the IRNN but less so on the RNN with tanh nonlinearity. KeRNL outperformed BPTT with tanh nonlinearity, possibly due to its sensitivity to tanh nonlinearity. By using gradients from the Ansatz instead of true gradients, the network is pushed towards longer time scales through a feedback alignment-like mechanism. Learning the kernel timescales in KeRNL is crucial for performance on longer sequences, with the inverse timescales being more important than sensitivity weights. This suggests that a feedback-alignment-like mechanism may enable learning even without instantaneous error signals. The network is pushed towards longer timescales through a feedback alignment-like mechanism. In the pixel-by-pixel MNIST task, KeRNL and BPTT performed relatively well with an IRNN, with KeRNL preferring a slightly lower learning rate. The KeRNL algorithm learns quickly on pixel-by-pixel MNIST but doesn't reach as high performance as BPTT. It learns the relative importance of time-scales and parameter sensitivities. KeRNL is comparable in speed to BPTT for batch learning but faster for online learning with time-dependencies of length T. Untruncated BPTT requires information sent back T steps in time for each weight update. KeRNL algorithm requires no backward unrolling in time, resulting in faster online learning compared to truncated online BPTT. Performance was tested on the A n, B n task, where the network predicts the next character in a sequence of letters. The network can predict the number of As by matching the number of Bs in sequences of randomly generated lengths. The minimum achievable average bit-loss for this task is 0.14. KeRNL was implemented in an LSTM layer with specific hyperparameters from BID15. Results show that 17-step BPTT and UORO outperformed KeRNL on the A n, B n task after 10^6 minibatches. KeRNL, a reduced-rank and forward-running approximation to backpropagation in RNNs, performs comparably to BPTT on hard RNN tasks with long time-dependencies. KeRNL is faster than truncated BPTT for short truncation lengths. KeRNL implementation in Python showed faster computation times compared to truncated BPTT. KeRNL simplifies assigning credit for current performance in neural activity by using temporal kernels and sensitivity weights. This approach, combined with the ability to learn these parameters, provides simplicity and flexibility in recurrent networks. The KeRNL ansatz acts as a regularizer on network solutions, making it well-suited for learning problems in RNNs. KeRNL acts as a regularizer on network solutions, potentially outperforming BPTT. It addresses the vanishing gradient problem with tanh units by incorporating long time-dependencies through eligibility. KeRNL can be implemented online with a shorter computation cycle than BPTT, moving towards biologically plausible learning. It simplifies credit assignment in neural activity using temporal kernels and sensitivity weights, offering simplicity and flexibility in recurrent networks. KeRNL is shown to be unbiased and capable of performing well on realistic tasks. The use of reduced-rank tensor products and eligibility traces allows for constructing nested families of relaxed approximations to gradient learning in RNNs. Performance testing on the adding problem and pixel-by-pixel MNIST involved varying parameters like \u03b7 and gc. KeRNL demonstrated robustness across different \u03b7m values, with hyperparameters optimized for each task. In implementing KeRNL on an LSTM, various hyperparameters were optimized for different tasks. The weight matrices were initialized using Xavier initialization, and alternative cost functions were used. Python numpy package was utilized for training on tasks, including a dummy RNN with specific node configurations and nonlinearity functions. In implementing KeRNL on an LSTM, various hyperparameters were optimized for different tasks. The weight matrices were initialized using Xavier initialization, and alternative cost functions were used. The dynamics of the LSTM (without peepholes) are described, but KeRNL did not perform well on next word prediction on the PennTreebank dataset. Testing across different learning rates and gradient clippings did not achieve state-of-the-art performance using KeRNL."
}