{
    "title": "HJlk-eHFwH",
    "content": "Voice Conversion (VC) involves converting speaker identity from a source to a target speaker. Many-to-many VC using non-parallel data, such as zero-shot learning, is a less explored area. A novel style transfer architecture called Adaptive Generative Adversarial Network (AdaGAN) is proposed in this paper, allowing for voice generation for target speakers not used in training. This architecture helps in learning normalized speaker-independent latent representation for generating speech with different speaking styles in VC. The AdaGAN architecture is proposed for voice conversion, achieving significant improvements in speech quality and speaker similarity compared to the state-of-the-art StarGAN-VC. It is less computationally complex, with 88.6% fewer FLOPS and 85.46% fewer trainable parameters. Mimicking human voice involves understanding complex speech production mechanisms and prosodic transfer concepts. Voice Conversion (VC) technique has gained attention for its real-world applications in privacy protection, military operations, voice repair, and more. It involves modifying spectral and prosodic features to convert a source speaker's voice to sound like the target speaker. With the advancement of deep learning techniques, VC has become more efficient. With the advancement of deep learning techniques, Voice Conversion (VC) has become more efficient. Deep learning-based style transfer algorithms are being used for non-parallel VC tasks, treating it as a style transfer problem. Techniques like Conditional Variational AutoEncoders (CVAEs) and Generative Adversarial Networks (GANs) have gained attention for non-parallel VC, despite the challenges in training GANs and their fragile convergence property. The training task for GAN is challenging, with fragile convergence. Few GAN-based systems show state-of-the-art results for non-parallel VC, but complexity is a concern. Zero-shot VC has only one available system by Qian et al. (2019), which converts a source speaker's voice to an unseen target speaker's voice with a few utterances. AdaGAN is a computationally less expensive style transfer framework that utilizes Adaptive Instance Normalization for style transfer. It is compared to StarGAN-VC for non-parallel many-to-many voice conversion, showing state-of-the-art results with significantly lower computational complexity. AdaGAN is a GAN-based framework for zero-shot voice conversion, introducing latent representation based many-to-many voice conversion using GAN for the first time. It shows better results in naturalness and speaker similarity compared to the baseline, despite having lower computational complexity. Developing non-parallel voice conversion frameworks is challenging due to training conditions with deep learning architectures. Various approaches have been proposed in the past decade, including ML-based, speaker adaptation, GMM-based with MAP adaptation, iterative alignment, ASR-based, speaker verification with i-vectors, and other methods. Recently, GAN-based methods have shown success in non-parallel voice conversion tasks, with CycleGAN-VC being a state-of-the-art method. This approach utilizes architectures proposed by various researchers, such as Michelsanti & Tan, Saito et al., and Shah et al. (2018a). Among the state-of-the-art methods in non-parallel voice conversion tasks, StarGAN-VC is highlighted as a leading approach for many-to-many voice conversion. AutoVC, a recent framework, achieved state-of-the-art results in zero-shot voice conversion by reformulating the traditional problem as a style transfer task. The text discusses the generation of probability density functions for speech utterances to achieve voice conversion (VC) by transferring the speaking style of one speaker to another. It introduces the concept of mapping functions to generate distributions with the speaking style of the target speaker while retaining the linguistic content of the source speaker's utterances. The text introduces the concept of AdaGAN for voice conversion, aiming to transfer the speaking style of one speaker to another while preserving linguistic content. AdaIN is used to capture both style and content in a single feature representation, enabling zero-shot voice conversion. AdaIN is a feature representation method used in AdaGAN for voice conversion. It maps the linguistic content and speaking style of source and target speakers by matching mean and standard deviation. This allows for transferring speaking style while preserving content. The AdaIN method in AdaGAN uses distribution properties of the target speaker to transfer speaking style while preserving content. AdaIN does not add learning parameters, reducing computational complexity. The AdaGAN architecture includes an encoder, decoder, and discriminator for voice conversion. It enables easy and efficient style transfer and zero-shot voice conversion. The AdaIN method in AdaGAN utilizes the target speaker's distribution properties to transfer speaking style while maintaining content. This process involves generating speech features from a latent space and using AdaIN to transfer distribution properties to improve the quality of the generated speech features. The training methodology in AdaGAN involves transferring algorithms to improve the quality of speech features through adversarial training. The process includes introducing reconstruction loss and selecting random speakers to learn mapping functions for many-to-many and zero-shot voice conversion. During training, voice conversion is achieved by transferring speaking style from the source speaker to the target speaker using a style transfer scheme. Reconstruction of source speech features is done to improve conversion efficiency. Testing involves providing features of the source speaker's speech and the target speaker's sample features to the encoder. AdaGAN requires 3 to 5 seconds of sample speech for testing. The AdaGAN model requires 3 to 5 seconds of sample speech features from the target speaker to transfer their speaking style to the source speaker. This sample speech is used to estimate the target speaker's distribution in the latent space. The speaking style is then transferred to the source speaker's latent space using AdaIN, and the decoder generates speech with the target speaker's speaking style. The generator of AdaGAN consists of an Encoder and Decoder, which generates speech with the target speaker's style for a given source speaker's speech during testing. During testing, AdaGAN transfers speaking styles between speakers using four loss functions: Adversarial, reconstruction, content preserve, and style transfer. Adversarial loss measures the distinguishability of converted data from normal speech, aiming for a distribution closer to normal speech. The loss functions used in AdaGAN during testing include adversarial, reconstruction, content preserve, and style transfer. These functions aim to make the converted speech indistinguishable from the original speech while retaining linguistic information and preserving content during style transfer. The loss functions in AdaGAN, including adversarial, reconstruction, content preservation, and style transfer, are crucial for achieving many-to-many and zero-shot voice conversion. These functions help create a latent space with speaking style features while maintaining linguistic content. The overall objective function of AdaGAN is defined with hyperparameters controlling the importance of each loss. The framework consists of a Generator and a Discriminator, optimized according to Algorithm 1. The AdaGAN framework consists of a Generator with an Encoder and Decoder modules. The Encoder converts 40 Mel cepstral features to a latent space of size 1x512, while the Decoder converts a normalized feature vector of size 1x512 to 1x40 target speech features. All layers in the Encoder and Decoder are fully-connected with cell sizes of 40 and 512 in the input and output layers, and 512 in the hidden layers. The Discriminator's goal is similar to traditional methods. The AdaGAN framework includes a Generator with Encoder and Decoder modules. The Discriminator's goal is to distinguish between generated and original input. It consists of fully-connected layers with ReLU and sigmoid activation functions. AdaGAN aims to learn a latent space for feature representation. The training procedure involves two latent space features corresponding to different sample features of the same speaker. The AdaGAN framework involves training on latent space features of different sample features from the same speaker and another speaker. The goal is to achieve a normalized latent representation of input features regardless of speaking style. The fundamental theorem behind AdaGAN states that the normalized latent representation of speech captures linguistic content, while speaking style is captured by the mean and standard deviation of the distribution. Optimization can satisfy the assumptions, making style transfer efficient using only the mean and standard deviation. AdaGAN is compared to StarGAN-VC in terms of computational complexity. AdaGAN is shown to be 88.6% less complex in terms of FLOPS and 85.46% less complex in terms of trainable parameters. StarGAN uses one-hot encoding for target speaker information, while AdaGAN requires a sample of 3-5 seconds from the speech. AdaGAN requires 3-5 seconds of sample audio from the target speaker for conversion. The experiments were conducted on the VCTK corpus with data from 20 speakers (10 males and 10 females). 80% of the data was used for training and 20% for testing, with 40-dimensional Mel Cepstral Coefficients and 1-dimensional F0 extracted from the speech of source and target speakers. For analysis-synthesis, AHOCODER and mean-variance transformation method were used for fundamental frequency conversion. Two subjective tests were conducted to evaluate AdaGAN: Mean Opinion Score (MOS) for naturalness and speaker similarity rating. Subjects rated converted voices on a 5-point scale for naturalness and speaker similarity. A total of 15 subjects (6 females and 9 males aged 18-31) participated in subjective evaluations of four conversion systems (M2M, F2F, M2F, F2M) developed using AdaGAN and Star-GAN. Eight audio files from each system were selected for comparison based on MOS scores. The subjective test involved 15 subjects aged 18-30 with no hearing impairments. Results showed a significant improvement in MOS score and speaker similarity with AdaGAN compared to baseline StarGAN. However, both methods struggled with F0 conversion and errors in statistical vocoders. The F0 conversion and errors in statistical vocoders were challenges faced by traditional methods like AHOCODER and WORLD-vocoder, while the neural network-based Wavenet-vocoder showed promising results in speech synthesis. AdaGAN outperformed StarGAN-VC in MOS tests for naturalness and speaker similarity. Traditional many-to-many voice conversion struggles with zero-shot voice conversion for unseen speakers, which AdaGAN aims to address. Zero-shot conversion involves transferring speaking styles between seen/unseen source and target speakers, allowing for conversion between any speakers regardless of training data availability. StarGAN-VC uses a one-hot vector for target speaker reference. AdaGAN outperformed StarGAN-VC in zero-shot voice conversion tasks by mapping input to latent space, allowing for conversion between seen and unseen speakers. Experimental results showed MOS scores for naturalness and speaker similarity using AdaGAN trained on 20 speakers. Different conversion samples were generated for seen-seen, seen-unseen, unseen-seen, and unseen-unseen speaker pairs. In this paper, a novel GAN-based framework called AdaGAN is proposed for non-parallel many-to-many voice conversion tasks. AdaGAN requires only 3s to 5s of sample speech from the target speaker to generate voices that sound similar to the target speaker. It outperformed the current state-of-the-art StarGAN-VC method in zero-shot voice conversion tasks. AdaGAN is a GAN-based framework for voice conversion tasks, aiming to convert a source speaker's voice to a target speaker's voice while maintaining linguistic content. It utilizes style transfer and adversarial training, with only one generator and discriminator, making it 88.6% less complex than StarGAN-VC. Subjective analysis on the VCTK corpus shows AdaGAN outperforms StarGAN-VC in evaluations. Additionally, AdaGAN has been extended for zero-shot conversion with promising results. AdaGAN is the first GAN-based framework for zero-shot voice conversion. Future work includes exploring high-quality vocoders like WaveNet for improved voice quality and optimizing network parameters for better perceptual optimization. The assumptions made in the study can be satisfied by optimizing the objective function. The objective function involves iteratively calculating terms for the loss function L sty X\u2192Y using latent representations of source and target speech. Steps include applying encoder and decoder sequentially to ensure reconstruction and minimizing the loss function L C X\u2192Y. The final output aims to normalize the latent representation for improved voice conversion. The final output of the process involves normalizing the latent representation of source and target speech for voice conversion. The mean and standard deviations of input sources are adjusted to minimize the loss function and achieve the desired outcome. The loss function satisfies necessary constraints for successful voice conversion. The text discusses the use of t-SNE visualization to analyze speech features extracted from Neural Networks. It shows that data points cluster based on speaking style, and normalized latent representations capture linguistic information-based features. This proves the efficiency of AdaGAN and its losses for practical use in voice conversion. The efficiency of AdaGAN and its losses for practical use in voice conversion is proven in Theorem 1."
}