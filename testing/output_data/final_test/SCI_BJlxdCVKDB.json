{
    "title": "BJlxdCVKDB",
    "content": "Deep Reinforcement Learning (DRL) has achieved breakthroughs in complex control tasks, but lacks explainability. Viper creates a decision tree policy to address this, but faces limitations. MoET, based on Mixture of Experts, offers a more expressive and interpretable model by using a gating function to partition the state space and multiple decision tree experts. This model supports non-differentiable decision tree experts and is integrated into imitation learning. Our algorithm integrates non-differentiable decision tree experts into imitation learning for Viper, showing improved performance and better mimicry of DRL agents. MoET policies are verifiable using automated theorem provers like Z3. Deep Reinforcement Learning has made breakthroughs in complex tasks but lacks interpretability. Recent works use decision trees for interpretable agent policies. PIRL uses program synthesis for policy generation. Viper (Bastani et al., 2018) utilizes decision trees to mimic DRL agent policies, allowing for a general representation and verification using integer linear programming solvers. It employs the DAGGER imitation learning approach to train a decision tree policy by prioritizing critical states based on the teacher DRL policy's Q-function. However, learning a single decision tree for the entire policy has limitations. In this paper, MO\u00cbT (Mixture of Expert Trees) is introduced as a technique based on Mixture of Experts (MOE) to address the limitations of learning a single decision tree for the entire policy. The MO\u00cbT model reformulates the learning procedure to support decision tree (DT) experts, optimizing the weighted log likelihood for experts independently and the gating function interchangeably. In this study, a procedure for DT learning in the context of MOE is proposed, combining non-differentiable DT experts with MOE model. The MO\u00cbT technique utilizes multiple local DTs specialized in different input regions, mimicking the DRL agent policy more accurately. A simple linear model with softmax function is used as the gating function to create a distribution over DT experts for each input point. MO\u00cbT technique uses a softmax gating function to create a distribution over DT experts for each input point. It improves interpretability by selecting the most likely expert tree. The technique achieves better rewards and lower misprediction rates in various environments. The Viper and MO\u00cbT policies show differences in learning capabilities, with MO\u00cbT policies translatable into SMT formulas for verification using the Z3 theorem prover. The paper introduces MO\u00cbT, a technique based on MOE for learning expert decision trees, using a softmax gating function to interpret DRL policies. MO\u00cbT outperforms Viper in representing DRL agent policies with smaller, more faithful, and performant models while maintaining verifiability. The importance of interpretability in machine learning models is highlighted, with Lipton discussing the various definitions of interpretability. The concept of interpretability in machine learning models is discussed by Lipton, who mentions properties such as transparency, simulability, decomposability, and algorithmic transparency. These properties relate to our understanding of how models work, with examples given for linear models and decision trees. The curr_chunk discusses the importance of explainable machine learning models, including techniques like activation maximization and TCAV. It also mentions recent works on generating contrastive robust explanations and using LORE to explain black-box model behavior. The curr_chunk introduces a model that combines local decision trees into a global policy. It also mentions tree-structured models with soft decisions and hierarchical routing mixture of experts. Additionally, it discusses knowledge distillation and model compression techniques to train simpler models from complex ones. The curr_chunk discusses the application of knowledge distillation and model compression techniques in training simpler models from complex ones, citing examples from decision tree ensembles, neural networks, reinforcement learning, and imitation learning. The curr_chunk presents a motivating example of the N \u00d7 N Gridworld problem to showcase differences between Viper and MO\u00cbT approaches. The agent must find its way out of the grid by moving up, left, right, or down, receiving a negative reward for each action. The episode ends when the exit is reached or after 100 steps. The optimal policy for the N \u00d7 N Gridworld problem involves taking specific actions for states above and below the diagonal. Viper used imitation learning to train a decision tree (DT) policy that mimics the optimal policy. The resulting DT has 9 nodes and partitions the state space using perpendicular lines. In contrast, MO\u00cbT uses 2 experts to partition the space along a linear function, with points on different sides corresponding to experts that always choose left or right. The DT policy is deeper and more complex compared to the MO\u00cbT model. The DT policy in the N \u00d7 N Gridworld problem requires larger depth as N increases, while MO\u00cbT can represent it in one decision step. Empirical results confirm that DT complexity grows with N, while MO\u00cbT complexity remains constant. Additionally, Viper and MOE learning frameworks are described as relevant methods used in the study. Viper training algorithm prioritizes critical states based on Q-values in a finite horizon Markov Decision Process. It uses a teacher policy to label data and sample trajectories for training a student policy, leading to faster learning and shallower decision trees. The MOE ensemble model consists of expert networks and a gating function that divides the input space into specialized regions. It is flexible in choosing expert models and outputs the probability of y given x using learnable parameters. The MOE ensemble model involves expert networks and a gating function that partitions the input space. Learnable parameters control the experts and gating function, with the gating function modeled using a softmax function. Experts output probabilities for each class, which are combined using the gating function to produce a final probability vector. The MOE model is typically trained using the EM algorithm to optimize an auxiliary function instead of directly optimizing likelihood. The MOE ensemble model involves expert networks and a gating function that partitions the input space. The optimization of an auxiliary functionL is performed by considering the joint likelihood of x and z, where z is the expert chosen for instance x. Expected log likelihood is used since z is not observed in the data. The distribution with respect to the current estimate of parameters \u03b8 is used in the iterative process. The adaptation of the original MOE model to a mixture of decision trees is explained, along with training and inference algorithms. The MOE ensemble model involves expert networks and a gating function that partitions the input space. Training is carried out by optimizing the weighted log likelihood for each expert independently and optimizing the gating function interchangeably. The training procedure for MO\u00cbT involves randomly initializing gating function parameters, training each expert on a weighted dataset, and optimizing the gating part. Our tree learning procedure modifies the original MOE algorithm by using DTs as experts. Unlike traditional models, DTs do not rely on explicit loss functions but instead use a specific greedy training procedure. The training of DTs is adjusted to consider the responsibilities of experts for instances, leading to a partitioning of the feature space into regions assigned to different experts. The text discusses how the training of Decision Trees (DTs) in the Modified MOE algorithm considers the responsibilities of experts for instances, leading to a partitioning of the feature space into regions assigned to different experts. Soft responsibilities fractionally distribute instances to experts, influencing the impurity measure and probability estimates in the tree nodes. The Gini index is commonly used as an impurity measure for determining splits in the tree. The Gini index G of the set D U is defined by considering fractional instance assignments to experts using responsibility coefficients. The modified definition sums the weights of instances assigned to a node instead of counting them, proposing a new definition for computing the Gini index. Similar adjustments can be made for other impurity measures like entropy. Instance assignments to experts are soft, while assignments to nodes within an expert are hard. The Gini index G of the set D U is defined by considering fractional instance assignments to experts using responsibility coefficients. Instance assignments to experts are soft, while assignments to nodes within an expert are hard. Probability estimates for y in the leaf node are computed by fractions of instances belonging to each class. Algorithm 1 performs decision tree training with modifications for probability estimates needed by MOE. Two ways of inference are considered: MO\u00cbT maximizes P (y|x, \u03b8), while MO\u00cbT h relies on the most probable expert. Adaptation of MO\u00cbT to imitation learning is integrated. MO\u00cbT model is adapted for imitation learning by integrating it into Viper's approach. MO\u00cbT h improves model expressiveness by using hard softmax partitioning for feature space. It combines a linear model with decision tree models for interpretability. The MO\u00cbT model integrates decision trees and linear models for interpretability and model expressiveness. It maintains simulatability and decomposability due to the simplicity of the models used. Algorithmic transparency is achieved through well-understood training processes. The translation of MO\u00cbT models to SMT formulas opens new possibilities. The translation of MO\u00cbT models to SMT formulas allows for logical reasoning and model validation using automated tools. SMT formulas help compare models by answering questions about differences in inputs, predictions, equivalence, and output classes. Symbolic reasoning of gating functions and decision trees enables the construction of SMT formulas that can be handled by existing tools. Comparisons between MO\u00cbT and Viper on OpenAI Gym environments are conducted. In comparing MO\u00cbT and Viper on OpenAI Gym environments, DRL agents were tested on CartPole, Pong, Acrobot, and Mountaincar. Results showed varying rewards for each environment, with MO\u00cbT, MO\u00cbT h, and Viper policies being compared using different training parameters and maximum depths for decision trees. In comparing MO\u00cbT and Viper on OpenAI Gym environments, DRL agents were tested on CartPole, Pong, Acrobot, and Mountaincar. The policies were trained for 2 to 8 experts using the Viper algorithm with 40 iterations. The best performing policy was chosen based on rewards and misprediction rate. High rewards indicate better learning of the teacher's policy, while a low misprediction rate shows alignment with the teacher's actions. Mispredictions were measured by comparing the student's actions to the teacher's. Effective depth of a MO\u00cbT model was calculated as log2(E) + D to ensure comparability with Viper. In MO\u00cbT, the effective depth is calculated as log2(E) + D, where E is the number of experts and D is the depth of each expert. Results show that MO\u00cbT and MO\u00cbT h outperform Viper in terms of misprediction rates, even at larger depths. In various environments such as Pong, Acrobot, and Mountaincar, MO\u00cbT and MO\u00cbT h consistently outperform Viper in terms of rewards and misprediction rates. Additionally, both MO\u00cbT and MO\u00cbT h achieve comparable performance in some environments. Further analysis of learned policies is available in the appendix. We analyze student policies Viper and MO\u00cbT h by visualizing their state-action space in the Mountaincar environment. The state space consists of car position and velocity features with three allowed actions. DRL, Viper, and MO\u00cbT h policies are visualized in Figure 2, showing actions taken in different parts of the state space. The state space is defined by feature bounds, and actions are colored accordingly. MO\u00cbT h can cover regions with arbitrary orientation hyperplanes, while Viper can only cover specific regions. MO\u00cbT h policy has slanted borders in yellow and green regions to capture DRL policy geometry more precisely, while Viper policy only has straight borders. Mispredictions are visualized across the state space for both policies, with Viper authors weighting states by importance based on the difference between Q values of optimal and non-optimal actions. The importance score is calculated using the difference between Q values of optimal and non-optimal actions in a state. This score is used to weight mispredictions, visualized as a binary vector showing regions of high importance in red. MO\u00cbT h policy shows fewer high intensity regions compared to Viper policy. The MO\u00cbT h policy has fewer high intensity regions, leading to fewer overall mispredictions. A quantitative difference between the mispredictions of Viper and MO\u00cbT h policies is computed as M = 15.51 for Viper and M = 11.78 for MO\u00cbT h policies. The translation of MO\u00cbT policy to SMT constraints for verifying policy properties is also discussed. The pole upright can be encoded using a formula with state representation and deviation. Transition function and policy function need to be encoded as well. Bastani et al. (2018) use a finite time horizon and linear approximation to solve issues with verifying the formula. Gating function and DT experts need to be translated to logical formulas for encoding. The encoding of exponentiation in Z3 is complex due to non-linear arithmetic. Simplifications are made for gating functions and DTs. Verification is done using MO\u00cbT h policies to show unsatisfiability of \u00ac\u03c8. Verification times for different expert numbers and depths are reported. MO\u00cbT h policies with varying numbers of experts show increasing verification times, indicating complexity of logical formulas. MO\u00cbT models offer a more interpretable and performant representation of DRL agents compared to Viper. The VIPER algorithm is used to train DRL agents in different environments like CartPole, Pong, Acrobot, and Mountaincar using various models such as policy gradient, deep Q-network, and dueling DQN network. Parameters like learning rate, batch size, step size, and number of epochs are specified for each environment. In our experiments, we set the learning rate, batch size, step size, and number of epochs for training DRL agents in environments like CartPole, Pong, Acrobot, and Mountaincar. The environments consist of different elements such as a cart and a pole hinged to the cart, with specific goals and termination conditions. The environment is a classical Atari game of table tennis with two players. The goal is to swing the end-point above the bar for at least the length of one link. The state consists of six variables, and the action is applying negative, neutral, or positive. Acrobot is an unsolved environment where the goal is to reach a hill by moving a car back and forth to build momentum. The car can move left, right, or stay neutral, with rewards of -1 received at each step. The state is defined by car position and velocity. In Acrobot, the goal is to reach a hill by moving a car back and forth. The state is defined by car position and velocity. The game is considered solved if the average reward over 100 trials is no less than -110. Visualization of a gating function is provided, showing how it partitions the state space for different experts. Additional tables show results of different models on evaluation subjects."
}