{
    "title": "HygjqjR9Km",
    "content": "Generative adversarial nets (GANs) use the maximum mean discrepancy (MMD) as the loss function, but it may hinder learning fine details in data. A repulsive loss function is proposed to address this issue by rearranging terms in MMD to actively learn differences in real data. Additionally, a bounded Gaussian kernel inspired by the hinge loss is introduced to stabilize training. The methods are applied to unsupervised image generation tasks on various datasets, showing significant improvement over traditional MMD. The repulsive loss function improves over the MMD loss in unsupervised image generation tasks using GANs. It achieves an FID score of 16.21 on the CIFAR-10 dataset and outperforms other loss functions. GANs aim to mimic real data generation by training a generator and discriminator network. Recent studies on improving GAN training have focused on designing loss functions, network architectures, and training procedures. The loss function, such as maximum mean discrepancy (MMD), quantitatively measures the difference between real and generated samples. MMD is a distance metric widely used to compare probability distributions and has been applied directly to compare generated samples to real ones in the GAN framework. In this paper, a repulsive loss for the discriminator is proposed to address issues with the existing MMD loss in GAN training. The repulsive loss aims to explore differences among real data, leading to significant improvements in image generation tasks on benchmark datasets. Additionally, a bounded Gaussian kernel is suggested to stabilize discriminator training, simplifying the use of a single kernel in MMD-GAN compared to previous methods. The paper introduces a repulsive loss for the discriminator in GAN training, aiming to improve image generation tasks. It also suggests using a single kernel in MMD-GAN to reduce computational costs. The GAN model consists of a generator G and a discriminator D, with the generator mapping a latent code to the data space. The study focuses on image generation tasks using convolutional neural networks (CNN) for both the generator G and discriminator D. Various loss functions are used to measure the difference between real and generated samples, including minimax loss, non-saturating loss, hinge loss, Wasserstein loss, and maximum mean discrepancy (MMD). MMD utilizes kernel embedding to measure the similarity between samples. The kernel k(a, b) measures similarity between samples. MMD-GAN is more effective than using MMD directly for the generator. Weak metrics like JS divergence and TV distance are preferred for GAN training. In MMD-GAN training, the discriminator minimizes L att D to repel groups and increase variance, similar to linear discriminant analysis. The attractive loss L att D in GAN training may slow down the process by focusing on similarities among real samples rather than fine details that separate them. This can lead to the discriminator leaving out some fine details in real samples, hindering the generator's access to them during training. During training, the repulsive loss for D encourages repulsion of real data scores {D(x)}. The generator G uses the MMD loss L mmd G, with D contracting {D(y)} and G expanding {D(y)}. D also learns to separate real data by exploring fine details, resulting in more meaningful gradients for G. The text discusses the use of a bounded kernel and a power iteration method to stabilize the training of MMD-GAN. It also introduces a general form of loss function for the discriminator D. The text discusses the use of Gaussian radial basis function and rational quadratic kernel in MMD-GAN, with a linear combination of kernels using different scales. Concerns about saturation of a single kernel and potential gradient issues are raised. Li et al. applied penalties on discriminator parameters but not on the MMD loss itself, suggesting a saturation issue may persist. The proposal introduces a bounded RBF (RBF-B) kernel for the discriminator in MMD-GAN to address saturation issues. The RBF-B kernel aims to prevent the discriminator from becoming over-confident by setting lower and upper bounds. This approach allows for the use of a single kernel, with specific parameters, while retaining the original RBF kernel for the generator. The proposal introduces a bounded RBF (RBF-B) kernel for the discriminator in MMD-GAN to address saturation issues. Random sampling kernel scale, instance noise, and label smoothing can also improve model performance and stability. Spectral normalization imposes an upper bound on the magnitudes of outputs and gradients at each layer of the discriminator. The proposal introduces a bounded RBF (RBF-B) kernel for the discriminator in MMD-GAN to address saturation issues. A generalized power iteration method is used to estimate the spectral norm of a convolution kernel, and spectral normalization is applied to the discriminator in all experiments. The efficacy of different loss functions and kernels is empirically evaluated on various datasets including CIFAR-10, STL-10, CelebA, and LSUN bedrooms. The study utilized various datasets including CIFAR-10, STL-10, CelebA, and LSUN bedrooms. The images were scaled to range [-1, 1] to prevent numeric issues. Different hyperparameters were used such as the Adam optimizer with momentum parameters, two-timescale update rule, and batch size 64. Fine-tuning on learning rates was also conducted. The models were trained on CIFAR-10, STL-10, CelebA, and LSUN bedroom datasets with various hyperparameters. Different kernel scales were used for MMD-rbf and MMD-rq. Evaluation metrics included Inception score, Fr\u00e9chet Inception distance, and multi-scale structural similarity. The study evaluated different loss functions on benchmark datasets using the Inception score, Fr\u00e9chet Inception distance, and multi-scale structural similarity metrics. The MMD-rep and MMD-rep-b models outperformed MMD-rbf and MMD-rbf-b, indicating the effectiveness of the proposed repulsive loss over the attractive loss. The study compared different loss functions for MMD-GAN training on benchmark datasets, finding that MMD-rep-b performed better than MMD-rbf-b. The use of repulsive loss stabilized training, outperforming non-saturating and hinge losses. Training on CIFAR-10 dataset showed FID scores with varying learning rate combinations. The study compared different loss functions for MMD-GAN training, finding that MMD-rep-b performed better than MMD-rbf-b. The model performed well with repulsive loss, and increasing \u03bb improved the MMD-rbf model. Larger \u03bb values may lead to more diverged models. The proposed methods were evaluated in Appendices A, C, and D. In Appendix A.2, a simulation study showed the local stability of MMD-rep trained by gradient descent. Appendix C.3 demonstrated that the proposed generalized power iteration imposed a stronger Lipschitz constraint than previous methods. Additionally, the RBF-B kernel stabilized MMD-GAN training for various spectral normalization configurations. In Appendix D.1, the gradient penalty was shown to work with the repulsive loss. The gradient penalty can be used with the repulsive loss, and using multiple neurons at the discriminator output layer is beneficial for the repulsive loss. The discriminator outputs represent learned input sample representations. MMD-rbf did not consider class structure, while MMD-rep learned to separate classes. Techniques reinforcing cluster structure learning may enhance MMD-GAN training. The proposed repulsive loss showed performance improvement over the attractive loss. The proposed repulsive loss in MMD-GAN actively learns the difference among real data by combining attraction and repulsion processes. It offers a performance gain over the attractive loss without additional computational cost. The use of a single kernel simplifies the model compared to using a linear combination of kernels. Additionally, a bounded Gaussian RBF kernel is proposed to address the saturation issue. The repulsive loss in MMD-GAN aims to distinguish real data by maximizing pair-wise distances. A bounded Gaussian RBF kernel stabilizes training and may be further improved with hyper-parameter tuning. The proposed approach is computationally less intensive than previous methods. The proposed approach is computationally less intensive than previous methods and integrates labels or pseudo-labels to improve sample quality. Using a pre-trained classifier can also help produce vivid image samples. The proposed repulsive loss can enhance generator gradients, and it can be combined with various techniques like ResNet architecture and self-attention modules for improved results. Future work may explore directions such as progressively growing the size of discriminator and generator. The section demonstrates that MMD-GAN trained by gradient descent is locally stable at equilibrium under mild assumptions. The text discusses the stability of GANs trained with MMD loss, presenting assumptions, propositions, and simulation studies to show that equilibrium is not constant almost everywhere. Proposition 1 states that GANs with MMD loss have equilibria under certain conditions. The text discusses the stability of GANs trained with MMD loss, showing that the model is locally stable using gradient descent methods. It also mentions non-realizable cases where the mapping between input and output distributions may not be represented by the generator. A simulation study demonstrates the stability of the MMD loss and a two-parameter MMD-GAN model. Figure S1 displays streamline plots of MMD-GAN using different models on various distributions. The equilibria of the models are shown, with MMD-GAN being locally stable in both cases. However, MMD-rep may not be globally stable in some scenarios. Decreasing the learning rate for G can reduce the occurrence of trivial solutions. Both MMD-rbf and MMD-rep have a similar nontrivial solution in non-realizable cases. The proof for Proposition 1 in the context of MMD-GAN is divided into two parts. It shows that GAN with MMD loss has equilibria for any discriminator parameter configuration and is locally exponentially stable. The discriminator loss is considered in a general form, with real data Xr, latent variable Z, and generated variable Yg. The MMD loss uses an isotropic stationary kernel, and the gradients of the loss are discussed. Eq. S2 discusses equilibria in MMD-GAN with kernel k. The model stability is proven using Theorem 5 in BID8 and Theorem 4 in Li et al. (2017a). A non-linear system with parameters (\u03b8, \u03b3) is shown to be exponentially stable at equilibrium. Proposition 1 states that the GAN trained with MMD loss and gradient descent is locally stable at equilibrium. The kronecker product is used in the equations, and a sequence of N samples is considered at equilibrium. Lemma A.1 states that the Hessian matrix of M is locally constant along certain directions in the parameter space of G. This leads to null(J_GG) being a subset of null(J_DG). By considering eigenvalue decomposition, the system is shown to be exponentially stable. The section highlights that a constant discriminator output may lack discrimination power, with specific assumptions made about the multilayer perceptron structure. D is a multilayer perceptron with each layer factorized into an affine transform and an activation function. Input data to D is continuous with a compact support in R^d. Proposition 2 states that for any x in the support, there exists a distortion \u03b4x such that D(x + \u03b4x) = c. The model weights and biases, along with the activation function f, play a role in the output range. The Jacobian and projected support \u015c are also considered in the analysis. The Jacobian and projected support \u015c are analyzed for a multilayer perceptron D with continuous input data. For linear activation, z c remains 0, while for nonlinear activations, exceptions exist where z c remains 0 due to discontinuities in the activation function. The text discusses the effectiveness of a discriminator trained using MMD loss function against fake samples, with various loss functions proposed to quantify the difference between real and generated sample scores. The text discusses different loss functions for quantifying the difference between real and generated sample scores, including the Hinge loss, Wasserstein loss, and maximum mean discrepancy (MMD). The DCGAN architecture was used for unsupervised image generation tasks on various datasets, with adjustments made to the generator and discriminator for different datasets. The text discusses methods for estimating the spectral norm of a weight matrix in convolutional neural networks. It introduces the PIM method for estimating the spectral norm and proposes a new method based on the linearity of convolution operations. This new method simplifies the calculation of the spectral norm by directly calculating the convolutional kernel. The PICO method estimates the real spectral norm of convolutional operations by simplifying the calculation through direct convolutional kernel computation. Similar approaches have been proposed in related studies, such as FFT and SVD methods, but PIM and PICO are preferred for efficiency in estimating singular values. The PICO method estimates the real spectral norm of convolutional operations to enforce an upper bound on the Lipschitz constant of the discriminator D in GAN. To address signal norm decrease issues, a constant C is multiplied after spectral normalization, enlarging the Lipschitz constant by C times the number of layers in the discriminator. In Section 5, the effects of coefficient C K on PICO performance were empirically evaluated. Four loss functions were tested: hinge, MMD-rbf, MMD-rep, and MMD-rep-b, with either PICO or PIM used at each discriminator layer. Five coefficients C K were tested: 16, 32, 64, 128. The effects of coefficient CK on PICO performance were evaluated with four loss functions: hinge, MMD-rbf, MMD-rep, and MMD-rep-b. Five coefficients CK were tested: 16, 32, 64, 128, and 256. FID was used to evaluate the performance of each combination of loss function and power iteration method. PICO, hinge, MMD-rbf, and MMD-rep methods were sensitive to the choices of CK. Higher CK values may improve FID scores and reduce diverged cases for hinge and MMD-rbf. MMD-rep performed best with CK = 64 or 128. PIM showed similar performance to PICO with CK = 128 or 256 on various datasets. MMD-rep-b outperformed hinge and MMD-rbf, while MMD-rep achieved better FID scores than both. The proposed repulsive loss may be a better choice than hinge and MMD loss for the discriminator based on FID scores. PICO outperformed PIM on LSUN-bedroom dataset with hinge and MMD-rbf, while achieving consistently better FID scores with MMD-rep and MMD-rep-b. However, PICO has a higher computational cost, similar to increasing batch size by two, making it less suitable for scenarios with memory constraints. It is recommended to use PICO when computational cost is not a major concern. The section explores applying the proposed repulsive loss with gradient penalty to impose the Lipschitz constraint on the discriminator. Various gradient penalty methods have been proposed for MMD-GAN, including penalizing the gradient norm of the witness function. The Scaled MMD incorporates gradient and smooth penalties to impose the Lipschitz constraint on the mapping directly. The same formation of gradient penalty is applied to the repulsive loss. The gradient-penalized repulsive loss was evaluated on the CIFAR-10 dataset, using \u03bb = 0.1 instead of 10 due to restrictions. The discriminator's output dimension was set to one, relying on the gradient penalty for Lipschitz constraint rather than spectral normalization. The repulsive loss was tested on CIFAR-10 dataset with gradient penalty for Lipschitz constraint instead of spectral normalization. Results showed improvement over attractive MMD loss for discriminator, with investigation into the impact of discriminator output dimension on performance. The repulsive loss was tested on the CIFAR-10 dataset with various discriminator output dimensions. Using more than one output neuron significantly improved performance, but the gain diminished with more neurons, potentially due to increased computation cost. Cost increased due to more output neurons. Generated samples on CelebA dataset and LSUN bedrooms. Spectral normalization applied to discriminator with PICO and PIM methods. FID score threshold used to determine poorly-performed cases.\u03c4 values for CIFAR-10, STL-10, CelebA, and LSUN-bedroom datasets. Box quartiles plotted for cases with FID < \u03c4. Number of poorly-performed cases shown for each dataset. Introducing \u03c4 helped address arbitrarily large FID scores in diverged cases."
}