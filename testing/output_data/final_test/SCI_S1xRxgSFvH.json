{
    "title": "S1xRxgSFvH",
    "content": "Deep CNNs have achieved state-of-the-art performance for machine learning and computer vision tasks, but their increasing depth has led to a higher number of parameters, making them hard to deploy in memory-constrained environments. To address this, a filter-sharing approach is proposed to compress deep CNNs by reducing their memory footprint through the repeated application of a single convolutional mapping of learned filters. Experiments on various datasets show that this approach effectively reduces parameter counts. Tiny ImageNet allows for reducing parameter counts of networks like VGGNet and ResNet while maintaining accuracy. This approach leverages scale-space regularities in visual signals to create more efficient and interpretable neural architectures. Deep CNNs have excelled in tasks like image understanding and natural language processing but are often overparameterized, requiring significant training time. Recent studies have identified redundancies and simplicities in neural network architectures, showing that large classification networks can be distilled down to smaller trainable sub-networks without compromising accuracy. Deep classification networks learn simplistic non-linearities for class identification, potentially contributing to their vulnerability to adversarial attacks and challenging the need for complex architectures. Knowledge distillation techniques have shown that small student networks can mimic larger teacher networks effectively. Our paper experiments with a new scheme to simplify CNNs by using a common set of convolutional filters at different levels for class disentanglement. This approach aims to reduce the size of networks while enhancing network understanding and redesign possibilities. Mathematically, a classification CNN is formulated as an iterative function with learned convolutional mappings applied repeatedly as different layers in a pipeline. The introduction of non-shared linear widths in the shared convolutional layer is investigated for CIFAR-10. This work is inspired by image processing literature seeking to characterize natural images by collating their responses to a set of filters. Our work aims to reconcile traditional image processing approaches with modern CNN architectures by identifying a common set of visual operators for different scales through end-to-end learning. This approach is similar to previous efforts in convolutional neural networks. Our work explores implementing convolutional neural networks with shared convolutional mappings to improve model flexibility and efficiency. Experimental analysis includes evaluating accuracy vs. model size tradeoff on datasets like Tiny ImageNet and ImageNet. The study focuses on compressing neural networks to improve practical usability, addressing concerns about over-parameterization and heavy computational requirements for real-world deployment. Existing compression methods for machine learning models aim to make them more usable in practical scenarios. These methods include pruning, quantisation, tensor decomposition, knowledge distillation, custom architectures, sharing-based, and hybrid methods. The study falls within the realm of sharing-based methods, which reduce the number of independent parameters in a network by equating some of its weights or filters. Different approaches, such as the hashing trick and k-means clustering, are used to decide which weights/filters to share. Recent works have focused on sharing convolutional weight matrices to further optimize compression. The study focuses on extending the filter-sharing paradigm to an entire convolutional pipeline, analyzing the tradeoff between accuracy and memory for different widths of a convolutional layer. The approach involves applying a single convolutional layer iteratively to mimic a deep feature extractor in a CNN architecture. The convolutional feature extractor F conv consists of convolutional layers with non-linearities or regularizers denoted by R i. Each layer f i is specified by weights and biases W i. The number of parameters in layer f i is determined by input and output channels, volume, and filter size. The biases are disregarded in practice. The total parameter count of the convolutional feature extractor F conv is determined by the parameters in the convolutional layers. A simplification method is proposed for common architectures by instantiating a single convolutional layer and applying it multiple times to achieve equivalent depth. This simplifies the CNN architecture and can lead to further simplification when all layers are the same. The CNN architecture simplifies by using a single convolutional layer applied iteratively to maintain the number of channels unchanged. The total number of independent parameters in F conv is determined by the volume of the convolutional layer, with a shared parameter count between all layers. This leads to a compression factor between the original architecture and its simplified counterpart. The compression factor between the original architecture and its simplified counterpart is determined by the depth of the network and the average per-layer volume. The convolutional operation in the architecture is defined by input tensor X and weights W, with 2D matrices \u03a6(X) and \u0393(W) representing the operation. The convolutional operation in the architecture is defined by input tensor X and weights W, with 2D matrices \u03a6(X) and \u0393(W) representing the operation. The function f can be defined as f(X) = \u03a8(\u03a6(X) \u00d7 \u0393(W)), reshaping the tensor back to size n \u00d7 h \u00d7 w. Investigating reorganizing input channels in the pipeline to improve performance by adding permutation matrices at appropriate points. Operations are implemented to be differentiable. In the architecture, the convolutional operation is defined by input tensor X and weights W, with 2D matrices \u03a6(X) and \u0393(W) representing the operation. Linear layers are used for differentiability, implementing blending of input channels. The weights are learned as part of the end-to-end pipeline. Results show improved performance with added flexibility. Shared architectures already perform well even without this flexibility. The filter-sharing approach is evaluated on various image classification benchmarks. For this study, two different architectures are used, one inspired by VGGNet and the other by ResNet. The VGGNet-like architecture is based on VGG-16 with multiple convolutional layers and max-pooling steps. Four variants of the convolutional feature extractor are defined, with E-VGGNet being equivalent to VGGNet. The study introduces different variants of the VGGNet-inspired architecture, including E-VGGNet, S-VGGNet, SL-VGGNet, and simplified SL-VGGNet with varying numbers of layers per block. Experiments are conducted on CIFAR-10/100 and Tiny ImageNet datasets to test the performance of these architectures. The study introduces different variants of VGGNet-inspired architecture and ResNet-like architectures based on He et al. (2016). Experiments are conducted on CIFAR-10/100 and Tiny ImageNet datasets to test the performance of these architectures with varying numbers of output channels and convolutional layers. The network pipeline includes a single 3\u00d73 convolutional layer sandwiched between two 1\u00d71 convolutional layers to control the number of parameters. It starts with a standalone convolutional layer outputting p channels, followed by b blocks at different scale levels. The E-ResNet architecture doubles channels and uses linear layers and max-pooling for downsampling. The E-ResNet architecture includes a single 3\u00d73 convolutional layer with two 1\u00d71 convolutional layers to control parameters. It uses linear layers and max-pooling for downsampling, with the final scale level ending in average pooling. The shared variant, SL-ResNet, uses n channels for all scale levels and shares weights across convolutional layers. Different block sizes are experimented with, and the network ends with a linear classifier for CIFAR-10 and CIFAR-100 datasets. For Tiny ImageNet and ImageNet, ResNet-like architectures are based on ResNet-34 and ResNet-50. ResNet-34 uses 'basic' blocks, while ResNet-50 uses 'bottleneck' blocks with multiple shared copies of a single block. The shared variants, SL-ResNet-34/50, keep the standalone convolutional layer unshared. Fig. 3 illustrates the improvements in accuracy due to the learned linear layers. The use of linear layers provides greater benefit for datasets with more classes like CIFAR-100 and Tiny ImageNet. Comparing accuracies of 'SL' variants of VGGNet with baseline models and other compression methods on CIFAR-10, CIFAR-100, and Tiny ImageNet shows comparable results with fewer channels. Using only 256 channels in the shared convolutional layer, a compression factor of 17\u00d7 is achieved compared to the VGGNet baseline. For CIFAR-100, 512 channels were needed for comparable accuracy, resulting in a compression factor of 4.3. Higher compression factors can be achieved by reducing channels, albeit with some loss in accuracy. SL2-VGGNet on Tiny ImageNet achieved comparable accuracy to the non-shared baseline using only 23% of its parameters. Our SL-ResNet architecture achieves a compression rate of 8.4 on Tiny ImageNet and 8 on ImageNet with negligible loss in accuracy. The linear layers used to blend input channels are visually depicted in Figure 4. The SL-ResNet architecture achieves a high compression rate with minimal loss in accuracy. Visualizing the blending layers reveals how input channels are utilized, showing patterns of information suppression. SWRN achieves top accuracy levels but does not reduce parameter count. The SL-ResNet architecture achieves high compression rates with minimal loss in accuracy by selectively pruning unused channels. Experiments show significant reductions in network size by exploiting this observation. Parameter count increases with the number of channels and depth, especially with bottleneck blocks in SL-ResNet50 variants. By applying magnitude-based weight pruning to the linear layers of the SL-ResNet-50 model, a significant fraction of blending weights can be removed without a noticeable drop in accuracy. This approach allows for serious reductions in the number of parameters without retraining the network. In this paper, the authors zero out an increasing fraction of linear layer parameters, achieving high compression rates without sacrificing accuracy. They extend filter-sharing to the entire convolutional pipeline for feature extraction, demonstrating superior compression rates on various benchmarks compared to existing methods. The authors demonstrate high compression rates by combining their method with weight pruning. They also highlight the flexibility of their architecture in achieving a balance between compression rate and accuracy. Additionally, their approach addresses energy demands in deep network processing by avoiding weight transfers to and from the file system. The authors propose a method to bypass bottlenecks in deep network processing by using a compact set of weights iteratively. Pruning methods reduce network size by removing weights, filters, or neurons, while quantization methods reduce memory requirements by decreasing parameter bit-depth. Various methods such as tensorization and knowledge distillation are used to reduce memory requirements and transfer knowledge from complex teacher models to lightweight student models in deep neural networks. As networks have become deeper and wider, the focus has shifted towards convolutional layers, leading to more generalized tensor decomposition schemes. Several methods have been developed to transfer knowledge from a complex teacher model to a lightweight student model in deep neural networks. These methods include using ensemble models, mimicking logits of deep models, and training with temperature-scaled softmax scores or Gaussian-blurred logits. Other approaches involve training deep, thin neural networks using auxiliary cues like hidden layer outputs or attention maps. Custom architecture methods like SqueezeNet and MobileNets propose new network architectures that are smaller yet perform well by using techniques like 1x1 filters and depth-wise separable convolutions. These methods aim to reduce parameter count and improve efficiency in deep neural networks. The CIFAR-10 dataset consists of 60,000 32x32 color images divided into 10 classes for training and testing. CIFAR-100 is similar but with 100 classes. Adjustable hyperparameters \u03b1 and \u03c1 resize the network for joint channel processing. Hybrid methods combine compression schemes, including filter-sharing and weight pruning. Tiny ImageNet 2 is a smaller version of ImageNet, with 120,000 64x64 images divided into 200 classes. ImageNet is a large-scale image classification benchmark with 1M training images in 1,000 categories. Validation results were obtained on a set of 50,000 images. The structure of VGGNet-like and ResNet-like architectures is detailed in Tables 3, 4, and 5. These architectures include convolutional layers with different output feature channels, allowing shared convolutions at different scale levels. The number of input feature channels varies depending on the specific architecture. The curr_chunk describes different types of convolutional layers with varying numbers of input and output feature channels, as well as their respective strides. The basicblock-x is a skip connection-based block used in ResNet-like architectures, consisting of two 3x3 convolutional layers and a skip connection. The curr_chunk discusses convolutional layers with shared parameters, including bottleneck-x and avgpool-x layers. It also mentions the internal architectures of non-shared and shared block variants. The curr_chunk describes the architectures for VGGNet and VGGNet-like networks used in experiments on CIFAR-10/100 and Tiny ImageNet datasets. It includes details on max-pooling, fully-connected layers, convolutional layers, BatchNorm, ReLu, and Dropout layers. The parameters of normalization layers are not shared even when convolutional weights are shared in S or SL architectures. The architectures for ResNet-like networks used in experiments on CIFAR-10/100 datasets are described. Different fully-connected layer sizes are set based on the number of classes in the dataset. The final fully-connected layer output size is determined by the number of classes in the dataset. The architectures for ResNet-like networks used in experiments on Tiny ImageNet and ImageNet datasets are described, with different values of b \u2208 {3, 5, 7}. The final fully-connected layer output size is set based on the number of classes in the dataset. In Tiny ImageNet, a 3 \u00d7 3 convolution without striding is used in the first scale level to account for the smaller resolution of the images. In the first scale level, a 3 \u00d7 3 convolution without striding is used to feed the convolutional architecture with an input image of size 56 \u00d7 56. Test accuracies and parameter counts for different variants of VGGNet are provided. Basic data augmentation steps are taken to train networks on CIFAR datasets, including flipping images horizontally, padding images, and normalizing RGB values. During training, networks are trained for 200 epochs using SGD optimizer with momentum 0.9 and weight decay 5e-4. Data augmentation includes random crop extraction, resizing to input resolution, and optional horizontal flip. Evaluation phase involves resizing images to standard resolution and extracting multiple crops for analysis. During training, networks are trained for 100 epochs using SGD optimizer with momentum 0.9 and weight decay 5e-4. Data augmentation includes crop extraction, resizing, and normalization. Different initial learning rates are used for VGGNet-like and ResNet-like architectures on Tiny ImageNet and ImageNet. Learning rates are decreased by a factor of 10 when the error plateaus. Evaluation on classification benchmarks shows detailed accuracy and memory usage numbers for various architectures trained on CIFAR-100 and Tiny ImageNet. Table 8 compares the accuracy and compression rate of the top-performing SL3-ResNet variant with existing baselines and competing compression methods for CIFAR-10. The compression factor of the proposed model is significant, with a final weight count of only 181K. The model has been directly compressed by sharing convolutional layers, resulting in a compression factor of 4.0. In Fig. 7, linear layers of VGGNet variants trained on CIFAR-10, CIFAR-100, and Tiny ImageNet are shown. Some input channels barely contribute to output channels, suggesting pruning weights without affecting accuracy. Results for 'SL' variants of ResNet on the same datasets are also discussed. The linear layers of ResNet variants trained on different datasets are analyzed in Fig. 8. Some input channels show minimal contribution to output channels, indicating potential for weight pruning without impacting accuracy. SL7-ResNet achieves the highest accuracy (93.2%) and compression rate (3.8), suggesting a balanced use of channels for optimal performance. The model is operating at full capacity, using all channels in a balanced way for optimal performance. Visual depictions of linear layers in 'SL' variants of VGGNet and ResNet show channel blending with highlighted weight distribution. In linear layers of 'SL' variants of VGGNet and ResNet, weight distribution is visualized along the y-axis, highlighting the lowest 32 weights in blue and the highest 32 in red."
}