{
    "title": "BylQSxHFwr",
    "content": "Designing a fine-grained search space for neural architecture search algorithms, our method utilizes atomic blocks for direct selection of channel numbers and kernel sizes in convolutions. A resource-aware architecture search algorithm dynamically selects atomic blocks during training and is accelerated by a dynamic network shrinkage technique. Unlike traditional methods, our approach can simultaneously search and train the target architecture in an end-to-end manner, achieving state-of-the-art performance on ImageNet with minimal searching cost. Neural Architecture Search (NAS) has surpassed human-designed neural networks. NAS algorithms involve humans in designing search spaces, with various approaches explored by researchers. Different methods like utilizing supernets with multiple choices, progressively growing supernet depth, and searching for scaling factors have been proposed by researchers. Researchers have proposed various methods for Neural Architecture Search (NAS), including using different kernel sizes in each layer of a supernet, adopting MobileNetV2 blocks for efficient networks, and searching for expansion ratios in building blocks. However, the search spaces typically have limited choices, and a more fine-grained search space is needed to find optimal neural architectures. In a supernet, the building block should be small for diverse model structures. Commonly used building block is convolution-channel-wise operation-convolution, reinterpreted as atomic blocks. AtomNAS algorithm enables efficient architecture search and network training simultaneously by selecting a subset of atomic blocks with importance factors. AtomNAS introduces a penalty term based on atomic block computation cost to select blocks efficiently. A dynamic network shrinkage technique removes ineffective blocks, reducing computation cost. Achieving 75.9% top-1 accuracy on ImageNet with 360M FLOPs, outperforming MixNet by 0.6% using 363M FLOPs. Our work introduces AtomNAS, achieving state-of-the-art performance on ImageNet with 360M FLOPs. It includes a fine-grained search space and an efficient end-to-end NAS algorithm named AtomNAS. No finetuning is needed after AtomNAS finishes. The search space in ENAS is represented using a directed acyclic graph to find the optimal subgraph efficiently. Other methods also optimize subgraphs within a supergraph but have limited search spaces. In contrast, our method proposes a fine-grained search space for more flexible network architectures under resource constraints. Structured network pruning methods aim to identify and remove unimportant connections in neural networks to create a more efficient and compact network. These methods focus on pruning structured connections, such as channel numbers, to improve network performance. Our method focuses on searching channel numbers and operations in a large search space using atomic blocks. We propose a fine-grained search space and a resource-aware atomic block selection method for architecture search. Additionally, we introduce a dynamic network shrinkage technique. In Section 3.3, a dynamic network shrinkage technique is proposed to reduce search cost in architecture search. The search space is represented as a Cartesian product of choices for each block in a neural network. A more fine-grained search space is presented by decomposing the network into smaller building blocks, allowing for more options in selecting convolution types and output channel numbers. The text discusses the structure of neural network architectures, focusing on the use of convolution operators and channel-wise operations. Various architectures like VGG, Residual Block, and MobileNetV2 are mentioned, each utilizing different types of convolutions and operators. The text also introduces the concept of atomic blocks, which are computationally independent building blocks within the network. The text introduces the concept of atomic blocks in neural network architectures, enabling channel selection and operator variations within the model. The search space is greatly expanded by individually determining the inclusion of each atomic block, leading to a supernet structure based on Eq. (1) and satisfying Eq. (3). Each atomic block can utilize different operators, such as convolution with varying kernel sizes. The search space for neural network architectures is expanded by allowing atomic blocks to be represented by combinations of operators, including convolutions with different kernel sizes. This approach enables fine-grained channel configuration and model structure discovery through differentiable neural architecture search. The final model is produced by discarding part of the atomic blocks during training and scaling the output with a factor \u03b1. The supernet uses a factor \u03b1 to scale atomic block outputs. The scaling factors are learned with network weights and blocks with factors below a threshold are discarded after training. The placement of \u03b1 in the supernet can be in BN layers or between specific operators f 0 and f 1 with regularization to prevent weight cancellation. The supernet uses scaling factors \u03b1 to adjust atomic block outputs. To prevent performance deterioration after discarding blocks, a penalty loss on \u03b1 is added. The network weights and scaling factors are updated iteratively to maintain performance. If the total FLOPs of dead blocks exceed a threshold, they are removed. Dynamic network shrinkage involves updating scaling factors \u03b1 using a loss function and removing atomic blocks with \u03b1 close to zero from the supernet. Regularization terms affect the BN scales during training, requiring recalculation of BN statistics. The training loss includes a penalty term with coefficient \u03bb and a weight decay term. The coefficient c_i is proportional to the computation cost of the i-th atomic block. In this paper, computation costs aware regularization is used to balance accuracy and efficiency in learning network structures. The loss function trades off between accuracy and FLOPs, with a dynamic network shrinkage algorithm proposed to remove \"dead\" atomic blocks. This algorithm aims to speed up the search process by cutting down the network architecture efficiently. The dynamic network shrinkage algorithm removes \"dead\" atomic blocks from the supernet based on scaling factors and FLOPs thresholds. The training process involves recalculating BN's running statistics and only incurs a 17.2% increase in search and train cost compared to training from scratch. AtomNAS is compared to training the model from scratch. Implementation details are described in Section 4.1, while comparisons with previous methods are made under various FLOPs constraints in Section 4.2. The search block in the supernet consists of different convolutions and skip connections. The overall architecture of the supernet is illustrated in Fig. 3. The supernet architecture includes 21 search blocks trained with RMSProp optimizer and EMA on weights. Results are presented in Table 1 and Table 3, with training done on 32 Tesla V100 GPUs for 350 epochs. The dynamic network shrinkage algorithm uses a momentum factor of 0.9999 and penalizes FLOPs gradually over the first 25 epochs. The L1 penalty term is set to 1.8\u00d710 \u22124, 1.2\u00d710 \u22124, and 1.0\u00d710 \u22124. AtomNAS uses different L1 penalty terms to obtain networks of varying sizes (AtomNAS-A, AtomNAS-B, and AtomNAS-C) with similar FLOPs as previous state-of-the-art networks. AtomNAS achieves new state-of-the-art results on the ImageNet 2012 classification task, surpassing models like PDARTS and DenseNAS in accuracy with lower FLOPs. AtomNAS-A+ achieves 76.3% top-1 accuracy with 260M FLOPs, outperforming heavier models like MnasNet-A2 and performing as well as Efficient-B0 by incorporating techniques like Swish activation and Squeeze-and-Excitation modules without searching for optimal configurations. Training is done from scratch with a total batch size of 4096 on 32 Tesla V100 GPUs for 250 epochs, resulting in improved results. AtomNAS-A+ achieves 76.3% top-1 accuracy with 260M FLOPs, outperforming models like MnasNet-A2 and Efficient-B0. AtomNAS-C+ improves ImageNet accuracy to 77.6%, surpassing MixNet-M by 0.6%. The fine-grained search space and resource-aware method boost performance significantly. The architecture of AtomNAS-C includes various atomic blocks with specific dimensions and operations, leading to improved performance in terms of accuracy and efficiency. The architecture of AtomNAS-C features atomic blocks with different kernel sizes, providing flexibility in channel number selection. AtomNAS learns the importance of using multiple kernel sizes and tends to keep more blocks at later stages to reduce FLOPs. AtomNAS-C features atomic blocks with different kernel sizes to reduce FLOPs. AtomNAS uses resource-aware regularization to minimize computationally costly atomic blocks compared to a baseline without FLOPs-related coefficients. Increasing the penalty coefficient \u03bb results in a network with similar FLOPs but inferior performance. The baseline network keeps more atomic blocks in earlier stages with higher computation cost, while AtomNAS optimizes block distribution for efficiency. AtomNAS-C improves performance by recalibrating BN statistics before inference, resulting in higher accuracies on ImageNet. The dynamic network shrinkage algorithm speeds up the search and train process significantly, with a total time of 25.5 hours for AtomNAS-C compared to 22 hours for training from scratch. This approach reduces GPU memory consumption and forward-backward computations. In this paper, the AtomNAS algorithm proposes a new approach for architecture search and network training. By reformulating the common structure as an ensemble of atomic blocks, a larger search space is enabled. The end-to-end algorithm achieves better accuracy with small extra cost compared to previous methods. The dynamic network shrinkage algorithm reduces GPU memory consumption and forward-backward computations, speeding up the process significantly. The AtomNAS models are evaluated for object detection and instance segmentation on the COCO dataset. They are pretrained on ImageNet without certain enhancements, used as backbone in Mask-RCNN, and fine-tuned on COCO. Training is done with MMDetection on COCO train2017 and evaluated on COCO val2017. The AtomNAS models are trained on COCO train2017 with a batch size of 16 and evaluated on COCO val2017. The learning rate starts at 0.02 and decreases at the 15th and 20th epoch. Results show that AtomNAS models outperform baseline models in object detection. The models demonstrate better transferability due to mixed operations being more important for object detection and instance segmentation."
}