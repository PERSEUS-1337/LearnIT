{
    "title": "B1xgv0NtwH",
    "content": "Targeted clean-label poisoning is an adversarial attack on machine learning systems where the adversary injects correctly-labeled, minimally-perturbed samples into the training data to cause misclassification during inference. Despite the attacks' effectiveness, no reliable defense has been demonstrated. This work proposes simple yet highly-effective defenses against these attacks. Our proposed defenses successfully detect and remove over 99% of poisoning examples in recent clean-label poisoning attacks on the CIFAR-10 dataset. These defenses serve as a strong baseline for future attacks on machine-learning systems deployed in high societal impact and high-cost settings. In high-stakes environments, machine-learning systems must be robust against adversarial manipulation of training data sourced from public platforms. Data poisoning attacks involve injecting malicious data during training to manipulate classifier performance at test time. Previous research has focused on attackers degrading model performance or defenders detecting and mitigating these attacks. In this paper, the focus is on defending against clean-label data poisoning attacks on neural networks, where attackers inject correctly-labeled, minimally-perturbed samples into the training data to cause misclassification during inference. These attacks are challenging as they do not require control over the labeling process, making them plausible in various applications. The defense strategy discussed exploits the nature of the poisoned examples to counter these attacks. The defense strategy against clean-label data poisoning attacks on neural networks involves leveraging anomalous feature representations of poisoned examples for detection. This defense is based on k-nearest neighbors in the feature space, allowing for a trade-off between attack power and defense impact on model accuracy. Additionally, a traditional data poisoning defense adapted for clean-label cases shows inferior performance compared to the proposed defense. Various baselines are also tested against state-of-the-art clean-label data poisoning. Our defense strategy against clean-label data poisoning attacks involves using k-nearest neighbors in feature space to detect nearly all poison instances without affecting overall performance. The attack involves optimizing base images of frogs to appear as planes in feature space, while still looking like frogs in input space. The attack involves optimizing base images of frogs to appear as planes in feature space, while still looking like frogs in input space. Under different attack methods, the decision boundary shifts to classify poison images as frogs, changing the target image's class. The poisons are likely surrounded by points of the target class rather than the base class, as illustrated with two poisons having majority non-poison neighbors. The text discusses defenses against adversarial attacks, focusing on evasion attacks and data poisoning attacks. It mentions that most defenses have concentrated on evasion attacks, where inputs are manipulated to cause misclassification. In neural networks, adversarial examples are perturbed to increase loss on the victim network. The search for optimal perturbation is facilitated by using the local gradient obtained via backpropagation. Many defenses against evasion attacks rely on the local gradient obtained through backpropagation on a victim network or a surrogate network. Obfuscated gradient defenses have been shown to be insufficient, as stronger attacks can reduce accuracy to near zero. Defenses that withstand strong attacks tend to have a \"smooth\" loss surface with respect to the input data manifold. Variants of adversarial training and regularizers have maintained modest accuracies against strong attacks. A k-NN-based defense identifies data at training time using true labels. Soft nearest neighbor regularizer improves robustness to evasion examples. Resistance to clean-label poisoning examples has yet to be explored. Backdoor attacks, a subset of data poisoning, modify training examples with a trigger to manipulate classification during inference. These attacks rely on the ability to modify data at inference time, which may not always be realistic. Various defenses to backdoor attacks have been proposed. Various defenses have been proposed to counter backdoor attacks and poisons, aiming to sanitize training data by removing poisons to neutralize their effects. One defense involves removing data falling outside an acceptable radius in feature space, while another projects feature representations onto the line connecting class centroids for data removal. Another defense uses feature clustering for data sanitization. Data sanitization is a defense against poisoning attacks, but it may fail against stronger methods that do not use uniform triggers. One such attack involves surrounding a target image with poisons in feature space. Another defense, NeuralCleanse, aims to detect triggers causing misclassification by removing inputs with similar activations. In response to insidious poisoning attacks using variable perturbations, baseline defenses are introduced for comparison in controlled experiments. Input space representations x_t and x_b are defined for target and base images, with x_w being a watermarked image. The goal is to misclassify the target image with true label l_t as having label l_b. The k-nearest neighbors (k-NN) defense uses the activation of the penultimate layer of a neural network to classify images. It takes a vote amongst the labels of a point's k nearest neighbors in feature space to determine if the point is anomalous. If the assigned label is not the mode amongst the neighbors, the point is discarded. The k-NN defense involves a tradeoff between removing poisons and maintaining accuracy. Memory-intensive implementations can be optimized by sampling or using fast k-NN methods. The \"L2\" defense removes points farthest from class centroids to enhance security. The L2 defense relies on calculating the centroid position to filter outliers, which is vulnerable to data poisoning with small class sizes. It is adapted from traditional poison defenses and not specific to neural networks. Robust feature extractors, trained with adversarial examples, are tested as a defense. This defense does not filter out poisons but ensures deep features of poisons are not near the target before retraining for the feature collision attack. The one-class SVM defense examines deep features of each class in isolation, using a radial basis kernel with a value \u03bd = 0.01 to identify outliers. The random defense filters out a random subset of training data, calibrated to be 1% on the feature collision attack and 10% on the convex polytope attack. The robustness of image classification models with the k-NN defense and other anomaly detection defenses are tested on the CIFAR-10 dataset. Poisoning examples were generated to cause misclassification of the target after network retraining, following attack experiments from previous studies. The watermarking technique is used to optimize poisons for successful attacks on a modified Alexnet network. The hyperparameter \u03b2 is set to 0.1, and poisons are trained over 10 epochs with a batch size of 128. Defenses against targeted misclassifications are evaluated using 50 poisons, resulting in a 100% attack success rate. The k-NN defense with k=5000 successfully identifies almost all poisons in attacks, filtering only 0.6% of non-poison datapoints. Setting k as the class size of the training data reduces the attack success rate to 0%. The L2 defense also filters out roughly half of the poisons, with a proportion of 0.01 of the training data being poisons. Large k does not noticeably reduce classification accuracy in the convex polytope attack, which generates 5 poisons to attack multiple architectures in a transfer learning setting. Eight architectures are tested, with two in a black box setting and six in a grey box setting. The defense strategies tested on eight different architectures, including DenseNet121 and ResNet18, showed that the k-NN and L2 defenses effectively filtered out poisons with minimal impact on non-poisons. The attack success rates were significantly reduced with these defenses, except for GoogLeNet under the L2 defense. Surprisingly, the 1-class SVM defense did not perform better on black-box architectures despite filtering a higher percentage of poisons. A feature-space visualization of the k-NN defense showed the filtered poisons and non-poisons. Figure 4 displays a visualization in feature space of fine tuning data points in target and base classes, following a projection scheme. The defense successfully filters out poisons around the target and outlying points in the target class, with no points in the base class filtered. The attack success rate using the k-NN defense is very low. Figure 5 shows a feature space visualization of the robust feature extractor defense on ResNet18 causing a failed attack. The defense mechanism using the k-NN approach effectively filters out poisons in feature space, leading to a failed attack on ResNet18. Test set accuracy drops to 79.8%, compared to over 92% with other defenses. The normalized distance from poisons to target is shown in Figure 6, demonstrating the effectiveness of the k-NN defense against clean-label poisoning attacks. The k-NN defense effectively filters out poisons in feature space, leading to a failed attack on ResNet18. It outperforms other baselines against existing attacks and provides benchmarks for measuring future defense-aware clean label attacks. Visual differences between filtered and non-filtered nonpoisons are not distinctive."
}