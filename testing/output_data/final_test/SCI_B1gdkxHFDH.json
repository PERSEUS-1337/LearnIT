{
    "title": "B1gdkxHFDH",
    "content": "We propose a fair approach to training machine learning models that are invariant to certain feature perturbations. This notion of fairness is connected to individual fairness and aims to address gender and racial biases in AI systems used in various domains like credit, hiring, criminal justice, and education. AI systems may perpetuate biases in training data, affecting underprivileged groups. The scientific community has proposed definitions of algorithmic fairness, but the abundance of incompatible definitions hinders adoption by practitioners. Group fairness and individual fairness are two types of formal definitions, with group fairness being more common due to its statistical analysis. In this paper, the focus is on individual fairness in ML models, which treat similar users similarly. The leading notion is metric fairness, which requires that the distance between outputs for similar inputs is limited by a factor L times the distance between the inputs. This metric encodes the intuition of how samples should be treated similarly by the model. Individual fairness in ML models focuses on treating similar users similarly. The concept of metric fairness ensures that the distance between outputs for similar inputs is limited by a factor L times the distance between the inputs. Despite its benefits, individual fairness is considered impractical due to ambiguity in choosing distance metrics. Dwork et al. (2011) suggest using probability metrics for one distance and leaving the choice of the other to regulatory bodies or civil rights organizations. In this paper, two data-driven choices of fair metrics are considered for ML models, one for observed sensitive attributes and another for unobserved attributes. An adversarial approach is used to train individually fair ML models, showing a connection between individual fairness and robustness. The paper is organized into four main sections, with Section 2 focusing on investigating algorithmic bias in ML models. The curr_chunk discusses bias and unfairness in ML models, proposing a training method to address these issues. It includes a theoretical investigation and an empirical study on the efficacy of the approach. The example of auditing an AI system for unfairness is used to illustrate the concept, highlighting the importance of investigating potential biases in systems. The curr_chunk discusses racial discrimination in the labor market and proposes an optimization problem to find inputs where the predictor performs poorly. It assumes Y is discrete and introduces sensitive directions provided by a subject expert or estimated from data. The goal is to have a metric insensitive to feature perturbations. The curr_chunk introduces a metric insensitive to feature perturbations within the sensitive subspace ran(A). It defines a transportation cost function on Z and uses the fair Wasserstein distance to compare probability distributions on Z. This distance helps determine if an AI system performs disparately on comparable samples. The investigator uses audit data to measure AI system performance with a nonnegative loss function. A budget parameter is set to detect discrepancies in performance without moving samples to incomparable areas. Equation 2.1 detects aggregate violations of individual fairness, emphasizing collective violations rather than individual ones. The implicit notion of fairness in the equation includes a small tolerance for discrepancies. The investigator uses audit data to measure AI system performance with a nonnegative loss function, emphasizing collective violations of fairness in equation 2.1 with a small tolerance for discrepancies. The dual of equation 2.1, equation 2.3, involves optimizing the c-transform function, which reveals unfairness in the AI system. The unfair map T \u03bb maps samples in the audit data to areas where the system performs poorly, highlighting discrepancies in performance. The unfair map T \u03bb reveals disparate treatment in the AI system by localizing unfairness to certain areas of the sample space. Fairness is illustrated through robustness using a binary classification dataset with two subgroups, where the decision heatmap of a vanilla logistic regression is shown. The decision heatmap of a vanilla logistic regression shows poor performance on the blue minority subgroup. A fair metric downweights differences in the horizontal direction, but a classifier based on this metric is unfair according to equation 2.4. A comparison is made with Dwork et al. (2011) on notions of fairness in ML models. Both definitions emphasize that fair models should perform similarly on similar inputs, with some differences between them. The main differences between the two definitions of fairness in ML models are that the first focuses on the output being similar to the training label, while the second considers differences between datasets. The second definition also incorporates the Wasserstein distance for fairness evaluation. The modified notion of individual fairness in training supervised learning systems is achieved by optimizing equation 2.1 efficiently. By using the Wasserstein distance induced by the fair metric, the system is robust to sensitive perturbations, encouraging individual fairness. This approach is an instance of distributionally robust optimization, ensuring the AI system performs well on all hypothetical populations of users with perturbed sensitive attributes. The approach to fair training involves minimizing objectives through distributionally robust optimization (DRO), ensuring system performance across various populations. The algorithm described minimizes equation 2.7 by learning a classifier insensitive to perturbations, utilizing adversarial training algorithms. The approach to fair training involves minimizing objectives through distributionally robust optimization (DRO) with uncertainty sets defined by various constraints and distances between probability distributions. Hashimoto et al. (2018) showed that DRO with a \u03c7 2 -neighborhood prevents representation disparity, while using a Wasserstein uncertainty set encodes individual fairness based on the sample space geometry. Our approach to fair training involves encoding individual fairness in the Wasserstein distance, similar to adversarial training methods. Various papers have explored adversarial approaches to algorithmic fairness, including enforcing equalized odds and considering uncertainty sets in training data. The approach involves encoding individual fairness in the Wasserstein distance for fair training. Previous research has explored adversarial methods for algorithmic fairness, such as demographic parity and fairness in text classification. The method aims to train individually fair ML models and certify fairness a posteriori through uniform convergence results for the DR loss class. The text discusses the discrepancy in the transportation cost function and the assumptions made for the ML task, including bounded feature space and non-negative, bounded functions. The error in the cost function diminishes in the large-sample limit with consistent estimators. The text discusses the uniform convergence result in terms of the entropy integral of the loss class, with Proposition 3.1 stating the convergence with probability at least 1 \u2212 t. It highlights the novelty of allowing error in the transportation cost function, which may impact the rate at which the uniform convergence error vanishes. This leads to individually fair classifiers in SenSR, defined as ML models that are fair on an individual level. Individually fair ML models have a small gap between the optimal value of the investigator's optimization problem and the risk. This gap implies that moving samples from one distribution to another does not significantly increase the loss. Proposition 3.2 shows that under certain assumptions, a small gap indicates individual fairness. Uniform convergence error is also discussed, with equation 3.5 serving as a certificate of individual fairness for ML models. In this section, results from using SenSR to train individually fair ML models for sentiment analysis and income prediction tasks are presented. The sentiment analysis task involves classifying words as positive or negative using GloVe embeddings and a neural network. The income prediction task deals with structured inputs and the sensitive attribute is observed. The implementation details can be found in Appendices C and D. The study presents results of training a neural network for sentiment analysis with 1000 hidden units, achieving 95% accuracy but violating individual fairness. To ensure fairness, sentiment scores for names should be the same. The fairness of the classifier is evaluated using male and female names from Caucasian and African-American groups. Individual fairness requires sentiment to be consistent across all names. The study compares sentiment differences between Caucasian and African-American names, as well as male and female names. They also analyze sentiment differences between sentences mentioning different cuisines to test generalization. The study uses 94 names as sensitive directions for evaluation. The study uses expert knowledge and a side dataset of popular baby names in New York City to define a sensitive subspace for fairness evaluation. The embeddings of these names form a group of comparable samples, reducing factor analysis to SVD. Results show significant race and gender gaps in sentiment analysis using a baseline neural network classifier. The study uses expert knowledge and a side dataset of popular baby names in New York City to define a sensitive subspace for fairness evaluation. Results show significant race and gender gaps in sentiment analysis using a baseline neural network classifier. Three other approaches were considered, with SenSR being the only one achieving individual fairness by practically eliminating gender and racial gaps. Using expert knowledge further improved fairness for SenSR-E. Utilizing expert knowledge improved fairness for SenSR-E, leading to a fairness overfitting effect but still showing improvement over other methods. Generalization check using SVD of a larger dataset of names resulted in better generalization. Fairness over-fitting is possible, highlighting the need for datasets and procedures to verify fairness generalization. SenSR's applicability extends beyond natural language processing tasks. SenSR's broad applicability was demonstrated outside of natural language processing tasks by applying it to a classification task on the Adult dataset to predict income levels. Gender and race were key features considered, with a focus on fairness in predictions to address disparities in pay. Metrics like spouse consistency and gender/race consistency were used to assess individual fairness in classifications. In the study, various group fairness measures were reported with respect to race or gender based on true positive rates. Metrics like Gap RMS R, Gap RMS G, Gap max R, and Gap max G were used to assess fairness. Balanced accuracy was used to measure predictive ability in predicting income levels. The study evaluated group fairness measures based on race or gender using true positive rates. Metrics like Gap RMS R, Gap RMS G, Gap max R, and Gap max G were used. The fair metric was learned using logistic regression to classify gender. Results showed individual and group fairness in classifiers like Baseline, Project, CoCL, Adversarial Debiasing, and SenSR. SenSR, a single hidden layer neural network, outperforms other classifiers in group fairness measures and individual fairness. It shows significant improvement in S-Con and group fairness compared to Baseline, Project, CoCL, and Adversarial Debiasing. SenSR's performance generalizes well on sensitive directions related to race and gender, indicating its effectiveness in addressing fairness issues. The text discusses the concept of group fairness versus individual fairness in machine learning systems. It explores the idea of training ML models that are fair by being invariant under perturbations in a sensitive subspace. The lack of consensus on a fair metric for individual fairness is identified as a barrier, leading to the consideration of two approaches for learning a fair metric from data. An algorithm is proposed to train individually fair ML models based on a data-driven choice of fair metric. The text discusses obtaining sharper bounds under additional assumptions on loss and transportation cost functions. The bound is kept general to maintain flexibility. The functions in the loss class are assumed to be L-Lipschitz. The functions in F are bounded, implying bounded differences and concentration. The Rademacher complexity of the DR loss class is bounded, implying sharp concentration around its expectation. The loss function is bounded, allowing for bounds on the difference between expectations. Uniform convergence results apply to bounded loss classes. The approach focuses on learning the sensitive subspace using a softmax regression model on a subset of training data with the sensitive attribute. The fair metric is defined based on the span of the sensitive subspace. This method can be extended to non-discrete sensitive attributes by using a generalized linear model. The sensitive attribute is not required during classifier training or testing, making it applicable even when the attribute is unavailable due to privacy concerns. The main barrier to wider adoption of individual fairness is disagreement over the fair metric. Learning a fair metric from human supervision involves a generalized Mahalanobis distance using groups of comparable samples. This approach is based on a factor model that considers the sensitive/irrelevant attributes of the data. The learned representations are embeddings of words in the vocabulary, with the sensitive attribute being gender bias. The goal is to minimize equation B.2 when v1 is similar to v2. One possible choice for \u03a3 is the projection matrix onto the orthogonal complement of ran(A), estimated from learned representations and comparable samples by factor analysis. Comparable samples have similar relevant attributes, allowing for estimation of ran(A) from the learned representations. The approach involves estimating ran(A) from learned representations and comparable samples using factor analysis. Algorithm 3 outlines the process for estimating \u03a3 in the fair metric, accompanying the implementation of the SenSR algorithm. Class imbalance is addressed by subsampling training samples to ensure an equal number of observations per class on each epoch. The SenSR algorithm implementation includes two inner optimization problems - subspace perturbation and full. The SenSR algorithm involves two inner optimization problems - subspace perturbation and full perturbation. These are implemented using the Adam optimizer for efficiency. Different learning rates are used for the subspace and full steps, while the parameters optimizer uses a learning rate of 0.001. The SenSR algorithm involves two inner optimization problems - subspace perturbation and full perturbation. Different learning rates are used for these steps, with observations on the impact of setting the rates too small or too big. Malfunctioning behaviors can be corrected during training without the need for hyperparameter optimization tools. The data is imbalanced, with 25% earning at least $50k per year and demographic and class imbalances present. In the study, race, gender, and class imbalances were observed in the outcomes, with 86% of individuals being white and 67% being male. 26% of white individuals and 31% of males earn at least $50k a year. Regularized logistic regression was used to classify females and males, with specific hyperparameters and data splits detailed in the experiment results. Minibatches were sampled to address class imbalance during training. The study used adversarial debiasing to address race and gender disparities in outcomes, with specific hyperparameters such as adversary loss weight, num epochs, and batch size defined. Classes were defined based on income levels, race, and gender, with TPR values calculated for different scenarios."
}