{
    "title": "S1ly10EKDS",
    "content": "Temporal difference (TD) learning is a popular algorithm for policy evaluation in reinforcement learning. A variance reduced TD (VRTD) algorithm was proposed to address optimization variance issues. This work corrects technical errors in the analysis of VRTD and shows its non-asymptotic convergence and variance reduction performance. VRTD converges to a neighborhood of the fixed-point solution of TD at a linear rate, reducing variance and bias errors compared to vanilla TD. In reinforcement learning, policy evaluation is crucial for identifying the optimal policy that maximizes cumulative rewards over time. The temporal difference (TD) learning algorithm is widely used for policy evaluation, utilizing the Bellman equation to update the value function incrementally. When the state space is large, function approximation is often used for efficient value function estimation. TD with linear function approximation has been shown to converge to a fixed point solution with i.i.d. and Markovian samples. The finite sample analysis of TD learning focuses on reducing variance to improve convergence accuracy. Two approaches proposed are batch TD, which transforms the mean square projected Bellman error into a convex-concave saddle-point problem, and using diminishing stepsize or small constant stepsize to reduce variance. The curr_chunk discusses applying stochastic variance reduction techniques to improve the performance of batch TD algorithms. It mentions that SVRG and SAGA can be used for this purpose, with two variants of SVRG proposed to save computation costs. Additionally, the TD with centering (CTD) algorithm is introduced to reduce variance in the original TD learning algorithm. The curr_chunk introduces Variance Reduced TD (VRTD) as an improvement to the original TD learning algorithm. VRTD applies variance reduction directly to TD learning, leading to better convergence accuracy compared to vanilla TD learning. However, technical errors in the analysis of VRTD have been pointed out in follow-up studies. Szepesv\u00e1ri (2017) discusses technical errors in the analysis of VRTD, highlighting the need to reanalyze its impact on TD learning convergence accuracy. The paper addresses questions regarding the effectiveness of variance reduction in improving convergence accuracy over vanilla TD. The paper discusses the impact of variance reduction on TD learning convergence accuracy, specifically focusing on the error reduction and improvement in accuracy with VRTD. It also addresses the bias error in vanilla TD due to sample correlation and investigates if VRTD can reduce this bias error. The main contributions include showing that a modified VRTD converges linearly with a constant step size \u03b1 and variance error at O(\u03b1/M). The VRTD method reduces variance and bias errors in TD learning by maintaining a constant step size \u03b1 and increasing batch size M for variance reduction. This approach allows for faster convergence compared to vanilla TD, which requires decreasing \u03b1 to reduce errors. The VRTD method reduces variance and bias errors in TD learning by maintaining a constant step size \u03b1 and increasing batch size M for variance reduction. This approach allows for faster convergence compared to vanilla TD, which requires decreasing \u03b1 to reduce errors. Our analysis of bias error for Markovian sampling takes a different path from previous techniques, showing that correlation among samples is eliminated due to concentration to a deterministic average. This analysis explains how variance reduction can also reduce bias error in on-policy TD learning. On-policy TD learning aims to minimize the Mean Squared Bellman Error (MSBE) by drawing samples independently from the stationary distribution of the MDP. Convergence has been studied for TD with linear function approximation and overparameterized neural network approximation. Variance reduction techniques have been proposed to improve the efficiency of TD algorithms. This paper aims to provide a technically solid analysis for VRTD, a variance reduction technique for TD algorithms. Various variance reduction methods have been incorporated into batch TD learning algorithms to improve efficiency. The paper focuses on VRTD, a variance reduction technique for TD algorithms, and excludes other topics like asymptotic convergence and off-policy TD learning. It addresses the problem of value function evaluation in a Markov decision process. The paper discusses VRTD, a variance reduction technique for TD algorithms, focusing on value function evaluation in a Markov decision process using a finite action set and stationary policy \u03c0 mapping states to actions via a probability distribution. The transition kernel P determines the probability of being in the next state, and the reward r is bounded by r max. The Markov chain is assumed to be ergodic, with the induced stationary distribution denoted as \u00b5 \u03c0. The value function for a policy \u03c0 is defined using fixed basis feature functions and a parameter vector \u03b8. Linear function approximation is used with a feature matrix \u03a6. The linear function approximation in VRTD aims to find the fixed-point parameter \u03b8 using a TD learning algorithm with iterative updates. Samples are generated from the distribution \u00b5 \u03c0, and the mean gradient g(\u03b8) is defined as A\u03b8 + b. The iteration converges to the fixed point \u03b8 * = \u2212A \u22121 b at a sublinear rate. The iteration in eq. (1) converges to the fix point \u03b8 * = \u2212A \u22121 b at a sublinear rate O(1/t) with diminishing stepsize \u03b1 t = O(1/t) using both Markovian and i.i.d. samples. Standard assumptions are made regarding problem solvability, geometric ergodicity, and the properties of the matrix A. The VRTD algorithm proposed in Korda and La (2015) aims to reduce variance in TD learning by using nested loops for updates with batch samples. The VRTD algorithm by Korda and La (2015) reduces variance in TD learning using nested loops for updates with batch samples. In each epoch, a batch of M samples is acquired, and a batch gradient gm(\u03b8m\u22121) is computed as an estimator of the mean gradient. Inner-loop updates randomly select one sample from the batch and update the corresponding component in gm(\u03b8m\u22121). The projection operator \u03a0R\u03b8 in Algorithm 2 denotes projection onto a norm ball with radius R\u03b8, similar to the SVRG algorithm by Johnson and Zhang (2013). The use of batch gradients at each inner-loop update aims to reduce gradient variance. Technical errors in the analysis of VRTD in Korda and La (2015) are pointed out, leading to incorrect variance reduction performance for VRTD. The batch gradient gm(\u03b8m\u22121) introduces a non-vanishing variance error due to not exactly equaling the mean gradient g(\u03b8m\u22121). The analysis of VRTD in Korda and La (2015) is criticized for technical errors, leading to incorrect variance reduction performance. The gradient estimator does not exactly equal the mean gradient, causing a non-vanishing bias error in the convergence bound. The exact convergence result contradicts the understanding in recent studies, which show variance and bias errors in vanilla TD. In Appendix A, a counter-example is provided to challenge the convergence bound in Korda and La (2015). The paper aims to rigorously analyze VRTD's variance reduction performance, focusing on two types of errors: variance error from gradient estimation and bias error from Markovian sampling. The study compares VRTD to vanilla TD, showing advantages under i.i.d. and Markovian sampling. Applying VRTD to i.i.d. samples eliminates bias error from time correlation among samples. Algorithm 1 is a modification of VRTD designed to avoid bias error in convergence analysis with i.i.d. samples. It draws new independent samples from the stationary distribution for updates, preventing correlation with batch gradients. This approach eliminates extra bias error and improves convergence. The VRTD algorithm, a modification of Algorithm 1, aims to avoid bias error in convergence analysis by using i.i.d. samples. It draws independent samples from the stationary distribution for updates to prevent correlation with batch gradients, improving convergence. The analysis shows that Algorithm 1 has guaranteed convergence only to a neighborhood of \u03b8*, with the error term being controlled by choosing an appropriate batch size M for variance reduction. The non-asymptotic convergence of Algorithm 1 is precisely characterized in Theorem 1. Theorem 1 establishes the non-asymptotic convergence of Algorithm 1, showing linear convergence to a neighborhood of the fixed point solution with an error term of O(\u03b1M). This contrasts with vanilla TD, which has a constant error term of O(\u03b1) for a fixed stepsize, requiring a smaller stepsize to reduce variance error but slowing down practical convergence. The VRTD algorithm achieves high accuracy with fast convergence, unlike vanilla TD. With access to the mean gradient in each epoch, Algorithm 1 converges linearly to the fixed point solution. The proof of convergence is different from SVRG due to the absence of an objective function in the TD learning problem. The VRTD algorithm (Algorithm 2) in TD learning exploits the Bellman operator's structure for parameter convergence. It uses Markovian samples from a single MDP path, leading to bias and variance errors. The bias is defined at each iteration, and analysis shows epochwise contraction and variance in gradient estimation. The variance term in the VRTD algorithm decays as batch size increases, capturing bias from sample correlation. Lemma 1 quantifies bias error controlled by batch size M, dependent on Markov chain convergence and stationary distribution. The returning time of a Markov chain depends on the stationary distribution. The bias error decreases as batch size increases in VRTD algorithm, reducing randomness in gradient estimation. The VRTD algorithm aims to reduce bias error by choosing a large batch size, allowing for a constant stepsize and fast convergence. Theorem 2 states that VRTD with Markovian samples converges to a neighborhood of \u03b8 *. The VRTD algorithm with Markovian samples converges to a neighborhood of \u03b8* at a linear rate, with the convergence error decreasing sublinearly with batch size M. Variance reduction in Algorithm 2 reduces both variance and bias of the gradient estimator. Numerical results for an MDP with \u03b3 = 0.95 and |S| = 50 are provided to verify theoretical results. The VRTD algorithm with Markovian samples converges to a neighborhood of \u03b8* at a linear rate, with the convergence error decreasing sublinearly with batch size M. Experimental results for different batch sizes M = 1, 50, 500, 1000, 2000 are presented in Figure 1, showing the square error over 1000 independent runs. The convergence analysis for VRTD with both i.i.d. and Markovian samples shows that the error decreases as batch size increases, supporting Theorem 1 and Theorem 2. Larger batch sizes reduce error without significantly slowing down convergence. VRTD with i.i.d. samples has smaller errors than VRTD with Markovian samples, indicating that Markovian samples introduce additional errors due to sample correlation. Our analysis of VRTD shows reduced variance and bias errors with increasing batch size. The technique developed to bound the bias of the VRTD gradient estimator demonstrates its advantage over vanilla TD. This variance reduction technique can be applied to other RL algorithms. Additionally, a counter-example disproves a claim made in a previous study regarding convergence bounds. The counter-example disproves a claim in a previous study regarding convergence bounds for VRTD. The erroneous steps in the proof lead to the conclusion that the statements in the theorem cannot hold. The counter-example disproves a claim in a previous study on VRTD convergence bounds. Lemma 2 provides bounds for i.i.d. and Markovian samples. Lemma 3 introduces G = (1 + \u03b3)R \u03b8 + r max. Algorithm 2 is analyzed for any m > 0 and 0 \u2264 t \u2264 M \u2212 1. Lemma 5 discusses Algorithm 2 with Markovian samples. The proof of Theorem 1 proceeds in the following steps. In Step 1, within the m-th epoch, the error of the last update is decomposed. Expectations are taken, and inequalities are used to derive results for all iterations within the epoch. Step 1: Inequality bounds are derived and used to decompose the error in each iteration within an epoch. Step 2: Variance error is bounded using Lemma 4. Step 3: Iterative process over m epochs is described, with expectations taken and results derived. Step 4: Lemma 1 is proven to aid in the main proof. The text discusses the application of concentration inequalities over Markov chains to bound certain equations. Theorem 3 from Dedecker and Gou\u00ebzel (2015) is introduced, providing a concentration bound for irreducible aperiodic Markov chains. This is relevant for proving Lemma 1, which is useful for the main proof in Theorem 2. The text discusses applying concentration inequalities to bound equations in Markov chains. Theorem 3 provides a concentration bound for irreducible aperiodic Markov chains, which is crucial for proving Lemma 1 and Theorem 2. The proof involves deriving bounds and decomposing errors in iterations within inner loops. The text discusses deriving bounds and decomposing errors in iterations within inner loops of Markov chains. By applying concentration inequalities, a concentration bound for irreducible aperiodic Markov chains is provided, crucial for proving Lemma 1 and Theorem 2. Taking expectations and arranging terms in equations lead to bias and variance errors in the analysis. The text discusses deriving bounds and decomposing errors in iterations within inner loops of Markov chains. Without loss of generality, computations required by VRTD under i.i.d. sampling to attain an -accuracy solution is at most 2mM gradient computations. Comparing with the vanilla TD algorithm, VRTD requires fewer gradient computations to obtain an -accuracy solution in the Markovian setting. In the Markovian setting, VRTD outperforms vanilla TD in terms of computational complexity by reducing bias and variance errors. VRTD keeps bias and variance errors at the same level, while vanilla TD requires more iterations due to bias error dominance, resulting in higher computational complexity. The section provides an analysis of computational complexity in achieving an -accuracy solution, based on convergence results from previous studies. Transition probabilities and feature matrix components are randomly sampled. Ground truth value of \u03b8 * is calculated for error evaluation. Different batch sizes are used in experiments with vanilla TD and VRTD. Convergence process is shown in Figure 2. In experiments comparing TD and VRTD on the Mountain Car game in OpenAI Gym, VRTD achieves smaller error with larger batch sizes. The performance of VRTD is quantified using the norm of the expected TD update. The state sample is transformed into a 20-dimensional feature vector for analysis. In the experiment, the state sample is transformed into a 20-dimensional feature vector using an approximation of a RBF kernel. The agent follows a random policy, with \u03b8 0 = 0, starting from the lowest point and receiving a reward of -1 at each time step. VRTD with batch size M = 1000 achieves smaller NEU than vanilla TD, as shown in Figure 3. Additional experiments compare the performance of VRTD with constant stepsize to the TD algorithm. In a comparison between VRTD and TD algorithms using Frozen Lake setting, VRTD with batch size M = 5000 and stepsize \u03b1 = 0.1 outperforms TD with a changing stepsize. VRTD reaches the required accuracy faster than TD in terms of squared error versus total gradient computations."
}