{
    "title": "HJGsj13qTE",
    "content": "The paper investigates how input images are compressed through transformations in Convolutional Neural Networks (CNNs) using singular value decomposition to analyze variations in feature space. The effective dimension of the embedding is studied across layers within a class, showing an initial increase before decreasing deeper into the network. The decrease rate of the effective dimension correlates with the model's training performance. The effective dimension deep in the network corresponds with the model's training performance. DNNs are powerful function approximators due to their ability to learn non-linear mappings. Previous research has explored visualization techniques for understanding feature spaces but does not formalize how transformations simplify high-dimensional input to low-dimensional output. Compression techniques have been explored in neural networks, focusing on the intrinsic dimension of the optimization landscape. Antognini and SohlDickstein studied low dimensional visualizations of parameter spaces during training. Dittmer et al. proposed a method to interpret ReLU layers for distinguishing correctly and incorrectly classified data. The text discusses how convolutional neural networks (CNNs) map high-dimensional datasets into lower-dimensional distributions during classification tasks. It explores how learned transformations in CNNs identify relevant features and remove uninformative attributes from deep feature spaces. The concept of effective dimensionality of a feature space is proposed to analyze sample complexity and understand changes in input image dimensionality through linear and nonlinear transformations in neural networks. The text discusses how neural networks transform images through linear and nonlinear processes, using singular value decomposition to analyze variances. It shows that the effective dimension of a class increases before decreasing across network layers, leading to sharp decision boundaries. The experiments focus on datasets like CIFAR-10 and Tiny ImageNet, subsampling images for faster computation. In this work, images from CIFAR-10 and Tiny ImageNet datasets are preprocessed for analysis. CIFAR-10 is chosen for initial experiments due to its computational efficiency, while Tiny ImageNet is used to validate findings. Images are subsampled and resized for analysis, focusing on training data transformations. For analysis, images from CIFAR-10 and Tiny ImageNet datasets are preprocessed. 10 classes are randomly sampled from 200 classes with 100 representative images per class. Three architectures are studied: a 12-hidden layer MLP, a CNN similar to VGG-16, and a CNN similar to VGG-19. The MLP has 1000 hidden units and is trained until convergence with stochastic gradient descent. VGG-16 weights are used for CIFAR-10, and for Tiny ImageNet, VGG-16 architecture is utilized. The VGG-16 weights for CIFAR-10 are used from BID7, while for VGG-16 on Tiny ImageNet, the default implementation in Keras is utilized from BID8. Data augmentation techniques such as shifts, rotation, and horizontal flipping are applied to all models. The classification accuracy of these models is presented in TAB0, indicating the quality of class separation in the final hidden layer. Activation matrices \u03a6 (l) are analyzed to understand how neural networks compress the feature space within a class. The text discusses dimensionality reduction using singular value decomposition (SVD) and proposes a metric for the dimensionality of feature vectors based on normalized singular values. It also mentions the importance of the trace of the covariance matrix for generalization bounds. In Section 3, the text demonstrates that the upper bound \u221a d l is loose due to the low-rank nature of activation matrices. The effective dimension of a matrix captures significant directions of variation between its rows. The work formalizes measuring complexity of feature embedding using the spectrum profile and studies transformations by neural networks. Singular values of activation matrices are computed to evaluate the effective dimensionality of data within a class. Average plots over all classes are presented, with singular value plots in the Appendix. The effective dimension of inputs increases before decreasing for tested architectures (VGG-16, VGG-19, MLP12) and datasets (CIFAR-10, Tiny ImageNet). The number of important directions of variation initially increases, leading to spherized data points, then decreases, resulting in more elliptical data. High-performing models show a sharp increase followed by a decrease in effective dimension, while lower-performing models exhibit a more gradual decay. The effective dimension of inputs increases before decreasing for tested architectures and datasets. High-performing models exhibit a sharp increase followed by a decrease in effective dimension, while lower-performing models show a more gradual decay. Geometrically, early DNN layers increase the \"sphericalness\" of the data, compressing extraneous dimensions. This early network sphericalization may abstract features common across classes. The effective dimension of inputs increases before decreasing for tested architectures and datasets. High-performing models exhibit a sharp increase followed by a decrease in effective dimension, while lower-performing models show a more gradual decay. In feature visualization, early layers extract features common across image classes, while later layers highlight these features as points separate by class. The trend in VGG-16 trained on ImageNet and tested on Tiny ImageNet is less drastic, possibly due to poorer performance by that model. The model in Figure 2 shows close to 100% training accuracy with a sharp increase followed by a decrease in effective dimension. In Figure 3, the model has close to 90% training accuracy with a similar trend in effective dimension. The correlation between a sharp decline in effective dimensionality and higher accuracy suggests that compressing embeddings into lower dimensions could improve decision boundaries in classification. The analysis involves computing t-SNE of vector activations at each layer to understand class separation. The study focuses on how effective dimension changes during training, showing initial spherization followed by dimensionality collapse in neural networks. This helps in understanding the correlation between performance and effective dimension. The compression of feature spaces in better-performing networks is more pronounced. Spectral normalization is used to scale the appendix by a factor \u03b1, preserving \u03c6 (l+1). The effective dimension decreases in high-performing networks, with smaller singular values decaying strictly. This decay dominates the effective dimension reduction. The effective dimension decreases in high-performing networks, with smaller singular values decaying strictly in later layers."
}