{
    "title": "BJxRrlBFwB",
    "content": "In a rich multi-player text-based fantasy environment, agents and humans engage in both actions and dialogue. A goal-oriented model trained with reinforcement learning and an imitation-learned chit-chat model outperform a strong inverse model baseline, showing the ability to converse naturally to achieve goals. In artificial dialogue agent literature, goal-oriented dialogue involves achieving specific tasks, while chit-chat focuses on imitating human small talk. Goal-oriented datasets cover specialized tasks like booking restaurants or obtaining weather information, providing clearer metrics of success. Chit-chat agents may focus on statistical modeling. Chit-chat agents focus on statistical regularities in dialogue data without accurately modeling underlying meaning, covering a wide range of language. Goal-oriented dialogue involves achieving specific tasks, requiring world knowledge and implicit goals. This study explores goal-oriented dialogue agents in a text-based fantasy environment. The environment in the text-based fantasy setting is rich and diverse, allowing for a wide range of interactions between characters and objects. The goal is to conduct open-ended dialogues to prompt specific actions from dialogue partners, whether emote actions or game actions. This task is ideal for bridging the gap between statistical chit-chat agents and goal-oriented dialogue agents. The task involves training models using reinforcement learning and self-play to achieve a wide range of actions in a rich fantasy environment. The environment consists of various locations, characters, and objects, with human-human gameplay episodes. One agent is trained with imitation learning on interactions, while the other agent conducts dialogue based on goals. The RL agent is trained to conduct dialogue based on goals in a multi-user text-based game environment. Two different RL approaches are compared, showing that both types of RL agents can effectively learn and outperform other approaches to achieve goals in conversation. The RL agent in a multi-user text-based game environment can interact with characters through free text and emote actions, using a game engine defined as a graph. Actions in the game result in changes in the graph's state, providing players with a local view expressed in text. This context influences the dialogue utterances and subsequent actions of the characters. The LIGHT environment consists of human-written game locations, characters, and objects in a fantasy medieval setting. Crowdworkers played characters in this world, interacting with each other in pairs, resulting in 10,777 episodes with an average of 18.3 actions each. The LIGHT environment contains human-written game elements in a fantasy medieval setting. It includes 10,777 episodes of rich human play, split into train, validation, and test portions. This data can be used for training models and gaining \"common sense\" knowledge about character interactions and environments. The environment serves as a proxy for learning about the world in a simulation without the complexities of 3D graphics. Players were not given specific goals but were asked to convincingly play their character roles. The tasks involve interaction between two agents in a LIGHT scenario. One agent acts as the environment, while the other acts as the RL agent. The RL agent must conduct open-ended dialogue to achieve a specific goal action in the future. The environment agent is fixed and trained via behavioral cloning from human-human interaction data. The two agents, M env and M RL, are provided with their views of the scenario, including setting name, scenario description, character names, and persona. Each agent can only access their own persona during the interaction sequence. The RL agent is given a goal to achieve, which consists of an action to be taken. The actions of the RL agent are its utterances, while the actions of the environment agent are considered as internal mechanics. The agent M RL is given a goal to achieve, which is an action that must be executed by the other agent. An episode ends when the action is completed or after a set number of turns. The RL agent only speaks and does not perform game or emote actions to ensure it cannot force the goal to be reached by taking actions itself. Two types of goals are experimented with: game actions and emote actions. The setting is formed by assigning roles randomly and selecting a goal action from the episode. The RL agent is given a goal to achieve, which is an action that must be executed by the other agent. The state observation given to the RL model consists of the agent's setting description, utterance and action history, and the agent's goal. The environment is Markovian as the entire history and goal is given to the RL agent. The terminal reward is +1 only if the goal is achieved, and 0 otherwise. The RL agent is given a goal action to achieve, with the episode ending after n steps. The models for M env and M RL are retrieval models using the LIGHT dialogue corpus. A 12-layer bidirectional transformer is used as the base architecture, pre-trained on a large dialogue corpus and fine-tuned on the task. A biencoder is used to score retrieval candidates, with a dot product between output vectors scoring the match. The dialogue utterance is produced from the training set candidates. The training process involves selecting the utterance with the largest output from the candidate sets. Emotes and actions follow a similar procedure, with different candidate sets. Actions are based on admissible actions at the game state, while emotes have all 22 candidates available. The model is trained using cross entropy loss, considering other batch elements as negatives. The environment agent remains constant during training episodes to ensure the RL models stick to using natural language semantics. The text discusses designing RL approaches for tasks involving learning latent variables and picking correct candidates. An inverse model is trained to imitate human actions for comparison with RL models and fine-tuning. The model consists of a Bi-encoder trained on human-human game logs without goals. The text discusses training an inverse model to imitate human actions from game logs as a strong baseline for RL agents, although it may not learn to plan optimally. The model is trained in a supervised manner using cross entropy loss. The text proposes a model for training RL agents by initializing weights from a strong baseline model and fine-tuning them. It suggests training most parameters with human-human data and optimizing some with model self-chat to address language drift issues. The model consists of two components, with the first mapping observations to discrete variables. The model proposed involves a transformer T s mapping observations to a state representation and a policy chooser c selecting a discrete latent variable. Another transformer T u generates dialogue utterances based on the state representation. The model is trained using RL to choose c, not output the final utterance. Initial training involves pre-training T s with an inverse model and clustering vectorial representations. The model involves using K-means to map observations to dialogue topics, which are used as initial functions. Clusters are coherent about a topic, and 50 topics are used as actions for RL setup. Pre-training T u involves appending the topic computed by F c to the initial human-human training data. This allows the model to generate an action based on both input and topic, enabling training a policy by RL to optimize the topic during an episode. During fine-tuning, the policy is trained by RL to optimize topics in each episode. The cluster chooser P is redefined as an MLP network with 2 layers, sampling discrete actions from a categorical distribution. The state vector encodes the goal, allowing the policy to learn strategies to achieve it. The Top-K model helps keep trainable parameters low by using an inverse model for context embedding. The Top-K model uses an inverse model for context embedding to keep the number of trainable parameters small. It trains a small transformer model with context and candidate utterance embeddings, using A2C for training the policy and value function. In chit-chat dialogue research, end-to-end neural approaches are prevalent. In chit-chat dialogue research, end-to-end neural approaches are commonly used, particularly large pre-trained transformers. Retrieval models have shown effectiveness in various tasks. Traditional goal-oriented dialogue focuses on specific tasks like restaurant booking, taxi services, and trip planning. Our work adapts similar architectures to learn goal pursuit from chit-chat data without specified goals. The classical goal-oriented dialogue literature extensively studies RL for improving dialogue managers, focusing on transitions between dialogue states. Recent works have shifted towards end-to-end learning and self-play mechanisms for reinforcement learning. In contrast to negotiation tasks with limited item types, our setup involves a diverse world with 3462 object types and a large vocabulary size. RL is also used in dialogue for visual question answering, chit-chat improvement, and human-bot conversations. RL is extensively used for learning to play games. Our work focuses on using RL for dialogue and actions in multiplayer games, specifically comparing models on game and emote tasks with varying steps allowed to complete the goal. Results for seen and unseen test environments are presented in Table 2. The study compares RL models for dialogue and actions in multiplayer games, showing improvements for topic RL and top-K RL over the baseline. Training curves demonstrate smooth improvements over time, with successful utterances achieving goal actions in diverse scenarios. Random topic prediction yielded poor results, indicating the model is learning appropriate topic acts. In this paper, agents interact to achieve goals in a diverse language world, bridging chit-chat and goal-oriented dialogue. Two RL approaches are explored: learning to pick a topic or utterance for successful goal completion. These strategies lead to natural chat and successful goal outcomes, with potential for further advancements in RL algorithms for natural language interaction at scale. The text discusses interactions between agents in a language world to achieve goals, using RL approaches for successful outcomes and potential advancements in natural language interaction algorithms. The curr_chunk details various constraints and actions related to object manipulation in the task environment. The curr_chunk provides constraints and actions related to object manipulation in a task environment, including informing actors of successful actions, eating and drinking objects, wearing and wielding items, and using emote actions within the platform."
}