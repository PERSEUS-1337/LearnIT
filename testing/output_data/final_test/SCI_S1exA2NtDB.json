{
    "title": "S1exA2NtDB",
    "content": "ES-MAML is a new framework for model agnostic meta learning based on Evolution Strategies (ES). It avoids the difficulties of estimating second derivatives with backpropagation on stochastic policies. ES-MAML is simple, easy to implement, and competitive with existing methods, often yielding better adaptation with fewer queries. In this paper, the focus is on meta-learning in reinforcement learning (RL) for data efficiency. Model Agnostic Meta Learning (MAML) is a popular technique for quickly adapting to new tasks. However, implementing MAML can be challenging due to the need to estimate second derivatives of the RL reward function. The original implementation of MAML was incorrect, leading to the development of unbiased higher-order estimators like DiCE and further analysis of credit assignment mechanisms. Challenges with high variance in policy gradient methods can be addressed through control variates, adaptive hyperparameter tuning, and learning rate annealing. To address these issues, an alternative approach based on Evolution Strategies (ES) is proposed for MAML, offering advantages such as not requiring estimation of second derivatives. Evolution Strategies (ES) for MAML offer advantages like not needing to estimate second derivatives, simplicity in implementation, flexibility with adaptation operators, and the ability to use deterministic policies for safer adaptation. ES explores parameter space for optimization, unlike policy gradient methods focused on randomized actions in the environment. In the context of MAML, Evolutionary Strategies (ES) shift exploration and task identification to the parameter space, allowing for the use of deterministic policies. ES uses total rewards instead of individual state-action pairs, leading to more stable training profiles. The paper discusses MAML, ES application, and numerical experiments. In Section 4, numerical experiments on exploration, compact architectures, deterministic policies, and comparisons with existing MAML algorithms in few-shot scenarios are presented. The original formulation of MAML involves reinforcement learning tasks with common state and action spaces, each task having an associated Markov Decision Process with transition distribution, episode length, and reward function. Stochastic and deterministic policies are discussed. Additional details can be found in the Appendix. The MAML problem involves finding a meta-policy that can quickly adapt to solve unknown tasks by taking a few policy gradient steps. Standard MAML approaches use policy gradients to compute the gradient of the objective function for training, known as PG-MAML. The adaptation operator contains important information for the optimization process. The original implementation of MAML incorrectly estimates the term (3), causing training to lose 'pre-adaptation credit assignment'. Variance in estimating (3) can be high, hindering training. Extensions like ProMP and T-MAML aim to address this issue with new estimators and control variates, but come with drawbacks. The DiCE estimator has drawbacks such as complicated solutions, problematic Hessian estimate variance, and unknown estimator bias. PG-MAML faces issues with stochastic policies leading to risky exploration behavior, especially in robotics applications with varying system dynamics. These challenges lead to the proposed ES-MAML method in Section 3. The ES-MAML method aims to improve the standard MAML algorithm by incorporating Evolution Strategies (ES) techniques for enhancing both the meta-learning training algorithm and the efficiency of the adaptation operator. This approach differs from other biologically-inspired algorithms and focuses on rigorous mathematical definitions for smoothings and approximations. Evolution Strategies (ES) techniques are used in Reinforcement Learning (RL) to optimize a blackbox function that calculates the total discounted reward of a policy in a given environment. Instead of directly optimizing the function, a smoothed objective is optimized. The ES-gradient is computed using Gaussian smoothing and can be approximated using Monte Carlo samples. Various unbiased estimators with reduced variance can be obtained by modifying the algorithm. The Forward-FD estimator involves subtracting the current policy value, while the antithetic estimator is also used in ES literature. The antithetic estimator in Evolution Strategies (ES) is preferred over the basic version for optimizing policy values. Algorithm 1 is used for computing the gradient, with various variants available. ES-MAML allows for direct use of new ES enhancements, treating the cumulative reward function as a blackbox. The MAML problem involves maximizing the cumulative reward of policy \u03b8 without needing to calculate second derivatives. The proposed algorithm optimizes the Gaussian smoothing of the MAML reward using ES methods, avoiding the challenge of estimating the Hessian. The algorithm (zero-order) ES-MAML optimizes the Gaussian smoothing of the MAML reward using ES methods. It involves estimating the adaptation operator U(\u00b7, T) without needing to calculate second derivatives. The ES-MAML algorithm optimizes the MAML reward using ES methods, estimating the adaptation operator U(\u00b7, T) without second derivatives. Algorithm 3 specifies the adaptation operator using ES gradient, while a first-order method can be applied directly to the MAML reward. This formulation, discussed in Appendix A.1, involves additional smoothing in equation 6 to eliminate the gradient of U(\u00b7, T) and the Hessian of fT. Algorithm 2 allows for flexibility in choosing new adaptation operators, such as modifying the ES gradient step to improve the estimator. Various techniques for enhancing the ES gradient estimator are discussed in Appendix A.3. DPP sampling is used to select diverse samples for estimating the ES gradient in U(\u00b7, T). It can help improve adaptation in MAML by reducing variance in the gradient estimator. Nondifferentiable operators like local search can also be used in Algorithm 2. The search method for selecting the best policy in a neighborhood of \u03b8 involves hill climbing, which iteratively perturbs candidate policies to improve rewards. This method ensures monotonic progress and is beneficial for challenging problems. The hill climbing method, along with genetic algorithms and CMA-ES, are competitive with gradient-based methods for solving difficult RL tasks. The performance of MAML algorithms can be evaluated based on the final meta-policy's ability to consistently produce better adaptations. Data efficiency in meta-learning corresponds to adapting with a low number of queries. In this section, experiments evaluate ES-MAML and PG-MAML in terms of data efficiency (K) and meta-training time. ES uses cumulative reward over an episode, while policy gradients use every state-action pair. Despite using less information, ES often matches or exceeds policy gradients in practice. ES often matches or exceeds policy gradients in practice, with advantages such as robustness against delayed rewards, action infrequency, and long time horizons. ES may also offer benefits in terms of wall-clock time due to its parallelizability over CPUs and not requiring backpropagation. Two experiments on environments with sparse rewards demonstrate the advantages of ES in traditional RL and MAML. In a study on environments with sparse rewards, ES-MAML effectively explores and adapts to tasks like the four corners benchmark. The meta-policy \u03b8* travels in circular trajectories to find the target corner, showcasing the benefits of ES in exploration. ES-MAML effectively explores and adapts to tasks like the four corners benchmark without requiring modifications for exploration. Experimentation with different adaptation operators showed performance drops with K = 10, 5. ES-MAML is not limited to single goal exploration and can handle more complex tasks like the six circles challenge. Visualizations show ES-MAML with the HC operator developing strategies to explore target locations. The HC operator helps develop exploration strategies in target locations, improving performance for both DPP-ES and HC operators. A modified task penalizing incorrect goals caused a drop in performance for MC and DPP-ES due to variance from the MC-gradient. HC adaptation enforces non-decreasing rewards, allowing ES-MAML to progress. ES-MAML's ability to train compact linear policies outperforms hidden-layer policies, demonstrated in Navigation-2D tasks. ES-MAML demonstrates the ability to train compact linear policies that outperform hidden-layer policies in various benchmark tasks. Using fewer perturbations for gradient estimation, ES-MAML with the HC operator shows superior performance, especially in unstable environments where deterministic policies are more stable than stochastic ones required for PG. ES-MAML explores in parameter space to mitigate issues caused by unstable environments, as demonstrated in the \"Biased-Sensor CartPole\" environment. Results show ES-MAML maintaining maximum rewards compared to PG in unstable environments like Swimmer and Walker2d. Linear policies outperform hidden-layer policies, even with fewer queries K in real-world applications. ES-MAML remains competitive with PG-MAML even with very low query values. In experiments on Forward-Backward and Goal-Velocity tasks across different environments and model architectures, ES-MAML performs similarly or better than PG-MAML on the Forward-Backward task. Despite PG-MAML generally outperforming ES-MAML on the Goal-Velocity task, ES-MAML holds its own in low query scenarios. ES-MAML is stable compared to PG-MAML, especially for low K values, where PG-MAML can be highly unstable. ES-MAML avoids the need for complicated alterations and is flexible in adaptation operators, including nonsmooth operators like hill climbing for better performance in sparse-reward environments. ES-MAML performs well with linear or compact deterministic policies, advantageous for adapting to unstable state dynamics. However, it is slightly inferior to full PG-MAML and lacks comparisons with and without the Hessian in RL MAML. The importance of second-order terms for proper credit assignment is highlighted, but heavily modified estimators used in experiments make direct performance comparisons challenging. Algorithm 4 may have high variance, leading to inaccurate Hessian estimates and potentially slower training on 'easier' tasks like Forward-Backward walking but increased exploration on more complex tasks. Algorithm 3 compared FO-NoHessian and another method on HalfCheetah and Ant tasks. FO-NoHessian was faster on Ant but performed poorly on HalfCheetah with low query numbers. Exact evaluation of adapted policies is challenging in PG-MAML and ES-MAML due to sampling limitations. The MAML gradient requires exact sampling from adapted trajectories, making unbiased estimates challenging. E-MAML reformulates the objective function to achieve unbiased estimation, improving performance. ESGRAD replaces the estimation of U with an improved estimator of \u2207 f T \u03c3 (\u03b8), potentially using data collected during the process. Active Subspaces is a method for finding a low-dimensional subspace where the gradient contribution is maximized, aiming to optimize in lower-dimensional subspaces for computational efficiency. This approach does not require explicit computation of the gradient, making it a guided exploration method leveraging knowledge gained from previous optimization steps. In the context of function optimization landscape, the active subspace method ASEBO was successfully applied to speed up policy training algorithms in RL. Regression-Based Optimization (RBO) is an alternative method of gradient estimation using Taylor series expansion and solving a regularized regression problem to recover the gradient, even with corrupted rewards. This method is not based on Gaussian smoothing like ES. In a preliminary experiment, RBO and ASEBO gradient adaptation were compared to ES on noisy data for the Forward-Backward Swimmer MAML task. RBO did not show increased robustness to noise compared to the standard MC estimator, suggesting ES-MAML's inherent noise robustness. ASEBO-MAML was also compared to ES-MAML on the Goal-Velocity HalfCheetah task. ASEBO-MAML outperforms ES-MAML on the Goal-Velocity HalfCheetah task in low-K setting, measured in iterations. However, ASEBO requires more time due to additional linear algebra operations. ES-MAML was more effective when measured by real time. Navigation-2D is a classic environment where the agent explores to adapt to the task, receiving rewards based on distance from a target point. PG-MAML and ES-MAML show different exploration strategies in this environment. ES-MAML and PG-MAML are compared in various tasks such as Forward-Backward and Goal-Velocity for HalfCheetah, Swimmer, Walker2d, and Ant. ES-MAML is also demonstrated on sine regression for supervised learning tasks. The meta-policy must learn to adapt to an unknown sine curve outside of given points for regression, using mean-squared error. Gradients are computed using Tensorflow for efficiency, with results shown in Figure A6. The adaptation step size is \u03b1 = 0.01, similar to previous work by Finn et al. (2017). ES-MAML achieves similar performance to PG-MAML in regression tasks, but requires more training iterations. The efficiency of using gradients in regression tasks is highlighted, raising questions about the interpretation of query number K in the supervised setting. In the (on-policy) RL setting, samples cannot typically be 're-used' to the same extent as in supervised learning. Rollouts in RL follow an unknown distribution, reducing their usefulness away from the adapted policy. The number of backpropagations or perturbations in RL have different costs compared to simulations. Default hyperparameters for RL experiments are K = 20 and horizon = 200. In (Mania et al., 2018), standard reward normalization and global state normalization are used. In Ant environments, differences in weighting on auxiliary rewards exist across previous work. To avoid local minima, we removed certain costs and focused on the main goal-distance cost and forward-movement reward. For ES-MAML and PG-MAML, default weightings and rewards were used, with N representing the number of possible tasks. Tasks were sampled without replacement to ensure each worker performs adaptations on all possible tasks. Standard ES-MAML settings were used, with 3 seeded runs and default TRPO hyperparameters from previous work."
}