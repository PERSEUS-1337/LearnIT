{
    "title": "HkwVAXyCW",
    "content": "Recurrent Neural Networks (RNNs) excel in sequence modeling tasks but face challenges with long sequences due to slow inference, vanishing gradients, and difficulty in capturing long term dependencies. The Skip RNN model addresses these issues by learning to skip state updates, reducing the computational graph size. It can perform fewer state updates while maintaining or improving performance compared to baseline RNN models. Source code is available at https://imatge-upc.github.io/skiprnn-2017-telecombcn/. Recurrent Neural Networks (RNNs) have become popular for sequential data tasks due to larger datasets, improved architectures, and gated units like LSTM and GRU. These units address the vanishing gradients problem and have shown impressive results in machine translation, language modeling, and speech recognition. Challenges remain in training and deploying RNNs for long sequences. Challenges persist in training and deploying RNNs for long sequences, with issues like throughput degradation and memory leakage. Sequence shortening techniques, such as cropping signals or reducing sampling rates, are common but suboptimal. A proposed model aims to learn which samples are necessary for solving tasks, adapting to the specific requirements of tasks like video understanding. The main contribution of this work is a novel modification for existing RNN architectures called Skip RNN, which reduces sequential operations by skipping state updates. This model adaptively determines whether the state needs to be updated or copied to the next time step. By adding a penalization term during training, the network can be encouraged to perform fewer state updates, allowing for training under different computation budgets. The proposed modification can be integrated with any RNN, including LSTM and GRU, and shows promising results on sequence modeling tasks. Conditional computation in RNN architectures has been explored in various sequence learning tasks, with the idea of increasing model capacity without a proportional increase in computational cost. This concept has been extended to the temporal domain, including determining the number of times an input needs to be processed before moving on and updating only a fraction of hidden states based on current input. Other approaches involve designing RNN architectures with layers dependent on input data or following periodic patterns. The Skip RNN model introduces conditional computation in time, allowing for the skipping of input samples to reduce sequential computation and shield the hidden state over longer time lags. This approach differs from hierarchical multiscale RNNs by applying the UPDATE and COPY operations to the entire stack of RNN layers simultaneously. The Skip RNN model introduces conditional computation in time, allowing for the skipping of input samples to reduce sequential computation and shield the hidden state over longer time lags. This can be seen as a learnable Zoneout mask shared between all units in the hidden state, or as a recurrent version of stochastic depth. The model generates a hard temporal attention mask on-the-fly, deciding which time steps should be attended and operating on a subset of input samples. The proposed method allows for training recurrent models on shorter subsequences without increasing complexity, unlike previous methods that used Expectation-Maximization or differentiable subsampling mechanisms. This method can be trained with backpropagation and does not degrade the complexity of baseline RNNs, making it suitable for long input sequences. The proposed LSTM-Jump BID15 model augments an LSTM cell with a classification layer to decide how many steps to jump between RNN updates, aiming to speed up RNN inference. However, training the model with REINFORCE BID12 requires defining a suitable reward signal, which may not generalize across tasks. Hyperparameters like the number of tokens read between jumps, maximum jump distance, and allowed number of jumps need to be chosen in advance, limiting the model's sampling flexibility compared to other approaches. The proposed approach is differentiable, allowing for easier optimization without modifying the loss function. An RNN with a binary state update gate selects whether to update the state or copy from the previous time step. The architecture includes a weights vector, scalar bias, sigmoid function, and a binarization function. The model formulation includes a scalar bias, sigmoid function, and binarization function. The state update gate determines whether to update the state or copy from the previous time step. The number of skipped time steps can be calculated in advance. The number of skipped time steps can be calculated in advance for efficient implementations in reducing the number of RNN updates. This leads to faster inference, reduced energy consumption, and fewer sequential operations needed to process an input signal. Our model allows skipping steps during operations, increasing memory capacity and improving long-term dependency modeling. It accelerates convergence in tasks with long sequences by propagating gradients through fewer time steps. The model is compatible with recent RNN advancements like normalization, regularization, variable computation, and external memory. The model is mostly differentiable, except for the f binarize function that outputs binary values. The straight-through estimator is used for optimizing functions with binary outputs, proving more efficient than other estimators like REINFORCE. It allows training model parameters to minimize loss without additional supervision or reward signals, as demonstrated in various works. The Skip RNN can learn when to update or copy the state without explicit supervision. Different trade-offs between performance and number of samples processed may be needed based on the application. Additional loss terms can encourage the model to perform fewer state updates. In the following section, the advantages of adding state skipping to LSTM and GRU RNN architectures for various tasks are investigated. Different budget loss terms can be used to encourage a specific number of samples or updates per sequence. In the study, the computational load for each model is measured by the number of RNN state updates and floating point operations. A baseline model is introduced that skips a state update with a certain probability. Training is done with specific parameters and gradient clipping is applied to all trainable variables. The study introduces a baseline model that skips state updates with a certain probability to measure computational load. Training is done with specific parameters and gradient clipping is applied to all trainable variables. The LSTM network is tasked with adding only two values marked with a 1 while ignoring those marked with a 0. In the study, marker distribution in sequences can lead to a high risk of missing markers. RNN models are trained on sequences of length 50 for an adding task, with the model aiming to minimize Mean Squared Error (MSE) between output and ground truth. The task is considered solved if the MSE is significantly lower than the output distribution variance. Skip RNN models can solve tasks with fewer updates compared to their counterparts, ensuring no markers are missed by skipping updates strategically. This strategy leads to better performance compared to baselines that randomly skip updates. The proposed Skip RNN models effectively learn whether to update or copy the hidden state based on input sequences. However, they show difficulties skipping a large number of updates at once. The MNIST handwritten digits classification benchmark is traditionally addressed with CNNs, but can be challenging for RNNs due to long term dependencies. The data is split with 5,000 training samples set aside for validation. After processing all pixels with an RNN with 110 units, the last marker is found. The Skip RNN models show improved performance on the MNIST handwritten digits classification task by skipping updates, leading to faster training and better results compared to traditional RNNs. This approach simplifies the optimization process and allows for capturing long term dependencies more easily. The models are trained for 600 epochs to minimize cross-entropy loss, with Skip RNNs showing lower variation among runs. Learning which samples to use is crucial for Skip RNN performance. Techniques like recurrent batch normalization and specific weight initialization schemes can boost RNN models. Reshaping sequences of pixels back into 2D images allows visualization of samples used by RNNs. The model learns to skip non-discriminative pixels, improving performance. One popular approach for video analysis tasks involves extracting frame-level features with a CNN and modeling temporal dynamics with an RNN. Videos recorded at high sampling rates create long sequences with temporal redundancies, challenging for RNNs. Previous works have addressed these issues by using short clips or downsampling data to cover long temporal spans efficiently. Instead of tackling the long sequence problem at the input data level, the network is allowed to learn which frames to use. The Charades BID4 dataset contains videos annotated for 157 action classes in a per-frame fashion. Frames are encoded using fc7 features from a Two-Stream CNN, fed into stacked RNN layers for Skip RNN models evaluation. Evaluation is done on 100 equally spaced frames instead of 25, with results reported in Table 3. The Skip RNN models outperform random methods when fewer updates are allowed, as they learn to attend to specific frames from RGB data without explicit motion information. Skip GRU performs fewer state updates compared to Skip LSTM in this challenging setup with longer time spans between updates. Properly distributing state updates along the sequence is crucial for model performance. The Skip GRU tends to perform fewer state updates than Skip LSTM in scenarios with low cost per sample. This behavior contrasts with the adding task results, where GRU models consistently outperform LSTM models. For large values of \u03bb, both Skip LSTM and Skip GRU converge to a similar number of used samples. Integrating RGB and optical flow information into an LSTM can improve action localization performance but at the expense of increased computational cost and memory usage. The Skip RNN model learns to attend to frames from RGB data without explicit motion information. It reduces sequential operations by skipping state updates, leading to faster and more stable training for long sequences. Skip RNNs can match or outperform baseline models like LSTMs and GRUs while lowering computational requirements. The Skip RNN model reduces sequential operations by skipping state updates, leading to faster training for long sequences. Additional experiments involve training the network to classify sinusoids with different periods, studying if Skip RNNs converge to the same solutions with fixed parameters but varied sampling periods. The study involves training RNNs with different sampling periods to classify input signals. Results show varying number of updates under different sampling conditions, attributed to the presence of numerous local minima in the cost function. The study involves training RNNs with different sampling periods to classify input signals. Results show that when \u03bb > 0, Skip RNN models converge to a similar number of used samples under different sampling conditions. The IMDB dataset contains movie reviews annotated into positive and negative sentiment classes. The study involves training RNNs with different sampling periods to classify input signals. Results show that Skip RNN models achieve similar accuracy rates to baseline models while reducing required updates. Negative sentiment movie reviews in the IMDB dataset have an average length of 240 words. The models are evaluated on sequences of length 200 and 400, with dropout applied to reduce overfitting. The study compares the accuracy of Skip LSTM and Skip RNN models with baseline models, showing that allowing the network to select samples boosts performance. Miyato et al. (2017) reduce overfitting by leveraging additional unlabeled data through adversarial training, achieving a state-of-the-art accuracy of 0.941 on IMDB. The study evaluates different models on UCF-101, showing that Skip RNN models improve classification accuracy with few updates. Non-recurrent architectures like CNNs with spatiotemporal kernels or two-stream CNNs have also achieved high performance on UCF-101. Carreira & Zisserman (2017) demonstrate the benefits of expanding 2D CNN filters into 3D. Carreira & Zisserman (2017) show benefits of expanding 2D CNN filters into 3D and pretraining on larger datasets, achieving high accuracy using RGB data and optical flow information for frequency discrimination task. The network learns to classify sine wave frequencies by using only the first samples, avoiding aliasing."
}