{
    "title": "HylgS2IpLN",
    "content": "Machine learning algorithms for human health (ML4H) must operate safely and reliably at scale, necessitating a focus on reproducibility. A systematic evaluation of over 100 recent ML4H papers reveals shortcomings in data and code accessibility compared to other machine learning fields. Recommendations are proposed to improve reproducibility in ML4H research. Many sub-fields of science are facing a reproducibility crisis, impacting trust in processes and results. This is particularly crucial in machine learning research, where algorithms need to reliably solve complex tasks with limited human supervision. Failure to replicate intended behavior can have serious consequences, especially in Machine Learning for Health (ML4H) applications that directly impact human lives. Recommendations are needed to improve reproducibility in ML4H research. In 2018, 12 AI tools using ML4H algorithms for medical diagnosis were cleared by the FDA. Regulatory bodies need to prioritize verifying the reproducibility of claims made by device manufacturers. Challenges in reproducibility in ML4H applications stem from data availability, quality, and consistency. This work presents a taxonomy of reproducibility for ML4H applications and defines metrics to quantify reproducibility goals. In ML4H, a taxonomy of reproducibility is used to define metrics for quantifying challenges. The concept of technical replicability is discussed, highlighting the goal of reproducibility beyond just replicating results. The analysis explores areas for further research in reproducibility within ML4H. The discrepancy in reproducibility has been historically noted in various fields, including natural and social sciences. To achieve full reproducibility, a study must meet three tiers of replicability: Technical Replicability, Statistical Replicability, and reproducibility related to code and dataset release. Technical replicability ensures results can be replicated under technically identical conditions, while statistical replicability ensures results hold under re-sampled conditions that may vary technically. Conceptual replicability is crucial for external validity, describing how well results reproduce under conditions matching the conceptual description of the effect. Task-definition impacts the level of generalizability and the difficulty of meeting this requirement. Technical replicability, statistical replicability, and reproducibility related to code and dataset release are essential for full reproducibility. Without technical replicability, results cannot be demonstrated; without statistical replicability, results will not reproduce under increased sampling and real-world variance. ML4H differs from general machine learning in critical ways and presents unique challenges related to reproducibility. A quantitative literature review shows that ML4H lags behind other subfields in reproducibility metrics. Over 300 papers were manually extracted and annotated from various venues to support this finding. The full statistical review procedure is detailed in the Appendix. ML4H faces challenges in technical replicability due to privacy concerns with health data, leading to limited availability of public datasets. Only 51% of ML4H papers use public datasets compared to over 90% in CV and NLP papers. ML4H also lacks in code release, preprocessing specification, and cohort description. Only about 13% of ML4H papers release their code publicly, compared to 37% in CV and 50% in NLP. Code release in ML4H is not always sufficient for full technical replicability due to various issues like incorrect running, missing critical details, or failure to reproduce reported results. Assessing statistical replicability in ML4H involves quantifying how often papers describe the variance around their results. The statistical replicability of ML4H papers is relatively low at around 38%, higher than CV, NLP, or the general domain. However, there is still room for improvement as studies show that published models often fail to reproduce when appropriate statistical procedures are implemented. In ML4H, performance differences between model architectures for language models and generative adversarial networks may not persist under robust hyperparameter search and statistical comparison techniques. Reproducibility issues in deep reinforcement learning practices are examined in BID16. The lack of multi-institution datasets in healthcare hinders conceptual replicability in ML4H. The lack of multi-institution datasets in healthcare is a significant issue in ML4H studies, with only a small percentage using multiple datasets. Using only a single dataset can be dangerous as it hinders the generalization of ML4H models over changing care practices and data formats. Studies have shown that transferring models between institutions or not using manually engineered representations can lead to significant performance degradation over time. Data providers in healthcare, such as hospitals and research centers, should create large data trusts for researchers to access and develop algorithms. This will enhance reproducibility in ML4H studies by allowing for the pooling of valuable data across institutions. Several examples, like MIMIC BID23, already exist for creating such repositories. Large scale datasets like MIMIC BID23, U.K and Japan Biobanks BID31 BID42, eICU BID38, Temple University Hospital EEG Corpus BID14, and Physionet BID11 are crucial for ML4H research. It is important to anchor research progress to publicly accessible datasets to avoid reliance on non-public or non-academic sources. Multi-institute datasets enable studies to assess generalizability to new contexts, with recent advancements like the eICU dataset BID38 facilitating the development of generalizable models. Observational Health Data Sciences and Informatics (OHDSI) BID18 facilitates collaborative health studies across institutions and countries. Data privacy risks and confounding variables are present in datasets like MIMIC BID21. Prospectively collecting data from new participants, as seen in projects like NIH's All of Us Research Program BID32, can mitigate some of these challenges. Google's Project Baseline BID45 and ML4H researchers are urged to enhance statistical best practices for model comparisons. High standards of statistical rigor, including periodic audits, can help prevent issues like \"overfitting via publication.\" Challenges in ML4H with fixed train/test sets can be used for replication studies. Technological solutions, such as using noised, simulated, or encrypted datasets, can address privacy concerns. Techniques like training distributed models without sharing data have also been proposed. In ML4H research, synthetic data and technology for producing synthetic patient-level data have been proposed. Observational studies in the biomedical sciences undergo intense scrutiny to prevent statistical artifacts, but ML4H lacks prospective checks like pre-registration. Verbatim application of pre-registration from epidemiology may not be practical due to the exploratory nature of model development. ML4H researchers and academic publishers should discuss implementing systematic re-release of new data and rotation of train/test splits to reduce statistical errors. Conferences and journals could require greater data/code release rates and additional availability statements to improve reproducibility. Insisting on high standards for reporting statistical variance, hyperparameter search procedures, and evaluation mechanisms would ensure technical and statistical replicability. Increased use of data standards like OMOP and FHIR in ML4H research would enhance reproducibility. Detailed descriptions of ML4H datasets, including confounders, biases, and missing data, are essential for replication. Adoption of \"specs\" or \"datasheets\" for datasets is crucial to address these concerns and align with practices in the broader machine learning community. In ML4H, reproducibility is framed around technical, statistical, and conceptual replicability. Through a literature review, it was found that ML4H lags behind other machine learning fields in reproducibility metrics. Opportunities for improvement include better data access, increased statistical rigor, and utilizing multi-source data for conceptual reproducibility. The analysis was based on publicly accessible papers, but potential biases exist in the selection and annotation process. The analysis of a large number of papers in ML4H has possible biases due to time constraints and limited access to papers. Some biases include missing information on external datasets or code products, papers not being publicly accessible, and potential differences in paper versions. Our analysis technique may miss updated versions of papers or papers in different repositories. Some papers naturally fit into multiple categories, but we omitted clearly multi-domain works to ensure comparison classes were pure. Different fields present different kinds of works, and not all fit into our framework. Largely theoretical works often lack real datasets or public experiments, and works focused on computational efficiency may not address predictive accuracy. We addressed these issues by answering questions as best we could and flagging any papers. We flagged papers that did not fit our scheme and excluded them from our analyses. All data resulting from these analyses is publicly accessible at the time of publication."
}