{
    "title": "rkem91rtDB",
    "content": "Inductive and unsupervised graph learning is crucial for tasks where label information is hard to obtain. The SEED framework proposes a method for inductive and unsupervised representation learning on graph structured objects by sampling subgraphs, encoding them into vectors, and using the distribution of these vectors as the output representation. The SEED framework proposes inductive and unsupervised representation learning on graph structured objects by sampling subgraphs and using the distribution of these vectors as the output representation. Empirical study shows up to 10% improvement compared to baseline methods, demonstrating a close connection between SEED and graph isomorphism. In this paper, the focus is on learning graph representations in an inductive and unsupervised manner for efficient inference over unseen data in critical applications like malware detection and anomaly detection. Unsupervised methods provide effective feature representations shared among different tasks, making inductive and unsupervised graph learning challenging compared to transductive or supervised methods. Graph learning is challenging, especially in inductive and unsupervised scenarios. Node alignment and reconstruction errors are key aspects in evaluating latent representations. The computation of reconstruction errors can be complex, making it difficult to capture detailed structural information. Previous deep graph learning techniques are utilized for this purpose. In this paper, a general framework SEED (Sampling, Encoding, and Embedding Distributions) is proposed for inductive and unsupervised representation learning on graph structured objects. SEED consists of three major components: subgraph sampling, subgraph encoding, and embedding subgraph distributions. It aims to generate graphs with similar graph statistics and provides a GAN based method for generating graphs with similar random walks. SEED is a framework for representation learning on graph structured objects. It consists of subgraph sampling, encoding, and embedding subgraph distributions. SEED focuses on efficiently autoencoding and comparing structural data in an unsupervised manner, using random walks with earliest visiting time as the subgraph class. SEED utilizes deep architectures to efficiently autoencode WEAVEs, evaluating reconstruction errors in linear time. By sampling multiple subgraphs to enhance information coverage, SEED represents an input graph as a collection of vectors. Graph similarity is measured by computing distribution distance between these vectors, with SEED outputting a vector representation reflecting the distance between subgraph distributions of two graphs. Unlike existing techniques, SEED focuses on embedding distribution of subgraph representations for graph learning. SEED framework demonstrates a direct relationship between SEED and graph isomorphism, showcasing its effectiveness in capturing structural information and maintaining stable performance. It outperforms competitive baseline methods by up to 10% in prediction accuracy and offers high-quality representations with adjustable sample sizes for a trade-off between effectiveness and efficiency. Kernel methods and deep learning are two approaches in graph learning. Kernel methods rely on handcrafted substructures or graph statistics for vector representations, but struggle with rich node and edge attributes. On the other hand, deep graph representation learning combines structural and attribute information to create unified vector representations for graphs. Existing works in this field are either transductive or supervised. Recent studies have focused on autoencoding specific graph structures like directed acyclic graphs, trees, and graphs decomposable into trees. Graph generation methods aim to generate graphs with similar statistics or random walks. Different approaches include supervised graph similarity learning and theoretical analysis of message-passing based graph neural networks. The SEED framework offers a new method distinct from existing kernel or deep learning approaches. The SEED framework is an unsupervised method for encoding subgraphs as vectors to reflect graph similarity. It supports complex attributes on nodes and edges, works for arbitrary graphs, and captures both structural and attribute information. The framework is discussed in sections 3.1 to 3.5, focusing on undirected graphs with rich node attributes but can also handle directed graphs with rich node and edge attributes. SEED encodes graphs into vectors through sampling, encoding subgraphs, and aggregating vector representations. Sampling selects subgraphs for efficient encoding and decoding, encoding converts subgraphs into vectors, and embedding distribution combines subgraph vectors into one representing the input graph. The paper proposes a competitive implementation for encoding graphs into vectors using a subgraph sampling method called WEAVE. This method aims to preserve a pre-defined distribution distance by sampling subgraphs from the input graph. The WEAVE method involves initialization of a starting node, next-hop selection, and termination based on timestamps. It uses a matrix representation denoted as t ] with attribute information and earliest visit time. The method differs from vanilla random walks in its sampling approach. WEAVEs differ from vanilla random walks by utilizing earliest visit time to preserve loop information in sampled subgraphs. This allows for a finer-granularity structural difference between graphs compared to vanilla random walks. While both methods are efficient in encoding and decoding, WEAVEs excel in capturing detailed structural information related to loops or circles. WEAVEs are efficient in encoding and decoding, utilizing autoencoders to generate dense low-dimensional vectors. The quality of the representation is evaluated through reconstruction errors, optimized via gradient descent. The latent representation includes node attribute information. The component generates dense low-dimensional vectors from sampled WEAVEs of an input graph, incorporating both node attribute and structural information. It evaluates the distance between subgraph distributions of two arbitrary graphs using the maximum mean discrepancy (MMD) without assuming prior distributions. Different metrics like KL-divergence and Wasserstein distance are considered for future comparison. The component generates dense low-dimensional vectors from sampled WEAVEs of an input graph, evaluating the distance between subgraph distributions of two arbitrary graphs using the maximum mean discrepancy (MMD). Different metrics like KL-divergence and Wasserstein distance are considered for future comparison. In our future work, we will explore various choices of distance metrics for subgraph comparison. Effective kernel options include the identity kernel and commonly adopted kernels like RBF. In this section, the text discusses the training of a deep neural network to approximate feature mapping functions for popular kernels in graph isomorphism tests. The theoretical connection between SEED and graph isomorphism is outlined, along with the impact of walk length in WEAVE on effectiveness. The full proof of theorems and lemmas is provided in detail. The discussion in this section focuses on the connection between SEED and graph isomorphism, as well as the impact of walk length in WEAVE on effectiveness. The full proof of theorems and lemmas is detailed in the Appendix. Graph isomorphism with node attributes is defined, along with the concept of identical distributions based on Wasserstein distance. The minimum walk length for WEAVEs is also discussed in relation to visiting all edges in a graph. The connection between SEED and graph isomorphism is presented. Two connected graphs, G and H, can be isomorphic if the Wasserstein distance between their WEAVE distributions is 0. This holds true even when considering graphs with node attributes. The theory can be extended to cases with both node and edge attributes. SEED framework's potential in capturing structural differences in graph data is highlighted. Theoretical results show the need to sample a large number of WEAVEs for graph isomorphism, but empirical studies demonstrate that SEED can achieve state-of-the-art performance with a small number of WEAVEs. Evaluation on 7 benchmark datasets, including Deezer and MUTAG, shows the effectiveness of SEED in online social network and chemistry domains. Deezer User-User Friendship Networks dataset from the music streaming service Deezer is described in Appendix F. Nodes represent users, edges show mutual friendships, and genre information is included as node features. GraphSAGE and GMN are two techniques evaluated as baselines in the experiments. GraphSAGE is an inductive graph representation learning method, while GMN is a Graph Matching Network. GraphSAGE and GMN are evaluated as baselines in the experiments. GMN utilizes graph neural networks for graph matching applications, while GIN provides an effective aggregator for graph representation learning. The objective is modified for unsupervised learning, focusing on downstream tasks like classification and clustering. Quality of learned graph representations is evaluated using classification accuracy and clustering with Normalized Cuts. SEED outperforms baseline methods in classification and clustering tasks, showing up to 0.18 absolute improvement in classification accuracy. GraphSAGE, GIN, and GMN are evaluated as baselines for graph representation quality in downstream tasks. SEED outperforms baseline methods like GraphSAGE in capturing structural information effectively in an unsupervised manner. SEED utilizes meta-parameters like walk length and sample numbers to balance effectiveness and computational efficiency. Experimental evaluation shows the impact of these parameters on graph representation quality in downstream tasks. SEED framework shows improved performance in classification and clustering tasks with varying sampling numbers and walk lengths. Performance increases with more subgraphs sampled, but diminishes when sampling number exceeds 200. Walk length growth enhances performance until it converges beyond 20. The boundary between labels becomes clearer with increased sampling number or walk length. The SEED framework addresses computational challenges in unsupervised and inductive graph learning. The SEED framework addresses computational challenges in inductive graph learning by sampling subgraphs for efficient evaluation and utilizing subgraph vectors for graph representation. The framework demonstrates a close connection to graph isomorphism and achieves state-of-the-art predictive performance on benchmark datasets. The SEED framework tackles computational challenges in inductive graph learning by sampling subgraphs for efficient evaluation and using subgraph vectors for graph representation. It shows a strong link to graph isomorphism and achieves top predictive performance on standard datasets. For connected graphs with m edges, a walk can cover all edges with length k \u2265 2|E(G)| \u2212 1 by removing an edge from a cycle to create a connected graph with m \u2212 1 edges, then applying an induction hypothesis. In the context of the SEED framework for inductive graph learning, a walk on a subgraph G can cover all edges with length k \u2265 2|E(G)| \u2212 1 by removing an edge from a cycle to create a connected graph with m \u2212 1 edges. This process involves replacing nodes in the walk to extend its coverage on the graph. In the SEED framework for inductive graph learning, a walk on a subgraph G can cover all edges with length k \u2265 2|E(G)| \u2212 1 by extending the walk to visit all edges of G. This is achieved by replacing nodes in the walk to create a new walk that covers all edges. Lemma is crucial for the proof of Theorem 1. Suppose two random walks on graphs G and H have the same representation. The number of distinct edges and nodes on both walks are equal. Proof involves showing that the number of distinct nodes and edges on the walks are the same through a contradiction argument. The proof involves showing that if two isomorphic graphs have the same representation of random walks, then the distribution of WEAVE on the graphs is the same. This is done by contradiction, where assuming different numbers of edges leads to a contradiction. The text discusses the mapping of walks on graphs A and B, showing that a bijective mapping exists between them. The proof involves demonstrating the injectivity of the mapping function. The text demonstrates the bijective mapping between walks on graphs A and B by proving the injectivity and surjectivity of the mapping function g. It also shows that the distribution of WEAVEs in graphs G and H are the same. The text discusses the bijective mapping between walks on graphs A and B, showing that different walks may correspond to the same WEAVE. It introduces the concept of representation vectors and compares the similarity of discrete probability distributions using the Wasserstein distance equation. The mapping function g is proven to be a bijection, indicating that g(w) and w correspond to the same WEAVE and share the same representation vector. The text discusses the bijective mapping between walks on graphs A and B, showing that different walks may correspond to the same WEAVE. It introduces the concept of representation vectors and compares the similarity of discrete probability distributions using the Wasserstein distance equation. The mapping function g is proven to be a bijection, indicating that g(w) and w correspond to the same WEAVE and share the same representation vector. For two isomorphic graphs G and H, their WEAVE's distributions P G and P H are the same, with a Wasserstein distance of 0 implying graph isomorphism. For isomorphic graphs G and H, a bijective mapping is established between walks on G and H. A walk of length k on G covers all edges, marking nodes based on the walk's representation vector. For isomorphic graphs G and H, a bijective mapping is established between walks on G and H. The mapping marks nodes based on the walk's representation vector, ensuring that all nodes are covered. This mapping helps prove the isomorphism between G and H by showing that edges in G correspond to edges in H. The isomorphism between graphs G and H is proven by establishing a bijective mapping between walks on G and H. This mapping ensures all nodes are covered and that edges in G correspond to edges in H. The number of edges in G and H are shown to be equal, and the representation vectors contradict any additional edges on G. The proof of isomorphism between graphs G and H is established by mapping walks on G and H, showing equal number of edges and contradicting any additional edges on G. The influence of node attributes on walk representation distributions is illustrated in Figure 6 for a 4-node ring with different cases. In Figure 6, the influence of node attributes on walk representation distributions is demonstrated. Different unique walks may correspond to one representation vector, with the number of attributes affecting the number of representation vectors and unique walks. For example, in Figure 6 (a), a ring graph without node attributes has 16 unique walks, encoded as different representation vectors. In Figure 6 (b), nodes with discrete attributes result in four different representation vectors for the same 16 unique walks. In Figure 6, node attributes influence walk representation distributions. Each unique walk corresponds to a representation vector, with the number of attributes impacting the number of representation vectors and unique walks. For instance, in Figure 6 (c), there are 16 unique walks with different representation vectors, leading to a variety of probabilities for each representation vector. The proof for Theorem 2 is similar to Theorem 1, as attributes affect the representation vector form and the number of unique walks corresponding to it. The proof for Theorem 2 involves showing that the walk representations distribution of isomorphic graphs with attributes are the same. By establishing a bijective mapping between possible walks of length k on the graphs, it is demonstrated that the representation vectors for each point in the distribution are equivalent. This confirms that the distribution of representation vectors for each graph is identical. The Wasserstein distance of distribution P G and P H is W 1 (P G , P H ) = 0, indicating that P G = P H. In attributed graphs G = (V, E, \u03b1, \u03b2), nodes and edges have attributes defined by labeling functions. Graph isomorphism with attributes involves a bijection f between G and H with node and edge attributes. Corollary 1 states that for connected graphs G and H with node attributes, enumerating all possible WEAVEs with a fixed-length k \u2265 2 max{|E(G)|, |E(H)|}\u22121 shows unique vector representations. The Wasserstein distance between graphs G and H's WEAVE distributions is 0 if and only if they are isomorphic with both node and edge attributes. Unique vector representations are generated from a well-trained autoencoder for each WEAVE. Experimental study includes Deezer User-User Friendship Networks dataset collected from the music streaming service Deezer, representing friendship networks of users from Romania, Croatia, and Hungary. The dataset includes graphs representing user friendships in three countries: Romania, Croatia, and Hungary. Nodes represent users, and edges represent mutual friendships. The dataset also includes bioinformatics data with proteins converted to graphs based on sub-structures and physical connections. Additionally, there is a scientific collaboration dataset collected from public collaboration datasets in High Energy Physics, Condensed Matter Physics, and Astro Physics. The dataset includes graphs representing user friendships in three countries: Romania, Croatia, and Hungary. Nodes represent users, and edges represent mutual friendships. It also includes bioinformatics data with proteins converted to graphs based on sub-structures and physical connections. Additionally, there is a scientific collaboration dataset collected from public collaboration datasets in High Energy Physics, Condensed Matter Physics, and Astro Physics. The ego-networks are generated for individual researchers in various fields. There are 5,000 graphs with 24,574,995 edges. IMDB-BINARY is a movie collaboration dataset with 1,000 graphs associated with 19,773 edges. IMDB-MULTI is a multi-class version with 1,500 graphs and 19,502 edges, derived from Comedy, Romance, and Sci-Fi genres. Graph Sample and Aggregate (GraphSAGE) is an inductive graph representation learning approach. GraphSAGE is an inductive graph representation learning approach that explores node and structure information through sampling and aggregating features from the local neighborhood of each node. It uses a forward propagation algorithm to aggregate information. Graph Matching Network (GMN) utilizes graph neural networks for graph matching applications, with a focus on preserving node features and graph structures. Graph Isomorphism Network (GIN) provides a simple approach for graph representation. Graph Isomorphism Network (GIN) is a neural network architecture for graph representation learning that uses the sum aggregator. It is originally a supervised learning method but can be modified for unsupervised settings. Two downstream tasks, classification, and clustering, are used to evaluate the quality of learned graph representations. For classification, a multi-layer fully connected neural network is built, and for clustering, Normalized Cuts (NCut) is used. Performance is evaluated using Accuracy (ACC) and Normalized Mutual Information (NMI) metrics. SEED outperforms baseline methods in 12 out of 15 cases, with up to 0.06 absolute improvement in classification and clustering accuracy. Node features bring little improvement in unsupervised settings for NCI and PROTEINS datasets, possibly due to high correlation with structural information. In this section, the effectiveness of DeepSet for distribution embedding is investigated. Deep neural networks are used to approximate feature mapping functions for kernels, and t-SNE visualization of graph representations is shown. SEED variants with different kernels can distinguish classes effectively on the MUTAG dataset. Representation evaluation is based on classification and clustering. In this section, the impact of node features and earliest visit time in WEAVE on the MUTAG dataset is investigated. Table 8 shows that utilizing both node feature and earliest visit time information is crucial for subgraph encoding. In this section, the impact of Nystr\u00f6m based kernel approximation on embedding distributions is evaluated. The SEED-Nystr\u00f6m baseline method is implemented to approximate RBF kernel based MMD during training with 200 sampled WEAVEs. Top 30 eigenvalues are considered in this experiment. The Nystr\u00f6m method is used to approximate RBF kernel based MMD during training with 200 sampled WEAVEs. It achieves comparable performance to the identity kernel and shows better scalability as the number of WEAVE samples increases. This method improves the scalability of the SEED framework, especially for cases requiring a large number of WEAVE samples."
}