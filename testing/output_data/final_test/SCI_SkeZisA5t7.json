{
    "title": "SkeZisA5t7",
    "content": "The information bottleneck theory of deep learning suggests that neural networks achieve good generalization by compressing their representations. Empirical evidence shows conflicting results, with compression observed in networks using saturating activation functions but not in networks using non-saturating activation functions. Improved mutual information estimation techniques were developed to explore compression in networks with different activation functions. The study explores compression in neural networks with different activation functions and network initializations. L2 regularization increases compression and prevents overfitting. Compression of the last layer is correlated with generalization in deep learning. The information bottleneck theory suggests that generalization in DNNs is a form of representation compression. The theory of information bottleneck suggests that deep neural networks use compression to eliminate irrelevant information from the input, retaining only the most relevant segments. This is achieved by maximizing the information shared with the target variable while minimizing the Lagrangian equation with a Lagrange multiplier \u03b2. The intermediate representation T acts as an information bottleneck by compressing the input to retain the most important information. The theory of information bottleneck suggests that deep neural networks use compression to eliminate irrelevant information from the input, retaining only the most relevant segments. In the context of deep learning, T represents a layer's hidden activity, X is a data set, and Y is the set of labels. Compression is indicated by a decrease in I(T, X) value, while I(T, Y) increases during training. The information plane visualizes the dynamic of training a neural network, with the learning trajectory aiming to move the layer values to the top left of the plane. The compression phase leads to layers stabilizing on the IB bound, observed in networks with tanh activation function. The behavior of networks using ReLU activation function instead of tanh did not show compression during training, leading to constant increase in mutual information between the network and its input. Measuring differential mutual information in deterministic DNNs is challenging. The entropy formula varies based on the nature of hidden activity variable T. The true mutual information value I(T, X) is infinite, but finite values are needed to observe training dynamics. Adding noise to hidden activity can avoid trivial infinite values. Two ways of adding noise are explored: directly adding noise to T or discretizing continuous variables into bins. Previous studies primarily relied on binning hidden activity to estimate mutual information. In DNNs, adding noise to hidden activity is crucial for estimating mutual information accurately. Consistency in the estimation process is vital, especially with non-saturating activation functions that result in unbounded hidden activity. The level of noise introduced by the estimation procedure must be proportional and consistent to adapt to the state of the network. Adaptive estimation schemes are crucial for accurately estimating mutual information in DNNs with unbounded activation functions. L2 regularization leads to more compression and clusters all layers to the same mutual information value. Compression in hidden layers does not show a significant correlation with generalization, but the compression of the last softmax layer does. In this study, a fully connected network with 5 hidden layers was used for a binary classification task. The network utilized ADAM BID7 optimizer and cross-entropy error function. Weight initialization was performed using random truncated Gaussian initialization. 80% of the dataset was used for training. During training, hidden activity for different epochs was saved to calculate mutual information. Estimating mutual information at different epochs and layers presents challenges due to inherent differences. Adaptive frameworks help choose estimator parameters for comparable estimates. Binning networks with saturating activation functions is straightforward, while non-saturating functions require specifying a range of activation values. In BID14, hidden activity in networks with ReLU units was binned using a single range from zero to m, the maximum value achieved by any ReLU unit during training. However, this approach is limited as activation values vary significantly between layers and epochs. The entropy-based binning strategy BID8 is used to choose bin boundaries for activation levels in different layers and epochs. This ensures that each bin contains the same number of unique observed activation levels, ignoring repeated levels near saturation regions. The entropy is calculated for the whole layer rather than individual units. Entropy-based adaptive binning (EBAB) involves converting vectors to bin indices and calculating entropy. The graphs illustrate discrepancies in maximum activation values for a ReLU network, with the last ReLU layer dominating. Entropy-based binning allows accurate mutual information estimates without adjusting bin numbers to hidden activity magnitude, eliminating the need to define a range in the binning process. Entropy-based adaptive binning (EBAB) involves converting vectors to bin indices and calculating entropy. This binning strategy applies to all activation functions, allowing comparison between different networks. Fixed width binning can be insensitive to layer behavior, with most activity concentrated in a single bin. Entropy-based binning provides a better spread of bin edges, improving mutual information estimates. The adaptive binning procedure significantly impacts mutual information estimates on the information plane. Non-adaptive binning consistently underestimates compression, as shown in the analysis of ReLU networks using different estimators. The adaptive KDE (aKDE) adjustment in estimation results in noise of varying magnitudes but similar proportions across layers and epochs. Scaling noise variance with activation values produces estimates similar to EBAB. Non-adaptive KDE resembles binning with non-adaptive range and widths. Network initialization with ReLU demonstrates compression when analyzed with EBAB and aKDE. Network initializations can lead to varying compression behavior in ReLU networks. While some show significant information compression like in FIG0, others exhibit different behaviors on the information plane as seen in FIG7. It is not guaranteed that a network with ReLU always compresses during training, unlike saturating tanh networks. Networks can learn without compression, but using non-saturating ReLU does not necessarily prevent compression either. Using a non-saturating ReLU does not guarantee prevention of compression during training. The compression phase can occur before or after the fitting phase, unlike networks with tanh activation. Averaging information plots from 50 network initializations shows that ReLU networks do not exhibit distinct phases of fitting or compression. Testing other non-saturating activation functions also yields similar results on the information plane. The information plots in FIG11 display the effects of different non-saturating activation functions on training dynamics. Activation functions like ELU, Swish, and centered softplus show compression behavior despite being non-saturating, as seen in the decrease of I(X, T) values. L2 regularization is used to prevent large values in network activations, implemented on hidden layers with ReLU. L2 regularization induces compression in networks with ReLU activation functions, leading to increased test accuracy and preventing overfitting. It clusters mutual information of all layers in a single position on information planes, promoting more compression. L2 regularization induces compression in networks with ReLU activation functions, clustering mutual information of all layers in a single position on information planes. A quantitative compression metric assigns a score to networks based on compression levels, with higher compression not showing a significant correlation with generalization. The compression in the last softmax layer is correlated with accuracy, as shown in FIG0. Compression in hidden layers does not show significant correlation with generalization. However, compression in the last layer is important for generalization and should be encouraged. In this paper, adaptive approaches to estimating mutual information in hidden layers of DNNs were proposed. Different non-saturating activation functions compress information at varying rates, with compression not always present and being sensitive to initialization. Experiments with larger convolutional neural networks could further explore this possibility. Compression does not always occur in later stages of training, and even similar activation functions can yield different compression scores. Further research is needed to understand factors contributing to compression in DNNs. L2 regularization strongly compresses information, leading layers to forget input details. The clustering of mutual information to a single point on the information plane is a novel finding. This could pave the way for optimizing regularization to stabilize layers and improve generalization. Limitations include using smaller, simpler networks compared to state-of-the-art models. Our methods for computing information are adaptive for any distribution of network activity, but not rigorously derived. Compression is not restricted to saturating activation functions, L2 regularization induces compression, and generalization accuracy is positively correlated with compression in the last layer. The work was supported by Yobota limited and the James S. McDonnell Foundation. The computational facilities of the Advanced Computing Research Centre at the University of Bristol were utilized."
}