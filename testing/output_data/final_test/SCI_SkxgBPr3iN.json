{
    "title": "SkxgBPr3iN",
    "content": "Natural Language Processing models lack a unified approach to robustness testing. WildNLP is introduced as a framework for testing model stability in natural settings with text corruptions. The robustness of models from various NLP tasks is compared, focusing on text representations and word embeddings. Adversarial training is used to improve robustness, showing that high performance does not guarantee sufficient robustness. Corrupted datasets and code for WildNLP are released for the community. In this article, the WildNLP framework is introduced for testing the robustness of natural language models in real-world scenarios. It aims to evaluate the behavior of NLP models, including word embeddings like ELMo, Flair, and BERT, which have not been extensively studied for their robustness. The framework allows for comprehensive testing across different NLP areas and models, moving away from focusing on specific attacks and enabling comparisons between models and datasets. The WildNLP framework introduces a systematic approach for testing corruption robustness in natural language models. It focuses on evaluating models in real-world scenarios where input data is unintentionally corrupted by actual users. The framework includes 11 aspects of robustness testing with varying severity levels and emphasizes the importance of corruption severity in model improvement through adversarial training. Additionally, the framework is designed to be easily extendable by the community, with code and datasets available for testing. The WildNLP framework introduces a systematic approach for testing corruption robustness in natural language models. It focuses on evaluating models in real-world scenarios where input data is unintentionally corrupted by actual users. The framework includes 11 aspects of robustness testing with varying severity levels and emphasizes the importance of corruption severity in model improvement through adversarial training. Additionally, the framework is designed to be easily extendable by the community. Testing corruption robustness of NLP tasks like question answering, natural language inference, named entity recognition, and sentiment analysis reveals that new forms of text representation do not significantly increase robustness. Model training on one aspect does improve performance on another aspect if corruption types are similar. The WildNLP framework introduces a systematic approach for testing corruption robustness in natural language models, focusing on real-world scenarios where input data is unintentionally corrupted. It includes 11 aspects of robustness testing with varying severity levels and emphasizes the importance of corruption severity in model improvement through adversarial training. Recent approaches to generating textual adversaries involve character perturbations like swap, substitution, deletion, and insertion, aiming to map word vectors to 'unknown' vectors in traditional word embeddings. The BID19 and BID9 approaches involve creating rules for word substitutions to produce correct samples in Q&A and NLI systems. Targeted robustness attacks are designed for specific problems or datasets, requiring access to model internals. WildNLP defines common text corruptions like article removal, character swaps, and QWERTY keyboard errors. The curr_chunk discusses various types of text corruptions, including character swaps on a QWERTY keyboard, random character removal, space removal, misspelling, rewriting digit numbers into words, using homophones, and masking profanity. It provides examples of how words can be corrupted in a sentence. Warsaw was considered one of the most beautiful cities in the world. Vandalism used to be rare. Choosing between affect and effect can be confusing. Being first or last has its advantages. Masking negative words is done to avoid moderation in online forums. To avoid moderation in online forums, negative words are masked using the Opinion Lexicon. Positive words are also masked in a similar manner. Punctuation marks are randomly inserted between words. Different perturbations like Swap, Qwerty, and Remove char can be controlled to affect a certain number of words. Corruption robustness is tested on various NLP tasks and models using datasets preprocessed by the WildNLP framework. The experimental setting involves testing the robustness of models trained with new context-aware word embeddings like ELMo, Flair, and BERT compared to older systems like GloVe and FastText. The study aims to verify if the increased context awareness in ELMo, Flair, and BERT improves model robustness. Various well-known NLP models are used, with open-source implementations for ELMo and BERT. The experimental setting involves testing models like BiDAF and BERT trained on the SQuAD dataset. BiDAF utilizes character and word embeddings with bidirectional attention flow, while BERT applies a bidirectional Transformer for language modeling. The SQuAD dataset consists of question-answer pairs from Wikipedia articles. The curr_chunk discusses the use of NER and SA task models, including the implementation of Flair and ELMo for NER, and ULMFiT for SA. It mentions the CoNLL 2003 dataset for NER training and the state-of-the-art status of Flair in NER tasks. The dataset is described as a collection of news articles annotated with entity types. The SA task model, ULMFiT, is highlighted for its pretraining on Wikipedia. The curr_chunk discusses model BID11, a language model pretrained on Wikipedia and fine-tuned for classification tasks. It includes adversarial training on corrupted data and compares ULMFiT with a CNN-based classification model. Testing on the IMDB dataset shows results for various models and corruption aspects, with a focus on model robustness measured by an overall mean of drops (Av-Drop). The curr_chunk discusses the robustness of Q&A models, highlighting the most damaging corruptions and comparing performance between BERT, ELMo, and GloVe-based systems. Despite differences in F1 and EM, BERT was found to be the most robust model with a 2.8 pp difference in Av-Drop compared to BiDAF GloVe. The severity of corruptions like Qwerty, Remove char, and Swap significantly impacted performance metrics across all Q&A models. The curr_chunk discusses the impact of OOV tokens on GloVe-based models compared to ELMo and BERT, showing a drop in F1 metric. Additionally, WildNLP tests sentence-level corruptions, revealing a decrease in F1 values for Q&A models, with BERT proving to be more robust. The curr_chunk compares the performance of models on sentence-level corruptions, showing that BERT is more robust than BiDAF-E. The Natural Language Inference task results indicate that the Dec-E model with ELMo embeddings outperforms InferSent, with lower drops in performance metrics. However, Dec-E still struggles with word corruption aspects, while excelling in sentence-level aspects like adding extra commas. The NER models show robustness with small differences in performance. The ULMFiT model is slightly less robust than CNN. CNN's drop in performance is mainly due to specific corruptions. The ULMFiT model is less affected by Positives and Negatives corruptions compared to CNN. Adversarial training can help overcome corruption errors and increase model robustness to various deformations. BID1 found that models trained on one type of noise do not generalize well to others in character-based translation models. They aim to improve robustness between related aspects of corruption severity. Results show that increased severity of corruption during training improves performance on data corrupted with the same aspect type but lesser severity. Testing on Qwerty 1 and Qwerty 5 corruptions in Q&A BiDAF ELMo models reveals improved performance, especially in NER models where models trained on both corruption types outperform the original model. The severity of Qwerty perturbation can make text unintelligible for humans at some point, with the boundary found to be level 5 for Q&A questions. However, models trained on Qwerty 5 perform well even at severity level 8, suggesting the model can decode this corruption beyond human ability. Models trained on Qwerty and tested on similar aspects perform better than original models not trained in an adversarial setting. The WildNLP framework tests corruption robustness of NLP models, including BERT, ELMo, Flair, GloVe, and Fasttext. Adversarial training partially alleviates the issue of lacking corruption robustness. The problem of adversarial examples in NLP remains challenging to quantify, requiring further work for models to be robust to natural noise. To improve models' robustness to natural noise, researchers are exploring robust word embeddings, model architectures, and better datasets."
}