{
    "title": "H1Xw62kRZ",
    "content": "Program synthesis involves automatically generating a program based on a specification. Neural approaches for program synthesis often use sequence generation models similar to neural machine translation. However, these models have limitations such as ignoring Program Aliasing and overlooking program syntax. To address these limitations, reinforcement learning is used. To improve program synthesis, reinforcement learning is applied on top of a supervised model to maximize the likelihood of generating correct programs. A training procedure is introduced to enhance the probability of generating syntactically correct programs. These contributions result in increased model accuracy. Neural program synthesis approaches have been proposed to automatically generate programs from limited training data, inspired by computation modules like RAM and CPU. These approaches aim to mimic program behavior but struggle with interpretability, generalization on arbitrary inputs, and require many examples and computation for learning. Program synthesis approaches BID16 BID28 BID7 learn explicit programs in a Domain-specific language (DSL) from few input-output examples. Instead of learning a single program from many examples, they learn multiple programs from just a few examples. Neural program synthesis techniques improve over program induction but face limitations like Program Aliasing due to supervised learning with reference programs. Neural program synthesis techniques face limitations such as Program Aliasing due to supervised learning with reference programs. To address this, the optimization objective is altered from maximum likelihood to policy gradient reinforcement learning to encourage the generation of any consistent program. Another limitation is overlooking the strict syntax of programs, which can be efficiently checked. Neural program synthesis techniques address limitations like Program Aliasing by using policy gradient reinforcement learning to generate consistent programs. A neural architecture is introduced to prune the search space of possible programs based on syntax, even without access to grammar definitions. This model learns syntax implicitly while training and is effective in program synthesis for the Karel programming language. The paper demonstrates the efficacy of a neural program synthesis system for the Karel programming language, which includes control flow constructs like loops and conditionals. Key contributions include the use of Reinforcement Learning to optimize program generation, introducing a method for pruning possible programs with a syntax checker, and a model that learns syntax and correct program production simultaneously. This approach improves performance, especially with limited training data, addressing a fundamental problem in Artificial Intelligence. Program synthesis in Artificial Intelligence dates back to the work of BID34, using a theorem prover to construct LISP programs from formal specifications. Techniques have evolved to use simpler input-output examples for synthesis. Rule-based approaches like FlashFill in Excel have shown promise but are complex to extend. Machine Learning methods, such as Bayesian probabilistic models, have been proposed as alternatives. Recently, methods using Neural Networks have been developed to learn program behavior through gradient descent over differentiable programming concepts. While these approaches struggle with generalization and require large amounts of training data, exceptions like Neural Programmer Interpreters and its extensions learn from program traces rather than just examples. Our system uses a single model to learn from a large number of programs, unlike other approaches that require a different model for each program. Recent works focus on inferring program source code by explicitly modeling control flow statements like conditionals and loops to improve generalization. However, these approaches often need to restart learning from scratch for each new program, making them unsuitable for synthesizing new programs on-the-fly from limited examples. The latest developments involve using large datasets of artificially generated programs to map embeddings of input-output examples to program information for generation. Our approach aims to optimize program correctness by leveraging reinforcement learning techniques similar to advances in Neural Machine Translation. Unlike other methods that focus on inferring program source code by modeling control flow statements, we directly optimize for the generation of consistent programs. This approach allows us to improve on existing methods that use supervised training to maximize the likelihood of a single reference program. Our approach focuses on generating imperative programs using reinforcement learning techniques, incorporating natural language descriptions to guide program generation. Unlike other methods, we optimize for consistency in program generation rather than inferring formal specifications. This approach allows us to handle complex domains with control flow operations and long execution paths. Incorporating domain-specific grammar knowledge has proven useful in ensuring syntactical correctness. Our approach focuses on generating imperative programs using reinforcement learning techniques, optimizing for consistency in program generation rather than inferring formal specifications. We operate directly over the terminals of the grammar, allowing us to learn the grammar jointly with the model. This approach can handle complex domains with control flow operations and long execution paths, incorporating domain-specific grammar knowledge for syntactical correctness. In FlashFill-type applications, Input and Output states are strings of characters, while in the Karel environment, states are grids describing object presence. States in programming languages represent machine registers and memory content. During training, N samples with Input/Output states and correct mappings are used to train a synthesizer \u03c3 to produce programs. Programs are evaluated on test cases with specification and held-out examples. The neural program synthesis architecture uses a sequential LSTM-based language model conditioned on input-output pairs. Each pair is encoded independently by a CNN to generate a joint embedding. The program is modeled one token at a time using an LSTM, with the input consisting of the embedding of the IO pair and the last predicted token. Multiple decoder LSTMs are run for each IO pair, all using the same weights. The decoder LSTM is used for each IO pair with the same weights. The next token probability is determined by a linear layer over the max-pooled hidden state of all decoder LSTMs. At test time, likely programs are found through beam search. Program synthesis allows for execution of hypothesized programs, filtering out incorrect ones. The remaining programs are maxpooled, modulated by a syntax model mask, and the next token probability is obtained through Softmax transformation. Parameters are estimated by performing. The default solution for estimating model parameters is supervised training using Maximum Likelihood estimation. However, this approach has drawbacks such as exposure bias and not accounting for program aliasing. Ideally, the model should learn to reason about mapping inputs to outputs. The proposed modification aims to align the target objective with program synthesis goals by using a reward function to evaluate sampled programs. This generic formulation allows for a wide range of objective functions, optimizing for generalization and preventing overfitting. Additional properties like program conciseness and runtime efficiency can also be incorporated into the reward. The proposed modification aligns the target objective with program synthesis goals by using a reward function to evaluate sampled programs. This allows for a wide range of objective functions, optimizing for generalization and preventing overfitting. Program conciseness and runtime efficiency can also be incorporated into the reward. The approximation method using beam search involves trying all possible next tokens for each candidate and keeping the most likely ones. However, the inner sum in the formula is not tractable to compute due to the large number of possible programs. To address this, a Monte Carlo estimate of the expected reward is used with S samples from the model, and an estimator of the gradient of the expected reward is built based on the REINFORCE trick. Sampling from a unique model may lead to repeatedly sampling the same programs when estimating the gradient. The proposed method involves approximating the learned distribution with a smaller support using Beam Search. The process includes generating embeddings of IO grids, decoding, and keeping the S most likely candidate prefixes at each step. The final samples obtained define a probability distribution for approximation. This approach aims to prevent repeatedly sampling the same programs when estimating the gradient. The proposed method involves approximating the learned distribution with a smaller support using Beam Search. This approximation introduces bias but aligns training and testing procedures. The support of the distribution q \u03b8 is reduced to S elements from the beam search, making the sum tractable without estimators. Differentiating the new objective function provides gradients for optimization. If S covers all possible programs, the objective of (7) is recovered. By using Beam Search to approximate the learned distribution with a smaller support, the objective function can be optimized to increase diversity in program outputs. This approach allows for the optimization of expected rewards when sampling multiple programs, resulting in a higher diversity of outputs. The model can assign probability mass to several candidate programs, leading to a more varied set of outputs. This is especially useful when the reward function only takes binary values, such as correctness. The formulation in the curr_chunk discusses the challenges of identifying correct programs in a bag of samples, highlighting the need to consider syntactic correctness in program synthesis tasks. The text also mentions the assumption of independently sampled C programs and the importance of pruning syntactically incorrect programs before prediction. In program synthesis tasks, modeling the syntactical correctness of programs is crucial. By using Bayes rule, we can focus on p(\u03bb | IO k i k=1..K , stx) without considering the conditional dependency on the IO pairs. Constructing a checker based on a grammar can help identify valid prefixes of programs, useful for tasks like error detection in compilers and autocomplete features in IDEs. The probability p ( stx 1...t | s 1 \u00b7 \u00b7 \u00b7 s t ) is not a probability but indicates the validity of a token sequence as a prefix to a valid program. In program synthesis tasks, modeling the syntactical correctness of programs is crucial. The quantity p ( stx 1...t | s 1 \u00b7 \u00b7 \u00b7 s t ) is implemented as a deterministic process for a target programming language. A mask is used to ensure syntactical correctness, allowing beam search to explore useful candidates and optimizing for correctness. This approach restricts the dimension of the model space. In program synthesis tasks, modeling the syntactical correctness of programs is crucial. To simplify learning, the model's space dimension is restricted. A neural network module representing the syntax checker is proposed to aggressively prune the search in program space. The syntaxLSTM is conditioned only on program tokens, not on IO pairs, ensuring it models only the language syntax. Its output is passed through an exponential activation function and added to the decoder LSTM's output. The syntaxLSTM in program synthesis tasks outputs penalties for syntactically incorrect tokens. It does not require changes to the training procedure and can be penalized for giving negative scores to valid programs. Reference programs are used for supervised training in the Karel programming language, an educational language with an agent in a gridworld. The Code initiative (hoc) involves an agent in a gridworld with capabilities to move, modify world state, and query the environment. The goal is to learn to generate programs in the Karel DSL. The language supports loops and conditionals but no variable assignment. Evaluation is done using a synthetic dataset, ensuring generated programs have observable effects on the world. The programs in the Karel DSL have observable effects on the world and eliminate spurious actions. Input and output pairs are generated by running programs on random grids. 5000 programs are split into validation and test sets. Input and output elements are represented as grids with 16 channels. The Karel DSL programs have observable effects on the world and eliminate spurious actions. Input and output grids go through convolution layers, residual blocks, and a fully connected layer to create a 512-dimensional representation. Multiple decoders are used for prediction, with a maxpooling operation over their outputs. RL_beam optimization improves generalization accuracy over supervised learning, even with a drop in exact match recovery. Models were trained on both a full dataset of 1 million examples and a reduced dataset of 10,000 examples. The small dataset of 10,000 examples helps understand data efficiency in program synthesis. Models are grouped based on training objectives, with MLE used as a baseline. The Karel DSL was previously used for program induction, not synthesis, by BID6. Optimizing for correctness in program synthesis consistently improves top-1 generalization accuracy. RL models use different reward objectives and are initialized from pretrained supervised models. Top-1 Generalization Accuracy measures the accuracy of the most likely program. Improved performance of RL methods confirms the hypothesis that better loss functions can combat program aliasing. RL_beam methods show improvements in both exact match accuracy and generalization on the small dataset. Comparing RL_beam to standard RL, consistent improvements in accuracy are noted across all levels of generalization by aligning the RL objective with beam search decoding. Consistent improvements in accuracy are achieved by encouraging diversity in solutions and penalizing long running programs. RL methods show dramatic improvements over MLE in settings with little training data, indicating the potential for data efficiency improvement. Extensive supervised pretraining is necessary for RL methods to benefit from Reinforcement Learning. Grammar pruning improves accuracy in program synthesis, with MLE_handwritten showing gains over MLE without grammar, and MLE_large demonstrating further improvements with more parameters. Learning fine-tuning in Neural Machine Translation literature leads to better performance on small datasets compared to handwritten grammar and larger models. RL methods, particularly RL_beam, excel in top-1 accuracy but show a drop in higher-rank accuracy. RL_beam_div and RL_beam_div_opt help diversify generated programs and penalize redundant ones, improving program synthesis tasks. The comparison of syntax models shows that leveraging handwritten syntax leads to slightly better accuracies than learning syntax or using no syntax on the full dataset. However, when training data is limited, learning syntax produces significantly better performance. Incorporating syntactic structure in the model architecture and objective helps gain more leverage from small training data. Interestingly, the learned syntax model even outperforms the handwritten syntax model, suggesting that the syntaxLSTM can learn a richer syntax. The syntaxLSTM can learn a richer syntax, modeling the distribution of programs and discouraging unlikely predictions. Comparison with MLE_large shows that the larger number of parameters does not explain the performance difference, highlighting the utility of jointly learning syntax. Experimental evidence supports the decomposition of models into specialized decoders for likely tokens and enforcing grammar. Experimental evidence supports the decomposition of models into specialized decoders for likely tokens and enforcing grammar. The percentage of syntactically correct programs drops significantly when the syntaxLSTM's mask is not applied during decoding, indicating the reliance on syntaxLSTM for producing correct programs. Comparison between handwritten and learned syntax masks reveals instances where syntaxLSTM labeled a token correct when it was actually syntactically incorrect. The syntaxLSTM plays a crucial role in ensuring the correctness of generated programs by predicting unlikely tokens as incorrect. Red errors, where valid tokens are predicted as incorrect, are more problematic as they cannot be recovered. These errors often involve tokens rarely seen in the training data, showing that the syntaxLSTM learns to model program distributions. By considering nonsensical programs as incorrect, the syntaxLSTM helps generate syntactically correct and likely programs. The objective function for computing this process is described in this section. The distribution q \u03b8 over S programs is obtained by performing a beam search over p \u03b8 and renormalizing. C independent samples are taken from this distribution to obtain a reward. When the reward function is based on a boolean property, Equation (10) simplifies. The term max j\u22081.. C R i (\u03bb j ) equals 0 only if all C sampled programs give a reward of zero. The probability of not sampling a correct program out of the C samples is q C incorrect, leading to the derivation of Equation (11). This can be computed without additional sampling steps due to a closed-form solution. The distribution q \u03b8 over programs is obtained through beam search and renormalization. C samples are taken to obtain a reward, with a closed-form solution available for computation. The state of the grid is represented as a 16 \u00d7 18 \u00d7 18 tensor, with decoders being two-layer LSTM with a hidden size of 256. Tokens of the DSL are embedded into a 256-dimensional vector for input to the LSTM. The LSTM used for syntax modeling is 768 dimensional, with a separate decoder LSTM for each IO pair. The model outputs scores for 52 tokens, combined with syntax model output for SoftMax probability distribution. Training uses Adam optimizer with batch sizes of 128 for supervised learning and 16 for RL methods, with 100 rollouts per sample."
}