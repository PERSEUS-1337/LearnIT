{
    "title": "S1lEX04tPr",
    "content": "A new approach called CM3 addresses challenges in cooperative multi-agent control problems by restructuring the problem into a two-stage curriculum. This approach involves learning single-agent goal attainment before multi-agent cooperation, using a novel multi-goal multi-agent policy gradient with a credit function for localized credit assignment. The CM3 architecture learns faster than existing adaptations. The CM3 architecture, designed for cooperative multi-agent control problems, learns faster than existing adaptations on challenging tasks like cooperative navigation, multi-vehicle lane changes, and strategic cooperation in a Checkers environment. In scenarios requiring cooperation among autonomous agents, individual goals must be achieved while also working towards a global optimum through collaboration. In multi-agent systems, individual goals can lead to defection, even in settings with a global objective. Multi-agent reinforcement learning (MARL) has been enhanced with deep RL methods and shows promise in complex agent interactions. Learning multi-agent cooperation in the multi-goal scenario presents challenges in exploration and credit assignment. Agents need to explore efficiently to achieve individual goals and cooperate for others' success. Traditional random exploration may not be effective in discovering the value of cooperative actions. Modularized and targeted approaches are needed to differentiate between individual goal attainment and cooperation. Methods for multi-agent credit assignment are essential when agents share a single goal. In the multi-goal multi-agent setting, efficient exploration and credit assignment are key challenges. The paper introduces CM3, a framework with three components to address these issues. The paper introduces CM3, a novel framework with three components to address challenges in multi-agent exploration. The framework involves curriculum learning to train agents in a single-agent setting before introducing them to a multi-agent environment. Additionally, it leverages the decomposition of agents' observations into self, others, and non-agent specific environment information for complex MARL problems. The paper proposes a novel framework, CM3, for multi-agent exploration. It involves curriculum learning and function augmentation to bridge single-agent and multi-agent training stages. A credit function is introduced for localized credit assignment in multi-goal MARL. The method is evaluated on challenging environments with high-dimensional state spaces. CM3 outperformed IAC, COMA, and QMIX in various domains, highlighting the importance of its three key components. Recent literature has focused on leveraging deep RL techniques for complex agent interactions in high-dimensional environments, posing challenges for traditional methods. Cooperative multi-agent learning is crucial for decentralized systems where agents must coordinate to achieve common goals. The multi-agent credit assignment problem arises when agents share a global reward. While credit assignment can be resolved with individual rewards, fully cooperative settings require agents to optimize for collective success. COMA is effective for agents with different rewards but does not address multi-goal cooperation. In contrast to previous methods, our decentralized agents must cooperate concurrently to attain all goals, focusing on learning low-level cooperation after goals are assigned. This differs from multi-task MARL and hierarchical methods, as we conduct centralized training with decentralized execution. In multi-goal MARL, agents cooperate to achieve goals from a finite set, act independently with limited observations, and use an episodic multi-goal Markov game framework. The approach involves centralized training of decentralized policies and multi-agent credit assignment. In multi-goal MARL, agents cooperate in a Markov game with fixed individual goals. Each agent receives observations and chooses actions to maximize rewards. The learning task is to find decentralized policies to achieve goals, with joint policies factorizing due to decentralization. In multi-goal MARL, agents cooperate in a Markov game with fixed individual goals. A centralized critic can speed up training of decentralized actors in a diverse set of cooperation problems. Centralized learning of decentralized policies is essential for maximizing rewards in strategic games. In multi-goal MARL, agents cooperate in a Markov game with fixed individual goals. Centralized learning of decentralized policies is essential for maximizing rewards in strategic games. Critics evaluate the joint policy against the reward for each goal. COMA addresses credit assignment using a counterfactual baseline in single-goal MARL, but is unsuitable for multi-goal MARL as it treats all goals as a global goal. The complete CM3 learning framework includes a credit function for multi-goal MARL, a new cooperative multi-goal policy gradient with localized credit assignment, and a curriculum for training speedup. Function augmentation efficiently bridges policy and value functions across curriculum stages for a synergistic learning framework. The proposed approach in the current text chunk introduces a mechanism for credit assignment in a multi-goal actor-critic algorithm to improve joint action outcomes in a cooperative multi-agent setting. This method aims to address the limitations of evaluating joint actions based on global Q-functions by learning a function that evaluates pairs of actions and goals for each agent. The credit function for goal and action by agent satisfies relations necessary for sample-based model-free learning. Centralized training with few agents can lead to decentralized performance at scale. The credit function is used as a critic in a policy gradient for multi-goal MARL, optimizing the standard loss function in deep RL. The cooperative multi-goal credit function based MARL policy gradient is derived by maximizing the overall objective J(\u03c0) through ascending the gradient. The policy is updated based on the advantage of an agent over counterfactual actions, with the strength of interaction between action-goal pairs impacting the policy gradient magnitude. The complexity can be reduced by omitting terms for sparse interactions. The reduction of variance in A \u03c0 n,m can be analyzed similarly to COMA, with stability improvement shown in ablation results due to the credit function. Multi-goal MARL faces challenges in exploration efficiency, where random exploration may not effectively balance individual task completion and cooperative behavior learning. Agents who can achieve individual goals independently can more easily discover cooperative solutions through additional exploration in a multi-agent environment. A MARL curriculum is proposed, starting with solving a single-agent MDP before exploring a cooperative multi-goal Markov game. The curriculum aims to prepare agents for efficient exploration and coordination in a multi-agent setting. In cooperative multi-agent environments, reducing dependencies on agent interactions allows for a simplified model focusing on a single agent. This reduction is feasible in various real-world scenarios like autonomous driving and traffic light control. The process involves deleting components related to other agents from state vectors, reward functions, and transition functions. Practical guidelines for this reduction are provided in the appendix. In cooperative multi-agent environments, reducing dependencies on agent interactions allows for a simplified model focusing on a single agent. Practical guidelines for this reduction are provided in Appendix G. A greedy policy is defined for cooperative multi-goal scenarios, with a curriculum that trains a single agent in Stage 1 to achieve a greedy policy for initialization in Stage 2. Leveraging the structure of decentralized MARL, egocentric observations are private and not accessible by other agents. In Stage 1, Q 1 and \u03c0 1 learn to achieve multiple goals in a single-agent environment. Between Stage 1 and 2, \u03c0 is constructed from the trained \u03c0 1 and a new module \u03c0 2. In Stage 2, these augmented functions are instantiated for each of N agents in a multi-agent environment with parameter-sharing. In Stage 1, the input space of policy and value functions is reduced to lower computation cost. In Stage 2, parameters are restored and new modules are activated to process additional inputs. CM3 is created using deep neural networks for function approximation, assuming parameter-sharing among homogeneous agents with goals as input. In Stage 1, actor-critic models are trained for all goals. In Stage 2, a Markov game is instantiated with multiple agents, where a second neural network is introduced to process inputs from other agents. This augmentation scheme allows agents to cooperate and achieve goals while sharing parameters and learning from surrounding agents. The augmentation scheme for deep policy and value networks allows agents to cooperate and achieve goals by sharing parameters. The global Q-function is constructed from individual Q-functions, and the performance of this method was evaluated on various challenging multi-goal MARL environments. In autonomous driving and strategic cooperation in a Checkers game, ablations of CM3 were evaluated across different domains. Three variants of cooperative navigation scenarios were created, increasing difficulty by giving individual rewards based on distance to targets. Agents in SUMO traffic simulator observe relative positions and velocities for real-world driving cooperation among drivers with personal goals. The experiment involves agent vehicles learning double-merge maneuvers in the SUMO traffic simulator to reach goal lane assignments. Sparse rewards are given to agents with limited field of view. A strategic game called Checkers was implemented to investigate the benefits of cooperation even when agents cannot maximize rewards individually. In the game, agents must clear paths for each other in a gridworld with disappearing colored squares. In cooperative navigation, agents learn to reach specified landmarks or goals. Different algorithms like COMA, IAC, and QMIX are used to train agents independently or jointly, with details provided in appendices. In cooperative navigation, agents use algorithms like COMA, IAC, and QMIX to reach goals. QMIX utilized a hypernetwork with individual goals for each agent network. Ablation experiments were conducted to analyze the speedup from curriculum with function augmentation and the benefit of the new credit function and multi-goal policy gradient in CM3. CM3 outperforms IAC and COMA in cooperative navigation scenarios, converging faster with curriculum learning. QMIX settles at suboptimal behavior due to processing all goals together, unlike CM3 and IAC which handle each goal separately. In SUMO, CM3 and QMIX found cooperative solutions with performances within the margin of error, while COMA and IAC struggled with credit assignment and local optima, hindering merge maneuvers in vehicles. CM3 addressed credit assignment challenges effectively, outperforming IAC in longer sequences of cooperative actions with sparser rewards. In Checkers, CM3 outperformed COMA and QMIX by converging to the global optimum with a score of 24 in 10k episodes faster. CM3 successfully addressed challenges in exploration of joint trajectory space and credit assignment for path clearing. COMA only solved Checkers due to the small bounded environment, while IAC underperformed in discovering cooperative actions with no instantaneous reward. These results demonstrate CM3's ability to achieve individual goals and find cooperative solutions. Results show CM3's superior performance compared to \"Direct\" in attaining individual goals and finding cooperative solutions in diverse multi-agent systems. Learning individual goal attainment before multi-agent cooperation and initializing Stage 2 with Stage 1 parameters are crucial for improving learning speed and stability. Function augmentation eases the learning problem, highlighting the importance of the credit function in maintaining a cooperative solution. CM3 is a general framework for cooperative multi-goal MARL that addresses the need for efficient exploration to learn both individual goal attainment and cooperative solutions. CM3 addresses the need for efficient exploration in multi-agent systems by bridging individual goal attainment and cooperation through a two-stage curriculum. It achieves local credit assignment using a credit function in a multi-goal policy gradient, outperforming existing MARL methods in various domains. Ablations show the importance of each component in the framework, motivating further analysis of its theoretical properties and potential generalization to different systems. Off-policy training with a large replay buffer allows RL algorithms to benefit from less correlated transitions. The algorithmic modification involves maintaining a circular replay buffer and conducting training while executing policies in the environment. Despite introducing bias in MARL, off-policy training was found to be beneficial. Off-policy training benefits CM3 in SUMO and Checkers by introducing bias in MARL. The credit function can be expressed in terms of the goal-specific action-value function, and elementary relations between global functions V \u03c0 n (s) and Q \u03c0 n (s, a) are stated. The proof of the policy gradient theorem is followed, and Q \u03c0 n (s, a) can be replaced by the advantage function A \u03c0 n (s, a) := Q \u03c0 n (s, a) \u2212 V \u03c0 n (s). The advantage function A \u03c0 n (s, a) is derived from Q \u03c0 n (s, a) and V \u03c0 n (s). The choice of agent label k in the gradient calculation is arbitrary. By summing over all agents, Proposition 2 is obtained. The relation between V \u03c0 n (s) and Q \u03c0 n (s, a) is explained, along with the application of COMA in cooperative multi-goal MARL. The application of COMA involves defining gradients and variances, with a focus on multi-agent exploration. A simple thought experiment in a two-player gridworld demonstrates the benefits of greedy initialization over random exploration. The probability of a symmetric optimal trajectory in a multi-agent scenario is calculated based on different actions and exploration strategies. Policies trained with few agent vehicles on an empty road are tested for generalization to heavy traffic situations. Results show better performance of CM3 compared to IAC and COMA in training and testing scenarios. Centralized training with few agents shows minimal performance decrease from train to test, supporting the feasibility for deployment in scenarios with many agents. CM3's higher sample efficiency does not incur greater computational cost. Test times are similar across all neural networks. The experimental domains and agent observations are defined, with CM3 leveraging a decomposition for faster training compared to IAC, COMA, and QMIX. The domain is adapted from Lowe et al. (2017) multi-agent particle environment. The multi-agent particle environment in Lowe et al. (2017) involves movable agents and static landmarks in a 2D world with position and velocity. Agents experience contact forces, inertia, and friction. The global state vector includes agents' positions and velocities. Agents observe themselves and others, taking actions from a set including movement options. Landmarks are randomly located with a certain probability. In the multi-agent particle environment, agents and landmarks are placed in a 2D world with random locations. Agents have individual rewards based on their distance from assigned landmarks and receive penalties for collisions. The episode ends when agents are close enough to their landmarks. The N = 1 case of the Markov game involves a single agent reaching landmarks in a simulated road environment. Vehicles are emitted at an average speed of 30m/s with small deviation, and the global state vector includes agents' positions and speeds normalized by road dimensions. The N = 1 case of the Markov game involves a single agent reaching landmarks in a simulated road environment. The agent's observation of itself includes speed, number of sub-lanes to goal lane, and distance to goal position. Observation of others includes vehicle occupancy and relative speed. All agents have the same discrete action space with options for maintaining speed, accelerating, decelerating, and shifting lanes. In a simulated road environment, agents have discrete actions like maintaining speed, accelerating, decelerating, and shifting lanes. Each agent's goal is to reach a specific lane, with departure times and rewards determined by certain conditions. In a simulated road environment, agents have discrete actions to reach a specific lane. Rewards and termination conditions are defined based on collision, time-out, reaching the end of the road with a specific sub-lane difference, and exceeding a certain speed limit. The environment is a gridworld with 5 rows and 13 columns, where agents cannot occupy the same grid location and collectible rewards are placed. In a gridworld environment, agents cannot share the same location. Red and yellow rewards are placed in a checkered pattern in the middle 3x8 region. The global state includes information on the presence of rewards and agents' locations and collected rewards. Agents observe others' normalized coordinates and their own location and collected rewards. In a gridworld environment, agents collect red and yellow rewards without touching the opposite color. Agents choose from movement actions to navigate the grid. Each agent has a specific goal and receives rewards based on the color collected. The global state includes reward presence, agents' locations, and collected rewards. In various domains, ReLU nonlinearity is used for neural network layers. All layers are fully-connected feedforward layers with discrete action spaces. The neural networks for COMA do not use recurrent networks or feed previous actions into the Q function. The Q network in COMA interprets the value of each output node as the action-value for the agent taking that action. Agent n's label vector and observation self are used in COMA. COMA's global Q function uses agent n's label vector and observation as input to differentiate between evaluations for different agents. It computes Q(s, (a n , a \u2212n )) for each agent by passing through two layers and connecting to a linear output layer. Stage 1 of COMA feeds the concatenation of state, goal, and action to a layer with 256 units. The global Q function of COMA utilizes agent n's observation and label vector to differentiate evaluations for each agent. It involves passing through convolutional layers and concatenating various inputs before connecting to two layers with 256 units each and a linear output layer. The concatenation passes through two layers with 256 units each and a linear output layer with 5 units. Individual value functions are defined for agent n, involving convolutional layers and concatenation with other inputs before connecting to hidden layers and output layers. The mixing network follows the architecture of Rashid et al. (2018) with specific hyperparameters and optimizer settings. The training curves for all three experimental domains with a minibatch size of 128 are shown in Figure 6."
}