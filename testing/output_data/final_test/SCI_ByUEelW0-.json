{
    "title": "ByUEelW0-",
    "content": "Long Short-Term Memory (LSTM) units can memorize long-term dependencies in time series data by modifying the cell state with rotation matrices parametrized by trainable weights. This enhancement has shown improved performance on tasks from the bAbI dataset. Recurrent Neural Networks (RNNs) have been successfully applied in various domains such as Natural Language Processing (NLP), speech recognition, computer vision, and differentiable programming language interpreters. In this paper, a mechanism to enhance a standard RNN to modify its memory is studied, aiming to capture sequence information more effectively. The focus is on improving the Long Short-Term Memory (LSTM) unit by enabling it to adjust its memory cells in a gated fashion at each time step. This enhancement is crucial for tasks like natural language understanding, where capturing long-term dependencies is essential. In this work, a new operation is introduced to enhance LSTM units by enabling rotations and swaps of memory elements, aiming to improve performance on bAbI tasks. The idea involves adding a new set of parameters for the RNN cell to enable rotation of the cell state, with preliminary tests showing promising results. The LSTM unit introduces gated modified states to prevent memory vanishing or exploding. A rotation matrix is applied to modify the memory while retaining an amplification factor of 1. The rotation is parametrized by a vector of angles, learned along with other parameters. The rotation matrix enables rotations and swaps of memory elements to enhance LSTM performance on bAbI tasks. The LSTM unit introduces gated modified states to prevent memory vanishing or exploding. A rotation matrix is applied to modify the memory while retaining an amplification factor of 1. The rotation is parametrized by a vector of angles, learned along with other parameters. The rotation matrix enables rotations and swaps of memory elements to enhance LSTM performance on bAbI tasks by limiting rotations to pairs of inputs and adding memory rotation after the forget and add gates. The RotLSTM unit introduces a rotation gate to LSTM cells, aiming to improve cell state representation without increasing parameters significantly. Empirical evaluation on the bAbI dataset shows promising results with rotations limited to pairs of inputs after forget and add gates. The RotLSTM unit introduces a rotation gate to LSTM cells, aiming to improve cell state representation without increasing parameters significantly. It differs from a regular LSTM by introducing a network producing angles u t and a rotation module marked U. Tasks include reasoning about size and path finding. Evaluation compares RotLSTM with traditional LSTM models on various tasks. The RotLSTM unit introduces a rotation gate to LSTM cells to improve cell state representation. Models are trained with fixed hyperparameters and varying cell state sizes to assess performance. The model architecture is based on the Keras example implementation and shows better performance than traditional LSTM. Input question and sentences pass through separate word embedding layers. The model architecture includes separate word embedding layers for questions and sentences. An RNN processes the question, which is then concatenated with word vectors from the story for the second RNN (Query). The output is passed through a fully connected layer with softmax activation. Dropout layers drop 30% of nodes, and training uses categorical cross-entropy loss. The optimizer is Adam with specific parameters, and models are trained for 40 epochs with shuffled data. The model architecture includes separate word embedding layers for questions and sentences. An RNN processes the question, which is then concatenated with word vectors from the story for the second RNN (Query). The output is passed through a fully connected layer with softmax activation. Dropout layers drop 30% of nodes, and training uses categorical cross-entropy loss. The optimizer is Adam with specific parameters, and models are trained for 40 epochs with shuffled data. All models were trained for 40 epochs, evaluated on validation, training, and test sets. Experiments were run 10 times with different random seeds for reproducibility. Comparing LSTM and RotLSTM units on the bAbI dataset, RotLSTM shows slight overall performance improvement and significant improvements on specific tasks like faster convergence and smaller state sizes. RotLSTM outperforms LSTM on tasks 1, 11, 12, and 13 consistently. The RotLSTM model outperforms the LSTM model on specific tasks, reaching top performance with smaller cell state sizes and faster convergence. It shows improvements in accuracy up to 2.5% and 20% in final accuracy. In tasks 18, 5, and 7, RotLSTM demonstrates significant performance increases, with accuracy improvements of 22.1% in task 5 and 14% in task 7. The RotLSTM model shows faster learning and better performance in early epochs compared to the LSTM model on specific tasks. However, signs of overfitting are more prominent in the RotLSTM model on certain tasks. Both models perform poorly on tasks requiring multiple supporting facts and time manipulation. The RotLSTM model focuses on relevant input sentences to answer questions without filtering out irrelevant information. It applies pairwise 2D rotations, but future work aims to explore rotating groups of elements and multi-dimensional rotations to enhance memory representations. The RotLSTM model uses pairwise 2D rotations to enhance memory representations for answering questions. Future work aims to explore rotating groups of elements and multi-dimensional rotations for richer memory representations. Further research could investigate the impact of adding constraints on learning times and performance across different datasets. In future work, the RotLSTM model plans to explore incorporating rotations on various domains and tasks using real-world datasets. Tuning hyperparameters of rotation models for better performance is a goal, along with a qualitative analysis of rotation gates to understand their impact on memory representation. The GRU unit is a successful mutation of LSTM with fewer gates. A novel gating mechanism for RNN units allows for applying rotations to the cell state, creating RotLSTM. Accuracy comparison between LSTM and RotLSTM models shows promising results. Future work includes exploring rotations on different tasks and domains using real-world datasets. Tuning hyperparameters and analyzing rotation gates for memory representation impact are also planned. The RotLSTM model, trained every 10 epochs, showed significant accuracy improvements of approximately 20% over the LSTM model on bAbI tasks 5 and 18. Adding rotations to LSTM-based architectures positively impacted most tasks, requiring fewer epochs to achieve similar or higher accuracy. The RotLSTM model could use a lower dimensional cell state vector while maintaining performance."
}