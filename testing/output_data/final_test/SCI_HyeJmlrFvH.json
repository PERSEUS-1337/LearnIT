{
    "title": "HyeJmlrFvH",
    "content": "As models and datasets grow in size and complexity, the need for communication-efficient variants of stochastic gradient descent on clusters increases. Alistarh et al. (2017) introduced QSGD and QSGDinf, two data-parallel SGD variants that quantize and encode gradients to reduce communication costs. We propose an alternative gradient quantization scheme with stronger theoretical guarantees than QSGD and comparable empirical performance to QSGDinf, addressing the challenges of training large neural networks in distributed settings. Models can no longer be trained on a single machine, leading to the use of distributed systems for training. Data-parallel versions of stochastic gradient descent (SGD) are scalable and take advantage of multi-GPU systems. In data-parallel SGD, a large dataset is partitioned among processors to minimize an objective function. Each processor has access to the current parameter vector of the model. In data-parallel stochastic gradient descent (SGD), each processor computes an updated gradient using local data and shares it with peers to aggregate and update the parameter vector. Increasing processing machines reduces computational costs but raises communication costs significantly. Communication time to share gradients and parameters becomes a performance bottleneck, hindering the benefits of cost reduction. Reducing communication costs in data-parallel SGD is crucial for efficiency. Gradient compression, such as gradient quantization, is a solution to reduce communication costs in data-parallel SGD. Unlike full-precision SGD, quantization only requires transmitting a few bits per iteration for each component of the stochastic gradient. Quantized SGD (QSGD) is a popular proposal for communication compression in this context. Quantized SGD (QSGD) compresses stochastic gradient vectors by normalizing them to have unit L2 norm and quantizing each element to a uniform grid of levels. QSGD provides convergence guarantees and allows users to adjust communication bandwidth and convergence time by changing quantization levels. Alistarh et al. present empirical results using L\u221e normalization, calling this variation QSGDinf. In response to the empirical results using L\u221e normalization, a new quantization scheme called QSGDinf was introduced. However, the theoretical guarantees on the number of bits transmitted no longer apply, and the performance is not as strong as SGD and QSGDinf. To address this, a new quantization scheme is proposed that provides stronger theoretical guarantees on variance, bandwidth, and cost while maintaining practical performance. This scheme uses an unbiased nonuniform logarithmic quantization scheme, similar to those used in audio compression systems. The new algorithm, NUQSGD, is a quantized data-parallel SGD algorithm with strong theoretical guarantees, allowing users to balance communication costs and convergence speed. It outperforms QSGD on deep models and large datasets, matching QSGDinf. The nonuniform quantization scheme concentrates levels near zero, reducing excess variance. Efficiently implemented in Pytorch, NUQSGD is benchmarked on image classification tasks. In the overparametrized regime, stronger bounds on excess variance are established as quantization levels increase rapidly. A bound on total communication costs is obtained, slightly stronger than QSGD. NUQSGD shows smaller variance than QSGD in deep models, leading to improved optimization performance in training loss and test accuracy. It matches QSGDinf in terms of variance. NUQSGD matches QSGDinf in terms of variance and loss/accuracy, reducing communication costs in distributed training. It provides faster parallel training on ImageNet compared to other methods. Stronger theoretical guarantees are established for excess variance and communication costs, with improved convergence guarantees. NUQSGD demonstrates strong empirical performance on deep models and large datasets, closing the gap between theory and practice. SignSGD, proposed by Seide et al. (2014), efficiently reduces communication costs by quantizing gradients to two values. Bernstein et al. (2018) later provided convergence guarantees for SignSGD, which is not unbiased. Sa et al. (2015) introduced Buckwild!, a lossy compressed SGD with convergence guarantees. TernGrad, a stochastic quantization scheme with three levels, also reduces communication costs significantly. NUQSGD uses a logarithmic quantization scheme for efficient communication and accuracy, with a generic implementation in Horovod. Logarithmic quantization schemes have been used in various systems for compression. In recent studies, various compression schemes have been proposed for network optimization. One such scheme is logarithmic quantization, used in NUQSGD for efficient communication and accuracy. This method is applied to high-dimensional machine learning models to minimize an unknown, differentiable, convex, and smooth function. The update rule for conventional full-precision projected SGD is defined by w t+1 = P \u2126 (w t \u2212 \u03b1g(w t )), where w t is the current parameter input, \u03b1 is the learning rate, and P \u2126 is the Euclidean projection onto \u2126. The stochastic gradient has a second-moment upper bound B when E[ g(w) 2 ] \u2264 B for all w \u2208 \u2126, with classical convergence guarantees for conventional full-precision SGD given access to stochastic gradients at each iteration. Projected SGD update is executed for T iterations with \u03b1 = 1/(\u03b2 + 1/\u03b3) where \u03b3 = r 2/T /\u03c3. Minibatched and data-parallel SGD are common variants to reduce variance and improve efficiency. Data-parallel SGD involves processors computing their own gradients and aggregating them for parameter updates. Data-parallel SGD is a special case of Algorithm 1 with identity encoding and decoding mappings. The convergence guarantees for full-precision data-parallel SGD are provided in Corollary 1. Communication costs are the main bottleneck in large-scale distributed systems for data-parallel SGD. To reduce communication costs and speed up training in large-scale distributed systems, a compression scheme called QSGD is introduced. It produces a compressed and unbiased stochastic gradient by normalizing and quantizing the gradients. Each processor broadcasts its compressed gradient, decodes received gradients, and sums them to produce a stochastic gradient. The quantization levels are distributed uniformly to avoid bias, and a lower bound on the number of quantized zeros is provided. The paper introduces a new scheme to quantize normalized gradient vectors, aiming to balance communication savings with variance for faster convergence. The proposed scheme deviates from uniform quantization levels to address the limitations of existing methods. The paper proposes a nonuniform quantization scheme for normalized gradient vectors to improve communication efficiency and reduce variance in stochastic gradient descent. This scheme, based on a randomized quantization approach, outperforms uniform quantization by better matching the properties of normalized vectors and reducing quantization error. Additionally, it controls communication costs and gradient variance by increasing the number of quantization levels near zero. Empirical results show that this scheme aligns well with the distribution of normalized coordinates in real datasets. The nonuniform quantization scheme proposed in the paper improves communication efficiency and reduces variance in stochastic gradient descent by better matching the properties of normalized vectors. It controls communication costs and gradient variance by increasing the number of quantization levels near zero, aligning well with the distribution of normalized coordinates in real datasets. The nonuniform quantization scheme with fine intervals for small values of r i in stochastic gradient vectors aims to reduce quantization error and control variance. The encoded local gradients are broadcasted as binary strings. Theoretical guarantees for NUQSGD are provided, comparing it with QSGD. Theorems 2, 3, 4, and 5 proofs are in Appendices B, C, D, and E respectively. Theorem 2 states that if g(w) has a second-moment bound \u03b7, then Q s (g(w)) has a variance upper bound \u03b5 Q \u03b7. For large d, the variance upper bound decreases with more quantization levels. The expected number of communication bits to encode the quantized stochastic gradient is bounded by Theorem 3. The code-length increases with d and s, but in the sparse case with s = o(log d) levels, the expected code-length is bounded. Corollary 1 combines the upper bounds on variance and code-length. Combining the upper bounds on variance and code-length, Corollary 1 provides guarantees for NUQSGD in smooth convex optimization. NUQSGD requires at most N Q communication bits per iteration in expectation. On nonconvex problems, weaker convergence guarantees can be established. Comparing QSGD and NUQSGD in terms of communication bits for a given suboptimality gap \u03b5 is a key consideration. The variance upper bound controls the convergence speed in algorithms, influenced by quantization schemes with small s levels and large d dimensions. NUQSGD shows smaller variance bounds for various s and d ranges, independent of dataset or network structure. An upper bound on expected bits used per iteration leads to total bits communicated over T iterations. The expected number of communication bits over T iterations by algorithm A is bounded by \u03b6 A,\u03b5 = T A,\u03b5 N A. NUQSGD provides stronger guarantees for suboptimality gap of \u03b5 without data assumptions. Empirical analysis compares NUQSGD to QSGDInf in terms of convergence and speedup. NUQSGD, QSGD, and QSGDInf methods are implemented and tested for performance and accuracy compared to SuperSGD. The study evaluates convergence, variance, and speedup using Pytorch with Horovod backend for quantization support. Impact of quantization on training performance is measured for ResNet models on ImageNet and CIFAR10 datasets. In experiments with ResNet models on ImageNet and CIFAR10 datasets, NUQSGD and QSGDinf show improved training loss compared to QSGD. Significant performance gaps are observed in test accuracy as well. In experiments with ResNet models on ImageNet and CIFAR10 datasets, NUQSGD and QSGDinf show improved training loss compared to QSGD. Significant performance gaps are observed in test accuracy. NUQSGD has smaller variance for large models with a small number of quantization bits. Implementation of quantization methods in Horovod for speedup behavior requires non-trivial refactoring. Horovod applies \"tensor fusion\" to merge gradient tensors for efficient transmission, but this can lead to accuracy loss. Tuning the fusion process minimizes this loss. Quantizing biases significantly impacts accuracy, so they are transmitted at full precision. Quantized values are packed into 32-bit numbers without additional encoding for efficiency. Compression and de-compression are implemented via CUDA kernels. The implementation of efficient CUDA kernels for compression and decompression is compared against various baselines, including full-precision SGD and Error-Feedback SignSGD. Results show that the compressed variant achieves efficient and scalable performance on ResNet34 and ResNet50 models on ImageNet, with reduced communication volume and minimal overhead. NUQSGD achieves significant speedup over standard data-parallel variants on ResNet50/ImageNet with 4 GPUs, matching 32-bit model accuracy. EF-SignSGD required extensive hyperparameter tuning to converge on ImageNet, but ultimately achieved full accuracy on ResNet50. The study explores a nonuniform quantization scheme for data-parallel and communication-efficient stochastic gradient descent. It shows that EF-SignSGD on ResNet50 achieves full accuracy but with increased cost due to quantization. The tuned variant is slower than NUQSGD-4bit but slightly better than NUQSGD-8bit in terms of speedup. The scheme offers a trade-off between variance and expected code-length. NUQSGD provides a trade-off between communication efficiency and convergence speed, offering stronger guarantees compared to QSGD. Experimental results show that NUQSGD matches the performance of QSGDinf on practical deep models like ImageNet. Future work aims to explore more complex reduction patterns for communication compression. NUQSGD offers a trade-off between communication efficiency and convergence speed, with experimental results showing performance matching QSGDinf on models like ImageNet. Future work aims to explore more complex reduction patterns for communication compression, such as ring-based reductions. The encoding process uses Elias recursive coding (ERC) to encode positive integers efficiently. The DECODE function reconstructs the coordinates by reading bits and decoding the index of the first nonzero coordinate. Elias coding assigns shorter codes to smaller values, making it suitable for the scheme. The Elias recursive coding scheme efficiently encodes positive integers with a recursive structure. The variance of Q s (v) for any quantization scheme is expressed in a lemma, where r i = |v i |/ v and p(r), s(r), \u03c4(r) are defined. The NUQSGD algorithm uses quantization levels L = (0, 1/2 s , \u00b7 \u00b7 \u00b7 , 2 s\u22121 /2 s , 1) with defined h i (v, s)'s based on quantization intervals. The Elias recursive coding scheme efficiently encodes positive integers with a recursive structure. The variance of Q s (v) for any quantization scheme is expressed in a lemma, providing an upper bound on the variance of Q s (v) based on quantization intervals. Lemma 2 bounds the expected number of nonzeros in Q s (v). The expected number of communication bits per iteration is bounded above by a quantized gradient Q s (v) determined by a tuple (v, \u03c1, h). The encoding produced by ENCODE(v) can be partitioned into two parts, R and E, containing codewords for runs of zeros and sign bits respectively. The encoding produced by ENCODE(v) consists of two parts: R for runs of zeros and E for sign bits and normalized quantized coordinates. Lemma 3 provides an upper bound on the expected code-length, showing that g1(x) is concave for all k > 0. The function g1(x) is concave for all k > 0. Additionally, the function g2(x) is concave on x < C and increasing up to some C/5 < x * < C/4. The variance for NUQSGD and QSGDinf is lower than SGD for most training instances, decreasing after the learning rate drops. SuperSGD has the lowest variance achievable. In overparameterized networks, NUQSGD and QSGDinf outperform QSGD in terms of test accuracy for training ResNet models on CIFAR10 and ImageNet. NUQSGD consistently achieves higher test accuracy compared to QSGD, which does not recover the test accuracy of SGD. NUQSGD and QSGDinf have lower variance compared to QSGD, leading to better performance in terms of training loss and generalization error. The normalized variance of NUQSGD and QSGDinf remains lower than SGD throughout training, especially on CIFAR10. Variance is measured on individual trajectories, making direct comparisons difficult. Variance decreases on CIFAR10 after learning rate drops and does not grow as much as SGD on ImageNet. Variance and normalized variance are evaluated at fixed snapshots during training. The variance of gradient estimates on the trajectory of single-GPU SGD on CIFAR10 is studied. QSGD shows high variance, while QSGDinf and NUQSGD have lower variance. A normalized variance measure is proposed to assess stochasticity. The mean normalized variance of the gradient is proposed as a measure of stochasticity in optimization algorithms. It is argued that noise in optimization is more problematic when it is significantly larger than the gradient. Normalized variance can be seen as the inverse of Signal to Noise Ratio (SNR) for each dimension. The impact of noise sources like quantization may only be noticeable when the gradient norm decreases. Figure 5 shows the mean normalized variance of the gradient over training iterations, highlighting differences between QSGD and unnormalized variance. The quantization noise of QSGD can slow convergence at the end of training compared to the beginning. Variance in optimization trajectories is not directly comparable but should be studied for general trends. Weak scaling results for ResNet152/ImageNet show superior scaling behavior for NUQSGD. Performance comparison for NUQSGD variants (bucket size 512) is presented in Figure 9. In Figure 9, a performance comparison is shown for NUQSGD variants (bucket size 512) and a convergent variant of EF-SignSGD with parameter tuning for convergence at ImageNet scale. Tuning included scaling factor, quantization granularity, and learning rate. Warm start was not attempted to maintain algorithm practicality. Bucket size 64 was found to be the highest for convergence on this model and dataset. 1-bit SGD scaling yielded good results. The EF-SignSGD algorithm sends 128 bits per bucket of 64 values, doubling its baseline communication cost. The GPU implementation is less efficient due to error feedback computation and less parallelism. Performance is close to 8-bit NUQSGD and inferior to 4-bit NUQSGD. The variance optimization problem R2 is an integer nonconvex problem with tight bounds in dimension d. The variance optimization problem R2 in dimension d is an integer nonconvex problem. An upper bound on the optimal objective of problem R2 can be obtained by relaxing the integer constraint. The resulting QSQP can be efficiently solved using standard interior point-based solvers. A coarser analysis yields an upper bound expressed as the optimal value of an LP, providing an upper bound on the nonuniform quantization of v. For the special case with s=1, the optimal level to minimize the worst-case bound obtained from problem P1 is 1/2. In the context of variance optimization in dimension d, the optimal value of P 0 increases monotonically in the range (d 0 , d 1 ). The optimal solution for P 0 lies on the boundary line of the feasibility region, with potential candidates being (d \u2212 (1/l 1 ) 2 , (1/l 1 ) 2 ) and (d, 0). The optimal value of P 0 can be determined by substituting into the objective function. Additionally, when considering exponentially spaced levels, efficient solutions can be found for the worst-case variance bound by solving quadratic and linear programs. The optimal value of p that minimizes the worst-case variance bound can also be determined. The numerical results obtained by solving QCQP Q 1 with L p versus p using CVX show that the variance upper bound increases as d increases and decreases as s increases. The optimal value of p shifts to the right as d increases and to the left as s increases, indicating near optimality in some cases. NUQSGD is guaranteed to converge to a local minima for smooth general loss on nonconvex problems. NUQSGD is guaranteed to converge to a local minima for smooth nonconvex optimization with a learning rate \u03b1 = O(1/\u03b2) on K processors, each with access to independent stochastic gradients of f. The algorithm guarantees E[\u2207f(wR)2] \u2264 \u03b5 where \u03b5 = O\u03b2(f(w0) \u2212 f*)/T + (1 + \u03b5Q)B."
}