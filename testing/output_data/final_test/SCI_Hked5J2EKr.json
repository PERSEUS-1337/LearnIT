{
    "title": "Hked5J2EKr",
    "content": "Expressing Metropolis accept/reject decisions as comparisons with a uniform [0,1] value and updating this value non-reversibly within the Markov chain state can improve random walk Metropolis and Langevin updates in high dimensions. This approach yields significant enhancements when using Langevin updates with persistent momentum, achieving performance similar to Hamiltonian Monte Carlo (HMC) with long trajectories. In scenarios where some variables are updated by other methods, such as in a Bayesian neural network model, utilizing Langevin updates allows for more frequent updates compared to HMC, which only allows updates between trajectories. Updating the uniform variable u non-reversibly within the Markov chain state can improve Metropolis accept/reject decisions. This approach can lead to clustered acceptances and rejections, which is beneficial in scenarios where rejections cause reversals of direction. By augmenting the variable of interest x with a variable s, updates to u can enhance the performance of MCMC methods like Langevin updates with persistent momentum. The joint distribution for x and s is uniform over the region where 0 < s < \u03c0(x). Slice sampling and Metropolis updates involve sampling s from [0, \u03c0(x)] for decision-making. Retaining s in the state can improve accept/reject decisions by utilizing its current value. Alternatively, a uniform variable u can be used, corresponding to s/\u03c0(x), for decision-making. In non-reversible updates, decisions are made using a uniform variable u = s/\u03c0(x). Updates for u involve translating it by a fixed amount \u03b4 and adding noise, with boundaries at 0 and 1. This update is expressed in terms of a variable v that is uniform over [\u22121, +1]. The full state includes x with density \u03c0(x) and v being uniform over [\u22121, +1]. Non-reversible updates involve changing a variable u = s/\u03c0(x) by a fixed amount \u03b4 and adding noise, with boundaries at 0 and 1. The update is related to a variable v that is uniform over [\u22121, +1]. When \u03c0(x) varies continuously, noise may not be needed for updating v, but it could be necessary if \u03c0(x) has a finite number of values to ensure ergodicity. The aim of these updates is for u to transition slowly between values near 0 and 1, with acceptances and rejections becoming more clustered. Acceptances and rejections become more clustered, with accepted proposals likely followed by another acceptance and rejected proposals likely followed by another rejection. Different types of updates for x can be intermingled, such as Gibbs sampling or standard Metropolis updates. Non-reversible updating of u can benefit simple random-walk Metropolis updates. The values for \u03c3 used were scaled down by 40 1/2. Autocorrelation times are shown for groups of 40 iterations. Little difference is seen between standard and non-reversible updates for estimating the mean of a single coordinate, but the non-reversible method is about 1.14 times better for estimating the mean of the log of the probability density. Simple Langevin updates also show a small benefit. The performance of Metropolis methods in high dimensions is limited by their ability to sample different values for the log of the density. The Metropolis update in high dimensions is limited by the random walk nature of changes in log density. Non-reversible updates for u can help by reducing the random walk behavior, especially when u is small. This can lead to more interesting results in sampling log densities. When applying the non-reversible acceptance method to Hamiltonian Monte Carlo, interesting results were obtained. This method involves simulating Hamiltonian dynamics with momentum variables and proposing new values by taking leapfrog steps. Horowitz's method uses only one leapfrog step but employs a trick to keep the steps going. Horowitz's method involves using a non-reversible acceptance method in Hamiltonian Monte Carlo, where one leapfrog step is taken with a trick to maintain direction. The method aims to reduce rejection rates by using small step sizes and clustering rejections in time for more efficient runs. The non-reversible method in Hamiltonian Monte Carlo reduces rejection rates by clustering rejections in time for more efficient runs. It outperforms the standard method in estimating the mean log probability. Persistent Langevin updates with non-reversible updates for sampling the posterior distribution of a Bayesian neural network model show promise in improving convergence by allowing hyperparameters to be updated more frequently. This approach aims to reduce random walks and cluster rejections in time for more efficient runs. Persistent Langevin updates with non-reversible updates aim to improve convergence by updating hyperparameters more frequently. Testing on a binary classification problem with 5 inputs and 300 training cases showed promising results with a network using one hidden layer of 12 tanh units. The best-tuned method had a smaller autocorrelation time compared to HMC, indicating potential for better performance."
}