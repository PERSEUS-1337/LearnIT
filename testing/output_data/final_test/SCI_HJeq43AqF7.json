{
    "title": "HJeq43AqF7",
    "content": "Syntax is a powerful abstraction for language understanding, and models for learning semantic representations of text benefit from integrating syntax. The deep inside-outside recursive autoencoder (DIORA) is a fully-unsupervised method for discovering syntax and learning representations for constituents within the induced tree. Unlike supervised parsers, DIORA does not rely on supervision from auxiliary downstream tasks and can be applied across different domains. Competing approaches lack explicit phrase representations with tree structures, limiting their use in phrase-based tasks. DIORA achieves state-of-the-art unsupervised parsing results on the WSJ dataset. Syntax, represented by parse trees, is crucial for various NLP tasks like relation extraction and text classification. Traditional supervised parsers rely on small datasets like the Penn Treebank, restricting their applicability to newswire domains. The proposed method aims to automatically extract shallow parses and full syntactic trees from any domain or language without the need for training data. The model extends existing work on latent tree chart parsers to build representations for internal constituents that follow syntactic and semantic regularities. This approach allows for easy integration of these representations into downstream tasks. The proposed method incorporates the inside-outside algorithm into a latent tree chart parser to improve syntax capture. It aims to extract shallow parses and syntactic trees without the need for training data, allowing for easy integration into downstream tasks. This news raised hopes for further interest-rate cuts. The news raised hopes for further interest-rate cuts. A new model, PRPN, uses an RNN-based language model with a syntax module for inferring syntactic distance. Unlike DIORA, PRPN does not explicitly model phrases and relies on post-hoc heuristics for span representations. The DIORA model outperforms PRPN in unsupervised parsing on the WSJ dataset, with better recall on constituent types and strong clustering of phrase representations. It aims to automatically discover syntactic structure from raw text by following true syntactic input structure. The model is an extension of latent tree chart parsers trained as an auto-encoder with the inside-outside algorithm BID10 BID11. The inside-outside algorithm BID10 BID11 is used in the auto-encoder model to compress input sequences into a single vector representing the sentence. The algorithm then expands outward to reconstruct the input sentence, optimizing the reconstructed leaves to match the original input. This process leverages syntactic regularities in the text for better reconstruction. The inside-outside algorithm BID10 BID11 compresses input sequences into a single vector representing the sentence using a deep neural network. Inside representations of sub-trees are calculated based on children constituents, followed by an outside pass to compute outside representations. Each cell in the chart contains inside and outside vectors and compatibility scores. Each token in the input sentence has a corresponding pre-trained embedding vector. The compatibility and composition functions in the inside-outside algorithm use embedding vectors to compute scores and composition vectors for neighboring constituents. The TreeLSTM model is utilized to produce hidden and cell state vectors for higher levels of the chart. The algorithm initializes vectors for spans of length 1 and uses soft weighting over all possible pairs of constituents to compute scores and vectors for each span. The outside computation in the model involves predicting descendant cells using a disambiguation process over possible outside contexts. Each context component includes a sibling cell from the inside chart and a parent cell from the outside chart. The model uses an auto-encoder-like language modeling objective for training, where the root's outside representation is initialized with a bias instead of its own inside vector. Reconstruction of X is conditioned on multiple sub-trees rather than a single Y. The model reconstructs X based on multiple sub-tree roots, each representing a subset rather than the entire X. Outside vectors are trained to predict their original input, with a reconstruction loss calculated using negative samples. The training objective is computed uniformly for all tokens in a batch, with a loss function described in Equation 14. To obtain a parse with DIORA, we use the CKY procedure to calculate a maximum score for each span and record a backpointer. By including context during CKY, we can inform our parser to make different decisions based on the outside cell values. Experiments on unsupervised parsing, segmentation, and phrase similarities evaluate the effectiveness of DIORA. The DIORA model, implemented in PyTorch, is compared against the PRPN BID12 unsupervised parser. Evaluation is done on predicting syntactic parses using data sets like the Wall Street Journal and MultiNLI. The model is compared to deterministic tree constructions like left/right branching and balanced trees. RL-SPINN BID6 and ST-Gumbel BID8 are chart parsing models trained for NLI tasks. Latent tree models struggle with attachments at the beginning and end of sequences, so a post-processing heuristic is incorporated to improve performance. This heuristic attaches trailing punctuation to the root of the tree, resulting in a significant increase in parsing results. PRPN-LM performs well on MultiNLI without the heuristic, while DIORA outperforms PRPN-UP initially before PRPN-UP surpasses it. The model comparison between PRPN-UP and DIORA shows that PRPN-UP outperforms DIORA initially, but DIORA surpasses PRPN-UP later on. The evaluation is not based on a gold standard but on the ability to replicate a trained parser's output. The unsupervised parsing results are presented in Table 2, with +PP referring to a post-processing heuristic for removing trailing punctuation. The focus is on extracting specific constituent phrases, like entities, for downstream analysis, and the model's performance on phrase segmentation is measured by the maximum recall of spans in the predicted parse tree. The evaluation compares PRPN-UP and DIORA models based on maximum recall of predicted parse tree spans. PRPN-UP excels in noun-phrases but lags in other types, while DIORA performs well across various types, including verb-phrases. However, DIORA struggles with prepositional phrases. DIORA aims to learn meaningful representations for text spans, unlike traditional language modeling methods. It evaluates phrase representations by measuring similarity within labeled datasets like CoNLL 2000 and CoNLL 2012. Evaluation metrics include precision@K and mean average precision. The study evaluates phrase representations using precision@K, mean average precision (MAP), and dendogram purity (DP). Hierarchical clustering is applied to the representations to compute DP. Two baselines are compared, one using Glove vectors for phrase representations and the other using ELMo for context-dependent word embeddings. Two variants of ELMo are examined, ELMo-L1 and ELMo-Avg, for obtaining token hidden states. The study compares the performance of different models on the CoNLL datasets. DIORA outperforms Glove on the CoNLL 2000 dataset but lags behind on CoNLL 2012. Results suggest that DIORA captures syntax well but lacks in semantics. Latent tree learning models and neural parsers are also discussed in the text. The study discusses various neural parsers such as IORNN, DIORA, and Neural CRF Parser BID24. DIORA, similar to BID22, uses a single grammar rule for constituents without structural supervision. Unsupervised parsing and segmentation have been explored in previous works like BID25 and BID26. DIORA is evaluated against larger datasets for unsupervised parsing, achieving segmentation for parallel corpora without using mapped translations. The method induces syntactic trees and segmentations over text using auto encoder language modeling objective on top of latent tree chart parsers. Third-quarter shipments slipped 7% from the year-ago period and 17% from this year's second quarter. The earthquake caused streets to buckle and crack, making them impassible. Comparison of parsing models shows PRPN-LM and DIORA performance. DIORA achieves state-of-the-art results in unsupervised parsing on the WSJ dataset. Future work can enhance the model further. Future work can improve unsupervised parsing performance by training larger models over larger corpora in various domains and languages. Injecting extra unsupervised objectives or light supervision could enhance capturing of semantics in addition to syntax."
}