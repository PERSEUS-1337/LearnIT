{
    "title": "SyxV9ANFDH",
    "content": "Granger causality is used to analyze interactions in large-scale networks. A new approach infers pairwise Granger causality between nonlinearly interacting stochastic processes using Statistical Recurrent Units (SRUs). The network topology of Granger causal relations can be inferred from a structured sparse estimate of internal parameters. An economy-SRU variant with fewer trainable parameters is proposed to reduce overfitting. The economy-SRU model uses random projections to create a low-dimensional sketch of its hidden state for feedback in recurrent processing. Internal weight parameters are regularized to extract time-localized predictive features for improved time series prediction. Extensive experiments show that the economy-SRU outperforms other models in inferring Granger causality in large-scale systems. Granger causality is a criterion for network models of interactions between stochastic processes, used in system identification problems like gene regulatory network mapping and human brain connectome. Most methods for Granger causal inference rely on model-based approaches with linear regression models. Our work focuses on addressing the limitations of linear model-based Granger causality tests in the presence of nonlinearities in measurements. We use recurrent neural networks to model the interactions between stochastic processes in a multivariate dynamical system, aiming to uncover the unknown nonlinear system dynamics. Granger causality is used to detect pairwise causal relations between stochastic processes in a multivariate system. Measurement samples are generated sequentially based on a nonlinear autoregressive model. The generative function captures the system dynamics. The scalar-valued component generative function f i captures interactions between stochastic processes up to time t \u2212 1. The residual e i,t includes all factors influencing the measurement of process i at time t. Equation 1 generalizes the linear vector autoregressive model with nonlinear dependencies. Granger causality is interpreted in the context of the time series model. Granger causality is determined by whether one time series can provide new information to improve predictions of another. The generative function f_i must not depend on past measurements of series j for j to not Granger cause i. This constraint implies that the past of series j has no causal influence on series i, aligning with the core principle of Granger causality. In this work, the detection of Granger noncausality between components of x involves identifying those components whose past is irrelevant to the functional description of each individual f i in the system. The assumption is made that there are no unobserved confounding factors influencing x, and a model-based inference approach is used to learn an autoregressive model similar to the generative model described in the system. The pairwise Granger causality between series i and the components of x is determined by fitting parameterized approximations denoted by g i to the measurements in series i. If g i's output is independent of past measurements in series j, then series j is considered Granger noncausal for series i. The goal is to design g i to accurately capture complex causal relationships induced by the function f i while being easily identifiable from limited measurements. Recurrent neural networks (RNNs), specifically the statistical recurrent unit (SRU), are ideal for inferring pairwise Granger causality between nonlinearly interacting stochastic processes. Unlike gated RNNs, SRU does not use sigmoid gating functions, making it less affected by nonlinearity. Introduced by Oliva et al. (2017), SRU is designed for modeling multivariate time series data with complex-nonlinear dependencies spanning multiple time lags. The SRU, devoid of sigmoid gating functions, can model short and long-term temporal dependencies in multivariate time series by maintaining multi-time scale summary statistics. It constructs predictive causal features that are highly component-specific and lag-specific, reflecting the localized nature of causal effects in reality. The paper proposes the use of statistical recurrent units (SRUs) for detecting pairwise Granger causality between nonlinearly interacting stochastic processes. They introduce a modified SRU architecture called economy SRU (eSRU) to reduce trainable parameters and enhance interpretability of learned predictive features. Compared to the standard SRU, the eSRU model is considerably more efficient. The proposed eSRU model reduces overfitting in time series measurements and outperforms other models like MLP, LSTM, and AG-CNN in inferring pairwise Granger causality. Each unknown generative function in the model is approximated by a distinct SRU network, which sequentially processes time series measurements to make next-step predictions. The eSRU model reduces overfitting in time series measurements and outperforms other models like MLP, LSTM, and AG-CNN in inferring pairwise Granger causality. It uses SRU networks to sequentially process time series measurements for next-step predictions, combining input samples with summary statistics of past samples. The networks utilize ReLU activation for generating recurrent statistics, feedback, and output features. The SRU model utilizes a single layer ReLU network to transform the last hidden state for next-step predictions in time series measurements. The summary statistic u \u03b1 i,t is sensitive to recent or older portions of the input series based on the scale parameter \u03b1. The model generates output features that are preferentially sensitive to specific past segments of the input series by combining summary statistics with different \u03b1 values. The SRU model uses a single layer ReLU network to predict next-step time series measurements based on summary statistics. The network interactions are sparse, with only a few dimensions of the input series influencing the output. Parameters are learned by minimizing the mean squared prediction error loss. The weight matrix is estimated to show that past measurements in one series do not affect the future value of another series, indicating no Granger causality. The weight matrix in the SRU model identifies Granger causal relationships between components of the input series. The optimization problem is nonconvex and solved using gradient-based methods. Correctly identifying all-zero columns is crucial for detecting Granger noncausality. The SRU model uses first-order gradient updates to minimize the penalized loss. Gradients are efficiently computed using BPTT. Higher granularity in model parameters improves fit but increases trainable parameters. The proposed Economy-SRU model aims to reduce overfitting in time series prediction by modifying the standard SRU model. This modification involves reducing the number of trainable parameters in the SRU network, leading to more structured high-dimensional summary statistics. The proposed Economy-SRU model reduces overfitting in time series prediction by modifying the standard SRU network. It achieves this by generating structured high-dimensional summary statistics with fewer trainable parameters, leading to a low-dimensional embedding that retains contextual information. The feedback network maps the summary statistics to a feedback vector using a ReLU network with significantly fewer parameters compared to standard SRU feedback generation. The proposed Economy-SRU model reduces overfitting in time series prediction by modifying the standard SRU network. It achieves this by generating structured high-dimensional summary statistics with fewer trainable parameters, leading to a low-dimensional embedding that retains contextual information. The modified SRU is less susceptible to overfitting due to the penalized optimization problem proposed for estimating parameters. The feedback network maps the summary statistics to a feedback vector using a ReLU network with significantly fewer parameters compared to standard SRU feedback generation. The proposed Economy-SRU model reduces overfitting in time series prediction by modifying the standard SRU network. It generates structured high-dimensional summary statistics with fewer trainable parameters, leading to a low-dimensional embedding that retains contextual information. The modified SRU is less susceptible to overfitting due to the penalized optimization problem proposed for estimating parameters. The feedback network maps the summary statistics to a feedback vector using a ReLU network with significantly fewer parameters compared to standard SRU feedback generation. The learned linear mixtures in the model are sensitive to past segments of the input time series, resulting in time-localized and component-specific output features. The use of Gaussian random matrices for initialization is recommended to reduce the probability of spurious encoders. The proposed group-sparse regularization of weight coefficients in W is crucial for attributing future patterns in a time series to past occurrences of highly time-localized patterns. Numerical experiments confirm the effectiveness of the regularization. The SRU-and eSRU-based time series models are evaluated for inferring Granger causal relationships, compared to existing MLP, LSTM, and Temporal Causal Discovery Framework models. The Temporal Causal Discovery Framework (TCDF) model by Nauta et al. (2019) uses NN/RNN time series models with fixed maximum layer sizes. Tuned hyperparameters for different datasets are listed in Appendix G. Performance is evaluated based on AUROC. The ROC curve shows the TPR vs FPR for detecting pairwise Granger causal relationships. SRU and eSRU models' ROC curves vary with the regularization parameter \u03bb. In experiments, ROC curves for component-wise MLP and LSTM models are obtained by varying \u03bb 1. For TCDF, the ROC curve is obtained by adjusting the threshold applied to attention scores of the AG-CNN model. Time series measurements x for Granger causal inference are generated using the Lorenz-96 model, known for its use in climate science. The model's dynamics are described by a set of ordinary differential equations representing advection, diffusion, and external forcing. In experiments, the accuracy of inferring Granger causal relationships between variables with Lorenz-96 dynamics is evaluated. For F = 10, eSRU shows the highest AUROC, especially with fewer time series measurements. For F = 40, SRU and eSRU are the top-performing models in capturing stronger nonlinear interactions. In experiments evaluating Granger causal relationships with Lorenz-96 dynamics, SRU and eSRU models perfectly recover the true causal network for F = 40. However, performance decreases with smaller F, possibly due to lack of regularization for weakly-interacting time series. Time series measurements are generated from a 3rd order linear VAR model with sparse Granger causal interactions. In experiments with Lorenz-96 dynamics, SRU and eSRU models recover the true causal network for F = 40 but perform less effectively with smaller F. Time series measurements are generated from a 3rd order linear VAR model with sparse Granger causal interactions. The regression matrices A i are jointly sparse, with non-zero coefficients set to 0.0994 for stability. Results show higher AUROC with more measurements available. eSRU outperforms other models for T = 1000, accurately recovering the true Granger causal network. Learning methods are applied to estimate connections in the human brain from simulated BOLD imaging data. The experiments involve simulated BOLD signals from 15 brain regions in human subjects to detect directed connectivity. eSRU shows robustness to overfitting and detects true Granger causal relationships. A single-layer cMLP model outperforms more complex models, but they may perform better with more time series measurements. In the final experiments, different time series models are evaluated for inferring gene regulation networks synthesized for the DREAM-3 In Silico Network Challenge. The time series x represents gene expression levels of n = 100 genes for E.coli and yeast. Five gene regulation networks are to be inferred from trajectories recorded during recovery from 46 perturbations. NN/RNN models with 10 neurons per layer are used, except for the MLP model with 5 neurons per layer. SRU and eSRU models are found to be more accurate. In this work, the proposed eSRU models are more accurate for inferring Granger causal relationships between stochastic processes. The causality can be inferred from the internal parameters of the models trained on time series measurements. Future work includes exploring different loss functions, incorporating unobserved variables, and inferring causality from multi-rate time series data. Initial efforts focused on nonparametric approaches to testing for nonlinear Granger causality. The nonparametric tests for Granger causality require large sample sizes to estimate conditional probabilities. Testing variable-pairs individually is computationally intensive, especially with a large number of variables. Model-driven approaches infer causal relationships from data generative model parameters, offering more sample efficiency. However, the scope of causal dependencies is limited by the choice of model. Nonlinear kernel regression models are effective for testing causal relationships. Nonlinear kernel regression models are effective for testing causal relationships, with Marinazzo et al. (2008) proposing a kernel Granger causality index to detect pairwise nonlinear Granger causality. Sindhwani et al. (2013) and Lim et al. (2014) model nonlinear dependencies in time series measurements using functions in a reproducing kernel Hilbert space. Shen et al. (2016) introduces additional constraints on kernel parameters for consistency in fitted nonlinear models. Shen et al. (2016) proposes a nonlinear kernel-based structural VAR model to capture instantaneous nonlinear interactions in time series data. Existing kernel regression models are limited in capturing nonlinear dependencies as they only consider additive linear combinations of functions. RNNs offer a solution for modeling complex nonlinear dependencies and inferring Granger causal relationships. However, current approaches test each pairwise causal relationship individually, lacking a comprehensive strategy. The strength of causal connections between time series is estimated by comparing prediction errors of unrestricted and restricted RNN models. Pairwise testing is computationally inefficient for large numbers of series and fails to leverage sparse network connectivity. Tank et al. (2018) proposed inferring Granger causal relationships directly from MLP or LSTM network weights, enforcing column-sparsity. Tank et al. (2018) proposed using MLP or LSTM networks to estimate Granger causal networks by enforcing column-sparsity in the input-layer weight matrices. While MLP models can learn short-range temporal dependencies, LSTM models generally outperform them in extracting the true topology of pairwise Granger causality, especially for long-range dependencies. Using simpler RNN models with meaningful regularization is recommended for inferring Granger causal relationships from underdetermined time series data. Tank et al. (2018) suggested using MLP or LSTM networks for this purpose, with LSTM models being more effective in capturing long-range dependencies. The unregularized SRU loss function is updated using the gradient descent stepsize \u03b7 and the soft-thresholding operator. The weight matrix W in the eSRU model is updated in the same way. The group-norm regularized weight matrix Wo is also updated. The gradient of the loss function associated with the SRU and eSRU models is evaluated using backpropagation through time. The performance of the eSRU variant is compared to a proposed design, showing similar results. Based on the results, the recommended eSRU design with a randomly constructed encoding map is preferred for its simpler design and reduced training complexity. Comparing time series models, the average AUROC consistently improves with group-sparse regularization of the output weight matrix. Activation function for SRU and eSRU models is ReLU. In our simulations, using the Exponential Linear Unit (ELU) activation function in SRU and eSRU models showed better performance compared to ReLU. The SRU model has a single-layer feedforward design for generating features, while the Economy-SRU model uses ReLU networks with a single-layer feedforward design for generating features. The modified feedback in eSRU can be either single or multi-layered. The eSRU model can have a single or multi-layered feedback network, with fewer trainable parameters compared to SRU. Simulation results used two-layer ReLU for DREAM-3 and three-layer design for other experiments. DREAM-3 gene networks have no self-connections, and Granger causal inference methods do not suppress self-interactions. Pytorch implementation of cMLP & cLSTM models can be found at the provided link. The Pytorch implementations of componentwise MLP, LSTM, and attention-gated CNN models can be found at specific GitHub repositories. Receiver operating characteristics (ROC) of Granger causal inference methods are compared in Figures 4-7, showing the trade-off between true-positive rate (TPR) and false-positive rate (FPR). The Economy-SRU model configuration is detailed in Table 11."
}