{
    "title": "B1euhoAcKX",
    "content": "Determinantal Point Processes (DPPs) are used in machine learning for subset selection due to their ability to balance quality and diversity in sampled sets. Sampling from a DPP can be costly, but DppNets, generative deep models, offer a solution by producing DPP-like samples for any ground set. These models use an inhibitive attention mechanism based on transformer networks to capture dissimilarity between feature vectors, maintaining the guarantees of DPPs. Empirical results show high likelihood of samples generated by DppNets. Sampling schemes that balance quality and diversity are crucial for various machine learning problems. Determinantal Point Processes (DPPs) offer a solution by probabilistically modeling this tradeoff. DPPs allow for efficient sampling over all possible subsets of elements in a ground set, providing a high likelihood for samples generated by DppNets. Determinantal Point Processes (DPPs) model subsets of elements in a ground set by assigning probabilities based on a kernel matrix. DPPs capture repulsive forces between similar elements and have applications in various natural processes and machine learning tasks. Recent applications of DPPs include video recommendation, minibatch selection, and kernel approximation. However, the high sampling cost makes practical application challenging for large datasets, requiring additional work such as subsampling and structured kernels. Approximate sampling methods also require significant pre-processing time and scale poorly with dataset size. The use of DPPs in practice is hindered by high pre-processing costs for datasets with variable components. To address this, scalable models like DPPNET, a neural network, are used to generate diverse samples efficiently while maintaining theoretical properties of DPP measures. DPPNET can also sample from conditional DPPs and provide greedy mode approximation. DPPNET is a deep network that generates DPP-like samples efficiently while preserving the theoretical properties of DPP measures. It provides an accurate approximation to DPPs and speeds up sampling. DPPs are beneficial for subset selection in machine learning, making them attractive for various applications such as document summarization and recommender systems. In various fields, problems range from document and video summarization to neural network pruning and object retrieval. DPPs are used for encouraging diversity in generative models and accurately modeling inhibition in neural spiking data. Efficient sampling methods for DPPs have been developed to avoid the time-consuming eigendecomposition step, including approximate sampling methods and MCMC-based sampling methods. Dual sampling methods have also been introduced to speed up sampling by leveraging specific structure in the DPP kernel. The dual sampling method introduced in BID25 allows for exact sampling if the DPP kernel can be composed as an inner product over a finite basis. However, MCMC sampling requires variable amounts of sampling rounds, which is unfavorable for parallelization. To address this, a generative network can be used to generate approximate samples, allowing for simple parallelization and benefiting from recent improvements in specialized architectures for neural network models. This extends the abilities of dual DPP sampling by taking variable feature matrices \u03a6 as input for sampling. In this section, a framework is developed to address the computational cost of DPP sampling using neural networks. The distribution modeled by a DPP with kernel L is represented by PL over the power set. DPPs are well-suited for machine learning problems due to their quality/diversity tradeoff and other properties. A deep generative model with the right architecture can maintain these properties, despite the expensive matrix inversion required for conditioning. The text discusses how Deep Point Processes (DPPs) are suitable for applications requiring diversity in conditioned sets, such as basket completion for recommender systems. Unlike standard deep generative models like VAEs and GANs, DPPs enable simple conditioning operations during sampling. The model developed returns a prediction vector based on an input set, allowing for easy conditioning operations. This architecture allows for sampling sets with trivial conditioning operations and can be modified for greedy sampling algorithms. The text discusses the log-submodularity property of Deep Point Processes (DPPs) and its implications in discrete optimization and machine learning. It shows that a generative model trained on a log-submodular distribution inherits submodularity, leading to efficient conditioning operations during sampling. The text discusses the log-submodularity property of Deep Point Processes (DPPs) and its implications in discrete optimization and machine learning. It shows that a generative model trained on a log-submodular distribution inherits submodularity, leading to efficient conditioning operations during sampling. The DPPNET converges to a loss smaller than a certain threshold, indicating log-submodularity. The models are trained by minimizing the distance between predicted and target probabilities, rather than optimizing log-likelihood. Sampling from a DPP involves adding items one by one until reaching a predetermined size, which is leveraged in the sampling process. The text discusses leveraging the log-submodularity property of Deep Point Processes (DPPs) in training generative models. By recording intermediary subsets during sampling, training is done on multiple subsets of varying sizes. This approach resembles an unrolled recurrent neural network. The goal is to model a DPP with a fixed kernel using a generative neural network. Additionally, sampling from a DPP over a changing set of items is useful for scenarios like product sales or social media posts with varying relevance. Dual DPP sampling provides a speed-up in this process. To leverage the speed-up of dual DPP sampling, a static DPPNET is augmented with the feature matrix \u03a6 as input. Inspired by dot-product attention, the attention mechanism computes similarity between keys and queries, with the reweighted value matrix fed to a trainable neural network. This approach enhances training efficiency for sampling from a variable ground set. Transformer network architecture for sampling from a variable ground set involves modifying the attention mechanism to focus on dissimilar items in the input subset. This modification results in a fixed-size input to the neural network and ensures that similarity to a single item can disqualify an element from the ground set. The attention vector a is computed using an inhibitive attention mechanism, allowing for efficient computation in O(kDN) time. The neural network is trained to learn marginal probabilities of adding items to a subset under a DPP with a kernel dependent on features. Dual sampling for DPPs is efficient when sampling from a DPP with a specific kernel. DPPNET is a dynamic model that can be trained on any DPP kernel, requiring only \u03a6 as input. It is evaluated for performance as a static DPP proxy and for generating diverse subsets. The models are trained using TensorFlow with hyperparameters tuned for maximizing the normalized log-likelihood of subsets. DPPNET is compared to DPP performance, UNIF, and k-MEDOIDS baselines. The performance of DPPNET, trained on a DPP with fixed kernel over the unit square, is evaluated using negative log-likelihood to measure diversity in subsets of data. This method is motivated by the need for diverse sampling methods on the unit hypercube, such as quasi-Monte Carlo methods and latin hypercube sampling. The ground set consists of 100 points. DPPNET, trained on a fixed kernel over the unit square, shows significant improvement in sampling methods compared to other baselines. Results in FIG2 show better performance quantitatively and visually. NLL of DPPNET samples closely matches true DPP samples, with mode sampling from DPPNET outperforming DPP samples. Evaluation on different datasets like MNIST, CelebA, and MovieLens is done with varying ground set sizes. Experimental results show that DPPNET, trained on different datasets like MNIST, CelebA, and MovieLens, provides a wider coverage of writing styles compared to baselines. DPPNET samples also have a lower NLL, with mode sampling outperforming true DPP samples. DPPNET samples show wider coverage of writing styles compared to baselines. NLL of DPPNET samples decays, but mode sampling remains competitive. Results for MNIST in Table 2 show DPPNET outperforms other baselines, even on single class digit matrices. Attention mechanism and neural network are crucial for modeling DPP samples. DPPNET sampling for dataset summarization focuses on areas of interest, with results for CelebA and MovieLens confirming modeling ability. DPPNET allows for faster sampling compared to standard DPP, as shown in Table 2. DPPNET sampling method outperforms other methods in dataset summarization based on RMSE and wallclock time. The subsets selected by DPPNET show superior performance. DPPNET outperforms DPP and MCMC in dataset summarization with lower RMSE and faster sampling. DPPNET shows almost 30-fold speed improvement compared to standard DPP sampling. Additionally, DPPNET's performance is evaluated on kernel reconstruction using the Nystr\u00f6m method, a popular method in scaling up kernel methods in machine learning. DPPs are a competitive approach for selecting subsets in kernel reconstruction. The quality of the kernel reconstruction is evaluated by learning a regression kernel on a training set and reporting the prediction error on the test set using the Nystr\u00f6m reconstructed kernel. DPPNET is compared to MCMC sampling method with quadrature acceleration on the Ailerons dataset, showing lower RMSE and faster sampling. DPPNETs, generative networks trained on DPPs over static and varying ground sets, enable fast and modular sampling in various scenarios. They achieve competitive performance in terms of NLLs and can be trained on any kernel type built from feature representations of the dataset. Dual DPP exact sampling requires a specific DPP kernel for faster sampling, unlike DPPNETs. DPPNETs are not exchangeable and can be trained to take variable-length encodings as input. The scaling of DPPNET's complexity with ground set size is an open question, but standard tricks like sub-sampling can be applied. Speedups can be achieved by combining sub-sampling from the ground set with DPPNET sampling. DPPNETs can be combined with DPP sampling for large set sizes. Training encoders to produce encodings suitable for dataset summarization via DPPNETs is of interest. End-to-end training of encoder and DPPNET simultaneously may yield interesting results. Maintaining log-submodularity in the generative model is a significant question for the machine learning community. The VAE encoder for MNIST encodings includes 2d-convolutional layers with 64 and 128 filters, followed by a dense layer of 1024 neurons. CelebA encodings are generated by a VAE using a Wide Residual Network BID44 encoder with 10 layers and a latent space of 32 full-covariance Gaussians. The decoder architecture consists of a 16K dense layer followed by a sequence of 4 \u00d7 4 convolutions with [512, 256, 128, 64]. The decoder architecture includes a 16K dense layer followed by a sequence of 4 \u00d7 4 convolutions with [512, 256, 128, 64] filters interleaved with upsampling layers and a final 6 \u00d7 6 convolution with 3 output channels for each of 5 components in a mixture of quantized logistic distributions representing the decoded image."
}