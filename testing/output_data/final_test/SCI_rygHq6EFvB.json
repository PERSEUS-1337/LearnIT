{
    "title": "rygHq6EFvB",
    "content": "In this paper, a novel approach for Multi-Source Domain Adaptation (MSDA) using Generative Adversarial Networks is proposed. The method focuses on transferring knowledge from multiple source datasets to a target domain by separating image features into domain, style, and content factors. By generating new labeled images, the approach outperforms state-of-the-art methods on common MSDA benchmarks. The text discusses Multi-Source Domain Adaptation (MSDA) methods, which aim to adapt classifiers trained on different source domains to work on a target domain. MSDA outperforms state-of-the-art methods and is crucial when dealing with multiple datasets in real-world applications. In this paper, a novel Generative Adversarial Network (GAN) is proposed for addressing the domain shift in Multi-Source Domain Adaptation (MSDA) problems. The solution involves generating artificial target samples by transforming images from all the source domains, which are then used to train the target classifier. This approach aims to obtain domain invariant representations in a multi-domain image-to-image setting. In multi-domain image-to-image translation tasks, obtaining domain invariant representations is crucial to reduce the number of learned translations. While existing methods align domain-specific distributions, this work emphasizes modeling style as a key factor for optimal domain invariant representation. The appearance of an image is influenced by content, domain, and style, with domain representing shared properties within a dataset and style capturing shared characteristics across datasets. The generator in the proposed translation process aims to obtain a domain invariant representation by first achieving style invariant representations. This is done through an encoder with whitening layers to align style-and-domain feature distributions, followed by a decoder that projects this invariant representation onto a new domain-and-style specific distribution using batch transformations. The TriGAN approach proposes a generative model for Multi-Source Domain Adaptation (MSDA) by using Whitening and Coloring batch transformations to create an intermediate invariant representation. This design choice ensures scalability with the number of domains. The main contributions include the TriGAN model based on style, domain, and content factors, and a style-and-domain translation generator using modified W C layers. The curr_chunk discusses novel W C layers introduced in the paper, including IW T and AdaIW T, tested on MSDA datasets and outperforming state-of-the-art methods. It also reviews previous approaches on UDA and deep models for image-to-image translation. Single source UDA approaches are classified based on strategies to cope with domain-shift problems using statistics. The curr_chunk discusses various approaches to address the domain-shift problem in UDA, including utilizing first and second-order statistics, correlation alignment, and domain alignment layers. It also introduces a novel W C transform in the proposed encoder for generative networks. In the proposed encoder, the W C transform is used for generative networks. Other style and domain dependent batch-based normalizations are also introduced. Different methods for domain adaptation include adversarial learning-based approaches, domain-agnostic representations, and adversarial learning in a generative framework using GANs. Notable approaches include SBADA-GAN, CyCADA, CoGAN, I2I Adapt, and Generate To Adapt. The TriGAN generator is a novel approach in dealing with multiple source domains in unsupervised domain adaptation (UDA). Unlike existing generative methods like CyCADA, CoGAN, I2I Adapt, and GTA, TriGAN addresses the challenge of handling multiple source domains by creating a style-invariant representation using IWT blocks. This allows for effective adaptation across different domains without significantly increasing the number of parameters needed to be trained. The encoder E creates a style-invariant representation using IWT blocks, while the decoder D brings back domain-specific information with cDWT blocks. A reference style is applied using Adaptive IWT blocks. Multi-source UDA methods involve borrowing knowledge from target k nearest-neighbour sources and using distribution-weighed combining rules to construct target hypotheses. Recent approaches like Deep Cocktail Network (DCTN) and Moment Matching Network (M3SDA) aim to reduce discrepancies between multiple sources. In (Peng et al. (2019)), M3SDA is proposed for reducing domain discrepancies using a deep generative approach. The method is based on image-to-image translation, with inspiration from pix2pix and CycleGAN. ComboGAN and StarGAN are also related works that address translation among multiple domains. Our goal is to obtain an explicit, intermediate image representation that is style-and-domain independent using IWT and DWT. This invariant representation simplifies the re-projection process onto a desired style and target domain through AdaIWT and cDWT, resulting in realistic translations among domains. Whitening and colouring based image-to-image translation methods have also been explored recently. The proposed approach for MSDA involves whitening layers that are data dependent and use Cholesky decomposition to compute whitening matrices. The method includes an overview, TriGAN architecture, and training procedure for labeled source datasets and an unlabeled target dataset with shared categories and domains. The proposed method involves training a generator to change the appearance of input images to match a desired domain. This generator is then used to create a labeled target dataset, which is used to train a target classifier. The generator consists of an encoder and decoder, with the encoder removing domain-specific and style-specific aspects. The role of E is to whiten, i.e., to remove, both domain-specific and style-specific aspects of the input image features in order to obtain domain and style invariant representations. At training time, G takes a batch of images with corresponding domain labels and a batch of style images. The TriGAN architecture consists of a generator network G and a discriminator D P based on the Projection Discriminator. The W C transform is a multivariate extension of per-dimension normalization and shiftscaling transform (BN) used in batch-based feature transformations. It involves computing the centroid and covariance matrix of elements in a batch to obtain domain and style invariant representations. The WC transform involves whitening and coloring operations on features to obtain style-and-domain invariant representations. The encoder E consists of Convolution-Normalization-ReLU-AveragePooling blocks and ResBlocks for this purpose. The encoder E utilizes Convolution-Normalization-ReLU-AveragePooling blocks and ResBlocks to obtain style-invariant representations by replacing BN layers with proposed normalization modules. The Instance Whitening Transform (IWT) is used to whiten first and second-order statistics of low-level features, emphasizing image-specific features for whitening and coloring operations. The encoder E uses normalization modules instead of BN layers to obtain style-invariant representations. The Instance Whitening Transform (IWT) is applied to whiten first and second-order statistics of low-level features, emphasizing image-specific features for whitening and coloring operations. Domain-specific whitening is performed in subsequent blocks using the Domain Whitening Transform (DWT). The encoder E utilizes normalization modules for style-invariant representations. Features in batch B are partitioned based on image domain labels, computed for domain-invariant spherical distribution. Whitened features are re-projected using learnable parameters \u03b2 and \u0393. Decoder D is symmetric to E, processing domain and style invariant features. The encoder E computes domain and style invariant features and projects them onto the desired domain with style from image x. The decoder D projects features onto a domain-specific subspace using domain-specific coloring parameters. The conditional Domain Whitening Transform (cDWT) conditions the coloring step on the domain label. The conditional Domain Whitening Transform (cDWT) conditions the coloring step on the domain label l. Partitioning B into subsets based on the desired output domain label lOi, coloring is performed using domain-specific style. The Style Path extracts style from image xOi using Convolution-IWT-ReLU-AveragePooling blocks and a MultiLayer Perceptron (MLP) F, describing style using first and second order statistics. Following Gatys et al. (2016), style is described using first and second order statistics extracted with IW T blocks. The coloring operation is the inverse of whitening, but adaptation is needed for the Style Path transformation. An MLP (F) is used to implement this adaptation, imposing style-specific statistics on the features of the last blocks of D to mimic the style of x O i. GAN Training is then performed. In the paper, a simplified notation for G is used, where G takes one image as input instead of a batch. G is trained with three different losses to change the style and domain of the input image while preserving its content. The losses include an adversarial loss based on the Projection Discriminator, an Identity loss, and a pixel-to-pixel L1 norm loss. The proposed method involves using a third loss based on equivariance to simple transformations to ensure that the generation process preserves the main content of images. This loss is implemented using affine transformations and a differentiable bilinear kernel. The Equivariance loss involves randomly choosing a parameter \u03b8 and applying it to the image to generate a second image, which is then compared with the original using the L1 norm. This self-supervision technique helps extract meaningful features from the images. The proposed method involves using equivariance to geometric transformations for self-supervision in extracting semantics. A loss similar to affine transformations has been proposed for image co-segmentation. The complete loss for G involves classifier training and creating a labeled training dataset for the target domain. Experimental setup and evaluation using MSDA datasets are described, along with an ablation study on TriGAN components' impact on classification accuracy. The impact of TriGAN components on classification accuracy is evaluated using two domain adaptation benchmarks: DigitsFive and Office-Caltech. DigitsFive consists of datasets like USPS, MNIST, MNIST-M, SVHN, and Synthetic numbers. USPS contains digits from U.S. envelopes, MNIST is a popular benchmark, MNIST-M is colored, SVHN includes real-world house numbers, and Synthetic numbers has computer-generated digits with variations. The Office-Caltech benchmark consists of 2533 images from 10 shared categories between Office31 and Caltech256 datasets. It includes domains Amazon (A), DSLR (D), Webcam (W), and Caltech256 (C). TriGAN was trained for 100 epochs using the Adam optimizer with specific learning rates for the generator and discriminator. Architecture details for the generator and discriminator are provided in the Appendix B. In the Digits-Five experiments, a mini-batch size of 256 was used for TriGAN training with images converted to 32 \u00d7 32 RGB. The final target classifier C utilized the same network architecture as in previous studies. In the Office-Caltech10 experiments, images were downsampled to 164 \u00d7 164 to allow for more samples in a mini-batch, with a mini-batch size of 24 for training. The ResNet101 architecture was used for the backbone target classifier C, initialized with a network pre-trained on the ILSVRC-2012 dataset. In experiments, weights are initialized with a pre-trained network on ILSVRC-2012 dataset. The output layer is replaced with a fully-connected layer for OfficeCaltech10 dataset. Training is done with Adam, using different learning rates. Ablation study and comparison with MSDA methods are conducted. TriGAN achieves higher accuracy than other methods on Digits-Five dataset. TriGAN outperforms all other methods in various settings, achieving an average accuracy of 90.08%. It significantly surpasses the second best method M 3 SDA by 10.38% in the mt, up, sv, sy \u2192 mm setting. StarGAN fails when translating digits, highlighting the importance of a well-designed generator for domain invariant representations in the MSDA setting. TriGAN achieves a classification accuracy of 97.0% on the Office-Caltech10 dataset, outperforming all other methods and beating the previous state-of-the-art approach by 0.6% on average. ResNet-101 pre-trained on ImageNet is used as the backbone network for this analysis. In the study, different models (A-E) are compared using the Digits-Five dataset to analyze feature whitening and standardization. Model A includes IWT, DWT, cDWT, AdaIWT, and L Eq, while Model B replaces L Eq with cycle-consistency loss. Model C replaces components with different normalization techniques. Model D ignores the style factor, and Model E has a different style path compared to Model A. Model A outperforms all ablated models, showing the importance of feature whitening. Model B indicates that L Cycle is detrimental for accuracy, while Model C is also outperformed by Model A. Model D's no-style assumption hurts classification accuracy. Our proposed method for multi-domain image-to-image translation tasks shows improved performance compared to Model A. Model E highlights the importance of incorporating style information in the translation process. Model F demonstrates the benefits of having a separate factor for domain. Experiments on the Alps Seasons dataset validate the effectiveness of our generator, outperforming StarGAN in terms of FID metrics for realism of generated images. TriGAN is a multi-source domain adaptation framework that aims to simplify the generation process by obtaining domain-style invariant representations. The FID scores of TriGAN are significantly lower than StarGAN, emphasizing the importance of enforcing domain and style invariance for multi-domain translation. TriGAN removes style and domain specific statistics from source images to re-project invariant representations onto the desired target domain and styles. State-of-the-art results were achieved on two MSDA datasets, with a detailed ablation study showing the importance of each component. Sample translations demonstrate the effectiveness of the approach, such as transforming an SVHN digit \"six\" with side-digits to a blue \"six\" with red background in the MNIST-M domain using AdaIWT blocks. The AdaIWT block in TriGAN consists of a sequence: Upsampling - Convolution - AdaIWT - ReLU, with specific parameters. Two AdaIWT blocks are used consecutively, followed by a Convolution layer. The Style Path outputs parameters for the AdaIWT blocks. The MLP consists of fully-connected layers. DWT blocks adopt a residual-like structure. The proposed Conditional Domain Whitening Transform (cDWT) blocks, similar to DWT blocks, include cDWT \u2212 ReLU \u2212 Convolution 3\u00d73 layers with identity shortcuts. These blocks are part of the G structure, which also includes IWT blocks, DWT block, cDWT block, and AdaIWT blocks. The discriminator architecture uses a Projection Discriminator. The Projection Discriminator (D P) in the architecture uses projection shortcuts instead of identity shortcuts. It consists of 2 blocks and employs spectral normalization. The TriGAN framework can handle N-way domain translations and experiments are conducted for Single-Source UDA scenario with grayscale MNIST as the source domain. The UDA settings involve MNIST \u2192 USPS datasets, with MNIST containing 28x28 grayscale images of digits 0 to 9 and USPS containing 16x16 grayscale handwritten digits. Images from both domains are up-sampled to 32x32. In this section, the performance of TriGAN is compared with GAN-based state-of-the-art methods for Single-source UDA setting in Digits Recognition. TriGAN achieved a classification accuracy of 98.0%, outperforming other methods such as PixelDA, UNIT, and GenToAdapt. The results are presented in Table 5, showcasing TriGAN as a competitive approach in both adversarial learning and reconstruction-based methods. TriGAN outperforms other GAN-based methods in Single-source UDA for Digits Recognition, achieving a classification accuracy of 98.0%. It excels in MNIST \u2192 SVHN adaptation, being 5.2% better than the second best method SBADA-GAN."
}