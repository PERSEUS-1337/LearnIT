{
    "title": "Bke5aJBKvH",
    "content": "Capturing long-range feature relations in convolutional neural networks (CNNs) is a key issue, with efforts to integrate trainable attention modules to adjust feature maps. This paper introduces the 'Recurrent Layer Attention network,' combining recurrent neural networks (RNNs) with CNNs to scale feature volumes across layers. Structural derivatives demonstrate compatibility with attention modules, and visualization of layer scaling coefficients shows semantic understanding of learned features. The Recurrent Layer Attention network achieves significant advancements in modeling relationships among layers. The Recurrent Layer Attention network combines RNNs with CNNs to scale feature volumes across layers, achieving performance enhancement in image classification and object detection tasks. This approach integrates attention mechanisms to empower CNNs with better representational power, inspired by the intrinsic characteristics of CNNs, RNNs, and recent works in computer vision. The Recurrent Layer Attention Network (RLA network) combines RNNs with CNNs to improve representational power in computer vision. It focuses on global weight balance among layers, adjusting features with an inter-layer attention mechanism and utilizing RNN to memorize feature hierarchy. This approach aims to enhance class discriminability by inheriting informative feature hierarchies. The RLA network combines RNNs with CNNs to improve representational power in computer vision by focusing on global weight balance among layers and utilizing an inter-layer attention mechanism. It achieves comparable results with state-of-the-art networks on image classification and object detection tasks, outperforming the original ResNet architecture. Additionally, the RLA network is compatible with recent attention works, enhancing its performance further. The RLA network introduces inter-layer attention and connections between shallow and deep CNN layers through RNNs. It demonstrates effectiveness in image classification and object detection tasks, outperforming leading deep networks with fewer parameters. Ablation studies confirm compatibility with existing intra-attention models and potential for architectural expansion. The RLA network introduces inter-layer attention and connections between shallow and deep CNN layers through RNNs, demonstrating effectiveness in image classification and object detection tasks. Attention mechanisms in computer vision bias neurons towards informative input components, with recent applications including spatial and channel-wise attention modules regulating neuron activations on feature maps. The RLA network is easy to implement and allows for architectural expansion. The RLA network introduces inter-layer attention through RNNs, modeling complex relationships among visual features in different CNN layers. In contrast to intra-layer attention modules, the RLA network addresses the lack of interaction among visual features captured in different CNN layers. The RLA network introduces inter-layer attention through RNNs to model complex relationships among visual features in different CNN layers, enhancing deep neural networks for vision tasks that do not require sequential processing. The deep neural network proposed in the study enhances the representational power of CNN by manipulating CNN and RNN to affect each other at every layer. The concept involves conveying information from shallow CNN layers to deep CNN layers using memory units, which is a novel approach. Local patterns in images, such as specific features like a cat's head, are defined as class-specific characteristics collected through convolution operations. The RLA network enhances CNN by combining convolution operations with other mapping functions like residual blocks or inception modules. It summarizes features, stores them in LSTM, and infers layer attention. The RLA module processes feature volumes into statistics, stores context in LSTM, and determines layer attention. The use of RNN is based on treating concatenated feature volumes along CNN layers as a sequence, with each element providing prior information on emerging local patterns. The RLA network combines convolution operations with mapping functions to enhance CNN. It utilizes LSTM hidden units to provide meaningful information to CNN, using attention mechanism to reduce activation of overlapped patterns and emphasize important ones. The network is described in subsections detailing its formulation, architectural derivatives, and light-weightness. The forward-passing procedure of each layer involves generating feature volume A l through G l. The feature volume A l is summarized as the context S l \u2208 R N, representing local patterns in the image. Various feature summarizing operations are investigated for proper statistics embedding. The context from earlier CNN layers is recurrently inserted into LSTM hidden units h l \u2208 R M. The LSTM hidden units h l are selectively embedded with the context from earlier CNN layers. The layer attention \u03b1 l is inferred to scale the feature volume A l using fully-connected layers. The scaled feature volume \u00c3 l is computed through element-wise multiplication with \u03b1 l. The RLA network introduces derivatives based on attention structure and context, experimented on in Subsection 4.1. It adopts an intra-layer attention mechanism, calibrating feature volumes within layers. The integration of RLA with intra-layer attention is depicted in Figure 2, known as Intra-layer Attention + RLA network (IA+RLA). The IA+RLA performance is evaluated using the Squeeze-and-Excitation block as the IA module. RLA network benefits from utilizing feature hierarchy. The RLA network introduces derivatives based on attention structure and context, experimented on in Subsection 4.1. It adopts an intra-layer attention mechanism, calibrating feature volumes within layers. RLA network benefits from utilizing feature hierarchy and adaptively scaling features layer by layer. Additionally, a structure named RLA-vector is designed to evaluate the effectiveness of exploiting feature hierarchy without applying inter-layer attention mechanism. The RLA-vector adjusts feature volumes channel-wisely using the intra-layer attention mechanism in the Squeeze-and-Excitation module. The RLA network introduces derivatives based on attention structure and context, experimented on in Subsection 4.1. It adopts an intra-layer attention mechanism, calibrating feature volumes within layers. RLA network benefits from utilizing feature hierarchy and adaptively scaling features layer by layer. Additionally, a structure named RLA-vector is designed to evaluate the effectiveness of exploiting feature hierarchy without applying inter-layer attention mechanism. The RLA-vector adjusts feature volumes channel-wisely using the intra-layer attention mechanism in the Squeeze-and-Excitation module. In contrast, the focus shifts to finding the context, a summarized feature volume, that preserves local pattern information during feature summarization. Global average pooled feature and global max pooled feature are employed as contexts, summarizing spatial dimensions of the feature volume using average pooling and max pooling. The RLA network introduces derivatives based on attention structure and context, experimented on in Subsection 4.1. It adopts an intra-layer attention mechanism, calibrating feature volumes within layers. RLA network benefits from utilizing feature hierarchy and adaptively scaling features layer by layer. Additionally, a structure named RLA-vector is designed to evaluate the effectiveness of exploiting feature hierarchy without applying inter-layer attention mechanism. The RLA-vector adjusts feature volumes channel-wisely using the intra-layer attention mechanism in the Squeeze-and-Excitation module. In contrast, the focus shifts to finding the context, a summarized feature volume, that preserves local pattern information during feature summarization. Global average pooled feature and global max pooled feature are employed as contexts, summarizing spatial dimensions of the feature volume using average pooling and max pooling. Exploiting GAP involves considering the size or multiple appearances of local patterns in spatial dimensions, while GMP is used for capturing the most salient part of local patterns. Non-linear operations are applied on GAP and GMP through two fully connected layers, known as GAP+MLP and GMP+MLP, to model the multiple appearance of similar local patterns at different channels. The goal of exploiting non-linearity is to produce more informative context, in contrast to the SE network which predicts channel-wise scaling coefficients of the feature volume. The RLA module introduces channel-wise scaling coefficients for feature volumes. The additional model parameters needed for RLA module depend on the number of neurons in LSTM cell, dimension of context, layers in CNN, and reduction ratio. Compared to ResNet-56 with SE module, ResNet-56 with RLA module only increases parameters by 0.14%. Ablation experiments were conducted on CIFAR and ImageNet-1K datasets for image classification and Microsoft COCO dataset for image detection using ResNet as the backbone CNN architecture. The RLA module introduces channel-wise scaling coefficients for feature volumes, with additional model parameters depending on LSTM cell neurons, context dimension, CNN layers, and reduction ratio. Ablation experiments on CIFAR and ImageNet-1K datasets for image classification and Microsoft COCO dataset for image detection were conducted using ResNet as the backbone CNN architecture. ResNet-56 was used for the ablation study, showing instances outperforming ResNet-56 on CIFAR-10 and CIFAR-100 datasets. Key concepts in the ablation study include RLA's compatibility with intra-layer attention mechanism, the impact of RNN for feature hierarchy, and the effectiveness of introduced statistics for context. The integrated structure of intra-layer attention and RLA works worse than SE and RLA alone but still better than ResNet on CIFAR-10 and CIFAR-100 datasets. Intra-layer attention adjusts features with similar receptive field sizes, while inter-layer attention focuses on features with different sizes. RLA-vector significantly outperforms SE network, showing that utilizing feature hierarchy through RNN aids CNN. Utilizing feature hierarchy through RNN aids CNN in interpreting images better. Various feature summary statistics affect performance differently, with GAP being the most appropriate context. Results show that considering multiple appearances of local patterns in feature volume has a negligible impact. GAP outperforms GMP in terms of error rate. The CIFAR dataset consists of 50K training images and 10K validation images. Training networks for 160 epochs with a learning rate initialized at 0.1. RLA network shows lower training/validation error. ImageNet-1K 2012 dataset has around 1.28M training images and 50K validation images. Training networks for 100 epochs with a learning rate set to 0.1. RLA network outperforms ResNet but ranks below the SE network in performance. The RLA network outperforms ResNet but ranks below the SE network in performance due to structural limitations. For example, with ResNet-50 as a backbone architecture, RLA scales the network using only 16 scale coefficients, while the SE module adjusts the network using 15104 scale coefficients. Integration of intra-layer mechanism and RLA network is expected to produce better results. The study uses the Microsoft COCO 2014 dataset with Faster R-CNN for detection, training networks for 490K iterations with ImageNet-1K 2012 pre-trained ResNet and RLA networks. The experimental results of the RLA network show improvements over the original ResNet baseline in object detection tasks. Grad-CAM visualization was used to understand how the RLA network learns and gains class discriminability. This visualization technique measures the importance of each pixel in a feature map towards the overall decision of the CNN. The Grad-CAM technique is applied to layers before scale operation with layer attention, focusing on three target classes with different visual characteristics: racer, ostrich, freight car. Racer class has small receptive fields like wings and wheels, ostrich class has medium receptive fields like the head, while freight car class lacks specific local patterns. The RLA network enhances layer attention on layers capturing salient features for each class. The RLA network enhances layer attention on layers capturing salient features for each target class, emphasizing class-specific local patterns and class-agnostic features in deep layers. The RLA network enhances layer attention on capturing salient features for each target class, emphasizing class-specific local patterns and class-agnostic features in deep layers. Comparing with original ResNet, it allocates more class-specific neurons until deep layers. The residual architecture learns the 'residual' while keeping previously obtained features in earlier layers. This finding aligns with previous research on class selectivity in deep layers. The mean layer attention curve along RLA network layers reveals higher values in shallow layers and lower values in middle layers, providing insight into the residual architecture. The intuition behind residual learning is to let layers learn perturbations with reference to an identity function, supported by the standard deviation of feature responses. Comparing with the original paper, the plot offers similar information in a more straightforward manner. The plot of layer attention values in the RLA network shows a decrease along layers, indicating the intuition of residual learning. The variance of layer attention values increases along layers, with the last residual block having the highest values close to 1. This aligns with the traditional understanding that deep layers in CNNs learn high-level features. The RLA network learns large layer attention values in the latest CNN layers, which are semantically salient, while deep layers contain abstract features with varying attention values. In this paper, the authors propose an inter-layer attention mechanism called 'Recurrent Layer Attention network' to enhance the representational power of CNN. They introduce new concepts such as weight balancing of CNN features along layers and linking shallow CNN layers to deep CNN layers via RNN. The RLA network is evaluated using CIFAR and ImageNet-1k 2012 datasets for image classification and object detection tasks. The authors propose a Recurrent Layer Attention network to enhance CNN's representational power. They visualize the network using Grad-CAM, report findings, and plan to integrate inter-layer attention with intra-layer attention mechanisms. Data augmentation and optimization details are based on He et al. (2016), including scale augmentation and nesterov SGD for optimization. For optimization, nesterov SGD with momentum 0.9 and weight decay of 0.0001 is used. ImageNet-1K pretrained ResNet, SE network, and RLA network are utilized for fast implementation on object detection tasks. Hyperparameters for RLA design include reduction ratio, context dimension, and number of neurons in LSTM cell. Selecting hyperparameters is crucial for model complexity and performance. Feature volumes over CNN layers with different channel numbers are downsampled for summarized feature volume. The size of LSTM hidden units, M, highly affects model and computational complexity. Experimental results show that the performance of RLA does not drop as M decreases, but rather increases. This observation is attributed to the clear separability of contexts, requiring only a small number of LSTM hidden units for reduction. The reduction ratio in the RLA network is not governed by a golden rule for hidden units. Experiments show that choosing smaller M and a reduction ratio does not significantly impact the top-1%-error compared to varying hidden units. However, decreasing M can lead to a decrease in performance due to the inability to scale the channel-sized vector of each layer in CNN. For a fair comparison with the SE network, M is set to 32. In the RLA network, the reduction ratio is set at r = 8 for RLA-18/50 and r = 16 for RLA-101. The connection between CNN and RNN in RLA-50 network is described, with M = 512 for RNN hidden units size. Further exploration of different LSTM hidden units size M may lead to improved performance, as seen in the parametric study on CIFAR dataset. The RLA network utilizes a reduction ratio of r = 8 for RLA-18/50 and r = 16 for RLA-101. When using a vanilla CNN, applying RLA to the backbone CNN is straightforward, but it becomes confusing when using a residual architecture like ResNet-50. The RLA network stores context and infers layer attention to scale features in each residual block. Grad-CAM visualization results show how the RLA network influences model parameters. The RLA network uses reduction ratios of r = 8 for RLA-18/50 and r = 16 for RLA-101. It enhances layer attention values to capture semantically important visual features and learns non class-specific features in deep layers. Visualization results show target classes like junco, achidna, killer whales, leonberg, tiger cat, and sleeping bags."
}