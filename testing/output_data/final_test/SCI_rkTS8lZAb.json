{
    "title": "rkTS8lZAb",
    "content": "Generative adversarial networks (GANs) are a learning framework that relies on training a discriminator to estimate the difference between target and generated distributions. A new method called boundary-seeking GANs (BGANs) is introduced for training GANs with discrete data, using importance weights computed from the discriminator's difference measure. This approach improves stability in training and has been effective in discrete image and character-based natural language generation. The objective of generative adversarial networks (GANs) is to improve stability in training by using continuous data. GANs involve a unique learning framework with a generator and discriminator, training without an explicit probability density formulation. They have shown the ability to generate diverse and realistic samples from high-dimensional data, but have limitations on the type of variables they can model. Generative adversarial networks (GANs) have limitations with modeling discrete variables due to the need for full differentiability in the generator and discriminator. This poses challenges for training with discrete data, such as language representations. Existing solutions for credit assignment with discrete operations are approximate and do not yet work with GANs. In this work, a theoretical foundation is provided for boundary-seeking GANs (BGAN), a method for training a generator of discrete data using a discriminator optimized for estimating an f-divergence. The approach is verified to work across various f-divergences on classification tasks, image, and natural language benchmarks. BGAN outperforms WGAN-GP in discrete settings and extends to the continuous case with improved stability properties during training. Boundary-seeking GANs (BGAN) is introduced as a method for training a generative model adversarially with discrete data, assuming a normal generative adversarial learning setting. The goal is to find optimal parameters for a function that describes well the empirical samples, with improved stability properties during training. Generative adversarial networks (GANs) address the problem of defining a difference measure between probability distributions P and Q\u03b8 without strong assumptions on their forms. GANs introduce a discriminator function D\u03c6 and a value function to work with high-dimensional data, using samples drawn from a simple prior. Neural networks with sigmoid output activation can be interpreted as binary classifiers in GANs. The discriminator maximizes the value function, while the generator minimizes it through optimization using back-propagation and stochastic gradient descent. In the non-parametric limit, the value function is related to the Jensen-Shannon divergence, and f-GAN extends this concept to all f-divergences. The f-divergence formalism provides a framework for GANs using various divergences like Jensen-Shannon, Kullback-Leibler, Pearson \u03c7 2, and squared-Hellinger. It involves a dual formulation and optimization over a family of functions T, allowing for a change from likelihood ratios to a maximization problem. The f-divergence framework for GANs involves optimization over a family of neural networks with parameters \u03c6. By maximizing a variational lower-bound, a neural estimator of f-divergence can be obtained, leading to powerful training capabilities for generators of continuous data. The discriminator and statistic network in GANs aim to estimate a difference measure using a neural network with unlimited capacity. The models minimize and maximize a variational lower-bound of a difference measure, extending to variants based on integral probability metrics. By utilizing f-divergence and positive activation functions, the target distribution can be estimated using the generated distribution. Theorem 1 states that the target density function can be expressed in terms of a generated density function and a scaling factor, referred to as the optimal importance weight. This weight is calculated as the derivative of a convex function with respect to the discriminator function. The optimal importance weight w(x) is derived from the derivative of a convex function with respect to the discriminator function T \u03c6(x). A sub-optimal T \u03c6 can still be used to estimate the target density function, with f-divergences providing suitable candidates for positive importance weights. The function w(x) = e F \u03c6(x) is consistent across different f-divergences. The bias in the estimator p(x) depends on the tightness of the variational lower-bound. GANs face convergence issues in suboptimal conditions. Training GANs with discrete data is challenging due to the non-differentiability of the value function. Boundary-seeking GAN is introduced as a solution for training GANs with discrete data. The boundary-seeking GAN is introduced as a method for training GANs with discrete data, using policy gradient based on KL-divergence and importance weights. It optimizes the generator parameters until convergence and offers a lower-variance gradient for each z, providing a unique reward signal for training the generator in an adversarial way. The boundary-seeking GAN introduces a method for training GANs with discrete data using policy gradient based on KL-divergence and importance weights. It optimizes the generator parameters until convergence, offering a lower-variance gradient for training the generator adversarially. The method involves training the generator using the gradient of the KL-divergence with explicit density function q \u03b8, and addresses reducing variance from estimating normalized importance weights. The gradient of the expected conditional KL-divergence w.r.t. the generator parameters is computed using a Monte-Carlo estimate. Minimizing the expected conditional KL-divergences is stricter than minimizing the KL-divergence in Equation 7, as it requires all conditional distributions to match independently. The training procedure for discrete BGAN is described in Algorithm 1, which requires additional computation for computing normalized importance weights. The observed variables are assumed to be independent conditioned on Z in multi-variate cases like discrete image data. Importance weights are applied uniformly across observed variables. Policy gradients REINFORCE is commonly used for dealing with discrete data in GANs. The reward in Equation 9 is the normalized importance weights, approaching the likelihood ratio in the non-parametric limit. The gradient of the reversed KL-divergence is derived using REINFORCE with baselines. The output of the statistic network can be considered as a reward. The output of the statistic network, F \u03c6 (x), is considered a reward, and b = log \u03b2 = E Q \u03b8 [w(x)] is an analog of a baseline. The gradient used in previous works on discrete GANs is discussed. For continuous variables, minimizing the variational lower-bound suffices for optimization with back-propagation. Convergence of the discriminator is straightforward, but there is no general proof of convergence for the generator. The value function can be arbitrarily large and negative. The generator objective is optimal when the generated distribution, Q \u03b8, is nonzero only for the set {x | T (x) = M}. The continuous BGAN objective aims to improve stability in GANs by minimizing the variational lower-bound with a specific condition on the generator's output distribution. This minimization can lead to looser bounds with no guarantee of improvement, highlighting a potential source of instability in GAN training. The continuous BGAN objective minimizes the log of the generator's output distribution to change a concave optimization problem to a convex one. The method relies on estimating likelihood ratios from the discriminator, connecting them to the policy gradient. This approach has been applied successfully to the discrete setting for the first time. Our method is similar to re-weighted wake-sleep (RWS) for training Helmholtz machines with discrete variables. It involves minimizing KL divergence and using policy gradients over the likelihood ratio. Neural variational inference and learning (NVIL) relies on reverse KL. Training GANs with discrete data, especially in language models with RNN generators, is an active area of research. Many REINFORCE-based methods have been proposed for language modeling, effectively using the sigmoid of the estimated loglikelihood ratio. Recent works have focused on improving credit assignment in training GANs with discrete data, using the sigmoid of the estimated loglikelihood ratio. Some approaches rephrase the problem into a GAN over a continuous space, bypassing the difficulty of training GANs with discrete data. Variants of GANs have been introduced to address stability issues, where generated samples tend to collapse to singular values resembling the data. Several early attempts at modifying the training procedure and identifying working architectures addressed stability issues in GANs, but significant progress was only made with the introduction of Wasserstein GANs (WGAN). WGANs rely on an integral probability metric (IPM) that is the dual to the Wasserstein distance, leading to improved stability in training compared to GANs based on f-divergences. IPMs restrict the set of possible functions, such as in WGANs where the set is defined as {T | T L \u2264 K}, ensuring a statistic network with a large number of parameters. Recent advancements in improving the stability of GANs include methods like weight-clipping in WGANs and gradient penalties in BID9 and BID32. These techniques aim to ensure Lipschitz continuity in the network, with BID32 introducing a gradient norm penalty based on f-divergences. This approach has shown improved stability in adversarial learning, although the exact role of the discriminator objective in this process remains unclear. The discriminator objective plays a crucial role in stabilizing adversarial learning, but correct regularization of the discriminator seems to be sufficient. The gradient estimator from BGAN is evaluated in a discrete setting using the CIFAR-10 dataset. A multinomial distribution is used as the generator, while a neural network models the softmax output for the discriminator. Experiments were conducted with a 4-layer CNN and 3 fully-connected layers, training the importance sampling BGAN on f-divergences and the REINFORCE counterpart for 200 epochs. Accuracy on the test set was reported, along with a baseline classification model trained on cross-entropy. Overall, BGAN performed similarly to the baseline on the test set, with the REINFORCE method performing slightly worse. Despite efforts, WGAN only achieved an error rate of 72.3% on the test set after 600 epochs of training. Training WGAN with gradient penalty failed completely. Training WGAN with gradient penalty failed completely despite successful training with higher-dimension discrete data. Image data for testing included binary MNIST and quantized CelebA datasets. CelebA images were downsampled to 32x32 and quantized to a 16-color palette. Deep convolutional GANs were used, with the generator fed a vector of 64 random variables. Sigmoid and softmax were used as output nonlinearities for MNIST and CelebA datasets, respectively. The importance-weighted BGAN was trained on discrete MNIST data and produced realistic handwritten digits. Comparison experiments with WGAN-GP showed the BGAN's stability. The generator trained on quantized CelebA successfully learned English language patterns. The BGAN generator produced realistic images on quantized CelebA dataset and showed stability in comparison with WGAN-GP. Next, BGAN was tested on a natural language setting with the BID5 dataset, generating character-level language with good diversity. The text discusses the use of deep convolutional neural networks for character-level language generation with stable results using BGAN. Despite lower quality compared to recurrent neural network-based methods, the discrete GAN shows stability and capability in generating text without pretraining or auxiliary supervised loss. In experiments using BGAN, the generator function was modified while maintaining the original GAN variational lower-bound. Gradient norm regularization was applied for stability. Training was conducted on high-dimensional data from CelebA, LSUN, and ImageNet datasets without conditioning on labels. The discriminator and generator were modeled as 4-layer Resnets. Results showed high sample quality, especially from the Imagenet model. BGAN models were trained on CelebA and LSUN datasets using a deep ResNet architecture with gradient norm regularization. The Imagenet model was trained on the full 1000 label dataset without conditioning. BGAN works well in high-dimensional settings and benefits from gradient norm regularization for stability. Training requires a delicate balance between the generator and discriminator to avoid destabilizing learning. Training with different generator loss functions can impact stability and sample quality. The Boundary-Seeking Loss (BGAN) objective shows resilience to over-training and outperforms other loss functions in generating reasonable samples in fewer epochs. This highlights the importance of balancing the generator and discriminator during training to avoid destabilizing learning. Training a DCGAN with the proxy loss and optimizing the discriminator for 1000 updates. Gradient descent is then performed on pixels, variational lower-bound, proxy, and boundary seeking losses separately. Over-optimizing the generator can lead to instability and poorer results. Results show that following the BGAN objective at the pixel-level causes the least degradation of image quality. Following the BGAN objective at the pixel-level causes the least degradation of image quality. Reinterpreting the generator objective to match the proposal target distribution reveals a novel learning algorithm for training a generative adversarial network (GANs). This proposed approach of boundary-seeking provides a unified framework for learning algorithms for both discrete and continuous variables. The effectiveness of training a GAN with the proposed learning algorithm, called a boundary-seeking GAN (BGAN), on both discrete and continuous variables was empirically verified. The generator objective in BGAN and the proxy have sharp initial gradients that decay quickly, while the variational lower-bound objective gradient slowly increases. BGAN performs the best in this task, indicating its objective will not disrupt adversarial learning. Experiments show quantitative measures for BGAN against WGAN-GP on the discrete MNIST dataset. WGAN-GP uses softmax probabilities directly for back-propagation, bypassing pixel-level sampling issues. The WGAN-GP model has been successful in generating samples resembling the target dataset despite challenges in estimating gradients. Three models were trained on the discrete MNIST dataset using different objectives, with WGAN-GP trained with a gradient penalty hyper-parameter of 5.0. The BGAN models were trained with a gradient norm penalty of 5.0. Experiments were conducted on discrete MNIST dataset using BGAN models trained with a gradient norm penalty of 5.0 BID32. Three new discriminators with double capacity were trained for each model to maximize JS and reverse KL divergences and Wasserstein distance. Discriminators were trained for 200 epochs with gradient-based regularizations. Final evaluation was done using 60000 MNIST training examples against samples from each generator. Our results show that the estimates from the sampling distribution from BGAN are consistently lower than that from WGAN-GP, even when evaluating using the Wasserstein distance. However, when training the discriminator on the softmax probabilities, WGAN-GP has a much lower Wasserstein distance. Despite quantitative differences, samples from these different models were indistinguishable in quality by visual inspection. This indicates that using the softmax outputs in the adversarial game can generate realistic-looking samples, but ultimately hinders the generator's ability to model a truly discrete distribution. The policy gradient in Equation 10 is validated theoretically and empirically, showing that the conditional KL-divergence must be zero for the generator to model a discrete distribution effectively. Empirical evaluation compares Monte-Carlo estimate of \u03b2 with a variance-reducing method in Equation 10. The study compares models using different sample sizes from the prior and conditional distributions. Results show that the variance-reducing method from Equation 10 outperforms estimating \u03b2. Increasing the number of conditional samples improves both methods. The study compares models using different sample sizes from the prior and conditional distributions. Results show that the variance-reducing method outperforms estimating \u03b2, with the generator consistently producing lower estimated GAN distances."
}