{
    "title": "ryepFJbA-",
    "content": "We propose studying GAN training dynamics as regret minimization to understand mode collapse. Undesirable local equilibria in the non-convex game lead to mode collapse, with sharp gradients around real data points. DRAGAN, a gradient penalty scheme, prevents degenerate local equilibria, resulting in faster training, improved stability, and better modeling performance for generator networks. Generative modeling involves estimating a distribution that closely matches the real data distribution using generative adversarial networks (GAN). GANs consist of a generator and discriminator playing a zero-sum game to reach equilibrium. However, GAN training dynamics are unstable and can lead to mode collapse. This paper suggests studying GAN training dynamics as a repeated game with no-regret algorithms to address this issue. Theoretical arguments in recent literature are based on unrealistic assumptions about GAN training dynamics, leading to a disconnect between theory and practice. Regret minimization is proposed as a more appropriate way to understand GAN training dynamics and address issues like mode collapse. The convergence of GAN training is analyzed from a new perspective to understand mode collapse. A unique solution and guaranteed convergence can be achieved in the artificial convex-concave case using AGD and regret minimization. However, in practical cases with non-convex game objective functions, such as when using deep neural networks, these convergence results do not hold. In non-convex games, global regret minimization and equilibrium computation are computationally hard. Recent literature suggests that AGD can lead to cycling or local equilibrium in GAN training, causing mode collapse. The prevalent view attributes mode collapse to minimizing strong divergence, but GAN training with AGD does not consistently minimize divergence. Mode collapse may result from undesirable local equilibrium, raising questions about stability. In GAN training, mode collapse can be mitigated by regularizing the discriminator to constrain gradients around real data points. This approach improves stability and explains the effectiveness of WGAN and gradient penalties in preventing mode collapse. A new training algorithm called DRAGAN (Deep Regret Analytic Generative Adversarial Networks) incorporates a gradient penalty scheme to address this issue. DRAGAN is a gradient penalty scheme that enables faster and more stable training of GANs, outperforming the state-of-the-art WGAN-GP BID8. Recent works have focused on stabilizing GAN training, with some requiring specific architectures or deviating from the original framework. Our work aims to provide a fast and versatile method for consistent stable training of GANs by proposing a new way of reasoning about training dynamics. We propose a new approach to GAN training dynamics by viewing AGD as regret minimization. Our novel proof shows asymptotic convergence in the nonparametric limit without requiring the discriminator to be optimal at each step. We discuss mode collapse in non-convex games and introduce the DRAGAN gradient penalty scheme to mitigate this issue. Our work connects GAN training process (AGD) with regret minimization for guaranteed convergence. The critical connection between GAN training and regret minimization is explored, leading to a novel proof for asymptotic convergence. Mode collapse situations are characterized, leading to the introduction of the gradient penalty scheme DRAGAN. The GAN framework involves a repeated zero-sum game between the generator and discriminator. The generator model G, parameterized by \u03c6, creates synthetic samples from a noise vector z. The discriminator model D, parameterized by \u03b8, determines the probability that a sample x is real. GANs typically use deep networks for both models. The game is defined by cost functions and the generator distribution converges to the real distribution with optimal updates. In a setting where a function is convex in the first argument and concave in the second, an equilibrium is guaranteed to exist. Finding such an equilibrium can be achieved through regret minimization algorithms, which are efficient and lead to convergence. No-regret algorithms can be used by both players to converge to an equilibrium pair. No-regret learning can be applied to the problem of equilibrium finding in the GAN game. After T rounds of play, players compute average iterates to reach an equilibrium value V*. Regret terms R1(T) and R2(T) show that solutions \u03b8T and \u03c6T are \"almost optimal\". Convergence is guaranteed under the no-regret condition. The Follow The Regularized Leader algorithm selects k on each round. FTRL BID10 selects k on each round by solving for arg min DISPLAYFORM6 with a convex regularization function \u2126(\u00b7) and learning rate \u03b7. If \u2126(\u00b7) = 1/2 \u00b7 2, FTRL becomes online gradient descent (OGD). GAN training involves alternating gradient updates similar to OGD. The GAN objective function includes stochastic components x and z sampled from data distribution and normal distribution. The full game is defined by taking expectations with respect to x and z. The full game is defined by taking expectations with respect to x and z, and the equilibrium computation proceeds similarly with stochastic inputs. A benefit of this stochastic perspective is the ability to get a generalization bound on the mean parameters after T rounds of optimization. The \"online-to-batch conversion\" implies that the optimal value is no more than the expected value, with limitations requiring a fresh sample to be used on every round. In this subsection, the discussion revolves around solving the artificial convex-concave case through regret minimization, a concept not widely known in GAN literature. The critical connection between regret minimization and alternating gradient updates for GAN training is highlighted. BID6 argues that with sufficient capacity and updates in function space, the GAN game can be considered convex-concave. This analysis provides a novel proof for the asymptotic convergence of GANs, without the need for the discriminator to be optimal at each step. The connection between regret minimization and GAN training dynamics challenges the popular view of GAN training as consistently minimizing a divergence. This questions recent developments like WGAN and gradient penalties aimed at improving training stability. The non-convex case leads to training instability and mode collapse, requiring a new perspective. Deep neural networks are chosen for G and D, with the function J(\u03c6, \u03b8) no longer needing to be convexconcave. Unique solutions and guaranteed convergence seen in the convex-concave case are no longer applicable. In non-convex settings, regret minimization and equilibrium computation are computationally hard. Recent work introduces the concept of local regret, showing that using a smoothed variant of OGD can lead to convergence to a local equilibrium in non-convex games. GAN training with AGD corresponds to a window size of 1, leading to convergence to a local equilibrium or cycling of updates. Future research is needed to address the cycling issue. In a local equilibrium, players do not have an incentive to switch strategies. Mode collapse in GAN training occurs when the generator maps multiple inputs to the same output, likely due to converging to bad equilibria. Mode collapse is attributed to disjoint supports of real and model distributions. The introduction of WGAN was motivated by the limitation of strong distance measures like KL-divergence or JS-divergence maxing out, hindering the generator from learning useful gradients. GAN training does not consistently minimize a divergence, leading to mode collapse due to undesirable local equilibria. The new perspective of GAN training as regret minimization offers an explanation for mode collapse and raises the question of how to avoid it. In algorithmic game theory BID14, the focus is on characterizing mode collapse in the GAN game to prevent undesirable local equilibria. Mode collapse is often caused by sharp gradients in the discriminator function around real data points, leading to a degenerate equilibrium. Regularizing the discriminator with a penalty helps improve GAN training stability, as shown in toy experiments with neural networks. This strategy partly explains the success of WGAN and gradient methods. The success of WGAN and gradient penalties in improving GAN training stability is partly explained by regularizing the discriminator with a penalty. However, the current penalty scheme can be brittle and lead to assigning both real and noise points the same probability. A better penalty choice is proposed, and practical optimization considerations suggest using a different penalty in experiments. Stochasticity can help escape bad equilibria, as shown in experiments with one hidden layer networks for G and D. The success of WGAN and gradient penalties in improving GAN training stability is explained by regularizing the discriminator with a penalty. The DRAGAN algorithm uses penalty schemes to constrain the norm of discriminator's gradients around real points, mitigating mode collapse. Small perturbations of real data are likely to lie off the data-manifold, allowing the discriminator to assign different probabilities to training data and noisy samples. Practitioners should consider this when choosing a penalty. The DRAGAN algorithm utilizes penalty schemes to regulate the discriminator's gradients around real data points, enhancing GAN training stability. Hyperparameters like \u03bb \u223c 10, k = 1, and c \u223c 10 are commonly set in experiments. Other regularization schemes, such as WGAN-GP and LS-GAN, also aim to improve GAN stability by constraining the discriminator's gradients. LS-GAN introduced LS-GAN with a margin between real and fake sample losses, while BID8 extended WGAN with a condition on D leading to norm-1 gradients between real and fake samples, similar to LS-GAN. WGAN-GP is a variation of LS-GAN with coupled penalties. The penalty in WGAN-GP does not follow from KR-duality as claimed. The optimal discriminator in WGAN-GP will have norm-1 gradients between real and fake samples only from the optimal coupling distribution. This highlights why gradient penalties may prevent mode collapse. Unlike coupled penalties, our method imposes gradient constraints only around real samples. Our method imposes gradient constraints only around real samples, referred to as \"local penalties\". Unlike coupled penalties in WGAN-GP, our algorithm works with AGD and consistently regularizes D \u03b8 (x) along the real data manifold. This approach avoids potential issues with penalties that depend on generated samples and results in a less restricted class of functions, improving modeling performance. Appropriate constraining of the discriminator's gradients can mitigate mode collapse, but caution is needed to avoid negative effects. Local penalties are highlighted as a solution to issues with coupled penalties, improving modeling performance. Experimental results comparing our algorithm with vanilla GAN and WGAN variants are detailed in section 3, showcasing improved stability and compatibility with different architectures. The use of inception score BID18 and sample quality as metrics is emphasized throughout the study. DCGAN is a popular architecture known for its performance in GAN literature. It is used to model CIFAR-10 and compared against vanilla GAN, WGAN, and WGAN-GP. Results show that DRAGAN outperforms WGAN variants, with vanilla GAN slightly better. DRAGAN is faster than WGANs, making its performance closer to the DRAGAN case. In the next section, it is shown that moving away from the DCGAN architecture can lead to unstable vanilla GAN training, but the DRAGAN penalty helps mitigate this issue. A new metric called the BogoNet score is introduced to compare the stability and performance of different GAN training procedures across various architectures. In our experiment, players are assigned networks from different architecture families (MLP, ResNet, DCGAN) to evaluate algorithm performance in games. We compare our algorithm to vanilla GAN training and WGAN-GP on 100 instances of hard games trained on CIFAR-10. Results show our algorithm outperforms others in both final inception scores and AUC. The results in TAB0 show our algorithm outperforms others in stability and modeling performance. Qualitative analysis confirms BogoNet score captures stability improvements. We compare DRAGAN and vanilla GAN on 50 hard architectures, assigning points based on performance. Results were judged blindly by two authors. Our algorithm outperforms others in stability and modeling performance, with an average score of 157.5 compared to the vanilla GAN's score of 92.5. The experiment demonstrates improvements in stability across various objective functions, showing that our algorithm is stable in all cases except for total variation. The vanilla algorithm failed in all cases. In contrast to the popular view of consistent minimization of divergence between real and generated distributions, this paper proposes studying GAN training as regret minimization. Mode collapse is attributed to undesirable local equilibria, with sharp gradients of the discriminator function around real data points. This insight motivates a novel penalty scheme, different from WGAN and gradient penalties. The paper proposes a novel penalty scheme called DRAGAN to improve stability in GAN training. Results show enhanced modeling performance across various settings. Experiment results on CelebA dataset and latent space walks demonstrate smooth transitions between images when using the DRAGAN penalty. The paper introduces the DRAGAN penalty to enhance stability in GAN training, showing improved modeling performance. Experiment results on the 8-Gaussians dataset compare WGAN-GP and DRAGAN, with DRAGAN's discriminator being more flexible. The DCGAN architecture is designed for stability following specific guidelines. The DCGAN architecture recommends using all-convolutional networks with batch normalization, ReLU activation, and specific layer configurations for stability. Experiments show that these constraints can be relaxed without compromising training stability. The algorithm demonstrates stability in training procedures by using different architectures and variations within each family, such as DCGAN, ResNet, and MLP. The variations include different nonlinearities, batch normalization, filter configurations, latent space dimensionality, and random selection of layers and hidden units. The study tested the stability of training algorithms using various architectures like DCGAN, ResNet, and MLP with random selection of parameters. Qualitative analysis confirmed that the BogoNet score reflects improvements in stability. Examples of bounty splits were shown with corresponding scores in FIG2."
}