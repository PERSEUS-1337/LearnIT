{
    "title": "r1DPFCyA-",
    "content": "This paper presents a probabilistic framework for k-shot image classification, leveraging feature-based representation and concept transfer. The approach achieves state-of-the-art results on k-shot learning datasets, accurately modeling uncertainty and providing well-calibrated classifiers. The paper focuses on k-shot learning, leveraging high-level feature representations and concept transfer to build machine systems for image classification. It aims to utilize existing feature representations and class information honed from large amounts of labeled data. K-shot learning has gained popularity in the academic community, with current state-of-the-art methods using complex deep learning architectures. This paper proposes a general framework for k-shot learning that combines a deep feature extractor trained on batch classification with traditional probabilistic modeling. It aims to leverage deep learning for powerful feature representations and transfer conceptual information from old classes using probabilistic inference. The approach is motivated by ideas from multi-task learning and is driven by a large number of training examples from original classes. The new classes in k-shot learning rely on existing classes and a small number of data points for probabilistic inference. Calibration, the agreement between a classifier's uncertainty and its mistakes, is crucial in applications like autonomous driving and medicine. Deep architectures show deteriorating calibration with increased depth and complexity, related to catastrophic forgetting in continual learning. Uncertainty, often overlooked in k-shot learning, is high in this setting. The main contribution of the paper is a probabilistic framework for k-shot learning that combines deep convolutional features with a model treating top-level weights of a neural network as data. This framework can be used to regularize weights at k-shot time in a Bayesian fashion. The framework presented in the paper regularizes weights at k-shot time in a Bayesian fashion, recovering L2-regularized logistic regression as a special case. It achieves state-of-the-art results on the miniImageNet dataset, showing a significant improvement for 1-and 5-shot learning. The approach also strikes a good balance between classification accuracy and calibration, learning new classes while retaining old ones. Contrary to current beliefs, architectures with better batch classification accuracy generalize well at k-shot time, challenging the necessity of episodic training for good performance in k-shot learning. The paper proposes a k-shot learning task where a model is trained on a large dataset and a small dataset of new classes to predict well on unseen images. Unlike other approaches, they use the large dataset to train a feature extractor for batch classification, which is then combined with a probabilistic model for k-shot learning. The paper introduces a probabilistic framework for k-shot learning, building on a general framework for multi-task learning with shared parameters. The framework consists of four phases: representational learning, concept learning, k-shot learning, and k-shot testing. It is shown that under certain assumptions, the method is related to regularized logistic regression. The paper introduces a probabilistic framework for k-shot learning, with a detailed derivation in Appendix A. A convolutional neural network (CNN) \u03a6 \u03d5 is used as a feature extractor, with separate mappings for the large dataset D and the small dataset D. The CNN is trained using the large dataset D for representational learning. The large dataset D is used to train the CNN \u03a6 \u03d5 using deep learning optimization. The parameters \u03d5 and softmax weights W are learned and fixed for later phases. A probabilistic method for k-shot prediction transfers structure from W to new softmax weights and combines it with training examples. During k-shot testing, the feature representation x * is computed for a test image u * and the prediction y * is found by averaging softmax outputs over the posterior distribution of softmax weights. The large dataset D is used to train the CNN \u03a6 \u03d5 using deep learning optimization. The parameters \u03d5 and softmax weights W are learned and fixed for later phases. A class of probabilistic models generates softmax weights from shared hyperparameters \u03b8, constraining new weights based on \u03b8 information from D. Uncertainty in W is low after initial training, allowing for a MAP estimate. The original data D is not needed for k-shot learning; instead, weights learned from D induce a predictive distribution over k-shot weights via Bayes' rule. This step is referred to as concept learning. During k-shot learning (phase 3), the predictive distribution from concept learning is used as a new prior on the weights. Bayes' rule is applied to combine this distribution with the softmax likelihood of k-shot training examples, resulting in a new posterior over the weights. The probabilistic model over the weights plays a crucial role in transferring knowledge to improve performance, with simplifying assumptions made due to the small number of k-shot samples and high-dimensional weights. After k-shot learning, a Gaussian model for weights is used with a simple Gaussian distribution. The weights are assumed to be independent, and a Gaussian model strikes a balance between complexity and learnability. The covariance matrix is restricted to be isotropic for better performance in k-shot learning. The method involves using MAP estimates for new class weights and relates to logistic regression. The method involves using MAP estimates for new class weights in k-shot learning, related to logistic regression. Logistic regression corresponds to the MLE solution of the softmax likelihood. Regularised logistic regression assumes zero mean, while the probabilistic framework allows for non-zero means and a principled way of choosing the regularisation constant. Embedding methods map k-shot training and test points. Embedding methods like Siamese Networks, Matching Networks, and Prototypical Networks train the weights for k-shot learning tasks by mapping points into a non-linear space and using different classification metrics. These methods learn representations for k-shot learning but do not directly leverage concept transfer. Amortised optimisation methods improve k-shot learning tasks by training a second network to initialise and optimise a CNN for accurate classification on small datasets. Training specifically for k-shot learning is essential for building features that generalize well at testing time. The proposed approach in the paper is flexible and does not require retraining when switching between different k-shot scenarios. Training with a larger number of k-shot classes can improve performance, chosen through cross-validation. Deep probabilistic methods, including the approach in this paper, treat weights as data for transfer learning. Related work focuses on training CNNs with imbalanced classes using a Gaussian mixture model. BID1 proposes an elegant k-shot learning approach. BID1 proposes a Gaussian model for k-shot learning, with promising but preliminary evaluation results. Qiao et al., 2017 also develop a method for k-shot learning that trains a recognition model to amortise MAP inference for softmax weights. The code for the experiments will be available after review, using the miniImageNet dataset derived from ImageNet ILSVRC12. The study utilizes a subset of the ImageNet ILSVRC12 dataset with 100 classes and 600 downscaled images per class. They use CNNs inspired by ResNet-34 and VGG for representational learning on the classes. Data augmentation is applied during training, but not during k-shot learning and testing. Details on architecture, training, and data augmentation can be found in the appendix. The study evaluates t-SNE embeddings of last layer weights, compares against baselines and state-of-the-art methods, and conducts 5-way k-shot learning on miniImageNet dataset. The study evaluates t-SNE embeddings of last layer weights and compares against baselines and state-of-the-art methods. Performance on the miniImageNet dataset is reported, with the best method using a modified ResNet-34 with 256 features consistently outperforming all competing methods. The isotropic Gaussian model using ResNet-34 features achieves state-of-the-art results and beats prototypical networks by a wide margin. The study evaluates t-SNE embeddings of last layer weights and compares against baselines and state-of-the-art methods on the miniImageNet dataset. The best method uses a modified ResNet-34 with 256 features, consistently outperforming all competitors. The isotropic Gaussian model with ResNet-34 features achieves state-of-the-art results, beating prototypical networks by a wide margin of about 6%. Different feature extractors of increasing complexity show a correlation with accuracy in k-shot learning, with deeper features leading to better performance. The study shows that using a ResNet trained with all available data achieves 74% accuracy, a significant increase of almost 10%. Gauss (iso) outperforms Log Reg on 1-shot learning and performs similarly on 5-and 10-shot. Training specifically for k-shot learning is not necessary for high generalization performance. Training a deep feature extractor on batch classification and building a simple probabilistic model achieves state-of-the-art results. The depth of the model in k-shot learning is limited by training nature. Nearest neighbours perform similarly to Gauss (iso) on 1-shot learning but are outperformed on 5-and 10-shot. Building a simple classifier on learned features works better for k-shot learning. Calibration is important for classifiers to predict probabilities accurately. In k-shot learning, calibration of classifiers is crucial for accurate probability predictions. Log Reg (C = 2\u03c3 2 W) and Gauss (iso) show better accuracy and calibration than Log Reg (cross-validation). The choice of regularisation constant in logistic regression impacts calibration quality, emphasizing the importance of selecting the right constant for optimal performance. In k-shot learning, calibration of classifiers is crucial for accurate probability predictions. Log Reg (C = 2\u03c3 2 W) strikes a good balance between accuracy and log-likelihood, providing a principled way of selecting the regularisation parameter. The third plot in Fig. 4 visualizes the trade-off between accuracy and calibration for Log Reg (C = 2\u03c3 2 W), highlighting the failure of Log Reg (cross-validation) in achieving a good compromise in 5-and 10-shot settings. Evaluation in an online setting is also briefly considered. In an online setting, evaluation is done on 80 old and 5 new classes to address catastrophic forgetting. Utilizing ResNet-34 trained on 500 images per class, a total of 85 weight vectors are employed during k-shot learning and testing. The MAP estimate helps maintain accuracy on old classes while performing well on new classes, emphasizing the importance of proper regularization in logistic regression. Training Logistic Regression in the presence of old weights is crucial for online learning. A comparison of probabilistic weight models on CIFAR-100 dataset shows that the simple Gaussian model performs well at k-shot time, balancing complexity and statistical efficiency. The study recommends using simple models and inference schemes for k-shot learning to avoid high computational costs. A probabilistic framework leveraging neural network features and class information is proposed for transferring knowledge to new classes. Experiments on miniImageNet show promising results. The study introduces a new approach for k-shot learning using a Gaussian model that achieves state-of-the-art results on miniImageNet. This approach challenges the belief that episodic training is necessary for good k-shot features and provides a flexible and extensible framework for general discriminative models. Preliminary results suggest that the probabilistic framework can mitigate catastrophic forgetting in online k-shot learning. The probabilistic k-shot learning approach involves four phases: Representational learning, where a CNN is trained on a large dataset; the parameters of the feature extractor and softmax weights are learned. The network parameters are fixed and shared across phases for multitask learning. Phase 2 involves concept learning by using softmax weights as data for training a probabilistic model to detect structure for k-shot learning. Phases 3 and 4 focus on leveraging the learned representation and probabilistic model to build a predictive model for new examples in k-shot learning. In a completely probabilistic approach based on the assumed model, the initial dataset forms the posterior distribution over concept hyperparameters. The k-shot learning phase combines new weights with k-shot data to form the posterior distribution. Inference in this model is challenging and requires approximations, especially in computing the posterior distribution over hyperparameters. Progress can be made by assuming the posterior distribution over weights can be well approximated by the MAP. In the k-shot testing phase, approximate inference is used to compute p(y | x, D) for new weights combined with k-shot data. This simplifies the learning pipeline as the model only needs access to the weights returned by representational learning. Intractabilities involving a small number of data points can be handled using standard approximate inference tools. Approximate inference methods for probabilistic models are briefly discussed in this section. While MAP inference was considered in the main text, other more complex schemes were found to not provide practical benefits. However, a detailed model comparison in Appendix E.4 includes other approximate inference methods. Gradients of densities with respect to weights can be computed, allowing for efficient MAP inference in the k-shot learning phase using gradient-based optimization or Markov Chain Monte Carlo (MCMC) sampling. Hybrid Monte Carlo (HMC) sampling, specifically the NUTS sampler, is employed due to high dimensionality and available gradients. The section discusses different priors on weights for Gaussian models, including Gaussian mixture models and Laplace distribution. The main paper uses a Gaussian model with MAP inference, with a comparison of models in Appendix E.4. The normal-inverse-Wishart distribution is a conjugate prior for the Gaussian, allowing for closed-form posterior calculation. More details can be found in BID26. The posterior for the Gaussian model can be written in closed form using the normal-inverse-Wishart distribution. The multivariate Student t-distribution can be obtained by integrating the model. The hyperparameters for the normal-inverse-Wishart distribution can be chosen in different ways, such as optimizing the log probability of held-out training weights. It is common to use uninformative or data-dependent priors for these hyperparameters. In the context of Gaussian mixture models, MAP inference is used due to intractable exact inference. The GMM parameters are estimated via maximum likelihood using the EM algorithm. The model considers cluster structures for weights and covariance, fitting components as a mixture of Gaussians with S components. In experiments, the MAP mean and covariance are computed for each cluster, with classes grouped into superclasses for CIFAR-100. In CIFAR-100, classes are grouped into 20 superclasses, each containing 5 classes. 20 components are initialized and fitted with data points in corresponding superclasses using MAP inference. Superclasses are merged into 9 larger superclasses to increase weight examples. Parameters of the mixture can be fit using maximum likelihood with EM algorithm. In CIFAR-100, classes are grouped into 20 superclasses, each containing 5 classes. 20 components are initialized and fitted with data points in corresponding superclasses using MAP inference. Superclasses are merged into 9 larger superclasses to increase weight examples. Parameters of the mixture can be fit using maximum likelihood with EM algorithm. The implementation of EM in scikit-learn is used for fitting. Weight log-likelihoods under this model and k-shot performance can be found in Appendix E.4. Sparse models are considered for modeling the weights, with a product of independent Laplace distribution being used. The Laplace prior is related to L1 regularized logistic regression, encouraging sparse weight. The prior considered factors along feature dimensions for L1 regularized logistic regression, encouraging sparse weight vectors. Parameters \u00b5 and \u03bb are fitted via maximum likelihood. An isotropic Laplace model with mean \u00b5 and scale \u03bb is also explored. MiniImageNet is constructed using classes split into training, validation, and test sets. Image files will be made available, and 600 images per class are extracted from the ImageNet 2012 Challenge dataset. Dataset BID23 consists of coloured 84 \u00d7 84 \u00d7 3 images used for representational and k-shot learning and testing. Data augmentation techniques such as random flipping, pasting images into 100 \u00d7 100 frames, and adjusting brightness, contrast, saturation, and lighting are applied for training deep models like ResNet. No data augmentation is used for k-shot learning and testing. CIFAR-100 contains 100 classes grouped into 20 superclasses, each with 500 training and 100 test images of size 32 \u00d7 32. The CIFAR-100 dataset consists of 100 classes grouped into 20 superclasses with 500 training and 100 test images of size 32 \u00d7 32. The classes are split into 80 base classes and 20 k-shot learning classes. Classes held out for testing are listed separately. Various probabilistic models are compared for the k-shot learning task in the appendix. The network architecture for k-shot learning task is based on ResNet-34 with convolution blocks and skip connections. Example code from tensorpack is used, with adjustments for smaller training samples and classes. The final architecture details are provided in Tab. 5. The curr_chunk contains a list of various objects such as beetles, file cabinets, merry-go-rounds, dugongs, tanks, cliffs, photocopiers, holsters, and more. The curr_chunk includes a variety of objects such as triceratops, tobacco shop, jellyfish, slot machine, robin, organ, hair slide, miniature poodle, orange, consomme, barrel, spider web, rock beauty, toucan, sloth, cocktail shaker, dishrag, French bulldog, street sign, beer bottle, parallel bars, wok, Saluki, Gordon setter, Arctic fox, and house. Table 2: Training classes for miniImageNet include a variety of objects such as combination lock, carton, poncho, white wolf, horizontal bar, iPod, catamaran, garbage truck, miniskirt, Ibizan hound, rhinoceros beetle, coral reef, cannon, goose, meerkat, and missiles. Table 3: Validation classes for miniImageNet consist of African hunting dog, malamute, black-footed ferret, cuirass, mixing bowl, hourglass, scoreboard, crate, and theater. Table 5: Network architecture for probabilistic k-shot learning using a VGG-style network for CIFAR-100. The network consists of 2D convolutions with specified kernel size and padding, with the output corresponding to the feature space representation of the image. Trained with a decaying learning rate schedule and momentum SGD using tensorpack with tensorflow. The network architecture for probabilistic k-shot learning is implemented in tensorpack using tensorflow. It is inspired by VGG networks but does not use batch normalization. Exponential linear units (ELUs) are employed for faster convergence, along with dropout and weight regularization in fully connected layers for network regularization. The network for probabilistic k-shot learning is implemented in tensorpack using tensorflow, inspired by VGG networks without batch normalization. It employs dropout and weight regularization in fully connected layers for network regularization. The t-SNE embedding of CIFAR-100 weights shows a clear structure by superclass, indicating useful information for k-shot learning. Despite unique classes in miniImageNet, meaningful patterns are still observed. The t-SNE embeddings of weights from a VGG network trained on CIFAR-100 and a ResNet-34 trained on miniImageNet show clear structures by superclass, motivating a framework for k-shot learning. Extended results on k-shot learning for miniImageNet with different network architectures are provided in Figure 8. The ResNet-34 and VGG-style network architectures show differences in performance for k-shot learning, with the more complex architecture performing better in terms of accuracy. The VGG-style network exhibits better calibration, indicating that calibration worsens with increased depth and complexity in deep architectures. The Bayesian regularisation parameter choice balances accuracy and calibration, outperforming cross-validated parameter choice. Optimized values for various models on CIFAR-100 dataset are provided, along with results for VGG style architecture. The log-likelihood and calibration of different methods and inference procedures are compared in Tab. 8. Most methods show similar accuracy and log-likelihood, with some variations in calibration. Multivariate Gaussian models tend to outperform other models due to the small dataset size and high dimensionality. Results for k-shot performance in CIFAR-100 are also provided, with accuracies measured on a 5-way classification task for different values of k. Among the models compared, there were no significant differences in accuracy except for Laplace MAP and GMM (iso), which consistently performed poorly. The findings were consistent in terms of log-likelihoods. Gaussian models were generally better calibrated than Laplace models, with low ECE and high accuracy. While some models were slightly better calibrated than the proposed method, the gain in calibration was not worth the increased computational resources needed. Interestingly, both GMM approaches did not perform well. The use of mixture models in k-shot learning framework is not beneficial due to the small number of data points and high-dimensionality of weight-space. A tree-structured mixture model outperformed simpler models in superclass assignments optimization. However, comparison against a simpler baseline like a single Gaussian model was not conducted. Our recommendation is that practitioners should use simple models and inference schemes to estimate parameters, as more complex methods do not show significant benefits over simpler models like isotropic Gaussian in terms of accuracy, log-likelihood, or calibration. This contrasts with the previous finding that a tree-structured mixture model outperformed simpler models in superclass assignments optimization."
}