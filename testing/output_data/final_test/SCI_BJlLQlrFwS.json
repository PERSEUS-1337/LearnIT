{
    "title": "BJlLQlrFwS",
    "content": "Continual learning in artificial intelligence faces challenges due to catastrophic forgetting, hindering neural networks from learning tasks sequentially. Previous methods have shown how to mitigate this issue and maintain performance on previous tasks. A new L1 minimization criterion is proposed to address catastrophic forgetting, with strategies to control forgetting more effectively. Evaluation on 3 datasets shows improvements over existing L2 strategies for mitigating catastrophic forgetting in machine learning applications. Continual learning in AI faces challenges like catastrophic forgetting, where systems struggle to retain performance on previous tasks while learning new ones. Various approaches aim to mitigate this issue, such as the proposed L1 minimization criterion. This criterion helps control forgetting more effectively and shows improvements over existing L2 strategies in machine learning applications. In the literature, approaches to mitigate forgetting include architectural, regularization, and memory approaches. These methods aim to help the network learn new tasks while preserving performance on previous tasks. Performance is often evaluated based on overall accuracy, but this work specifically focuses on what has been forgotten and learned, which is crucial for safety-critical systems. Regularization strategies in continual learning focus on minimizing changes to influential weights for previous tasks, either from a Bayesian perspective or by minimizing KL-divergence between posterior and prior distributions. In this paper, a new framework is proposed to mitigate catastrophic forgetting by directly penalizing changes in classifier likelihood functions. The authors analyze catastrophic forgetting, introduce a generic L1 minimization criterion to address it, propose two strategies to implement this criterion, and discuss how the cross-entropy loss can be adjusted for better control over forgetting. The paper introduces a new framework to combat catastrophic forgetting by penalizing changes in classifier likelihood functions. It proposes strategies to implement this criterion and evaluates them on three datasets, showing improvements over traditional regularization methods like EWC and SI. The paper introduces a framework to prevent catastrophic forgetting by penalizing changes in classifier likelihood functions. It discusses minimizing cross entropy loss for each task to achieve optimal performance and the importance of retaining performance on previous tasks. Bayesian continual learning is described as maximizing posterior given a prior, with joint training being effective but costly as tasks increase. The Bayesian formulation in EWC minimizes dissimilarity between prior and posterior likelihood distributions using KL-divergence. EWC penalizes changes in weights after learning each task, controlled by a hyperparameter \u03bb. The weights of the network are updated sequentially, with a small change \u2206\u03b8 after each task. The weights are updated sequentially with a small change \u2206\u03b8 after each task to minimize catastrophic forgetting. This minimization criterion can be interpreted as a regularization objective with four broad use cases. The most restrictive version of the criterion penalizes changes to individual predicted likelihoods from \u03b8 * to \u03b8 * + \u2206\u03b8. This can be achieved by regularizing a sum over the individual changes. Another case involves preserving the change in predicted likelihood for the predicted label at \u03b8 * , which may be desired in safety-critical systems like autonomous driving. To achieve this, an expectation over the difference in individual predicted likelihoods can be used and then regularized. The criterion penalizes changes in individual predicted likelihoods from \u03b8 * to \u03b8 * + \u2206\u03b8. This can be achieved by regularizing a sum over the individual changes, preserving the change in predicted likelihood for the predicted label at \u03b8 * by computing the expectation over the difference in individual predicted likelihoods. The loss penalty proposed in Kirkpatrick et al. (2016) and corrected in Husz\u00e1r (2018) represents the upper-bound of the squared L2 change in predicted likelihood. Two strategies are proposed to minimize the expected change in predicted likelihood: Method 1 involves direct regularization per task, while L1 regularization is known to produce sparser solutions than L2 regularization. The L1 penalty penalizes change in predicted likelihoods more strongly than L2, leading to better preservation but potentially hindering learning. Four methods (DM-I, DM-II, DM-III, DM-IV) can be used with direct minimization. Introducing parameters c1 and c2 helps constrain learning after task i is learned. After learning a task, the change in predicted likelihood is bounded by a threshold c. It is beneficial to adjust the threshold for each task to limit forgetting. This approach helps control the amount of forgetting compared to other methods like EWC or SI. Soft regularization strategies update all weights, potentially causing small perturbations that can affect sensitive weights. The approach of freezing important weights to minimize changes in classifier likelihood is discussed. This method, denoted as DM-p, aims to reduce the magnitude of parameter updates and limit the impact on classifier performance. This strategy is compared to other regularization techniques like EWC and SI, which update all weights and may cause small perturbations affecting sensitive weights. The approach of freezing important weights to minimize changes in classifier likelihood is discussed in the curr_chunk. It impairs the gradient update by setting the gradients of top p% important parameters to 0 for each task i \u2265 2. The methodology and results of experiments are described, comparing the performance of different methods in continual learning literature. The curr_chunk discusses various strategies for minimizing losses and implementing synaptic intelligence in neural networks. Different soft regularization strategies and freezing methods are proposed and evaluated through experiments on feedforward ReLU networks. Hyperparameter search is conducted to find the best configuration for each strategy, with final results reported across multiple seeds. The curr_chunk discusses the performance evaluation of proposed methods across multiple seeds with the best parameter configuration. It includes assessing the retention of predicted likelihood after training on multiple tasks. The likelihood retention after n tasks is calculated and reported in Table 4. Additionally, insights on hyperparameter choice are provided in Table 1. The curr_chunk discusses hyperparameter choice for different methods, highlighting the need for a higher \u03bb in EWC compared to L1 methods. It also explores the degree of preservation in DM-p and the improvements over L2 methods in P-MNIST and Sim-EMNIST datasets. In experiments with the 5-task variant of P-MNIST, DM-I, II, III, IV, and DM-p outperform EWC and SI, reaching an average final accuracy of \u223c94%. On S-MNIST, EWC struggles to go beyond \u223c69% due to the dataset's difficulty, but DM-p and DM-I, II, III, IV show improvements of \u223c7-10%. In experiments with different preservation methods, DM-III and DM-IV perform best in terms of accuracy improvement by directly regularizing change in predicted likelihood for the ground truth. However, retention results vary across datasets, with the L1 criterion showing clear advantage for P-MNIST but not as much for S-MNIST or Sim-EMNIST. The hyperparameter \u03bb chosen for S-MNIST and Sim-EMNIST during the search is crucial for EWC's performance. A huge \u03bb leads to over-preservation of past classifier likelihood, affecting learning for the newest task. DM-III retains the most predictions empirically, with DM-p showing better retention than plain EWC for P-MNIST and S-MNIST. In this paper, a new criterion is proposed to mitigate catastrophic forgetting in connectionist networks during sequential learning tasks. The criterion involves direct minimization of the change in classifier likelihood and offers two ways to improve classifier performance: softregularizing the change in classifier likelihood and freezing influential weights. These methods outperform or perform similarly to existing L2 strategies. The study discussed the effectiveness of preserving classifier likelihood with respect to ground truth to maintain classifier performance. Future work includes comparing L1 strategies with non-L2 strategies like IMM and VCL for continual learning, and exploring direct minimization strategies on complex image classification datasets like CIFAR100 and ImageNet."
}