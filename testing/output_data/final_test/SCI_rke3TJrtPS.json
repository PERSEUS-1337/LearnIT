{
    "title": "rke3TJrtPS",
    "content": "In this paper, a new algorithm called Projection Based Constrained Policy Optimization (PCPO) is proposed for optimizing control policies while satisfying constraints. The algorithm involves an iterative two-step process where the policy is updated unconstrained in the first step and then reconciled with constraints in the second step through projection. Theoretical analysis shows a lower bound on reward improvement and an upper bound on constraint violation for each policy update. Empirical results demonstrate superior performance with significantly less constraint violation and higher rewards compared to other methods. Recent advances in deep reinforcement learning have shown impressive performance in various domains, but real-world applications like self-driving cars and unmanned aerial vehicles require agents to operate within specified constraints. A new algorithm called Projection Based Constrained Policy Optimization (PCPO) has been proposed to optimize control policies while satisfying constraints. This algorithm has demonstrated superior performance with significantly less constraint violation and higher rewards compared to other methods. In this work, the problem of learning control policies that optimize a reward function while satisfying predefined constraints is addressed. Policy updates are performed using conditional gradient descent with line search to ensure constraint satisfaction. Another approach adds weighted constraints to make optimization easier but requires a feasible base optimization problem. Projection based Constrained Policy Optimization (PCPO) is proposed as an iterative algorithm to address the issues of extensive hyperparameter tuning in optimization. PCPO performs policy updates in two stages: maximizing reward without constraints and then reconciling constraint violations by projecting the policy back onto the constraint set. This approach allows for efficient updates without violating constraints or requiring line search. Projection based Constrained Policy Optimization (PCPO) efficiently addresses constraint violations by projecting the policy back onto the constraint set. The algorithm provides performance bounds based on information geometry and policy optimization theory, ensuring tolerable constraint violations and reward degradation with small policy update step sizes. The choice of projection type impacts PCPO's convergence, with the Fisher information matrix's singular value influencing training. PCPO algorithm outperforms state-of-the-art algorithms in various control tasks, achieving more reward with fewer constraint violations. It frames policy learning as a Constrained Markov Decision Process (CMDP) to guide the agent in obtaining rewards while avoiding costs. The CMDP is defined by < S, A, T, R, C >, where S is states, A is actions, T is transition probability, R is reward function, and C is cost function. The policy \u03c0 selects actions to maximize cumulative discounted reward while keeping cumulative discounted cost below a threshold. The trajectory \u03c4 depends on policy \u03c0, with initial state distribution \u00b5. Kakade & Langford (2002) derived an identity relating policy performance to the advantage function. Learning constraint-satisfying policies is challenging due to non-smooth optimization landscapes. An algorithm is needed to make progress in maximizing reward while satisfying constraints. PCPO is a trust region method that updates policies for reward improvement and constraint satisfaction. Inspired by projected gradient descent, it consists of two steps for each policy update - a reward improvement step and a projection step. The PCPO method involves two steps for policy updates: optimizing a reward function with a KL divergence constraint and projecting the intermediate policy onto the constraint set using distance measures like L2 norm or KL divergence. This ensures both reward improvement and constraint satisfaction. Using KL divergence projection in the probability distribution space allows for provable guarantees in PCPO. Theorem 3.1 provides a lower bound on reward improvement and an upper bound on constraint violation for each policy update when the current policy satisfies the constraint. If the step size \u03b4 is small, the worst-case scenario is (1 \u2212 \u03b3) 2. Theorem 3.2 provides bounds on reward improvement and constraint violation for each policy update when the current policy violates the constraint in PCPO with KL divergence projection. It defines lower and upper bounds based on the gradient of the cost advantage function and the Hessian of the KL divergence constraint. Theorem 3.2 states that policy performance degradation increases with more constraint violation in PCPO. The proof involves non-expansive policy projection and bounding KL divergence. For large neural network policies, approximating updates with a small step size \u03b4 is computationally more feasible. In PCPO, the reward function and constraints are approximated with first and second order expansions. The algorithm involves linearizing the objective function and updating the policy parameter \u03b8. Projection can be done using L2 norm projection in the parameter space. In PCPO, the reward function and constraints are approximated with first and second order expansions. Projection can be done using L2 norm or KL divergence, with linearization of the cost constraint at \u03c0 k. Linear approximation of the constraint set is used for constraint satisfaction, solved using convex programming. The update for the projection step involves using L = I for L2 norm projection and L = H for KL divergence projection. However, inverting H for PCPO is impractical for large neural networks. The PCPO algorithm uses the conjugate gradient method for efficiency in inverting H. The update rule in PCPO with KL divergence and L2 norm projections affects reward improvement. L2 norm projection leads to more reward fluctuation due to not using the Fisher information matrix, while KL divergence projection can be unstable if the Fisher information matrix is ill-conditioned. The reward and cost updates in PCPO may be unstable due to pathological curvature. These projections converge to different stationary points with varying rates related to the Fisher information matrix. Assumptions are made to ensure the validity of the analysis, including minimizing a negative reward objective function over a closed and convex constraint set. The coefficient \u03b7 for reward updates is defined in terms of step size, gradient, Fisher information matrix, largest singular value, and gradient of cost constraint function. The largest singular value of matrix A and the gradient of the cost constraint function are key factors in the convergence of PCPO with KL divergence and L2 norm projection. The objective value changes based on the singular value of the Fisher information matrix, affecting the choice of projection type. The condition number of H is bounded by L2\u03b4, and adaptively choosing the projection type is guided by observing the Fisher information matrix. The gradient of the objective at a stationary point of PCPO with KL divergence projection belongs to -a. In Appendix G, a comparison is made between the optimization trajectories and stationary points of KL divergence and L2 norm projections in Policy Learning with Constraints. Learning constraint-satisfying policies in safe RL involves exploration of the environment or expert demonstrations. Our algorithm uses trial and error learning to overcome scalability issues with rule-based approaches in safe RL. To ensure constraint satisfaction, PCPO uses projection onto the constraint set during exploration. This approach has been explored in other contexts for general constrained optimization. PCPO is a new algorithm that ensures constraint satisfaction by projecting policy parameters onto the constraint set during exploration. It eliminates the need for hyperparameter tuning and provides provable guarantees on learning constraint-satisfying policies. Comparing to previous works like \u03b8-projection and CPO, PCPO offers a different approach to constrained policy optimization. PCPO is a new algorithm that ensures constraint satisfaction by projecting policy parameters onto the constraint set during exploration. It eliminates the need for hyperparameter tuning and provides provable guarantees on learning constraint-satisfying policies. In contrast, CPO simultaneously considers the trust region and the constraint, using line search to select a step size. However, when the current policy violates the constraint, CPO's update rule becomes infeasible. PCPO ensures a feasible solution, allowing the agent to improve the reward while ensuring constraint satisfaction simultaneously. In comparison to existing approaches, our method is evaluated on four control tasks with safety and fairness constraints. These tasks involve controlling traffic lights, autonomous vehicles, and moving in specified circles while staying within safe regions. Our method is evaluated on four control tasks with safety and fairness constraints, including traffic light and autonomous vehicle control. Baselines compared include Constrained Policy Optimization (CPO), Primal-dual Optimization (PDO), and Fixed-point Policy Optimization (FPO). Experimental Details: Different agents are tested in tasks such as gather, circle, grid, and bottleneck. The agents have varying state and action spaces. Neural networks with specific hidden layer sizes are used to represent Gaussian policies in the simulations. In experiments using different agents in tasks like gather, circle, grid, and bottleneck, neural networks with specific hidden layer sizes are employed to represent Gaussian policies. The step size is small, and the Fisher of reward improvement step is reused in the KL projection step to reduce computational costs. The experiments are conducted in rllab, and the overall performance is evaluated by learning curves of discounted reward and constraint values for all tested algorithms and tasks. The experiments evaluate different algorithms in various tasks using neural networks with specific hidden layer sizes to represent Gaussian policies. PCPO outperforms other algorithms by improving reward and constraint satisfaction efficiently. Constraints are better satisfied in easier tasks compared to more complex ones due to policy behavior complexity and non-convexity of the constraint set. Even with linear approximation, PCPO outperforms CPO by a significant margin. PCPO outperforms CPO in Grid and Bottleneck tasks with significantly less constraint violation. It achieves the same reward with less constraint violation in point circle and point gather tasks as well. PCPO enables cautious exploration under constraints, showing superiority over other algorithms. PCPO with L2 norm projection is more constraint-satisfying than PCPO with KL divergence projection. L2 norm projection leads to reward fluctuation in some tasks, while KL divergence projection results in more stable reward improvement across all tasks. The update direction of the constraint may deviate from the reward update direction, reducing reward improvement, especially in high-dimensional policy spaces. Ill-conditioned Fisher information matrices can hinder constraint satisfaction in tasks like ant circle, ant gather, grid, and bottleneck tasks. In PCPO, L2 norm projection leads to higher reward in ant circle task but lower reward in point gather task. PDO struggles to adjust constraint values quickly, hindering policy learning efficiency. FPO learns near constraint-satisfying policies with slightly better reward improvement than PDO but requires more engineering effort. PCPO optimizes for a reward function using policy projections to ensure constraint satisfaction, outperforming PDO and FPO. The algorithm achieves superior performance in reward improvement and constraint satisfaction. Future work will explore using Fisher information to iteratively choose the projection for policy updates. Projections for approximate policy iteration algorithms aim to robustly learn constraint-satisfying policies with reward improvement. The KL divergence projection is used to prove policy performance bounds, ensuring feasibility of the current policy. The derivation uses KL divergence projection to prove policy performance bounds for constraint-satisfying policies with reward improvement. The lower bound on reward improvement and upper bound on constraint violation for each policy update are (1 \u2212 \u03b3) 2, where \u03b4 is the step size. The current policy's performance bound is analyzed when it is infeasible due to constraint violation. By proving KL divergence between \u03c0 k and \u03c0 k+1, worst-case performance degradation is determined. If the current policy violates the constraint, the KL divergence constraint is defined with step size \u03b4. The gradient of the cost advantage function and Hessian of the KL divergence constraint are used in the analysis. The current policy's performance bound is analyzed when it is infeasible due to constraint violation. By proving KL divergence between \u03c0 k and \u03c0 k+1, worst-case performance degradation is determined. If the current policy violates the constraint, the KL divergence projection is used, with a lower bound on reward improvement and an upper bound on constraint violation for each policy update. The proof follows a similar structure as in Theorem A.2. Theorem C.1 discusses the PCPO problem, optimizing reward and projecting policy onto constraint set. The optimal solution assumes H is invertible. The primal problem is convex with quadratic inequality constraints, satisfying Slater's condition for strong duality. The Karush-Kuhn-Tucker (KKT) conditions are necessary and sufficient for optimality in the primal and dual problems. The Lagrangian is formed, leading to optimal solutions that satisfy specific equations. The analysis assumes the minimization of a negative reward objective function. The text discusses minimizing a negative reward objective function with specific constraints and assumptions. It introduces a lemma for projection characterization and mentions the optimal solution. The Fisher information matrix guarantees positive semi-definite. The text discusses the optimization problem with specific reward update coefficients and convergence theorems for PCPO with KL divergence and L2 norm projections. The proof is based on working in a Hilbert space and the non-expansive property of the projection. The text discusses proving the change of the objective value in stationary points for PCPO with KL divergence and L2 norm projections. The assumption of \u03c3 max (H) \u2264 1 is needed to justify the bound. The objective value for PCPO with KL divergence projection improves by defining the condition number and using Eq. (29). The text discusses using neural networks to implement a Gaussian policy in various tasks, specifying the network architecture and activation function. The GAE-\u03bb approach is utilized, and a learned model is not used to predict undesirable states as in CPO. The text discusses using neural networks to implement a Gaussian policy in various tasks, specifying the network architecture and activation function. It examines the performance of algorithms with different metrics, showing learning curves of cumulative constraint value and reward. PCPO with L2 norm projection has less constraint violation than KL divergence projection, suggesting issues with estimating the Fisher information matrix in high-dimensional policy space. PCPO also shows more reward improvement compared to CPO under the same constraint violation. The experiment compared the performance of PCPO and CPO in various tasks, showing that PCPO has more reward improvement than CPO under the same constraint violation. Line search in CPO is found to be more conservative in optimizing policies, leading to smaller steps and slower learning. To test the stability of PCPO and CPO in constraint-critical tasks, the constraint threshold was set to zero and the safe area reduced. PCPO showed better reward improvement and constraint satisfaction than CPO, even with a more difficult constraint. PCPO with L2 norm projection exhibited high constraint variance in the point circle task and converged to a suboptimal reward in the point gather task. In experiments with smaller training samples, PCPO shows more reward improvement and constraint satisfaction than CPO, despite increased reward and cost fluctuation. PCPO with KL divergence projection performs worse in reward than TRPO in the Grid task. TRPO outperforms PCPO with KL divergence projection in constraint satisfaction due to ill-conditioned Fisher information matrix causing inaccurate constraint updates. Increasing conjugate gradient method epochs can help improve constraint violation in PCPO. In experiments, the number of iterations in the conjugate gradient method was set to 10 to balance computational efficiency and accuracy across algorithms. The Fisher information matrix was found to be ill-conditioned, with more iterations leading to less error and better constraint satisfaction. Figure 11 illustrates the values of reward and constraint, condition number of Fisher information matrix, and approximation error of constraint update direction over training epochs with CG method's iterations of 10 and 20. More iterations lead to better constraint satisfaction. Fig. 13 shows update direction combining objective and cost constraint directions for both projections, indicating stationary points with g \u2208 -a at the boundary of constraint set. The projection has stationary points with g \u2208 \u2212a at the boundary of the constraint set. PCPO with KL divergence projection converges to a local optimum, while L2 norm projection converges to infinity. Both projections may converge to the same stationary point if the gradient direction of the objective is zero in the constraint set or boundary. The red star represents the initial point, the red arrows show the optimization paths, and the region below the black line is the constraint set. Both projections converge to different solutions."
}