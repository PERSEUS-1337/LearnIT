{
    "title": "r1lFYoRcFm",
    "content": "Learning from scalar rewards in continuous action space environments is challenging and often requires millions or billions of interactions. State aligned vector rewards are introduced to help deep reinforcement learning agents handle the curse of dimensionality. The agent learns to map action distributions to state change distributions using a quantile function neural network. A new reinforcement learning technique inspired by quantile regression is also introduced, allowing agents to learn faster in high dimensional state spaces compared to training with scalar rewards. The Markov Decision Process (MDP) is a model used to maximize cumulative rewards by choosing actions based on states and transition probabilities. Deep neural networks have enabled this formulation to handle high-dimensional spaces efficiently. Deep neural networks have allowed the Markov Decision Process to scale to high dimensional visual inputs, extending reinforcement learning to continuous action spaces. However, neural networks require large amounts of training data to converge, making it impractical for real-world interactions. Dopamine circuits in mammal brains are better modeled by viral vector strategies, and human reinforcement learning incorporates effector-specific value estimations. In this work, a deep reinforcement learning algorithm is improved by modeling a d-dimensional vector reward, aligning state and action spaces, and learning a mapping from action distribution to state change distribution. An example is given where a sensible reward to guide an agent to a goal is defined as r = ||p|| \u2212 ||p + a|| in the environment's vector space. In this example, the focus is on the L1 norm in the vector space, specifically when the agent tries to reach a goal by taking actions that move it closer in one direction but further in another. The scalar reward may not capture this distinction, highlighting the importance of using a vector reward to differentiate between the two dimensions. In this example, the importance of using a vector reward to differentiate between dimensions when the agent tries to reach a goal by taking actions that move it closer in one direction but further in another is highlighted. The reward should be dimension-wise aligned with the position of the agent, but the action space is often not state-aligned, requiring methods like training a deep neural network to approximate the quantile function. The paper introduces a new reinforcement learning method called quantile regression reinforcement learning (QRRL) that trains a deep neural network to approximate the quantile function for position change prediction. It also presents an architecture that learns a probability distribution over possible state changes based on a distribution over possible actions, allowing for faster training with more informative rewards. The paper introduces a new reinforcement learning algorithm for training stochastic continuous action policies with arbitrary action probability distributions. It discusses approximation techniques for the inverse cumulative distribution function and shows that a neural network can learn to approximate the quantile function. The algorithm minimizes the 1-Wasserstein distance of a scalar probability distribution to a uniform mixture of Diracs. The paper presents a new reinforcement learning algorithm for training stochastic continuous action policies with arbitrary action probability distributions. It discusses approximation techniques for the inverse cumulative distribution function and shows that a neural network can learn to approximate the quantile function. By minimizing the quantile divergence, the algorithm effectively models an approximate distribution implicitly in the network parameters. The PCPN does not impose constraints on the probability distribution Z, allowing for various shapes. It uses quantile regression loss to approximate position change quantile functions. The network takes an observation o and a Gaussian action distribution A as input to learn the conditional position change distribution. In this section, a new reinforcement learning objective based on quantile regression is established. The technique allows for a continuous action space and is applicable to reinforcement learning problems. The main idea behind Quantile Regression Reinforcement Learning (QRRL) is to model the action and policy in a distinct way, connecting with the rest of the paper in Section 4. Quantile Regression Reinforcement Learning (QRRL) models the policy implicitly in network parameters to allow for complex stochastic policies. Each action dimension is modeled with a quantile function, with the network output as the action. Training the network involves approximating quantile functions and maximizing expected rewards. Quantile regression is linked to the Wasserstein metric, aiding in training the network effectively. Quantile regression in reinforcement learning shapes probability mass towards good actions based on accumulated rewards. It uses advantage estimation and enforces monotonicity. The focus is on the effect of quantile regression loss on action probabilities between two network samples. In reinforcement learning, quantile regression approximates the optimal action distribution. The analysis focuses on single-dimensional action quantile functions. Training involves sampling actions and considering loss effects on action probabilities. Quantile regression loss is positively correlated to the slope of the quantile function. The quantile regression loss is inversely proportional to the probability of action. Minimizing the quantile loss increases the likelihood of action, while maximizing it decreases the likelihood. The QRRL actor objective involves monotonically increasing the quantile function with the advantage estimation. In experiments, QRRL is used to train the agent network AN \u03b7 through a constraint optimization approach. The additional loss term is weighted with a constant Lagrange multiplier, and QRRL can be adapted to off-policy settings. The State Aligned VEctor Reward (SAVER) agent utilizes QRRL to train the agent network through a Pretrained Critic Policy Network (PCPN). The PCPN is pre-trained with random observations and action probability distributions, and its weights are frozen during agent training. This setup allows for training multiple agents to solve different tasks in the same environment using the same PCPN. Training on vector rewards is straightforward as the output of the PCPN provides state-aligned actions. The SAVER agent estimates a value per action dimension and calculates a vector advantage at each timestep. Three experiments were conducted to test the approach, including modeling a robot arm. The code is publicly available for reproducibility. The SAVER agent is compared against an A2C agent. In a comparison between SAVER and A2C agents, the SAVER implementation focuses on using vector rewards instead of scalar rewards. The experiment involves training on higher dimensional environments, showing the benefits of vector rewards. The neural architecture and hyperparameters are fixed based on SAVER and A2C performance. In a first experiment, the agent moves freely in a d-dimensional hypercube using (angle, step size) pairs for actions. Step sizes are re-scaled and episodes end after 1,000 steps. The PCPN is pretrained with 100,000 batches of 128 transitions. A small hyperparameter search was done for the learning rate, settling on 0.01 for SAVER and 0.0003 for A2C. The performance of agents is measured by the mean length of the last 100 episodes. SAVER trains faster in high dimensional cubes and can find the goal in a 16-dimensional cube within 1,000 steps. Changing the action representation involves a softmax distribution for choosing dimensions and a scalar Gaussian distribution for network outputs. The PCPN model uses a softmax distribution for choosing dimensions and a scalar Gaussian distribution for network outputs to learn a complex position change distribution. Training involves pretraining with 10,000 batches of 128 transitions each, with the x-axis showing the number of training steps and the y-axis showing the average episode length. The experiment involved searching for appropriate learning rates in a 4-dimensional hypercube for SAVER and A2C. The mean episode lengths during agent training were plotted in Figures 4 and 5, showing the advantage of training with vector rewards in higher dimensional hypercubes. The PCPN was pretrained with 10,000 batches of 128 transitions each, with the corresponding mean episode lengths plotted in Figure 5. Even in low-dimensional problems, SAVER trains faster than A2C due to more informative vector rewards. Using vector rewards is common in Multi-Objective Reinforcement Learning literature. Recent work applies these techniques to deep reinforcement learning agents, while we directly use the vector reward as a training signal for a neural network to balance different objectives. Our work focuses on a neural network that learns to balance different objectives using a multi-dimensional reward. Previous studies have explored multiple problem-aligned rewards and the impact of correlated rewards on learning performance. While these approaches also show improved training performance, they require hand-engineered reward functions. Our approach leverages state spaces as metric spaces, allowing for a straightforward vector reward interpretation. This makes it easier to apply to a wide range of tasks compared to methods requiring hand-engineered reward functions. We capitalize on state-aligned vector rewards, a novel approach not seen in deep learning algorithms. Our PCPN incorporates the full probability distribution of possible state changes, including non-Gaussian distributions. Unlike BID5 and BID9, our approach does not constrain the action distribution to an explicitly parameterized distribution, allowing for richer policies. Quantile networks can be used for generative modeling and show potential for future work. A new reinforcement learning technique called QRRL is introduced, allowing for complex stochastic policies in continuous action spaces. The SAVER agent, trained through a quantile network, shows faster training in high dimensional metric spaces. SAVER has potential in mathematics and related fields, using a fully connected network for reinforcement learning. The network maps hidden representations to action mean and standard deviation, as well as scalar value estimates. For PCPN, mappings are done from input to hidden representations, then to multiple hidden representations based on state dimensions. The QRRL critic is implemented as a separate network with hidden representations of varying sizes. The architecture and hyperparameters are kept fixed for all experiments, with the only adjustment being the learning rate."
}