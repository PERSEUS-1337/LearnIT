{
    "title": "HJJ23bW0b",
    "content": "Learning to predict complex time-series data is a fundamental challenge in various fields like Machine Learning, Robotics, and Natural Language Processing. Predictive State Recurrent Neural Networks (PSRNNs) are a cutting-edge method for modeling time-series data by combining probabilistic filters and Recurrent Neural Networks. PSRNNs use Hilbert Space Embeddings of distributions to embed predictive states into a Reproducing Kernel Hilbert Space and update these states using Kernel Bayes Rule. Practical implementations of PSRNNs rely on Random Features to map input features into a new space for efficient kernel approximation. However, PSRNNs often require a large number of Random Features, leading to slow model execution and training. In this paper, the theory of Orthogonal Random Features (ORFs) is extended to Kernel Ridge Regression to create Orthogonal PSRNNs (OPSRNNs). OPSRNNs are smaller and faster than PSRNNs, outperform LSTMs, and achieve similar accuracy with fewer features. This advancement addresses the challenge of predicting temporal sequences in various fields like machine learning and robotics. Recurrent neural networks (RNNs) are the leading approach for modeling time series data in various disciplines. Predictive State Recurrent Neural Networks (PSRNNs) combine probabilistic models and RNNs, offering strong statistical theory and outperforming other models. PSRNNs have shown significant performance improvements despite their simple bi-linear operations. Predictive State Recurrent Neural Networks (PSRNNs) outperform complex RNN architectures like LSTMs and GRUs by leveraging Hilbert Space embeddings and Kernel Bayes Rule. They manipulate distributions over observations and can be initialized using a method-of-moments algorithm. Practical implementations rely on Random Features for success, although a significant number of RFs may be required. Orthogonal Random Features (ORFs) have been effective in reducing the number of required Random Features (RFs) for kernel machines, resulting in decreased kernel approximation error. ORFs guarantee an improvement in pointwise kernel approximation error compared to RFs, but these guarantees are not directly applicable to Predictive State Recurrent Neural Networks (PSRNNs). PSRNNs first obtain model parameters via ridge regression before using them for calculations. In this work, Orthogonal Random Features (ORFs) are used to initialize OPSRNNs, which are smaller and faster than conventional PSRNNs. The orthogonal version of the KRR algorithm leads to better spectral properties and smaller upper bounds on failure probabilities. OPSRNNs are compared with LSTMs and conventional PSRNNs on robotics tasks, showing improved performance. Orthogonal Random Features (ORFs) are utilized to initialize OPSRNNs, which outperform LSTMs and conventional PSRNNs on robotics tasks. OPSRRNs demonstrate superior accuracy with fewer features compared to PSRNNs. Previous structured constructions focused on computational gains, while ORFs aim to enhance estimator accuracy. The theoretical and experimental verification supports the effectiveness of ORFs for pointwise kernel approximation. Theoretical guarantees for pointwise kernel approximation were experimentally verified, but only for specific kernels. Previous methods lacked strong concentration results with exponentially small probabilities of failure, which this paper aims to provide. This paper introduces the use of orthogonal random features in kernel ridge regression for recurrent neural networks, a novel approach not covered in existing literature on orthogonal recurrent neural networks. Previous research has focused on enforcing orthogonality or using random orthogonal initialization in RNN architectures to address gradient vanishing or exploding issues. While related, these approaches lack theoretical guarantees on the superiority of the orthogonal method. Orthogonality is a key factor in disentangling deep representations, particularly in the context of predictive state recurrent neural networks (PSRNNs). PSRNNs combine predictive state ideas with random features, allowing for initialization via Bayes Rule instead of random initialization. This approach offers theoretical guarantees on the effectiveness of orthogonality in RNN architectures. Predictive state recurrent neural networks (PSRNNs) utilize random features (RFs) to embed a predictive state in a Reproducing Kernel Hilbert Space (RKHS). PSRNNs focus on expectations of future observations, contrasting with latent states that deal with unobserved quantities. The use of RFs with translation invariant kernels in PSRNNs allows for effective initialization and disentanglement of deep representations. Predictive state recurrent neural networks (PSRNNs) use random features (RFs) to embed a predictive state in a Reproducing Kernel Hilbert Space (RKHS). PSRNNs focus on future observations' expectations, contrasting with latent states dealing with unobserved quantities. PSRNNs can be initialized using the Two Stage Regression (2SR) approach, which involves solving three Kernel Ridge Regression problems in two stages to learn the initial state q1 and update tensor W. Ridge regression is necessary for model stability and statistical consistency. The 2SR algorithm uses random features to embed predictive states in a RKHS. It involves solving Kernel Ridge Regression problems to learn initial state q1 and update tensor W. Ridge regression ensures model stability and consistency. The algorithm is provably consistent in the realizable setting, but scalable implementations use approximate kernel values with RFs. Orthogonal Random Features (ORFs) are introduced to obtain smaller, faster models compared to Random Features (RFs) in practice. ORFs can be used in PSRNNs by extending theoretical guarantees to the kernel ridge regression setting. ORFs are constructed to approximate values of kernels defined by radial basis functions, such as Gaussian kernels, for OPSRNN models. K is shift-invariant and can be described by an integral using Bochner's Theorem. Common RBF kernels with corresponding functions and probability density functions are listed. The standard unbiased MonteCarlo estimator of K(x, y) is given, using a random embedding \u03a6 m,n. Regression models can be trained to predict quantities from the state. Random features provide an equivalent description of the original kernel via a linear kernel in a new dataset. Orthogonal random features are obtained by sampling vectors from a joint distribution satisfying an orthogonality condition. Orthogonal random features are generated by sampling vectors from a joint distribution that satisfies an orthogonality condition. The Gram-Schmidt orthogonalization is used to define the directions of the vectors, leading to a continuous-orthogonal joint distribution. The isotropicity of the distributions ensures that the kernel estimator remains unbiased for RBF kernels. Strong theoretical guarantees support the use of orthogonal estimators in kernel ridge regression models, outperforming those based on independent vectors. Another approach involves discrete matrices, with a random diagonal matrix denoted as D. A discrete approach using random diagonal matrix D and Hadamard matrix H leads to efficient computation of random feature maps, with the advantage of time and space efficiency. The product of orthogonal matrices in G_HAD ensures orthogonal rows, allowing for accurate computations. In this paper, discrete-orthogonal distributions are shown to provide accurate PSRNN models, outperforming approaches with unstructured random features. The bias in estimators decreases with dimensionality, but the lower variance of kernel estimators compensates for this. Theoretical analysis of discrete-orthogonal joint distributions in the RNN context is left for future work. Theoretical guarantees for the initialization phase of OPSRNN are extended, focusing on kernel ridge regression with orthogonal random matrices. Kernel ridge regression with orthogonal random features provides better spectral approximation of the kernel matrix than unstructured random features, leading to smaller empirical risk. The results also offer exponentially small bounds on the probability of failure for random orthogonal transforms. The model aims to learn a function f* from labeled datapoints (x_i, y_i), where y_i = f*(x_i) + \u03bd_i, and \u03bd_i are independent Gaussians. The empirical risk of the estimator f is defined. The empirical risk of the kernel ridge regression estimator f is defined using a formula involving dataset size N, regularization parameter \u03bb, and identity matrix I. Bounds on the empirical risk are derived using a modified version of the formula for the estimator based on random feature map mechanism. The modified formula for the kernel ridge regression estimator f KRR is used to measure spectral similarity with random feature maps. The risk of the estimator can be upper-bounded in terms of the \u2206 parameter for a \u2206-spectral approximation. Smooth RBF kernels, including Gaussian kernels, are considered. The class of smooth RBF kernels, including Gaussian kernels, is defined by a fixed function \u03c6 : R \u2192 R and a sequence of probabilistic measures. The k th moments of random variables X n = w satisfy certain conditions, leading to tighter bounds on spectral approximation. Theorem 1 establishes bounds on \u2206, resulting in tighter upper bounds on the empirical risk of the estimator. The setting assumes structured blocks with a fixed number of rows, but experiments suggest validity without this assumption. Theorem 2 discusses spectral approximation for smooth RBF kernels, where \u2206 iid is the smallest positive number for a \u2206-approximation of the kernel matrix. Theorem 1 provides bounds on \u2206, leading to tighter upper bounds on the empirical risk. Theorem 2 discusses spectral approximation for smooth RBF kernels using random unstructured and random orthogonal features, resulting in risk bounds for kernel ridge regression. In practice, gains are seen with orthogonal random features compared to unstructured ones, leading to smaller upper bounds on failure probability. Theoretical guarantees for orthogonal random features are extended to the initialization phase of OPSRNNs, showing better performance in robotics time-series datasets. In robotics time-series datasets, OPSRNN models with continuous-orthogonal and discrete-orthogonal joint sampling gave similar results. The datasets used include a 3-link simulated swimmer robot and a Human Motion Capture dataset with 48 skeletal tracks from three human subjects walking. All models were implemented using Tensorflow in Python. The curr_chunk discusses different datasets used for training and testing models, including tracks from human subjects walking, a handwriting database, and Moving MNIST videos. The datasets vary in features and characteristics, such as 3D positions of skeletal parts, tablet coordinates, pressure levels, and pixel images. The data is split for training and testing purposes. The curr_chunk focuses on model optimization and evaluation using a two-stage regression approach with history features, ridge-regression parameter, kernel width, learning rate, and PSRNN. Models are assessed based on Mean Squared Error for one-step predictions. In the first experiment, the effectiveness of Orthogonal RF in learning a good PSRNN via 2SR is compared with Standard RF. Results show that Orthogonal RF outperforms Standard RF when the ratio of RF to input dimension is small, but both approaches yield similar results for large RF to input ratios. In the second experiment, the effectiveness of Orthogonal RF in learning a good PSRNN via 2SR initialization combined with refinement via BPTT is evaluated. The MSE decreases as the number of epochs increases for both Orthogonal RF and Standard RF in PSRNN learning. However, Orthogonal RF consistently achieves better MSE results than Standard RF across all datasets, demonstrating its effectiveness in improving downstream applications. Additionally, Orthogonal RF offers significant performance improvements for kernel ridge regression and serves as a better initialization for the BPTT gradient descent procedure, leading to a superior final model convergence. The experiments compared the performance of PSRNNs, OPSRNNs, and LSTMs. OPSRNNs outperformed LSTMs on all datasets, achieving superior performance and significant compression. The use of structured orthogonal constructions with recurrent neural networks led to models with better performance than baselines. Theoretical guarantees were provided for orthogonal random features in kernel ridge regression models, an important component in RNN-based architectures for state prediction. In this paper, theoretical guarantees were provided for RNN-based architectures using orthogonal random features in kernel ridge regression models. The experiments showed that OPSRNNs outperformed LSTMs on all datasets, achieving superior performance and significant compression. The study considered randomized kernel estimators and random feature maps, proving bounds that are better than standard nonorthogonal mechanisms. The proof involves showing that K + \u03bbI N is a \u2206-spectral approximation of K + \u03bbI N by utilizing basic properties of spectral and Frobenius norms. The goal is to compute function g from Lemma 1 for random feature maps constructed using standard independent sampling and orthogonal procedures. The text discusses random feature map mechanisms for the RBF kernel, focusing on independent sampling and orthogonal procedures. It highlights the equivalence of the random feature map approximation of the RBF kernel and provides a formula for the function g. The text discusses the proof of Theorem 2 for the continuous-orthogonal distribution in the context of random feature map mechanisms for the RBF kernel. It focuses on independent sampling and orthogonal procedures, highlighting the equivalence of the random feature map approximation of the RBF kernel. The text focuses on proving an inequality for one block, assuming m \u2264 n. Using Taylor expansion for e^x, the goal is to show a specific formula. By applying a trigonometric formula, a vector-addition and vector-subtraction operator are used. It is noted that certain configurations can be assumed without loss of generality. The text discusses the distribution of independent random variables and derives a formula using Taylor expansion. It also presents a technical lemma for calculating a specific expression. The text discusses the distribution of independent random variables and derives a formula using Taylor expansion. It also presents a technical lemma for calculating a specific expression involving max i1,j1,...,i l ,j l E[|v i1 v j1 | \u00b7 ... \u00b7 |v i l v j l |]. The text presents a technical lemma for calculating the upper bound on the expected value of a specific expression involving independent random variables. It utilizes isotropicity of Gaussian vectors and union bound to derive the bound with a probability guarantee. Lemma 5 provides a conclusion for large n, where B satisfies B = Ak and A is given."
}