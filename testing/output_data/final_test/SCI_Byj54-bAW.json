{
    "title": "Byj54-bAW",
    "content": "Several state of the art convolutional networks, including DenseNet, interconnect different layers to improve information flow and gradient. DenseNet has shown superior performance in image recognition tasks, but its theoretical understanding is limited. This work analyzes the impact of layer interconnection on a network's expressive power, comparing DenseNet connections with other types of inter-layer connectivity through tensor analysis on convolutional arithmetic circuits. The analysis explores the impact of layer interconnections on convolutional arithmetic circuits (ConvACs) and their relation to standard convolutional networks. DenseNet, along with other densely connected networks like FractalNet and ResNet, have achieved state-of-the-art performance by adding dense connections between layers to improve information flow and gradient. DenseNets have excelled in tasks such as CIFAR-10, CIFAR-100, SVHN, and ImageNet. DenseNets have achieved top performance on various datasets like CIFAR-10, CIFAR-100, SVHN, and ImageNet with models up to 1 thousand layers deep. The effectiveness of DenseNet connections compared to other models like ResNets is still under investigation, with theories on their superior performance yet to be fully understood. ConvACs, a type of product pooling layers, have been shown to have greater expressive power than rectifier based models. Empirical evidence of ConvACs' relevance was demonstrated through SimNets architecture. BID11's generative ConvAC achieved state-of-the-art performance in image classification with missing pixels. Various works have studied different aspects of ConvACs from a theoretical perspective, including the inductive bias introduced by pooling geometries and the use of quantum entanglement measure for analysis. BID9 utilizes quantum entanglement measure to analyze inductive bias from correlations in ConvAC channels. BID10 enhances ConvACs by allowing overlapping receptive fields, improving expressive capacity. Inter-layer connectivity in ConvACs for sequential data processing was explored, different from ResNet, FractalNet, and DenseNet. The tensor analysis framework was extended to study the effect of dense connections. The study focuses on the effect of dense connections in deep ConvACs like DenseNets, FractalNet, and ResNet. It explores the expressive capabilities and provides performance bounds and guidelines for hyperparameter selection. The paper introduces notation and concepts from tensor algebra, presents tensor representations of ConvACs, and extends the analysis to densely connected networks. In Section 4, tensor representations for densely connected ConvACs are introduced. Performance bounds and design guidelines for these networks are derived in Section 5. The term tensor refers to a multi-dimensional array, with the order corresponding to the number of indexes needed to access an entry. A vector is a tensor of order 1, while a matrix is a tensor of order 2. The notation [I] represents the set {1, 2, ..., I}. The tensor product \u2297 is a crucial operator in tensor analysis. The tensor product \u2297 is essential in tensor analysis for defining the rank of a tensor. A tensor A can be expressed as a sum of rank-1 tensors, and the rank of the tensor is defined as the minimum number Z that satisfies a certain condition, equivalent to the CP decomposition. The matricization operator [A] is also important in tensor algebra. The matricization operator BID9 is used to re-order elements of a tensor into a matrix. A convolutional neural network called ConvAC utilizes linear activation functions and product pooling. The input of the network is modeled by X, where features are selected from a parametric family F. The network evaluates score functions to determine if an input belongs to a class in set Y. The network evaluates score functions to determine class membership in set Y. Score functions are expressed as homogeneous polynomials with coefficients stored in grid-tensors. For shallow ConvAC with 1x1 convolutions, score functions are computed from weight vectors. The tensor analysis framework involves obtaining an expression in terms of network parameters. The framework involves obtaining an expression for the grid-tensor A y representing the network architecture. The rank of A y is bounded by Z, and results were generalized for a deep ConvAC with size-2 pooling windows. The grid-tensor is given by a hierarchical tensor decomposition with weights in hidden layers. Dense connectivity in convolutional neural networks involves using a growth rate k to connect previous layers to the forthcoming layer through concatenation. Dense blocks are created to group layers with compatible spatial dimensions, as proposed by BID7. These blocks do not include operations like pooling that alter input dimensions. In DenseNet architecture, transition layers serve as a link between dense blocks and perform pooling operations. Transition layers in the original DenseNet included a convolution layer before pooling, but this work considers transition layers with only pooling operations. This modification does not impact the model's generality. In ConvACs, dense blocks of size greater than 1 can be represented as size 1 blocks due to linear activation function. Dense connections within a block limit growth rate to k = 1, but broader connectivity is achieved by allowing connections between blocks. Concatenation of outputs from different blocks is possible with proper pooling. In ConvACs, dense blocks are represented as size 1 blocks due to linear activation function, limiting growth rate to k = 1. Connections between blocks allow broader connectivity. The network's function of a densely connected shallow ConvAC corresponds to a grid tensor. The rank of this tensor is bounded by rank(A y) \u2264 Z + M. Adding dense connections increases the number of parameters in the network. Adding dense connections in a L-layered dense arithmetic circuit increases the network's parameters and width of hidden layers. Inter block connections virtually increase the width from r l to r l + r l\u22121 for all layers. The study explores whether increasing network width via dense connections enhances its expressive power. Proper pooling of features is necessary before broader connectivity via dense inter-block connections. Proper pooling of features is essential before connecting them in dense inter-block connections. Three possible ways of realizing such connections are proposed: via product, average, or max pooling. Inter block connections increase the network width, and spatial size reduction is required before concatenating features in the current layer. Pooling with window size L jump w pool is used for spatial size reduction. The type of pooling (product, average, or maximum) affects the connectivity. Adding inter block connections via average pooling does not change the grid tensor A y, but adds extra polynomial terms of degree less than N to the network function of a ConvAC. This result holds true even when connections are made by addition, as seen in ResNet and FractalNet. The number of polynomial terms a ConvAC can realize can be computed by expanding polynomial products of every layer via binomial expansions. This analysis is left for future contributions. Product pooling connections lead to intra-block connections of polynomial terms of the same order, increasing the network's expressiveness. The text discusses the generalization of intra-block connections in a ConvAC network with exponential width decay. It introduces the concept of growth-rate and defines a ConvAC with specific parameters. The analysis of inter-block connections via maximum pooling is left for future work. The (L, r, \u03bb, k) ConvAC is defined to have weak dense gain G w if score functions cannot be realized by a (L, r, \u03bb, 0) ConvAC with r < G w r. A strong dense gain G s is achieved when p = 1. A bound for weak dense gain G w is presented, serving as a guideline for tailoring M and widths r 0 , . . . , r L\u22121 to exploit expressiveness. The expressive gain provided by dense inter-block connections can be quantified, with considerations on including broader dense connections. Connections like those in ResNet and FractalNet may be more beneficial as they do not increase model size but enhance trainability. Theorems show potential for strong dense gain within certain bounds, but it is uncertain if this holds true outside of that range. The impact of additional parameters from dense connections is addressed in Proposition 3. The ratio between the number of parameters added by dense connections and standard connections in a ConvAC is crucial for deciding whether to add dense connections to a model. The factor G directly relates to the dense gain of a ConvAC, guiding practitioners on which connections to add. Lemma 1 states that for a random tensor A, if Z \u2264 M P/2, the rank of A is equal to Z with probability 1. Lemma 1 states that for a random tensor A, if Z \u2264 M P/2, the rank of A is equal to Z with probability 1. Using the definition of the matricization operator, it is shown that the rank of the matricization [A] is always less than or equal to Z. By permuting the rows and columns of [A], the rank of the resulting matrix U remains the same as [A]. If rank(P) = Z, then rank(U) \u2265 Z, leading to the conclusion that rank(U) is greater than or equal to Z. The proof involves defining a mapping from x to P, showing that det P(x) is not the zero-polynomial, and concluding that det P(x) = 0 for some x. The lemma is proven by showing that det P(x) is not the zero-polynomial, and then demonstrating that det P(x) = 0 with probability 1. Random tensors A and B of even order P \u2265 2 are considered, along with tensors Z 1 and Z 2 drawn from a continuous distribution. The rank of the tensor C = A \u2297 B is shown to be Z 1 Z 2 with probability 1 using properties of the Kronecker product. The lemma is proven by showing that det P(x) is not the zero-polynomial, and then demonstrating that det P(x) = 0 with probability 1. Random tensors A and B of even order P \u2265 2 are considered, along with tensors Z 1 and Z 2 drawn from a continuous distribution. The rank of the tensor C = A \u2297 B is shown to be Z 1 Z 2 with probability 1 using properties of the Kronecker product. In the proof, tensors of order P > 2 are reformulated to have the same form as a grid tensor, and the output of a ConvAC can be stored into vectors of mappings. The weight vectors for the convolution of the l-th layer are denoted as a l,j,\u03b3. The ConvAC outputs for layer l are calculated recursively up to the L-th layer, resulting in score functions. Dense connections via average pooling from preceding layers are considered, with additional coefficients needed for convolutions. The outputs of the L-th layer of this ConvAC are denoted as vectors \u03b4. The entries of \u03c9 l,j (x) come from preceding layers with average pooling, resulting in polynomials with degree no greater than 2 l + 2 l\u22121. This is lower than the degree of the entries of \u03b4 l,j (x). By expressing a l,j,\u03b3 , \u03b4 l,j + \u03c9 l,j as a formula, we can recursively apply this up to the L-th layer, leading to g(x) containing polynomial terms of x of order less than N. The theorem is proven by showing that the rank of the grid tensor is bounded by terms of x of order less than N. This result applies to additive and residual connections in neural networks like ResNet and FractalNet. The assumption r0 \u2264 M is made for notation purposes and does not affect the generality of the results. The rank of the grid tensor is shown to be bounded by min{r0, M} with probability 1 when weights are independently generated from a continuous distribution. This assumption does not impact the generality of the results for r0 values greater than M. The rank of the grid tensor is bounded by terms of x of order less than N, applying to additive and residual connections in neural networks like ResNet and FractalNet. The assumption r0 \u2264 M is made for notation purposes and does not affect the generality of the results. The grid tensor rank is bounded by min{r0, M} with probability 1 when weights are independently generated from a continuous distribution, impacting results for r0 values greater than M. By induction up to the L-th layer, a bound for the grid tensor rank is obtained with hidden layer widths decaying at an exponential rate of \u03bb \u2208 R. The obtained bound is simplified, leading to the proof of Theorem 5.1. The maximum dense gain value Gw is achieved when rank(Ay dense) reaches its maximum possible rank, which corresponds to rank(Ay stand). This is proven in Theorem 5.1. For Theorem 5.2, a core tensor is derived from the hierarchical tensor decomposition with a virtually increased width. The maximum dense gain Gw is obtained when rank(Ay dense) reaches its maximum possible rank. The maximum dense gain value Gw is achieved when rank(Ay dense) reaches its maximum possible rank, proven in Theorem 5.2 using expressions to compute the ratio of interest."
}