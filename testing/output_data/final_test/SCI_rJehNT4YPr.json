{
    "title": "rJehNT4YPr",
    "content": "The learning of hierarchical representations for image classification has seen success with large-scale labeled data. However, evaluating classifiers on sparse test images raises concerns about generalization to real-world images. Adversarial learning complicates performance comparisons. A new framework, the MAximum Discrepancy (MAD) competition, efficiently compares image classifiers without fixed test sets. The MAximum Discrepancy (MAD) competition adaptsively samples a test set from unlabeled images to maximize discrepancies between classifiers. Human labeling on small image sets reveals relative performance and insights for improvement. Results of eleven ImageNet classifiers are reported, with potential for future additions. Large-scale human-labeled datasets like ImageNet have advanced research in image classification with novel network architectures and optimization algorithms. The conventional model evaluation methodology for image classification involves pre-selecting a test set of natural images, collecting human labels for each image, and ranking classifiers based on accuracy. However, this approach faces challenges due to the vast size and complexity of the natural image manifold. The limited scale of affordable testing poses a challenge due to the vast size and complexity of the natural image manifold. Current test sets may not be sufficient to represent hard natural images encountered in the real world, leading to a drop in accuracy with even a minute distribution shift. The conventional model comparison methodology faces challenges with fixed test sets, leading to potential overfitting and limited generalization to real-world natural images with diverse content variations. Adversarial images designed to mislead one classifier can also fool others, complicating performance comparisons among classifiers. In order to address the limitations of existing classifiers in image classification, a larger test set of millions or billions of images is proposed. The challenge lies in efficiently utilizing such a large-scale test set due to constraints in human labeling. The MAximum Discrepancy (MAD) competition methodology is introduced to overcome this challenge by starting with a large unlabeled image set and identifying images that cause strong disagreements among classifiers. The MAximum Discrepancy (MAD) competition methodology aims to improve image classification by selecting a set of images that cause strong disagreements among classifiers. A weighted distance over WordNet hierarchy is used to quantify the discrepancy between classifiers on each image, leading to the selection of model-dependent images that are most informative. Subjective experiments on the MAD test set reveal the strengths and weaknesses of classifiers, helping to improve generalizability to natural image manifold. The MAD competition methodology aims to improve image classification by selecting images that cause disagreements among classifiers. It verifies improvements achieved by recent DNN-based methods with minimal testing budget and is extensible to future classifiers. MAD can be applied beyond image classification to research fields with discrete-valued outputs and enormous sample spaces. The general problem of model comparison in image classification is formulated based on the natural image manifold. The goal is to compare the performance of multiple image classifiers with limited resources for subjective testing. A conventional model comparison method involves sampling a set of natural images and asking human annotators to provide ground-truth labels, which is time-consuming and expensive. The MAD competition methodology aims to efficiently falsify classifiers by sampling an image set from the natural image manifold. It allows for comparing classifier performance without the need for extensive human labeling, making it a cost-effective alternative. The MAD competition methodology aims to efficiently falsify classifiers by sampling an image set from the natural image manifold. It allows for comparing classifier performance without extensive human labeling, making it a cost-effective alternative. In contrast, the proposed approach leverages semantic hierarchies in WordNet to measure the distance between predicted class labels, considering semantic relations crucial in distinguishing classifiers with similar design philosophies. The text discusses how semantic hierarchies in WordNet are used to measure the distance between class labels, highlighting the importance of semantic relations in distinguishing classifiers with similar design philosophies. The weighted distance between nodes in the hierarchy reflects semantic similarity, with shorter distances indicating closer semantic relationships. The text discusses designing weighted distances in semantic hierarchies to encourage differences between classifiers. By assigning smaller distances to paths with lower tree depth levels, the aim is to maximize discrepancies between classifiers. This approach aims to discriminate between class labels effectively. The text discusses cases where classifiers make correct or incorrect predictions, highlighting the importance of selecting images that provide strong evidence in ranking their relative performance. By restricting images to contain only a single salient object, the possibility of both classifiers making correct predictions is reduced. MAD automatically identifies failure cases to differentiate between classifiers. In a multiclass image classification problem, two classifiers make incorrect predictions, revealing their weaknesses and potential for improvement. To compare their performance, top-k images are selected to form a test subset, resulting in a final test set. The number of images in the test set is independent of the image set size, ensuring no impact on human labeling costs. In scenarios where computational prediction cost is negligible, MAD suggests expanding the image set to include more natural images. A subjective assessment environment is used to collect human labels for images associated with two classifiers. When both classifiers fail to identify an image correctly, human labeling is stopped. Top-k images with the largest distances are selected to form a subset for testing classifier performance. After selecting top-k images with the largest distances, human labels are sourced for the subset to test classifier performance. Pairwise dominance matrix is updated and a global ranking vector is computed using empirical classification accuracies. Pairwise statistics are aggregated into a global ranking by comparing classifiers in pairs. Laplace smoothing is employed when k is small to smooth the estimation. The MAD competition involves creating a pairwise dominance matrix and aggregating results into a global ranking using Perron rank. Additional classifiers can be easily added without changing the workflow. In the MAD competition methodology, classifiers are added by enlarging the matrix and inserting pairwise comparison statistics. The global ranking vector is updated accordingly. The focus is on comparing ImageNet classifiers using a dataset of 168,000 natural images from Flickr. The dataset for the MAD competition methodology consists of 168,000 natural images from Flickr, collected after 2013 to ensure content independence from ImageNet. No data cleaning is required at this stage, and the size of the dataset is chosen to be three times larger than the ImageNet validation set. Comparing ResNet34 and ResNet101 on ImageNet validation set for generalizability. Also, evaluating WRN101-2, ResNeXt101-32\u00d74, SE-ResNet-101, WSL-ResNeXt101-32\u00d748, and EfficientNet-B7 classifiers. Using maximum discrepancy principle for constructing S with prediction confidence constraint. The confidence constraint is applied to filter out candidate images that do not meet a certain threshold, ensuring they are informative for improving decision rules. Images misclassified with high confidence are considered hard counterexamples. Class diversity is encouraged by limiting the number of images with the same predicted label, while non-natural images are excluded. In the context of filtering candidate images based on a confidence constraint and excluding non-natural images, human annotators label images in a subjective experiment. An image is discarded if more than three annotators find it difficult to label, and a majority vote is used for final labeling. The annotated images are categorized into Case I, Case II, and Case III, with Case II being the most prevalent at 53.5%. In the current MAD competition, 32.9% of images are related to Case I and 13.6% to Case III. The pairwise accuracy matrix A shows that MAD-selected images are visually more challenging with various distortions. An interesting finding is that when two classifiers perform similarly on a subset, they tend to make different incorrect predictions on selected images. In the MAD competition, images in Case III show different model biases between classifiers. WSL-ResNeXt101-32\u00d748 focuses on foreground objects, while EfficientNet-B7 attends to background objects. Common failure modes include reliance on relation inference, bias towards low-level visual features, and difficulty in recognizing rare object instantiations. Global ranking results by MAD show steady progress in image classification. The MAD competition revealed interesting findings, with VGG16BN outperforming ResNet34 and ResNet101. Networks with the squeeze-and-extraction mechanism moved up in the ranking, showing the benefits of modeling dependencies between feature maps. Notable models in the ImageNet ranking include WSL-ResNeXt101-32\u00d748, EfficientNet-B7, PNASNet-5-Large, and NASNet-A-Large. In the MAD competition, NASNet-A-Large ranks high despite using a global search strategy, while PNASNet-5-Large drops in ranking due to its progressive cell-wise search strategy. WSL-ResNeXt101-32\u00d748 and EfficientNet-B7 remain the top performers, confirming their effectiveness in the competition. The MAD competition evaluated the effectiveness of large-scale hashtag data pretraining and compound scaling method in image classification. An ablation study on the key hyperparameter k showed stable ranking results when k > 15, supporting the choice of k = 30. The methodology presented compared image classification models and mitigated conflicts between natural image manifold evaluation and human labeling effort. The MAD competition focuses on selecting natural images to distinguish classifiers, exposing flaws, enhancing model interpretability, and analyzing focus and bias in predictions. It is effective for large sample spaces and costly ground-truth labels, applicable to models with discrete outputs like medical and hyperspectral image classification. The MAD competition is useful for selecting natural images to distinguish classifiers, exposing flaws, enhancing model interpretability, and analyzing focus and bias in predictions. It is applicable to models with discrete outputs like medical and hyperspectral image classification, as well as spotting rare failures in high-cost applications such as autonomous cars. MAD has limitations and should be viewed as complementary to conventional accuracy comparison for image classification. The MAD competition is a method for selecting natural images to distinguish classifiers and enhance model interpretability. It is complementary to conventional accuracy comparison for image classification. Future research directions include marrying the MAD competition with Bayesian probability theory to model uncertainties during image selection. The MAD competition is a computational method for comparing hierarchical image representations based on human perceptual sensitivity. It focuses on generating adversarial examples to evaluate image classifiers on new test sets, emphasizing the discriminability of competing models. Unlike traditional adversarial images, MAD-selected images aim to fool at least one classifier and highlight their unique characteristics. The MAD competition focuses on generating adversarial examples to evaluate image classifiers on new test sets, emphasizing the discriminability of competing models. In experiments, classifiers may make high-confidence predictions by leveraging object relations without \"seeing\" the predicted object, overlooking conflicting semantic cues. An ideal classifier should utilize both low-level visual features and high-level semantic features when making predictions."
}