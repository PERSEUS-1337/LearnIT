{
    "title": "SJzQ0OZQom",
    "content": "Large deep neural networks require significant memory and can be slow for real applications. To address this, a novel neural network operator called chopout is introduced. Chopout allows for training networks to perform well with truncated sub-networks, enabling size reduction and improved efficiency. It is easy to implement and can be integrated into existing neural networks. Additionally, chopout can reduce network size and latent representations post-training by truncating layers. Experimental results demonstrate its effectiveness. At test time, chopout functions as an identity operation, passing input vectors without modification, contrasting with dropout. Chopout in prediction mode is different from dropout, as it involves training sub-networks by cutting out parts of the original network. It can be extended to higher dimensions by randomly truncating channels. Back-propagation follows the same principle. Experiments on autoencoders show the effectiveness of chopout. By applying 58 chopout on the hidden layer of the autoencoder, the reconstruction is well preserved even after truncation. Chopout is also used for embeddings trained through skip-gram models on the text8 corpus. The consistency of embeddings is shown in the results. Additionally, chopout can be applied for network pruning to explore distribution P m ({0, 1, \u00b7 \u00b7 \u00b7 , d}) in neural networks."
}