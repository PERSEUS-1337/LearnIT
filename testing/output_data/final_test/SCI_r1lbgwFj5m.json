{
    "title": "r1lbgwFj5m",
    "content": "In this extended abstract, the value of pruning in compressing neural networks is examined. It is found that training a simple, smaller network from scratch is better when time-constrained than pruning a large network. The architectures obtained through pruning, rather than the learnt weights, are valuable and powerful when trained from scratch. These architectures can be easily approximated without further pruning, providing a family of new, scalable network architectures for different memory requirements. In this work, it is shown that pruned-and-tuned networks are consistently outperformed by simpler networks with linear rescaling of channel widths trained from scratch. This suggests that there is little value in weights learned through pruning. When architectures obtained through pruning are trained from scratch, they outperform pruned-and-tuned networks and simpler networks with linear rescaling of channel widths. Fisher-pruning is used as a method to produce compact, powerful architectures by setting unimportant weights to zero or severing channel connections. Recent work has shown that powerful channel pruning methods can produce small, efficient networks suitable for real-time applications. Different studies have explored the benefits of training pruned models from scratch and proposed single-shot pruning schemes to create reduced architectures. Experiments are conducted to compare pruned networks with smaller, simpler networks. The study evaluates the performance of pruned networks compared to smaller, simpler networks using WRN-40-2 on CIFAR-10 dataset. The base network has 2.2M parameters distributed in 18 residual blocks with two convolutional layers each. The focus is on whether pruned networks outperform smaller networks and if the pruned architecture itself is useful. The network has a N m channel output which goes through a second layer producing DISPLAYFORM0. It is first trained from scratch and then Fisher-pruned BID16. Channels of activations between convolutions are pruned to introduce a bottleneck, reducing parameters. The process involves fine-tuning the network and removing channels with the smallest effect on loss. Test error and parameter count are recorded before each channel removal. The resulting trajectory of Test Error versus Number of Parameters is compared to training smaller networks from scratch, such as WRN-40-k and WRN-40-2 networks. In the study, networks with varying parameters were trained from scratch and compared to pruned-and-tuned networks. The smaller networks trained from scratch consistently outperformed the pruned-and-tuned networks, with the difference becoming more pronounced as the networks decreased in size. The average error across multiple runs was shown for networks trained from scratch, Fisher-pruned and fine-tuned WRN-40-2 networks, and bottlenecked WRN-40-2 networks. The study compared networks trained from scratch to pruned-and-tuned networks. Smaller architectures trained from scratch outperformed pruned networks. Fisher-pruned architectures, when trained from scratch, showed improved performance. Channel structures of pruned networks were linearly scaled and trained from scratch, resulting in high-performing networks. Profiled channel structures were used to create copycat networks. The study compared networks trained from scratch to pruned-and-tuned networks. Pruning trajectories showed improved performance when architectures were trained from scratch. Channel structures in pruned networks were linearly scaled, with later layers being more expendable. Blocks 7 and 13 showed minimal pruning due to strided convolutions reducing spatial resolution. The study compared networks trained from scratch to pruned-and-tuned networks, showing improved performance with pruning trajectories. Blocks 7 and 13 contain strided convolutions that reduce spatial resolution, with minimal pruning observed. Copycat Fisher architectures were trained to mimic Fisher-pruned structures, varying parameter counts for similar performance. The study compared networks trained from scratch to pruned-and-tuned networks, showing improved performance with pruning trajectories. Copycat WideResNets in this work achieve competitive results with lower parameter counts compared to WideResNets produced by the pruning method of BID12. Training details include the use of SGD with momentum, cross-entropy loss minimization, data augmentation, and specific hyperparameters. Pruning involves fine-tuning the network with the lowest learning rate, measuring the effect on loss for each channel, and removing the channel with the lowest value. The FLOP penalty hyperparameter had little impact, so it was set to zero. Pruning is costly and time-consuming; it is better to train a smaller network from scratch under time constraints. Pruning should be viewed as architecture search, with the resulting structure being more important than the weights. Pruned architectures trained from scratch perform well. Pruning involves fine-tuning the network with the lowest learning rate, measuring the effect on loss for each channel, and removing the channel with the lowest value. The FLOP penalty hyperparameter had little impact, so it was set to zero. Pruning is costly and time-consuming; it is better to train a smaller network from scratch under time constraints. Pruning should be viewed as architecture search, with the resulting structure being more important than the weights. Pruned architectures trained from scratch perform well and are easily emulated, as shown by copycat networks. Future work could expand this analysis to other network types, datasets, and pruning schema, and use distillation techniques between pruned architectures and the original to boost performance."
}