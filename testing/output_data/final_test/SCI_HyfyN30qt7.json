{
    "title": "HyfyN30qt7",
    "content": "Convolutional Neural Networks (CNN) are widely used in various fields such as computer vision, speech recognition, and natural language processing. However, the computational demands of deep learning networks hinder real-time performance on low-power systems like mobile devices. To address this issue, quantizing the weights and activations of these networks has been proposed to speed up runtime, albeit with a trade-off of increased error. The NICE method introduced in this work trains quantized neural networks using noise injection and learned clamping to improve accuracy, achieving state-of-the-art results on tasks like ImageNet classification with low-bit architectures like ResNet-18/34/50. Deep neural networks like ResNet-18/34/50 with 3-bit weights and activations are implemented on an FPGA for low power real-time applications. Despite their success in various fields, the computational and storage requirements of deep learning models pose challenges for devices with limited resources, making them infeasible for smart phones and IoT. Many researchers have developed less demanding deep learning models to address the challenges of running them on smart phones and IoT devices. One approach is quantizing networks, such as using 16-bit fixed point weights with minimal impact on accuracy. Further reductions in precision, down to 1-5 bits per parameter, are being explored as a current challenge in network quantization. This paper introduces a novel approach called NICE for neural network quantization, which involves noise injection during training to emulate quantization noise and statistics-based initialization for parameter and activation clamping. The proposed strategy improves performance without increasing complexity and can be applied to existing architectures. Our approach for neural network quantization, NICE, can be directly applied to existing architectures without the need for modification during training. It allows for quantizing all parameters to fixed point values, including batch-norm components. This makes integration of neural networks into hardware devices easier, such as FPGA and ASIC. Additionally, a case study of hardware implementation is presented as a proof-of-concept. The quantization code will be made publicly available upon acceptance. Quantization of neural networks to low-precision representations has been actively studied. Some approaches include using wider networks or adding linear scaling layers. Training a quantized neural network involves keeping two sets of weights and approximating gradients with a straight-through estimator. Stochastic or deterministic functions can be used for quantizing parameters. One leading approach for quantization is distillation, where a teacher-student setup is used to train a quantized neural network to imitate a full precision network. Another approach involves representing parameters with learned basis vectors for optimized non-uniform representation and computing MAC operations with bitwise operations. Additionally, learning the clamping value of activations to balance clamping and quantization errors is proposed, with the difference being the direct learning of clamps value using STE. BID19 introduced a complex parametrization for weights and activations, approximated with piecewise linear functions. BID35 and Dong et al. gradually increased quantized weights for better convergence. BID22 showed that 4-bit neural networks can achieve full-precision performance with larger batches and proper learning rate annealing. 8-bit and 32-bit integer representations were used for batch normalization and biases. In this work, a training scheme for quantized neural networks is proposed for fast inference on hardware with integer-only arithmetic. Additive random noise is injected into the network weights to emulate quantization effects, with a combination of well-known and novel techniques applied for maximum performance. The generalization of neural networks is better understood through quantization, where the generalization error scales with the number of parameters in over-parameterized networks when quantized. The training scheme for quantized neural networks involves using random weight perturbation for regularization, a gradual training scheme, and clamping activations and weights to reduce quantization error. The approach aims to optimize the tradeoff between quantization error and dynamic range. The method involves initializing values using layer statistics, learning optimal clamping values to minimize quantization error, and injecting noise during training to simulate quantization effects. Previous research has shown that quantization error can be approximated as a uniform random variable, even with coarse quantizers. The proposed method allows updates during the backward pass to immediately influence the forward pass. The method involves using a Bernoulli distributed mask to quantize weights and add noise for a dropout-like effect. Empirical evidence supports using a mask with a probability of 0.05. Instead of directly quantizing weights, a modified value is used in the forward pass to avoid significant changes in network behavior. Gradually adding quantized parameters helps improve scalability for deeper networks. The method involves gradually adding quantized parameters to the network by injecting noise into specific blocks of layers. This gradual process reduces the amount of noise injected and improves convergence, allowing the network to adapt to quantization. The method involves gradually adding quantized parameters to the network by injecting noise into specific blocks of layers. The network adapts to quantization errors, ensuring minimal changes during quantization phases. Optimal block size is a single layer with corresponding activation. Weight quantization is done based on a defined parameter per layer, and network activations are uniformly quantized. The method involves injecting noise into specific layers to gradually add quantized parameters to the network. The network adapts to quantization errors by replacing the conventional ReLU activation function with clamped ReLU. The clamping range is learned with other parameters via backpropagation, and a quantized version of the truncated activation is obtained. The Round function is handled using the STE approach for gradient propagation, and the derivative of the activation with respect to the clamping range is calculated for updating. The method involves injecting noise into specific layers to add quantized parameters gradually. The biases are clamped and quantized similarly to the weights. The effectiveness of the method was demonstrated on image classification datasets and a regression scenario. The CIFAR-10 results are provided in Appendix C. The experiments use a pre-trained FP32 model, which is then quantized using NICE for ResNet networks on ImageNet. Our approach, NICE, involves fine-tuning a pre-trained network for 120 epochs with SGD optimizer. Results show state-of-the-art performance for 4 and 5 bits quantization and comparable results for 3 bits quantization. Additionally, NICE outperforms FAQ 8,8 results for the 5,5 setup on all tested architectures. The method is also applied to a regression task of joint image denoising and demosaicing. The network used in the regression task of joint image denoising and demosaicing is modified with Dropout, skip connections, and quantization. NICE is applied for 500 epochs with Adam optimizer and data augmentation. The input image is quantized to 16 bit, while convolution outputs are quantized to 8 bits. NICE achieves better results than WRPN for image denoising and demosaicing, especially for low weight bitwidths. The importance of each part of NICE is shown using ResNet-18 on ImageNet, with noise addition and gradual training contributing more to accuracy for high bitwidths, while clamp learning is more effective for low bitwidths. Our quantization scheme enables FPGA implementation by using uniform quantization for weights and activation, avoiding the need for a costly code-book. Integer-only arithmetic is achieved by representing activations and parameters as X = N \u00d7 S, with scaling factors constrained to q \u2208 [1, 256], p \u2208 [\u221232, 0] without loss of accuracy. The FPGA implementation uses quantization with scaling factors constrained to q \u2208 [1, 256], p \u2208 [\u221232, 0] for efficient integer arithmetic. The PipeCNN implementation is used for regression and classification tasks, with layers calculated sequentially. The FPGA is used for inference only, with weights quantized to 4 bits and OpenCL kernel compiled to Intel's Arria 10 FPGA for running DeepISP architecture. The FPGA implementation utilized quantization with scaling factors constrained to specific ranges for efficient integer arithmetic. The processing on the FPGA was more energy-efficient compared to an NVIDIA Titan X GPU. A dedicated ASIC manufactured using a similar process would be even more efficient. The NICE training scheme for quantized neural networks showed state-of-the-art results. Our solution for training quantized neural networks using back propagation in full precision arithmetic outperforms current works on ImageNet for various bitwidths and network architectures. Additive uniform noise improves results for 4 and 5 bit setups more than for 3 bits, suggesting potential for further improvement with non-uniform noise. The 4 bit setup is of interest due to hardware compatibility with Nvidia's upcoming INT4 tensor cores. NICE is a simple \"plug-and-play\" modification that does not require architectural changes. The distribution of quantization error is uniform for 4 and 5 bits, deviating for 3 bits. Activation clamp values converge with \u03b1 set to 5. In an experiment with ResNet-18 on CIFAR-10, activation clamp values converge to smaller values than the initialization, indicating a regularization effect similar to weight decay. Implementing systems with arbitrary precision often use FPGAs and ASICs due to their customizable nature. Redundancy in floating point representation in Neural Networks has been shown, leading to regularization effects when quantizing weights or activations. Custom low-precision representation can be used in Neural Networks to reduce redundancy and improve efficiency. With increasing on-chip memory size, it is feasible to fit all parameters inside an ASIC or FPGA using low bitwidth, leading to reduced latency, power consumption, and resource utilization. Quantizing activations can also save routing resources and increase operational frequency, resulting in higher throughput. FPGAs have become more popular in recent years for implementing such systems. FPGAs have gained popularity as inference accelerators due to their reconfigurability, unlike ASICs which offer higher throughput with lower energy consumption. FPGA-based inference accelerators are described as heterogeneous systems using OpenCL or standalone accelerators using HLS compilers."
}