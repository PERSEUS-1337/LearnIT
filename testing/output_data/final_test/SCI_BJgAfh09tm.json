{
    "title": "BJgAfh09tm",
    "content": "Latent space GAN methods and attention-based encoder-decoder architectures have shown success in text generation and Unsupervised NMT. A new architecture is proposed that can generate parallel sentences in two languages simultaneously and translate bidirectionally by sharing a latent space. The model combines NMT with back-translation and adversarial training to enforce a shared latent state between languages. A GAN is then used to generate code representing the shared latent space, which is decoded to produce text in either language. Experiments were conducted on Europarl and Multi30k datasets for English-French translation. Neural machine translation (NMT) and neural text generation (NTG) have shown success in NLP tasks. Researchers have utilized GANs for text generation due to their success in image generation. A Bilingual-GAN agent is proposed to create a shared latent space for translation and text generation in two languages. The proposed Bilingual-GAN aims to unify neural text generation (NTG) and neural machine translation (NMT) by leveraging shared latent space for both tasks. Techniques like Adversarially Regularized Autoencoders (ARAE) are used to learn continuous manifold of autoencoder latent space for text generation. The approach involves learning the continuous manifold of the autoencoder latent space to generate text samples. Adversarial learning is used to create a shared latent space between languages for unsupervised machine translation. The proposed architecture combines text generation and machine translation in a bilingual agent. The method involves using a bilingual agent for text generation and machine translation simultaneously, even with complex latent representations. Recent works focus on unsupervised neural machine translation using a shared encoder-decoder model and a common latent space for both languages. Recent works in unsupervised neural machine translation involve using back-translation and word translation dictionaries to enforce language independence in the latent code space. Some models also utilize BPE sub-word tokenization with FastText embeddings to embed sentences in a common space. Additionally, GAN frameworks have been successfully applied in various NLP applications such as machine translation, dialogue models, and question answering. The use of GAN in NLP poses challenges due to the discrete nature of text, making back-propagation difficult for discrete outputs. A latent code-based solution was proposed to derive a latent representation of text using an AE and adversarial training of a generator. The Bilingual-GAN consists of a translation unit and a text generation unit, with a complete architecture described in Figure 1. The translation system in the Bilingual GAN consists of an encoder and a decoder for two languages, with a loss function based on cross-entropy. The encoding of sentences is used as word embeddings to convert input sentences. The Bilingual GAN translation system includes an encoder and decoder for two languages, using word embeddings to convert input sentences. The system is trained with three losses: reconstruction, cross-domain, and adversarial, applied to both languages in every batch. The reconstruction loss aims to reconstruct the input, while the cross-domain loss allows for translation of inputs. The implementation of translation in the Bilingual GAN system involves a process where the original sentence is translated into another language and then checked for accuracy. An adversarial loss is used to ensure the encoder produces language-independent code, with a discriminator predicting the language of the sentence. Input noise is introduced to prevent the encoder-decoder pair from learning the identity. Input noise is added to the encoder to prevent learning the identity function and increase robustness. Random word drops and shuffling are used, along with Gaussian noise for the decoder input. The translation function in Bilingual-GAN affects model supervision. The translation function in the trained model can be changed from word-by-word to using the model itself for translation after a few epochs. This approach is seen in BID15, where the translation function is updated after 1 epoch. In this case, a word-by-word translation lookup table is created using a multilingual embedding space. The translation function in the trained model can be updated to use the model itself for translation. This involves creating a word-by-word translation lookup table in a multilingual embedding space. The space created by BID6 allows for supervised neural machine translation loss, with options for embedding the sentence words before feeding into the encoder. Different embedding methods are experimented with and results are shown in section 4.3. The Bilingual-GAN model utilizes FastText embeddings, with specifications for encoder, decoder, and discriminator. Training optimizers include Adam for encoder and decoder, and RMSProp for discriminator. Specifications were adapted from BID15. The NMT system pre-trains to learn a shared latent space for translation. The NMT system pre-trains to learn a shared latent space enforced by a GAN setup between a critic and the encoders. A bilingual generator is trained adversarially to mimic samples from the shared latent space, allowing the decoders to generate parallel bilingual sentences. The proposed bilingual generator is a GAN trained to learn the hidden state manifold of the RNN-based encoder using Wasserstein GAN gradient penalty approach. The GAN setup involves using the encoder output of an NMT system as 'real' code, which is a latent state space matrix capturing hidden states. A generator neural network creates a 'fake' code matrix by generating noise, which is then evaluated by a discriminator neural network. The process includes a gradient penalty term \u03bb = 10 to train the GAN effectively. The discriminator neural network, with 5 convolutional and 1 linear layer, calculates generator and discriminator losses. Unlike previous updates, 1 discriminator update per generator update is used. Both English and French sentences are fed to the encoder in one training iteration to produce real codes. The GAN is trained on supervised and unsupervised NMT scenarios, using both real codes to reduce biases. In the NMT scenarios, supervised training involves feeding parallel English and French sentences, while unsupervised training ensures non-parallel sentences. The generator code can be decoded in either language using pre-trained decoders. Latent-space text generation uses a single code vector to summarize the hidden sequence. By concatenating the forward and backward latent states, a code matrix is produced for the decoder to attend to. The generator aims to mimic the concatenated latent codes. The section presents experiments on translation and generation using Europarl and Multi30k datasets. Preprocessing steps include removing long sentences and tokenizing using Moses tokenizer for Europarl, while using the tokenized version for Multi30k. For the BPE experiments, a sentencepiece subword tokenizer by Google BID23 was used to tokenize the dataset without further processing. BPE is a subword tokenization method that results in a common embeddings table for both English and French languages. The BPE was trained on 200k sentences for training and 40k sentences for testing. Splits for unsupervised training ensured no translations in the other language's training set. In the supervised case, the same sentences were randomly chosen in both languages with a validation set of 40k sentences. For the Multi30k dataset, 12,850 and 449 sentences were used for training and validation in the unsupervised case. The test set consisted of 1,000 sentences from Flickr 2017. Hyperparameter search involved a vocabulary size of 8k for Europarl and 15k for final experiments. For Multi30k, a vocabulary of 6,800 common words was used. BLEU-N scores were calculated using a specific equation, with results reported for BLEU-4. The results of BLEU-4 and BLEU-N scores for generated sentences are reported in TAB1 and TAB2. Perplexity is used to evaluate sentence fluency, with forward and reverse perplexities described in TAB4 for models trained on Europarl and Multi30k datasets. The fluency of synthetic samples was evaluated using reverse perplexities by training an RNNLM on the samples. Hyperparameters were used in the experiments, and abbreviations were explained before discussing the results, including MTF for model translation from and NC for New Concatenation method. Levels of supervision were detailed in a previous section. The results section focuses on training a neural machine translation system, with details on different techniques used such as FastText, Xlingual, BPE, NoAdv, and 2Enc. It highlights the impact of removing adversarial loss on system performance. The results section discusses the impact of removing adversarial loss on a neural machine translation system. It also mentions the benefits of using 2 layers for the encoder and how a new concatenation method improved the model. Additionally, it notes that BPE performed poorly on English to French translation, suggesting further investigation is needed. Further investigation is needed to explain the significant score difference. Results show the effectiveness of using trainable FastText embeddings, with pretrained embeddings potentially offering better performance. It is crucial to allow embeddings to change during training rather than fixing them. Evaluation of text generation focused on fluency in English and French, as well as the validity of concurrently generated translations. Performance was measured on supervised and unsupervised scenarios, utilizing pre-trained NMT models. The results of text generation evaluation focused on fluency in English and French using pre-trained NMT models. English sentences had higher BLEU scores, potentially biased by the NMT. Lower BLEU scores were observed for the Multi30k dataset due to its smaller test size. Perplexity results showed lower forward perplexities for synthetic samples, indicating higher fluency, but higher reverse perplexities due to ungrammatical sentences in the synthetic data. The BLEU scores are used to evaluate the ability of the Bilingual-GAN to generate parallel sentences. Google Translate is used to provide reference translations for the evaluation. The results show better performance on the Multi30k dataset in the supervised scenario, but lower scores on the Europarl dataset. Our BLEU scores are lower on the Europarl dataset compared to conventional NMT systems. However, the unsupervised model shows slightly higher scores. The proposed Bilingual-GAN approach for generating parallel sentences is a novel method and can serve as a benchmark for future research. The curr_chunk discusses examples of aligned generated sentences from models trained using the Europarl and Multi30k datasets. The sentences were rated by a group of people based on fluency using a Likert scale. The study involved rating sentences based on fluency using a Likert scale. The Bilingual-GAN approach received positive feedback, with the supervised approach outperforming the unsupervised one. The research proposed a novel method combining neural machine translation with adversarial language generation to produce bilingual, aligned sentences, highlighting the commonalities between language generation and translation. The study explored learning a large code space of an RNN over the entire sequence length for bilingual text generation. Promising results were obtained, with potential improvements in sentence quality and language-specific performance. Various generation methods can be incorporated to enhance performance, and efforts are needed to address language biases in the learned code space."
}