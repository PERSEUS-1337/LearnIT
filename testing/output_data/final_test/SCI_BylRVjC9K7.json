{
    "title": "BylRVjC9K7",
    "content": "Researchers have focused on generating adversarial examples and understanding their origins, with past research emphasizing changes in decision boundaries. This paper explores the origin of adversarial examples from a knowledge representation perspective, highlighting how neural networks store knowledge in a hybrid way that can lead to misclassification with small modifications. A one-step distribution imitation method is proposed to imitate the distribution of the nearest different class neighbor. Experiments demonstrate the effectiveness of this approach in imitating distributions from a training set. Adversarial examples can impact classification results in deep networks by imitating distributions from a training set. Ways to alleviate adversarial examples include changing data encoding and constructing learning frameworks with improved representations. The robustness and security of deep neural networks have become a focus of academic and industrial attention. The discovery of adversarial examples in CNNs has sparked interest in security and robustness. Researchers are investigating the sources of adversarial examples and developing methods to generate them automatically. This paper explores the origin of adversarial examples from a knowledge representation perspective, showing that they can be derived from training data and are not network-dependent. It also suggests that adversarial examples can take various forms beyond small perturbations, and discusses possible solutions to mitigate this issue. Adversarial attacks have become systematic, with algorithms developed to generate examples that can skew model classifications. These attacks are not limited to neural networks and can affect various machine learning techniques. Most research focuses on image classification tasks, but other machine architectures are also vulnerable. Ways to alleviate this issue are being discussed. Machine architectures vulnerable to adversarial attacks include more than just neural networks. Recent research has shown that adversarial examples are prevalent in the physical world, with universal perturbations capable of generating universal and adversarial examples. Defense techniques, such as incorporating adversarial examples into training sets, have also been explored to mitigate these attacks. Researchers have explored adversarial perturbations in classification objectives, with some studies suggesting that multi-camera views can constrain adversarial examples. However, other research shows that adversarial examples can affect image and scene segmentations. Efforts have been made to understand the underlying reasons for these examples, including linear vibration from vector computation and the use of geometrical and topological methods to explore perturbation effects. Researchers are using geometrical and topological methods to study perturbation effects on high-dimensional decision space. There is a growing interest in undermining deep neural network principles and expanding their applications. Adversarial examples are challenging to detect visually and are often generated using complex algorithms. These examples may only represent a small portion of the total image space. This paper is the first to discuss adversarial examples from an internal knowledge representation perspective. The paper discusses adversarial examples from an internal knowledge representation perspective, providing a formal description of explanations and proposing a distribution imitation procedure in experiments. The discussion section concludes with potential solutions from a knowledge representation viewpoint. The curr_chunk discusses how human beings achieve abstraction and sparsity through hierarchical storage of knowledge, similar to object-oriented programming. It highlights how humans can learn from prototypes and recognize objects by understanding their transformations. The current chunk explains the architecture of the AlexNet neural network, which is used for image classification tasks. It consists of convolutional layers to extract local features and fully connected layers to combine these features into a higher-level image entity. The network then uses a Softmax classifier to output probabilities for each possible class. Despite its intuitive design, there is still a gap between abstract and sparse representation in comparison to human cognition. The current chunk discusses the gap between human cognition and neural networks in representing knowledge. It explores the execution procedure of neural networks through extraction and transformation steps, showing how different inputs can activate different parts in the transformation process. The current chunk discusses the extraction and transformation steps in neural networks. It explains how inputs are processed to determine parameters for transformations. The ideal condition for a classifier is to give a high value for the correct class and low values for others before the final probability distribution. The outputs should be fixed when inputs belong to the same class. The system processes test inputs using transformations denoted by inputs A and B, resulting in output pattern DISPLAYFORM4. Neural networks extract local features and weighted sampling of all training inputs. Visualization techniques like DeepDream highlight features learned by the network through images that activate specific channels. In the last layer of the neural network, objects like 'container ship' and 'revolver' are recognized based on their repeating structures. The network uses a hybrid distribution of prototypes to represent objects, making decision boundaries between different objects closer. This allows for small perturbations to be possible. The boundaries between objects in the neural network become smaller, allowing for small perturbations to lead the classifier into wrong results. This can result in adversarial examples. Differentiating transformations within the same class can be represented by prototypes, such as normal distributions N(10,3), N(30,4), and N(45,6). The representation approach helps differentiate between these prototypes. The representation approach in neural networks helps differentiate between prototypes by combining subclasses, leading to hybrid representations that can make classification more difficult. Small perturbations can further decrease the difference in means between classes, affecting the classifier's precision and ability to recognize objects. Neural networks combine subclasses to create hybrid representations, making classification challenging. Adversarial examples exploit this hybrid knowledge representation by imitating target class distributions, leading to unclear bounds for high-dimensional distributions. Weight values from PCA can depict these distributions, with similar weights indicating higher similarity and increased likelihood of falling between different object classes. The curr_chunk discusses experiments on distribution imitation impact on classification using adversarial examples. An example using Fast Gradient MethodGoodfellow et al. FORMULA0 is shown, reducing classifier accuracy from 99.8% to 45%. Successful adversarial examples are illustrated, altering classifier recognition from a 7 to a 9. The classifier misclassifies a number 7 as a number 9 due to adversarial examples, which manipulate principal components to mislead the classifier. The angle between projections on normal and adversarial examples is used as a metric for similarity, with smaller angles indicating higher similarity. This approach is based on the distribution imitation impact on classification using adversarial examples. The text discusses using PCA for classification by modifying weight values to imitate distribution and classify images. It explains the process of computing PCA on training set, getting coefficients and weights, and choosing nearest neighbor based on weight values for classification. The goal is to modify test images to achieve a different classification result. The text discusses using PCA weights to modify test images for misclassification. It explains the process of computing PCA on training set, getting coefficients and weights, and choosing nearest neighbor based on weight values for classification. The goal is to imitate the original image into similar patterns with a different class neighbor. The experiment involves modifying test images using PCA weights to imitate goal images. It calculates weight value differences and reconstructs the image using a new weight vector. The experiment is conducted on 5,000 images from cifar-10 dataset to depict distribution differences and involves PCA computation on the whole training dataset. The experiment involves modifying test images using PCA weights to imitate goal images. A 24-layer network is used for image classification, with a common structure. Results show that modifying weight values reduces accuracy from 0.851 to 0.532, leading to absurd classifications. Further tests were conducted using an improved classifier with ResNet connections, resulting in an accuracy of 0.588, similar to the previous setting. The main structure of the original RGB image is preserved, but with added color masking that can sometimes be visible. The position and visibility of the masking depend on weight changes, with more critical positions leading to total destruction of the original image structure. Modifications to RGB images may not always be easily visible due to the three color channels. Additional experiments were done on the MNIST digit dataset. Further experiments were conducted on the MNIST digit dataset with flexible imitation settings. Different weights have varying impacts, and the modification strength determines the overall result pattern. A hybrid approach combines two modification methods by dividing a 28*28 input image into four 14*14 patches. Modifications are applied to scores ranging from 10 to higher, with a linear increase in ratio from 0 to a certain level for scores 10-50, and a fixed ratio for scores above 50. This piecewise function results in an increase in classification error from 1.04% to 34.20% when the highest ratio is set to 5. The error rate increased from 1.04% to 34.20% with visible modifications on the MNIST dataset. The classifier is most easily fooled into recognizing the modified digit as 8 due to its wide distribution in high dimensional space. Modified digit images are still recognizable to humans but can successfully fool neural networks. Noise is added or removed from original images, similar to perturbations in face recognition. Modifications on black and white digits are more easily recognizable compared to RGB images. In a series of batch experiments, the classification accuracy gradually decreases from 98.96% to 65.80% as the modification ratio increases from 0.5 to 5. Adversarial images can be caused by distribution imitations on the dataset itself, showing that they can take various forms beyond small perturbations. This one-step distribution imitation procedure poses a danger as it can be conducted without knowledge of the network. This paper discusses the origin of adversarial examples from a knowledge representation perspective, highlighting how neural networks store knowledge in a hybrid way that can lead to misclassification with small modifications. Experiments show that distribution imitation on a training set without network knowledge can impact classification results, explaining the universality of adversarial examples. Modified images can retain original structures and information. Adversarial images can take various forms beyond small perturbations, indicating the importance of dataset quality in classification robustness. While deep neural networks are highly efficient, the rarity of adversarial examples may explain their effectiveness. Research on layer properties reveals the minimum perturbation magnitudes needed to alter classification results, suggesting adversarial examples exist under specific conditions. Focusing on altering what classifiers learn from the start, rather than changing learning frameworks, can enhance classification accuracy. Focusing on changing what classifiers learn from the beginning can enhance classification accuracy by providing a concentrated learning space to resist adversarial examples. This can be achieved through pre-processing steps in the training set. One way to enhance classification accuracy is by adding pre-processing steps on datasets to make the data more compact for neural networks. Another approach is to use more prototypical datasets, such as the EMMI dataset, to create subclasses and improve classifier training. Additionally, constructing a better network representation can also contribute to the success of deep networks. Constructing a better network representation is crucial for the success of deep networks. Recent research on neural network compression highlights the presence of shared and redundant parameters. Separating prototypes and transformations in learning frameworks involving detection and classification is challenging, as defining a single optimization problem becomes nearly impossible due to state overlapping between different prototypes. The current challenge lies in the dilemma of hybrid representations leading to inevitable adversarial examples. A new learning framework may be necessary to represent knowledge more compactly. It is evident that current datasets and models are still far from achieving human-level vision understanding. There is a long journey ahead before machines can match or surpass human vision capabilities."
}