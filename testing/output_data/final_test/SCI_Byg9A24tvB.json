{
    "title": "Byg9A24tvB",
    "content": "The study focuses on improving adversarially robust generalization by inducing dense feature regions in the feature space using the Max-Mahalanobis center (MMC) loss. This loss encourages the model to learn compact representations around optimal centers for different classes, enhancing robustness. The study demonstrates that applying the MMC loss improves robustness against adaptive attacks while maintaining high accuracy on clean inputs. DNNs trained with the SCE loss are vulnerable to adversarial attacks, leading to the need for improved defenses. Various defenses have been proposed to enhance the adversarial robustness of classifiers, but many are ineffective against adaptive attacks. Adversarial training methods have shown state-of-the-art robustness, although they can sacrifice accuracy on clean inputs and are computationally expensive. Efforts have been made to address these challenges. Recent efforts have focused on faster verification methods and accelerating adversarial training procedures to improve robust learning. The sample complexity of robust learning is shown to be significantly larger than standard learning, possibly due to insufficient training samples in commonly used datasets. While some work utilizes extra unlabeled data, there is a focus on efficiently exploiting labeled data. Manipulating the local sample distribution in the feature space is considered as a strategy to enhance robust classifier training. Manipulating the local sample distribution in the feature space can enhance robust classifier training by inducing high-density feature regions for reliable predictions. Previous work has focused on improving intra-class compactness using contrastive loss and triplet loss objectives. In this paper, the center loss is discussed as a method to avoid pair-wise or triplet-wise computation for image pairs or triplets, aiming to minimize the squared distance between features and class centers. However, it is noted that the center loss, when used alone, may not induce strong intra-class compactness for reliable robustness in classifier training. The study also analyzes the sample density distribution induced by the SCE loss and its variants to demonstrate their effectiveness in enhancing feature space distribution. The study proposes a novel training objective, the MaxMahalanobis center (MMC) loss, as a substitute for the SCE loss. It aims to induce high-density regions in the feature space and learn more structured representations by presetting untrainable class centers and minimizing the squared distance between features and centers. The MaxMahalanobis center (MMC) loss is introduced as a training objective to improve adversarial robustness by controlling inter-class dispersion and enhancing intra-class compactness. Experimental results show reliable robustness with faster convergence rates compared to the SCE loss. The MaxMahalanobis center (MMC) loss improves adversarial robustness by controlling inter-class dispersion and enhancing intra-class compactness, maintaining high clean accuracy with faster convergence rates compared to the SCE loss. It can be further enhanced under different attacks when combined with existing defense mechanisms like AT methods. The logarithm is defined element-wise. Adversarial examples can easily fool DNNs. Various attacking methods have been introduced in recent years. A comprehensive evaluation is performed considering five threat models and representative attacks for each model. White-box l \u221e distortion attack uses the PGD method. White-box l 2 distortion attack uses the C&W method. In the current chunk, various attack methods are applied, including l2 distortion attack using the C&W method, black-box transfer-based attack using the MIM method, black-box gradient-free attack using the SPSA method, and general-purpose attacks involving adding Gaussian noise or random rotation to input images. Additionally, adaptive attacks are used to evaluate the robustness of the method. The adaptive attacks are more powerful than non-adaptive ones, as explained in Sec. 4.2. The difficulty of training robust classifiers is attributed to the insufficiency of training samples in common datasets. Efforts have been made to improve training with extra unlabeled data, while this study focuses on better utilizing labeled training samples for robust learning by manipulating the local sample distribution in the feature space. In this section, the concept of sample density in the feature space is defined and analyzed in relation to training objectives. The Max-Mahalanobis center (MMC) loss is introduced as a superior method for inducing high-density regions in the feature space to train robust models. The goal is to manipulate the local sample distribution for more reliable predictions. The feature distribution in \u2206B is induced by the training loss L, where minimizing the loss value guides the movement of feature points. Sample density varies mainly along the orthogonal direction w.r.t. the loss contours, with density along a contour being approximately the same. The MMC loss changes sample density mainly along the radial direction, with loss contours represented as concentric circles. The generalized SCE loss and its variants, such as the large-margin Gaussian Mixture (L-GM) loss and Max-Mahalanobis linear discriminant analysis (MMLDA) loss, impact sample density of features in the feature space. The g-SCE loss involves a transformation of features using a logit function, while L-GM loss assumes features distribute as a mixture of Gaussian. MMLDA loss also assumes a similar Gaussian mixture but with differences in the approach. The MMLDA loss, unlike the g-SCE loss, does not have trainable \u00b5 * i but calculates them before training with optimal inter-class dispersion. Both losses belong to the g-SCE loss family with quadratic logits. The contours of the g-SCE loss are represented by a closed-form solution in the space of z. The contours of the g-SCE loss are represented by a closed-form solution in the space of z, where the function does not provide an intuitive closed-form solution due to the term log l =y exp(h l ). The function can be locally approximated with L y,\u1ef9 (z) = log[exp(h\u1ef9 \u2212 h y ) + 1] nearby the feature point z, assuming a scaled identity covariance matrix. The g-SCE loss contours are represented by a closed-form solution in the space of z. If \u03c3 y = \u03c3\u1ef9, the solution is a hypersphere; otherwise, it degenerates to a hyperplane. The g-SCE loss leads to feature points moving to infinity for lower loss values, while MMC encourages structured features. The sample density approximation depends on specific classes, and the training subset includes data with the true label of class k. The g-SCE loss approximation in the feature space induces sample density, with \u03c3 k = \u03c3k determining the density nearby a feature point. The solution set of C < C * is empty, avoiding biases on predictions. If \u03c3 k < \u03c3k, C can be minimized to zero, but as C \u2192 0, the sample density tends to zero, encouraging the feature point to move away from the hypersphere center. The softmax normalization in training causes points with low loss values to spread sparsely in the feature space, moving away from the hypersphere center. Batch normalization prevents points from moving to infinity, and the sample density is proportional to the g-SCE loss. The g-SCE loss has limitations in sample density proportional to N/L^2, inspiring the design of a new training loss. The features with loss values in a certain range will be encouraged to locate between two hyperplane contours without explicit supervision. Additionally, the center loss is proposed to improve intra-class compactness of learned features. The Max-Mahalanobis center (MMC) loss is proposed to improve intra-class compactness by learning structured representations and inducing high-density regions in the feature space. It is defined in a regression form without the softmax function, using centers of the Max-Mahalanobis distribution (MMD) to create a mixture of Gaussian distribution. The Max-Mahalanobis center (MMC) loss utilizes centers of the MMD to create high-density regions in the feature space, improving intra-class compactness. The MMD centers are fixed and optimized to maximize the minimal angle between them, ensuring optimal inter-class dispersion. The MMC loss induces sample density in the feature space, enhancing structured representations. The MMC loss induces higher sample density in the feature space, encouraging feature points of the same class to gather around their corresponding center for robust learning. The MMC loss enhances robust learning by monitoring inter-class dispersion and minimizing intra-class compactness. It avoids degradation issues seen in other loss functions and leads to faster convergence. The network trained with MMC can still provide normalized predictions in the test phase. The MMC loss introduces extra supervision on intra-class compactness, improving robustness compared to other losses. It also provides normalized predictions in the test phase and adapts better to various tasks. In this section, the MMC loss is empirically demonstrated on MNIST, CIFAR-10, and CIFAR-100 datasets using ResNet-32 architecture. Different baselines for MMC include SCE, Center loss, MMLDA, and L-GM. Hyperparameters for other losses follow original papers. Training is done with momentum SGD optimizer for 40 epochs on MNIST and 200 epochs on CIFAR-10. The learning rate decays during training on MNIST, CIFAR-10, and CIFAR-100 datasets. Adversarial examples are crafted using PGD with 10 steps. MMC loss shows faster convergence with comparable performance on clean images compared to SCE loss. Implementing the AT mechanism is computationally expensive and sacrifices accuracy on clean images. Adaptive versions of attacks are necessary for reliable robustness. The adaptive versions of attacks, using \u2212L MMC (z, y) and L MMC (z, y t ) in untargeted and targeted modes for PGD, are more effective than non-adaptive ones. Visualizing the PGD attacking procedure shows the efficiency of adaptive attacks over non-adaptive ones. In the white-box l \u221e distortion setting, PGD attacks were conducted with different parameters to evaluate accuracy. Results showed convergence with PGD-100 and PGD-200 attacks compared to PGD-50. An ablation study substituted center sets to investigate robustness effects in MMC, resulting in \"MMC-10 (rand).\" In Table 1, higher sample density in \"MMC-10 (rand)\" improves robustness under adaptive attacks. Using optimal center set \u00b5* in \"MMC-10\" further enhances performance. When combined with AT mechanism, models perform better against different attacks. Investigation into white-box l2 distortion includes C&W attack with binary search steps set to 9. In Table 1, higher sample density in \"MMC-10 (rand)\" improves robustness under adaptive attacks. Using optimal center set \u00b5* in \"MMC-10\" further enhances performance. When combined with AT mechanism, models perform better against different attacks. Investigation into white-box l2 distortion includes C&W attack with binary search steps set to 9. The C&W attack requires larger distortions to evade networks trained by MMC. Transfer-based attacks using PGD and MIM are performed in untargeted mode. Gradient-free attacks using SPSA method are also conducted. In SPSA, batch size is set to 128, learning rate is 0.01, and \u03b4 = 0.01. Stronger SPSA attacks with batch sizes of 4096 and 8192 are evaluated, showing convergence of accuracy under black-box SPSA attacks to white-box PGD attacks. Training with MMC loss leads to robustness under black-box attacks, indicating reliable robustness. General-purpose attacks like Gaussian noise and rotation transformation are tested, showing MMC method's general robustness. AT methods are less robust to simple attacks. Results in Table 2 show that MMC loss improves robustness to attacks compared to AT methods. Results on CIFAR-100 in Table 4 and Table 5 demonstrate MMC's significant improvement in robustness while maintaining better performance on clean inputs. Averaged distortion of C&W attacks on CIFAR-100 is larger for targeted attacks but smaller for untargeted attacks due to the increase in the number of classes. When the number of classes increases, it is easier to achieve a coarse untargeted attack but harder to make a subtle targeted attack. Results show that MMC benefits from deep network architectures and can improve robustness. Models trained by SCE are comparably sensitive to adversarial perturbations for different architectures, indicating that SCE cannot fully exploit model capacity for robustness. MMC provides effective robustness promotion with less computational cost by avoiding the softmax function in training. In this section, the authors propose the MMC loss to improve robustness by inducing high-density regions in the feature space. The method shows reliable robustness against adaptive attacks, maintains high performance on clean inputs, and requires minimal extra computation compared to the SCE loss. Additionally, it is compatible with existing defense mechanisms and provides insights for future research on new objectives beyond the SCE framework. The authors also provide proofs for the theorems proposed in the paper, including calculations for sample density and volume approximation. The authors propose the MMC loss to enhance robustness in the feature space by inducing high-density regions. The solution of L y,\u1ef9 = C results in hypersphere contours, with the volume of \u2206B y,\u1ef9 approximated as 2\u03c0\u0393(\u00b7). The authors propose the MMC loss to induce high-density regions in the feature space, resulting in hypersphere contours. The volume of \u2206B y,\u1ef9 is approximated as 2\u03c0\u0393(\u00b7). Technical details include experiments conducted on the NVIDIA DGX-1 server with eight Tesla P100 GPUs and a generation algorithm for crafting Max-Mahalanobis Centers. The Max-Mahalanobis centers are generated in low-dimensional cases, forming intuitive shapes like line segments, equilateral triangles, and regular tetrahedrons. A tree structure is used to assign virtual centers to nodes, with crafted MM centers shifted accordingly. The virtual centers assigned to nodes in a tree structure are shifted to form crafted MM centers. These centers are used in attacking mechanisms with adaptive adversarial objectives, as demonstrated in Fig. 6. Different objectives are preferred based on the adversarial goals, such as decreasing confidence in the true label or misleading the classifier to predict an untrue or target label. In face recognition, previous work has focused on angular margin-based softmax losses, utilizing operations like weight normalization, feature normalization, and angular margin. These methods aim to improve generalization in facial tasks. However, the MMC loss differs from AMS losses in how it induces inter-class margins. AMS losses focus on intra-class compactness, while MMC loss explicitly forces weights to have large inter-class margins. The MMC loss in face recognition encourages intra-class compactness without explicitly forcing weights to have large margins. It fixes class centers to be optimally dispersed and promotes compact intra-class distribution. Unlike AMS losses, MMC loss maintains the radial degree of freedom for features, enhancing model capacity. The MMC loss penalizes feature norm deviation to maintain feature compactness and prevent bias towards easy examples, similar to FN but more flexible."
}