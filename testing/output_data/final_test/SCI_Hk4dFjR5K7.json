{
    "title": "Hk4dFjR5K7",
    "content": "In this paper, the ADef algorithm is proposed to create a new type of adversarial attack on deep neural networks by applying small deformations to images. The instability of deep neural networks to small perturbations in input was observed, leading to misclassification of visually indistinguishable images. The algorithm's results were demonstrated on MNIST and ImageNet datasets using convolutional neural networks like Inception-v3 and ResNet-101. Research has been conducted to investigate the issue of adversarial examples in deep neural networks. Adversarial perturbations can cause misclassification of visually indistinguishable images, which can be exploited by malicious attackers. Various algorithms have been developed to construct these perturbations, highlighting the importance of addressing this vulnerability in classifiers and other automatic systems. Adversarial perturbations can be constructed in a black-box setting or in the physical world, motivating the investigation of defenses to make networks invulnerable. Most work focuses on small perturbations, but this may not accurately measure image similarity. In this work, the construction of adversarial deformations is investigated, where the misclassified image is created as a deformation y = x \u2022 (id + \u03c4), rather than an additive perturbation y = x + r. The similarity is measured through a norm of \u03c4, quantifying the deformation between y and x. An efficient algorithm called ADef is developed based on DeepFool, constructing the smallest deformation to misclassify the image. Testing is done on MNIST and ImageNet datasets. The ADef algorithm successfully fools classifiers in the majority of cases with small, imperceptible deformations. Results are tested on MNIST and ImageNet datasets, including adversarially trained networks for MNIST. The algorithm implementation can be found at the provided link. Additional details on the mathematical aspects can be found in the master's thesis. A first order method efficiently formulates a smoothing operation independent of the optimization step. Success rates for adversarial attacks with deformations on ImageNet are reported for the first time. Deformations have been discussed in previous works introducing learnable modules to increase DNN performance and a method to measure classifier invariance to geometric transformations. An adversarial example is a perturbation that changes the classification of an image while remaining imperceptible. Various methods, such as FGSM and DeepFool, have been proposed to find minimal adversarial perturbations. Deformations, like translations, are natural image transformations that can be used in attacks. Deformations in images can lead to large perturbations in Lp-norms. In the discrete setting, images are represented as collections of values of a function, allowing for deformations with respect to vector fields. Deformations in images can be measured by the size of the corresponding vector field, \u03c4, in a norm defined by p-norms. The goal is to find deformations that change the output label of a classifier from the original label. Deformations in images are measured by the vector field size \u03c4 in a p-norm. The function g is approximated linearly around the zero vector field. By solving a scalar equation in the least-squares sense, we can select \u03c4 with small norm to change the classifier's output label. Deformations in images are measured by the vector field size \u03c4 in a p-norm. The solution to the least-squares equation provides a smooth deformation of the image x. Iterating the deformation process continues until the deformed image is misclassified. The iteration process involves defining the iteration as DISPLAYFORM10, terminating and outputting an adversarial example y = x DISPLAYFORM11, introducing an overshoot factor on the total deforming vector field, and selecting the target label k to minimize the vector field for a better approximation in the linearization. The candidate set for labels is based on the smallest entries of F \u2212 F l in absolute value. The deforming vector field concentrates on edges in the image x, ensuring valid image results without violating pixel value bounds. ADef algorithm performance is evaluated on MNIST and ImageNet datasets, with results summarized in tables. MNIST-A network consists of two convolutional layers. The network MNIST-A has two convolutional layers with specific sizes and activation functions, while MNIST-B has different layer configurations with dropout during training. ADef is used to create adversarial deformations during testing by pursuing incorrect labels. ADef performs smoothing with a Gaussian filter, uses bilinear interpolation for pixel intensities, and overshoots at decision boundaries. It generates adversarial deformations for images from the ILSVRC2012 validation set using pretrained models like Inception-v3 and ResNet-101. Images are preprocessed by scaling and center-cropping before applying ADef. The algorithm ADef focuses on the label of second highest probability, uses a Gaussian filter, bilinear interpolation, and an overshoot factor. It aims to generate successful adversarial examples by producing small deformations that do not displace pixels too far from their original positions. The success rate of ADef depends on the choice of epsilon (\u03b5), with a threshold set at \u03b5 = 3. The ADef algorithm focuses on generating adversarial examples with small deformations that do not displace pixels too far from their original positions. Despite perturbations larger than common constraints, ImageNet images can be deformed into adversarial examples that are visually similar to the originals. Additional examples are provided in appendices, and MNIST classifiers are trained using adversarial training procedures. The ADef algorithm generates adversarial examples with small deformations, providing increased robustness against adversarial perturbations. Networks trained against PGD attacks are more resistant to adversarial deformations than those trained against ADef. ADef can also be used for targeted adversarial attacks by restricting the deformed image to have a particular target label. In this work, a new efficient algorithm called ADef is proposed for constructing adversarial attacks on DNN image classifiers. The algorithm uses iterative gradient descent steps to deform images, fooling state-of-the-art classifiers with almost imperceptible changes. This vulnerability of neural networks to deformations raises concerns about the effectiveness of training on specific adversarial examples as a defense strategy. Future work will explore the implications of ADef further. In future work, the effectiveness of ADef in designing defense strategies against adversarial attacks on neural networks will be studied. Initial results show that PGD trained networks on MNIST are more resistant to deformations compared to ADef trained networks. Further tests on ImageNet are needed for conclusive results. Figures 5 and 6 display the distribution of deforming vector fields from experiments. In the experiments, adversarially trained networks show more robustness against deformations. ImageNet is more vulnerable than MNIST to adversarial deformations. The standard deviation of the Gaussian filter in ADef impacts the resulting vector field. Further experiments on Inception-v3 with different standard deviations were conducted. The effect of varying the standard deviation (\u03c3) in ADef on adversarial distortions is analyzed. Increasing \u03c3 leads to higher distortion and lower success rate. The constraint \u03c4 * T \u2264 3 may be conservative for smooth high dimensional vector fields. Adversarial deformations for models MNIST-A and MNIST-B are shown, with some features resembling the target class. In figures 11-20, deformed images attacking Inception-v3 and ResNet-10 models are shown with a targeted attack on the 50th highest probability label. Deformed image: hartebeest."
}