{
    "title": "S1g2JnRcFX",
    "content": "Mini-batch stochastic gradient descent (SGD) is commonly used for large scale distributed training, but it often faces communication bottlenecks due to network delays and bandwidth limits. Recent works suggest reducing communication frequency with algorithms like local SGD, which runs SGD independently on different workers and averages the sequences occasionally. This approach has shown promising results in practice but lacks thorough theoretical analysis. Local SGD achieves linear speed-up in the number of workers and mini-batch size compared to mini-batch SGD, with reduced communication rounds up to a factor of T^{1/2} in asynchronous implementations. Local SGD can be used for large scale training of deep learning models, aiming to explore theoretical and practical aspects. Stochastic Gradient Descent (SGD) consists of iterations with iterates x t , x t+1 \u2208 R d , learning rate \u03b7 t > 0, and stochastic gradient g t \u2208 R d with E g t = \u2207f (x t ). Parallel SGD can be achieved by computing an average of stochastic gradients independently on separate workers. Communication bottleneck is a major issue in large scale deep learning applications. In large scale deep learning applications, the communication bottleneck is a major issue. Mini-batch parallel SGD increases compute to communication ratio by having each worker compute a mini-batch before communication. Recent work explores limitations of this approach, reporting performance degradation for large mini-batch sizes. An alternative approach is to reduce communication frequency by allowing sequences to evolve locally on each machine and only averaging them occasionally (local SGD). An extreme instance of local SGD is one-shot SGD, where local sequences are exchanged only once after convergence. Practical schemes involve more frequent averaging of parallel sequences for various training tasks like perceptron training, deep neural networks, and federated learning. The optimal frequency of communication rounds remains a theoretical challenge without a concise answer. Local SGD involves running parallel mini-batch SGD sequences that are synchronized by averaging after a certain number of iterations. This frequent synchronization leads to a linear speedup in convergence rate, making local SGD as efficient as parallel mini-batch SGD in terms of computation while reducing communication costs. The method is particularly effective for finite-sum convex optimization problems. Local SGD involves running parallel mini-batch SGD sequences that are synchronized by averaging after a certain number of iterations. The communication rounds can be reduced without affecting convergence by adjusting the synchronization delay. Increasing mini-batch size or communication interval can improve the compute to communication ratio. Increasing the mini-batch size or communication interval in local SGD can improve the compute to communication ratio. Practical guidelines are provided for choosing the parameters, and the technique can potentially be applied to analyze other SGD variants. While convergence guarantees are not yet provided for the non-convex setting, the positive results presented here may lead to further investigation of local SGD. The positive results of local SGD may spark further investigation in the application. Techniques like compressing stochastic gradients, sparsification methods, and asynchronous updates can reduce communication costs and improve performance. The analysis shows that asynchronous SGD can tolerate delays up to O(T/K) on convex functions, similar to local SGD. Asynchronous SGD maintains a synchronized sequence with delayed gradients, while local SGD evolves different sequences for each worker. Among the first theoretical studies of local SGD in the non-convex setting show linear speedup for certain constraints. Local SGD with averaging in every step is equivalent to mini-batch SGD. Batch sizes for mini-batch SGD are asymptotically optimal, but may not be practical. Local SGD with averaging only at the end is identical to one-shot SGD. Local SGD with averaging only at the end is equivalent to one-shot SGD. Our upper bounds become loose as the number of local updates approaches the total number of updates. We introduce local SGD formally in Section 2, provide a convergence sketch in Section 3, and present numerical results in Section 4. Asynchronous local SGD is analyzed in Section 5. Technical proofs, experimental setup details, and implementation guidelines are deferred to the appendix. The algorithm local SGD generates parallel sequences with synchronization indices. The sequences evolve based on stepsizes and synchronization intervals. The synchronization can occur every iteration or only at the end, known as one-shot averaging. The gap of a set of integers is defined as the maximum difference between consecutive integers. Parallel SGD converges at a rate O \u03c3 2 T 5 on strongly convex and smooth functions with carefully chosen stepsizes. For local SGD, capitalizing on the convexity of the objective function is not sufficient for convergence. The averaged iterate of K independent SGD sequences converges at rate O \u03c3 2 T, indicating no speedup can be achieved. Local SGD aims to decrease the variance \u03c3 2, similar to parallel SGD, by averaging stochastic gradients to reduce variance by a factor of K. Theorem 2.2 states conditions for convergence, while Corollary 2.3 provides an asymptotic result. Corollary 2.3 states the asymptotic result for local SGD with mini-batch computation, reducing variance by a factor of b. The convergence rate is O(1/(KTb)) for T large enough, achieving linear convergence. Local SGD achieves linear speedup in the number of workers K and mini-batch size b, converging at a rate of O(1/(KTb)). This results in a reduction of communication rounds by a factor of O(T/(Kb)), improving time-to-accuracy with more frequent communication at the beginning of optimization. Increasing communication frequency is a good strategy to reduce communication while maintaining the constraint H = O(T/(Kb)). The proof shows that the virtual sequence behaves like mini-batch SGD and the true iterates do not deviate much from it. The rate is obtained by exploiting a technical lemma. Bounding the variance and deviation by deriving upper bounds on E g t \u2212\u1e21 t 2 and imposing conditions on I T and stepsize \u03b7 t. Optimal averaging scheme with quadratically increasing weights for optimal convergence rate. The proof of the theorem is a reformulation of Lemma 3.3 in BID35. Theoretical speedup S(K) is shown through numerical experiments in a distributed setting, considering gradient computations and communication time. Communication is typically more costly than a single gradient computation, denoted by a factor \u03c1 \u2265 1. Theoretical speedup S(K) of local SGD on K machines compared to SGD on one machine is examined, showing that increasing H can reduce negative scaling effects due to parallelization. The optimal values of H change with the number of workers K, with H = 1 never being optimal for a small number of workers. Experimental results suggest that adaptively increasing the number of workers over time is a good strategy when the time horizon is unknown. The study focuses on a logistic regression problem with a regularization parameter set to \u03bb = 1/n using the w8a dataset. The best stepsize is determined through extensive grid search for each configuration of (H, K, B). The study focuses on determining the best stepsize (H, K, B) through grid search for logistic regression with regularization parameter \u03bb = 1/n using the w8a dataset. Results show that large values of H are advisable for training with many data passes, but for smaller T, the O(1/ \u221a K) dependency is significant. Asynchronous local SGD is presented as a solution for deploying the algorithm on massively parallel systems. Asynchronous local SGD allows for synchronized sequences without communication bottlenecks. Load-balancing techniques optimize performance in heterogeneous settings. Each worker evolves a local sequence independently, leading to convergence rates similar to synchronous SGD. SGD converges with rate O G 2 KT, proving convergence of synchronous and asynchronous local SGD. Local SGD achieves linear speedup on strongly convex functions when parallelized among K workers, saving up to O(T 1/2) in global communication rounds compared to mini-batch SGD. Future research could focus on deriving more concise convergence rates for local SGD, exploring bias and variance terms, relaxing assumptions, and investigating data dependence. The theory can be extended to non-convex objective functions without limitations. Recent work has shown convergence of SGD to a stationary point for non-convex optimization problems. The author acknowledges help in spotting typos and expresses gratitude for support. In this section, the proof of Lemma 3.2 and Lemma 3.3 is provided, along with the introduction of the virtual sequence. The assumption about all workers completing their updates by the algorithm's termination is discussed, and the tracking of weighted averages on each worker is explained. Algorithm 2 allows for load balancing in heterogeneous settings by enabling workers to switch to computing updates for other sequences if needed. This flexibility was not possible in the synchronous model. The procedure used to generate figures in the report is detailed, along with empirical examination of speedup on a logistic. In this report, the empirical examination focuses on the speedup of a logistic regression problem using the w8a dataset. The regularization parameter is set to \u03bb = 1/n, and convergence is proven for a special weighted sum of iterates. Different weighted averages are evaluated to reach a target accuracy, with specific formulas provided. The report focuses on speeding up logistic regression with the w8a dataset. Different weighted averages are tested with specific formulas provided for step sizes and parameters optimization through grid search."
}