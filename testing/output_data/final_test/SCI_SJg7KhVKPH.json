{
    "title": "SJg7KhVKPH",
    "content": "State of the art sequence-to-sequence models for large scale tasks perform a fixed number of computations for each input sequence regardless of difficulty. This paper introduces Transformer models that can predict output at different network stages and explore methods to estimate computation needed for a sequence. Unlike Universal Transformers, which use the same layers iteratively, this approach applies different layers at each step to adjust computation and model capacity. Our approach on IWSLT German-English translation matches the accuracy of a well-tuned baseline Transformer with fewer decoder layers. Modern neural sequence models can have billions of parameters, but our proposed Transformers adapt the number of layers to each input for a good speed-accuracy trade-off at inference time. Our work extends Graves (2016; ACT) by applying different layers at each stage for dynamic computation in recurrent neural networks. We explore various designs and training targets for the halting module, supervising through simple oracles for good performance on large-scale tasks. Universal Transformers (UT) utilize ACT for dynamic computation, repeatedly applying the same layer. We introduce mechanisms to estimate network depth and apply a different layer at each step, demonstrating improved speed without accuracy loss by varying the number of steps. UT's use of a layer with as many weights as a standard Transformer impacts speed due to multiple applications. Our approach extends the resource-efficient object classification work to structured prediction, exploring dynamic computation decisions. We encode input sequences with a standard Transformer encoder and experiment with approaches to address challenges in self-attention for dynamic computation. The approach investigates mechanisms to control computation in the decoder network for sequence generation, achieving up to 76% less computation while maintaining performance on translation tasks. The model presented in the study can make predictions at different layers using the Transformer sequence-to-sequence model. The encoder and decoder networks consist of stacked blocks with sub-blocks and skip-connections. The decoder generates the target sequence step-by-step based on the source sequence processed by the encoder. The decoder in the Transformer model processes input tokens step-by-step through N decoder blocks to generate hidden states. Output classifiers are attached to each decoder block to enable predictions at different stages of the network, allowing for dynamic computation. The decoder in the Transformer model processes input tokens through N decoder blocks to generate hidden states. Output classifiers enable predictions at different stages, allowing for dynamic computation. The model can choose different output classifiers at each time-step, resulting in various output classifier combinations. Two training methods are considered: aligned training optimizes all classifiers simultaneously, while mixed training exposes the model to hidden states from different layers. Aligned training optimizes all classifiers simultaneously, using a compound loss function with uniform weights for better BLEU scores. During inference, hidden states may not be available for all time-steps due to early exits, leading to a mismatch between training and testing. Mixed training reduces this mismatch by incorporating hidden states from different layers. Mixed training aims to address the mismatch between training and testing by utilizing hidden states from different blocks of previous time-steps for self-attention. Various mechanisms are presented to predict the decoder block for model termination and token output, balancing speed and accuracy. Two approaches are considered: sequence-specific depth and token-specific depth, each determining exits at different stages for optimal performance. The distribution of exiting at each time-step is modeled with a parametric distribution to compute the appropriate blocks for processing. The distribution q t is optimized to match an oracle distribution q * t with cross-entropy, and the exit loss is back-propagated to the encoder-decoder parameters. Different approaches are described for modeling the exit distribution q t and inferring the oracle distribution q * t, such as sequence-specific depth using a multinomial classifier. The text discusses a token-specific multinomial classifier that determines the exit after the first block and proceeds to the predicted depth before outputting the next token. It also mentions a token geometric-like classifier that makes binary decisions after each block to continue to the next block or stop and emit an output distribution. The exit distribution and oracle distribution are conditioned on the source sequence by feeding the average of encoder outputs to a multinomial classifier. Two oracles are considered to determine which of the N blocks should be chosen based on sequence likelihood and correctly predicted tokens at each block. The text discusses oracles based on sequence likelihood and correctly predicted tokens at each block to determine the optimal block selection. Regularization terms are used to balance speed and accuracy in choosing the block with the most correct tokens. Test metric-based oracles like BLEU are considered but computationally expensive. The token-specific approach allows for different exits at each time-step, with two options for the exit distribution. The decoder exits based on a threshold signal tuned for accuracy-speed trade-off. Two classifiers are trained to minimize cross-entropy with respect to oracle distributions. The oracle distributions for the decoder include likelihood-based and correctness-based approaches. The likelihood-based oracle selects the block with the highest likelihood plus a regularization term, while the correctness-based oracle considers the prediction correctness at each time-step. Additionally, thresholding the model predictions based on a hyper-parameter threshold is also considered. The thresholds \u03c4 are tuned on the valid set to maximize BLEU score. After 10k evaluations, the best performing thresholds are selected based on the highest BLEU in each cost segment. Evaluation is done on benchmarks like IWSLT'14 German to English and WMT'14 English to French tasks. The study focuses on training models for translation tasks using Transformer big architecture and BPE types. Training details include using Adam optimizer, batch sizes, and gradient normalization. Models are evaluated on benchmarks like IWSLT'14 De-En and WMT'14 En-Fr tasks. The study compares two training regimes for a model using a big Transformer architecture. The exit prediction classifiers are parameterized by a linear layer with the same input dimension as the embedding dimension. The models are evaluated on translation tasks using benchmarks like IWSLT'14 De-En and WMT'14 En-Fr tasks. Aligned training outperforms mixed training for fixed and randomly sampled exits in Transformer models with residual connections. It is conceptually simple, fast, and competitive with individual baseline models. In the remaining paper, aligned training is used to train models with six output classifiers. Training time only increases marginally compared to a baseline with a single output classifier. Adaptive depth classifiers are compared in terms of BLEU and computational effort. The study compares depth token-specific models trained with different oracles and sequence-specific models trained with different regularization weights. Results show that the aligned model can achieve the accuracy of a standard Transformer with half the layers. Geometric-like classifiers for token-specific halting mechanisms outperform multinomial classifiers in terms of speed-accuracy trade-off. The study compares depth token-specific models trained with different oracles and sequence-specific models trained with different regularization weights. Geometric-like classifiers for token-specific halting mechanisms outperform multinomial classifiers in terms of speed-accuracy trade-off. The correctness oracle outperforms the likelihood oracle for geometric-like classifiers, while at the sequence-level, likelihood is the better oracle. The best accuracy of the aligned model is 34.95 BLEU at exit 5, while Tok-C geometric-like achieves 34.99 BLEU at AE = 1.97 with 61% fewer decoding blocks. Confidence thresholding performs well but cannot outperform Tok-C geometric-like. In this section, the study examines the impact of two main hyperparameters on IWSLT'14 De-En: \u03bb regularization scale and RBF kernel width \u03c3. Tok-LL Geometric-like models are trained and evaluated with default thresholds. Results show that higher \u03bb values lead to lower exits, while wider kernels favor higher exits. Testing on WMT'14 English-French benchmark reveals that adaptive depth still improves, but the benefits are reduced in large-scale setups. Confidence thresholding performs well, while sequence-specific depth approaches show only marginal improvements over the baseline. The study examines the impact of hyperparameters on IWSLT'14 De-En, showing that higher \u03bb values lead to lower exits and wider kernels favor higher exits. Tok-LL geometric-like models outperform Tok-C in certain scenarios. Confidence thresholding matches N=6 baseline accuracy with fewer decoding blocks but requires significant overhead. Tok-LL geometric-like approach provides a better trade-off considering output classifier overhead. The study shows that higher \u03bb values lead to lower exits and wider kernels favor higher exits in the IWSLT'14 De-En test set. Figures 7 and 6 display outputs for the test sets, with model probabilities for each token. Less computation is needed for periods and end of sentence markers. The model uses more computation when less confident in predictions. Anytime prediction was extended to structured prediction, introducing effective methods for sequence models to make predictions at different points in the network. In this section, the study experiments with different weights for scaling the output classifier losses. Results show that a simple correctness-based geometric-like classifier achieves the best trade-off between speed and accuracy. By assigning higher weights to specific output classifiers, the number of decoder layers can be significantly reduced without sacrificing accuracy compared to a well-tuned Transformer baseline. Uniform weighting of the classifiers also yields good results. The study experiments with different weights for scaling output classifier losses, finding that a correctness-based geometric-like classifier achieves the best trade-off between speed and accuracy. By assigning higher weights to specific output classifiers, the number of decoder layers can be reduced without sacrificing accuracy. Gradient scaling is used to balance gradients in the decoder, with richer gradients for lower blocks compared to upper blocks. The study explores gradient scaling to balance gradients in the decoder, showing that scaling can benefit lower layers without significantly impacting higher layers. The experiment also involves different weights for scaling output classifier losses to optimize speed and accuracy trade-off. The FLOPS computation for the decoder network is detailed, focusing on dot-products and matrix-vector products. The total computational cost at each time-step is broken down, with key parameters and variables for estimation provided in Table 4. The study also explores gradient scaling in the decoder to balance gradients across layers. The computational cost of the decoder network is detailed, with a focus on dot-products and matrix-vector products. The cost is input-dependent with depth adaptive estimation and may not occur if tokens exit early. The average flops per token is calculated for different scenarios, including baseline and adaptive depth models. Confidence thresholding is also discussed in relation to the final output prediction cost."
}