{
    "title": "SyxtJh0qYm",
    "content": "The proposed neural probabilistic model based on variational autoencoder can be conditioned on observed features and sample the remaining features in \"one shot\". It is trained using stochastic variational Bayes and has shown effectiveness in feature imputation and image inpainting tasks. Various generative probabilistic models based on neural networks have been proposed in recent years, including variational autoencoder (VAE) and generative adversarial net (GANs). These models learn a distribution over objects and allow sampling from it, with a focus on learning conditional distributions. The paper introduces a Variational Autoencoder with Arbitrary Conditioning (VAEAC) model for learning conditional distributions of the form p(x I |x U \\I ), where U is the set of all features and I is its arbitrary subset. This model allows conditioning on an arbitrary subset of features, affecting the prior on latent Gaussian variables used for generating unobserved features. The proposed model, VAEAC, allows conditioning on an arbitrary subset of features, affecting the prior on latent Gaussian variables. It is trained using stochastic gradient variational Bayes. Two main applications are feature imputation and image inpainting, where missing features are restored and unobserved parts of images are filled in realistically. Experimental evaluation shows successful sampling from conditional distributions. The VAEAC model is effective in feature imputation and image inpainting tasks, outperforming current techniques in terms of quality. The paper discusses related works, variational autoencoders, the model definition, training procedure, evaluation, and concludes in different sections. The VAEAC model is compared to the Universal Marginalizer and GAIN models in terms of feature imputation. GAIN does not use unobserved data during training, making it easier for missing features imputation, but becomes a disadvantage with high missingness rates at the testing stage. At the testing stage, the missingness rate is high, impacting the performance of GAIN in imputing missing features. VAEAC outperforms GAIN by learning the conditional distribution over MNIST digits with one horizontal line of the image. Various methods, such as filling missing data with noise and using Markov chains, have been proposed to approximate the true conditional distribution of unobserved features. BID0 suggests a computationally expensive LSTM-based model for missing feature imputation. Image inpainting is a classic computer vision problem, with earlier methods relying on local and texture information. Recent approaches use neural networks with adversarial, reconstruction, texture, and other losses. VAEAC is a model similar to GANs, focusing on face inpainting using adversarial and semantic parsing losses. VAEAC is a model similar to GANs, focusing on face inpainting using adversarial and semantic parsing losses. It is a \"single-shot\" method with low computational cost compared to other high testtime computational complexity methods. Variational autoencoder (VAE) is a generative model with latent variables, where a latent variable z is generated from the prior distribution p(z), and then the data x is generated from the generative distribution p \u03b8 (x|z). The generative model's parameters \u03b8 induce the distribution p \u03b8 (x) = E p(z) p \u03b8 (x|z), modeled by a neural network. Parameters \u03b8 are optimized by maximizing likelihood of training data from true data distribution p d (x). Variational lower bound is optimized efficiently using backpropagation and stochastic gradient descent, with q \u03c6 (z|x) approximating the posterior p(z|x, \u03b8) using a Gaussian distribution. The reparameterization trick is used to compute the gradient of the variational lower bound with respect to \u03c6. The reparameterization trick is used to optimize the variational lower bound with respect to \u03c6 in a conditional variational autoencoder (CVAE). CVAE approximates the conditional distribution p d (x|y) and outperforms deterministic models in multi-modal scenarios. It learns the distribution of x, allowing for diverse and realistic object sampling. The Conditional Variational Autoencoder (CVAE) uses a variational lower bound derived similarly to VAE, with three trainable neural networks compared to VAE's two. Modifications like Gaussian stochastic neural network and hybrid model are proposed but not used due to described disadvantages. Unobserved features of an object are described using a binary mask, with observed features denoted as x 1\u2212b. The model p \u03c8,\u03b8 (x b |x 1\u2212b , b) aims to build a conditional distribution for unobserved features x b, using parameters \u03c8 and \u03b8. The distribution p(b) is introduced to define different unobserved feature masks, allowing for precise modeling for some features and less precision for others. This distribution can be arbitrary and should have full support over {0, 1}D to evaluate arbitrary conditioning. The model introduces a log-likelihood objective function for specific conditioning. It includes special cases like variational autoencoder and conditional variational autoencoder. The generative process involves generating z and sampling unobserved features x b. The model distribution over unobserved features is determined by a Gaussian distribution. The latent vector z components are conditionally independent given x. The latent vector components are conditionally independent given x. To use architectures like multi-layer perceptron and convolutional neural network, x 1\u2212b is considered as x \u2022 (1 \u2212 b) with fixed length. The generative network output has a fixed length, and only unobserved components are used to compute likelihood. The model's theoretical analysis is in appendix B.1, with a variational lower bound optimization problem derived for log p \u03c8,\u03b8 (x b |x 1\u2212b , b). A fully-factorized Gaussian proposal distribution q \u03c6 is used for the reparameterization trick and to compute KL divergence analytically. During optimization, the parameters of the prior distribution of z may tend to infinity, leading to potential numerical instabilities. To prevent this, a Normal-Gamma prior is imposed on the parameters, adding regularizers to the model log-likelihood. Hyperparameters \u03c3 \u00b5 and \u03c3 \u03c3 are chosen accordingly. This distribution, close to uniform near zero, minimally impacts the learning process. The proposal network in the optimization process deals with missing features by redefining the mask distribution as conditioned on x. This allows the network to determine which features are real and which are missing, by omitting the missing features in the reconstruction loss. The VAEAC model uses additional missing features mask in the proposal network to address missing data. Performance is validated using real-world datasets, including UCI datasets for missing features imputation and CelebA dataset for image inpainting. Comparison is made with classical methods like MICE and MissForest, as well as GANs-based method GAIN. In image inpainting, VAEAC's results are compared with other models based on peak signal-to-noise ratio. The VAEAC model evaluates the quality of imputations on datasets with missing features. It uses a missing features mask in the proposal network and compares results with classical methods like MICE. The evaluation includes datasets from the UCI repository with randomly dropped values for training and testing. The code for VAEAC is available on GitHub. The VAEAC model evaluates imputation quality on datasets with missing features by using MICE, GAIN, and MissForest. Imputations are sampled from the learned distribution, increasing dataset size. NRMSE is reported for continuous datasets in experiments. See appendices for more details. The VAEAC model can learn joint data distribution for missing feature imputation, showing competitive performance with state-of-the-art methods in terms of RMSE, PFC, regression R2-score, and classification accuracy. The image inpainting problem involves restoring unobserved pixels in a natural way, aiming to capture a distribution of all possible inpaintings rather than just one most probable inpainting. Unlike previous methods, VAEAC focuses on generating sharp and realistic samples using adversarial losses. In the current work, VAEAC focuses on generating diverse and realistic image inpaintings using adversarial losses. The model's performance is demonstrated on binarized MNIST, Omniglot, and CelebA datasets. The research highlights the lack of consideration for diverse inpainting in modern papers and introduces peak signal-to-noise ratio (PSNR) as a metric for evaluation. The study compares the performance of VAEAC in generating image inpaintings with other models using peak signal-to-noise ratio (PSNR) as a metric. Results show that VAEAC outperforms competing methods in terms of PSNR for most proposed masks, even with one sample. The best PSNR over 10 inpaintings is also higher than competing models for diverse inpaintings. The study shows that VAEAC performs well in generating image inpaintings compared to other models, achieving high PSNR scores. VAEAC can solve inpainting problems effectively, even with limited samples, and outperforms other methods in generating diverse inpaintings. The Universal Marginalizer model is used to estimate marginal distributions over unobserved features by optimizing a specific objective. The proposed modification of the Universal Marginalizer training procedure allows efficient learning for various masks, including those discussed. Skip-connections are found to be beneficial in image inpainting, improving log-likelihood and image realism by allowing local information to pass through them. The neural networks used for image inpainting have He-Uniform initialization of convolutional ResNet blocks and skip-connections implemented using concatenation. The proposal network structure is the same as the prior network except for skip-connections. Simple fully-connected networks with one hidden layer can also be used for inpainting on MNIST. The dataset is split into a 3:1 train-test ratio, with 50% of values randomly dropped before training. Each experiment is repeated 5 times with different splits and dropped features, then averaged. The experiments were conducted multiple times with various train-test splits and dropped features, with results averaged and standard deviation computed. The model performed better when learning the concatenation of object features x and targets y. Y was treated as an additional unobserved feature during testing. Training involved using a specific distribution and normalizing real-valued features. For regression, R2-score was reported using averaging for combination, while accuracy was reported for classification. The experiments involved using averaging as a combination method for classification problems to report accuracy. Imputed values of y were not fed to the classifier or regressor for a fair comparison of feature imputation quality. MNIST and Omniglot datasets were used with specific reconstruction loss functions and masks for feature imputation. The study used a random rectangular mask for image inpainting with 32 latent variables. The CelebA dataset consists of color images of celebrities. The reconstruction loss was based on a fully-factorized Gaussian distribution. The mask was used to sample unobserved regions in the images. In Li et al. FORMULA1, six different masks O1-O6 are used for testing, with coordinates provided in table 6 and visualizations in FIG0. A rectangle mask with random corners is used for training, rejecting masks with width or height less than 16pt. 64 latent variables are used, with the best model selected over 50 epochs based on validation IWAE log-likelihood estimation. Higher PSNR values can be achieved by using only masks O1-O6 during training. In Yeh et al. (2017), four types of masks are used: center mask, half mask, and random mask with specific characteristics. The generation process involves using a pixelwise-independent Bernoulli distribution to create a mask of unobserved pixels. A 600x600 image is generated with a uniform distribution, interpolated to 10000x10000, and masked using a Heaviside step function. Masks are sampled from this binary image, with specific criteria for acceptance. The same mask distribution is used for training and testing in VAEAC with 64 latent variables. The best model is selected based on validation IWAE log-likelihood estimation over 50 epochs. For missing feature imputation, GAIN was re-implemented in PyTorch based on the paper by Yoon et al. (2018). One-hot encoding is used for categorical features, showing better performance in terms of NRMSE and PFC compared to processing them as continuous features. Reconstruction loss is enforced for categorical features to ensure equal contribution to the overall loss. Additionally, a modification of the loss function is applied for binary and categorical features to penalize incorrect reconstructions. In GAIN for missing feature imputation, a modification of the loss function is applied to penalize incorrect reconstructions of categorical and binary features. A mix of L2 and cross-entropy reconstruction losses with specific weights is used, resulting in better performance in terms of NRMSE and PFC. The best model selection is done using a validation set containing 5% of observed features. Using a specific hyperparameter \u03b1 = 10 and a hint distribution from a Bernoulli distribution with p = 0.01 provides better results compared to the original model. The modifications to the GAIN model for missing feature imputation include using a hint distribution with \u03b1 values of {0.1, 0.5, 1, 2, 10}. Results show that the modified GAIN provides consistently better imputations than the original. VAEAC networks can model the union of CVAE networks, allowing for similar distribution modeling. The CVAE model can approximate distributions with a mixture of Gaussians, including categorical-continuous variables. However, limitations arise from poor proposal distribution. Concatenating object features and targets leads to better results in modeling datasets. The CVAE model can approximate distributions with a mixture of Gaussians, including categorical-continuous variables. Generating data from different distributions may confuse the classifier, while filling gaps using joint distribution information can simplify the learning process. The authors did not address the relationship between unobserved components and masks distribution in the original paper. The authors observed that training UM with unobserved mask distribution p(b) is crucial for generating realistic samples. They developed a generative process for p(b) using randomly chosen permutations of unobserved components. In experiments, a uniform distribution over permutations was used. The authors propose a hybrid model, a weighted mixture of variational lower bound and a single-sample Monte-Carlo estimation of log-likelihood, called Gaussian Stochastic Neural Network. This model aims to address the gap between proposal and prior distributions in Conditional VAE. The authors introduce a hybrid model and Gaussian Stochastic Neural Network to improve segmentation accuracy compared to CVAE. They address the \"holes problem\" by differentiating between sampling z from prior and proposal distributions. The Monte-Carlo estimator and Importance Sampling estimator are asymptotically equivalent, but the former requires more samples for accurate estimation. Both methods underestimate log-likelihood with small sample sizes, but Monte-Carlo shows stronger underestimation. GSNN and hybrid models struggle with distributions having multiple local maximums, leading to blurry image samples. The hybrid model for VAEAC is derived by conditioning distributions on x and a categorical latent variable z with K values. The generator network models mapping from z to parameters v, defining the generative distribution. The optimization problem is formulated using these parameters. The optimization problem for VAEAC involves optimizing the log-likelihood of the initial model with different values of alpha. The generative process is not influenced by z, resulting in blurry images in the GSNN model. VAE uses a proposal network to separate the prior distribution into K components based on the true data distribution. The proposal network in VAEAC separates the prior distribution into K components corresponding to different modes. VAEAC can fit multiple modes, while GSNN learns their average. The hybrid model's distribution becomes blurrier as \u03b1 approaches zero. The model distribution's dependence on \u03b1 can be derived analytically or evaluated experimentally. In the next sections, VAEAC is shown to learn complex multimodal distributions of synthetic data, while GSNN and hybrid models struggle. Experimental evaluation demonstrates that GSNN hinders learning distributions with multiple local optima and increases Monte-Carlo log-likelihood estimation. GSNN increases Monte-Carlo log-likelihood estimation with a few samples and decreases Importance Sampling log-likelihood estimation. Using \u03b1 = 1 ruins multimodality of the distribution, recommending \u03b1 = 1 or close to it. VAEAC outperforms GSNN in learning distribution over inpaintings based on test loglikelihood. In experiments, VAEAC shows similar convergence speed to VAE on MNIST dataset, but is 1.5 times slower due to using three networks. MICE and MissForest outperform VAEAC, GSNN, and NN on some datasets, as random forest is a more natural structure for those datasets. VAEAC, GSNN, and NN exhibit similar imputation performance metrics. The distribution of imputations has only one local maximum for datasets from (Lichman, 2013). (Yoon et al., 2018) doesn't use unobserved data during training, making it easier for missing features imputation. However, this becomes a disadvantage when the missingness rate is high at the testing stage. GAIN fails to learn the conditional distribution for a given mask distribution p(b), but it is still considered suitable for image inpainting. GAIN can learn conditional distributions with pixel-wise independent Bernoulli distribution. UM and VAEAC samples are compared, with UM unable to learn certain distributions that VAEAC can."
}