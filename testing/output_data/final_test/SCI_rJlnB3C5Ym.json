{
    "title": "rJlnB3C5Ym",
    "content": "Network pruning is a common technique used to reduce the computational cost of deep models in low-resource settings. Pruning involves removing redundant weights while preserving important ones to maintain accuracy. Surprisingly, fine-tuning a pruned model often results in comparable or worse performance compared to training from scratch with randomly initialized weights. This finding challenges the belief that training a large, over-parameterized model is necessary for good performance. Our study suggests that pruning a neural network can lead to an efficient final model without the need for training a large, over-parameterized model. The pruned architecture itself plays a crucial role in efficiency, rather than relying on inherited \"important\" weights. Comparing with the Lottery Ticket Hypothesis, we found that optimal learning rate did not bring improvement over random initialization. Over-parameterization in deep neural networks leads to high computational cost. Network pruning is an effective technique to improve the efficiency of deep networks by reducing computational cost and memory footprint. The process involves training a large model, pruning it based on certain criteria, and fine-tuning the pruned model. Preserved weights after pruning are crucial, and fine-tuning is preferred over training from scratch. Selecting important weights accurately is an active research topic in the field. In this work, structured pruning methods show that starting with a large model is not necessary. Directly training the small target model from random initialization can achieve the same, if not better, performance. For structured pruning methods with autodiscovered target networks, training the pruned model from scratch can achieve comparable or better performance than fine-tuning. The obtained architecture may be more crucial than preserved weights. Unstructured pruning methods may achieve comparable accuracy with training from scratch on smaller datasets but not on large-scale benchmarks like ImageNet. Pruning and fine-tuning from a pretrained large model can save training time. The results suggest that the choice of hyper-parameters, data augmentation, and computation budget impact the effectiveness of network pruning algorithms. Comparing predefined and automatic pruning methods, it is found that rethinking structured pruning algorithms is necessary. Over-parameterization during training may not be as beneficial as previously believed, and inheriting weights from a larger model may lead to suboptimal results. Our results suggest that automatic structured pruning algorithms excel in identifying efficient structures and conducting implicit architecture search rather than focusing on \"important\" weights. Structured pruning methods often involve searching for the optimal number of channels in each layer, as demonstrated in our experiments in Section 5. The paper is organized with background and related works in Section 2, methodology for training pruned models in Section 3, and experimental results in Section 4 for both predefined and automatic pruning methods. Recent success of deep convolutional networks has led to increased demand for computation resources. Large model sizes, memory footprints, and high computation operations hinder the use of deep neural networks in resource-constrained settings. Various methods like weight quantization and low-rank approximation have been proposed to address these issues. Automatic pruning algorithms excel in identifying efficient structures and conducting implicit architecture search, rather than focusing on individual weights. Network pruning methods, including individual weight pruning, have gained attention for their competitive performance and compatibility. Techniques such as pruning based on weight magnitude and data-free algorithms have been proposed to obtain highly compressed models. Variational Dropout and sparse learning are also used to prune redundant weights effectively. Structured pruning methods, such as channel pruning, offer benefits without requiring dedicated hardware or libraries. Channel pruning operates at a fine-grained level within conventional deep learning frameworks, making it a popular choice. Heuristic methods like pruning based on filter weight norm and group sparsity are commonly used to facilitate the pruning process. Several methods have been proposed for channel pruning in deep learning models. These methods include imposing sparsity constraints on scaling factors, minimizing feature reconstruction error, optimizing reconstruction error of response layers, using Taylor expansion for pruning, analyzing intrinsic correlation within layers, and proposing layer-wise compensation filter pruning algorithms. These techniques aim to improve model efficiency and reduce redundancy in channel structures. Our work is related to recent studies on pruning algorithms. Mittal et al. (2018) shows that random channel pruning can perform as well as more sophisticated criteria. The Lottery Ticket Hypothesis suggests certain connections with randomly initialized weights can achieve comparable accuracy when trained in isolation. Zhu & Gupta (2018) also contribute to this field. In this work, the authors reveal that fine-tuning a pruned model with inherited weights is not better than training it from scratch. They describe their methodology for training a small target model from scratch and categorize network pruning methods into predefined and automatic architectures. When pruning channels in a target architecture, the ratio of channels to prune in each layer is a common criterion. The pruning algorithm locally prunes the least important channels, resulting in a consistent pruned architecture. The ratio in each layer is typically determined through empirical studies or heuristics. Examples of structured pruning methods include BID14, Luo et al. (2017), He et al. (2017b), and He et al. (2018a). Automatic structured pruning involves globally comparing the importance of structures across layers. Unstructured pruning methods are also utilized. In network pruning literature, datasets like CIFAR-10, CIFAR-100, and ImageNet are commonly used benchmarks. Network architectures such as VGG, ResNet, and DenseNet are popular choices. Various predefined and automatic pruning methods are evaluated, including structured methods like BID14, Luo et al. (2017), He et al. (2017b), He et al. (2018a), and unstructured pruning by Han et al. (2015). In network pruning literature, datasets like CIFAR-10, CIFAR-100, and ImageNet are commonly used benchmarks with popular architectures such as VGG, ResNet, and DenseNet. Evaluations include structured methods like BID14, Luo et al. (2017), He et al. (2017b), He et al. (2018a), and unstructured pruning by Han et al. (2015). For experiments on CIFAR datasets, 5 random seeds are used, and mean accuracy with standard deviation is reported. The training budget involves determining the appropriate number of epochs to train the small pruned model from scratch, considering the reduced computation required compared to the large model. In experiments, small pruned models are trained for the same epochs using Scratch-E, and for the same computation budget using Scratch-B. Increasing training epochs in Scratch-B involves extending learning rate decay schedules proportionally. Training the small target model for fewer epochs may converge faster, but increasing training epochs within a reasonable range is usually not harmful. Scratch-E is sufficient in most cases, while Scratch-B is used in others. In experiments, small pruned models are trained using Scratch-E and Scratch-B for the same computation budget. Scratch-E is sufficient in most cases, while Scratch-B is needed for comparable accuracy in others. The evaluation protocols follow original papers' setups, either adopting original implementations or re-implementing simpler pruning methods to achieve similar results. The accuracy of re-trained large models using Pytorch is higher than reported in original papers, possibly due to different deep learning frameworks. To account for this, relative accuracy drop from unpruned large model is reported. Standard training hyper-parameters and data-augmentation schemes are used, along with SGD optimization with Nesterov momentum and stepwise decay learning rate schedule. Random weight initialization scheme is adopted. In this section, experimental results comparing training pruned models from scratch and fine-tuning from inherited weights are presented. Different pruning methods are compared, including predefined, automatic structured pruning, and magnitude-based unstructured pruning. The results can be reproduced using the provided code link. In this study, various pruning methods were compared, including L1-norm based Filter Pruning and Soft Filter pruning. Results showed that scratch-trained models achieved similar or better accuracy compared to fine-tuned models, with Scratch-B models outperforming on ImageNet. The study compared different pruning methods, with Scratch-B models consistently outperforming fine-tuned models. Results showed that Scratch-B models achieved better performance with significantly reduced training budgets. Regression based Feature Reconstruction prunes channels by minimizing feature map reconstruction error of the next layer using LASSO regression. Pruned models like \"VGG-16-5x\" are defined, and Network Slimming imposes L1-sparsity on channel-wise scaling factors from Batch Normalization layers to automatically discover target architectures. The small models trained from scratch can reach the same accuracy as fine-tuned models, with Scratch-B consistently outperforming the finetuned models. Pruning can be done on channels, residual blocks in ResNet, or groups in ResNeXt. Scratch-E generally outperforms in residual blocks pruning. Table 5 displays the results of residual block pruning using Sparse Structure Selection BID6. Scratch-E performs better than pruned models on average, with Scratch-B outperforming both. Weight pruning is done in convolution layers for fully-convolutional network architectures. Fine-tuning is not required for the pruned models. Based on the results presented in Table 6, when the prune ratio is small (\u2264 80%), Scratch-E sometimes falls short of fine-tuned results on CIFAR datasets, while Scratch-B performs at least on par. However, for larger prune ratios (95%), fine-tuning can outperform training from scratch. On the ImageNet dataset, Scratch-B generally performs worse than fine-tuned results. The Scratch-B result is often worse than fine-tuned results by a noticeable margin, despite decent accuracy. This could be due to the difficulty of training on sparse networks like CIFAR or the complexity of datasets like ImageNet. Unstructured pruning significantly alters weight distribution compared to structured pruning. The value of architecture search for automatic network pruning algorithms is assessed by comparing pruning-obtained models and uniformly pruned models. The parameter efficiency of architectures obtained by an automatic channel pruning method (Network Slimming) is compared with a naive predefined pruning strategy. The architectures obtained by Network Slimming are more parameter efficient, achieving the same accuracy with 5\u00d7 fewer parameters than uniformly pruned architectures. The automatic channel pruning method (Network Slimming) produces architectures with higher parameter efficiency compared to a naive predefined pruning strategy, achieving the same accuracy with 5\u00d7 fewer parameters. Unstructured magnitude-based pruning results in less efficient architectures. Channel/weight pruned architectures show consistent patterns, indicating potential redundancy in original large models. Automatic pruning methods are valuable for improving efficiency in model design. The architectures obtained by pruning may not always be more efficient than uniformly pruned ones, especially on modern architectures like ResNets and DenseNets. Sparsity patterns of pruned architectures show near-uniform patterns across stages, explaining their performance. In contrast, VGG's pruned sparsity patterns consistently outperform uniform pruning. Network Slimming produces non-uniform sparsity patterns for VGG, enhancing its efficiency compared to ResNet and DenseNet. The text discusses the imbalanced redundancy in VGG compared to ResNet and DenseNet, suggesting network pruning techniques to identify redundancy better. It explores deriving generalizable design principles from pruned architectures and conducting experiments to answer this question. Techniques like \"Guided Pruning\" and magnitude-based pruning are used to construct new architectures based on sparsity patterns from pruned architectures. The text discusses deriving generalizable design principles from pruned architectures to construct new sparse models. Results show that these guided design patterns can be transferred to different architectures and datasets, leading to efficient models without the need for training large models. The text discusses network architecture search methods, including reinforcement learning and evolutionary algorithms. Traditional methods require thousands of iterations to find the goal architecture, while network pruning as architecture search only requires one-pass training. Pruned architectures obtained by different approaches are shown in Figure 6, with \"Guided Pruning/Sparsification\" using average sparsity patterns in each layer stage to design the network. Using the average sparsity patterns in each layer stage to design the network, \"Transferred Guided Pruning/Sparsification\" involves applying sparsity patterns from a pruned VGG-16 on CIFAR-10 to design the network for VGG-19 on CIFAR-100. This approach leads to better parameter efficiency, even when guidelines are transferred from a different dataset and model. Gordon et al. (2018) also utilizes a similar pruning technique to automate the design of network architectures, such as BID1 which prunes channels using reinforcement learning. In network architecture search, sharing or inheriting trained parameters during searching is a popular method for reducing training budgets, but the target architecture is still trained from scratch to maximize accuracy. Training predefined target models from scratch offers benefits over conventional network pruning procedures, including smaller model size, faster training, and avoiding the need for pruning criteria implementation and hyper-parameter tuning. Automatic structured pruning can find efficient architectures, but achieving accuracy through training the pruned model from scratch is also viable. When training predefined target models from scratch, benefits include smaller model size, faster training, and avoiding the need for pruning criteria implementation and hyper-parameter tuning. Evaluating pruned architectures against uniformly pruned baselines is important to demonstrate the method's value in identifying efficient architectures. If uniformly pruned models are not worse, one could skip the pipeline and train them from scratch. Pruning and fine-tuning may not always outperform baselines in accuracy, but can be faster in cases where a pre-trained large model is available with limited training budget or when multiple models of different sizes are needed. The Lottery Ticket Hypothesis suggests that a sub-network within a large network, along with its original initialization, is crucial for effective training. However, new research shows that random initialization is sufficient for a pruned model to achieve competitive performance, eliminating the need for reusing the original initialization. The conclusions suggest that random initialization is enough for a pruned model to perform well. Important differences in evaluation settings include the use of structured pruning methods, large modern network architectures, and momentum SGD with a high initial learning rate. In this section, the difference in learning rate is highlighted as the key factor causing differing outcomes between the current study and previous work by Frankle & Carbin (2019) on unstructured pruning on CIFAR. The study compares models trained with the original initialization (\"winning ticket\") and randomly re-initialized weights, showing that the winning ticket only improves performance with a small learning rate (0.01), which however leads to lower accuracy compared to a larger learning rate (0.1). Structured pruning with both large and small learning rates does not show a performance improvement with the winning ticket. In this study, two initial learning rates (0.1 and 0.01) are experimented with using momentum SGD for unstructured pruning. Results for unstructured pruning and L1-norm based filter pruning are shown in FIG3 and Table 9. The winning ticket as initialization only improves with a small learning rate (0.01), but this leads to lower accuracy compared to a larger learning rate (0.1). The original initialization in (Frankle & Carbin, 2019) only outperforms random initialization with a small initial learning rate of 0.01. Structured pruning like BID14 shows that the original initialization is comparable to random initialization for both large and small initial learning rates. However, small learning rates result in lower accuracy compared to widely-used large learning rates. The winning ticket only improves with unstructured pruning and a small initial learning rate, but this leads to inferior accuracy compared to larger learning rates. Frankle & Carbin (2019) also mention that the winning ticket cannot be found on ResNet-18/VGG using a large learning rate. The original initialization may be helpful with small learning rates due to the weights of the final model. Initialization is beneficial with a small learning rate as the weights of the final model may not deviate much from the original initialization. Experiments on the Lottery Ticket Hypothesis using structured pruning methods show that using winning tickets does not improve accuracy. Soft Filter Pruning prunes filters during training but allows for the possibility of recovering pruned weights. Results without using pretrained models and with pretrained models are shown in tables. The authors used code from He et al. (2018b) for obtaining the results. Results show that Scratch-E outperforms pruned models most of the time, while Scratch-B outperforms them in nearly all cases. The study demonstrates that small pruned models can match the accuracy of fine-tuned models in classification tasks. Evaluation on the PASCAL VOC object detection task using Faster-RCNN with L1-norm based filter pruning method BID14 shows promising results for transfer learning. The study compares two approaches, Prune-C (classification) and Prune-D (detection), for model transfer and pruning. Results are reported in TAB2, showing the performance of pruned models on the detection task. Prune-C involves pruning on classification pre-trained weights, while Prune-D prunes after transferring weights to the detection task. Scratch-E/B denotes pre-training the pruned model from scratch on classification and transferring to detection. The study compares Prune-C and Prune-D approaches for model transfer and pruning. Results in TAB2 show that the model trained from scratch can outperform fine-tuned models in the transfer setting. Prune-C outperforms Prune-D in detection, suggesting that pruning early in the classification stage may prevent the model from getting stuck in a bad local minimum. The study compares Prune-C and Prune-D approaches for model transfer and pruning. Results show that training from scratch can outperform fine-tuned models in the transfer setting. When the prune ratio is large, training from scratch is better than fine-tuned models by an even larger margin. Fine-tuned models are significantly worse than unpruned models in this case. ThiNet is a aggressively pruned model with reduced FLOPs and parameters. Pruning algorithms use fewer epochs for fine-tuning compared to training the large model. Fine-tuning for more epochs does not significantly improve performance. In experiments with fine-tuning models, it was observed that training for more epochs did not significantly improve accuracy. Even models trained from scratch performed similarly to fine-tuned models. The study used L1-norm filter pruning BID14 and extended the standard training schedule for CIFAR and ImageNet datasets. Results showed that fine-tuning for more epochs only marginally increased accuracy or even caused a slight decrease. When extending the training schedule of CIFAR from 160 to 300 epochs, scratch-trained models perform on par with fine-tuned models. Weight distribution of convolutional layers for different pruning methods is compared using VGG-16 and CIFAR-10. Results show the weight distribution of unpruned models, fine-tuned models, and scratch-trained models for Network Slimming BID4 and unstructured pruning methods. The weight distribution of fine-tuned and scratch-trained pruned models differs from unpruned models, indicating less redundancy in pruned architectures. Fine-tuned models show significant differences in weight distribution compared to scratch-trained models, potentially affecting accuracy. Additional results on sparsity patterns are provided in this section. In this section, additional results on sparsity patterns for pruned models are presented. The sparsity patterns of PreResNet-164, PreResNet-110, and DenseNet-40 pruned on CIFAR-10 and CIFAR-100 are discussed, showing close to uniform patterns across stages for certain prune ratios. Sparsity patterns of DenseNet-40 and VGG-16 pruned on CIFAR-100 and CIFAR-10 respectively are discussed. The patterns show close to uniform sparsity across stages for certain prune ratios, with later stages exhibiting more redundancy."
}