{
    "title": "Syl1pGI9wN",
    "content": "Sequence generation models like recurrent networks can be trained with various learning algorithms, including maximum likelihood learning and reinforcement learning like policy gradient. Other algorithms such as RAML, SPG, and data noising have also been developed. This paper establishes a formal connection between these algorithms by presenting a generalized entropy regularized policy optimization formulation. The algorithms can all be reformulated as special instances of this framework, with differences in reward function configurations and hyperparameters. This unified interpretation provides a systematic view of exploration and learning efficiency properties. The proposed algorithm dynamically interpolates existing algorithms for improved learning in tasks like machine translation, text summarization, and image captioning. It addresses the exposure bias issue in maximum-likelihood estimation training. Recent efforts have been made to address the exposure bias issue in maximum-likelihood estimation (MLE) training, with many resorting to reinforcement learning (RL) techniques. RL-based approaches aim to avoid the training/test discrepancy by using the same decoding strategy, but they can face challenges of poor sample efficiency and high variance. To strike a balance between MLE and RL, a diverse set of methods has been developed for more practical training. A variety of methods have been developed to address exposure bias in MLE training, bridging the gap between MLE and RL. For instance, RAML incorporates reward-aware perturbation, SPG leverages reward distribution for sampling, and data noising shows improved results. This paper introduces a unified perspective on learning algorithms, presenting a generalized entropy regularized policy optimization framework. MLE, RAML, SPG, and data noising can all be seen as special instances of this framework, differing only in reward choice and hyperparameter values. The paper introduces a new algorithm that gradually expands the exploration space by annealing reward and hyperparameter values as training progresses. This interpolation algorithm shows significant improvement over existing methods in machine translation and text summarization experiments. The paper introduces a new algorithm for sequence generation models, including reinforcement learning approaches like policy gradient and actor-critic. Softmax policy gradient incorporates reward distribution for high-quality sequence samples. Reward augmented maximum likelihood (RAML) goes beyond maximum likelihood by incorporating task metrics like BLEU for model learning. The paper introduces a new algorithm for sequence generation models, including reinforcement learning approaches like policy gradient and actor-critic. RAML shows that MLE and maximum-entropy policy gradient minimize KL divergences in opposite directions. SPG and RAML are instances of a general entropy regularized policy optimization framework, providing a more principled formulation for both algorithms. Other learning methods for sequence models include learning-to-search paradigm and Scheduled Sampling. Empirical comparison shows improved performance of the proposed algorithm. Policy optimization for reinforcement learning is extensively studied in robotics and game environments. Reinforcement learning is extensively studied in robotics and game environments. Various approaches, such as relative entropy regularization and trust-region methods, have been developed for policy optimization algorithms. The entropy-regularized policy optimization framework connects to a broad set of learning algorithms for sequence generation, including maximum likelihood learning. This formulation provides new insights into exposure bias issues. The curr_chunk discusses the formulation of a sequence generation model in the context of machine translation. It introduces the notations for input (x) and target sequence (y), and aims to learn a model parameterized with \u03b8. The model can be a recurrent network, and the framework subsumes other learning methods. Policy optimization in reinforcement learning aims to learn the parameter \u03b8 of the model p \u03b8 (policy) to maximize the expected reward. Entropy regularized policy optimization (ERPO) stabilizes learning by adding information theoretic regularizers. A generalized formulation of ERPO is presented, with an objective written as DISPLAYFORM0 involving Kullback-Leibler divergence and Shannon entropy. Entropy regularized policy optimization (ERPO) introduces Kullback-Leibler divergence and Shannon entropy to stabilize learning in reinforcement learning. Different forms of the distribution q lead to various policy optimization algorithms, such as relative entropy policy search and maximum entropy policy gradient. The objective can be maximized using an EM-style procedure with coordinate ascent steps optimizing q and \u03b8 iteratively. The E-step in ERPO involves optimizing q with Lagrange multipliers, where q has a closed-form solution. The weight \u03b1 encourages q to be close to p\u03b8, while the weight \u03b2 acts as the temperature of the q softmax distribution. A large \u03b2 makes q a uniform distribution. The M-step maximizes the log-likelihood of samples from q. In sequence generation, equations can be expressed at the token level. The ERPO framework decomposes rewards along time steps and has key hyperparameters (R, \u03b1, \u03b2). Different values of these hyperparameters correspond to different learning algorithms. Maximum likelihood estimation (MLE) is widely used for sequence generation but suffers from exposure bias. The ERPO framework introduces key hyperparameters (R, \u03b1, \u03b2) that correspond to different learning algorithms. Maximum likelihood estimation (MLE) is commonly used for sequence generation but faces exposure bias. MLE can be viewed as a policy optimization algorithm with specific reward and weight configurations, where any deviation from the ground-truth data results in a negative infinite reward. This reformulation provides a new statistical explanation for the exposure bias problem. The ERPO framework introduces key hyperparameters (R, \u03b1, \u03b2) for different learning algorithms. A small \u03b1 value makes the model distribution ignored during sampling, leading to poor performance at test time. The \u03b4-reward allows extreme pruning of the sample space, making MLE implementation simple and efficient. Common rewards like BLEU permit exploration in a broader space but lack the regular shape of the \u03b4-reward for sample space pruning. The ERPO framework introduces key hyperparameters for different learning algorithms, allowing for extreme pruning of the sample space. This can lead to poor performance at test time with a small \u03b1 value. The rewards in the huge sample space are sparse, making exploration inefficient. Various approaches have been developed to combine exploration and computation efficiency, all reformulated within the ERPO framework. The RAML framework, within the ERPO framework, incorporates task metric reward into training, outperforming vanilla MLE. It uses an exponentiated reward distribution and maximizes log-likelihood of perturbed data samples. RAML reduces to MLE when using MLE \u03b4-reward. The relation between MLE and RAML is maintained in the new formulation. The RAML framework incorporates task metric reward into training, outperforming vanilla MLE by maximizing log-likelihood of perturbed data samples. RAML allows for larger exploration space compared to MLE, addressing exposure bias. SPG BID8 adapts vanilla policy gradient to use reward for sampling, with a common reward R. SPG, a variant of the policy gradient algorithm, addresses exposure bias by leveraging both reward and model distribution for exploration. It moves beyond RAML by using a common reward R and requires additional optimization techniques for practical training. Adding noise to training data is a common technique for model regularization. The ERPO framework can subsume data noising as a special instance, where data is noised using a locally relaxed variant of R \u03b4. This technique replaces tokens in the data with samples from a distribution, aiding in model regularization during training. The data noising strategy involves replacing tokens with a randomly selected token, equivalent to using a reward function that expands the exploration space locally. This approach addresses exposure bias and is related to the ERPO framework, which incorporates data noising as a special case for model regularization during training. The ERPO framework connects various learning algorithms by specifying hyperparameters (R, \u03b1, \u03b2). Different points in the hyperparameter space lead to varying levels of exploration and learning efficiency. This unified perspective aids in understanding existing algorithms and developing new ones for improvement. The interpolation algorithm presented in this example exploits the idea of starting learning from a restricted problem configuration and gradually expanding the exploration space to reduce discrepancies at test time. It involves annealing hyperparameter values to transition from using a common reward to a smooth reward, and exploring by both R and p \u03b8. The algorithm presented in this example gradually adjusts hyperparameter values to transition from a common reward to a smooth reward, exploring using both R and p \u03b8. The interpolation involves introducing a categorical random variable z to determine the scalar weights in the numerator of q. The algorithm introduces a categorical variable z to determine weights in q, transforming the product of experts into a mixture resembling the bang-bang rewarded SPG method. The M-step maximizes the objective with z marginalized out, and the interpolation algorithm pseudo-code is provided in the supplements. Ranzato et al. also develop an annealing strategy mixing MLE and policy gradient training, which can be seen as a special instance of the ERPO framework presented. The ERPO framework is introduced with a categorical variable z to determine weights in q, resembling the bang-bang rewarded SPG method. The proposed algorithm shows improved performance compared to BID24 in machine translation and text summarization tasks. The algorithm consistently outperforms previous methods and uses a sequence-to-sequence model with LSTM networks. Configuration details and dataset information are provided in the supplementary materials. The dataset used for machine translation is based on the IWSLT 2014 BID5 German-English data with a final size of around 146K/7K/7K for train/dev/test. The BLEU metric is used for evaluation, showing improved performance with approaches like RAML compared to vanilla MLE. Scheduled Sampling is also compared to combat exposure bias during training. Our proposed new algorithm outperforms existing algorithms by gradually increasing exploration space for better generation results. Test-set BLEU scores in FIG2 demonstrate the algorithm's improvement with annealing, surpassing others. Using the English Gigaword corpus, we pre-processed data for text summarization, resulting in 200K/8K/2K source-target pairs. ROUGE metrics are used for evaluation, with the summation of -1, -2, and -L metrics as the reward in learning algorithms. The proposed interpolation algorithm outperforms existing algorithms in text summarization, achieving the best performance on all three ROUGE metrics. The RAML algorithm, known for machine translation, lags behind in this task. A unified perspective on various learning algorithms for sequence generation is presented, showing their equivalence in a generalized entropy regularized policy optimization framework. This principled treatment allows for systematic comparison and further improvement. The proposed interpolation algorithm shows consistent improvement in machine translation and text summarization, outperforming existing algorithms. It aims to address the exposure bias problem by exploiting the policy gradient algorithm. The framework presented allows for systematic comparison among algorithms and inspires further enhancement in various settings such as robotics and game environments. The MIXER algorithm BID24 incorporates an annealing strategy that mixes between MLE and policy gradient training, using ground-truth examples for evaluation. It can be seen as a specific instance of the proposed interpolation algorithm, addressing exposure bias in machine translation and text summarization. The MIXER algorithm BID24 incorporates an annealing strategy that mixes between MLE and policy gradient training. It addresses exposure bias in machine translation and text summarization. The interpolation algorithm follows a restricted annealing strategy for token-level hyperparameters.\u03bb1, \u03bb2, \u03bb3 are set differently for the first m steps and after. Algorithm 1 summarizes the interpolation algorithm. Training examples are obtained for each step, updating \u03b8 and annealing \u03bb until convergence. SETTINGS C.1 DATA PRE-PROCESSING: In machine translation, data pre-processing follows BID20. 200K out of 3.8M training examples from BID25 are sampled for efficiency. RAML uses n-gram replacement for sampling, drawing 10 samples per training example. Scheduled Sampling uses inverse-sigmoid decay with a hyperparameter set to 500 and 600 for machine translation and text summarization tasks, respectively. MIXER BID24 incorporates an advantage function for policy gradient. The proposed interpolation algorithm initializes with specific parameters. The proposed interpolation algorithm in machine translation adjusts weights (\u03bb 1 , \u03bb 2 , \u03bb 3 ) based on validation-set rewards. Additional results show improvement over baselines, comparable to using dropout rate of 0.2. Our algorithm outperforms MLE by 1.5 BLEU points and RAML by 0.49 BLEU points. Results with dropout 0.2 show improvements of 1.42 BLEU and 0.64, respectively. Testing with dropout 0.5 yielded similar results. The proposed interpolation algorithm shows clear superiority over existing methods. Convergence curves of comparison algorithms are illustrated in FIG6. Model BLEU scores are as follows: MLE 26.63, RAML 27.64, SPG BID8 26.89, MIXER BID24 27.00, Scheduled Sampling 27.03, and Ours 28.13. Results of machine translation with dropout at 0.3 are presented in Table 3. The weights (\u03bb 1 , \u03bb 2 , \u03bb 3) are adjusted based on validation-set rewards."
}