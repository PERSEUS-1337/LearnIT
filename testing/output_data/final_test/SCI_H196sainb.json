{
    "title": "H196sainb",
    "content": "State-of-the-art methods for learning cross-lingual word embeddings have evolved from relying on bilingual dictionaries or parallel corpora to utilizing character-level information. This approach allows for building a bilingual dictionary between two languages without the need for parallel data supervision. The model even outperforms existing supervised methods on cross-lingual tasks for some language pairs, including distant ones like English-Russian or English-Chinese. Additionally, experiments on the English-Esperanto low-resource language pair show promising results. Experiments on the English-Esperanto low-resource language pair demonstrate the potential impact of unsupervised machine translation methods. The skip-gram with negative sampling method factors a word-context co-occurrence matrix to create word vectors reflecting semantic similarities. Word embeddings exhibit similar characteristics in continuous spaces. Recent studies have shown that continuous word embedding spaces have similar structures across languages, even distant ones like English and Vietnamese. Researchers have proposed a method to learn a linear mapping from a source to a target embedding space using a parallel vocabulary of anchor points. Various approaches have been developed to improve cross-lingual word embeddings, some of which rely on bilingual word lexicons. Recent attempts have been made to reduce the need for bilingual supervision by using identical character strings to create a parallel vocabulary. Recent methods in cross-lingual word embeddings have limitations with similar languages sharing a common alphabet. Some recent approaches like distribution-based and adversarial training aim to obtain cross-lingual word embeddings without parallel data, but their performance is below supervised methods. In contrast, a new model introduced in this paper outperforms supervised methods without using cross-lingual annotated data, leveraging adversarial training on two large monolingual corpora. The paper introduces an unsupervised approach for cross-lingual word embeddings that outperforms supervised methods without using parallel data. The method involves training a discriminator and a mapping generator in a two-player game, extracting a synthetic dictionary, and fine-tuning the mapping with a Procrustes solution. An unsupervised selection metric is introduced to select the best model and hyper-parameters. The method presented in the paper outperforms supervised approaches in cross-lingual word embeddings without using parallel data. It introduces a cross-domain similarity adaptation to address the hubness problem and proposes an unsupervised criterion for mapping quality evaluation. The paper introduces a method for learning cross-lingual word embeddings without parallel data. It includes high-quality dictionaries for 12 language pairs and demonstrates effectiveness in low-resource languages like English-Esperanto. The approach involves unsupervised training with adversarial training and model selection. Results on cross-lingual tasks are compared to supervised methods, highlighting differences from recent related work. The paper focuses on learning cross-lingual word embeddings by mapping two sets of embeddings trained independently on monolingual data. It utilizes a known dictionary of word pairs to learn a linear mapping between the source and target space. The translation of a source word is determined by maximizing the cosine similarity between the mapped source embedding and target embeddings. The paper discusses learning cross-lingual word embeddings by aligning English and Italian word embeddings using adversarial learning and Procrustes refinement. The alignment is achieved by a rotation matrix W and a distance metric called CSLS. The paper introduces a method, dubbed CSLS, to expand the space of high-density points in word embeddings. Results show better performance on word translation tasks with a simple linear mapping. The approach involves learning a mapping W without cross-lingual supervision using an adversarial criterion. The paper introduces a method called CSLS to expand high-density points in word embeddings. It involves learning a mapping W without cross-lingual supervision using an adversarial criterion. The approach includes training a discriminator to distinguish between source and target language embeddings, with W aiming to prevent accurate predictions. The method CSLS aims to maximize identification of embedding origin by learning a mapping W to make source and target embeddings similar. The discriminator's objective is to predict the origin of embeddings, with a loss function to train W to prevent accurate predictions. The model is trained using deep adversarial networks following a standard procedure. The deep adversarial networks of BID17 involve training the discriminator and mapping matrix W with stochastic gradient updates to minimize L D and L W respectively. The matrix W obtained through adversarial training shows good performance but still lags behind the supervised approach. Rare words are harder to align due to their less frequent updates and varied contexts. To improve mapping accuracy, a synthetic parallel vocabulary is built using the learned W. To refine mapping accuracy, a synthetic parallel vocabulary is created using adversarial training. The Procrustes solution is applied to this dictionary for improved results. Iterative application of this method shows minimal enhancements, with improvements typically below 1%. The goal is to establish reliable matching pairs between languages by enhancing the comparison metric for better word translations. In order to improve word translations between languages, solutions have been proposed to address the issue of hubness, where certain vectors are more likely to be nearest neighbors of many other points. This problem has been observed in various applications, such as matching image features and translating words. Existing solutions aim to mitigate hubness by considering a single feature distribution, but in the case of two domains, additional methods are needed. In contrast to existing methods that consider a single feature distribution, we propose a bi-partite neighborhood graph approach for word translations between languages. This method connects each word in one language to its K nearest neighbors in the other language, addressing the issue of hubness observed in various applications. The method proposed connects words in one language to their nearest neighbors in the other language, addressing hubness issues. It defines a similarity measure between mapped source and target words, improving word translation retrieval accuracy without parameter tuning. The method uses unsupervised word vectors trained on Wikipedia corpora, with a size of 300 \u00d7 300. Words appearing less than 5 times are discarded. The discriminator has two hidden layers of size 2048 with Leaky-ReLU activation functions. Dropout noise with a rate of 0.1 is applied to the input. A smoothing coefficient of s = 0.2 is included in the discriminator predictions. Stochastic gradient descent is used with a batch size of 32, a learning rate of 0.1, and a decay of 0.95 for both the discriminator and word vectors. The learning rate is halved when the unsupervised validation criterion decreases. Rare words have lower embedding quality compared to frequent words, so only the top 50,000 words are fed to the discriminator. Sampling word embeddings uniformly at each training step is effective. Using an orthogonal matrix for the linear operator improves performance by preserving the quality of embeddings. In this work, a simple update step is proposed to ensure that the matrix W stays close to an orthogonal matrix during training. The method alternates the update of the model with a specific update rule on the matrix W, ensuring it stays close to the manifold of orthogonal matrices. The refinement step involves generating a new dictionary at each iteration and applying the Procrustes solution on correct word pairs using the CSLS method to select accurate translation pairs. To improve the quality of the dictionary, mutual nearest neighbors are considered using CSLS, reducing its size but enhancing accuracy. Model selection in unsupervised settings is challenging without a validation set, so an unsupervised criterion is used to measure source and target embedding space closeness. The top 10k source words are considered, and CSLS is used to find translations, with average cosine similarity computed for evaluation. The effectiveness of an unsupervised approach for cross-lingual word embeddings is demonstrated through empirical evidence on various benchmarks. The average cosine similarity between translations is used as a validation metric, showing a strong correlation with evaluation tasks. This criterion is utilized for training stopping and hyperparameter selection. The approach is compared to state-of-the-art supervised methods, showcasing its performance. In the appendix, a high-quality bilingual dictionary is created for various language pairs to address the issue of word polysemy in existing dictionaries generated by online tools like Google Translate. The MUSE library provides bilingual dictionaries with up to 100k word pairs, publicly available for comparison with previous approaches. Evaluation includes precision@k for source and target words, as well as cross-lingual word similarity tasks using SemEval 2017 competition data. The study utilizes the MUSE library's bilingual dictionaries for word translation retrieval, reporting results on sentence retrieval precision@k using bag-of-words aggregation methods on the Europarl corpus. The idf-weighted average is used to merge word into sentence embeddings, with results compared to previous work. In experiments using MUSE bilingual dictionaries, the study outperformed previous approaches in sentence translation retrieval and cross-lingual word similarity tasks. Different similarity measures were combined with a supervised baseline, showing stable performance with a single parameter defining neighborhood size. In experiments using MUSE bilingual dictionaries, the study outperformed previous approaches in sentence translation retrieval and cross-lingual word similarity tasks. The Procrustes-CSLS approach showed a strong gain in performance across all language pairs, with up to 7.2% improvement in accuracy. It outperformed previous models on the English-Italian word translation task, achieving an accuracy of 44.9%. Additionally, in the Italian-English sentence retrieval task, the approach improved accuracy from 53.5% to 69.5%, surpassing previous methods by more than 20%. The study found a significant performance boost in word translation tasks using fastText embeddings trained on Wikipedia compared to CBOW embeddings from WaCky datasets. The gain in accuracy was mostly due to the change in corpora, with fastText embeddings incorporating more syntactic information. This resulted in a 2% increase in accuracy out of an 18.8% gain. The similar co-occurrence statistics of Wikipedia corpora were hypothesized to be the reason for this improvement. Results from monolingual evaluation tasks also showed better performance when using embeddings trained on Wikipedia. The adversarial approach in learning cross-lingual embeddings without parallel data shows strong performance on language pairs like es-en and en-fr, achieving P@1 of 79.7% and 77.8%. Systems also perform well on distant languages like en-ru and en-zh, where methods relying on identical character strings are not applicable. The method exploits similarities obtained with CSLS to build a synthetic vocabulary, showing significant accuracy gains compared to the Procrustes method. The adversarial approach in learning cross-lingual embeddings without parallel data shows strong performance on language pairs like es-en and en-fr. The refinement step on the synthetic bilingual vocabulary constructed after adversarial training brings a significant gain in performance, outperforming the supervised baseline on en-it and en-es. The method can retrieve the correct translation of a source word with up to 83% accuracy. The unsupervised approach outperforms the supervised baseline in cross-lingual word translation tasks. The strong similarity in cooccurrence statistics between languages contributes to the better performance. The refinement step can potentially use more anchor points, leading to a significant gain in accuracy. The quality of the generated dictionary has a significant impact on the BLEU score, with up to a 15% improvement in sentence retrieval using bag-of-words embeddings. Our method shows a significant impact on the BLEU score for sentence retrieval using bag-of-words embeddings, especially for low-resource languages like English-Esperanto. The unsupervised approach achieves 28.2% on English-Esperanto and 25.6% on Esperanto-English, outperforming the supervised method. The dictionary used for the language pair lacks consideration for word polysemy, leading to lower results. People commonly use P@5 to address this issue. The impact of a dictionary on machine translation is shown by applying it to the English-Esperanto Tatoeba corpora BID44. Using word-by-word translation, the BLEU score with CSLS method achieves 11.1 and 14.3 on English-Esperanto and Esperanto-English respectively. Translated sentences convey meaning but contain some errors. The translations in the text contain errors, such as \"mi\" being translated as \"sorry\" instead of \"i\". Improving translations using a language model is possible. Previous works have focused on bilingual lexicon induction without parallel corpora, utilizing statistical similarities between languages to create small dictionaries. These methods require a seed bilingual lexicon for closely related languages. Recent studies in statistical decipherment show that word embeddings can significantly improve machine translation by aligning embedding spaces. Cross-lingual word embeddings are used to extract bilingual lexicons and for various cross-lingual language processing systems. Approaches have been proposed to learn bilingual dictionaries mapping from the source to the target space. Approaches have been proposed to learn bilingual dictionaries mapping from the source to the target space. BID47 showed that adding an orthogonality constraint to the mapping can significantly improve performance. The Procrustes approach in BID43 addressed the hubness problem for cross-lingual word embedding spaces by incorporating a nearest neighbors reciprocity term. BID43 also introduced the inverted-softmax to down-weight similarities involving often-retrieved hub words. Recent work by BID2 and BID43 utilized character strings and digits to create bilingual dictionaries with low supervision. BID2's method iteratively refines the dictionary until convergence, while BID43 applied the Procrustes algorithm. However, these methods require cross-lingual information and are not suitable for languages without a common alphabet. The iterative EM-based algorithm with a seed lexicon has also been explored in other studies. Recent work by BID2 and BID43 utilized character strings and digits to create bilingual dictionaries with low supervision. In other studies, the iterative EM-based algorithm with a seed lexicon has been explored. BID50 employed adversarial training, but their approach differs from ours. Their model selection criterion does not correlate with overall model performance and does not allow for hyper-parameter tuning. Despite considering small vocabularies, their method obtained weak results compared to supervised approaches. In this work, the authors demonstrate the ability to align word embedding spaces without cross-lingual supervision, using adversarial training to initialize a linear mapping between source and target spaces. They achieve results comparable to or better than previous supervised approaches by proposing a simple unsupervised validation metric and introducing the similarity measure CSLS. The authors propose a similarity measure CSLS to improve word translation accuracy without supervision, achieving high-quality dictionaries between languages. Their method works well for low-resource language pairs like English-Esperanto and can be a step towards unsupervised machine translation. They also investigate merging English monolingual embedding spaces using different corpora and word embedding methods. The model shows varying word alignment accuracy based on word frequency, corpus used (Wikipedia or Gigaword), and embedding models (skip-gram, CBOW, fastText). It can align embeddings from the same corpus but struggles with different corpora, especially on rare words. Performance worsens with different models and corpus types. The text discusses examples of unsupervised word-by-word translations in Esperanto-English, highlighting the potential for improvement using a simple language model."
}