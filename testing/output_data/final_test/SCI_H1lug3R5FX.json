{
    "title": "H1lug3R5FX",
    "content": "Adversarial examples are perturbations to input that cause misclassifications in machine learning models. A geometric framework is proposed to analyze the high-dimensional geometry of these examples, emphasizing the importance of codimension. The framework shows a tradeoff between robustness under different norms, inefficiency of adversarial training in data balls, and sampling conditions for sufficient analysis. Deep learning breakthroughs have led to adversarial examples, where imperceptible perturbations cause incorrect classifications. Defenses are quickly overcome by stronger attacks, leading to the development of provably robust methods against specific attack models. In this paper, a geometric framework is proposed for analyzing adversarial examples in machine learning models. The \"Manifold Hypothesis\" is leveraged to understand the vulnerability of models and design counter-measures in security-critical settings like health care and autonomous vehicles. The data is modeled as being sampled from low-dimensional manifolds in high-dimensional spaces to address the threat model of attacks. The paper proposes a geometric framework to analyze adversarial examples in machine learning models, focusing on low-dimensional manifolds in high-dimensional spaces. Adversarial examples arise from learning a decision boundary that correctly classifies points on the data manifold but misclassifies points near it due to high codimension. The paper introduces a geometric framework inspired by manifold reconstruction literature to formalize the manifold hypothesis and attack model. It emphasizes the role of codimension in vulnerability to adversarial examples, showing how increasing codimension allows for more directions to construct perturbations off the data manifold. This work investigates the role of codimension in adversarial examples, showing that different classification algorithms are less sensitive to changes in codimension. The study reveals a tradeoff between robustness under different norms and highlights the insufficiency of training against adversarial examples drawn from balls around the training set. Nearest neighbor classifiers are shown to not suffer from this insufficiency due to their geometric properties. This paper explores adversarial examples using techniques from manifold reconstruction literature, providing experimental evidence on synthetic datasets and MNIST. Previous work has considered the relationships between adversarial examples and high dimensional geometry, with some focusing on the robustness of classifiers to random noise in terms of distance to the decision boundary. In contrast to previous work on adversarial examples and high dimensional geometry, this paper presents a geometric framework for proving robustness guarantees for learning algorithms without assumptions on the decision boundary. It highlights the importance of codimension and sampling requirements, showing that adversarial examples exist even when the data manifold is perfectly classified. Additionally, it explores the spacing between data manifolds and sampling requirements for learning algorithms. In a geometric framework, 1-nearest neighbors is robust to adversarial examples, with decision and medial axes defining maximum margin boundaries. Kernel methods enable non-linear maximum margin decision boundaries by projecting data into higher-dimensional feature spaces. Attempts to incorporate maximum margins into deep learning have been made through designing loss functions that encourage large margins at the output. Manifold reconstruction aims to discover the structure of a k-dimensional manifold embedded in R d using algorithms that produce geometrically accurate representations of the manifold. These algorithms output simplicial complexes, such as subsets of the Delaunay triangulation, to approximate the unknown manifold. The field of manifold reconstruction focuses on producing geometrically accurate representations of k-dimensional manifolds in R d. Algorithms output subsets of the Delaunay triangulation and Voronoi diagram to provide geometric and topological guarantees. These algorithms have been adapted to the learning setting, with a particular emphasis on the medial axis under different norms. In higher-dimensional spaces, manifold reconstruction algorithms face the curse of dimensionality. The curse of dimensionality in manifold reconstruction algorithms limits progress on higher-dimensional manifolds due to poorly shaped \"sliver\" simplices. Techniques to remove these simplices have prohibitive sampling requirements, but recent algorithms have been proposed for surfaces in high dimensions. The use of tubular neighborhoods for analysis is borrowed from previous work by Khoury & Shewchuk (2016). The paper focuses on learning robust decision boundaries without reconstructing data manifolds. Robustness guarantees for two learning algorithms are presented in terms of a sampling condition on the underlying manifold, which scales with the dimension of the manifold, not the embedding space. Data is modeled as sampled from low-dimensional manifolds in a high-dimensional space, with k representing the manifold dimension. The codimension of the manifold is d\u2212k. The \"Manifold Hypothesis\" states that data is often sampled from high codimension manifolds. Data is modeled as being sampled from C class manifolds, with each sample point having a class label indicating the manifold it is from. The nearest point to a given point on a manifold is where a ball centered at that point intersects the manifold. The decision axis \u039bp is a set of points where the boundary of a ball intersects two or more class manifolds, without the interior of the ball intersecting any manifold. It is inspired by the medial axis concept and is optimal as a decision boundary for separating points on different class manifolds. The decision axis \u039bp separates class manifolds when they do not intersect and maximizes the distance between points and class manifolds. The reach rchp(T) of a set T is the minimum distance between points on the manifold and points in T under the norm \u00b7p. In this paper, the focus is on exploring conditions for learning a decision boundary that correctly classifies M. The decision axis \u039bp is a boundary that separates class manifolds and maximizes the distance between points and class manifolds. The reach rchp(T) is the minimum distance between points on the manifold and points in T under the norm \u00b7p. The decision axis under different norms can lead to a tradeoff in robustness. Schott et al. (2018) studied the vulnerability of classifiers to attacks under various norms, showing a drop in accuracy under certain attacks. As dimensions increase, an \u221e-robust classifier becomes less robust to attacks under the 2 norm. The robustness of a classifier decreases under different norms, leading to a tradeoff in robustness. Decision boundaries must have a certain norm to cross, with no single boundary being optimally robust in all norms. The minimum distance from one sphere to a decision axis is upper bounded. The minimum distance from S1 to \u039b\u221e under the L2 norm is upper bounded. An adversary can construct an adversarial example for a perturbation as small as O(1/\u221ad), making classifiers less robust to L2-perturbations. Theorem 1 extends to concentric cylinders and intertwined tori by considering 2-dimensional planar cross-sections with nontrivial curvature. Researchers have attributed this phenomena to overfitting. The Madry et al. FORMULA8 defense is successful in creating a robust decision boundary under the L\u221e metric, different from the most robust boundary under the L2 norm. Schott et al. FORMULA8 models also face a similar tradeoff between accuracy against different types of attacks. Madry et al. FORMULA8 suggests training a robust classifier with the help of an adversary to improve robustness against perturbations. In practice, learning a decision boundary that correctly classifies points around the training set can be insufficiently robust. The volume of the boundary is often a poor model for the entire space. To construct a \u03b4-cover, sample points are placed on a grid with spacing \u2206, with the furthest points from the sample covering the space. The volume of the cover is bounded from below by placing balls at each point, revealing that it only covers a small percentage of the total space. In high-dimensional space, the volume covered by a finite set of points sampled from a manifold is limited, leading to a loss of coverage as the codimension increases. This phenomenon is highlighted by the inefficiency of ball-based learning algorithms compared to nearest neighbor classifiers. The volume of balls centered at samples shrinks faster than the volume of the manifold M. Theorem 2 shows that in high codimensions, the fraction of M covered by X approaches 0. Training set sizes realistic in practice result in almost no coverage by X, making it a poor model of M. High classification accuracy on X does not guarantee accuracy in M. The proof of Theorem 2 is in Appendix A. An alternative way to define the ratio vol(X) / vol(M) is as vol(X \u2229 M) / vol(M). In the special case DISPLAYFORM3, a \u03b4-cover of a manifold M in the norm \u00b7 2 is a finite set of points X where for every x \u2208 M, there exists X i such that x \u2212 X i 2 \u2264 \u03b4. An explicit \u03b4-cover X of \u03a0 can be constructed by placing sample points at the vertices of a regular grid. The construction of an explicit \u03b4-cover X of \u03a0 involves placing sample points at the vertices of a regular grid. The size of the sample scales exponentially in k, the dimension of \u03a0. However, X \u03b4 covers only a small fraction of \u03a0 \u03b4, making it a poor model of M. High classification accuracy on X does not guarantee accuracy in M. An upper bound on the volume of X \u03b4 is derived by placing (d \u2212 k)-dimensional balls in the normal space at each point of \u03a0. The percentage of \u03a0 \u03b4 covered by X is bounded, approaching 0 as the codimension increases. The number of points needed to cover \u03a0 with balls of radius 1 depends only on k, while covering the tubular neighborhood \u03a0 1 depends on both k and d. Theorem 3 provides a lower bound on the number of samples needed to cover \u03a0 1. Theorem 3 provides a lower bound on the number of samples necessary to cover the 1-tubular neighborhood \u03a0 1 of a bounded k-flat \u03a0. The number of points needed to cover \u03a0 1 increases polynomially in d and exponentially in k, while the number needed to cover \u03a0 remains constant as d increases, depending only on k. The proof of Theorem 3 in Appendix A shows that generating adversarial examples in -balls around the training set does not accurately model M. Deep learning performs well on tasks because it is easier to perform well on M than on M. Adversarial training, training on adversarial examples in a -ball around the data, is a natural approach for robust models. The text discusses the inefficiency of a learning algorithm in high codimensions and the existence of a classification algorithm that requires exponentially fewer samples to correctly classify a manifold. This is based on a theoretical model of adversarial training and the concept of generating adversarial examples in balls around the training set. Theorem 5 provides a sampling condition for nearest neighbor classifiers to correctly classify manifolds in high codimensions, requiring substantially fewer samples compared to other classification algorithms. The sampling guarantees for nearest neighbor classifiers on k-dimensional manifolds are discussed, showing that the bounds in Theorem 5 may be pessimistic in some cases. The difference in sampling requirements between nearest neighbor classifiers and other learning algorithms can lead to exponential differences in the sizes of training sets needed for robustness. The text discusses the necessity of a gap in Theorem 5 for decision boundaries, the tight bounds for \u03b4-covers for \u03a0, and the exponential ratio of |X L| to |X nn|. It also mentions the vulnerability of FORMULA8 to BIM attacks as codimension increases. Theorem 6 highlights the robustness of decision boundaries for both L and nearest neighbor classifiers with dense samples of M. In Section 7.1, experimental verification of theoretical results is conducted regarding the efficiency of nearest neighbors compared to L in achieving robust decision boundaries. Two synthetic datasets, CIRCLES and PLANES, are introduced to explore the impact of codimension on finding adversarial examples. The CIRCLES dataset consists of concentric circles with densely sampled points, while the PLANES dataset involves two 2-dimensional planes with specific sampling strategies. The training and test sets are sampled on a grid, with attacks using FGSM and BIM methods. Results show a decrease in robustness as codimension increases on the CIRCLES dataset. The text discusses training against a PGD adversary to improve robustness, but it may not guarantee robustness. Adversarial examples are still easy to find for certain values of epsilon, while nearest neighbor achieves perfect robustness. Comparison of nearest neighbors with robust and natural models on MNIST dataset is also explored. Nearest neighbors demonstrate superior robustness to BIM attacks compared to the naturally trained and robust models. At epsilon = 0.5, nearest neighbors maintain 78% accuracy while the robust model drops to 0%. Similar results are shown for FGSM attacks in Appendix B.2. The nearest neighbor attacks are generated by iteratively finding the k nearest neighbors and computing an attack direction away from the true class. The study compares the robustness of nearest neighbors and robust models against adversarial attacks. Nearest neighbors can be tricked, but the robust model is not susceptible. Ensemble methods may offer a balance. Adversarial examples for nearest neighbors align better with human intuition. The framework presented can describe the robustness of any classifier. No single model can be robust to all attacks under all norms. The curr_chunk discusses the role of codimension in generating adversarial examples and the importance of understanding decision boundaries in deep networks for developing new attacks and defenses. It also introduces a novel gradient-free geometric attack and suggests exploring the geometric properties of decision boundaries for black-box attacks in the future. The curr_chunk discusses the decision axis in classification tasks and the maximum achievable accuracy on a dataset with two classes. It also mentions the existence of a decision boundary where points are classified. The curr_chunk discusses the decision boundary in classification tasks and the geometric properties of a hypercube in relation to a d-sphere. The curr_chunk discusses the geometric properties of the medial axis in relation to decision boundaries in classification tasks. The last step involves applying Stirling's approximation to obtain an asymptotic result. The proof constructs an upper bound by assuming disjoint balls centered at samples. The distance from a point q to any other data manifold Mj is lower bounded by d(q, Mj) \u2265 2rchp\u039bp\u2212. It is necessary for there to exist a sample x on Mi such that d(q, x) < 2rchp\u039bp\u2212 for f nn(q) = i. The distance from q to the nearest sample x on Mi is d(q, x) \u2264 \u03b4 for some \u03b4 > 0. The question is how large can we allow \u03b4 to be and still guarantee that f nn correctly classifies M? It is necessary and sufficient for q to be in Brchp\u039bp(x) for some sample x on Mi to guarantee f L(q) = i. The distance to the nearest sample x on Mi is d(q, x) \u2264 \u03b4, so it suffices that \u03b4 \u2264 rchp\u039bp\u2212. For f nn(q) = 1, it suffices that \u03b4 \u2264 2\u221a1\u2212. In this setting, if \u03b4 is too large, a sample on \u03a02 can be misclassified by L. The minimum \u03b4-cover size of \u03a0 is related to its volume, leading to a cancellation of factors in the inequality. The expression (1 + \u03b5)\u2212k/2 is monotonically decreasing. The inequality is derived from the monotonically decreasing expression (1 + \u03b5)\u2212k/2 on the interval [0, 1]. Additional experiments support theoretical predictions, including using different optimization algorithms and attack methods. Adversarial perturbations are mostly in the normal space direction. Nearest neighbor classifier is robust in high codimensions. Increasing sampling density does not notably improve adversarial training robustness. Increasing codimension reduces robustness of decision boundaries learned by Adam on CIRCLES, reproduced using SGD. In high codimensions, the robustness of decision boundaries learned by Adam on CIRCLES decreases when using SGD. SGD shows less variance than Adam, attributed to implicit regularization. Nearest neighbors are more robust to FGSM attacks than naturally trained models, and comparable to robust models up to \u03b5 = 0.3. Nearest neighbors are more robust to FGSM attacks than the robust model, maintaining 78% accuracy at \u03b5 = 0.5 compared to the robust model's 39%. Adversarial perturbations in different codimensions show angles less than 20\u00b0 with the normal space. Figure 10 demonstrates that adversarial perturbations align well with the normal space, with angles less than 20\u00b0. High codimension allows for many directions to construct adversarial perturbations. The robustness of decision boundaries decreases as codimension increases, as shown in Section 7.1. Nearest neighbors remain robust to FGSM attacks even with high codimension, as illustrated in Figure 11. Increasing sampling density improves robustness of adversarial training compared to nearest neighbors, especially in low-codimension scenarios. However, nearest neighbors remain more sample efficient and robust even with a smaller training set. The proposed gradient-free attack in this section only requires oracle access to a model, allowing for querying the model for a prediction. It involves constructing an adversarial perturbation by projecting every point in the test set onto a ball centered at a given point. The gradient-free attack involves projecting points in the test set onto a ball centered at a given point to construct an adversarial perturbation. This attack reduces the accuracy of a pretrained robust model to 90.6%, close to the current state-of-the-art for whitebox attacks. Simple datasets like CIRCLES and PLANES help diagnose issues in learning algorithms. BID3 identified the problem of \"obfuscated gradients\" in various proposed defenses against adversarial examples. They found that seven out of nine defenses suffered from obfuscated gradients, including shattered, stochastic, and exploding/vanishing gradients. However, they stated that the Madry et al. (2018) defense does not suffer from obfuscated gradients. The defense of Madry et al. (2018) is shown to suffer from shattered gradients, causing the gradient field to be nonexistent or incorrect. The normalized gradient field around data manifolds is nearly 0, indicating obfuscated gradients in the defense mechanism. The gradient field around data manifolds is nearly 0, with notable gradients near the decision axis at y = 1. BID3 proposes criteria to identify obfuscated gradients based on the effectiveness of one-step attacks like FGSM. Adversarial examples generated using PGD, FGSM, and BIM for the PLANES dataset show that FGSM produces examples at the decision axis y = 1. The adversarial perturbation in PGD is normal to the data manifold, with examples closer to the manifold. PGD splits perturbation between normal and tangent spaces, while BIM produces examples near the decision axis. BIM ignores gradient magnitude, applying FGSM iteratively. BIM successfully navigates a gradient field overfit to PGD attack particulars, making network less robust to FGSM and BIM attacks. Two synthetic datasets, CIRCLES and PLANES, are introduced in Section 7. Adversarial examples generated using PGD, FGSM, and BIM show that FGSM and BIM attacks are more effective as they ignore gradient magnitude. BIM attacks are more effective as they ignore gradient magnitude. For PGD, arrows are drawn from the test sample to the adversarial example. Iterative attacks BIM and PGD have 30 iterations with a step size of 0.05. Experiments on synthetic data use a fully connected network with 1 hidden layer and ReLU activations. The learning rate for Adam is set at \u03b1 = 0.1. The learning rates for Adam and SGD were set to \u03b1 = 0.1. Experiments were conducted for 250 epochs using PyTorch. Adversarial training procedures were implemented following previous works. The Madry et al. (2018) models consist of two convolutional layers with 32 and 64 filters, followed by max pooling. The volume of S is calculated using the gamma function. A finite sample X of size n approximates S based on the ratio vol X / vol S. Results show that the percentage of volume of S covered by X quickly approaches 0 as n increases. The Delaunay triangulation of a point set always exists and has many desirable properties for mesh generation and manifold reconstruction. It is a triangulation of the convex hull of X into d-simplices, with an empty circumscribing ball property. The Delaunay triangulation of a point set is not unique and has a duality with the Voronoi diagram. Visualizations of synthetic datasets CIRCLES and PLANES are provided for experiments on codimension effects on adversarial examples. Our fully connected network architecture is trained on the CIRCLES dataset in R3, with decision boundaries visualized for different z values. The visualization demonstrates how optimization algorithms learn boundaries extending into normal directions with no data. The robust neural network predictions show a qualitative difference between nearest neighbor and BIM examples. Nearest neighbor examples resemble numbers from a target class, indicating better learning of human decision boundaries. The model's classifications that remain unchanged can be considered errors similar to being fooled by adversarial examples."
}