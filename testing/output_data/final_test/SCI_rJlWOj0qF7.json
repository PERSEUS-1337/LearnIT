{
    "title": "rJlWOj0qF7",
    "content": "We present a novel method to impose tree-structured category information onto word embeddings, resulting in N-ball embeddings. Inclusion relations among N-balls encode subordinate relations among categories. Using a geometric construction method, we create large N-ball embeddings that preserve pre-trained word embeddings. Experiments show that N-ball embeddings outperform word embeddings in nearest neighborhood tests and validating categories of unknown words. Source codes and data-sets are available for free. Word embeddings are vector representations of words that capture semantic and syntactic information. Researchers have proposed representing words as regions instead of vectors to enhance semantic reasoning. By extending word vectors into regions, they found that hyponym regions do not always fall inside their hypernym regions. External hyponym relations were used to achieve 95.2% accuracy in validating categories of unknown words. Source codes and data-sets for this method are freely accessible. In this paper, the author explores the challenge of constructing strict inclusion relations among regions when representing hypernym relations using word embeddings. They propose a novel approach of using n-dimensional balls to impose tree-structured category information onto word embeddings, aiming to represent subordinate relations among categories accurately. The method aims to preserve pre-trained word embeddings while imposing structure on the data. The proposed novel geometric approach achieves zero energy costs of imposing tree structures onto word embeddings by using N-ball embeddings and category information. The presentation includes the structure of N-ball embeddings, the geometric approach to construct them, experiment results, related work review, and conclusions with ongoing research. The text discusses the topological relations between N-balls, measuring similarity using cosine values of central vectors, and the use of category information to determine word sense similarity. The text discusses using category information to determine word sense similarity based on the lowest cosine value below which two word senses are not considered similar. It introduces N-ball embeddings encoding word embeddings and tree structures of hyponym relations among word senses. A unique vector is created to describe the location of a word sense in hypernym trees, with a virtual root as the parent of all tree roots. The text discusses using category information to determine word sense similarity based on cosine values of word embeddings. It introduces N-ball embeddings encoding word senses and hyponym relations. To avoid overlap, a constant non-zero vector is added to prevent N-balls from containing the origin point. The text introduces a spatial extension code to encode subordinate relations among word senses in N-ball embeddings. This code, a constant non-zero vector, is concatenated with pre-trained word embeddings and PLC to preserve inclusion relations among objects. The text introduces a spatial extension code to encode subordinate relations among word senses in N-ball embeddings. The challenge lies in updating N-balls' relations without deteriorating others easily. A depth-first recursion process is proposed to update sizes and locations using three geometric transformations: Homothetic, Shift, and Rotation. The text introduces a rotation transformation that keeps the radius length and rotates the angle of a vector. To maintain improved relations, the principle of family action is used, where transformations are applied to all related objects. The H-tran transformation is prioritized as it preserves inclusion and disconnectedness relations among N-balls. S-tran plus R-tran are applied when an N-ball is too close to the origin. When an N-ball is too close to the origin, a S-tran plus R-tran is applied, potentially changing pre-trained word embeddings. Using a depth-first procedure, parent balls are constructed after their child balls. The final parent ball is the minimal cover of all candidate parent balls constructed. The GloVe word embeddings of BID19 are used as pre-trained vectors for word senses in N-ball embeddings. Hyponym relations from WordNet 3.0 of BID17 are extracted, with the largest tree containing 43,669 word senses. Subordinate relations in the category tree are preserved in N-ball embeddings with zero energy cost. Homothetic, shifting, and rotating transformations are applied in the construction process. Source code and data sets are available at https://github.com/GnodIsNait/nball4tree.git. The experiment examines the effect of geometric transformations on pre-trained word embeddings in N-ball embeddings. Method 1 analyzes the standard deviation of word senses in N-ball embeddings compared to pre-trained word embeddings. Only a small fraction (1.3%) of word senses show significant deviation. The quality of word embeddings can be evaluated by computing the consistency of similarities between human-judged word relations and vector-based word similarity relations using standard data sets like WordSim353 and Stanford's Contextual Word Similarities (SCWS). The quality of word embeddings is evaluated by comparing human-judged word relations with vector-based word similarity relations using data sets like WordSim353 and SCWS. Some words cannot be used directly due to various reasons, resulting in 318 paired words from WordSim353 and 1719 pairs from SCWS for evaluation. The Spearman's correlation values are consistent across all testing cases, indicating that N-ball embeddings are a reliable extension to word embeddings. The quality of word embeddings is evaluated by comparing human-judged word relations with vector-based word similarity relations using data sets like WordSim353 and SCWS. Following BID13, qualitative evaluations are done by manually inspecting nearest neighbors and comparing results with pre-trained GloVe embeddings. Precise neighborhoods N-ball embeddings separate word senses of a polysemy effectively. Typed cosine similarity function enriched by category information produces better neighborhood word senses than the normal cosine measurement. Neighbors of Berlin in GloVe using normal cosine measurement are Vienna, Warsaw, Munich, Prague, Germany, Moscow, Hamburg, Bonn, Copenhagen, and Cologne. However, the word sense \"Berlin.n.02\" as a family name does not appear in the neighbors. Word embeddings are biased by the training corpus without structural constraints, leading to sparse data problems. Some words with similar meanings have negative cosine similarity values due to sparse data. For example, \"tiger\" and \"linguist\" have a -0.1 cosine similarity value, but when constrained by category information, their similarity increases. Using N-ball embeddings, upper-categories of a word sense can be identified by sorting them in increasing order. The top 6 nearest neighbors based on cosine similarity values are listed, such as \"beijing\" being similar to \"london\" with a cosine value of 0.47. Word senses are categorized with direct upper-categories marked accordingly. The fourth experiment aims to validate the category of an unknown word using pre-trained word embeddings and a tree structure of hyponym relations. The task involves deciding whether a word belongs to a specific category based on the embedding approach. The study involves determining the category of an unknown word using pre-trained word embeddings and a hyponym tree structure. It focuses on deciding if a word fits into a particular category based on its embedding. The dataset includes 1,000 word senses of nouns and verbs with at least 10 direct hyponyms each, creating hyponymy relations in the training set. Test data is generated from true values, false values, and words not existing in the dataset. The study created hyponymy relations in the training set and testing set. An N-ball solution was developed to validate membership by constructing embeddings for hypernym paths and known members, recording geometric transformations, and locating members within the N-ball. The N-ball method uses Support Vector Machines to determine membership. Results show high precision and robustness, with recall rates varying based on training set size. The method's recall may not reach 100% due to population standard deviation changes with training data percentage. In representational learning, margin-based score functions are the state-of-the-art approach where a positive sample's score should exceed a negative sample's score by a margin. When category information is strictly used, precision can reach 100%. Related work includes exploring hypernyms in distributional semantic models. The BID21 model identifies hypernyms in a distributional semantic model using entropy-based methods. Other works embed words and entities using Gaussian distributions, manifolds, and Poincar\u00e9 balls. The proposed method imposes tree-structured category information onto word embeddings. The N-ball method imposes tree-structured category information onto word embeddings, resulting in region-based word sense embeddings. It has shown great performance in validating unknown words and is being further investigated for multi-lingual knowledge graph embedding. N-balls combine vector information from deep learning with region information from symbolic structures, bridging Connectionism and Symbolicism for commonsense representation in Artificial Intelligence. N-balls contribute to Qualitative Spatial Reasoning. N-balls introduce a new aspect to Qualitative Spatial Reasoning in Artificial Intelligence."
}