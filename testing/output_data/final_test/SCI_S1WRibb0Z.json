{
    "title": "S1WRibb0Z",
    "content": "Deep neural networks are efficient at practical tasks, with depth being key to their efficiency. Certain deep convolutional networks have higher expressive power than shallow networks. Recurrent neural networks corresponding to the Tensor Train decomposition also show exponential efficiency compared to shallow networks. Recurrent TT-Networks are exponentially more efficient than shallow convolutional networks, showing higher expressive power. The paper discusses the expressive power of deep neural networks, particularly in computer vision and audio/text processing. It highlights the connection between Hierarchical Tucker tensor decomposition and CNNs, proving deep CNNs are more expressive. The paper also explores the connection between recurrent neural networks and Tensor Train decomposition. The paper explores the connection between Tensor Train decomposition and recurrent neural networks, showing that a shallow architecture of exponentially larger width is needed to emulate a recurrent neural network. The approach involves splitting images into small overlapping patches and arranging them in a specific order. Lower-dimensional representations of x are used with parameter-dependent feature maps organized into a representation map. The map typically consists of an affine transformation followed by a nonlinear activation function, resembling traditional convolutional maps in the image case. Score functions are written in a specific form. The activation function in score functions is represented by a feature tensor \u03a6(X) and a trainable weight tensor W y. Different tensor decompositions can be used to reduce memory requirements and complexity, with the Hierarchical Tucker (HT) decomposition proving the expressive power property. Tensor Train-Networks (TT-Networks) have exponentially larger representation power than shallow networks. Matricizations of tensors are used for convenient manipulation in practice. Tensor decompositions, such as CANDECOMP/PARAFAC or CP-decomposition, involve transposing and reshaping tensors into matrices. The CP-rank of a tensor X is the minimal r for which this decomposition exists. Storing all entries of a tensor X requires O(nd) memory, while its canonical decomposition takes only O(dnr). However, determining the exact CP-rank and finding the canonical decomposition are NP-hard problems. A tensor X can also be represented in the Tensor Train (TT) format. Storing a tensor in the TT-format requires less memory and achieves data compression. The algorithm for finding the TT-decomposition is based on SVDs, making it more stable than CP-format. TT-ranks are important in this decomposition process. The TT-format allows for efficient storage of tensors with reduced memory usage and data compression. TT-ranks play a crucial role in the decomposition process. The Hierarchical Tucker (HT) format is a generalization of the TT-format, involving a dimension tree. Bilinear and multilinear units are used in constructing tensorial networks, performing bilinear or multilinear mappings. The computation of score functions for class labels is described, which are then used in the loss function. The proposed architecture implements score functions for class labels using bilinear units. The network is a recurrent-type neural network with multiplicative connections and no non-linearities. Tensor Train decomposition is connected by constructing tensors W y with TT-cores from {G k } d k=1. The network in Fig. 1 implements TT-decomposition of weight tensors, with output size equal to TT-ranks. Other tensor decompositions are considered in Fig. 4, such as CP-decomposition shown in 4a. The HT-Network in Fig. 4b utilizes Hierarchical Tucker decomposition, constructed with a binary tree where nodes represent bilinear units and leaves represent linear units. Inputs flow through the tree to the root, producing an output number. The ranks, denoted as rank HT X, correspond to the sizes of the intermediate unit outputs. The Hierarchical Tucker decomposition in HT-Networks uses a binary tree structure with nodes representing bilinear units and leaves representing linear units. The ranks, denoted as rank HT X, correspond to the sizes of the intermediate unit outputs. The main theoretical focus is on comparing the expressive power of TT-Networks, CP-Networks, and HT-Networks in terms of complexity measured by the rank of the corresponding tensor decomposition. The goal is to determine how complex a CP or HT-Network would need to be to realize the same score function as a given TT-Network. Theorem for the Tensor Train decomposition proves that a random d-dimensional tensor in TT format with ranks r and modes n will have exponentially large CP-rank. TT-ranks cannot exceed CP-ranks. A useful lemma states that for any matricization X(s,t) of a tensor X with rank CP X = r, rank X(s,t) \u2264 r. This lemma is used to provide a lower bound on CP-rank in the theorem. The lemma provides a lower bound on the CP-rank of a tensor by estimating that rank CP X \u2265 r for a matricization of tensor X with matrix rank r. The set of tensors representable in TT-format with rank TT X \u2264 r forms an irreducible algebraic variety M r defined by polynomial equations. Any proper algebraic subset of M r necessarily has measure 0. The main result of the theorem is that for even d = 2k, a specific set is defined where the Lebesgue measure on it is proven to be 0 using a particular matricization of X. The proof involves showing that for certain subsets, the rank of X must be greater than or equal to a certain value, leading to the conclusion that the subset has measure 0. The tensor X constructed has a TT-rank equal to (r, 1, r, ..., r, 1, r) and a matricization with a submatrix equal to the identity matrix of size q d/2 \u00d7 q d/2. This implies that the canonical rank CP X \u2265 q d/2, proving that for all even d = 2k, the subset defined has measure 0. The proof shows that for most TT-Networks, the equivalent CP-Network will have exponentially large width. A comparison of expressive powers between HT- and TT-Networks is provided, with sharp bounds given in Table 2. Theorem 1 states that TT-Networks are exponentially more expressive than CP-Networks for almost any tensor. An example is provided to illustrate this, showing how a TT-Network of width n can compute similarity measures efficiently. The CP-Network for the same function would have exponentially larger width. The hypothesis is that Tensor Trains with equal TT-cores can achieve the same result as in Theorem 1. Numerical verification has been done on randomly generated tensors to support this hypothesis. In this section, the authors experimentally check if CP-Networks require exponentially larger width compared to TT-Networks to fit a dataset to the same level of accuracy. Possible reasons for discrepancies between theory and practice include optimization issues and the existence of feature maps not accounted for in the theory. TT-and CP-Networks were trained using TensorFlow and Adam optimizer with batch size 32. The authors implemented TT-Networks in TensorFlow and used Adam optimizer with batch size 32. They assessed the expressivity of the format by choosing the best performing run based on training loss. They used two-dimensional datasets for the first experiment and computer vision datasets MNIST and CIFAR-10 for subsequent experiments. MNIST contains handwritten digits, while CIFAR-10 consists of natural images classified into 10 classes. The TT-Networks were able to implement nontrivial decision boundaries. In experiments with TT-and CP-Networks, patch size is 8x8 with 4 feature maps using affine maps and ReLU activation. Both networks perform well on MNIST dataset, achieving 1.0 train accuracy and 0.95 test accuracy without regularizers. For CIFAR-10, TT-Network achieves 0.45 test accuracy and CP-Network achieves 0.2 test accuracy without regularizers. The expressive power of TT-Network is only slightly better than CP-Network. The TT-Network's expressive power is only slightly better than the CP-Network, with recent studies focusing on depth efficiency and worst-case guarantees for deep and shallow networks. Two works analyze depth efficiency through tensor decompositions, including the Hierarchical Tucker decomposition and its generalization for activation functions like ReLU. The TT-Network's expressive power is compared to the CP-Network, with recent studies focusing on depth efficiency and worst-case guarantees for deep and shallow networks. Previous works have analyzed depth efficiency through tensor decompositions, while our work focuses on recurrent architectures and the expressivity of RNNs. Other related works have explored different classes of recurrent models, such as models with predefined feature maps and recurrent neural networks with multiplicative connections. In this paper, the connection between recurrent neural networks and Tensor Train decomposition is explored to prove the expressive power theorem. It states that a shallow network with exponentially large width is needed to mimic a recurrent neural network. Future work aims to address optimization issues by utilizing Riemannian geometry properties of TT-tensors and extending the analysis to networks with non-linear functions in recurrent connections."
}