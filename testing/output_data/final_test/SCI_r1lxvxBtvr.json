{
    "title": "r1lxvxBtvr",
    "content": "TransINT is a novel KG embedding method that preserves implication ordering among relations in the embedding space. It maps entities tied by a relation to continuous vectors, enabling automatic training on missing but implied facts. TransINT achieves state-of-the-art performances in Link Prediction and Triple Classification on FB122 dataset, even on instances that cannot be inferred by logical rules. The angles between the continuous sets embedded by TransINT provide an interpretable way to understand the relationships. KG embeddings like TransINT capture statistical patterns in multi-relational knowledge but lack common sense knowledge essential for reasoning. Current research aims to incorporate common sense rules into KG embeddings to improve understanding and interpretation of relationships. TransINT is a new KG embedding method that preserves the implication ordering among relations in the embedding space, allowing for the representation of n-ary relations in KGs. It restricts entities tied by a relation to specific regions in the embedding space based on the order of relation implication, enabling better understanding and interpretation of relationships. TransINT is a KG embedding method that maps sets of entities under relation constraints to a continuous set of points, allowing for the identification of semantic patterns and implication rules. This method provides consistency and interpretability to model behavior in knowledge graphs. TransINT is a method that connects sets of entities under relation constraints to a continuous set of points, enabling interpretable reasoning in applications like recommender systems and question answering. It defines relations as sets and implication as inclusion of sets, providing a new approach to mapping relations in knowledge graphs. Relations are defined as sets and implication as inclusion of sets in set-theoretic logic. A binary relation x, y entities have a relation set R i. Logical implication between two relations is denoted as r 1 \u21d2 r 2 if \u2200x, y. The orange dot represents the origin in vector translation. The process involves projecting and subtracting vectors to determine relationships. The red line is unique as it represents the translation of a relation to the origin in vector space. TransE aims for the sum of entity and relation embeddings to be close to the target entity. However, it struggles with complex relations. TransH addresses this by projecting entities onto unique hyperplanes for each relation, ensuring their distance is constant. TransH embeds entities to vectors and assigns relation-specific hyperplanes and fixed vectors for each relation. It interprets entity vectors tied by a relation as belonging to a specific line in vector space. This approach embeds a relation set in a knowledge graph to a particular set in a high-dimensional space, defining a relation space for each relation. TransINT embeds relations in a knowledge graph by intersecting hyperplanes and projecting vectors, ordering relation spaces by implication. For example, familial relations like is_father_of and is_mother_of have distinct hyperplanes, with is_parent_of being implied. TransINT embeds relations in a knowledge graph by intersecting hyperplanes and projecting vectors, ordering relation spaces by implication. H is_parent_of is represented as a line, unlike in TransH where all H j 's are hyperplanes. Two hard geometric constraints are imposed on (H j , #\u00bb r j )'s for distinct relations, requiring specific projections onto H is_parent_of. The relation space of is_parent_of includes those of is_father_of and is_mother_of, as illustrated in Figure 3b. TransINT embeds relations in a knowledge graph by intersecting hyperplanes and projecting vectors, ordering relation spaces by implication. The ordering is isomorphic, guaranteeing that if one relation subsumes another, their spaces are also ordered accordingly in the embedding space. This is a natural consequence of rank-based geometry in R d. The method is formally stated with an isomorphic guarantee and discussed for intuitive understanding. Points in R d are projected to linear subspaces by projection matrices, denoted by capital letters for matrices and arrows for vectors. In TransINT, relations in a knowledge graph are embedded by intersecting hyperplanes and projecting vectors, ensuring an isomorphic ordering of relation spaces. Points in R d are projected onto linear subspaces by projection matrices, with each subspace having a projection matrix. The relation space is defined algebraically as the set of vectors that become a specific point when projected onto a subspace. The main theorem establishes an isomorphism between embeddings assigned to relations by constraints. In TransINT, relations in a knowledge graph are embedded by intersecting hyperplanes and projecting vectors, ensuring an isomorphic ordering of relation spaces. The projection matrix P i bounds regions with thickness 2, centered around Sol(P i, #\u00bb r i). The main theorem establishes an isomorphism between embeddings assigned to relations by constraints, showing that relations with intersection constraints subsume the intersected relations. Geometrically, projection is decomposition into independent directions. Projection is decomposition into independent directions; a vector bound by constraints is free in other directions. The more constraints, the less freedom a vector has. Shared parameters are initialized and trained to satisfy constraints. Mapping degree of freedom in logical space to R d is possible. Projection matrices are constructed for entities and relations in a knowledge graph. The H's are defined top-down with linearly independent rows. Projection constraints are applied to initialize the vectors. Parameters for training include A for head relations and c for non-head relations. The training process involves constructing negative examples and using a margin-based loss function to discriminate between correct and incorrect fact triplets in a knowledge graph. The score function is defined for each fact triplet, and stochastic gradient descent is used to minimize the objective. Automatic grounding of positive triples is facilitated by a parameter sharing scheme. TransINT guarantees two advantages during training: meeting intersection and projection constraints at all steps, and automatically training with related triples even if they are missing in the knowledge graph. This eliminates the need to manually create missing triples and allows the model to be trained with existing knowledge. TransINT is evaluated on Freebase 122 against the current state-of-the-art method for predicting gold entities in fact triples with missing head or tail. The evaluation includes metrics such as mean reciprocal rank (MRR), median of the ranks (MED), and Hits@N. Lower MED and higher MRR and Hits@N are considered better. TransH and KALE also use a \"filtered\" setting to address correct entities ranked before the gold. TransINT is evaluated on Freebase 122 against the current state-of-the-art method for predicting gold entities in fact triples with missing head or tail. The evaluation includes metrics such as mean reciprocal rank (MRR), median of the ranks (MED), and Hits@N. Lower MED and higher MRR and Hits@N are considered better. The \"filtered setting\" is used to address correct entities ranked before the gold entity, ignoring corrupted triplets in the KG. The dataset FB122 is a subset of FB15K with 122 Freebase relations on \"people\", \"location\", and \"sports\" topics. The hyperparameters for training TransINT are detailed. TransINT's hyperparameters for training include learning rate, margin, embedding dimension, and learning rate decay. Optimal configurations are selected from candidate values for each parameter. Training involves creating mini-batches, training for a maximum number of epochs with early stopping, and experimenting with normalization of entity vectors, relation vectors, and relation subspace bases. Additionally, the experiment settings involve using transitive rules in FB122 for KALE, but TransINT only deals with implication rules. TransINT focuses on implication rules and intentionally puts itself at a disadvantage compared to KALE to test its robustness. It evaluates performance without grounding negative examples, using two settings: TransINT G (grounding) and TransINT N G (no grounding). While the filtered setting performs better, TransINT outperforms other models in both raw and filtered scenarios. TransINT outperforms all other models by large margins in all metrics, especially in the filtered setting. The gap between TransINT G and KALE is significantly higher than that between TransINT N G and KALE, showing TransINT's robust performance even without grounding. The results suggest that emphasizing true positives may be as important as avoiding false negatives, and TransINT's automatic grounding of positive training instances could be a key factor in its success. TransINT achieved superior performance compared to other models, especially in the filtered setting. Emphasizing true positives and automatic grounding of positive training instances were key factors in its success. Norm regularization did not help in training TransINT, instead, a large margin was important. The task involved classifying whether an unobserved instance is correct, using a similarity score and a certain threshold. Mean average precision was used to evaluate classification precision over all distinct relations in the test instances. The experiment settings were the same as those provided by KALE. Triple Classification results for TransINT G and TransINT N G are superior to all other baselines, even outperforming KALE. The embeddings generated by TransINT are better, even for relations not affected by implication rules. Negative example grounding does not improve performance on unaffected relations. Grounding does not help performance on unaffected relations but greatly boosts performance on those affected by rules. TransINT improves the quality of embeddings for affected relations without necessitating negative example grounding. Traditional embedding methods learn soft tendencies between objects, such as semantic similarity, using cosine similarity. TransINT extends this idea to semantic relatedness between groups of objects, with angles between relation spaces indicating overlap in area. The hypothesis suggests that for relations r1 and r2, their intersection in relation spaces determines their semantic relationship. The angle between the relation spaces indicates the degree of disjointness or overlap between the relations, with implications for semantic implication or relatedness. Membership imbalance in overlapped regions is considered for determining semantic relationships. The angle between relation spaces indicates the degree of overlap, with implications for semantic relatedness. Relation pairs with angles less than 20\u00b0 can be inferred by implication rules, while those within 20-70\u00b0 show strong tendencies of relatedness. Table 3 confirms trends with respect to /people/person/place_of_birth. Our analysis of relation pairs shows a consistent trend with our hypotheses, especially in relation to /people/person/place_of_birth. TransINT's method of mapping entities to optimal regions in embedding space is unified and coherent with knowledge graph embeddings. This approach opens up new possibilities for explainable reasoning in applications like recommender systems and question answering. Our work is related to Order Embeddings and their extensions, as well as enforcing common sense logical rules in embedded knowledge graphs. Wang et al. constraints embeddings to satisfy logical implications via linear programming, while Guo et al. induces embeddings to follow logical rules during learning. TransINT is a new KG embedding method that embeds sets of entities to continuous sets in R d, achieving state-of-the-art performances in Link Prediction and Triple Classification on the FB122 dataset. It also proposes an interpretable criterion for mining semantic similarity among sets of entities. The proofs for Main Theorems 1 and 2 are provided, along with explanations of key concepts in R d. In this section, we introduce lemmas and definitions for R d, marking new elements with *. We denote matrices with capital letters and vectors with arrows. A hyperplane in R 3 is defined by a single equation, giving it a rank of 1. In contrast, a line in R 3 has a rank of 2 due to two constraints. Consider two linear subspaces H 1, H 2 defined by equations, A 1(#\u00bbx - #\u00bbb 1) = 0 and A 2(#\u00bbx - #\u00bbb 2) = 0. In R d, hyperplanes H 1, H 2 are defined by equations A 1(#\u00bbx - #\u00bbb 1) = 0 and A 2(#\u00bbx - #\u00bbb 2) = 0. Lemma 1 states that projecting a vector onto a subspace results in the same vector. Orthogonality is shown by decomposing a vector into two orthogonal components. Lemma 2 and 3 discuss properties of projection matrices. If one subspace includes another, the order of projecting a point onto them does not matter. In R d, hyperplanes H 1, H 2 are defined by equations A 1(#\u00bbx - #\u00bbb 1) = 0 and A 2(#\u00bbx - #\u00bbb 2) = 0. Lemma 4 and 5 discuss properties of subspaces and projection matrices. If one subspace includes another, the order of projecting a point onto them does not matter."
}