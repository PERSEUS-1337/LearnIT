{
    "title": "ryeYHi0ctQ",
    "content": "Multiview stereo uses deep learning to reconstruct scene depth from images captured by a moving camera. A new convolutional neural network, DPSNet, adopts a plane sweep approach inspired by traditional geometry-based methods. It builds a cost volume from deep features, aggregates costs contextually, and regresses the depth map, allowing for end-to-end training. DPSNet utilizes conventional multiview stereo concepts in a deep learning framework to achieve state-of-the-art reconstruction results. The network addresses issues with photo-consistency constraints in reconstructing 3D geometry by leveraging convolutional neural networks. Neural networks (CNNs) have shown promise in addressing the issue of multiview stereo by leveraging semantic information from the scene. The Deep Plane Sweep Network (DPSNet) is introduced as an end-to-end CNN framework for robust multiview stereo, fully modeling the plane-sweep process for disparity estimation. DPSNet fully models the plane-sweep process for stereo reconstruction using a differentiable warping module and cost aggregation. The network achieves state-of-the-art results on standard datasets, reducing unreliable matches and improving reconstruction accuracy. Recent advancements in CNN-based depth estimation have led to improvements in reconstruction accuracy. Various methods for stereo matching have been developed, including Siamese network structures, convolution and deconvolution layers, and end-to-end CNN approaches. These methods aim to compute matching costs, refine initial depth estimates, and minimize the distance between estimates and ground truth. Recent advancements in CNN-based depth estimation have improved reconstruction accuracy. Methods like BID19 leverage geometric knowledge to build a cost volume from deep features and enable learning of contextual information in a 3D volume. BID0 introduces a pyramid pooling module for global contextual information and a stacked hourglass 3D CNN for regional support. Single-image methods, like BID4 and BID22, use CNN features and superpixel-based CRF to improve depth estimates. Recent studies aim to improve depth estimates from single images by utilizing view synthesis as supervision for depth and camera pose estimation. This involves training a depth network and a pose estimation network on sequential images with a loss computed from warped images. Unlike previous methods that use warping for self-supervised learning, this network computes warps with respect to multiple depth planes to generate plane-sweep cost volumes for training and testing, enhancing the robustness of depth estimates. In multi-view stereo, depth is inferred from multiple input images acquired from arbitrary viewpoints using aggregation and regularization techniques. The DeMoN system consists of encoder-decoder networks for optical flow, depth/motion estimation, and depth refinement, alternating between estimating optical flow and depth/motion to improve depth estimates. Monocular visual odometry is performed in an unsupervised manner, with stereo images used in training for 3D depth estimation with metric scale. Camera parameters are assumed to be known or estimated by conventional geometric methods in networks that can handle an arbitrary number of views. BID18 introduces an end-to-end learning framework based on a viewpoint-dependent voxel representation for encoding images and camera parameters. The voxel representation limits scene resolution due to GPU memory constraints. BID16 refines scene geometry by relating optical flow and depth, designed for small baseline image sequences. BID14 uses calibrated pose data to compute plane-sweep volumes for initial depth prediction, incorporating intra-feature aggregation and max-pooling for information gathering. Our proposed DPSNet is trained end-to-end for dense depth estimation from input images, incorporating context-aware cost aggregation. Unlike BID35, which focuses on reconstructing full 3D objects, we concentrate on accurate depth estimation for a reference image. Our cost volume is constructed by concatenating input feature maps to enable precise depth map inference. Our Deep Plane Sweep Network (DPSNet) refines cost slices with context features for accurate depth estimation. It consists of four parts: feature extraction, cost volume generation, aggregation, and depth map regression. The framework includes passing images through convolutional layers and extracting multi-scale features using a spatial pyramid pooling module. The multi-scale features extracted by SPP are effective in visual perception tasks like visual recognition, scene parsing, and stereo matching. After upsampling and concatenating the feature maps, 32-channel feature representations are generated for input images. Cost volumes for multiview images are generated using traditional plane sweep stereo, with the number of virtual planes set to reduce image noise. The cost volume generation network sets virtual planes perpendicular to the z-axis of the reference viewpoint and warps paired features into the coordinates of the reference feature using pre-computed intrinsics and extrinsic parameters. The cost volume generation network utilizes a concatenation of features to learn a representation and create a 4D volume for depth labels. A spatial transformer network is used for the warping process, and concatenating features improves performance. The DPSNet learns a cost volume generation through a series of 3D convolutions on the concatenated features. The cost aggregation method in the end-to-end learning process involves using 3D convolutions with 3x3x3 filters and residual blocks. During training, one paired image is used to obtain the cost volume, while during testing, any number of paired images can be used by averaging the cost volumes. The context network refines the cost slices by taking each slice of the cost volume and the reference image features, outputting the refined cost slice. Dilated convolutions are used in the context network for cost aggregation. The context network in the cost aggregation process utilizes dilated convolutions to enhance contextual information. It consists of seven convolutional layers with varying receptive fields. Shared weights are used for processing all cost slices, followed by upsampling the cost volume to the original image size. Continuous depth values are regressed using a proposed method, and the predicted depth is calculated from the weighted sum of labels. The training process involves calculating depth based on predicted labels and scene depth. Parameters in the network include feature extraction, cost volume generation, and cost aggregation. The training loss is formulated using the Huber norm, with a weight value set for depth from the initial cost volume. The model is trained from scratch using image sequences, ground-truth depth maps, and camera poses from public datasets for 1200K iterations. The model is trained from scratch for 1200K iterations using the ADAM optimizer with a batch size of 16. Training is done on four NVIDIA 1080Ti GPUs and takes about four days. Evaluation metrics include Abs Rel, Abs R-Inv, Abs diff, Sq Rel, RMSE, RMSE log, and inlier ratios. For comparisons, state-of-the-art methods like COLMAP, DeMoN, and DeepMVS were chosen. Depth maps were estimated from unstructured views using test sets in MVS, SUN3D, RGBD, and Scenes11. Results in Table 1 show DPSNet outperforms in most measures, accurately recovering scene depth in homogeneous regions and object boundaries. DeMoN struggles with reconstructing scene details like keyboards and fine structures, while DPSNet excels due to differential feature warping penalizing inaccurate reconstructions. In the experiment, DPSNet produces accurate results in handling textureless regions, thanks to the cost aggregation network. Additional evaluation criteria include completeness, geometry error, and photometry error. Results for COLMAP, DeMoN, and DeepMVS are reported from a previous study. The ETH3D dataset is used, and all methods are not trained on it. Our DPSNet, trained on the ETH3D dataset, outperforms other comparison methods in terms of completeness and error metrics. While filtered COLMAP achieves the best performance, its completeness is only 71%, whereas our DPSNet shows promising results with 100% completeness. Unlike COLMAP and MVSNet designed for full 3D reconstruction, DPSNet aims to estimate dense depth maps. DPSNet outperforms other methods in completeness and error metrics. An ablation study examined different components' effects on DPSNet performance. Cost volumes generated using feature concatenation outperform traditional absolute difference in depth label selection. CNN learns 3D scene information from stacked features for depth estimation. The proposed cost aggregation network significantly improves performance in depth refinement compared to using a stacked hourglass for feature aggregation. Cost aggregation regularizes noisy cost slices while preserving edges well, as shown in Figure 6. The proposed cost aggregation network improves depth refinement by regularizing noisy cost slices and preserving edges well. It enhances the reliability of correct matches by improving cost profiles and depth label sampling. Uniform depth label sampling in the inverse depth domain produces more accurate depth maps. Depth label sampling in the inverse depth domain is crucial for accurate depth maps. DPSNet performance improves with more input images, as shown in FIG7, where adding views helps in object boundary distinction. Rectified Stereo Pair CNNs and DPSNet have similarities but differ in obtaining correspondences. This study demonstrates the application of plane sweeping in rectified stereo matching using DPSNet on the KITTI dataset. The study applies DPSNet on the KITTI dataset, achieving performance similar to BID24 in terms of D1-all score. A multiview stereo network is developed based on traditional techniques, with a plane sweep algorithm formulated as an end-to-end network. Additionally, a context-aware cost aggregation method is proposed for improved depth estimation. The study proposes a context-aware cost aggregation method for improved depth regression in a deep learning framework. It suggests integrating semantic instance segmentation and employing viewpoint selection for better depth prediction. The network currently requires pre-calibrated parameters, but future work aims to estimate camera poses in an end-to-end learning framework."
}