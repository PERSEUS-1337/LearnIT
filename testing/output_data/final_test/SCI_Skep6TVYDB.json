{
    "title": "Skep6TVYDB",
    "content": "Zeroth-order optimization involves minimizing an objective function $f(x)$ with oracle access to adaptively chosen inputs. Two GradientLess Descent (GLD) algorithms are introduced in this paper, which do not require gradient estimates and are numerically stable. The analysis shows convergence within an $\\epsilon$-ball of the optimum in $O(kQ\\log(n)\\log(R/\\epsilon))$ evaluations for any monotone transform of a smooth and strongly convex objective with latent dimension $k \\ge n. These rates are poly-logarithmically dependent on dimensionality and invariant under monotone transformations, making them optimal. Our analysis is optimal, leveraging a geometric perspective. Monotone invariance and low latent dimensionality are crucial for the success of our algorithms, as shown on synthetic and MuJoCo benchmarks. Zeroth-order optimization aims to minimize an objective function without computing gradients, useful in applications like reinforcement learning, attacking neural networks, and hyperparameter tuning. The standard approach involves estimating gradients from function values and applying a first-order optimization algorithm. Nesterov & Spokoiny (2011) analyze these algorithms as gradient descent on a Gaussian smoothing of the objective, providing accelerated optimization. Zeroth-order optimization involves minimizing an objective function without computing gradients. Nesterov & Spokoiny (2011) analyze these algorithms as gradient descent on a Gaussian smoothing of the objective, providing accelerated optimization with an O(n \u221a Q log((LR 2 + F )/ )) iteration complexity for LLipschitz convex functions. Various techniques such as variance reduction, conditional gradients, and diagonal preconditioning have been successfully adopted in this setting. Direct search algorithms have gained attention due to high variance in gradient-based optimization methods, especially in deep learning and reinforcement learning. These algorithms find a descent direction and move without scaling the step size by the function difference. The first direct search approaches date back to the 1950s and recent theoretical bounds have been established. Recent theoretical bounds have surfaced for direct search algorithms, with Gratton et al. (2015) and Dodangeh & Vicente (2016) providing iteration complexities for convex functions. Stochastic approaches, such as those by Stich et al. (2013) and Gorbunov et al. (2019), offer improved complexities for functions with condition numbers. Zeroth order optimization shows a linear dependence on input dimension, prompting recent works to address this limitation for high-dimensional structures. Recent works have addressed the limitation of high-dimensional structures in optimization algorithms. Some papers focus on low-dimensional structures of the function f(x), with different approaches such as Lasso, low-rank approximation, random embeddings, and sparse recovery. The algorithm discussed in the current text chunk inherently picks up low-dimensional structures in f(x) and achieves a convergence rate dependent on k log(n). The text discusses black-box optimization methods like Bayesian optimization and genetic algorithms, which aim to solve broader problems but have weaker theoretical guarantees and require substantial computation. Bayesian optimization, for example, has exponential iteration complexities in dimension and relies on an inner optimization loop of the acquisition function. GradientLess Descent (GLD) is a class of truly gradient-free algorithms that are parameter free and provably fast. These algorithms are based on the intuition that taking a small step in a randomly chosen direction from a starting point can significantly reduce the objective function value. The analysis relies on high dimensional geometry, guaranteeing an expected decrease in the optimality gap based on geometric properties of sublevel sets. The analysis of GradientLess Descent (GLD) algorithms focuses on reducing the optimality gap using geometric properties of sublevel sets of smooth and strongly convex functions. Results are invariant under monotone transformations, extending convergence to a class of non-convex functions. Monotone transformations of convex functions may not be convex, but are always quasi-convex. This has implications for the maximization of quasi-concave utility functions in economics. The text discusses the Iteration Complexity Monotone k-Sparse k-Affine Nesterov & Spokoiny (2011) algorithm, which focuses on optimizing functions with low-dimensional structures. By carefully selecting a sampling distribution, the algorithm can achieve fast convergence rates without the need for expensive sparse recovery or subspace finding methods. The step size is shown to be on the order of O( ) when the function admits a rank k matrix structure, leading to logarithmic dependence on the dimensionality for convergence. Additionally, the algorithm demonstrates robust convergence rates even under more realistic assumptions. Theorem 1 discusses the convergence of the GLD algorithm for optimizing functions with low-dimensional structures. It shows that with constant probability, the algorithm can find a solution x T such that x T \u2212 x * \u2264 after a certain number of function evaluations. The analysis also reveals that the rates achieved are optimal with a matching lower bound, indicating that gradient-free methods inherently require a certain number of function evaluations to converge. The GLD algorithm is competitive against standard optimization procedures, as shown in experiments on synthetic benchmarks. Notations and definitions are provided for a real-valued function on a compact subset of R^n. The function is defined as \u03b1-strongly convex for all x, y in X. The function f is \u03b1-strongly convex for all x, y in X and \u03b2-smooth. A monotone transformation of f preserves level sets and our results generalize to any such transformation. The condition number Q \u2265 1 is defined as the minimum ratio \u03b2/\u03b1 over all functions g that are monotone transformations of f and are \u03b1-strongly convex and \u03b2-smooth. When working with low rank extensions of f, the condition number within a rank k subspace is considered. The function f is \u03b1-strongly convex and \u03b2-smooth, with a condition number Q \u2265 1 defined as the minimum ratio \u03b2/\u03b1 for monotone transformations. The GLD template involves sampling from a distribution centered around x t with scaling r t, updating x t+1 based on f (x t+1 ) < x t. The analysis of GLD shows that moving in a random direction from any point can significantly improve the objective. The key to fast convergence is choosing step sizes carefully, with a step size of \u0398(1/ \u221a n) recommended for large steps while maintaining a high probability of improving the objective. If f(x) has a latent rank-k structure, this step size can be effective. The step size for optimization can be increased to \u0398(1/ \u221a k) if f(x) has a latent rank-k structure, ensuring fast high-dimensional optimization. The convergence rates are optimal with a matching lower bound, assuming f(x) is strongly convex and smooth with condition number Q. Theorem 7 states that for certain x values, a random sample y from a uniform distribution has a probability of at least 1/4. This is proven using Lemma 8 about the intersection of balls in high dimensions. Upon further inspection, a simpler Gaussian sampling procedure can replace uniform sampling for high-dimensional balls, concentrating mass near the surface. This is beneficial when f(x) has a low-dimensional structure, as any affine projection of a Gaussian remains Gaussian. Additionally, if f(x) can be represented as g(PAx) where PA is a rank-k projection matrix, Gaussians projected onto a k-dimensional subspace remain Gaussian. The algorithm has a dimension dependence on k when projecting Gaussians onto a k-dimensional subspace. The speed-up allows for a larger sampling radius while maintaining a high probability of progress. Binary search can be used to find the correct radius if k is unknown. The assumption of low-rank is restrictive, but fast rates hold for early optimization stages even with a full-rank function bounded by \u03b4. Theorem 11 states that for a function f(x) = g(PAx) + h(x) with a rank k matrix PA, where g and h are convex and |h| \u2264 \u03b4, convergence remains fast until the optimality gap approaches \u03b4. Gradient-approximation algorithms can be accelerated based on the square-root of the condition number Q, but gradient-less methods relying on random sampling may not be accelerated according to lower bounds. Theorem 11 discusses the convergence of functions with a rank k matrix PA, while the following text introduces Algorithm 1: Gradientless Descent with Binary Search (GLD-Search) and Theorem 12 regarding random sampling and positive measure regions. GLD-Search and GLD-Fast are two algorithms following the Gradientless Descent template, with the latter optimized for known upper bounds on function condition numbers. These algorithms show fast convergence rates for monotone transforms of convex f(x). Experimentally proven efficacy is shown in the Experiments section. The algorithm allows for a choice of radii at each iteration, with a binary search procedure ensuring progress. The naive binary sweep across an interval in [r, R] gives convergence guarantees without prior knowledge of the condition number. GLD-Search and GLD-Fast are algorithms for Gradientless Descent, with the latter optimized for known upper bounds on function condition numbers. GLD-Fast reduces runtime by exploiting the assumption of a good condition number upper bound. GLD-Fast is an algorithm for Gradientless Descent that exploits the assumption of a good condition number upper bound to reduce runtime. It involves slowly halving the search space diameter and has been tested on a class of objective functions. GLD algorithms were tested on objective functions and compared to ARS by Nesterov & Spokoiny. GLD-Fast is comparable to ARS and achieves low error faster in high dimensions. GLD-Search is competitive in low dimensions without needing function information. The objective function involves a diagonal matrix with evenly spaced elements from \u03b1 to \u03b2. The objective function f \u03b1,\u03b2,n is \u03b1-strongly convex and \u03b2-strongly smooth. Experiments were conducted on f 1,8,n with imperfect curvature information \u03b1 and \u03b2. GLD-Search is independent of the condition number, while GLD-Fast only requires an upper bound on the condition number. ARS needs strong convexity and smoothness parameters for testing. ARS requires strong convexity and smoothness parameters for testing. Three different distributions of the approximation error are tested with ARS-alpha, ARS-beta, and ARS-even receiving different inputs based on the approximation factor. GLD-Fast outperforms ARS in robustness and speed when the condition number is over-approximated, while still steadily converging when underestimated. Experiments on f 1,8,n with a monotone transformation show that the convergence of GLD is unaffected, but ARS exhibits inflection points in the convergence curve for low-dimensional cases of the transformed function. GLD-Fast outperforms ARS in robustness and speed, especially in higher dimensions. GLD algorithms are tested on BlackBox Optimization Benchmarking functions to show practicality, with superior performance highlighted in the plots. The GLD algorithms demonstrate superior performance on various non-convex and high conditioning BBOB functions, showcasing their robustness. GLD-Fast outperforms GLDSearch, particularly in higher dimensions, aligning with theoretical expectations. Experiments on Mujoco benchmarks validate the effectiveness of the approach in non-convex, high-dimensional settings. Unlike ES, our algorithm eliminates queries to form a gradient direction. Our algorithm achieves competitive performance on maximum reward by removing queries that produce less reward than the current arg-max. We tested the affine invariance of GLD on policy parameters using Gaussian ball sampling under the HalfCheetah benchmark. This involved projecting the state s of the MDP with a linear policy to a higher dimensional state W s using matrix multiplication with an orthonormal W. GLD is a robust zeroth-order optimization algorithm with strong theoretical convergence bounds. It performs well in non-convex settings and is flexible for easy modifications like using momentum terms or adaptive gradient methods. GLD is a robust zeroth-order optimization algorithm with strong theoretical convergence bounds, performing well in non-convex settings and allowing for easy modifications like using momentum terms or adaptive gradient methods. The algorithm can adaptively change the distribution of ball-sampling radii or shape them into ellipsoids with various length-scale factors. Additionally, GLD could be combined with random restarts or other restart policies developed for gradient descent. The text discusses the proof of a lemma involving a ball of radius tangent to a sublevel set, utilizing strong convexity and smoothness properties. The lemma is proven by considering the intersection of two hyperspherical caps and bounding the volume of the intersection. The volume of a spherical cap is calculated using the regularized incomplete beta function. To obtain a lower bound on the volume, a specific equation is derived. As the value converges to 1, it is shown that one hyperspherical cap is contained within another. The proof involves defining a ball tangent to a sublevel set and utilizing strong convexity and smoothness properties. Our guarantee follows from Lemma 15 and the convexity of f. To prove Eq. 2, we apply Lemma 8 after ensuring the radius of B x and B q are in the proper ranges. By choosing k 1 in the log 2 space, we can find a point between 2 and , satisfying the theorem statement. Similarly, we can prove the existence of k 2. Finally, we show that r q \u2265 (1 \u2212 1/(4n)) by demonstrating x \u2212 q \u2264 4n or equivalently \u03bd x \u2212 x * \u2264 4n. Without loss of generality, let = 1 and B 2 is centered at the origin with radius r 2. By Markov's inequality, we see that n i=2 X 2 i \u2264 2r 2 1 = 2/n with probability at most 1/2. Since X 1 is independent and r 2 \u2265 1 \u2212 1/n, it suffices to show that X 1 has standard deviation at least r 1 / \u221a n \u2265 1/(2n), ensuring a constant probability of deviating below. The proof of Theorem 10 involves projecting points onto the column space of A, applying Theorem 7 on a k-dimensional subspace, and using Lemma 9. Sampling from a Gaussian distribution y \u223c N (x, r 2 k I) yields z * as the minimum of g(x) with constant probability due to boundedness on h. The proof strategy shows that progress can only be made with a radius size of O(log(nQ)/(nQ)), larger radii cannot find descent directions with high probability. For a simple ellipsoid function f(x) = xDx, where D is a diagonal matrix, the optima is x* = 0. By standard concentration bounds, with exponentially high probability, i>1 X 2i \u2265 n/2. Chernoff bounds also show that |xi| \u2264 0.1/(Q\u221an). The probability of descent is upper bounded by \u03a6(\u2212l), where \u03a6 is the cumulative density of a standard normal and l = \u2126(log(nQ)). With high probability, the objective function and each coordinate can change by at most O(log(nQ)/(Qn)). The proof extends to any symmetric distribution D, where the p.d.f. takes the form p(v) = pr(r)u(\u03b8), inducing the uniform distribution over the unit sphere. The proof shows that if a random variable Y follows a distribution D, then Y can be expressed as Y = Rv/v, where R is a random scalar with a probability density function p(r) and v is a standard Gaussian vector. The argument implies that Y is a descent direction with polynomially small probability when R \u2265 \u2126(log(nQ)/Q). By using a binary search between minimum and maximum search radii, the objective value decreases multiplicatively in each iteration with constant probability. The proof involves using a binary search between minimum and maximum search radii, where the objective value decreases multiplicatively in each iteration with constant probability. By setting specific parameters and applying strong convexity, the result is derived. The low-rank result is obtained by applying different theorems. The proof also includes halving the radius every H iterations and maintaining a bound on the difference between iterations. After applying strong convexity and binary search, the objective value decreases multiplicatively with constant probability. By halving the radius every H iterations, a bound on the difference between iterations is maintained. This leads to the conclusion that after log(X/) epochs, xT - x* \u2264. The low-rank result is obtained by applying Theorem 10 and Theorem 11. However, the latent dimension k is unknown, requiring an extended binary search with an extra log(n) factor in the cost."
}