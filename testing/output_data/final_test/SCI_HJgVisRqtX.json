{
    "title": "HJgVisRqtX",
    "content": "Deep learning has seen significant success but faces criticism for its drawbacks. A new model called SEGEN offers an alternative approach by using a genetic-evolutionary strategy to build unit models from sampled sub-instances. This model aims to address the limitations of traditional deep learning models. SEGEN is a model that combines traditional machine learning or deep learning models with diffusive propagation and ensemble learning strategies. It requires less data and computational resources, with sound theoretic interpretability. Experimental results show its advantages over state-of-the-art representation learning models. The deep learning models have the capacity to capture good projections from input data space to output space, with applications in speech processing, language modeling, information retrieval, computer vision, and multi-task learning. Various deep learning models have been proposed, but they also face criticism for their disadvantages. SEGEN (Sample-Ensemble Genetic Evolutionary Network) is a new model proposed to address the disadvantages of traditional deep learning models, such as the need for large amounts of data, powerful computational facilities, heavy parameter tuning costs, and lack of theoretical explanation. This new model aims to overcome these limitations and provide a more practical solution for real-world applications. SEGEN (Sample-Ensemble Genetic Evolutionary Network) is a brand new model that offers an alternative approach to deep learning models. It uses a genetic-evolutionary learning strategy to train a group of unit models, which can be traditional machine learning models or deep learning models with a simpler structure. The model evolves by selecting good unit models from each generation based on their performance and creating the next generation with genetic crossover and mutation. The learning results of data instances are effectively combined from each generation. SEGEN effectively combines learning results from unit models using diffusive propagation and ensemble learning strategies. It offers advantages over existing deep learning models from both bionics and computational perspectives. In the bionics perspective, SEGEN models the evolution of creatures, where those suitable for the environment have a higher chance of survival and passing on good genes to offspring. Each unit network model in SEGEN is treated as an independent creature learning its own model variables. SEGEN utilizes diffusive propagation and ensemble learning to combine results from unit models. It requires less data and resources, with simpler architecture and shared hyper-parameters among unit models. The parent model's better performance increases the chance of passing variables to the child model. SEGEN combines diffusive propagation and ensemble learning to merge results from unit models, with a focus on interpretability and efficiency. The model uses genetic algorithms and ensemble learning to explain information inheritance and result ensemble. An example using network embedding problem BID25 BID2; BID20 is provided, along with applications on different data categories using CNN and MLP as unit models. The paper discusses the SEGEN model, which combines diffusive propagation and ensemble learning to merge results from unit models for interpretability and efficiency. Deep learning research and applications are also highlighted, showcasing the use of various deep learning methods in different applications such as speech processing, language modeling, and computer vision. Network embedding has become a hot research topic, projecting graph data into feature vectors. Various models like TransE, TransH, and TransR have been proposed. Recent works include Deepwalk, LINE, node2vec, HNE, and DNE. Perozzi et al. extended word2vec to network scenarios with Deepwalk. LINE algorithm preserves local and global information in networks. The LINE algorithm is proposed to embed networks, preserving local and global structures. BID9 introduces a flexible node neighborhood concept and biased random walk sampling. BID2 learns network embeddings with text and image data, while BID4 uses a task-guided model for author identification. The SEGEN model is illustrated for network representation learning, with a focus on large network datasets represented as graphs. The SEGEN framework focuses on network representation learning for large network datasets, such as online social media, e-commerce websites, and academic bibliographical data. It involves network sampling, sub-network representation learning, and result ensemble to extract sub-networks from input network data. The SEGEN framework aims to learn network representations for large datasets like social media and academic data. It involves sampling sub-networks from input data and learning a mapping to project nodes into a low-dimensional space while preserving the original network structure. SEGEN framework involves three steps: network sampling, sub-network representation learning, and result ensemble. It samples small-sized sub-networks from large-scale input data for representation learning and combines the learned results to obtain the final output. The SEGEN framework involves three steps: network sampling, sub-network representation learning, and result ensemble. In network sampling, different strategies are used to create small-sized sub-networks that capture local and global structures. One strategy, BFS based network sampling, randomly selects a seed node and expands to unreached nodes through Breadth-First-Search. It adds k-1 nodes from the neighbors of the seed node, or goes to 2-hop neighbors if necessary. The BFS based network sampling strategy selects a seed node and expands to unreached nodes through Breadth-First-Search, adding k-1 nodes from neighbors or 2-hop neighbors if needed. If the connected component size is smaller than k, another seed node is chosen to complete the sampling. The DFS based strategy works similarly but uses Depth-First-Search to reach unreached nodes. If the component size is less than k, it also selects another seed node to continue sampling. The Hybrid-Search (HS) network sampling strategy combines BFS and DFS to sample sub-networks with varying network connection patterns. It randomly selects seed nodes and expands to other nodes using either BFS or DFS, aiming to balance between capturing local and deeper network structures. The Hybrid-Search (HS) network sampling strategy randomly selects seed nodes and expands to other nodes using either BFS or DFS with probabilities p and (1 \u2212 p) respectively. The process continues until k nodes are selected, forming a sub-network. The sampled sub-networks are represented by the HS based network sampling strategy as pool G HS. In real-world networks, connections among nodes are usually sparse. In real-world networks, connections among nodes are usually sparse. To handle this, two sampling strategies are introduced. One is node sampling, where nodes are randomly picked, potentially resulting in isolated nodes. The other is biased node sampling, where nodes with more connections have higher probabilities of being sampled. This strategy considers the degrees of nodes to improve network properties. The biased edge sampling strategy focuses on sampling edges with probabilities proportional to their weights, capturing global network structures. This strategy complements biased node sampling and improves network properties by considering edge connections. The Genetic Evolutionary Network (GEN) model utilizes biased edge sampling and biased node sampling strategies to select sub-structures of the input network globally. The model evolves multiple generations of unit models to learn representation feature vectors of nodes. The Genetic Evolutionary Network (GEN) model evolves multiple generations of unit models to learn representation feature vectors of nodes. The initial generation of unit models is represented as set DISPLAYFORM1 i, with variables denoted as vector \u03b8 1 i. Variables are assigned random values from a standard normal distribution. The SEGEN framework is a general framework that works well for different types of data and base models. Autoencoder is an unsupervised neural network model that projects data instances to a lower-dimensional feature space through encoder and decoder steps. Each sampled sub-network is represented by an adjacency matrix. The encoding results in an objective feature space denoted as z i \u2208 R d. The autoencoder model projects data instances to a lower-dimensional feature space through encoder and decoder steps. The objective feature space is denoted as z i \u2208 R d. The relationships among variables are represented with equations. The traditional autoencoder model aims to minimize the loss between original and reconstructed feature vectors. In network representation learning, nodes in sub-networks have correlated feature vectors in the latent space. Connected nodes have closer representations, while isolated nodes have distant representations. The traditional autoencoder model aims to minimize loss between original and reconstructed feature vectors. To address the issue of trivial solutions with sparse input vectors, an extended correlated autoencoder model is proposed for networks. The objective includes correlation and regularization terms, with weight adjustments to preserve non-zero entries in the reconstructed vector. Training batches are sampled for each sub-network instead of fitting each unit model with all sub-networks in the pool. In the GEN model, sub-network training batches are sampled for each unit model in the learning process. The unit models learn variables based on sub-networks in the batches, with a focus on smaller hidden layers for faster learning. Different performance levels can be observed among unit models in the generation set. In the SEGEN framework, unit models evolve through generations using a genetic algorithm style method. Well-trained unit models are selected based on their performance on a validation set. The probability of each unit model being chosen for crossover and mutation operations is determined. In the SEGEN framework, unit models evolve through generations using a genetic algorithm style method. A normalization of loss terms among unit models is necessary for real-world applications. The crossover process selects pairs of parent models from a set to generate the next generation of unit models. The genes of parent models are denoted as variables for crossover and mutation, with uniform crossover proposed to obtain the chromosomes of their child model. In the SEGEN framework, unit models evolve through generations using a genetic algorithm. The crossover process selects parent models based on performance to pass chromosomes to the child model. Mutation alters some variables in the chromosome vector with a low probability. In the SEGEN framework, unit models evolve through generations using a genetic algorithm. The chromosome vector \u03b8 2 k defines a new unit model with knowledge inherited from parent models. The hierarchical result ensemble method involves local and global ensemble steps for sub-networks on different sampling strategies. The K th generation of the GEN model M K contains m unit models. In the SEGEN framework, the K th generation of the GEN model M K contains m unit models. This part introduces how to fuse learned representations from sub-networks with unit models. Representation vectors are created for nodes in sub-networks using unit models. Nodes not selected in sub-networks will not have a learned representation feature vector. In the SEGEN framework, representation vectors are created for nodes in sub-networks using unit models. Nodes not selected in sub-networks will not have a learned representation feature vector. To compute the representation for these non-appearing nodes, the learned representation is propagated from their neighborhoods. Different network sampling strategies capture various local/global structures for node representation learning. In the SEGEN framework, representation vectors are created for nodes in sub-networks using unit models. Different network sampling strategies capture various local/global structures for node representation learning. In the global result ensemble step, features are grouped together as the output, denoted by different representation feature vectors for nodes. Weight parameters can be learned with the complete network structure, but may introduce extra time costs and degrade efficiency. In the SEGEN framework, representation vectors are created for nodes in sub-networks using unit models. SEGEN is a deep model that utilizes an \"evolutionary layer\" for validation, selection, crossover, and mutation operations between successive generations. This allows for the immigration of learned knowledge from generation to generation, illustrating the advantages of SEGEN compared to other deep learning models. SEGEN utilizes an evolutionary layer for validation, selection, crossover, and mutation operations between generations, allowing for the transfer of learned knowledge. Research on genetic algorithms provides theoretical foundations for SEGEN. The unit models in different generations may perform differently due to various factors. SEGEN combines results from multiple unit models to achieve better performance. SEGEN utilizes an evolutionary layer for validation, selection, crossover, and mutation operations between generations, allowing for the transfer of learned knowledge. It combines results from multiple unit models to achieve better performance than each individual model. The key parameters used in SEGEN include sampling, original data size, sub-instance size, and pool size. The space complexity of the SEGEN model involves storing large-scale networks in matrix representation and sub-networks through network sampling. SEGEN's space cost is linear to n, much smaller than O(n^2). The time complexity is reduced to O(Kmcf(n)n + Kmdn), linear to n, making it advantageous compared to existing deep learning models. SEGEN offers advantages such as requiring less data for unit model learning, fewer computational resources, less parameter tuning, and a sound theoretic explanation. SEGEN will be tested on real-world network datasets including social networks, images, and raw feature representation datasets. The experiments will cover detailed settings, convergence analysis, parameter analysis, and results on social network datasets. Additionally, experiments will be conducted on image and raw feature datasets using CNN and MLP models. The experiments in SEGEN involve network datasets from Twitter and Foursquare, with specific user and social connection numbers. Sub-networks are sampled from the input datasets using network sampling strategies, leading to the creation of unit models in K generations. The ensemble output is generated based on the learning results from the final generation. SEGEN combines learned sub-network representations using genetic algorithm and ensemble learning to generate network feature vectors. The model also includes diffusive propagation for nodes not sampled in sub-networks. Evaluation includes network recovery and community detection tasks, with parameter sensitivity analysis provided. Comparison models include LINE for scalable network embedding. The LINE model optimizes an objective function preserving local and global network structures using an edge-sampling algorithm. DEEPWALK extends word2vec to network embedding, utilizing local information from random walks. NODE2VEC introduces a flexible node neighborhood concept for representation learning. HPE projects information from heterogeneous networks to a low-dimensional space for user preference learning. Evaluation of network representation learning is typically task-based. In this paper, the evaluation of learned representation features from comparison methods is done using application tasks like network recovery and clustering. Metrics such as AUC, Precision@500, Density, and Silhouette are used for evaluation. The default parameter setting for SEGEN in experiments is Parameter Setting 1 (PS1). Model training convergence analysis and detailed analysis of pool sampling and model learning parameters are available in the Appendix. Performance analysis of SEGEN and baseline methods is provided in TAB0 with specified parameter settings. In TAB0, method performance rankings are shown with network recovery and community detection results. Different parameter settings are evaluated using metrics like AUC, Prec@500, Density, and Silhouette. PS2 and PS3 are additional parameter settings for specific tasks. In TAB0, method SEGEN with PS2 shows strong performance for network recovery and community detection tasks. It achieves a high AUC score of 0.909 for network recovery, ranking second overall. Additionally, it achieves the second highest Prec@500 score for both tasks among comparison methods. SEGEN with PS3 ranks second/third among comparison methods for community detection, with good performance for density and silhouette metrics. Variants based on specific sampling strategies show biased performance, excelling in network recovery but lagging in community detection. Model SEGEN, combining different sampling strategies, achieves balanced performance for various tasks, outperforming baseline methods like LINE, HPE, DEEPWALK, and NODE2VEC. SEGEN demonstrates effectiveness in network representation learning, as shown in experimental results on different datasets. In the experimental results, SEGEN, utilizing a Convolutional Neural Network (CNN) model, outperforms classic methods like LeNet-5, SVM, Random Forest, Deep Belief Net, and state-of-the-art method gcForest on the MNIST dataset. The CNN model in SEGEN consists of 2 convolutional layers, 2 max-pooling layers, and two fully connected layers with ReLU activation and Adam optimization. The best unit model's learning results in the final generation are used as the final results. SEGEN (CNN) outperforms baseline methods like LeNet-5, SVM, Random Forest, Deep Belief Net, and gcForest on the MNIST dataset with an accuracy rate of 99.37%. In addition, SEGEN's Multi-Layer Perceptron (MLP) architecture varies for different benchmark datasets like YEAST, ADULT, and LETTER, with specific neuron numbers and layers for each dataset. SEGEN (MLP) utilizes neurons in layers 16-64-48-32-26, trained with Adam optimization algorithm. The ensemble strategy selects the best unit model for final predictions. SEGEN (MLP) performs well on raw feature datasets, especially YEAST and ADULT. It competes with gcForest on the LETTER dataset. SEGEN introduces a unique approach by building unit models generation by generation, different from traditional deep learning models. SEGEN can use traditional machine learning or deep learning models with smaller architecture. It requires less data and resources but offers more insight into its learning process. Experimental analysis shows that larger sub-network and pool sizes improve network recovery, while smaller sizes are better for community detection. SEGEN performs best with smaller sub-network sizes for community detection and larger sizes for network recovery tasks. Optimal parameter values vary for different tasks, with sub-network sizes of 45 and 50 for community detection and network recovery respectively. Parameter sensitivity analysis was conducted on batch size and generation size for Foursquare and Twitter networks. For network recovery, SEGEN performs best with generation size of 50 and batch size of 5. For community detection, optimal values are 5 for generation size and 35 for batch size. Twitter network shows that smaller generation size leads to better performance in Prec@500 evaluation. SEGEN evaluated by Prec@500 with minor AUC fluctuation. Optimal values for network recovery task: generation size 5, batch size 10. For Twitter community detection task, optimal values are generation size 5, batch size 40."
}