{
    "title": "HklPzxHFwB",
    "content": "Domain adaptation in deep reinforcement learning is a challenge where agents struggle to perform in environments with limited data. SADALA, a new RL agent, focuses on learning a compressed state representation and filtering out irrelevant visual features to improve domain transfer performance. It outperforms previous methods in RL environments like Visual Cartpole and DeepMind Lab. Deep reinforcement learning agents, like SADALA, aim to excel in tasks by maximizing rewards through actions based on observations. Despite successes in various domains, deep RL faces challenges like high sample complexity and task specificity. To address this, agents are often trained in simulations and must transfer knowledge to real-world scenarios, facing the reality gap issue. Training deep RL agents in simulations and transferring knowledge to the real world presents challenges due to the reality gap. The difference in dynamics and visual observations between simulation and reality makes transfer difficult. A method using attention and a \u03b2 variational autoencoder is proposed to learn a state representation that can solve tasks in both domains without requiring target information. Our approach focuses on learning disentangled state representation, eliminating the need for target domain samples during training. This representation allows the RL agent to prioritize relevant state information while disregarding distracting details. Domain randomization, a popular transfer method in RL, involves training on multiple source domains to ignore variations. However, manual selection of variation modes can lead to failure when faced with new variations. Additionally, domain randomization may destabilize training for some RL algorithms like A3C and DDPG. Despite its success in transfer tasks, domain randomization has limitations. Some visual domain adaptation methods use image-to-image translation with Generative Adversarial Networks to map source domain inputs to target domain inputs. However, this approach incurs additional overhead at inference time due to high-dimensional images and requires samples from the target domain. Some recent work focuses on modifying inputs from the source domain to match the target domain using a GAN. This approach addresses domain shift without adding overhead at inference time in the target domain, but still requires samples from the target domain during training. Recent work in domain adaptation for reinforcement learning takes a different approach by mapping all domains to a canonical domain where the RL agent operates. This method involves training a translation network using domain randomization, showing simulation to reality transfer. However, there is overhead from the mapping network. Another approach involves using a \u03b2-VAE to learn a compressed, disentangled state representation for image inputs in RL, but the state representation is not task-specific. The \u03b2-VAE is used to learn a compressed state representation for image inputs in RL, preserving information needed for image reconstruction. The RL agent is trained on multiple source domains to address distribution shift between domains. Transfer between related MDPs is formalized, with source domain denoted as DS and target domain as DT. Both domains are MDPs defined by states, actions, transition function, and reward function. In transfer learning between related MDPs, states and actions may differ, but transition and reward functions are similar. A family of MDPs denoted as M share the same transitions, rewards, and actions but have different states generated from latent state factors. The observed states are rendered by specific renderers for each MDP. The target dynamics are considered similar to the source dynamics when rewards, actions, and true states are identical, but observed states differ. Deep RL agents learn a mapping from states to actions using a deep neural network. The network implicitly learns a mapping function for a specific MDP, but struggles to transfer to other MDPs within the same family due to overfitting. DARLA proposed learning a feature extractor F that maps observed states to latent state factors, which are disentangled and lead to better transfer results in RL tasks. SADALA, similar to DARLA, learns to ignore irrelevant features by utilizing a soft attention mechanism in mapping extracted latent state factors to solve the MDP. The SADALA framework utilizes a soft attention mechanism to assign importance weights to features in a neural network, allowing the RL agent to ignore or down-weight irrelevant features during inference. The SADALA framework uses a \u03b2-VAE to extract latent state factors, which are then fed into an attention mechanism to generate weighted features for deep reinforcement learning. Training involves optimizing a neural network jointly for feature selection and policy learning. The \u03b2-VAE is trained using a modified loss term to encourage disentanglement of the latent space. The SADALA framework utilizes a \u03b2-VAE to extract latent state factors for deep reinforcement learning. Training involves optimizing a neural network jointly for feature selection and policy learning, with the \u03b2-VAE trained to encourage disentanglement of the latent space. Freezing the network weights of the \u03b2-VAE is crucial to prevent overfitting to the source domain. The framework can work with various deep RL algorithms with minor modifications. The SADALA framework utilizes a \u03b2-VAE for deep reinforcement learning, with a focus on feature selection and policy learning. A new loss term is added to encourage sparsity in feature selection by applying L1 regularization on attention weights. Another modification ensures that the ordering of latent state variables remains consistent across frames within an episode. This consistency is enforced by maintaining the same weight for a latent state factor regardless of input, similar to learning a pseudo-attention weight. The SADALA framework uses a \u03b2-VAE for deep reinforcement learning, focusing on feature selection and policy learning. It includes a new loss term for sparsity in feature selection and maintains consistent ordering of latent state variables. The framework is tested on transfer learning tasks using A3C as the deep RL algorithm, including the Visual Cartpole task with modifications for transfer learning across different color configurations. The agent in the SADALA framework learns to ignore irrelevant factors in the latent state to balance the pole. In another task, it navigates to collect \"good\" objects while avoiding \"bad\" objects in a first-person view environment. The agent in the SADALA framework is trained to ignore distracting factors like the color of walls and floor while focusing on relevant factors like object types and positions. The algorithm's results are tested using a \u03b2-VAE for state representation, similar to DARLA. The \u03b2-VAE in the SADALA framework has learned to represent cart position and pole angle, with some minor inaccuracies in color reconstruction. The extracted latent state factors are sufficient to solve the MDP, as shown in reconstructions with attention. In the reconstruction, the cart is centered and the pole is almost upright, with attention weights learning to ignore color. Performance comparison of algorithms shows single-task learner outperforms transfer-specific agents on source tasks. Domain randomization is less effective due to task complexity. The agent must optimize reward across multiple domains, but DARLA, SADALA, and SADALA with reduced variance perform less well due to imperfections in the \u03b2-VAE. The \u03b2-VAE struggles to reconstruct input images perfectly, leading to potentially confusing states for the RL agent. Transfer of knowledge varies among different agents, with SADALA transferring the most knowledge to the target domain. The single-task agent in SADALA lacks incentive to learn a transferable state representation, hindering knowledge transfer. DARLA, on the other hand, learns a disentangled state representation that helps the RL agent focus on important features. However, the RL policy may struggle to ignore irrelevant factors when faced with unseen information. Domain randomization pushes the neural network to learn a sufficient state representation. SADALA improves on DARLA by incorporating an explicit attention mechanism that helps ignore unimportant features. The attention weights are bounded between 0 and 1 due to the sigmoid activation, reducing variance across inputs. SADALA with variance reduction performs differently on tasks compared to DARLA and SADALA without variance reduction. In cases where latent state factors are perfectly disentangled, static attention weights should suffice for solving tasks and transferring knowledge effectively. SADALA is a three-stage method for zero-shot domain transfer that learns a feature extractor to represent input states as disentangled factors. It filters these factors using an attention mechanism to select the most important ones for solving a source task. The method aims to transfer knowledge to related target tasks and has been validated on a high-dimensional continuous-control problem like Visual Cartpole. SADALA demonstrates successful transfer learning on high-dimensional continuous-control problems and 3D naturalistic environments. The attention mechanism helps differentiate important latent features for robust transfer. Agents are evaluated on source domains and a Collect Good Objects task in Deepmind Lab. The goal is to pick up \"good\" objects for rewards and avoid \"bad\" objects. The beta-VAE is trained on a set of MDPs covering all latent state factor combinations, while the RL network and attention mechanism are trained on a subset of N Deepmind Lab MDPs. The neural network for the Visual Cartpole task consists of a beta-VAE network and a policy network with specific architecture details. The beta-VAE is utilized as a feature extractor for the policy network in the Visual Cartpole task. The encoder from the beta-VAE is frozen and passed through dense layers to output attention weights, which are then used to calculate the policy and value of the input state. The Domain Randomization agent also utilizes similar networks. The neural networks used for the Domain Randomization agent and Single-Task learner have a similar structure. They pass the input through convolutional filters and dense layers to output the policy and value. For the Deepmind Lab task, the networks have additional convolutional layers and larger dense layers in the policy network. The reconstruction loss in Deepmind Lab is handled in the latent space of a denoising autoencoder with 100 latent factors. The neural networks for the Domain Randomization agent and Single-Task learner have a similar structure, passing input through convolutional filters and dense layers to output policy and value. In Deepmind Lab, additional convolutional layers and larger dense layers are used in the policy network. The reconstruction loss is handled in the latent space of a denoising autoencoder with 100 latent factors. Training details include the use of Relu activations, Adam optimizer with specific learning rates, and training on sets of images and states for different tasks. Domain randomization involves varying parameters for implementation."
}