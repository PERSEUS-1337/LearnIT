{
    "title": "rJrTwxbCb",
    "content": "We empirically study the properties of common loss surfaces through their Hessian matrix in deep learning. The spectrum of the Hessian consists of a bulk near zero and outliers. Our findings support conjectures that increasing parameters scales the bulk, while changing data affects outliers. This has implications for non-convex optimization in high dimensions, challenging classical notions of basins of attraction. The discussion focuses on the geometry of loss surfaces in supervised learning problems, emphasizing the connection between wide/narrow basins and over-parametrization. It explores the relationship between large eigenvalues and the covariance matrix of gradients, suggesting a new perspective on the data-architecture-algorithm framework. The study shows that small and large batch gradient descent converge to different basins but are connected through their flat regions, belonging to the same basin. The study delves into the geometry of loss surfaces in supervised learning, focusing on second-order properties. It considers data in input-label pairs, a parametrized model, and a predictor aiming to approximate the true label. The goal is to minimize a non-negative loss function to find the optimal parameter w*. In supervised learning, the goal is to minimize a non-negative loss function to find the optimal parameter w*. The law of large numbers states that L w \u2192L w almost surely as N \u2192 \u221e for fixed M. However, in modern applications like deep learning, the number of parameters M can be comparable to or larger than the number of examples N. Gradient descent is a classical algorithm used to find w*, where the optimization process is carried out using the gradient of L. Gradient computation and line-search can be expensive, leading to the development of more complex algorithms like Newton-type methods that utilize second-order information. The discussion focuses on the use of stochastic gradient descent (SGD) as an alternative to gradient descent (GD) due to computational cost and practicality issues. SGD approximates the true gradient by using averages of losses over mini-batches, making it more efficient for real-life time constraints. Understanding the geometry of the loss surface can provide insights into how SGD and GD algorithms compare in locating optimal parameters. The comparison between stochastic gradient descent (SGD) and gradient descent (GD) algorithms focuses on their nature of solutions, paths followed, and generalization performance. The issue of expensive line-search in optimization has two classical solutions: using a small, constant step size or scheduling the step size according to a rule. In deep learning, step size values are often determined heuristically through trial and error. Obtaining the Hessian of the loss function is computationally expensive, but obtaining its largest and smallest eigenvalues and eigenvectors is more feasible. The question arises whether knowing only the large magnitude eigenvalues and eigenvectors is sufficient for optimization. The Hessian matrix at a critical point determines the nature of the point, with positive eigenvalues indicating a local minimum and a mix of positive and negative eigenvalues indicating a saddle point. Gradient-based methods converge to points where the gradient is zero, with recent studies confirming convergence to minimizers. The Hessian matrix at critical points determines the nature of the point, with positive eigenvalues indicating a local minimum. Recent studies confirm convergence to minimizers, but a significant assumption is the non-degeneracy of the Hessian. Comparison of GD and SGD in neural networks shows that large eigenvalues of the Hessian can create the illusion of local minima, with SGD's noise helping to overcome this obstacle. The observation from BID6 and numerical justifications suggest that low-dimensional neural networks with few hidden units were used due to computational limits. However, in higher dimensions, local minima concentrate near the global minima in certain non-convex functions. Studies by BID22 and BID2 show that high error local minima traps are avoided in over-parametrized models. This explains why optimizers like GD and SGD often yield similar training accuracies despite finding comparable solutions in terms of training error. Recent work compares generalization performance of small batch and large batch methods, showing that large batch methods generalize slightly worse despite similar training accuracies. Small batch methods find wider basins, which are believed to generalize better. The landscape of neural networks is observed to have flat regions where weights can be perturbed without changing loss significantly, known as flat minima. The landscape of neural networks contains flat minima, wide regions with better generalization properties. A new loss function utilizing the Hessian has been proposed to target these flat minima. Computational complexity issues have been addressed using the R-operator, but exact numerical calculations of the Hessian show near-zero eigenvalues and a spectrum composed of bulk and outlier components. The Hessian of neural networks shows mostly zero eigenvalues with outliers depending on the data, indicating flatness in weight space. A study explores the local geometry at the bottom of the landscape, highlighting the connectedness of basins and the presence of near-zero eigenvalues in the Hessian. The Hessian of neural networks can be decomposed into two matrices, with the second term being ignored during training progress. The covariance term leads to degeneracy in the Hessian when parameters exceed samples. Empirical examination of the eigenvalue spectrum reveals dependencies on data complexity, network size, batch methods, and the presence of negative eigenvalues post-training. The Hessian of neural networks can lead to degeneracy with the covariance term, causing issues with negative eigenvalues. The notion of isolated basins in the landscape may be misleading due to the flatness of local geometry. Loss evaluation on a line connecting two solutions can show them to be in the same basin, challenging the idea of isolated basins. Data complexity and over-parametrization are used vaguely in the paper, requiring further research for precise definitions. Further research is needed to link complexity to the spectrum and define overparametrization precisely. The notion of basins can be defined, but it's unclear if practical algorithms can locate the bottom accurately. Issues with algorithms may prevent reaching the bottom of basins, requiring further study. Further research is needed to link complexity to the spectrum and define overparametrization precisely. The notion of basins can be defined, but it's unclear if practical algorithms can locate the bottom accurately. Issues with algorithms may prevent reaching the bottom of basins, requiring further study. The algorithm's fault needs further investigation, and notions of sharp vs. wide minima should be viewed cautiously. Recent work shows how 'sharp minima' can still generalize with proper modifications to the loss function. Non-linear transformations are needed to deform relative widths of basins, and the focus is on relative values for consistent comparison across different setups. The full spectrum of the Hessian at random initial and final training points of a two hidden layer network with 5K parameters trained using gradient descent is shown in FIG2. The exact full Hessian is computed via Hessian-vector products up to machine precision. The eigenvalues are ordered on the x-axis, indicating the scale of degeneracy, while the y-axis shows their values. The Hessian can be decomposed into two meaningful matrices to study its spectrum. The model function f is the output of a network dependent on parameters, and the loss function is a convex function. Examples include regression with mean-square loss and classification with negative log-likelihood loss. The gradient and Hessian of the loss are calculated, with the Hessian rewritten for convexity. The Hessian of the loss can be rewritten as the sum of rank one matrices, implying a product of an M \u00d7 N matrix with its transpose. This approximation is valid when (f (\u0175)) and \u2207 2 f (\u0175) are uncorrelated, allowing us to ignore the second term. The Hessian of the loss can be approximated as a sum of rank one matrices, involving an M \u00d7 N matrix and its transpose. This approximation is valid when (f (\u0175)) and \u2207 2 f (\u0175) are uncorrelated. Theoretical tools are needed to map eigenvalues of the population matrix to the sample covariance matrix. Recent results on this can be found in BID3. Independent inputs are required for these results, and extensions to correlated data are currently unavailable. Experimental results on the spectrum of the full Hessian are discussed, focusing on how data, model, and algorithm impact the Hessian's spectrum. Redundancies in the data can lead to a lower number of non-trivial eigenvalues. The data may have redundancies leading to fewer non-trivial eigenvalues in the Hessian. In a classification problem with k classes and small deviations, there could be around k non-trivial eigenvalues. Testing this idea involved using a neural network with specific dimensions and training with SGD on ReLU network. The experiment involved training neural networks with different numbers of clusters and analyzing the large eigenvalues in the Hessian. The study tested the effect of increasing network size while keeping data, architecture, and algorithm constant. Four networks with varying hidden layer sizes were trained on a subset of the MNIST dataset to observe changes in the Hessian's eigenvalues. The study trained neural networks with different hidden layer sizes on a subset of the MNIST dataset. The Hessian's largest 120 eigenvalues were analyzed, showing consistent shapes for large positive eigenvalues as the number of parameters increased. The training landscape's nature was described for both small batch (mini-batch size of 10) and large batch (mini-batch size of 512) methods using a fixed learning rate. The study analyzed the Hessian's eigenvalues for neural networks trained on MNIST with different hidden layer sizes. The large batch method showed points with larger positive eigenvalues, consistent with previous research. Negative eigenvalues at the end of training were significantly smaller than positive ones. The study analyzed the Hessian's eigenvalues for neural networks trained on MNIST with different hidden layer sizes. Negative eigenvalues indicate algorithm hasn't reached a local minimum yet. Training stops when loss value stops decreasing or test accuracy stops increasing. In experiments, training continued beyond saturation point with loss decreasing in smaller values. Convergence to a local minimum may occur at large time-scales. Negative eigenvalues are smaller compared to positive ones. Ranking negative eigenvalues in percentages is preferred over order. The study analyzed the Hessian's eigenvalues for neural networks trained on MNIST with different hidden layer sizes. Negative eigenvalues indicate algorithm hasn't reached a local minimum yet. Training stops when loss value stops decreasing or test accuracy stops increasing. In experiments, training continued beyond saturation point with loss decreasing in smaller values. Convergence to a local minimum may occur at large time-scales. Negative eigenvalues are smaller compared to positive ones. Ranking negative eigenvalues in percentages is preferred over order. The experiment in Section 3.2 showed that adding more weights scales small scale eigenvalues proportionally, with the number of outliers remaining unchanged. The ratio of negative x-axis indicates the order of the eigenvalue in percentages, y-axis indicates the eigenvalues. Negative eigenvalues can only come from the second term of the decomposition in Equation 5, confirming that the effect of the ignored term is small. The discussion around overparametrization and GD vs. SGD is revisited. In the final section, it is argued that overparametrization does not necessarily lead to GD and SGD falling into different basins. Previous beliefs that SGD performs better due to non-convex landscapes trapping large-batch methods have been challenged by experiments showing GD can achieve similar loss values as SGD for large systems. MNIST can be trained by GD to reach the same loss values as SGD. Training accuracy and test accuracies are similar for both algorithms, with GD slightly falling behind as the network size increases. Over-parametrization leads to a flat landscape that is easy to optimize. Large batch methods find a different basin compared to small batch methods for generalization. Large batch (LB) methods find different basins compared to small batch (SB) methods, characterized by wider basins with larger eigenvalues. Despite the presence of numerous flat directions, LB converges to sharper basins separated by wells from the wider basins found by SB. Two solutions with different generalization error qualities are shown to be in the same 'basin' by evaluating loss without encountering barriers between them. The common pitfalls in testing this include the issue with epoch-based time scales in larger scale neural networks. One way to compare training profiles in larger scale neural networks is to calculate various statistics of the model at each epoch. However, comparing training with different batch sizes can be problematic as larger batch models take fewer steps in a given epoch. To ensure a fair comparison between different batch sizes, it is important to scale the comparison with the number of steps taken by each algorithm. The experiments focus on training neural networks with symmetries in weight space. The issue arises from flip symmetries in network architectures, leading to similar configurations that may appear different due to reordering. To address this, dynamics are switched in an already trained system to alleviate the problem. The experiments involve training neural networks with symmetries in weight space to address flip symmetries in network architectures. The dynamics of an already trained system are altered by training a bare AlexNet on full CIFAR10 data with a batch-size of 1,000, followed by training with a smaller batch-size of 32. The key observation is a jump in training and test losses, a drop in accuracies, and the small batch reaching slightly better accuracy towards the end. This aligns with the idea that small batch solutions generalize better, with sharpness confirmed by line interpolations. The experiments involve training neural networks with symmetries in weight space to address flip symmetries in network architectures. The dynamics of an already trained system are altered by training a bare AlexNet on full CIFAR10 data with a batch-size of 1,000, followed by training with a smaller batch-size of 32. The key observation is a jump in training and test losses, a drop in accuracies, and the small batch reaching slightly better accuracy towards the end. Line interpolations confirm sharpness, suggesting connections between different solutions in weight space. The loss increases initially due to fluctuations in directions of large eigenvalues, settling at a point within the same level set. Accuracy comparison shows a slight advantage for one solution over another, with no barrier between them. The connected structure of the solution space is highlighted, with recent work showing connectivity in solution paths for rectified neural networks. The exploration of interiors of level sets in the high dimensional flat space of neural networks may lead to better generalization. The training curve consists of a high gain part and a noisy part, with speculation that larger batches and step sizes can accelerate training. Using larger batches with larger step sizes can accelerate training, as seen in BID10 where Imagenet training with a minibatch size of 8192 matched small batch performance. Standard pre-processing and initialization methods were used, but the effects of these on the Hessian are complex and require further study. The singularity of the Hessian cannot be ignored, and the cluster of zero eigenvalues is expected in practical applications. In deep learning, even large batch methods can reach the same basin as small batch methods, challenging common intuition. The generalization gap between the two is not due to different basins but rather the same basin. Paths between the two solutions lie in the same level set, indicating no barrier between them. This suggests a shift in perspective on the energy landscape in deep learning. In deep learning, large batch methods can reach the same basin as small batch methods, challenging common intuition. The generalization gap is not due to different basins but rather the same basin. Paths between the two solutions lie in the same level set, indicating no barrier between them. This suggests a shift in perspective on the energy landscape in deep learning. Additionally, the spectrum of the Generalized Gauss-Newton matrix can be characterized theoretically under certain conditions, showing the relationship between the scaled gradient and parameters in linear models. One of the first steps in studying G involves understanding its principle components, specifically how the eigenvalues and eigenvectors of G relate to those of \u03a3. In the simplest case where \u03a3 = Id, the eigenvalues of G follow the Mar\u010denko-Pastur law as N, M \u2192 \u221e. The width of the nontrivial distribution depends on the ratio \u03b1, with M \u2212 N trivial eigenvalues at zero if M > N. Different relaxations to setting the expected covariance to identity have been proposed in literature. One of the earliest relaxations in studying principle components involves a phase transition for the largest eigenvalues of the sample covariance matrix known as the BBP phase transition. The analysis becomes more challenging when correlations are present, as the spectrum of the covariance matrix is not necessarily diagonal. Recent work by BID3 addresses this more general case with non-trivial correlations. Recent work by BID3 has provided new insights on correlations in the spectrum of the covariance matrix. The results show a precise mapping of the spectrum of \u03a3 to G as M, N \u2192 \u221e for fixed \u03b1 = M/N. The spectrum decomposition includes zeros, bulk, right outliers, and left outliers, with eigenvectors of outliers of G closely related to the corresponding eigenvalues of \u03a3. The theorem describes how outlier eigenvalues are obtained in the sample covariance matrix assuming the population covariance is known. An example is given with logistic regression loss and tanh unit, showing outlier eigenvalues for different scenarios. The eigenvectors of outliers in G are closely related to the corresponding ones in \u03a3. The Hessian of the loss is just the first term, with the gradient per sample in a specific form. In the case of more than one class, the spectrum changes, resulting in one large outlier eigenvalue and a bulk close to zero."
}