{
    "title": "rylVvNS3hE",
    "content": "In convolutional neural networks (CNNs) utilizing Batch Normalization and ReLU activation, filter level sparsity is observed during training with L2 regularization or weight decay. An empirical study explores the mechanism behind this sparsification process, revealing the impact of various factors on regularizers. The findings highlight the emergence of filter level sparsity in feedforward CNNs, with filters representing weights and nonlinearity associated with specific features. Additional results on modern network architectures like ResNet-50 are also discussed. The curr_chunk discusses the emergence of sparsity in filters after training in networks using Batch Normalization and ReLU activation, even with regularizers like L2 and weight decay. The sparsity disappears when regularization is removed. The study focuses on the implications of this sparsity on neural network speed and its role in the adaptive vs vanilla SGD generalization debate. The study observed higher sparsity with adaptive SGD compared to (m)SGD, even with leaky ReLU. Sparsity is influenced by L2 regularization and hyperparameters like mini-batch size and network size. Selective features receive more regularization, explaining the impact of unrelated parameters on sparsity. The text explains how parameters like mini-batch size, network size, and task difficulty indirectly affect sparsity by influencing feature selectivity. Adaptive methods like Adam may lead to higher sparsity due to learning more selective features. Synthetic experiments show that the interaction of L2 regularizer with the update equation in adaptive methods causes stronger regularization than weight decay. Feature sparsity can be measured by per-feature activation and scale. The text discusses how feature sparsity can be measured by per-feature activation and scale, with thresholds set at 10^-12 and 10^-3 respectively. Zeroing inactive features does not impact test error, validating the chosen thresholds. Different thresholds show comparable levels of sparsity. BasicNet, a 7-layer convolutional network, is used for experiments on CIFAR-10/100 with various gradient descent approaches. The experiments on CIFAR-10/100 involve using different gradient descent methods with specific base learning rates and hyperparameters. The study shows that convolutional filter sparsity increases with adaptive gradient descent and L2 regularization. However, sparsity decreases with Weight Decay and is absent with SGD. Leaky ReLU does not prevent sparsification. The emergence of sparsity is not specific to CIFAR-10/100 and BasicNet, as shown in experiments with VGG-11/16 and ResNet-50 on ImageNet and Tiny-ImageNet. Decreasing minibatch size increases sparsity across network architectures and datasets. Both SGD and Adam (L2: 1e-5) learn selective features in layer C6, with Adam being more selective. Strong regularization or infrequent gradient updates can lead to feature pruning. Adam tends to learn more sparse features in later layers compared to SGD. The higher degree of sparsity in later layers compared to SGD is explained. Future investigation is needed to understand why Adam shows higher feature selectivity. Feature selectivity is quantified using max pooling over the entire feature plane. Adam and other adaptive gradient descent methods may learn more selective features than SGD. The interaction of L2 regularizer with Adam is also discussed. The role of L2 regularization in the low gradient regime for different optimizers is explored, showing that coupling L2 regularization with certain adaptive gradient update equations leads to faster decay than weight decay or L2 regularization with SGD. This disparity in regularization affects parameters that receive frequent updates differently from those that don't, especially for certain adaptive gradient descent approaches. Additionally, as tasks become more difficult, the fraction of features pruned decreases, impacting the selectivity of the learned features. In a synthetic experiment based on grayscale renderings of 30 object classes from ObjectNet3D, two sets of images were created with different backgrounds - one clean and one cluttered. Training BasicNet showed higher sparsity with the clean background set compared to the cluttered set. Liu et al. (2017) used explicit filter sparsification heuristics based on Batch Norm's scale parameter \u03b3 to enforce sparsity on filters. Based on previous experiments using BatchNorm's scale parameter \u03b3 for filter sparsification, it was found that feature importance is less affected by scaling reparameterization. The criterion for implicit feature pruning was based on the learned scale parameters (\u03b3), which can indicate feature importance. Studies suggest that good generalization ability is associated with reduced selectivity of learned features, with less selective units playing a stronger role in overall task performance. Ablation of selective features in neural networks may not significantly impact overall accuracy, as shown in various experimental evaluations. The emergence of feature selectivity is linked to poor generalization performance, and removing class-specific features may not affect performance significantly. Our findings suggest that the emergence of selective features in Adam, along with the increased propensity for pruning these features with L2 regularization, can improve generalization performance and network speed. This highlights the importance of considering seemingly unrelated hyperparameters that can impact network capacity, test accuracy, and generalization gap. Our work also sheds light on the practical performance gap between Adam and SGD, opening avenues for further exploration into feature selectivity in adaptive SGD methods. The emergence of selective features in Adam, along with the increased propensity for pruning these features with L2 regularization, can improve generalization performance and network speed. Future work should reconsider the selective-feature pruning criteria and examine non-selective features as well, which could also be pruned to impact accuracy."
}