{
    "title": "rJgYxn09Fm",
    "content": "We introduce a parameter sharing scheme in convolutional neural networks (CNNs) using a learned linear combination of parameter tensors from a global bank of templates. This hybrid approach combines traditional CNNs and recurrent networks, resulting in significant parameter savings on image classification tasks without sacrificing accuracy. The scheme often leads to networks with recurrent structures and competitive accuracy compared to state-of-the-art neural architecture search procedures. Our hybridization of recurrent and convolutional networks shows benefits in training speed and generalization to new test examples. While CNNs have been extensively studied for their circuit-like structure, our approach combines the strengths of both traditional computer programs and neural networks for improved performance on algorithmic tasks. Our approach combines the strengths of traditional computer programs and neural networks by introducing modularity into CNNs, allowing for parameter reuse across multiple layers. This enables learning loop-like structures within the network, leading to significant improvements over standard CNN models. Our parameter sharing scheme for CNNs allows loop-like structures to emerge within the network, combining the strengths of CNNs and RNNs. This approach introduces modularity and enables parameter reuse across layers, leading to significant improvements in model performance. Our method introduces a parameter sharing scheme for CNNs, allowing for the emergence of loop-like structures within the network. This approach combines the strengths of CNNs and RNNs, leading to significant improvements in model performance. The scheme involves learning layer templates through a linear function, facilitating the sharing of global parameter templates across different network layers. This re-parameterization is fully differentiable, enabling the learning of sharing weights and template parameters. Experimental results demonstrate the advantages of our hybrid CNNs in various settings, particularly in terms of parameter efficiency and image classification tasks. In image classification tasks, a parameter sharing scheme reduces unique parameters needed for accuracy on CIFAR or ImageNet tasks. Re-parameterizing with this scheme cuts parameters without accuracy loss, suggesting standard CNNs may be overparameterized. The hybrid networks aim to expand the range of tasks trainable by neural networks. Our hybrid CNNs show improved generalization and learning speed on tasks involving planar graphs encoded as image input. The parameter sharing scheme in our architecture biases networks with loops, aiding in learning traditional algorithms. Additionally, layers often converge to the same parameter values during training. The text discusses how layers in a CNN often converge to the same parameter values, allowing for representation of the network as a loopy wiring diagram. The approach achieves competitive accuracy with existing neural architecture search techniques. The method is detailed in Sections 2, 3, and 4, with conclusions and future research pathways in Section 5. The text discusses the use of recurrent variants of CNNs for visual tasks, including the proposal of a convolutional LSTM as a feedback architecture. It contrasts previous works that combine CNNs and RNNs in a fixed manner with their approach of learning the recurrence structure itself. Additionally, it explores possible connections between residual networks and recurrent networks in terms of feature refinement. BID17 explores training residual networks with some layers sharing identical parameters, creating a predetermined recurrence structure. This approach does not show the same performance gains as the soft parameter sharing scheme discussed in Section 4. The concept of hypernetworks, where one part of a neural network is parameterized by another, is closely related to this idea. The shared template-based reparameterization is a simple form of hypernetwork implementation that has not been well explored for reducing the size of neural networks. Prior work has focused on parameter reduction through representation bottlenecks, sparsifying connection structure, and pruning trained networks. Some researchers focus on extending neural networks for new tasks, like emulating computer programs. They use different strategies, such as changing activation functions to bias the network towards extrapolating mathematical formulas correctly. In CNNs and variants like ResNets and DenseNets, there is a boost in emulating programs by learning iterative procedures within the network. In CNNs like ResNets and DenseNets, each convolutional layer contains parameter sets with no explicit relation between layers. In contrast, RNNs have a strict structure with a single shared parameter set among all time steps, resembling loops with fixed length and content. Some RNN variants are less strict on loop length or content but are typically fixed beforehand. Our method proposes learning soft sharing schemes through a relaxation of parameter sharing schemes. The LSM matrix S relates to the network structure in CNNs. Trained with a new method, the LSM captures similarities between layers, enabling parameter sharing. This allows for folding the network, revealing a recurrent computation structure. Each layer's parameters are expressed as a linear combination of parameter templates. The LSM matrix S in CNNs captures similarities between layers, enabling parameter sharing and revealing a recurrent computation structure. Each layer's parameters are expressed as a linear combination of parameter templates, allowing for joint optimization with gradient-based methods. This soft sharing scheme decouples the number of parameters in the network from its depth. Our soft sharing scheme reduces the total number of parameters in the network while maintaining accuracy. It allows for alternative interpretations of the method and enables the extraction of hard sharing schemes. This approach helps in detecting implicit self-loops in a CNN trained with our method by learning template layers shared among the network. The convolutional layer U (i) (X) = W (i) * X is a global feature extractor with coefficients \u03b1 (i) determining relevant features. Networks with layers i and i + 2 are functionally equivalent, allowing for folding into a model with two layers and a self-loop. Parameter sharing reduces the total number of parameters in the network while maintaining accuracy. The text discusses identifying layers in a network that perform similar operations by comparing their coefficients. A similarity matrix is created to show the similarity between pairs of layers. The layer similarity matrix (LSM) can be used to extract recurrent loops from trained CNNs. Adding a recurrence regularizer to the training objective can push parameters towards more structured models. The text discusses adding a recurrence regularizer to the training objective to push parameters towards more structured models. It also explores training variants of standard models with soft parameter sharing, showing parameter savings or improved performance. Additionally, it demonstrates converting a trained model into explicitly recurrent form and examines how parameter sharing can improve generalization on synthetic tasks. Details on the initialization for coefficients \u03b1 are provided in Appendix B. The CIFAR-10 and CIFAR-100 datasets consist of colored images labeled among 10 and 100 classes respectively. The CIFAR dataset consists of colored images labeled among 10 and 100 classes, split into training and testing sets. The training set is pre-processed for data augmentation using Wide ResNets as a base model with soft parameter sharing method. The network consists of 3 stages with different numbers of channels and kernel sizes, grouped into 3 layer groups for sharing templates. The SWRN-L-w-k model utilizes parameter sharing in most layers, with k templates per group. SWRN 28-10 outperforms the base model on CIFAR-10 and CIFAR-100, showing improved optimization. SWRN 28-10-1, with a single template per sharing group, performs similarly to WRN 28-10 but with fewer parameters. Results are average of 5 runs. SWRN-L-w model trained for 200 epochs with SGD and Nesterov momentum of 0.9, batch size of 128, and learning rate decay at epochs 60, 120, and 160. Weight decay of 5 \u00d7 10 \u22124 applied on all parameters except \u03b1. SWRN models show superior performance with no parameter reduction, with SWRN 28-10 presenting lower test errors on C-10 and C-100 compared to base WRN 28-10. SWRN 28-10-1 performs similarly to WRN 28-10 with fewer parameters. Parameter reduction is beneficial on CIFAR-10. On CIFAR-10, parameter reduction (k = 2) improves test performance, with SWRN 28-18-2 outperforming ResNeXt-29 16x64 with fewer parameters (55M vs 68M) and no bottleneck layers. Our parameter sharing scheme enhances accuracy-parameter efficiency. Our method surpasses recent NAS algorithms like DARTS, SNAS, and ENAS on CIFAR-10 with a test error of 2.69% after training less than 10 hours on a single NVIDIA GTX 1080 Ti. Our models achieve 2.69% test error on CIFAR-10 after training less than 10 hours on a single NVIDIA GTX 1080 Ti. Comparison to NAS techniques shows similarities in gradient-based methods using extra parameters for soft selection. The learned template coefficients can transform networks into a CNN-RNN hybrid. This method may complement standard NAS approaches. Soft parameter sharing in architecture search is complementary to standard NAS methods, focusing on recurrent patterns rather than operations. This approach, tested on the ILSVRC 2012 dataset, shows distinct forms of implementation compared to recent NAS methods. Soft parameter sharing in architecture search focuses on recurrent patterns and is tested on the ILSVRC 2012 dataset. The method outperforms base models like WRN 50-2 and DenseNets, approaching the performance of ResNet-200 with similar parameter count. Soft parameter sharing in architecture search is demonstrated through a SWRN 28-10-4 model, where stages share parameter templates. Folding stages leads to a CNN with recurrent connections, showing comparable performance to base models on CIFAR-10 dataset. In the case of soft parameter sharing, larger values of k result in increased layer similarities, allowing for folding layers into a single layer with a self-loop. The layer similarity matrix (LSM) can reveal implicit recurrences in the network, even without a recurrence regularizer. Networks trained with soft parameter sharing exhibit rich structures, as seen in the per-stage LSM for CIFAR-trained SWRN 28-10-4. Folding layers can lead to minimal error increase, showcasing the effectiveness of this approach. Training CNNs with soft parameter sharing can lead to better extrapolation on tasks with natural recurrent algorithms. A synthetic algorithmic task of computing shortest paths on 32x32 grids was used for evaluation. Curriculum learning was employed to observe model adaptation to more difficult examples during training phases. The task involves predicting shortest paths on a grid with obstacles and query points. Curriculum learning is used with 5 phases of increasing difficulty. Different models are trained, including a CNN with soft parameter sharing. The study involves training different models, including a CNN with soft parameter sharing and an SCNN with a recurrence regularizer. The models are trained for 50 epochs using Adam with a fixed learning rate. F1 scores are compared due to heavily unbalanced classes. The SCNN outperforms the CNN and adapts better to harder examples in new curriculum phases. In this work, the study focuses on extracting recurrences from feed-forward models to create more modular and compact CNNs. Parameter sharing among layers leads to models with lower error rates on CIFAR and ImageNet datasets. Shared parameters also enable collapsing layers into recurrent blocks, improving extrapolation bias. The approach offers a more flexible behavior and can be beneficial for reducing parameters in training. The parameter structure biases extrapolation, making networks more flexible and adaptable to out-of-domain examples. This architecture discovery method is competitive with NAS algorithms but has a smaller training cost. It can be applied to various CNN model families with negligible computational cost, challenging the rigid definitions of CNNs and RNNs. Future interest lies in adapting the method for models with non-uniform layer parameter sizes. In Section 4.3, an example of implicit recurrences and folding in a SWRN 28-10-4 trained on CIFAR-10 is presented. The network's last 6 layers in the second stage fold into 2 layers with a self-loop. Figure 6 shows non-trivial recurrences emerging naturally, resulting in a structurally rich model. Each stage can be folded to yield blocks with complex recurrences, determined by row/column repetitions in the Layer Similarity Matrix. In a SWRN 40-8-8 network trained on CIFAR-10, layers can be folded together based on similarity in the Layer Similarity Matrix. This folding reduces the total number of layers from 40 to 24 with minimal impact on test error. The network consists of 3 stages, each with 10 layers sharing 8 templates. During experiments on CIFAR-10, hard parameter sharing was observed in a SWRN 40-8-8 network across different runs, leading to implicit recurrences and a \"chessboard\" pattern. Using an orthogonal initialization BID31 for coefficients \u03b1 resulted in better performance compared to other schemes. The L \u00d7 k matrix A was initialized such that A T A = I, ensuring specific values for \u03b1 (i) across layers. The study found that initializing the matrix A with the identity matrix (A T A = I) or enforcing sparsity by randomly setting half of its entries to zero yielded similar results to orthogonal initialization for RNNs. This sparse initialization method was considered the simplest as each coefficient \u03b1 could be initialized independently. Additionally, having A T A = I resulted in the Layer Similarity Matrix also being the identity at initialization. The orthogonal initialization leads to a Layer Similarity Matrix with no structure at the start of training, but rich patterns emerge naturally after optimization."
}