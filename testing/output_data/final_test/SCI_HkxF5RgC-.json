{
    "title": "HkxF5RgC-",
    "content": "Recurrent Neural Networks (RNNs) are efficient for solving sequence-based problems, but their performance depends on network size. By optimizing with model pruning and GPU mapping, we developed an efficient implementation for sparse RNNs. Speedups of over 6x were achieved compared to the next best algorithm, enabling larger networks to advance the state-of-the-art. Case studies on NMT and speech recognition tasks showed recurrent layer acceleration by up to 3x. Sparse RNNs can be efficiently run on GPUs by combining model pruning and network compression methods. This approach allows for larger networks to be implemented, leading to significant performance improvements. Sparse RNNs can be efficiently run on GPUs with larger networks, leading to performance improvements. Various optimizations are necessary for high performance, with case studies showing speedups of up to 3\u00d7 on non-synthetic workloads. The appendix includes a case study on a machine translation task. In this work, the focus is on simplifying complex neural network models for efficient deployment on smaller hardware. The approach includes network pruning and other simplification techniques like quantization, weight sharing, and tensor approximations. The goal is to make the models run more efficiently, with a particular emphasis on RNNs, LSTMs, and GRUs. Network pruning induces sparsity in network model weights, serving as a regularizer, compression method, or reducing computational workload. Applicable to various tasks like speech recognition, machine translation, and image captioning, pruning techniques include fine-grained, unstructured pruning, structured pruning at a small scale, and pruning entire filters for a dense workload with smaller dimensions. The focus is on simplifying neural network models for efficient deployment on smaller hardware, utilizing techniques like quantization, weight sharing, and tensor approximations. In BID8, authors utilize GPUs' on-chip storage for recurrent weights. A recurrent network's operation is simplified in Equation 1, with input-to-hidden weight matrix (W x t) processed in parallel. In a persistent RNN implementation, weights U r are stored in on-chip register files, while activations (h t\u22121) are in shared memory. Each row is processed by one warp. The number of thread blocks is set to the number of Streaming Multiprocessors (SMs) in the system for matrix multiplication. Work is mapped onto the GPU in a persistent approach, with one row processed by a single warp. The software pipeline of a persistent RNN includes load, operate, reduce, and synchronize stages for processing input activations and outputting final results. Performance is mainly influenced by math throughput in the operate stage. The performance of persistent RNNs is influenced by math throughput in the operate stage. To optimize shared memory load, threads can reuse activations multiple times. Sparse persistent RNNs use <index, value> pairs for data format. In sparse persistent RNNs, data is represented as <index, value> pairs where each pair denotes a nonzero weight's location and value. Threads in sparse RNNs store a fraction of nonzero weights, with all nonzero elements in a thread belonging to the same row. The number of thread blocks varies based on hidden layer size and sparsity level. Weights are shared across timesteps, stored in on-chip memory to avoid reloading. The pipeline includes 4 stages: Load, Operate, Reduce, and Store. In sparse persistent RNNs, computations are done in series. Threads work together to generate results for rows within each block. The layout of the sparse matrix differs from the original dense matrix, leading to different activations for nonzeros in the same storage column. This can cause bank conflicts in shared memory due to unique indices in a warp pointing to different locations. In sparse persistent RNNs, shared memory bank conflicts limit the performance of the operate stage. Solutions are provided for addressing shortcomings in the implementation, such as padding rows with fewer nonzero weights to ensure load balance. This optimization reduces wasted registers on unnecessary zeros by roughly 20%. The shared memory in sparse persistent RNNs is divided into 32 banks to achieve high bandwidth, but conflicts arise due to multiple column indices landing in the same bank. Two methods, wide memory loads and bank-aware weight layout, are introduced to reduce shared memory bank conflicts by over 80%. In a dense persistent RNN, weight reuse across samples can occur with a minibatch size larger than one. Wide memory loads can reduce shared memory bank conflicts by over 80%. By batching 4 activations from different samples together, the total bank conflicts can be reduced to at most 1/4. However, this method consumes 4 times the shared memory, limiting the maximum hidden layer size. The shared memory size in RNNs can be reduced by 4\u00d7 to decrease hidden layer size. Using ld.shared.v2 can reduce bank conflicts by 2\u00d7 with 2\u00d7 storage overhead. Bank-Aware Weight Layout optimizes shared memory access sequence to reduce conflicts. Reordering nonzeros' locations can improve efficiency but only needed when sparsity pattern changes. The shared memory size in RNNs can be reduced by 4\u00d7 to decrease hidden layer size. Using ld.shared.v2 can reduce bank conflicts by 2\u00d7 with 2\u00d7 storage overhead. Bank-Aware Weight Layout optimizes shared memory access sequence to reduce conflicts. A greedy algorithm in Appendix A generates a weight layout to reduce shared memory bank conflicts. Lamport Timestamps are used for correct ordering between work from different thread blocks in sparse network training. Our optimized implementation of Lamport timestamps in sparse network training reduces memory usage by initializing output buffers for each time step to -0.0f, eliminating the need for extra flags. This approach ensures valid data without the additional memory overhead, doubling the memory usage instead of quadrupling it. Our optimized implementation of Lamport timestamps in sparse network training reduces memory usage by initializing output buffers for each time step to -0.0f, eliminating the need for extra flags. This approach ensures valid data without the additional memory overhead, doubling the memory usage instead of quadrupling it. The final implementation only doubles the memory requirements, with advantages and disadvantages for each method discussed. Global synchronization requires memory round trips but only needs to be called once per timestep. Lamport timestamps require no extra memory movement but multiple checks for current values. Overlapping load and operate stages by preloading activations requires double-buffering, doubling shared memory usage and halving maximum effective layer size. Larger layer sizes can be accommodated with global barriers due to reduced shared memory usage. Lamport timestamps were faster in experiments, except for very large layer sizes. Setup and experiments were performed to demonstrate the benefits of the sparse persistent RNN technique. Experiments show benefits of sparse persistent RNN technique with sparsity levels ranging from 80-99%. Pruning recurrent networks can lead to significant sparsity without loss of accuracy. Common RNN hidden layer sizes are 1024 to 3072, with potential for larger sizes. Network size for RNN hidden layers ranges from 1024 to 3072, with potential for larger sizes. Batch sizes vary from 1 to several hundred inputs, depending on deployment scenario. Timesteps used by the network can range from 10 to hundreds, depending on the task. Our sparse persistent approach, compiled in CUDA 9.0, is compared against dense GEMM, sparse GEMM, and dense persistent approaches on a NVIDIA Tesla V100. Results show significant speedups with our approach, achieving a 15.0\u00d7 speedup over dense GEMM at 1% density. Our approach achieves a 15.0\u00d7 speedup over dense GEMM at 1% density. Sparse persistent RNNs fail after a layer size of 5632 at 10% density due to insufficient registers. Increased pressure on shared memory and global barriers for synchronization limit performance as the layer size grows. Our technique scales well to larger batch sizes compared to dense persistent kernels. Our sparse persistent RNN technique demonstrates significant speedups over dense GEMM by varying layer sizes and densities. Even at higher densities, performance can be improved by utilizing a persistent approach. The approach allows for extremely large networks with high sparsity, achieving up to a 10.6\u00d7 speedup over dense GEMM. However, at 5% density, the overhead of the loading phase may outweigh the benefits of a persistent approach. In Figure 4, sparse persistent RNNs improve performance in pruned networks by increasing layer size and sparsity. Optimizations are crucial for peak performance, with 10% density being sufficient for most layer sizes. The approach outperforms others, even up to 30% density for smaller layers. Different batch sizes can also impact performance. Our approach is the winner even up to a density of 30%. Dense GEMMs and our approach scale the best, tempering the benefit offered by other algorithms. The number of timesteps does not significantly impact performance. An efficient algorithm for recurrent layers is described, requiring pruning during the training process for neural machine translation and speech recognition networks. Results show a speedup of 2.0\u00d7 to 3.1\u00d7 in machine translation networks and 1.2\u00d7 to 2.9\u00d7 in speech recognition networks with load-balancing in mind. The current approach focuses on optimizing load-balancing and achieving a speedup of 1.2\u00d7 to 2.9 on Deep Speech 2's recurrent layers through pruning. By compressing column indices and using lower-precision data types, more nonzeros can be accommodated in registers. The maximum layer size is 11520 on a V100 GPU, limited by shared memory usage. Swapping Lamport timestamps for a global barrier can double the maximum layer size. The bottleneck for 1% density and a layer size of 11520 is loading activations. The bottleneck for 1% density and a layer size of 11520 is loading activations into shared memory, rather than the operate stage. Using a lower-precision data type for the activations would remove the shared memory bandwidth and storage burden, allowing for more efficient large layers. The work can be extended to multiple GPUs for larger layer sizes and more nonzero parameters. Load balancing is handled with zero-padding, and different approaches exist for handling non-uniform sparsity. Defining classes for different sparsity levels and assigning rows accordingly can be done without altering the network structure. Based on sparsity, load-balance aware pruning can improve network accuracy without impacting performance. Sparse GEMM's performance remains consistent with load-balancing, but the benefit over dense GEMM increases. Sparse persistent RNNs are introduced as an efficient algorithm for accelerating pruned recurrent networks. The optimized technique accelerates pruned recurrent networks on a recent GPU, achieving significant speedups compared to dense and sparse implementations. Larger networks can be deployed with performance increases of around 5\u00d7, even for denser workloads. The approach also speeds up NMT and speech recognition networks' recurrent layers by up to 3\u00d7, with load-balanced pruning showing significant improvements in network performance. Our technique accelerates pruned recurrent networks on GPUs, achieving significant speedups compared to dense and sparse implementations. Load-balanced pruning can improve network throughput, necessary for high performance and accuracy in some recurrent layers. The algorithm optimizes rows of nonzero weights to minimize bank conflicts. Case studies in the appendix demonstrate the utility and generality of the approach. In this section, performance results for LSTM BID20 layers are shown. Each thread in an LSTM is responsible for four gates, compared to one gate in an RNN. This results in an increased number of weights for the same layer size. Pruning a neural machine translation network is explored to understand the impact on accuracy with sparsity. The focus is on execution speed and accuracy, with sparsity being a trade-off between the two. Simple pruning and training procedures are outlined. Our technique allows for higher performance in neural machine translation networks by using magnitude-based pruning. We use OpenNMT BID22 for translation from English to German, with a 2-layer LSTM architecture. Training is done with two GPUs and performance results are gathered on a NVIDIA Tesla V100 accelerator. When deciding which weights to prune, only the weight magnitude is considered. Two pruning methods, na\u00efve and row-balanced, are compared in terms of accuracy impact. Na\u00efve pruning treats all gates and rows equally, leading to sparser forget gates in LSTMs. Row-balanced pruning ensures each row in a layer has the same number of nonzero values, promoting load balance and potentially higher performance. When comparing two pruning methods, na\u00efve and row-balanced, for weight pruning, it was found that the difference in network accuracy was negligible, less than 0.1 BLUE points in each case. Therefore, the focus is on row-balanced pruning. Sparse techniques are not limited to inference, as recent work has shown that training with pruned weights is viable. In the experiments, a simple methodology of pruning to the target density after one-half epoch of training is adopted. A \"master copy\" of un-pruned parameters is updated during the backwards pass, while the pruned weights are used for computation. During training, weights are pruned every half-epoch for a total of 21 pruning steps, changing the sparsity pattern. This approach aims to improve accuracy for a given number of nonzero parameters. The sparsity pattern evolves until epochs 12 and 13 fine-tune the final pattern. The effectiveness of larger, sparser networks compared to smaller dense networks with the same number of parameters is demonstrated. A larger, sparser network outperformed a slightly smaller, less sparse network. Despite a simple training procedure, network accuracy was close to the dense baseline, with only a 0.4 BLEU difference at the largest configuration. The study demonstrates the effectiveness of larger, sparser networks compared to smaller dense networks with the same parameters. Despite a simple training method, network accuracy was close to the dense baseline, with only a 0.4 BLEU difference at the largest configuration. Ways to potentially improve accuracy include more elaborate pruning schedules, sensitivity analysis, adjusting hyperparameters, and more fine-tuning. The study also highlights the importance of considering network throughput in addition to accuracy when evaluating network performance. Our study focuses on the performance of LSTM recurrent kernels on an NVIDIA V100 GPU, utilizing state-of-the-art algorithms for different layer sizes. The use of shared memory per thread block allows for more efficient support of larger layer sizes. The performance of our approach on sparse networks is highlighted, showing potential speedups in training time for production networks composed of multiple layers. The findings suggest that a pruned network can run significantly faster than a dense network of the same size, leading to proportional speedups in training time. Our study shows that a pruned network can outperform a dense network of the same size, leading to faster training times. Our technique allows sparse networks to surpass dense ones, even at smaller layer sizes. Additionally, our algorithm can be applied to pruned networks, resulting in the best BLEU score for a given performance target and the highest throughput for a given network accuracy. The study demonstrates that pruned networks can outperform dense networks of the same size, leading to faster training times. Sparse networks can surpass dense ones, even with smaller layer sizes. The algorithm can be applied to pruned networks, achieving the best BLEU score for a performance target and highest throughput for network accuracy. The performance of various algorithms on different versions of the network is evaluated, with random patterns generated for each workload. The study shows that sparse networks can outperform dense networks in terms of accuracy, especially with larger and sparse layers. Sparse persistent kernels are better than dense ones for a given network, providing higher accuracy with a pruned network. Speed is better with dense persistent kernels, but accuracy is higher with sparse persistent kernels. Pruned networks using sparse persistent kernels offer higher accuracy with a tradeoff in speed. Without this technique, there would be no reason to prune a network for batched inference performance. Sparse networks can outperform dense networks in accuracy, especially with larger and sparse layers. Pruned networks using sparse persistent kernels offer higher accuracy with a tradeoff in speed. Our technique extends to LSTMs with little effort, allowing for pruned recurrent layers to run more efficiently. Comparisons show that a larger, sparse network can be more accurate than a smaller, dense one, making pruning a network and using our technique the best choice for achieving performance targets."
}