{
    "title": "B1xhQhRcK7",
    "content": "This paper discusses evaluating learning systems in safety critical domains like autonomous driving, focusing on finding failure scenarios and assessing failure probabilities. The standard method, Vanilla Monte Carlo, can miss failures, leading to unsafe agents. To address this issue, an adversarial evaluation approach is proposed, focusing on adversarial situations to provide unbiased failure probability estimates. Identifying these situations is challenging due to the rarity of failures. The paper proposes a continuation approach to identify failure modes in less robust agents, allowing for faster evaluation of catastrophic failures in machine learning systems. This approach demonstrates efficacy in humanoid control and simulated driving domains, estimating failure rates in minutes to hours rather than days. The paper discusses the importance of reliably evaluating the risk of failure in deploying machine learning systems, especially in safety critical domains like autonomous driving. It highlights the limitations of random testing and the need for statistical evaluation to ensure confidence in the failure rate of a policy. The example of a self-driving car company's cost-benefit analysis is used to illustrate the standard approach of estimating expected return through i.i.d. samples from the data distribution. In the context of evaluating machine learning systems in safety critical domains like autonomous driving, the paper discusses the limitations of random testing and the need for statistical evaluation to ensure confidence in the failure rate of a policy. It is highlighted that for tightly bounded returns, the sample estimate quickly converges to the true expectation, but with catastrophic failures, efficiency may be an issue. The example of a self-driving car with a failure probability greater than 10^-8 per mile having negative expected return is used to illustrate the challenges of testing for confidence in low failure rates. To address these issues, a novel adversarial evaluation approach is proposed to overcome the cost and supervision challenges in real-world testing scenarios. The paper discusses the challenges of evaluating machine learning systems in safety critical domains like autonomous driving. It introduces a continuation approach to learning a failure probability predictor (AVF) to identify difficult situations and optimize testing. This approach leverages data from less robust agents to provide a stronger learning signal and save time and resources during evaluation. The paper introduces a continuation approach to learning a failure probability predictor (AVF) for evaluating machine learning systems in safety critical domains like autonomous driving. The AVF guides the adversarially acting evaluator in two settings: failure search and risk estimation. In failure search, efficient adversaries can identify and debug potentially unsafe policies, while in risk estimation, the failure probability of an agent can be estimated efficiently. The paper presents three key contributions: 1. Demonstrating the limitations of random testing in efficiently obtaining adversarial inputs. 2. Introducing a continuation approach for learning failure probability predictors in rare failure scenarios. 3. Extensively evaluating the method on simulated driving and humanoid locomotion domains through adversarial testing, showing significant efficiency improvements. The paper introduces a notation for reliability assessment of a trained agent, focusing on catastrophic failures. It discusses the randomness sources from the agent and environment, emphasizing the importance of evaluating reliability efficiently. The paper discusses the evaluation of an agent's performance on the environment distribution over initial conditions X \u223c P X, emphasizing the cost of evaluating neural networks and the dominance of simulating environments. Failure search aims to find catastrophic failures of the agent through adversaries specifying initial conditions x and observing the outcome C. The adversary can choose X t \u223c P t, observe C t = c(X t, Z t), and return if C t = 1. Historical data can be collected while training the agent to design a more efficient adversary. The failure probability of the trained agent is evaluated. The failure probability of the trained agent is evaluated by estimating p with high accuracy. An algorithm is considered correct if it produces an estimate within a certain interval with high probability. The approach involves estimating the failure probability predictor that returns the probability of failure given an initial state X. The failure probability predictor (AVF) f *: X \u2192 [0, 1] estimates the probability of failure given an initial condition. A continuation approach is used to learn an approximation f \u2248 f * with low evaluation cost. When f * is known, the optimal adversary has a simple form. The adversary minimizes the expected rounds until failure by evaluating the agent on instances that maximize f * : X \u2192 [0, 1]. Sampling diverse initial conditions increases robustness. Algorithm 2 (AVF Adversary) in Appendix C outlines the procedure. The failure probability estimation method uses importance sampling with a proposal distribution Q. The estimator minimizes variance by choosing the proposal distribution Q*. The proposal distribution Q* minimizes variance. Sampling from Qf is done using rejection sampling method to produce a sample from Qf. To increase sampling robustness, a hyperparameter \u03b1 > 0 is introduced to redefine q f as proportional to f \u03b1. The pseudocode for the procedure is provided in Algorithm 1. The classical approach of fitting a softmax classifier to data does not work well for rare failures with binary signals. The failure signal is binary, with the agent failing once every 110k episodes after training for 300k episodes. To learn a classifier f, more than 300k episodes are needed to see enough failure cases. A continuation approach is proposed to learn AVFs from agents that fail more often, providing a boost to the learning process. The procedure involves sampling proposal instances and evaluating the agent. The problem of agent evaluation has additional structure that can be leveraged. The failure signal is binary, with the agent failing once every 110k episodes after training for 300k episodes. To learn a classifier f, more than 300k episodes are needed to see enough failure cases. A continuation approach is proposed to learn AVFs from agents that fail more often, providing a boost to the learning process. The procedure involves sampling proposal instances and evaluating the agent. Agent evaluation has additional structure that can be leveraged by learning from earlier agents in training. This approach eliminates the need for gathering additional data at evaluation time. The continuation approach in learning AVFs relies on the assumption that agents fail in related ways to the final agent. By using this approach, the state-dependent component is learned faster, even if learning the robustness factor is challenging. Despite potential inaccuracies in learning, the AVF remains useful for failure search and risk estimation. Despite potential inaccuracies in learning, the AVF is still useful for failure search and risk estimation. The shape of the learned AVF matters more than its magnitude, capturing the intuition that it may learn the underlying structure of difficult states. Flexible parameterizations, like neural networks, can allow for interactions between parameters and states if there is enough data. In the Driving domain, the agent controls a car in the TORCS simulator and is rewarded for forward progress without crashing. Initial conditions are defined by the shape of the track, which is sampled from a procedurally defined distribution. Failure search is evaluated using different adversaries to find adversarial problem instances. The AVF adversary finds adversarial inputs with significantly fewer episodes than random testing on Driving and Humanoid tasks. An on-policy actor-critic agent is used in the experiments, controlling a humanoid body in the MuJoCo simulator and rewarded for standing without falling. A catastrophic failure is defined as the humanoid falling below a fixed threshold. An off-policy D4PG agent is used for the humanoid tasks. The adversaries evaluated in this study include the naive (VMC), AVF, and prioritized replay (PR) adversaries. Adversarial testing is shown to be more effective in detecting catastrophic failures compared to naive evaluation methods. The PR adversary conducts experiments starting from initial conditions that led to failures during training, offering a simple alternative to the naive adversary. The AVF adversary is significantly more efficient than the random adversary, requiring much fewer episodes to detect failures. For example, on Humanoid, the AVF adversary only needs 1.1 minutes compared to over 2.7 hours for the random adversary. This highlights the importance of using more advanced adversaries for effective evaluation in training models. The AVF adversary is much faster than the random adversary, requiring significantly less time to detect failures. For example, on the Humanoid domain, the random adversary takes 3 days compared to just 6 minutes for the AVF adversary, even after including training time. The prioritized replay adversary also shows better efficiency than random testing, with minimal overhead, but may not always detect all failures. The AVF adversary is faster at detecting failures compared to the random adversary, even on the Humanoid domain where it only takes 6 minutes. The Figure 1 shows the reliability of the AVF and VMC estimators, with the AVF estimator approaching ground truth faster than the VMC estimator. Error bars are shown for 2 standard errors, which are difficult to see in the Driving domain due to running many trials. The adversarial training for the agent involves very few training instances with support under P X, leading to the PR adversary falling back to the VMC adversary. Classical techniques like cross-entropy method or subset simulation are not compared due to optimizing a binary failure signal instead of a continuous score. The continuation approach is crucial as fitting a useful proposal distribution requires learning from weaker agents. Fitting a useful proposal distribution requires learning from weaker agents to compare approaches for risk estimation. The AVF estimator is compared to the vanilla Monte Carlo estimator (VMC) by running each estimator multiple times and reporting the fraction of cases when the estimates fail to fall within a certain interval. Results are summarized in Fig. 1, showing that the choice of \u03c1 does not significantly affect the results. The AVF estimator outperforms the VMC estimator in terms of efficiency, requiring significantly fewer experiments to achieve a 3-approximation on both the Driving and Humanoid domains. This demonstrates good coverage as well, with the AVF sampling from most possible failures. Efficient risk estimation is crucial, and the AVF excels in this aspect. The AVF estimator outperforms the VMC estimator in efficiency by quickly eliminating worst policies and identifying the most reliable agent from a fixed set of Humanoid agents. The AVF estimator is more efficient than the VMC estimator in selecting the most reliable agent from a fixed set of Humanoid agents. VMC struggles to rank policies with no failures, while AVF quickly identifies weak policies and selects a more robust one. Random testing can be inefficient in selecting the best policies, as shown in a scenario with 500,000 episodes. Adversarial testing improves efficiency and is recommended for practical use in various settings. Concerns about underestimating failure probabilities and high variance in risk estimations are addressed. Using an optimized adversary is a significant improvement in revealing failures within a practical budget. The use of an optimized adversary is a significant improvement in revealing failures within a practical budget. The AVF over-estimates failure probabilities due to training on weaker agents. A simplified Differentiable Neural Dictionary (DND) is used in rare failure scenarios. Efficiency lower bounds are ensured by running VMC and AVF estimators in parallel, with a 2x slow-down in the worst case. BID14 provides better guarantees. Our work on reliability is motivated by research on adversarial examples, showing that machine learning systems can perform poorly on specific inputs. Unlike previous work, we do not allow adversaries to generate inputs outside the trained distribution. Previous research focused on L p norm balls in the image domain, but questions have been raised about their practical value. Simulated reinforcement learning environments provide a valuable testbed for researchers interested in adversarial examples, as the ground truth is provided by the simulator. The method presented adapts the proposal distribution using data from related agents and separates controllable randomness for better practicality in RL tasks. In robotics, assessing the safety of motion control algorithms is crucial for real-world deployment. A recent paper proposed using an adaptive mixture importance sampling algorithm to quantify collision probability for an LQR controller with EKF state estimation. Unlike other methods, this approach requires an analytic model of the environment. Additionally, recent work has focused on safe reinforcement learning for developing highly reliable agents. Recent work on safe reinforcement learning emphasizes the importance of adversarial testing to detect rare, catastrophic failures in RL agents. This approach complements traditional evaluation methods and can play a crucial role in improving agent robustness. The results of this study lay the groundwork for future research in developing deployable and reliable agents. The work focuses on evaluating and developing robust agents by addressing the high variance of empirical estimators in problems with heavy-tailed losses. Statistical learning theory aims to minimize expected risk by finding a model that performs well on data points. The expected risk is estimated using empirical risk on a test set of data points sampled from D. This approach is justified by the empirical risk being close to the true expected risk with high probability. The empirical risk is close to the true expected risk with high probability. Hoeffding's inequality provides bounds on the empirical test risk compared to the expected risk. These bounds guarantee efficiency in many settings but become weak for heavy-tailed losses. Achieving a fixed error bound may require a large number of data points, as seen in the example of a self-driving car taking trips with negative rewards for crashes. Achieving a fixed error bound for heavy-tailed losses may require a large number of data points. Random sampling may not detect costly failure modes unless a large number of samples are used. The empirical estimate error depends on the estimator and concentration inequalities used. Uninformed random sampling approaches will face similar issues. The approach relying solely on uninformed random sampling will face similar issues. Proposition 3.2 proves that the variance of P is minimized when the variance of Ut is minimized. The variance-minimizing proposal distribution is characterized for the general stochastic case in Proposition 3.3, which addresses rejection sampling. The rejection sampling procedure, as proven in Proposition 3.3, generates samples from Qf by accepting instances X \u223c PX with probability f1/2(X). The AVF-guided Search algorithm is used in the Driving domain with the TORCS 3D car racing game. In the Driving domain, the agent receives a 15-dimensional observation vector summarizing its position and velocity on the track. Rewards are based on velocity along the track's center, with collisions resulting in a negative reward. Problem instances involve track shapes defined by waypoints. In the Humanoid domain, the agent receives a 67-dimensional observation vector summarizing joint angles and velocities. The agent receives rewards based on head height, with termination if below 0.7m. Problem instances are defined by initial standing poses sampled from random trajectories. Modifications were made to decrease agent failure probabilities. The modifications were made to decrease agent failure probabilities during training. The approach used an asynchronous batched implementation of Advantage Actor-Critic on Driving, with Population-Based Training and evolving learning rate and entropy cost weights. At test time, the most likely predicted action was taken to reduce failure probability. Additionally, a hand-crafted form of adversarial training was employed. The modifications aimed to decrease agent failure probabilities during training by using a more difficult distribution of track shapes with sharper turns. The D4PG agent on Humanoid was trained for 4e6 learner steps, using different exploration rates on actors. Demonstrations from a previous agent were also utilized for defining the initial pose distribution. The training process for the AVF models involved using 1000 demonstration trajectories, updating both critic and policy networks with demonstration data, and ignoring the initial training phase where the agent fails frequently. The training datasets included the last 150,000 episodes for Driving and 200,000 for Humanoid. Information about training iteration and noise applied to the policy was included before applying the MLP. The AVF models were trained to minimize cross-entropy loss using the Adam optimizer for 20,000 and 40,000 iterations on Driving and Humanoid respectively. The AVF architecture uses a 4-layer MLP with 32 hidden units per layer on Driving and a simplified Differentiable Neural Dictionary architecture on Humanoid. The model retrieves the K = 32 nearest neighbors and their labels to make predictions, using a weighted average approach. The weights of each point are embedded into 16 dimensions using a 1-layer MLP. Different hyperparameters were tried for the number of MLP layers on Driving and the number of neighbors on Humanoid. The VMC adversary was run for 5e6 experiments for each agent, while the AVF adversary was run for 2e4 experiments. The PR adversary selected problem instances causing failures on the actor with the least. The adversary selected problem instances causing failures on actors with the least noise, starting with the actor with the least noise and progressing to the next. Different values of the sample size parameter were tested, showing that greater optimization pressure improved results. In the failure search experiments on the Humanoid domain, the best version of each agent is evaluated using the AVF model selection procedure. The sample size parameter n in Algorithm 2 determines the selection pressure for running experiments on problem instances. Larger values of n provide more selection pressure, while smaller values draw samples from a larger fraction of X, offering robustness to inaccuracies in the learned f. For the experiments, n = 1000 for Driving and n = 10000 for Humanoid were used. No hyperparameter selection was performed. In the experiments, n = 1000 for Driving and n = 10000 for Humanoid were used without hyperparameter selection. Additional experiments showed that greater optimization pressure improved results for the AVF adversary. The AVF estimator can estimate failure probability faster than the VMC estimator in both domains. The AVF estimator outperforms the VMC estimator in estimating failure probabilities across different choices of approximation levels. Ground truth failure probabilities are shown in Figure 4, representing different training stages of the agent. The reliability of AVF and VMC estimators for different r values is shown in Figure 3. The x-axis displays the number of evaluation episodes, while the y-axis shows the probability that P does not fall within the interval (p/r, pr). Error bars of 2 standard errors are included in the graph. The reliability of AVF and VMC estimators is evaluated multiple times with error bars of 2 standard errors. The curves are not smooth, especially for the VMC estimator, due to the property of \u03c1-estimators with small sample sizes. Robustness for agents from different training points is shown in Figure 4, with failure probability estimated using 160,000 experiments with VMC. The robustness does not improve monotonically, and selecting the final agent may not provide a robust agent. The AVF estimator showed significant improvements over the VMC estimator for model selection, despite non-monotonicity of robustness over training. The failure probability for agents not failing in ground truth measurements was adjusted to 1/160,000. Running ground truth measurements for longer would require a significant number of experiments and CPU-days. It was noted that robustness is not monotonically increasing, making uniform exploration with the AVF estimator a better model selection strategy than simply choosing the last agent. This contrasts with the standard setting in deep reinforcement learning where expected return usually increases over training. The optimization process using VMC does not effectively decrease failure probability, indicating a need for a departure from current practices to train highly reliable agents. Techniques like vanilla Monte Carlo may fail to provide necessary guarantees for rare failure events. This issue has received little attention in the RL community. Recent work has adapted deep reinforcement learning techniques to handle constraints, but they are not designed to specifically satisfy constraints when violations would be caused by rare events. This suggests a natural extension to the rare failures case using the techniques developed here. Safe reinforcement learning aims to avoid catastrophic failures either during learning or deployment, typically relying on strong knowledge of transition dynamics or assumptions about the accuracy of learned models. Training controllers to be robust to system misspecification by exposing them to pre-specified or adversarially constructed noise has garnered significant interest recently. This approach is seen as complementary to adversarial testing methods, which do not offer guarantees."
}