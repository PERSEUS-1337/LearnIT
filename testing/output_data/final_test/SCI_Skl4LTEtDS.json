{
    "title": "Skl4LTEtDS",
    "content": "In this work, a curriculum of progressively growing action spaces is used to accelerate learning in complex tasks with large combinatorial action spaces. The approach involves off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously and efficiently transfer data and value estimates. The efficacy of this approach is demonstrated in control tasks and challenging StarCraft micromanagement tasks with large, multi-agent action spaces. The value of curricula in machine learning and biological systems is well established for complex behaviors. In this work, curricula are used to accelerate learning in complex tasks with large action spaces. The approach involves off-policy reinforcement learning to estimate optimal value functions for multiple action spaces simultaneously. The curricula are internal to the agent, simplifying the exploration problem without changing the environment. The approach involves growing the action space of reinforcement learning agents over training to guide exploration towards rewards and meaningful experiences. Each action space is a superset of the previous one, leading to superior policies. Domain knowledge is needed to identify a suitable hierarchy of action spaces, but it is often easy to find. Continuous action spaces can be discretised for increasing resolution, and curricula can be used to handle large combinatorial action spaces induced by multiple agents. The method proposed involves using off-policy reinforcement learning to improve sample efficiency in curriculum learning by identifying a restricted action space for more meaningful experiences. Data from exploration in the restricted action space can be used to learn value functions in less restricted action spaces. In the proposed approach, value functions are learned for different levels of restriction simultaneously, with relationships between them used to accelerate learning. The method is demonstrated in simple control tasks with increasing action resolution and then applied to more complex problems like StarCraft micromanagement with hierarchical clustering for restricted action spaces. Our method improves sample efficiency by allowing agents in a cluster to gradually increase the number of groups that can act independently. It outperforms learning specific action spaces from scratch and an actor-critic baseline. Curriculum learning has a long history in reinforcement learning and neural network training. In reinforcement learning and neural network training, Bengio et al. (2009) and other researchers have explored the use of curricula to automate the choice of task mixtures throughout training. Different approaches include handcrafted schedules and sensitivity analysis to define curricula over action dimensions. In some settings, environments can be controlled to induce a curriculum with progressively more challenging initial states or automatically tuned difficulty levels in procedurally generated tasks. In reinforcement learning, curricula are used to automate task mixtures during training. Different approaches include handcrafted schedules and altering environments to induce progressively more challenging states. Skills can be learned in easier settings and transferred to more complex tasks. Reward shaping can also be used to facilitate learning policies in simpler tasks that can then be transferred to more difficult tasks. In reinforcement learning, reward shaping can aid in learning policies on simple tasks and transferring them to harder tasks. Various methods involve adapting state space resolution, growing neural network complexity, and widening action spaces during planning. In reinforcement learning, methods involve adapting state space resolution, growing neural network complexity, and widening action spaces during planning. Recent work by Czarnecki et al. (2018) trains mixtures of policies with an actor-critic approach, learning a single value function for the current mixture. In contrast, we learn a different value function for each policy and use optimal value functions to induce additional structure on our models. Our approach constructs a scheme for off-action-space learning, outperforming an actor-critic algorithm without the need for population-based training. Other works address generalization and representation for value functions with large discrete action spaces, which could be combined with our method to shape exploration. Our problem is formalized as a Markov Decision Process (MDP) with states, actions, transition function, and discount factor. We aim to maximize the return by learning a policy that maps states to actions. Q-learning is used to iteratively improve the Q-function, which satisfies the Bellman optimality equation. This off-policy method allows for learning in off-action-space scenarios. In Q-learning, a hierarchy of actions is defined by identifying parent actions in a restricted action space. Each action space is a subset of the next, with the optimal policy denoted as \u03c0 * (a|s) and corresponding value functions Q * (s, a) and V * (s). Domain knowledge can help define these hierarchies, such as identifying nearest neighbors in the action space. The curr_chunk discusses building a hierarchy for multi-agent action spaces using action-embeddings and off-policy value-based reinforcement learning. It mentions using Q-learning and its adaptations to update value functions for different action spaces. The approach involves learning an estimated optimal value function for each action space simultaneously. The curr_chunk discusses off-action-space learning in reinforcement learning, where samples are drawn from a behavior policy based on a value function to train higher value functions efficiently. Sampling at the beginning of each episode restricts exploration to the low-regime, avoiding forcing low-value functions to generalize to inaccessible states. This approach aims to update all value functions simultaneously for different action spaces. The curr_chunk discusses the importance of suitable function approximators for value functions in less restricted action spaces. It suggests an iterative decomposition of value estimates and leveraging a hierarchical structure in action spaces for efficient learning. Both Q*(s,a) and \u2206(s,a) are learned components that can be regularized or restricted. In experiments, \u2206(s,a) was simply initialised to be small and not further regularised. The modified Bellman optimality equation implies a Q-learning update, allowing for iterative improvement of Q*. Policies with low values are easier to learn, leading to higher Q* accuracy earlier in training. In experiments, policies with low values are easier to learn, leading to higher Q* accuracy earlier in training. By sharing parameters between function approximators, a joint state representation can be learned and iteratively decoded into estimates of Q*. This approach has been successful in improving the efficiency of multi-task solutions using deep learning. In cooperative multi-agent control, a linear schedule is used to increase the mixing parameter \u03b1 over training. Initially set at \u03b1 = 0, the parameter is chosen with probabilities based on \u03b1 values. This approach has shown empirical success with minimal tuning effort. Other strategies for curriculum tuning exist but may add complexity and overhead. Hierarchical clustering is proposed to reduce the exponentially large action space in cooperative multi-agent control. Agents are grouped together based on proximity, allowing for more efficient action selection. Hierarchical clustering is used in cooperative multi-agent control to group agents based on proximity, allowing for more efficient action selection. The unsupervised clustering algorithm splits groups into smaller groups at each level, with k-means clustering based on spatial position. Value function estimation involves computing state-value scores and group-action deltas for each group at each level. The estimated value of the parent action is based on the entire parent group taking the same action as the child group. This approach addresses the multi-agent value-learning problem at each level. Hierarchical clustering in cooperative multi-agent control groups agents based on proximity for efficient action selection. Value function estimation involves computing state-value scores and group-action deltas at each level. Joint-action value is estimated as the mean of group-action values. Clustering changes for every state may interfere with generalization, addressed by including clustering as part of the state and using a functional representation for consistent semantics. The functional representation used produces group-action values agnostic to group identifiers. Spatially resolved embeddings are computed and pooled over group locations. Two classes of problems are investigated: simple control problems with coarse/fine action discretisation and cooperative multi-agent scenarios in StarCraft. Two simple examples are examined: Acrobot and Mountain Car environments with discretised action spaces and sparse rewards. In a study on continuous control, experiments were conducted with growing action spaces. Different levels of action resolution were tested, showing that training from scratch with lower resolutions led to finding the goal but incurred high actuation costs. The results were visualized in Figure 1. Training with a growing action space explores to find the goal early, transitioning smoothly into a solution that minimizes actuation costs while achieving the objective in real-time strategy games like StarCraft and StarCraft II. These games are popular for benchmarking reinforcement learning algorithms, with some focusing on full gameplay and others on micromanagement tasks. In experiments focusing on large-scale micromanagement scenarios in StarCraft, the difficulty is increased by randomizing starting locations and having opponent units controlled by scripted logic. This poses a challenge for agents to explore and find the enemy while facing a strong defensive position. In large-scale micromanagement scenarios in StarCraft, agents face the challenge of finding the enemy in a strong defensive position. The action space allows for attack-move or move actions in eight directions, with a stop action available. Experiments use k-means clustering with k=2, splitting into four or eight groups. The approach involves off-policy learning with n-step Q-learning for faster value propagation. The architecture efficiently represents value functions in the action-space hierarchy. The architecture efficiently represents value functions in the action-space hierarchy by extracting features from units and maps, embedding them with a small MLP, and passing them through a residual CNN to produce a final embedding. This embedding is used to compute a scalar value and produce embeddings for each group, which are then decoded for further analysis. The architecture efficiently represents value functions in the action-space hierarchy by extracting features from units and maps, embedding them with a small MLP, and passing them through a residual CNN to produce a final embedding. This embedding is used to decode into group action-scores, allowing for efficient decoding into value-function contributions for groups of any size. The approach involves using a single evaluation MLP for each group to decode the embedding, with two approaches for combining outputs. Our method, Growing Action Spaces (GAS), outperforms baselines and ablations on micromanagement tasks. GAS(2) benefits from efficient exploration and data transfer from A 0 to A 2, enabling higher performance. In comparing performance, GAS(2) outperforms the Mix&Match baseline on StarCraft micromanagement tasks. GAS(2) shows faster learning and higher winrates in most scenarios, suggesting that separate value functions for each action accelerate transfer learning. The policies learned by GAS exhibit good tactics in StarCraft micromanagement tasks, using separate groups to position the army strategically. GAS outperforms Mix&Match baseline, showing faster learning and higher win rates. Ablation study on GAS (2) without off-action-space update performs somewhat worse on average. The ablation study on GAS without off-action-space update shows varying impact on different tasks. The 'SEP-Q' variant fails to compose the value function effectively, especially in less restricted action spaces. Target choice in constructing the target does not improve learning speed as intended. The use of a higher n-step objective in the experiment slightly degrades asymptotic performance, potentially due to decreased average group size impacting spatial resolution in the CNN architecture. Higher values of n also lead to empty groups and a degenerate masked pooling operation. There is no fundamental limitation on the growth of the action space, but hierarchical approaches may be more effective. In this work, an algorithm for growing action spaces with off-policy reinforcement learning is presented. The approach involves learning value functions for all levels of a hierarchy of restricted action spaces simultaneously and transferring data and representations between different levels. The effectiveness of the approach is demonstrated in discretised continuous control tasks and challenging multiagent scenarios. Future work could explore further advancements in this area. The algorithm presented involves growing action spaces with off-policy reinforcement learning by learning value functions for different levels of a hierarchy of restricted action spaces simultaneously. Results show performance on discretised continuous control tasks and challenging multiagent scenarios. Future work may focus on efficiently exploring action spaces and deeper hierarchies. In the study, a variant of the 'Match' objective of M&M was used in a value-based setting, showing similar performance in experiments. Different parameters were used for GAS and Acrobot experiments, with fully-connected ReLU layers in the model architecture. In the study, a model with 64 hidden units is trained with GAS for Starcraft micromanagement scenarios controlling units against a scripted opponent. The opponent remains stationary until engaged in an \"attack-closest\" behavior. The agent must find and engage its opponent, attacking into a defensive position with good unit positioning. Scenarios with 50 hydralisks and 80v80 marines are imbalanced due to this. Optimal strategies require sophisticated unit positioning, with the 95 zerglings vs 50 marines scenario needing the most precise positioning. Surrounding the opponent in a concave formation can be advantageous, ensuring outer units are in attack range but out of range of center units. Timing of groups is crucial. Our model demonstrates precise unit control by coordinating group timing and positioning for optimal engagement. Features extracted for units include position, velocity, hitpoints, armor, damage values, range, weapon cooldown, and miscellaneous attributes. Map features include ground height encoding and tile information. Features are normalized to a range of 0-1 for consistency. The features for the map include ground height encoding, walkable, buildable, and fog of war. A frame-skip of 25 is used for control, with calculations for enemy health points and units. The reward function is based on damage dealt, units killed, and winning by eliminating all enemy units. The custom model architecture for Starcraft micromanagement involves embedding unit features to size 128, using a grid downsampled by a factor of 8. The features are then concatenated with map features and processed through a ResNet encoder with four residual blocks. The output is an embedding of size 64, with a decoder using mean pooling. Each evaluator is a 2-layer MLP with 64 hidden units and 17 outputs for actions. The custom model architecture for Starcraft micromanagement involves embedding unit features to size 128, using a grid downsampled by a factor of 8. The features are concatenated with map features and processed through a ResNet encoder with four residual blocks. Each evaluator is a 2-layer MLP with 64 hidden units and 17 outputs for actions. Data is collected by 64 parallel actors in a short queue, with batches of 32 6-step segments used for updates. Different optimizers and learning rates are used for Q-learning and MM baseline experiments. Exploration strategy and target network are also implemented. The action-space grows linearly from 1.0 to 0.1 over the first 10000 model updates, with a target network copying parameters every 200 updates. A lead-in of 5000 updates holds the action-space constant at A 0, preventing growth during high policy entropy. The action-space then grows linearly at a rate of 10000 updates per level of restriction, reaching A 1 after 10000 updates and A 2 after 20000 updates."
}