{
    "title": "BJk59JZ0b",
    "content": "Actor-critic methods in reinforcement learning update a policy (actor) to increase the expected return (critic). A novel method, guide actor-critic (GAC), learns a guide actor maximizing the critic locally and updates the policy through supervised learning. GAC utilizes second-order optimization in the action space based on the Hessians of the critic. The deterministic policy gradient method is a special case of GAC. Experimental results show GAC as a promising reinforcement learning approach. \"Our method shows promise as a reinforcement learning approach for continuous controls.\" Reinforcement learning aims to learn an optimal policy for maximizing cumulative rewards. It has been effective in tasks like playing games and controlling robots. Methods include value-based, policy-based, and actor-critic approaches. Value-based methods estimate return through a value function, while policy-based methods directly learn the policy. Policy-based methods, also known as policy search methods, learn a parameterized policy to maximize expected return without learning the value function. These methods, such as REINFORCE, use gradient ascent to update the policy parameter for higher sample returns. Compared to value-based methods, policy search methods are simpler and suitable for continuous problems. However, they may converge slowly due to high variance in sample returns. Actor-critic methods combine the advantages of both value-based and policy search methods. Actor-critic methods combine the advantages of value-based and policy search methods by learning an actor to maximize a critic. These methods converge faster than policy search methods and examples include actor-critic, natural actor-critic, trust-region policy optimization, and asynchronous advantage actor-critic. They utilize the value of the critic for updates and ignore higher-order information. In this paper, the importance of utilizing second-order information from the critic in actor learning is emphasized. While deterministic policy gradients and stochastic value gradients are commonly used, they do not take advantage of the Hessian of the critic. The Hessian can accelerate actor learning, leading to higher data efficiency, but the computational complexity of second-order methods is a challenge for deep reinforcement learning. Our contribution in this paper is a novel actor-critic method for continuous controls called guide actor-critic (GAC). Unlike existing methods, GAC utilizes second-order information of the critic in a computationally efficient manner. The actor learning is separated into two steps: first, a non-parameterized Gaussian actor is learned to locally maximize the critic under a KL divergence constraint, then the Gaussian actor guides the learning of a parameterized actor through supervised learning. Learning the mean of the Gaussian actor is equivalent to performing a second-order update in the action space using Hessians of the critic, with the step-size controlled by the KL constraint. In this section, a background of reinforcement learning is provided, along with a discussion on existing second-order methods for policy learning in deep reinforcement learning. The focus is on discrete-time Markov decision processes with continuous state and action spaces. The agent takes actions according to a policy and receives rewards, transitioning between states based on a transition function. The expected return in reinforcement learning is defined by the cumulative rewards with a discount factor. The action-value function expresses the return after taking an action in a state. Second-order methods like the Newton method use the Hessian of the objective function as the curvature matrix. The natural gradient method uses the Fisher information matrix as the curvature matrix in optimization. Unlike the Hessian matrix, the Fisher information matrix provides information about changes in policy measured by an approximated KL divergence. Actor-critic methods based on natural gradient have been shown to be efficient. There is a comparison between the Hessian and Fisher information matrix in policy search. Second-order updates for Deterministic Policy Gradient (DPG) and Soft Value Function (SVG) have not been extensively explored. The Hessian matrix for DPG contains the Hessians of the actor and critic, while the actor-critic method's Hessian includes the actor and critic value. Second-order methods are efficient in reinforcement learning but can be computationally intensive due to the large number of weight parameters in deep learning. Inverting the curvature matrix or solving a linear system has cubic computational complexity. The guide actor-critic (GAC) method proposes second-order updates without computational issues faced by existing methods in deep reinforcement learning. Unlike previous approaches, GAC separates the learning of the parameterized actor from the critic, avoiding the loss of useful curvature information. The guide actor-critic (GAC) method separates the learning of the parameterized actor into two steps: 1) learning a guide actor that maximizes the critic, and 2) learning a parameterized actor based on the guide actor. This approach allows for second-order updates for the guide actor independently of the actor's parameterization. The optimization problem for learning the guide actor is formulated in Section 3.1 and its solution is presented in Section 3.2. Sections 3.3 and 3.4 demonstrate that the solution involves second-order updates. Section 3.5 outlines the learning step for the parameterized actor using supervised learning. The pseudo-code and source code of the method are available online. Greedy maximization should be avoided in the process. The guide actor-critic method aims to maximize the critic by learning a guide actor, avoiding greedy maximization due to noisy estimates. Constraints are added to maximize the critic subject to state distribution induced by past trajectories. This approach differs from optimizing for a policy parameter, as it focuses on maximizing a policy function instead. The guide actor-critic method introduces constraints such as the KL divergence and Shannon entropy to ensure stability and stochastic behavior in reinforcement learning. These constraints control the exploration-exploitation trade-off and maintain the guide actor as a proper probability density. The method involves fixing a value and adaptively reducing \u03ba based on actor's entropy. The optimization problem is solved using Lagrange multipliers, resulting in a guide actor weighted by p Qps, aq. Setting different values for \u00d1 leads to different actor updates. Dual variables \u03b7 \u2039 and \u03c9 \u2039 correspond to constraints and are obtained by minimizing a dual function. The method involves setting a value for \u00d1 and using a softmax policy with a temperature parameter \u03c9 \u2039. Two assumptions are made: the actor is a Gaussian distribution with mean and covariance functions, and Taylor's approximation is locally accurate up to the second order. This allows for the computation of the gradient and Hessian of the critic, enabling the rewriting of Taylor's approximation. The method involves setting a value for \u00d1 and using a softmax policy with a temperature parameter \u03c9 \u2039. The actor is assumed to be a Gaussian distribution with mean and covariance functions, and Taylor's approximation is locally accurate up to the second order. This allows for the computation of the gradient and Hessian of the critic, enabling the rewriting of Taylor's approximation. The guide actor can be obtained in a closed form and the dual function can be evaluated through matrix-vector products. The guide actor can be obtained in a closed form and the dual function can be evaluated through matrix-vector products. Inverting F \u03b7 psq is required to evaluate the dual function, which is computationally cheap when the action dimension is not large. The mean and covariance of the guide actor are computed using the gradient and Hessian of the critic, resembling second-order updates. The quality of the guide actor depends on the accuracy of sample approximation and Taylor's approximation. The accuracy of Taylor's approximation in p Qps, aq relies on the action a 0 being close to actions sampled from \u03c0 \u03b8 pa|sq. Two approaches are proposed: Taylor's approximation around the mean using E \u03c0 \u03b8 pa|sq ras, and a second-order update in the action space to maximize p Qps, aq. The update equation for maximizing the critic involves using a damped Hessian matrix to control step size, with the damping term representing the KL constraint. Taylor's approximation is used locally, and an expectation of Taylor's approximation over the distribution can be utilized instead. The update equation for maximizing the critic involves using a damped Hessian matrix to control step size, with the damping term representing the KL constraint. The mean update corresponds to a second-order optimization step involving rotating an expected gradient using expected Hessians. The expectations can be approximated using sampled actions. In maximizing the critic, a damped Hessian matrix controls step size with the KL constraint. Sampling actions for approximating expectations can help avoid local optima. GAC-0 and GAC-1 refer to different approximations in the experiments. The covariance update requires a positive definite matrix F psq. The Gauss-Newton approximation is used in GAC to overcome issues with the Hessian matrix. A parameterized actor is learned to represent the guide actor, using supervised learning approaches. The guide actor is a Gaussian distribution. The parameterized actor in GAC is a Gaussian distribution with a state-dependent mean and covariance, learned by minimizing the expected KL divergence to the guide actor. This reveals connections to deterministic policy gradients (DPG) and updating the mean parameter with biased DPG through gradient descent. The GAC algorithm updates the mean parameter with biased DPG, where bias terms depend on \u03b7. When \u03b7 is 0, bias terms vanish, making GAC equivalent to DPG. Unlike DPG, GAC learns both mean and covariance. A state-independent parametrized Gaussian distribution is proposed for practical purposes. The mean is learned by minimizing mean-squared-error, and the covariance is determined by the average of guide covariances. The GAC algorithm updates the mean parameter with biased DPG, where bias terms depend on \u03b7. When \u03b7 is 0, bias terms vanish, making GAC equivalent to DPG. GAC learns both mean and covariance, with the covariance determined by the average of guide covariances. Computational efficiency is achieved by executing a single gradient update in each learning iteration. DPG is interpreted as a first-order optimization method in the action space, using the gradient to update the policy parameter. GAC, utilizing second-order information, should theoretically learn faster than DPG. The GAC algorithm, utilizing second-order information, learns faster than DPG. The critic's accuracy is crucial for actor-critic methods' performance. The critic is represented by neural networks with a parameter \u03bd, updated using gradient descent to minimize the squared Bellman error. The target critic with a slowly moving target improves learning stability, with a target value computed using mini-batch samples from a replay buffer. The expectation for the squared error is approximated over the current actor. The method described requires computing gradients for the Gauss-Newton approximation, which is computationally efficient for linear models and deep neural networks using automatic differentiation. Other evaluation methods like Retrace BID22 can also be used. Our method is a non-trivial extension of model-free trajectory optimization (MOTO) with a log-nonlinear Gaussian policy and a more complex critic compared to existing methods like trust region policy optimization (TRPO). Our method extends model-free trajectory optimization (MOTO) with a log-nonlinear Gaussian policy and a complex critic. It differs from TRPO in optimizing the guide actor and using a quadratic approximation of the critic. It is also related to maximum-entropy RL, which maximizes expected cumulative reward with an entropy bonus. The optimal policy in maximum-entropy RL is the softmax policy. The softmax policy and soft state-value function in maximum-entropy RL are defined by optimal soft action-value and state-value functions. BID11 proposed soft Q-learning for learning the optimal policy, using importance sampling and Taylor's approximation. This method differs from previous approaches by converting intractable integrals into matrix-vector products. In contrast to previous methods, GAC learns the guide policy via the critic without requiring a model of the transition function. GAC is evaluated on the OpenAI gym platform with the Mujoco Physics simulator using neural networks for the actor and critic. Comparing GAC-0 and GAC-1 against existing methods like DDPG, Q-NAF, and TRPO, both GAC variants perform comparably and outperform others in the Half-Cheetah task. GAC-0 and GAC-1 show comparable performance on various tasks, with GAC-1 learning faster on Humanoid. GAC-0 is expected to be more stable but prone to local optima, while GAC-1 introduces randomness to escape poor optima. GAC-S, using S+1 samples for Taylor's approximation, is expected to outperform both. High fluctuations in expected returns are observed on Hopper and Walker2D tasks compared to TRPO. Good policies are learned but quickly diverge to poor ones before improving again. The large step sizes near local optima in GAC may result in high performance fluctuations. Reducing the KL bound or adding regularization to the Gauss-Newton approximation could help avoid this issue. DDPG is the most computationally efficient method, while GAC has low costs for tasks with low-dimensional actions. The computation cost of GAC can be reduced by using external tuning parameters for step-size parameters. Actor-critic methods are efficient for real-world problems but lack second-order information of the critic. A novel framework utilizing Hessians of the critic for actor learning was proposed, with experiments showing promising results. The proposed method is related to deterministic policy gradients (DPG) and may have a connection to stochastic policy gradients. The actor-critic framework aims to improve performance by imposing a KL constraint and using more efficient policy evaluation methods like Retrace. The optimization problem subject to a KL constraint is discussed for further improvement. The optimization problem subject to a KL constraint is solved using Lagrange multipliers. The Lagrangian includes dual variables, and the solution is obtained by setting the derivative to zero. The dual function is derived by substituting the solution into the constraint terms of the Lagrangian. The final expression for the dual function involves a normalization term. The text discusses second-order optimization in the action space using Taylor's approximation and curvature matrix calculations. It shows how GAC performs optimization steps and computes averaged Taylor's approximation for a set of samples. The text discusses second-order optimization in the action space using Taylor's approximation and curvature matrix calculations. It shows how GAC performs optimization steps and computes averaged Taylor's approximation for a set of samples. In the optimization step, a curvature matrix is used, and the interpretation is valid when a specific equality holds. Despite this assumption, Taylor's approximation can still be used for policy updates. The text discusses the use of the Normalized Advantage Function (NAF) in second-order optimization in the action space. NAF represents the critic with a quadratic function with a negative curvature, allowing for negative definite Hessians to be obtained. However, a drawback of NAF is that it assumes the action-value function is quadratic regardless of states. The text introduces Input Convex Neural Networks (ICNNs) as an alternative to NAF for representing a negative critic with concave functions. ICNNs assume the action-value function is concave w.r.t. actions, similar to NAF, which may not hold for most reward functions. The text discusses the derivation of NAF with Q-learning, where the weight mean-squared-error is defined and the guide actor is determined by a specific formula. The policy obtained by performing Q-learning with NAF is also highlighted. The source code for GAC is available at https://github.com/voot-t/guide-actor-critic. The network architecture for GAC and DDPG includes neural networks with two hidden layers for the actor and critic networks. NAF uses neural networks with two hidden layers for each function, with 200 hidden units per layer. The Adam optimizer with learning rates of 0.001 and 0.0001 is used for the critic. The Adam optimizer is used with learning rates of 0.001 for the critic network and 0.0001 for the actor network. The target networks have a moving average step of 0.001. The replay buffer has a maximum size of 1000000 and mini-batches size of 256. The actor and critic networks' weights are initialized as described, with the output layers having initial weights drawn uniformly. The initial covariance in GAC is set as an identity matrix. DDPG and QNAF use the OU-process for exploration. TRPO implementation is from https://github.com/openai/baselines with a batch size of 1000. GAC has a fixed KL upper-bound of 0.0001. The entropy lower-bound \u03ba is adjusted heuristically by \u03ba \" maxp0.99pE\u00b4E 0 q`E 0 , E 0 q, ensuring gradual decrease without being too small. The dual function is minimized using the SLSQP method with initial values \u03b7 \" 0.05 and \u03c9 \" 0.05. The number of samples for computing the target critic value is M \" 10. Update \u03bd using Adam and moving average. Compute a n,0 for each s n and update the actor network. The text chunk discusses the computation and optimization process for a guide actor in a reinforcement learning setting. It involves updating policy parameters, solving for optimal values using non-linear optimization methods, and updating the actor network. Experiments are conducted on the OpenAI gym platform with Mujoco Physics simulator, using provided state space, action space, and reward function without normalization or gradient clipping. The discount factor is set to 0.99 for each episode. The experiments involve updating policy parameters and optimizing the actor network in a reinforcement learning setting. The discount factor is set to 0.99 for each episode. Experiments are repeated 10 times with different random seeds, and results are averaged over 10 trials. The total computation time for training the policy is reported in TAB1, with the mean and standard error computed over 10 trials in hours. TRPO is not included due to performing fewer updates using batch data samples."
}