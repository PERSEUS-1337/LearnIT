{
    "title": "B1e0X3C9tQ",
    "content": "Variational autoencoders (VAEs) are a popular deep generative model, but the effectiveness of VAEs in generating realistic samples is often questioned due to Gaussian encoder/decoder assumptions. A study rigorously analyzes the VAE objective to determine when this belief holds true. A simple VAE enhancement is proposed based on these insights, producing high-quality samples and competitive FID scores compared to GAN models. The enhancement retains the original VAE architecture's desirable attributes without requiring additional hyperparameters or sensitive tuning. The code for the model is available at the provided URL. The desire is to learn a probabilistic generative model of observable variables x on a r-dimensional manifold embedded in R^d. The assumption is made to handle cases where x has low-dimensional structure relative to a high-dimensional space. The manifold \u03c7 is assumed to be a simple Riemannian manifold with a ground-truth probability measure \u00b5 gt on it. The variational autoencoder (VAE) aims to approximate the ground-truth measure on a manifold using a parameterized density across R^d. It utilizes encoder and decoder distributions to minimize the negative log-likelihood, as direct marginalization over latent variables is generally infeasible. The VAE cost is a bound on the average negative log-likelihood, involving encoder/decoder distributions and SGD optimization via a reparameterization trick. It includes a reconstruction cost and penalizes posterior deviations from the prior distribution. Integration over training samples is approximated via a finite sum. The VAE cost involves encoder/decoder distributions, SGD optimization, and a reconstruction cost. Posterior deviations from the prior distribution are penalized. The integration over training samples is approximated via a finite sum. The commonly adopted distributional assumption for continuous data in VAEs is that both q \u03c6 (z|x) and p \u03b8 (x|z) are Gaussian. This design choice has been criticized as a limitation of VAEs, with generative adversarial networks (GAN) being favored in quantitative tests of generative modeling quality. In Section 2, the implications of VAE Gaussian assumptions are closely examined, leading to diagnostic conclusions about recovering the ground-truth distribution. It is shown that the VAE global optimum can be reached uniquely when r = d, but not necessarily uniquely when r < d. Section 3 delves into the non-uniqueness of solutions in reaching the VAE global optimum. Section 3 further explores the non-uniqueness issue of VAE global optima when r < d. An optimal VAE parameterization can perfectly reconstruct all x \u2208 \u03c7 using a degenerate latent code with only r active dimensions. This indicates that VAE can uniquely learn a mapping to the correct ground-truth manifold when r < d, but not necessarily the correct probability measure within the manifold. In Section 4, an enhancement for VAE is proposed to address regimes when r < d by learning the manifold in the first stage. The two-stage procedure in the study learns the manifold in the first stage and the correct probability measure in the second stage. Experimental results show that this approach can generate high-quality samples with reduced blurriness in VAE models. It is the first demonstration of a VAE pipeline producing stable FID scores comparable to GAN models without additional penalties. The study demonstrates a two-stage procedure in VAE models that learns the manifold and correct probability measure. It shows that high-quality samples with reduced blurriness can be generated without additional penalties. The extended version of the work includes additional results and a discussion on VAE modeling paradigms. The study argues that VAE Gaussian assumptions can still optimize the VAE objective and recover the ground-truth probability measure. A \u03ba-simple VAE is a VAE model with explicit Gaussian assumptions and parameterizations. It has dim[z] = \u03ba latent dimensions, with Gaussian encoder q \u03c6 (z|x) = N (z|\u00b5 z , \u03a3 z ) and Gaussian decoder p \u03b8 (x|z) = N (x|\u00b5 x , \u03a3 x ). The encoder and decoder moments are defined by specific functions, allowing for complex parameterizations like deep neural networks. This VAE model aims to recover the ground-truth probability measure. A \u03ba-simple VAE, with \u03ba \u2265 r, can achieve optimality criteria (i) and (ii) by considering cases where r = d and r < d. The distinction between these cases has practical implications. The minimal value of (1) occurs when certain conditions hold, ensuring the approximate posterior matches the actual. The approximate posterior produced by the encoder in a \u03ba-simple VAE can perfectly match the actual posterior and the marginalized data distribution can match the ground-truth density. The VAE Gaussian assumptions do not prevent the optimal ground-truth probability measure from being recovered as long as the latent dimension is sufficiently large. This is contrary to the belief that a richer class of distributions is needed to achieve this. When r < d, if q \u03c6 (z|x) and p \u03b8 (x|z) are arbitrary, inf \u03c6,\u03b8 L(\u03b8, \u03c6) = \u2212\u221e. To reach the global optimum, set q \u03c6 (z|x) = p \u03b8 (z|x) and collapse all probability mass to the correct manifold \u03c7. The density p \u03b8 (x) becomes unbounded on \u03c7, approaching negative infinity. Gaussian assumptions in the VAE model may prevent this by causing the KL term to blow up. When r < d, setting q \u03c6 (z|x) = p \u03b8 (z|x) and collapsing probability mass to manifold \u03c7 can lead to unbounded density p \u03b8 (x) on \u03c7. Gaussian assumptions in VAE model may prevent this by causing KL term to blow up. Theorem 2 states that for any \u03ba \u2265 r, there are VAE model parameters {\u03b8 * t , \u03c6 * t } such that DISPLAYFORM0 DISPLAYFORM1 for measurable sets A \u2286 R d with \u00b5 gt (\u2202A \u2229 \u03c7) = 0. Theorem 2 explains that VAE Gaussian assumptions do not hinder the minimization of the objective function, allowing for a globally optimum solution. This solution closely approximates the groundtruth measure for all practical purposes, making it an effective approximation to \u00b5 gt. The theorem highlights the distinction between achieving the minimal objective in the \u03ba-simple VAE and a simpler case described by Theorem 1. The \u03ba-simple VAE can achieve the minimal objective by recovering the ground-truth probability measure almost everywhere. However, in the r < d case, a different parameter set could reach the lower bound without closely approximating \u00b5 gt due to lack of uniqueness. This issue is not specific to VAE Gaussian assumptions and persists even with unconstrained q \u03c6 (z|x). The VAE model relies on a density p \u03b8 (x) defined across all of R d, as it lacks access to the groundtruth low-dimensional manifold. The VAE model's key takeaway is that the Gaussian assumptions may not be the root cause of failure to recover ground-truth distributions. Instead, a structural deficiency lies in the non-uniqueness of solutions that optimize the VAE objective without learning a close approximation to \u00b5 gt. Further exploration is needed to disambiguate optimal solutions and their relationship with ground-truth manifolds in Section 3. In Section 3, the focus will be on locating the correct ground-truth manifold and learning the correct probability measure within it. Previous work by BID10 briefly touched on similar results but with stronger assumptions. The analysis requires non-zero and infinitely differentiable ground-truth densities, limiting practical usage in cases where dimensions are greater than one or ground-truth densities are not smooth enough. The properties of optimal \u03ba-simple VAE solutions will be examined to understand their effectiveness. The focus in Section 3 is on finding the correct ground-truth manifold and learning the correct probability measure within it. The properties of optimal \u03ba-simple VAE solutions are examined to understand their effectiveness, with a focus on reducing the VAE cost by choosing a smaller value of \u03b3. Despite necessary optimality conditions, practical VAE applications often fix \u03b3 \u2248 1 during training. The text discusses the consequences of favoring a smaller value of \u03b3 in VAE solutions, particularly in relation to reconstructing data from the ground-truth manifold \u03c7. Theorem 4 states that any x \u2208 \u03c7 can be perfectly reconstructed by the VAE encoder/decoder pair at globally optimal solutions, despite potential stochastic factors. Further insights can be gained by examining the VAE objective function behavior. When examining the VAE objective function at small nonzero values of \u03b3, it is found that the squared eigenvalues of f Sz (x; \u03c6 * \u03b3) decrease proportionally to \u03b3. This leads to the VAE data term integrand behaving as DISPLAYFORM2 around optimal solutions. The inclusion of additional latent variables does not significantly lower this behavior. Adding dimensions to the latent space in a VAE does not improve the data term value significantly. However, it can negatively impact the KL regularization factor, especially when \u03b3 is close to 0. The parameter r represents the number of low-noise latent dimensions preserved by the VAE model for reconstruction. It is important to keep r as small as possible without compromising data fit, as values below r can lead to significant reconstruction errors. The VAE aims to produce optimal reconstructions using minimal clean latent dimensions, pushing unnecessary dimensions' variance to one. This allows for learning a minimal representation of the ground-truth manifold independently of the actual distribution within it. The VAE objective focuses on achieving optimal reconstructions with minimal latent dimensions, neglecting the distribution within the subspace. There is an intrinsic bias in the VAE objective that prioritizes approximating the manifold over fitting the data distribution. The VAE objective aims to minimize error and prevent degradation in reconstructions by controlling the parameter \u03b3. By exploiting the relationship between d and r, the dilemma of balancing \u03b3 can be resolved. The practical utility of VAE properties goes beyond learning \u03b3, especially in developing generative models for high-dimensional data with significant low-dimensional structure. The VAE can handle low-dimensional manifolds but struggles with estimating the correct probability measure within them. The decoder may map samples correctly, but inaccurate estimation of the measure disrupts ancestral sampling capability. The VAE struggles with estimating the correct probability measure within low-dimensional manifolds, affecting ancestral sampling capability. A two-stage remedy involves training a \u03ba-simple VAE to estimate the ground-truth manifold \u03c7 and then training a second VAE to learn the distribution q \u03c6 (z) for generating samples approximating the original ground-truth \u00b5 gt. The efficacy of the second-stage VAE is based on ensuring that samples from q \u03c6 (z) cover the full ambient space R \u03ba adequately. By setting \u03ba \u2265 r, the VAE can effectively handle situations where the manifold dimension is equal to the ambient dimension, as shown in Section 2.1. The second-stage VAE, as shown in Theorem 1, can handle situations where d = r = \u03ba, making the troublesome factor zero. The revised aggregated posterior q \u03c6 (u) should resemble N (u|0, I), and the second-stage VAE can be small if d \u03ba \u2265 r. Jointly training the two VAE stages does not generally improve performance, as the second-stage parameters can be influenced by the first stage reconstruction term. The critical mismatch between q \u03c6 (z) and N (u|0, I) was empirically tested by fusing encoders and decoders from the first and second stages of a VAE model. Joint training of the two stages did not improve performance, highlighting the importance of separate training. Quantitative evaluation of generated samples showed that GAN models outperform VAE approaches. The 2-Stage VAE model, based on InfoGAN architecture, was evaluated against three baseline VAE models. The second-stage VAE used small, 3-layer networks for comparison. The evaluation focused on Fr\u00e9chet Inception Distance (FID) scores and related metrics for standardized comparisons. The curr_chunk discusses various VAE models, including a Gaussian layer with fixed and learned \u03b3, a cross-entropy layer, a Gaussian decoder VAE model with normalizing flows, and the Wasserstein autoencoder (WAE) with two variants. Experiments were conducted using a neutral architecture, and results from competing GAN models were also presented. The curr_chunk discusses different VAE models like MM GAN, WGAN, NS GAN, DRAGAN, LS GAN, and BEGAN, tested on various datasets. Results show the 2-Stage VAE outperformed GAN models on two out of four datasets. The 2-Stage VAE outperformed GAN models on two out of four datasets, with a mean FID score below 40, while GAN models had scores above 45. The WAE-MMD model's poor performance on MNIST and Fashion MNIST datasets was due to a fixed \u03ba value of 64, which may be larger than needed for simpler data types. The 2-Stage VAE showed superior performance compared to GAN models on two datasets, with FID scores below 40. Despite some advantages of GAN models, the default 2-Stage VAE performed well and demonstrated competitive sample quality. Representative samples can be found in BID8. The 2-Stage VAE outperformed the WAE-GAN model in experiments using CelebA data, with results reported in Table 2. The second-stage VAE's performance was not solely due to a larger network structure, showing promising results without hyperparameter tuning. The text chunk discusses predictions made by different models in various settings, including MNIST Fashion, CIFAR-10, and CelebA datasets. The models mentioned include GAN, LSGAN, WGAN, WGAN GP, DRAGAN, BEGAN, VAE, VAE + Flow, and WAE-MMD, with their respective performance metrics. The FID scores of various models, including VAE, VAE + Flow, WAE-MMD, and 2-Stage VAE, are compared using a neutral architecture. The values represent the optimal FID obtained through hyperparameter search, excluding outlier cases. Best GAN indicates the lowest FID across all GAN approaches for each dataset. Only default settings were used for VAE results, with no tuning and no instances of mode collapse. The FID scores for various models, including VAE, WAE-MMD, and 2-Stage VAE, were compared using a neutral architecture with no tuning. Results showed that the 2-Stage VAE model can reduce the gap between q(z) and p(z), leading to convergence of \u03b3 to zero at global minimums. Empirical support for this is shown in Figure 1a. The decoder variance \u03b3 tends towards zero during training, leading to tighter image reconstructions with lower error. Figure 1b demonstrates the impact of noise factors on different directions in the latent space, showing how perturbations are muted in superfluous dimensions but cause significant changes in needed dimensions. This reduces the mismatch between q \u03c6 (z). Reduced mismatch between q \u03c6 (z) and p(z) is achieved by the VAE with a learnable \u03b3, which converges close to 0 during training. The eigenvalues of \u03a3 z should be close to 0 or 1, indicating informative directions for representing the manifold. The proposed 2-Stage VAE can overcome issues with mismatch between q \u03c6 (z) and p(z) by achieving a standard Gaussian aggregated posterior. This is supported by empirical evidence showing that the latent sample matrices from the enhanced second stage are closer to a standard Gaussian matrix compared to the first stage. The 2-Stage VAE model improves the latent representation, reducing the difference from a standard Gaussian matrix. While not claiming superiority to GANs in image realism, it narrows the gap significantly, making VAEs viable for a wider range of applications. The 2-Stage VAE model enhances the latent representation, making VAEs suitable for a broader range of applications. For more detailed results and discussions, refer to BID8."
}