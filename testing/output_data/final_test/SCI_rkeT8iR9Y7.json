{
    "title": "rkeT8iR9Y7",
    "content": "Although stochastic gradient descent (SGD) is crucial for deep learning success, understanding its dynamics in high-dimensional spaces is limited. Researchers have analyzed SGD from a geometrical perspective, focusing on the stochasticity of minibatch gradients. A model using von Mises-Fisher distribution shows that directional uniformity of minibatch gradients increases during SGD. Empirical verification using deep convolutional networks confirms a higher correlation between gradient stochasticity and directional uniformity compared to gradient norm stochasticity. Stochastic gradient descent (SGD) is a key optimization algorithm for deep learning, despite efforts to improve it with second-order information. The learning dynamics of SGD are not well understood due to non-convexity and high dimensionality of objective functions. Gradient stochasticity, or signal-to-noise ratio, has been proposed as a tool for analyzing SGD dynamics. Two phases have been identified based on gradient mean in SGD. In SGD, two phases are identified based on gradient mean: \"drift phase\" with high mean and rapid optimization progress, followed by \"diffusion phase\" behaving like Gaussian noise. Gradient stochasticity includes norm and directional balance, leading to convergence when norms vanish or angles are uniformly distributed. In this paper, the von Mises-Fisher distribution is used to analyze the directional balance of minibatch gradients in SGD. It is proven that SGD increases this balance, which is empirically verified on deep convolutional networks with techniques like batch normalization and residual connections on MNIST and CIFAR-10 datasets. The directional stochasticity of minibatch gradients is analyzed using angles and the concentration parameter of the vMF distribution. The gradient norm stochasticity decreases as batch size increases, showing the importance of understanding directional statistics in stochastic gradient descent dynamics. Most studies on SGD dynamics focus on two-phase behavior, with different phases identified based on network depth and activation functions. The information transition in SGD is not always associated with signal-to-noise ratio transition when using ReLU activation functions. The previous work focused on the inner product between minibatch gradients and identified transient and stationary phases. Experimental verification was limited to specific settings and did not include the effects of batch normalization and residual connections. Norms and angles were defined, and loss functions of neural networks were discussed. The text discusses the definition of loss functions in neural networks, including per-example and minibatch loss functions. It also introduces notation for gradients and optimization parameters during training. Additionally, it presents characteristics of the von Mises-Fisher distribution in a 2-dimensional space. The concentration parameter \u03ba determines sample concentration on mean direction \u00b5 in the vMF distribution. Maximum likelihood estimates for \u00b5 and \u03ba are 1\u2212r 2 where x i 's are random samples. SGD uses minibatch gradient\u011d(w) = \u2212\u2207 w f I (w) instead of full batch gradient g(w) = \u2212\u2207 w f (w). Batch size m affects randomness of\u011d(w). The batch size m affects the randomness of the minibatch gradient \u011d(w). A large batch size reduces the variance of \u011d(w) with convergence rate O(1/m). Empirical verification shows that gradient norm stochasticity is low at random initial points. The gradient norm stochasticity is low at random initial points but may increase after SGD updates due to the balance of directions of minibatch gradients, motivating an investigation into directional statistics. The asymptotic behavior of angles between uniformly random unit vectors in high-dimensional space is analyzed to understand this balance. The distribution of angles between minibatch gradients and uniformly random unit vectors becomes increasingly concentrated as dimensionality grows, converging to an asymptotic distribution after SGD iterations. Comparing these distributions is non-trivial, requiring alternative methods to measure the uniformity of minibatch gradients. To measure the uniformity of minibatch gradients, a density plot is drawn for 3,000 minibatch gradients at different weights, showing convergence to an asymptotic density. The vMF distribution is proposed to model this uniformity, with the concentration parameter \u03ba indicating how uniformly unit vector directions are distributed. With a large batch size, the norm of minibatch gradients becomes deterministic, and \u03ba measures the concentration of gradients around the full batch gradient. Lemma 1 establishes the relationship between the norm of averaged unit vectors and \u03ba as an estimator. The norm of averaged unit vectors is related to \u03ba, the estimator of \u03ba. Lemma 1 shows that \u03ba decreases as we move away from a certain point. If points are not on a single ray, there exists a positive number \u03b7. This connection is made with SGD. The connection between the norm of averaged unit vectors and \u03ba, the estimator of \u03ba, is established in Lemma 1, showing that \u03ba decreases as we move away from a certain point. This relationship is linked to SGD through the concept of local minibatch solutions and the direction of gradient descent. Theorem 3 states that \u03ba(\u00b7) decreases with perturbation along the averaged direction. Corollary 3.1 allows for computing \u03ba(w 0 t) using \u011d. In practice, using n b \u2248 n/m minibatches at each epoch is common to transition from w 0 t to w 0 t+1. The average of normalized minibatch gradients is the maximum likelihood estimate of \u00b5 with a large n b. The average of normalized minibatch gradients is a key metric in analyzing the learning dynamics of SGD when training deep neural networks. Despite variations in batch sizes, the trend shows a smooth decrease in \u03ba, indicating effective learning with minibatches of size 64. Validation loss and other statistical quantities are also monitored during training. The study evaluates different types of deep neural networks, including FNN, DFNN, and CNN with various layers and configurations. Batch normalization is tested to improve the smoothness of the loss function. MNIST and CIFAR-10 datasets are used for training and analysis. The importance of a sufficiently large batch size is empirically analyzed. The study empirically analyzes the need for a large batch size, settling on a practical size of 64. A fixed learning rate of 0.01 is used to achieve training accuracy > 99.9%. The decrease in \u03ba over training is observed regardless of network depth, with FNN showing a more pronounced decrease compared to DFNN. This difference diminishes when using a larger batch size. The study compares the performance of different neural networks, showing that batch normalization helps improve loss function behavior. The CNN, trained on CIFAR-10, may not align as well with theoretical assumptions. However, when equipped with residual connections and batch normalization, the loss function is conjectured to better meet theoretical assumptions. The study demonstrates that batch normalization improves neural network performance by ensuring well-behavedness. BID27 and BID25 support this conjecture by showing how batch normalization guarantees boundedness of Hessian and residual connections eliminate singularities. The minimum average \u03ba of different network configurations at the end of training implies that the directional distribution of minibatch gradients is close to uniform. For more detailed analysis, refer to Supplementary F. The gradient stochasticity (GS) metric was used to identify two phases of SGD learning in deep neural networks. It includes gradient norm stochasticity (GNS) and directional uniformity \u03ba. The relationship among these quantities, training and validation losses were investigated for CNN models trained on CIFAR-10. SNR and \u03ba were highly correlated, while normSNR was less correlated. Scatter plots in log-log scales confirmed these relationships. The directional uniformity \u03ba metric correlates better with gradient stochasticity than gradient norm stochasticity. This relationship is especially prominent during the early stage of learning, indicating that the directional statistics of minibatch gradients play a significant role in the learning dynamics of SGD. The scatter plots further illustrate this difference in correlations. The stochasticity of gradients is crucial for understanding SGD's learning dynamics and has been recognized as a key factor in its success. The paper presents a theoretical framework using the von Mises-Fisher distribution to explain the directional stochasticity of minibatch gradients. The directional uniformity of minibatch gradients improves over the course of SGD training, especially with batch normalization and skip connections. The stochasticity of gradients is mainly determined by directional stochasticity rather than gradient norm stochasticity. Future research directions include exploring the relationship between directional uniformity and generalization error, and further investigating the impact of handling gradient stochasticity on SGD optimization. The directional uniformity of minibatch gradients improves during SGD training, focusing on passive analysis. Theorem 1 proves that considering directional statistics of minibatch gradients can enhance SGD optimization. Lemma A.1 defines selector random variables BID11 and provides key equations for gradient analysis. Theorem B.1 and Theorem B.2 discuss Slutsky's theorem and the delta method for analyzing the limiting behaviors of random variables. Lemma B.1 states properties of mutually independent uniformly random unit vectors. Theorem 2 states that mutually independent uniformly random unit vectors u and v have certain properties. The Lipschitz continuity of a function is discussed in relation to these vectors. The derivative of \u03ba with respect to u is bounded, implying Lipschitz continuity of h(\u00b7). If all p i 's are not on a single ray from the current location w, there exists a positive number \u03b7. The proof involves regarding w as the origin and showing strict inequality when all p i 's are not on a single ray from the origin. The proof is similar to that of Lemma 2. Differentiating with respect to , we rewrite pj = x j and use f (0) in the proof of Lemma 2. Since f (0) < 0, by using x j = 1 and applying the Cauchy inequality, we define r = min j p j. To prove Corollary 3.1, we need to show \u03ba(w 0 t+1) < \u03ba(w 0 t). If the batch size is large and the learning rate \u03b7 is small, \u011d i (w can be converted. Additionally, if A is positive-definite, then \u03bb max (A) and \u03bb min (A) are maximal and minimal eigenvalues of A, respectively. If the condition number of the positive definite Hessian matrix of f Ii at a local minibatch solution pi is close to 1, then the direction to pi from w is approximately parallel to its negative gradient at w. This implies Lipschitz continuity of h(\u00b7) for all w \u2208 R. The experiments involve fully connected networks with different architectures, including FNN with a single hidden layer and DFNN with three hidden layers. The CNN architecture includes multiple convolution layers with varying filter sizes and numbers. No biases are used in the weighted layers, and Xavier initializations and cross entropy loss functions are applied. The CNN architecture includes multiple convolution layers with varying filter sizes and numbers. Batch normalization is applied before ReLU activations on hidden layers, and identity skip connections are added every two convolution layers. Data augmentations are not used, and pixel values are scaled into [0, 1]. For validation on CIFAR-10, 5000 images are randomly chosen from the training set. In a study on the vMF distribution, random samples were taken in a 10,000-dimensional space to estimate \u03ba. Larger sample sizes are needed as \u03ba approaches 0. Various simulations were run to investigate the accuracy of \u03ba estimation. In a study on the vMF distribution, random samples were taken in a 10,000-dimensional space to estimate \u03ba. The number of samples and true \u03ba were varied, with \u03ba approaching the true value as sample size increased. Estimation error rapidly became zero with 3,000 samples when true \u03ba was large, but did not completely narrow for low true \u03ba. Dimensionality was also varied to investigate \u03ba, with 3,000 samples used for consistency. Five simulations were run for each scenario, reporting mean and standard deviation. The study involved running five simulations for each scenario, reporting mean and standard deviation. The trend of increasing \u03ba's with respect to dimensions was observed, suggesting caution in comparing absolute values of \u03ba's across different network architectures. The plots from four other training runs showed strong correlation between GS and \u03ba, while GNS was less correlated to GS. The evolution of various training metrics was plotted, normalized for easier comparison. The study involved running five simulations for each scenario, reporting mean and standard deviation. The trend of increasing \u03ba's with respect to dimensions was observed, cautioning against comparing absolute values of \u03ba's across different network architectures. The plots from four other training runs showed strong correlation between GS and \u03ba, while GNS was less correlated to GS. The evolution of various training metrics was plotted, normalized for easier comparison. SNR and \u03ba are highly correlated, while normSNR is less correlated. Scatter plots suggest that SNR is largely driven by directional uniformity. The study found that SNR and \u03ba are highly correlated, while normSNR is less correlated. Scatter plots suggest that SNR is largely driven by directional uniformity. The evolution of training and validation loss, SNR, normSNR, and directional uniformity \u03ba are plotted. SNR and \u03ba show high correlation, while normSNR is less correlated. Scatter plots indicate that SNR is mainly influenced by directional uniformity."
}