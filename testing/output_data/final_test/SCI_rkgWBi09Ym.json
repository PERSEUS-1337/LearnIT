{
    "title": "rkgWBi09Ym",
    "content": "Generative Adversarial Networks (GANs) have shown success in producing realistic synthetic images, but struggle with diverse training sets. To address this, a Multi-Modal Gaussian-Mixture GAN (GM-GAN) is proposed, with a mixture of Gaussians in the latent space. A supervised variant allows for conditional sample synthesis. A new scoring method evaluates the model's performance based on diversity and quality of generated data. GM-GANs outperform baselines in experiments with synthetic and real-world datasets. GM-GANs outperform baselines in both Inception Score and alternative scoring methods. The unsupervised variant maps latent vectors to different classes, aiding in unsupervised clustering. The model allows for post-training control of quality-diversity trade-off in generated samples. Generative Adversarial Networks (GANs) are a family of methods for learning generative models in machine-learning. GANs consist of a Generator (G) that generates samples from data distribution (p X) by converting latent vectors from a lower-dimension space (Z) to a higher-dimension data space (X). A Discriminator (D) is trained to distinguish real training samples from fake samples generated by G. G is trained to produce samples that resemble real training samples, while D aims to differentiate between real and fake samples. Generative Adversarial Networks (GANs) involve a Generator (G) creating samples resembling real training data, while a Discriminator (D) distinguishes between real and fake samples. G learns solely through interaction with D, both implemented using deep networks. Various methods have been proposed to enhance GAN performance, but generated samples may still not be realistic. In this work, the focus is on improving GAN performance when the training dataset has large inter-class and intra-class diversity. Various methods have been proposed to enhance GAN performance, including architectural changes and modifications to the loss function. Our work focuses on improving GAN performance by investigating the significance of the probability distribution used for sampling latent vectors in the latent space. Previous research has explored modifications to the loss function and structure of the latent space, but the choice of probability distribution for sampling latent vectors has been rarely studied. Our work aims to enhance GAN performance by exploring the impact of modifying the probability distribution for sampling latent vectors in the latent space. We propose using a multi-modal distribution that aligns with the sparse nature of high-dimensional datasets, such as natural images. The architectural modification proposed in this paper can be used in conjunction with other improvements. Supervision can be added by linking labels with mixture components. The paper describes GM-GAN models, a method for measuring the trade-off between sample quality and diversity, empirical evaluation on diverse datasets, and a clustering method using GM-GANs. The paper introduces a method for clustering datasets using GM-GANs, optimizing a target function with a multi-modal distribution for the prior instead of a unimodal one typically used in GANs. The paper proposes using a mixture of Gaussians as a multi-modal prior distribution in GM-GANs for clustering datasets, allowing for a more flexible model with fixed or dynamically changing parameters. In the supervised setting of GM-GAN, the discriminator returns a vector o \u2208 R N where N is the number of classes in the dataset. The Generator's goal is to generate samples classified as real samples from a specific class based on the latent vector z sampled from the Gaussian mixture. In GM-GAN, the discriminator's output vector o is based on the number of classes in the dataset. The Generator aims to generate real samples from a specific class using the latent vector z from the Gaussian mixture. The training procedure for GM-GANs involves modifying loss functions to accommodate class labels and is detailed in Algorithm 1. The training procedure for GM-GANs involves setting parameters like batch size, learning rate, and mapping functions. It includes steps for sampling real and fake data, computing losses, and updating weights for the Generator and Discriminator. A new scoring method for GANs is introduced as an alternative to the commonly used Inception Score. The Inception Score, as proposed in BID28, has drawbacks such as being limited to evaluating GANs for natural images, measuring inter-class diversity only, combining quality and diversity into one score, and varying scores based on different parameters. The Quality-Diversity trade-off measures the quality of samples based on their probability, implying that samples from dense areas in the source domain are mapped to high-quality samples in the target domain. The proposed scoring method for GAN models evaluates the trade-off between sample quality and diversity by sampling from dense areas of the source domain. This method relies on a pre-trained classifier to measure both quality and diversity of generated samples. The quality and diversity of generated samples are measured using a pre-trained classifier. Quality is assessed by the Euclidean distance from the sample's representation to its nearest neighbor in the training set. Diversity considers both inter-class and intra-class variations, measured by the average MS-SSIM metric between generated images. The diversity of generated images in a set is evaluated using the MS-SSIM metric and a pre-trained classifier. The diversity score is calculated based on the entropy of classification predictions. Empirical evaluation compares the performance of GM-GAN with baseline models. The performance of the proposed GM-GAN model is compared to the AC-GAN BID24 model using standard normal latent space distribution. The network architectures and hyper-parameters are similar to baseline models, with experiments conducted on 6 datasets. The GM-GAN models are evaluated on a toy dataset to understand their properties. The dataset consists of 5,000 training samples drawn from a mixture of 9 Gaussians. Two instances of the GM-GAN model were trained, one supervised and one unsupervised. Both variants generate samples closer to the original distribution compared to baseline models. The GM-GAN model shows a trade-off between quality and diversity, with high quality and low diversity for \u03c3 = 0.25 and vice versa for \u03c3 = 2.0. The Generator can map each Gaussian in the latent space to samples in the data-space aligned with a single Gaussian. GM-GAN models converge faster than classical GAN models. The models were evaluated using Inception Score BID28 on complex datasets, showing superior performance compared to baseline models. The GM-GAN models were trained on CIFAR-10 and STL-10 datasets with different numbers of Gaussians in the latent space. Results showed higher Inception Scores compared to baseline models, with the supervised GM-GAN model outperforming AC-GAN significantly. In our experiments, we control the quality-diversity trade-off by varying the probability distribution used to sample latent vectors from the latent space. This is done by adjusting the covariance matrix of each Gaussian with a scaling factor \u03c3. When \u03c3 < 1, samples have higher quality but lower diversity, and when \u03c3 > 1, samples have lower quality but higher diversity. In experiments, the quality-diversity trade-off is controlled by adjusting the covariance matrix of Gaussian distributions with scaling factor \u03c3. Lower \u03c3 values result in higher quality but lower diversity samples, while higher \u03c3 values lead to lower quality but higher diversity samples. GM-GANs show this trade-off when trained on Toy and MNIST datasets, as demonstrated in Figures 3 and 4. Samples from GM-GANs trained on MNIST show decreasing quality and increasing diversity as \u03c3 increases. Quality and Diversity Scores were calculated for different \u03c3 values, with models trained 10 times for evaluation. The Quality and Diversity Scores of GM-GAN and baseline models trained on CIFAR-10 and STL-10 datasets are compared in FIG3. Our proposed model outperforms the baseline in supervised training, showing higher quality and diversity scores. In unsupervised training, the baseline model generates higher quality but less diverse images compared to our model for lower \u03c3 values. Visual examination reveals that baseline model samples belong to a single class, while our model's samples are more diverse. The experiments show that as \u03c3 increases, Quality Score ascends and Combined Diversity Score descends. GM-GAN's unsupervised variant maps latent vectors from different Gaussians to different classes in the data space. The sparse latent space with multiple non-overlapping Gaussians leads to this phenomenon, utilized to develop a new clustering algorithm. The proposed method involves training an unsupervised GM-GAN with K Gaussians to generate synthetic samples for clustering. A multi-class classifier is then trained on these samples to assign soft-cluster assignments to the original dataset. The unsupervised clustering procedure using GM-GANs involves training a GM-GAN with K Gaussians on a set of samples X, sampling latent vectors, generating samples, labeling samples by the Gaussian they were generated from, uniting all samples, training a classifier on the samples and labels, and clustering X using the classifier. Cluster X using classifier c. Method ACC NMI MNIST K-Means 0.5349 0.500 AE + K-Means 0.8184 -DEC 0.8430 -DCEC BID8 0.8897 0.8849 InfoGAN 0.9500 -CAE-l 2 + K-Means BID1 0.9511 -CatGAN 0.9573 -DEPICT BID4 0.9650 0.9170 DAC BID2 0.9775 0.9351 GAR BID11 0.9832 -IMSAT BID9 0 Table 3: Clustering performance of our method on different datasets based on ACC and NMI metrics. Tested on MNIST, Fashion-MNIST, and a subset of the Synthetic Traffic Signs Dataset. The unsupervised clustering performance of our method is evaluated using Normalized Mutual Information (NMI) and Clustering Accuracy (ACC) metrics, with scores presented in Table 3. Algorithm 2 can be implemented with other GAN variants for similar results, addressing challenges in modeling data with diverse characteristics. In an empirical study, a variant of the basic GAN model called GM-GANs is proposed to address the problem of large inter-class and intra-class diversity in training sets. The model uses a mixture of Gaussians in the latent space, similar to the target data distribution. It can be used with or without label supervision and outperforms baselines in both Inception Score and alternative scoring methods. The quality-diversity trade-off can be controlled by adjusting the model. The quality-diversity trade-off in GM-GAN models can be controlled by adjusting the latent space distribution post-training. This allows for sampling higher quality or lower diversity samples as needed. GM-GANs have been used in computer vision for tasks like super resolution, text-to-image translation, and image in-painting. Additionally, the unsupervised variant of GM-GAN can be utilized for unsupervised clustering with quantitative evaluation support. GANs have been utilized in various tasks such as music generation, text generation, and speech enhancement. Efforts have been made to improve GANs through architectural changes and modifying loss functions. Conditional GANs have been introduced to enhance sample quality and stabilize training. Info-GAN aims to impose structure on the latent space by decomposing input noise. The text discusses the use of latent codes in Generative Adversarial Networks (GANs) to discover object classes in an unsupervised manner. Adversarial AutoEncoders employ GANs for variational inference, while BID16 combined a Variational Auto-Encoder with a GAN to learn high-level abstract visual features. The model can learn a latent space where visual features can be modified using arithmetic of latent vectors. Parameters of the Gaussian distribution for sampling the latent vector can be fixed or learned. Two variants of the model are investigated - static where parameters are fixed and dynamic where parameters can change during training for potentially better results. In the Static Multi-Modal GAN (Static GM-GAN) model, parameters of the Gaussian distribution are fixed before training. Each mean vector is sampled from a uniform distribution, and covariance matrices have a specific form. In the Dynamic GM-GAN model, parameters for each Gaussian can change during training, allowing for better results. The Dynamic GM-GAN model modifies the architecture to allow each Gaussian to have a unique covariance matrix and wander to new locations during training. A categorical random variable determines from which Gaussian the latent vector should be sampled, and the re-parameterization trick is used to optimize parameters. The Dynamic GM-GAN model modifies the architecture to allow each Gaussian to have a unique covariance matrix and wander to new locations during training. In contrast, the curr_chunk discusses the Inception Scores of Static GM-GAN models trained on CIFAR-10 and STL-10 datasets with different values of \u03c3, showing varying performance. Additionally, the GM-info-GAN variants with multi-modal Gaussian distribution outperform the vanilla model, especially with 5 Gaussian components. The curr_chunk discusses the performance of unsupervised GM-GAN models with different numbers of Gaussian components on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Best results were achieved with 5 Gaussian components."
}