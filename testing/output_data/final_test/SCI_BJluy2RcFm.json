{
    "title": "BJluy2RcFm",
    "content": "Janossy pooling is a method for representing permutation-invariant functions of sequences by averaging permutation-sensitive functions applied to all reorderings of the input sequence. To address computational complexity, three types of approximations are considered: canonical orderings, functions with k-order interactions, and stochastic optimization with random permutations. This framework unifies existing work and suggests modeling and algorithmic extensions, showing improved performance in experiments. Pooling is a key operation in deep learning architectures, merging related features into a single summary feature. It is essential in CNNs for constructing abstract features from linear activations in image neighborhoods. In neural networks for graphs, pooling combines embeddings of vertex neighbors to create new embeddings. The pooling operator must be invariant to input feature ordering, allowing for translation invariance in CNNs. Pooling is crucial in deep learning architectures, merging features into a single summary. In CNNs, it allows invariance to translations and rotations in images, while for graphs, it enables invariance to graph isomorphisms. Existing pooling methods like max-pool, min-pool, sum, or average are limited. Flexible and learnable pooling operators are needed for variable-size inputs, especially in graph applications where the number of neighbors can vary widely. Pooling is viewed as a permutation-invariant function on sequences of features in a space H. In deep learning architectures, pooling is essential for merging features into a single summary. Existing pooling methods like max-pool, min-pool, sum, or average are limited, especially for variable-size inputs in graph applications. The goal of the paper is to model and learn permutation-invariant functions on sequences of features in a high-dimensional Euclidean space. The paper aims to model and learn permutation-sensitive functions for constructing flexible and learnable permutation-invariant neural networks. It discusses DeepSets by Zaheer et al. (2017) and the composition of functions to achieve permutation invariance. The model involves observations in training data, embeddings, middle-layer functions, and a neural network mapping to the final output space. The neural network f learns embeddings for elements in H, with component embeddings added before a second neural network transformation \u03c1 is applied. The architecture can approximate any symmetric function on h-sequences, justifying the use of average pooling to make neural networks permutation-invariant. Zaheer et al. (2017) focused on functions of sets, extended to functions of multisets by Xu et al. (2019). The work extends functions of sets to functions of multisets, utilizing Janossy pooling to represent multiset functions. The embedding h is permuted in all possible ways, and the final permutation-invariant output is computed using two functions. The architecture can approximate any symmetric function on h-sequences but struggles to encode structural knowledge about y. The study introduces Janossy pooling, a learnable permutation-invariant pooling layer for variable-size inputs. It allows modeling higher-order dependencies in the input sequence h. The approach applies a permutation-sensitive function to every permutation of h, aiming to capture useful intermediate representations towards the final output. Janossy pooling introduces a learnable function f to every permutation of the input sequence h, which is then fed to a second function \u03c1. This framework aims to make computations tractable and unify existing methods in the literature. DeepSets is identified as a special case of Janossy pooling where the function f depends only on the first element of the sequence h. Janossy pooling introduces a learnable function f to every permutation of the input sequence h, allowing for flexibility and tractability. Functions that depend on the first k arguments of h enable the Janossy pooling layer to capture up to k-ary dependencies in h. Permutation-sampling learning algorithm \u03c0-SGD can be used to learn permutation-invariant neural networks by modeling permuted sequences using sequence or vector models. This approach is theoretically justified and explains the success of permutation sampling and LSTMs in relational models. The authors establish a connection between DeepSets and infinite de Finetti exchangeability. They also link Janossy pooling to finite de Finetti exchangeability by formalizing the Janossy pooling function f, which can take variable-size sequences as input. The function f is implemented with a neural network and is used to define permutations of integers in a set. Janossy pooling is a method to construct permutation-invariant functions from powerful permutation-sensitive functions like neural networks. By passing the output through a second function, learnability can be improved, although theoretically it may not add representational power. Equation 4 represents one layer of Janossy pooling, enhancing learnability by capturing common aspects across terms. While intractable, Janossy pooling unifies existing methods and offers trade-offs between learnability and computational cost. Strategies are presented to mitigate this, allowing for effective solutions. A method to achieve permutation-invariance without summation in equation 3 is by ordering elements of h based on values and feeding the reordered sequence to f. This allows for the use of complex f models like RNNs to capture relationships in the canonical ordering of h without summing over all permutations of the input. Examples of this approach exist in the literature, such as BID30 ordering nodes in a graph based on a user-specified order. In the literature, BID30 orders nodes in a graph based on a user-specified ranking like betweenness centrality. However, BID29 shows that ordering by Personalized PageRank achieves lower classification accuracy than a random ordering. Instead of pre-defining a canonical order, one can learn it from data by searching over all permutations of the input vector h. In practice, discrete optimization in Janossy pooling involves using heuristics and ensemble methods to handle permutations. Structural constraints are imposed to simplify the sum over permutations, focusing on k-ary dependencies in the input sequence. A k-ary permutation-invariant Janossy function is defined as projecting the sequence to a length k sequence. The Janossy pooling method involves using heuristics and ensemble methods to handle permutations in discrete optimization. A k-ary permutation-invariant Janossy function is defined by projecting the sequence to a length k sequence, balancing computational savings and the capacity to model higher-order interactions. The value of k can be chosen as a hyperparameter based on a-priori beliefs or typical tuning strategies. The Janossy pooling method involves using heuristics and ensemble methods to handle permutations in discrete optimization by defining k-ary permutation-invariant functions. The computational savings obtained from k-ary Janossy pooling come at the cost of reduced model flexibility. Theorem 2.1 states that for any k, F k\u22121 is a proper subset of F k if the space H is not trivial. This implies that Janossy pooling with k-ary dependencies can express any Janossy pooling function with (k \u2212 1)-ary dependencies, but not vice versa. The DeepSets function in equation 1 pushes the modeling of k-ary relationships to \u03c1, as shown in Theorem 2.1. This implies that higher-order Janossy pooling operations cannot be expressed by f in equation 2. Tractable Janossy pooling involves sampling random permutations of the input h during training, offering computational savings and flexibility for more complex models like LSTMs and GRUs. The approach involves sampling random permutations of the input h during training, providing a tractable approximation for equation 3 and a theoretical framework for understanding and extending such methods. Increasing the number of sampled permutations reduces variance, with the exact algorithm obtained when all permutations are sampled. The analysis is in a supervised learning setting but easily extends to other scenarios. In a supervised learning setting, training data is given to minimize empirical loss. Computing the gradient for large inputs is challenging, leading to stochastic optimization using permutation sampling. The estimator is unbiased, but composition with nonlinear functions may introduce bias. The composition of a nonlinear function and nonlinear loss may introduce bias in the estimator. A stochastic approximation algorithm for gradient descent is proposed using stochastic gradient descent updates. The algorithm optimizes a modified objective and is sensitive to random input permutations. The algorithm introduces an upper bound to the original objective function L via Jensen's inequality. By minimizing this upper bound, it forms a tractable surrogate to the original Janossy objective, implicitly regularizing functions to be insensitive to permutations of training data. The function class used to model f must be rich enough to include permutation-invariant functions for the global minima of J to include those of L. The function \u03c1 plays a different role in the \u03c0-SGD formulation compared to k-ary Janossy. The function \u03c1 in the \u03c0-SGD formulation differs from k-ary Janossy pooling, as it is absorbed directly into f = \u03c1 \u2022 f. The convergence properties of the algorithm are discussed, showing almost sure convergence to the optimal \u03b8 under conditions similar to SGD. Variance reduction is achieved in optimizing equation 6 and equation 10. The \u03c0-SGD algorithm optimizes the Janossy pooling layer by using various approaches such as importance sampling, control variates, Rao-Blackwellization, and output regularization. This optimization leads to improved inference time calculations for the output y(i) of input x(i). At test time, the output y(i) of input x(i) is estimated by combining \u03c0-SGD and Janossy with k-ary dependencies. Sampling s \u223c Unif \u03a0 |h| and computing k = f (|h|, \u2193 k (h s ); \u03b8 (f ) ) can be used instead of summation in some cases. The k-ary Janossy pooling approach is exact inference of a simplified model, while \u03c0-SGD with k-ary dependencies is approximate inference. The GraphSAGE model can be seen as a \u03c0-SGD approximation of k-ary Janossy pooling. Tractable Janossy pooling approaches, including k-ary dependencies and sampling permutations for stochastic optimization, are empirically evaluated. Janossy pooling with k-ary dependencies simplifies tasks for neural networks by modeling higher-order dependencies during pooling. It is evaluated experimentally with different values of k in arithmetic and graph tasks, where permutation-invariance is crucial. The code for the results is available on GitHub. In this section, the code on GitHub is used to predict various permutation-invariant functions like sum, range, unique sum, unique count, and variance of sequences of integers. Tasks include predicting the sum of 5 integers, range of 5 integers, sum of unique elements in a sequence of 10 integers, count of unique elements, and variance of a sequence of 10 integers. The text discusses predicting various permutation-invariant functions like sum, range, unique sum, unique count, and variance of sequences of integers using Janossy pooling tractable approximations. The tasks include predicting the sum of 5 integers, range of 5 integers, sum of unique elements in a sequence of 10 integers, count of unique elements, and variance of a sequence of 10 integers. Different approaches are explored, including Janossy (k = 1) and Janossy k = 2, 3. Janossy pooling models are constructed with k-ary dependencies, where k=1 for DeepSets and k=2,3 for feedforward networks. The models have the same number of parameters by modifying the embedding dimension. Full k=|h| Janossy pooling uses LSTM or GRU with 50 or 80 hidden units, trained with \u03c0-SGD stochastic optimization. At test time, approximations are made using 1 or 20 sampled permutations. The feedforward network with one hidden layer uses tanh activations and 100 units. The implementation is based on DeepSets code by Zaheer et al. (2017). Performance of LSTM and GRU models was similar, with GRU slightly outperforming. LSTM results were moved to Table 3 for clarity. Each model was trained with 15 tasks. The results in Table 3 of the Supplementary Material show that models trained with \u03c0-SGD using LSTMs and GRUs achieve top performance or are comparable on all tasks. Adding complexity to \u03c1 can yield small but meaningful performance gains, especially in the variance task. Modeling full-dependencies can be advantageous even with approximate training using \u03c0-SGD. Janossy pooling with lower complexity performs better for a more complex \u03c1 (MLP) compared to Linear. When \u03c1 is an MLP, it takes more epochs for k \u2208 {2, 3} to find the best model. For k = 1 (DeepSets), a more complex \u03c1 (MLP) is required as the pooling increases the complexity of modeling high-order interactions. Janossy pooling in graph neural networks enables vertex classification by modeling high-order interactions. The GraphSAGE algorithm involves sampling vertex attributes and using permutation-invariant operations like mean and max. Janossy pooling with \u03c0-SGD and k-ary subsequences is repeated twice to generate embeddings. In graph neural networks, Janossy pooling with \u03c0-SGD and k-ary subsequences is explored for vertex classification. The impact of increasing k in dependencies and sampled permutations at inference time is investigated. The model is implemented using PyTorch code and experiments are conducted on Cora, Pubmed, and Protein-Protein Interaction datasets. The curr_chunk discusses experiments conducted on Cora, Pubmed, and Protein-Protein Interaction datasets. It includes details on citation networks, vertex representations, and classifying paper topics. The impact of increasing k-ary dependencies on accuracy is shown in Table 9 of the Supplementary Material. The experiments on Cora, Pubmed, and Protein-Protein Interaction datasets show that the choice of k values has little impact on Cora and Pubmed due to small neighborhood sizes, while in PPI, increasing k improves performance. Mean-pooling performs well, indicating an easy task and the benefits of using the entire neighborhood. Increasing the number of sampled permutations at test time improves accuracy in the PPI task, with significant gains seen when using seven sampled permutations compared to one. This method offers a cost-effective way to enhance inference performance under the Janossy pooling framework. In the context of improving accuracy in the PPI task, different approaches to approximating the intractable Janossy-pooling layer are discussed, including Canonical orderings, k-ary dependencies, and permutation sampling. The method proposed by BID36 computes the posterior distribution of all permutations but is intractable for large inputs. Janossy pooling is noted to be trivially applicable to permutation-invariant outputs. Janossy pooling is applicable to permutation-invariant outputs. Vinyals et al. (2016) suggests using ancestral sampling for model learning. DeepSets by Zaheer et al. (2017) is a unary Janossy pooling approach. BID8 adds inductive biases to DeepSets using Deep Lattice Networks. Higher-order pooling can be used to extend BID8. Janossy pooling provides a general framework for capturing dependencies within a permutation-invariant pooling layer. Permutation sampling can be used as a stochastic gradient procedure to learn a model with a Janossy pooling layer, providing an approximate solution to original permutation-invariant functions. Randomly permuting sequences and feeding them forward to an LSTM has been found effective in relational learning tasks requiring permutation-invariant pooling layers. LSTM is effective in relational learning tasks that require permutation-invariant pooling layers. Our work has a strong connection with finite exchangeability, dropping the projectivity requirement for generative models. de Finetti's theorem states that joint distribution can be represented as a mixture distribution over conditionally independent random variables, while finitely exchangeable sequences involve a mixture over dependent random variables. Janossy pooling with k=1 yields a log-likelihood of conditionally iid random variables, contrasting with higher-order Janossy pooling which exploits dependencies among random variables. This framework is inspired by Janossy densities modeling finite exchangeable distributions. The Janossy pooling method is inspired by Janossy densities, which model finite exchangeable distributions as mixtures of non-exchangeable distributions applied to permutations. It also explores connections between permutation-invariant deterministic functions and exchangeability in probability distributions, including permutation equivariance. Weight-sharing schemes are proposed for maintaining general neural network equivariances. The paper discusses general neural network equivariances characterized as automorphisms of a colored multi-edged bipartite graph. It proposes a matrix completion model invariant to permutations of rows or columns and studies other invariances through a probabilistic perspective. The approach of permutation-invariance through Janossy pooling unifies existing approaches and focuses on k-ary interactions and random permutations in neural networks. Placing restrictions on additional neural networks can help control the trade-off between model capacity and tractability. The second approach involves a random permutation method that modifies the relationship between tractable approximate loss J and original Janossy loss L, showing strong empirical performance. Future work is needed to determine the best applications for this method and understand its convergence criteria. Understanding the relationship between loss-functions L and J can provide insight into the procedure's black-box nature and how random permutation optimization compares to canonical ordering. The random permutation optimization method can improve canonical ordering. Applying the methodology to more challenging tasks involving graphs and non-Poisson point processes is important. Theorem 2.1 states that Janossy pooling with k-ary dependencies can express any function with (k-1)-ary dependencies. The Janossy function f k belongs to F k and is associated with f k\u22121. The existence of f k is demonstrated by considering permutations of sequences. The equality of f k and f k\u22121 is only possible if H is a singleton, which is precluded in the assumptions. Proposition 2.2 states that \u03c0-SGD convergence is similar to SGD convergence, with almost sure convergence to the optimal \u03b8. This is a familiar application of stochastic approximation algorithms used in training neural networks. The algorithm in equation 9 converges to \u03b8 with probability one, as shown by the supermartingale convergence theorem. The accuracy scores for all models are included in the analysis. The accuracy scores for all models, including those using LSTM, are shown in TAB0. Mean absolute error is reported instead of accuracy to evaluate differences. Results show a drop in error as k increases and with more permutations at test-time. Using an RNN for f and training with \u03c0-SGD is beneficial for exploiting sequence dependencies. Sampling more permutations at test time also reduces variance. In the implementation section, k-ary models were constructed to have the same number of parameters regardless of k. Results show a modest improvement in k-ary models, with the embedding dimension reduced from 100 to 33. Performance on graph tasks is plotted against the number of inference-time permutations. The accuracy scores for models using LSTM are shown in a table. Sorting the sequence beforehand reduces the number of combinations needed to sum over for k-ary models with k \u2208 {2, 3}. In the implementation section, k-ary models were constructed to have the same number of parameters regardless of k. The embedding dimension was reduced from 100 to 33 for a modest improvement in k-ary models. Different activation functions were used for the layers in the models, with specific neuron counts for each type of model. The number of parameters was adjusted to be consistent for different values of k. In the implementation section, k-ary models were constructed with consistent numbers of parameters. Optimization was done using Adam with a tuned learning rate. Training was performed on GeForce GTX 1080 Ti GPUs. The datasets used for graph-based tasks are summarized in Table 9. Implementation was in PyTorch using Python 2.7, with a custom LSTM aggregator implemented. Each vertex in the forward pass was associated with a p-dimensional vertex attribute. In the implementation section, k-ary models were constructed with consistent numbers of parameters using Adam optimization. Training was done on GeForce GTX 1080 Ti GPUs. Each vertex is associated with a p-dimensional vertex attribute. For every vertex, k 1 neighbors are sampled, and their features are fed through an LSTM to generate representation vectors for the vertex and its neighbors. This process is repeated for a second convolution layer. The implementation section describes constructing k-ary models with consistent parameters using Adam optimization on GeForce GTX 1080 Ti GPUs. Each vertex has a p-dimensional attribute, and k 1 neighbors are sampled for LSTM processing. A second convolution layer is applied with distinct learnable weights for fully connected and LSTM layers, sampling k 2 vertices from each neighborhood. ReLU activation and embedding normalization may be applied after each convolution. The final fully connected layer produces a score, followed by softmax or sigmoid activation based on the dataset. The loss function is cross entropy for Cora and Pubmed, and binary cross entropy for PPI."
}