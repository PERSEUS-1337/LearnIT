{
    "title": "S1lFD4HnnE",
    "content": "Neural networks are vulnerable to small adversarial perturbations, and adversarial robustness is sensitive to the input data distribution. Semantics-preserving transformations on the input data can significantly affect the robustness of adversarially trained models. Input data distribution alone can impact the adversarial robustness of neural networks, regardless of the tasks themselves. The study focuses on the impact of input data distribution on adversarial robustness in neural networks, specifically looking at the sensitivity of adversarially trained models to changes in data distribution. The findings suggest that even small shifts in data distribution can lead to significant differences in robustness. The study highlights the sensitivity of adversarial robustness in neural networks to changes in data distribution, which can significantly impact model performance. This raises concerns about the reliability of performance estimations and the deployment of robust training algorithms in real-world applications. The robust accuracy of PGD trained models is sensitive to gamma values of gamma-corrected CIFAR10 images, indicating that different lighting conditions can affect image dataset robustness. This finding offers new insights into adversarial vulnerability, complementing previous research on data distribution influences on model performance. Different data distributions can have drastically different properties of adversarially robust generalization, as shown theoretically on Bernoulli vs mixtures of Gaussians and empirically on standard benchmark datasets. Gradual semantics-preserving transformations of data distribution can also cause large changes to datasets' achievable robustness. New datasets with different input data distributions but fixed true classification reveal unexpected phenomena. Our experiments show that adversarial training, a popular method for robust models, loses stability in robust accuracy under \"semantic-lossless\" shifts. Unlike preprocessing or transfer learning, we treat shifted data distribution as a new underlying distribution. MNIST has a binary pixel distribution, while CIFAR10 has a continuous spectrum. By applying \"smoothing\" to MNIST and \"saturation\" to CIFAR10, we create datasets with different properties. The text discusses the use of smoothing and saturation techniques on MNIST and CIFAR10 datasets to manipulate data distributions. This manipulation affects the robust accuracies of neural networks trained on these datasets. The goal is to maintain semantic information while creating more binary datasets. The text explores the difficulty of achieving robustness in neural networks trained on MNIST and CIFAR10 datasets using adversarial training. Different models and attack settings are used to measure robust accuracies against clean and adversarially perturbed data. Results on MNIST and CIFAR10 variants show that standard training maintains stable clean accuracy, indicating similar task difficulties. However, PGD adversarial training results in a slight drop in clean accuracy and significant drops in robust accuracy and robustness. Binarized MNIST with adversarial training shows almost identical clean and robust accuracy, suggesting high robust accuracy does not conflict with high clean accuracy. CIFAR10 results follow a similar trend, with clean accuracy maintaining original levels until saturation. After PGD training, the robust accuracy significantly increases from 43.2% to 79.7% before level 16, while clean test accuracy drops from 85.4% to 80.0%. Beyond level 16, clean and robust accuracy are almost the same, but robustness in predictions continues to increase, indicating instability. For saturation levels smaller than 2, PGD training results in worse robust accuracy, e.g. at level 1, robust accuracy is 33.0%. Standard training accuracies drop significantly after saturation level 64 due to high saturation levels. The high degree of saturation in CIFAR10 models causes information loss, leading to a gap between robust accuracy and robustness in predictions. In contrast, MNIST variants show that drops in robust accuracy are due to adversarial vulnerability. Robust accuracy under PGD training is more sensitive to input data distribution differences than clean accuracy under standard training. A semantically-lossless shift in data transformation can greatly affect robust accuracy, raising concerns in practice. The text discusses concerns about the impact of different lighting conditions, cameras, and preprocessing on image datasets, leading to variations in performance measures during adversarial training. It demonstrates this phenomenon using CIFAR10 images with different gamma mappings. Gamma mapping adjusts image exposure and can significantly affect performance. The text demonstrates how different gamma values affect image brightness and performance in adversarial training. While clean accuracies remain consistent, robust accuracy varies significantly. This highlights the importance of interpreting robustness benchmarks across different image datasets with varying exposures. The algorithm's robustness varies across different datasets, raising questions about reliable evaluation methods for image classifier robustness. Saturation affects pixel values, shrinking the perturbation space and changing the volume of \"perturbable region\" significantly. The study compared perturbable volumes of original and saturated CIFAR10 images to analyze robustness sensitivity. Results showed that differences in perturbable volume did not significantly impact robustness on MNIST and CIFAR10 variants. The study analyzed the impact of perturbable volume on robustness sensitivity of CIFAR10 images. Results showed that differences in perturbable volume did not significantly affect robustness on MNIST and CIFAR10 variants. The controlled perturbable volume across all cases was defined as the -\u221e ball, with saturation pushing data points towards domain boundaries, increasing inter-class distances which positively correlated with robust accuracy. The study analyzed the impact of inter-class distance on dataset robustness. Scaled variants of MNIST were compared, showing that despite similar inter-class distances, smoothed MNIST was less robust than scaled binarized MNIST and original MNIST. This complexity indicates that inter-class distance alone cannot fully characterize dataset robustness. The LeNet5 model with a widen factor of 1 consists of 32-channel conv filter + ReLU + size 2 max pooling + 64-channel conv filter + ReLU + size 2 max pooling + fc layer with 1024 units + ReLU + fc layer with 10 output classes. MNIST images are not preprocessed before input. Adam optimizer with initial learning rate of 0.0001 is used for training for 100000 steps with batch size 50. WideResNet-28-4 is used for experiments on CIFAR10 with \"per image standardization\" preprocessing. Stochastic gradient descent with momentum 0.9 and weight decay 0.0002 is used for training with batch size 128 for 80000 steps. Learning rate is adjusted at different steps. Manual hyperparameter search was conducted with no observed improvements. The study conducted manual hyperparameter search but did not find any improvements. The experiments used specific settings unless stated otherwise. The research explored the relationship between data distribution proximity to boundaries and robustness. In high dimensional space, the volume of perturbation space varies greatly based on the location of data points. Different datasets show significantly different perturbable volumes, with the original CI-FAR10 dataset having a log volume of -12354. The perturbation space is smaller than the full ball, with differences in volume based on saturation levels. CIFAR10 images may appear similar to humans but have large differences in perturbable volumes. PGD attacks do not reduce accuracy when allowing perturbation outside the data domain boundary. The perturbable volume hypothesis is rejected as perturbation outside the domain boundary may make attacks less effective. Inter-class distance is calculated using different distances instead of the \u221e distance for robustness measurement. The inter-class distances are calculated using different distances instead of the \u221e distance for robustness measurement. Binarized MNIST has a larger inter-class distance compared to CIFAR10 variants. Inter-class distance shows a positive correlation with robust accuracy under transformations, except for original MNIST which is slightly more robust than smooth-2 MNIST. The inter-class distance is slightly more robust than smooth-2 MNIST, suggesting it cannot fully explain the robust variation across different dataset variants."
}