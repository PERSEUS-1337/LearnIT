{
    "title": "HJgzt2VKPB",
    "content": "Computer vision has seen significant advancements in static image analysis through deep features trained on large-scale datasets. However, video understanding has not progressed as much. New datasets and models have been introduced, but frame-by-frame classification methods still perform well. A new dataset called CATER has been created to address biases in existing video datasets and requires spatiotemporal understanding for solving. It tests the ability to recognize compositions of object movements that involve long-term reasoning. CATER is a challenging dataset for analyzing modern spatiotemporal video architectures. While deep features have improved static image analysis, video descriptors struggle to outperform hand-crafted descriptors. Recent works have shown some improvements by merging 2D and 3D models, but simpler 2D models still perform well in video benchmarks. This raises the question of whether videos can be easily understood by averaging predictions over frames. Reasoning about high-level cognitive concepts such as intentions, goals, and causal relations in videos requires reasoning over long-term temporal structure and order. For example, inferring that an actor is carrying a gun in a movie clip involves reasoning over space and time, not just a single frame. Similarly, in games like cup-and-balls magic or the shell game, one needs to reason over time to understand the deception involved. The task involves an operator hiding a ball under one of multiple cups, requiring the observer to identify which cup covers the ball. This task necessitates the ability to model the world over time, understand occlusion and containment, and anticipate adversarial manipulation. Frame-by-frame prediction models are inadequate for such complex tasks. The protagonist leaves the table, retrieves a hidden firearm from the bathroom, and returns with the intention of shooting someone. The difficulty in determining the location of a ball in a cup-and-ball game highlights the challenge of designing complex tasks for computers in video understanding. The limitations of existing video benchmarks hinder the performance of spatiotemporal models compared to static counterparts. In this work, a new approach to developing a video understanding dataset is taken, inspired by the CLEVR dataset. Previous studies have shown that adding ground truth object information significantly boosts action recognition performance. Inspired by the CLEVR dataset, a new diagnostic dataset called CATER is introduced for Compositional Actions and TEmporal Reasoning in dynamic tabletop scenes. The dataset includes tasks such as primitive action recognition, compositional action recognition, and adversarial target tracking under occlusion and containment. Full metadata with rendered videos is provided for more complex prediction tasks like detection, tracking, and forecasting. The CATER dataset allows for detailed model diagnostics and benchmarking of video understanding models. It reveals that even state-of-the-art models struggle with the dataset, providing insights into model behavior by changing parameters such as occlusion duration and camera motion. The evolution of video understanding for action recognition from hand-designed models to spatiotemporal deep networks is also discussed. The evolution of video understanding for action recognition has progressed from hand-designed models to sophisticated spatiotemporal deep networks. While advancements have been made in image domain datasets, video models still struggle to outperform previous hand-crafted descriptors. Models capable of temporal modeling, such as RNNs and 3D convolutions, have shown promise but have not yet surpassed traditional methods. Recent advancements in video action understanding have shown that simpler per-frame prediction models, like variants of two-stream architectures, have performed as well as more complex temporal modeling approaches using RNNs and 3D convolutions. Despite efforts to merge image and video models, 2D models have remained top performers in the Kinetics Challenge. Significant work has been done to collect video action datasets, including using human actors in controlled environments or online crowd sourcing. Recent advancements in video action understanding have shown that simpler per-frame prediction models have performed as well as more complex temporal modeling approaches. Efforts have been made to collect video action datasets through human actors in controlled environments or online crowd sourcing. Various popular video benchmarks like UCF-101, HMDB-51, Kinetics, and VLOGs struggle with biases favoring image-based baselines. The need for diverse benchmarks is emphasized to enable comprehensive diagnostics and validation in video understanding. CATER fills a missing gap in the benchmark landscape. CATER fills a gap in video action understanding benchmarks by providing size, label distribution, and resilience to bias. The work involves using synthetic data for computer vision applications, aligning with previous research in this area. Our work aims to develop a benchmark for video-based action understanding, building upon previous benchmarks for scene understanding. We extend the CLEVR benchmark for spatiotemporal reasoning in videos with tasks that require such reasoning to be solved. The CATER dataset and tasks involve identifying active actions, compositional actions, and quantized spatial localization of objects in videos. Task 3 in the CATER dataset involves quantized spatial localization of objects in videos, requiring spatiotemporal understanding due to occlusions. Object tracking, including adversarial tracking, is used for long-term video understanding and tasks like detecting humans carrying or exchanging objects. State-of-the-art deep trackers are included in the benchmark. The CATER dataset includes state-of-the-art deep trackers for benchmark evaluation, offering videos that require long term temporal reasoning. It provides diagnostic tools for evaluating video models in various scenarios and allows control over dataset parameters through synthetic rendering. The dataset is free of object or scene bias, using simple objects for video rendering tasks like object localization and spatiotemporal action composition. Sample videos and tasks are described in the dataset. The CATER dataset includes sample videos showcasing object shapes, sizes, colors, and materials. It builds upon CLEVR, introducing new object shapes like inverted cones and a 'snitch'. Four atomic actions are defined: 'rotate', 'pick-place', 'slide', and 'contain', each afforded by specific objects. The CATER dataset introduces new object shapes like inverted cones and a 'snitch', with four atomic actions: 'rotate', 'pick-place', 'slide', and 'contain'. The 'contain' action is special, only afforded by cones, allowing for recursive containment. Allen's temporal algebra defines temporal relations between intervals. The CATER dataset introduces new object shapes like inverted cones and a 'snitch', with four atomic actions: 'rotate', 'pick-place', 'slide', and 'contain'. Allen's algebra defines relations between intervals, which are grouped into three broad classes for composite actions. Videos are rendered with random objects on a 2D plane, including a snitch and a cone, with actions split into 30-frame slots. The CATER dataset introduces new object shapes like inverted cones and a 'snitch', with four atomic actions: 'rotate', 'pick-place', 'slide', and 'contain'. Videos are split into 30-frame slots, and actions are added by iterating through objects without colliding. Additional videos with camera motion are rendered, with the camera moving randomly between predefined 3D coordinates every 30 frames. The CATER dataset introduces new object shapes and atomic actions like 'rotate', 'pick-place', 'slide', and 'contain'. Camera motion videos are rendered by moving the camera to different 3D coordinates every 30 frames, with constraints to avoid jarring viewpoint shifts. Animations are labeled with atomic actions and their compositions, including temporal relationships unique to CATER. The CATER dataset introduces new object shapes and atomic actions like 'rotate', 'pick-place', 'slide', and 'contain'. Camera motion videos are rendered by moving the camera to different 3D coordinates every 30 frames, with constraints to avoid jarring viewpoint shifts. Animations are labeled with atomic actions and their compositions, including temporal relationships unique to CATER. Actions in the dataset occupy a well-defined temporal extent, requiring temporal logic that reasons about relations between intervals using Allen's interval algebra with thirteen basic relations. This allows for the exploration of fine-grained temporal relationships in video understanding tasks, such as recognizing spatiotemporal compositions of atomic actions like human body movements described as exercises or dances. The CATER dataset introduces new object shapes and atomic actions like 'rotate', 'pick-place', 'slide', and 'contain'. It involves recognizing spatiotemporal compositions of atomic human body movements as exercises or dance routines. Three tasks on CATER test for higher-level reasoning abilities through single or multi-label classification setups. The tasks aim to understand the effects of actions on the world state by rendering 5500 total videos for comparison with existing benchmarks. Task 1 on CATER involves atomic action recognition with 14 classes defined for object shapes and actions. Each video is treated as a multi-label classification problem. The dataset is split into training and test sets in a 70:30 ratio. Task 2 involves compositional action recognition by combining basic actions from Task 1 into pairs of atomic actions to recognize complex actions. This task focuses on recognizing actions as a whole or in parts through spatiotemporal composition. Task 2 involves compositional action recognition by combining basic actions from Task 1 into pairs of atomic actions to recognize complex actions. The compositions are limited to pairs of 14 atomic actions with temporal relations categorized as 'before', 'during', and 'after'. This results in 301 classes after removing duplicates. Multiple compositions can be active in a video, making it a multi-label classification problem evaluated using mAP. Task 3, Snitch localization, tests models' ability to recognize the effect of actions on the environment, similar to understanding the location of objects after certain activities. Task 3, Snitch localization, involves predicting the location of a special object called the Snitch in a video. The object can be contained by other objects, requiring long-range reasoning to determine its final location. The problem is simplified as a classification task by dividing the grid into cells and identifying which cell the Snitch is in at the end of the video. The task is evaluated using standard accuracy metrics. The task of Snitch localization involves predicting the location of a special object in a video by dividing the grid into cells. Evaluation includes standard accuracy metrics and mean L1 distance from ground truth. Experimentation with CATER using state-of-the-art video understanding models like I3D and R3D is conducted. The ResNet model is adapted for video analysis, incorporating 3D spatiotemporal features. Non-local networks enhance this by adding a spatiotemporal interaction layer, outperforming other architectures on benchmarks. The Temporal Segment Networks approach, using RGB and flow modalities, is also explored for snitch localization. These models learn from individual frames or clips and aggregate predictions at test time. Simple averaging is effective on recent datasets but may lose temporal information. Incorporating LSTM for temporal modeling in video analysis, a 2-layer LSTM with 512 hidden units is trained on subclips from train and test videos. The LSTM produces an output at each clip and enforces a classification loss at the end. The aggregated prediction is taken from the last clip at test time, with performance averaged over three runs to control for random variation. LSTMs show improved performance on CATER for action recognition in videos compared to simple average pooling. Additionally, a state-of-the-art visual tracking method is experimented with for task 3, providing a symbolic reasoning baseline for the dataset. In the context of action recognition in videos, various video models have been proposed in literature, including 2.5D convolutions, VLAD-style aggregation, and other multi-modal architectures. The focus is on popular and best performing models, with a random baseline provided for comparison. Implementation details for all baselines are available in the supplementary materials. Task 1 specifically examines atomic action recognition using R3D with and without non-local blocks, showing performance differences based on the number of frames in the clips. In Task 2, compositional action recognition is explored, predicting confidence over 301 classes. Existing models struggle with this task as they need to reason about spatiotemporal compositions. Non-local blocks improve performance in this task, unlike in Task 1. Task 3 focuses on snitch localization, using softmax cross entropy loss for training and classification accuracy for evaluation. Long term reasoning in Table 3 shows that LSTM helps with temporal modeling on CATER, unlike local temporal cues. Tracking uses a pre-trained model without additional training. Task 3 focuses on snitch localization using LSTM for temporal reasoning on CATER. TSN and flow modalities are experimented with, showing lower performance compared to R3D. Longer clips with higher sample rates yield higher performance due to the need for long term temporal reasoning. The use of LSTM for aggregation improves performance in most models, but tracking approaches only solve a third of videos due to occlusions and drift. Performance is analyzed with different grid granularities in Table 4, showing tracking as a strong baseline. Comparison of model performance on existing benchmarks and CATER is done in Table 3. The dataset generation process allows for diagnostics not possible with previous datasets, using the R3D+NL model for visualizations. In Figure 4, the aggregate performance of the model is analyzed over multiple bins. It is observed that performance drops if the snitch keeps moving until the end, with LSTM outperforming avg-pooling in handling snitch motion. The tracker is less affected by snitch movement, indicating the effectiveness of classic computational pipelines for spatiotemporal understanding. Additionally, performance drops if the snitch is contained in the end, making it harder to spot and track. Diagnostic analysis of localization performance is conducted by binning the test set based on certain parameters. Localization performance is analyzed by binning the test set based on parameters. The performance drops if the snitch keeps moving until the end, is contained by another object, or has increasing displacement from its start position. The tracker is less affected by these factors compared to the models. The number of objects in the scene has a relatively stable effect on performance. Videos that the models predict correctly are visualized next. The easiest videos for the avg-pooled model have little snitch motion, while the LSTM-aggregated model performs better with snitch motion early in the video. The hardest videos have sudden snitch motion towards the end, as shown by the bright golden trail. Most models struggle on the proposed dataset, as analyzed using CATER. Most models struggle on the proposed dataset CATER, especially on the snitch localization task requiring long term reasoning. Average pooling clip predictions or short temporal cues perform poorly on CATER, unlike previous benchmarks. Solving temporal reasoning challenges like those in CATER is crucial for advancements in machine video understanding. Our fully-annotated dataset can be used for spacetime action localization tasks. While high-level semantic tasks like activity recognition can be addressed with current architectures and richly labeled datasets, \"mid-level\" tasks such as tracking still pose challenges, especially under long-term occlusions. Addressing these challenges will enable broader temporal reasoning tasks capturing intentions, goals, and causal behavior. The analysis includes the top confident correct and incorrect predictions on test videos for localization tasks, showing the ground truth positions of the snitch over time and softmax prediction confidence scores for each location. The model uses ResNet-3D and non-local blocks for experiments on location classification. Different architectures are trained with specific loss functions for multi-label classification tasks. Test videos are split into temporal and spatial clips for aggregation using average pooling. The model uses ResNet-3D and non-local blocks for location classification experiments, splitting test videos into temporal and spatial clips. Aggregating predictions over 250 frames at test time, either by averaging or using LSTM, proved beneficial. ImageNet initialization was found to optimize networks more easily, consistent with prior research. The model utilizes ResNet-3D and non-local blocks for location classification experiments, benefiting from ImageNet initialization. Data distribution over classes is shown in Figure 6, with visualization of sample videos and tracking results in supplementary video 3. The hardest videos for task 3 involve sudden motion towards the end, making the task more challenging. The model visualizes tracking results for locating the snitch, showing that it struggles with occlusion and complex operations. The model automatically focuses on the snitch towards the end of clips, indicating its importance for localization tasks. Histograms of training and validation data distribution for different tasks on the dataset: (a) recognizing atomic actions, (b) recognizing spatiotemporal compositions of actions, and (c) snitch localization task in a 6x6 grid."
}