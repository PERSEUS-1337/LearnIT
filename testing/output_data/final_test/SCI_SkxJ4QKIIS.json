{
    "title": "SkxJ4QKIIS",
    "content": "Learning in recurrent neural networks (RNNs) is often done through gradient descent using backpropagation through time (BPTT), which does not accurately model how the brain learns. Experimental results on synaptic plasticity suggest three-factor learning rules involving eligibility traces of local neural activity and a third factor. Eligibility propagation (e-prop) is a new factorization of loss gradients in RNNs that aligns with these rules for biophysical spiking neuron models. It performs competitively with BPTT on the TIMIT speech recognition benchmark for training artificial LSTM networks and spiking RNNs. Analysis indicates that diverse learning signals and consideration of slow internal neural dynamics are crucial for e-prop's learning efficiency. The brain appears capable of tasks like counting, memorizing, and reasoning, which e-prop may help replicate. The brain can perform tasks like counting, memorizing, and reasoning efficiently due to its temporal processing capabilities. Recurrent neural networks (RNNs) are commonly used to model this, but their training algorithm, backpropagation through time (BPTT), does not align with how the brain learns. Synaptic plasticity is influenced by spike-timing dependent plasticity (STDP) and a third factor, possibly a neuromodulator, which can modulate plasticity even with a delay. This suggests the presence of local mechanisms that retain traces of recent neural activity. Researchers have simulated how interesting learnt behaviors can emerge from three factor learning rules, with the third factor being a global signal emitted when a reward is received. In feed-forward networks, backpropagation-inspired learning algorithms outperform rules based on a global third factor, suggesting that backpropagation provides important details not captured by all three factor learning rules. The aim is to develop a learning algorithm for RNNs that is general and efficient like BPTT but remains plausible. The researchers aim to develop a learning algorithm for RNNs that is efficient like BPTT but remains plausible. They propose eligibility propagation (e-prop) as an alternative to BPTT, which relies on a diversity of learning signals and a few eligibility traces per synapse. E-prop, when applied to LSTM networks and LSNNs, shows promising learning efficiency and compatibility with experimental data. Neural Networks (LSNNs) are competitive with BPTT on the TIMIT speech recognition benchmark and can solve temporal credit assignment problems with long delays. Real-time recurrent learning (RTRL) computes loss gradients like BPTT but requires more operations. Recent works suggest eligibility traces can approximate RTRL under certain conditions. The network dynamics are approximated with a trained estimator, but algorithms derived for specific neuron models without long-short term memory make it challenging to tackle RNN benchmark tasks. Other mathematical methods have suggested approximations to RTRL compatible with complex neuron models, but they lead to gradient estimates with high variance or require heavier computations for large networks. E-prop solves this issue by reducing computational and memory costs, as it computes the same loss gradients but only propagates forward in time the terms that can be computed locally. This provides a new interpretation of eligibility traces. E-prop provides a new interpretation of eligibility traces for a broad class of RNNs, including LSTMs and LSNNs. It simplifies the learning signal while approaching the performance of BPTT. More complex strategies can be combined with e-prop for even more powerful online algorithms. E-prop computes gradients for network weights based on learning signals specific to neurons, allowing loss-independent eligibility traces to be defined for any RNN model. It can be implemented online without the need for backpropagation through time or storing past neural activity, addressing a major plausibility issue in learning algorithms. E-prop addresses plausibility issues in learning algorithms by computing gradients for network weights based on neuron-specific learning signals. This approach eliminates the need for backpropagation through time or storing past neural activity. To avoid weight sharing between feedback and feedforward pathways, fixed random values can replace feedback weights in LIF neurons, leading to symmetric and random e-prop variants. This method was applied to a recurrent network of spiking neurons, using leaky-integrate and fire (LIF) neurons to model membrane voltage dynamics and spike generation. The hidden state h t j represents the membrane voltage and spikes are modeled by a binary observable state z t j P t0, 1u. Non-differentiability of spiking neurons' binary output is addressed using a pseudo-derivative. The eligibility trace e t ji for a LIF neuron is influenced by pre- and post-synaptic activity, resembling STDP. Substituting e t ji with a voltage-dependent STDP does not significantly affect e-prop performance. Introducing firing rate adaptation enhances the spiking network model's working memory capabilities. In a network model, slower neural dynamics are modeled with firing rate adaptation in ALIF neurons. LSNN is a recurrent network of ALIF and LIF neurons. ALIF neurons have eligibility vectors with a slow component that decays much slower than non-adaptive LIF neurons. E-prop and LSNNs were tested on a task involving working memory in rodents, requiring memorization over a delay of hundreds of milliseconds. LSNNs with 100 neurons can be trained by e-prop to solve a task involving decision-making based on visual cues. Adaptive neurons in LSNNs have a slow component in the eligibility vector that allows them to hold information about relevant cues for extended periods. Non-adaptive LIF neurons cannot solve the same task, highlighting the importance of adaptation in neural networks. E-prop in LSNNs can hold information about relevant cues for extended periods, alleviating the need to propagate signals backwards in time. Comparing E-prop and BPTT on RNN benchmarks, E-prop performs similarly to BPTT with only a slight increase in error rate. E-prop is a novel learning algorithm that fits experimental data on synaptic plasticity and maintains competitive performance with BPTT. It can utilize neuron models with enhanced memory capabilities for solving temporal credit assignment problems efficiently. Additionally, e-prop has been shown to be applicable in reinforcement learning tasks. Dopaminergic neurons encode diverse information beyond global reward prediction errors, while performance monitoring neurons in cortices provide learning signals."
}