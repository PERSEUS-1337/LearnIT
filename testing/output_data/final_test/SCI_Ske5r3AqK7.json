{
    "title": "Ske5r3AqK7",
    "content": "In this paper, a novel approach is proposed to embed words in a Cartesian product of hyperbolic spaces based on the notion of delta-hyperbolicity. The method connects to Gaussian word embeddings and introduces a hypernymy score. The Glove algorithm is adapted to learn unsupervised word embeddings in Riemannian manifolds, and the analogy task is solved using Riemannian parallel transport. Extensive experiments show that these embeddings outperform popular baselines. Word embeddings are crucial for neural network models in natural language processing. Popular models like Glove, Word2Vec, and FastText learn word vectors from raw text corpora based on word co-occurrence statistics. These embeddings are used for word similarity and analogy tasks, with a linear algebraic structure that aids in solving word analogy. However, unsupervised word embeddings still face challenges. Unsupervised word embeddings struggle with revealing asymmetric word relations and hierarchical structures, limiting automatic text understanding. To address this, a proposal suggests using probability density functions like Gaussian distributions to encode generality/specificity of words. However, this method loses arithmetic properties of point embeddings. A new approach combines embedding words as points in hyperbolic spaces and mapping them bijectively to Gaussian embeddings. The text discusses mapping word embeddings to Gaussian embeddings in hyperbolic spaces to improve word similarity benchmarks and hierarchical structure understanding. This novel approach combines the arithmetic properties of point embeddings with the generality/specificity encoding of Gaussian distributions, outperforming traditional methods like Euclidean Glove. The text explains the use of hyperbolic geometry to embed words, emphasizing the notion of average \u03b4-hyperbolicity in graphs. Real-world graphs, being complex networks, are better embedded in a hyperbolic space due to their hierarchical structure. This approach is suitable for word embeddings as words exhibit an \"aristocratic\" community with a hierarchical structure. Recent supervised methods can embed tree or directed acyclic graphs in a low-dimensional space to improve link prediction by utilizing hyperbolic geometry or imposing a partial order in the embedding space. These methods leverage external information like WordNet along with raw text corpora to learn word embeddings with hierarchical information. Fully unsupervised models also aim for the same goal by moving away from the \"point\" assumption and learning various structures. Recent supervised methods utilize hyperbolic geometry to embed tree or directed acyclic graphs for link prediction. Fully unsupervised models aim to learn word embeddings with hierarchical information by moving away from the \"point\" assumption and leveraging various structures. In the hyperbolic space, models choose to embed words in the Poincar\u00e9 ball to improve hypernymy detection. The GLOVE BID31 algorithm is an unsupervised method for learning word representations in the Euclidean space from statistics of word co-occurrences in a text corpus. It aims to capture the meaning and relations of words geometrically. The model uses notations for word occurrences and probabilities, with embeddings for target and context words represented as w i and w k respectively. The GLOVE model aims to learn word embeddings in Euclidean space from word co-occurrence statistics. It uses embeddings for target and context words, with a weighted least-square loss optimization to enforce equality. The model does not have a clear correspondence to the Euclidean inner-product in hyperbolic space, but provides a distance function for comparison. BID36 proposed representing words as Gaussians with mean vectors and covariance matrices to enrich Euclidean word embeddings. BID30 suggested embedding words from the WordNet hierarchy. The Euclidean distance in GLOVE was replaced with a new loss function involving a hyperparameter h and a differentiable distance function d. The choice of h = cosh^2 was found to yield better results in some cases. The Fisher geometry of Gaussians is hyperbolic, with a direct correspondence between diagonal Gaussians and the product space. This connects the WORD2GAUSS algorithm to hyperbolic word embeddings. The Fisher geometry of Gaussians is hyperbolic, connecting WORD2GAUSS to hyperbolic word embeddings. Representing words in hyperbolic spaces allows for the use of Riemannian adaptive optimization tools like RADAGRAD, leading to improved results. The connection between the Fisher geometry of Gaussians and hyperbolic word embeddings allows for the use of RADAGRAD for optimization, leading to better results. This connection also enables analogy computations and hypernymy detection using Gaussian distributions. The Poincar\u00e9 ball allows for analogy computations and hypernymy detection using Gaussian distributions, with formulas described in closed-forms in the appendix for easy implementation. The curvature of the space causes differences in solutions, but continuously deforming the Poincar\u00e9 ball to the Euclidean space recovers Euclidean counterparts for analogy computations. The novel principled score introduced in section 5 can be applied on top of unsupervised learned Poincar\u00e9 Glove embeddings for hypernymy detection, predicting relations like is-a(dog, animal). Hyperbolic word embeddings are mapped to Gaussian embeddings, and a hypernymy score is defined. The method of BID30 uses an entailment score to predict is-a relationships based on the Euclidean norm encoding generality/specificity, but this choice is not intrinsic to the hyperbolic space. The training process in hyperbolic space is intrinsic to the distance function, but the \"is-a\" score depends on parametrization. An isometry must be determined before using trained word embeddings non-intrinsically. Mapping hyperbolic embeddings to Gaussian embeddings reveals that generality is encoded in the variance. The space of Gaussians with Fisher distance maps naturally to the hyperbolic upper half-plane. In hyperbolic space, isometries are crucial for mapping word embeddings to Gaussian distributions. By aligning specific and generic words through re-centering and rotation operations, the transformation to Gaussian N(\u00b5, \u03c3^2) is achieved. This process involves finding the correct isometry and mapping geodesics in D2 for generality encoding. In hyperbolic space, isometries are essential for mapping word embeddings to Gaussian distributions. The process involves aligning words through re-centering, rotation, and isometric mapping to H2. The isometry is obtained using the weakly-supervised method WordNet 400 + 400. Two methods are proposed to identify sets G and S: Unsupervised 5K+5K and defining G as the 5k most frequent words and S as the 5k least frequent words. In hyperbolic space, isometries are crucial for mapping word embeddings to Gaussian distributions. The weakly-supervised method WordNet 400 + 400 is used to obtain the isometry. Sets G and S are identified using two methods: Unsupervised 5K+5K, where G is the 5k most frequent words and S is the 5k least frequent words, and Weakly-supervised WN x+x, which defines G and S based on the WordNet hierarchy levels. Gaussian embeddings are proposed by BID36 using KL divergence for distributions P and Q, aiming to encode Q easily as P. However, the statement is questioned due to the non-monotonic nature of KL divergence in the context of Gaussian distributions. The variance magnitude in n-dimensional Gaussian distributions is crucial for determining generality/specificity. A simple score is proposed for entailment between Gaussian embeddings, which is invariant to covariance matrix rescaling. Mapping word embeddings to Gaussians using isometries is essential, with Gaussian embeddings aiming to encode distributions easily. The proposed is-a score algorithm is illustrated in Algorithm 1, which calculates the hypernymy score using Poincar\u00e9 embeddings. The algorithm converts half-plane coordinates to Gaussian parameters and embeds words in a hyperbolic space to determine the best geometry for representing symbolic data. This selection of the right metric space is crucial for understanding inductive bias. The \u03b4-hyperbolicity metric quantifies tree-like structure in metric spaces. The averaged \u03b4-hyperbolicity, \u03b4 avg, indicates hyperbolic geometry suitability. The ratio 2 * \u03b4 avg /d avg characterizes \"hyperbolicness.\" The methods are trained on a weighted graph of co-occurrences. The \u03b4-hyperbolicity metric quantifies tree-like structure in metric spaces, indicating hyperbolic geometry suitability. The symbolic data's low hyperbolicity suggests a preference for hyperbolic space over Euclidean space for word embedding. To calculate distances between words, a new distance metric is proposed based on similarity measures. The discrete metric spaces derived from co-occurrences exhibit very low hyperbolicity, implying a high degree of \"hyperbolicness.\" The study reports empirical results on word embeddings in hyperbolic spaces. Models were trained on a corpus from English Wikipedia with 1.4 billion tokens. The co-occurrence matrix had 700 million non-zero entries. Models were trained for 50 epochs on a vocabulary of 189,533 word types, with some experiments on the 50,000 most frequent words. The study focused on word embeddings in hyperbolic spaces, training models on a restricted vocabulary of the 50,000 most frequent words. Different models were used, including Poincar\u00e9 and Euclidean baselines, with varying dimensions and optimization techniques. Results were compared against a Euclidean GloVe model on tasks of similarity and analogy. The study compared different models for word embeddings in hyperbolic spaces, using Poincar\u00e9 models with various learning rates and an \"initialization trick\" for improved performance. Results showed that the Poincar\u00e9 models outperformed the Euclidean GloVe model in word similarity and analogy tasks. The study compared different models for word embeddings in hyperbolic spaces, showing that Poincar\u00e9 models outperformed Euclidean GloVe in word similarity and analogy tasks. The models selected the best solution for word analogy using a 2-fold cross-validation method, with hyperbolic embeddings performing the best. The study compared word embeddings in hyperbolic spaces, with 100D hyperbolic embeddings being the best. Different methods were classified based on supervision used for word embedding learning and hypernymy score. Results for Hyperlex and WBLess were reported, along with selected words and their hypernymy scores. Our fully unsupervised 50x2D model sets a new state-of-the-art on unsupervised WBLESS accuracy and matches the previous state-of-the-art on unsupervised HyperLex Spearman correlation. Weak supervision leads to significant improvements in hypernymy score, achieved through post-processing steps after training word embeddings. The unsupervised learned embeddings outperform most supervised embedding learning baselines on HyperLex. The Gaussian variances of hyperbolic embeddings correlate with WordNet levels. A small amount of supervision can boost performance on hypernymy detection. Model 50x2D with the initialization trick achieves state-of-the-art results on hypernymy detection. The model h(x) = x^2 achieves state-of-the-art results on hypernymy detection and performs well on similarity and analogy tasks. It outperforms the vanilla Glove baseline and offers interpretability through Gaussian word embeddings. The study explores the connection between statistical manifolds of Gaussian distributions and hyperbolic geometry to interpret entailment relations in hyperbolic embeddings. The model presented achieves state-of-the-art results in word similarity, analogy, and hypernymy detection tasks across various settings and dimensions. The study compares Vanilla GloVe with Poincar\u00e9 GloVe models trained with different optimization methods and learning rates. The best results were achieved with learning rates of 0.01 for h = cosh 2 and 0.05 for h = (\u00b7) 2. Only the best results are reported, and similarity was measured using Spearman's correlations on benchmark datasets. The study compared Vanilla GloVe with Poincar\u00e9 GloVe models trained using different optimization methods and learning rates. The similarity results were reported using Spearman's correlations on benchmark datasets. The Poincar\u00e9 models used the Poincar\u00e9 distance as a similarity measure to rank neighbors. The models were initialized with counterparts on restricted and unrestricted vocabularies, with caution advised for analyzing results on the restricted vocabulary due to potential loss of word pairs. Analogies were solved using 3COSADD for Euclidean baselines and specific methods for Poincar\u00e9 models. In analogy computations, Eq. (6) can be rewritten using tools from differential geometry as DISPLAYFORM0, where parallel transport P x\u2192y = (\u03bb x /\u03bb y )gyr[y, x] along the unique geodesic from x to y is denoted. The exp and log maps of Riemannian geometry were linked to the theory of gyrovector spaces. The exp and log maps of Riemannian geometry in relation to gyrovector spaces were discussed, showing the recovery of Euclidean counterparts when deforming hyperbolic space into Euclidean space. Plots illustrating the mapping method from Poincar\u00e9 disk(s) to Gaussian space were presented, with colors indicating WordNet levels. Figures demonstrated the steps for 20D embeddings in 2D spaces. The text discusses the use of a semi-supervised method for centering and rotation in 2D spaces using WordNet hierarchy. Two models are compared, one trained with h = (\u00b7)2 and one with h = cosh2, showing that words trained with h = cosh2 are closer together. The concept of \u03b4-hyperbolicity is introduced, defined as the difference between certain distance sums in a metric space. The \u03b4-hyperbolicity of a space is defined as the smallest \u03b4 > 0 such that for any triangle, there exists a point at distance at most \u03b4 from each side. A low hyperbolicity indicates a tree-like structure, while a high hyperbolicity suggests long cycles.\u03b4 worst and \u03b4 avg were analyzed for specific graphs, indicating underlying geometry and embedding possibilities. The Euclidean space R n is not \u03b4-hyperbolic for any \u03b4 > 0, described as \u221e-hyperbolic. The Poincar\u00e9 disk D 2 has a \u03b4-hyperbolicity of log(1 + \u221a 2) 0.88. A product D 2 \u00d7 D 2 is \u221e-hyperbolic. \u03b4-hyperbolicity is a worst-case measure, so we choose \u03b4 avg as a measure of hyperbolicity due to its robustness. The hyperbolicity of the metric space induced by different h functions on the matrix of co-occurrence counts was computed. Results showed that higher powers of cosh led to improved similarity experiments in more hyperbolic spaces. However, higher powers also resulted in words being embedded closer together, with smaller distances. To determine if this benefit comes from contracting distances or making the space more hyperbolic, it would be interesting to learn the curvature of the Poincar\u00e9 ball jointly with the h function. Additionally, an investigation into why WordSim behaved differently compared to other benchmarks was conducted. The geometry of words in WordSim was compared to other benchmarks, showing no significant difference in hyperbolicity. However, WordSim contains more frequent words. Future work could explore if learned biases align with computed hyperbolicities."
}