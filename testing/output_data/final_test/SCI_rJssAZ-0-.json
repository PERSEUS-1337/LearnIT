{
    "title": "rJssAZ-0-",
    "content": "Deep reinforcement learning algorithms have been successful in various domains, but tasks with sparse rewards are challenging in large state spaces. This work introduces an experience-based tendency reward mechanism to address goal-oriented tasks, providing additional hints based on past experiences. The mechanism offers dense learning signals on successful states and simplifies the agent's learning process in multi-phase curriculum learning. Extensive studies show the method's advantages in sparse reward domains like Maze and Super Mario Bros. Reinforcement learning (RL) aims to learn the optimal policy by maximizing cumulative rewards. Deep RL has been successful in domains with short-term, dense rewards like Atari games. However, real-world problems often have sparse, binary rewards, requiring the agent to navigate through many states before receiving feedback. This study demonstrates a method that outperforms prior approaches in tasks with long time horizons and large state spaces, using a keyframe scheme to solve robot manipulation challenges directly from perception and sparse rewards. Sparse reward tasks in reinforcement learning pose significant challenges, as the environment only provides a final success signal when the agent completes the entire task. Various approaches have been proposed to address this issue, including intrinsic motivation, hierarchical reinforcement learning, curriculum learning, and experience-based off-policy learning. One promising method is reverse curriculum generation. Reverse curriculum generation is a promising approach in reinforcement learning, but it can lead to inefficiency in training due to mixing old and new states. To address this issue, a Tendency Reinforcement Learning (TRL) architecture is proposed to shape a reward function with former experience, stabilizing and accelerating training. The Tendency Reinforcement Learning (TRL) architecture shapes a discriminative reward to judge success tendency in each state, combining it with the final goal reward. A set of start states is defined as a phase, extending from the final goal to the task start. The tendency reward influences training in each phase, updated using collected experience for efficiency. Three novel techniques are introduced: hybrid reward design, automated curriculum with a phase administrator, and an optional keyframe scheme. The major contribution of this work is a reliable tendency reinforcement learning method capable of training agents in large state space tasks using raw pixels as perception. The model is tested in various domains, proving to be more efficient and competitive than prior works. The curr_chunk discusses different approaches to improving reinforcement learning methods, such as curriculum learning and reverse curriculum generation, to address slow training progress and sparse reward tasks. These methods aim to break down difficult tasks into easier ones and generate automated curriculums for training agents effectively. Our approach proposes using tendency reward to summarize past successful and unsuccessful states, avoiding the need to store entire histories. This design choice scales better to larger tasks with longer planning horizons compared to previous methods. Other approaches like hierarchical RL and intrinsic motivations also aim to tackle sparse reward problems. Our method introduces tendency reward to summarize past states, improving exploration without the need for storing entire histories. It scales well to larger tasks with longer planning horizons. Other approaches like hierarchical RL and intrinsic motivations also address sparse reward problems. Additionally, inverse reinforcement learning infers reward functions from expert demonstrations, while Keyframe-based Learning uses keyframes for learning. Our optional keyframe scheme requires only a few key states for robotic manipulation tasks, making it robust to noise. The agent interacts with the environment sequentially, receiving observations and choosing actions based on its policy to maximize rewards. The reinforcement learning model aims to maximize expected return by accumulating rewards from each state. Three key assumptions are made for practical learning problems, including the ability to reset the agent to any start state, providing a goal state, and ensuring communication between start and goal states. These assumptions can be met in robotic manipulation tasks by storing start states with low-dimensional data and utilizing standard robotic APIs for control. The robotic manipulation tasks can benefit from standard robotic APIs to apply settings efficiently. Assumptions enable the agent to train from recommended start states and reset to the goal state. Symbol references are provided in Table 1. Traditional RL struggles with goal-oriented tasks, but TRL reduces aimless searching with discriminative tendency reward shaping. The application reduces aimless searching by utilizing a discriminative tendency reward shaping, a phase administrator for customized curriculum, and a keyframe scheme for large state space tasks. The agent learns from previous experiences through a tendency classifier and adjusts curriculum automatically. The training framework overview is shown in FIG0 and the algorithm is sketched in Alg. 1. The tendency reward is a binary classifier used in training to provide guiding hints for speeding up the process in large state space tasks. When the agent steps onto a state, it receives a positive or negative reward based on the evaluation. Revisited states do not provide rewards to prevent stalling, detected using a Gaussian kernel with recent frames. The tendency reward is a binary classifier used in training to provide guiding hints for speeding up the process in large state space tasks. Revisited states do not provide rewards to prevent stalling, detected using a Gaussian kernel with recent frames. In our experiments, the parameter d is 20% of the max steps of each environment, and \u03c3 = 1 & \u03b4 = 0 are constants. The hybrid reward is defined with a scale factor \u03bb (10 \u22123 in our experiments) to prioritize positive rewards over T(\u00b7) outputs. The agent's progress in achieving the final goal is monitored, and adjustments are made to the generation process based on performance. The phase administrator adjusts the number of steps taken by the agent based on its performance, ensuring new states meet difficulty criteria. The adjustment is made using a sigmoid function based on the number of training iterations. In tasks with a vast state space, reaching any state can be challenging for the agent. Our tendency RL algorithm utilizes keyframes to reduce the search space in tasks with a vast state space. By identifying important states, the algorithm can sample directly from these key states, decreasing the search space significantly. Our tendency RL algorithm uses keyframes to reduce the search space in tasks with a vast state space. If J is larger than 30% of K, the algorithm samples nearby key states rather than extending from the current position. The method is distinct from LfD methods as it only uses keyframes to shrink the search space, showing robustness to imperfect keyframe settings. Five experiments with varying difficulties were designed, using A3C BID16 as the RL solver. The model's efficiency and validity are tested, followed by visualizing tendency hints and starting phases. The keyframe scheme's compatibility is verified, showing robustness to different keyframe qualities. The model is applied to long-term challenges with large state spaces, demonstrating advantages. Training on a 40x40 maze task takes 35000 steps (1 hour and 40 minutes) with 6 GPU threads. An experiment investigates the influence of tendency reward on training effectiveness under different conditions. Our model's performance is evaluated under three conditions: using tendency reward without history phases, using history phases without tendency reward, and using neither. The results show that our model performs well, with an improvement seen when using a phase administrator for automated curriculum learning. The agent undergoes 40000 training steps to complete a level in Super Mario Bros. The influence of tendency reward is explored by analyzing positive and negative tendency hints in the game states. The distribution of tendency hints and phases in Super Mario Bros game shows the agent's ability to find enemies, dodge or kill them, and navigate obstacles. The tendency reward guides the agent to learn new states farther from the goal. The Maze is enlarged to 100x100 with a 9x9 observation. The Maze was enlarged to 100x100 with a 9x9 observation. Training with 8 key states took around 50000 steps. The agent's robustness to imperfect keyframes was tested. Comparison with two LfD methods was done, showing sensitivity to demonstration data and the need for human effort in reward engineering. The Text Representation Learning (TRL) model does not require expert demonstration and human elaboration for training. It is stable and practical for solving real-world robotic manipulation problems with minimal assumptions. Keyframes are used to reduce search space in long-term challenges with large state space, with less than 15 keyframes added in experiments for satisfying results. In a 3D manipulation task with a large state space, a camera is attached to the gripper providing a 64 \u00d7 64 depth image for observation. Learning curves of data-efficient RL and PBRS from demonstrations are designed with keyframes of different qualities. Tendency RL achieves satisfactory performance with minimal human labor. The model is trained with 5 key states for 33360 iterations on GPU, solving the task reliably. Experiments with varying numbers of key points show the relationship between training efficiency and keyframe scale. The conveyance challenge involves picking up a ball and placing it in the correct basket. The agent is trained for 99240 iterations on GPU for a task involving picking up a ball and placing it in the correct basket. The model explores the distribution of \"favorable\" and \"adverse\" states by moving through the environment under different conditions, recording tendency rewards at each position. Areas near the target basket have the highest tendency. The task involves a robotic manipulation challenge with a large state space and a panorama camera. Keyframes are provided to aid training, but the agent still shows confusion due to the complexity of the task. The phase administrator plays a significant role in arranging phases for the robotic manipulation challenge with a large state space. A tendency reinforcement learning algorithm is developed to tackle goal-oriented tasks efficiently, using a binary tendency classifier and a phase administrator. The method is found to be stable and effective in converging on a reliable policy within hours, even in complex environments. In future work, the goal is to apply tendency RL method to real-world robotic manipulation tasks with perceptions. Additionally, there are plans to extend the binary tendency classifier into a predictor for expected success rates in current states and release code and trained models for further research. The text discusses a framework for temporal abstraction in reinforcement learning, including definitions and formulas related to states, phases, start states, success states, task start phase, and keyframes. It also includes algorithms for phase administration and TendencyRL. The text introduces a maze environment with a 100x100 size, one exit at [100, 50], and a starting point at [0, 50]. The agent must navigate using four actions (N, S, E, W) with a 9x9 observation limit, receiving 1 point for reaching the exit. The second environment is level 1-1 of Super Mario Bros where the agent must complete the level with a 16x13 observation, receiving 1 point for reaching the destination. Mario has 5 actions and cannot earn rewards by killing enemies or collecting coins. The third environment involves a grasping task on a robotic arm simulation in MuJoCo with a 6-DOF arm and a two-finger gripper. The agent's observations are 64x64 depth images. The agent manipulates joints mapped into 3D coordinates to move the gripper in 6 directions and grasp a target cube. It receives a reward for capturing the cube on a plane. In another task, the agent controls a crane with a 3-finger gripper to interact with a ball and place it in a green-colored basket. The agent observes through a camera fixed on the gripper producing 64x64 RGB images. The agent uses a camera fixed on the gripper to capture 64x64 RGB images. In another task, it stacks plates using an electromagnet and panorama camera. The goal is to pile up all plates for a reward. The policy network has convolutional neural network layers with specific filters and strides. The policy network utilizes convolutional neural network layers with specific filters and strides, including 4x4, 3x3, and 2x2 filters with varying numbers of units. Additionally, LSTM layers with different units and fully connected layers with hidden units are incorporated into the network architecture. This setup is particularly crucial in the pick-and-place challenge. The training result of Super Mario Bros shows that the model outperforms another based on curiosity-driven BID19. Our method achieves an absolute advantage in completion degree 3, even when running levels 1-2 and 1-3 which require new skills. The maximum entropy IRL method failed in a maze task due to computing limitations. The model's robustness was tested by setting keyframes off the optimal trajectory. The agent learned new policies, finding shortcuts to the exit. The tendency reward provided slight guiding hints without hindering exploration for new policies. The agent's exploration for new policies involves testing policies in 8 special phases with keyframes. The training efficiency is higher with 13 keyframes, showing a clear boundary between \"favorable\" and \"adverse\" states. The gripper jaw being open concentrates positive tendency hints in conveyance challenge. The experiment demonstrates the agent's robustness to imperfect keyframe settings, as it learns to determine the target basket location at a lower height. The regular reverse curriculum algorithm may fail in cases where there is an irreversible process in the system. The experiment shows the agent's ability to adapt to imperfect keyframes and learn the target basket location at a lower height. Absorbing states prevent progress in training, but additional keyframes can overcome this limitation. When sampling states near a keyframe not in an absorbing set, the sampled states may belong to an absorbing set. Phase extensions are limited to absorbing sets, but the generated phase may cover nearby states, allowing the keyframe to still be reached according to the phase administrator algorithm."
}