{
    "title": "r1xwA34KDB",
    "content": "Human reasoning involves recognizing common underlying principles across many examples by utilizing variables. The by-products of such reasoning are invariants that capture patterns without mentioning specific details. This paper addresses whether machines can learn and use variables solely from examples. Unification Networks are proposed to incorporate soft unification into neural networks to learn variables and lift examples into invariants for solving tasks. The approach is evaluated on four datasets to show that learning invariants improves performance over baselines. Humans have the ability to process symbolic knowledge and maintain symbolic thought. Humans process symbolic knowledge and maintain symbolic thought by recognizing invariant patterns with placeholders instead of specific entities. Symbolic cognitive models view the human mind as an information processing system operating on formal symbols. The language of thought hypothesis frames human thought as a structural construct with varying sub-components. Symbolic thought with variables is learned at a young age through symbolic play, where children understand that objects can be substituted for others. In symbolic cognitive models, variables are used to solve specific tasks by binding them to values. Recognizing symbols that can act as variables is crucial for generalizing principles across instances. The approach presented in the study can learn and apply the notion of a variable, as demonstrated in solving tasks in the bAbI dataset. The study discusses the concept of variables in machine learning and proposes a differentiable neural network approach called soft unification, which utilizes attention mechanisms for selecting relevant information. This approach aims to learn and apply the notion of variables in solving tasks, as demonstrated in the bAbI dataset. The study introduces an end-to-end differentiable neural network approach for learning and utilizing variables through soft unification. The proposed architecture can lift examples into invariants for reasoning tasks, with empirical results on four datasets and analysis of learned invariants. The implementation using Chainer is publicly available. Reasoning with variables involves identifying and assigning values to varying components in examples. The study introduces a neural network approach for learning variables through soft unification. It can lift examples into invariants for reasoning tasks.Variables involve identifying and assigning values to varying components. A variable X is defined as a pair X (x, s d ) where x is a discrete random variable with support S. The representation of a variable \u03c6 V (X:s d ) is the expected value of x given the default symbol s d. The default symbol captures the variable's bound meaning. The study introduces a neural network approach for learning variables through soft unification, lifting examples into invariants for reasoning tasks. An invariant is a pair representing a tokenized story with symbols and a function determining the symbol's variable degree. Unification binds variables in the invariant to symbols in the example. Soft unification binds variables in the invariant to symbols in the example by computing probability mass functions and returning a unified representation. It incorporates learnable components such as base features, variableness, and unifying features of a symbol. In this work, the focus is on the interaction between the network f and the invariants produced by g. Soft unification is end-to-end differentiable and can be integrated into existing architectures. Three architectures using MLP, CNN, and memory networks are presented to model f \u2022 g. Symbols are represented as learnable embeddings and the variableness of symbols is captured by a learnable weight. Each symbol is considered a variable independently. The sigmoid function \u03c3 is used independently for each symbol, with the intuition that symbols may need to vary for different inputs. The Unification MLP (UMLP) combines soft unification into a multi-layer perceptron for fixed length inputs. The upstream MLP computes the output symbol based on embedded input, with details in Appendix A. The Unification CNN (UCNN) model uses a convolutional neural network to compute unifying features for embedded symbols, allowing for flexibility in the boundary of the model. The grid of symbols is processed through separate convolutional networks for f and g, with the final prediction being a softmax function applied to the output of global max pooling. The Unification CNN (UCNN) model utilizes convolutional neural networks to generate unifying features for symbols in a grid. Soft unification can be tailored to specific domains, such as incorporating convolution in structured inputs. Unification Memory Networks (UMN) allow for soft unification at various stages, including within Memory Networks handling list structures like tokenized stories. Memory network f uses the final hidden state of a bi-directional GRU for sentence representations and context attention. The Unification CNN (UCNN) model utilizes convolutional neural networks to generate unifying features for symbols in a grid. Soft unification can be tailored to specific domains, such as incorporating convolution in structured inputs. Unification Memory Networks (UMN) allow for soft unification at various stages, including within Memory Networks handling list structures like tokenized stories. Memory network f uses the final hidden state of a bi-directional GRU for sentence representations and context attention. The attended sentences are processed using a bi-directional GRU for unifying features, resulting in a new unified representation of the memory slot. The prediction is based on the hidden state of the invariant after iterations, requiring pre-training for correct context attentions. To differentiate between questions and learn different invariants, models are extended with a repository of invariants. Predictions from each invariant can be aggregated by summing them or using features like memory representations. In UMN, predictions are weighted using bilinear attention based on hidden states. The repository of invariants is initially formed using bag-of-words representation of questions and finding dissimilar ones based on cosine similarity. Four datasets consist of context, query, and answer in fixed length sequences of symbols, shapes in a grid, and story-based formats. The curr_chunk discusses the use of different models for various types of reasoning tasks, such as fixed length sequences, grids, and iterative reasoning. Synthetic datasets are used to evaluate the performance and quality of learned invariants. For fixed length sequences, sequences of length 4 with 8 unique symbols are generated to predict constants, sequence heads, tails, and duplicate symbols. Training is done using 5-fold cross-validation. The curr_chunk discusses the use of a grid with 8 unique symbols for spatial organization and prediction tasks. It also mentions the bAbI dataset, a benchmark for memory-based networks, with 20 natural language reasoning tasks. The dataset is used with a 1k English set and 0.1 validation set. Each token is considered a unique symbol for evaluation. The curr_chunk discusses generating logical reasoning tasks using logic programs with varying paradigms. Tasks involve learning f(C, Q) = True \u2194 C Q over 12 classes of logic programs. 1k and 10k logic programs are generated per task for training, with 0.1 validation and 1k for testing. Soft unification impact is probed in three aspects. The impact of soft unification on performance over unseen data, multiple invariants, and data efficiency is investigated by training UMLP, UCNN, and UMN with and without unification. Models are trained using back-propagation with Adam on an Intel Core i7-6700 CPU, with specific objective functions and regularization techniques. The impact of soft unification on performance is investigated by training UMLP, UCNN, and UMN with and without unification. Mean squared error and negative log-likelihood are added to the objective function for iterative tasks. Soft unification leads to higher accuracy with fewer iterations and better generalization to unseen examples in test sets compared to plain models. Despite having more parameters, models with unification maintain higher accuracy in each iteration. Soft unification improves model accuracy with fewer iterations and better generalization to unseen examples. The models are biased towards learning structural patterns, resulting in better recognition of common symbol patterns. Fluctuations in accuracy are caused by penalizing certain variables, leading to relearning tasks with fewer variables. Strong supervision leads to better performance. Incorporating strong supervision and more data per task leads to better performance in unifying sentences. The dependency on the efficacy of the model limits the ability to learn from a small number of examples per task. The increase in error rate with more invariants may be due to having more parameters. The increase in error rate with 3 invariants is speculated to be due to more parameters and pathways in the model, making training more difficult. After training, learned invariants can be extracted by applying a threshold on symbols to determine if they are used as variables. The threshold magnitude depends on regularization, training steps, and batch size. Sample invariants describe common patterns in tasks, with parts contributing to the final answer becoming variables. Changing certain symbols does not affect predictions, attributing this behavior to the nature of the task. The learned invariants in the approach provide an approximation of the underlying general principle present in the data, such as the structure of multi-hop reasoning. The model captures invariants related to multi-hop reasoning and temporal reasoning, but certain aspects remain hidden. The model can entail chains of logical reasoning tasks and learn how variables affect behavior. Regularizing terms help prevent the model from using extra symbols as variables. The model captures invariants related to multi-hop and temporal reasoning, using symbols as variables and binding them occasionally. Regularizing terms helps prevent extra symbols, with unifications showing patterns like one-to-one or many-to-one bindings. The model can optimize by using fewer variables, squeezing information into a single variable. The model uses symbols as variables and occasionally binds them, showing patterns like one-to-one or many-to-one bindings. It optimizes by using fewer variables and squeezing information into a single variable. The model differentiates between variables by encoding differences in magnitude, supporting generalization in neural networks. The model differentiates between variables by encoding differences in magnitude, supporting generalization in neural networks. MemNNs can discover simple linguistic patterns based on verbal forms like (X, dropped, Y) or (X, took, Y), but it is unknown if the model truly learned these representations. Our approach presents these linguistic patterns explicitly as invariants through g, shedding light on their utility without solely relying on f's output. These symbolic patterns may be mistakenly anthropomorphized, but they do not necessarily correspond to the machine's understanding. The Chinese room argument by Searle (1980) questions machine understanding. Learning invariants through examples relates to inductive inference. Our approach allows for soft alignment and generalization between sequences. Existing neuro-symbolic systems focus on inducing rules adhering to logical semantics.\u03b4ILP constructs networks following first-order logic semantics. Lifted Relational Neural Networks ground logic rules into neural networks. Neural Theorem Provers build networks using backward chaining. Neural Theorem Provers build neural networks using backward-chaining on a background knowledge base with templates. The learned invariants make the model more interpretable, but transparency is of the data, not the model itself. Learning invariants or interpreting them does not explain the reasoning model f from the perspective of explainable artificial intelligence. The text discusses causal attribution in neural networks, relating it to gradient-based model explanation methods like Layer-Wise Relevance Propagation and Grad-CAM. It also mentions a gradient-based usefulness measure and counterfactual thinking in model interpretation. The curr_chunk discusses a new approach for learning variables and lifting examples into invariants using soft unification in neural networks. The approach is end-to-end differentiable and aims to apply the technique to multi-modal tasks, such as visual question answering. The Unification MLP (UMLP) model is used to model f as a multi-layer perceptron. The Unification MLP (UMLP) model uses symbol embeddings to create a multi-layer perceptron with hidden layers and tanh non-linearity. It processes queries by concatenating task id encodings and utilizes a bi-directional GRU for unification features. The Unification CNN (UCNN) model uses symbol embeddings to create an input grid and appends task ids for each symbol. The task involves using a one-hot vector for input, followed by convolutional layers with relu non-linearity. Padding is applied to the grid to maintain the shape of the hidden output. A unification function is modeled similarly to the convolutional layers. The architecture includes embedding sizes and an iterative memory network. The final hidden state is obtained using a bi-directional GRU for context and query representation. The memory network uses a d-dimensional vector M for queries and context. The initial state is set as the query vector. The network iterates using context attention and produces a final prediction. Weight matrices and bias vectors are tied across iterations. Training sizes for randomly generated sequences and grid tasks are shown in a table. The grid task has smaller training sizes due to limited combinations in a 3x3 grid with 8 unique symbols. The unification models are trained on a 5-fold cross-validation over generated datasets for 2000 iterations with a batch size of 64. Training details include pre-training f for 40 epochs, then f and g together for 260 epochs. The objective function incorporates negative log-likelihood and mean squared error between intermediate states as an auxiliary loss. The models are pre-trained with \u03bb U = 0 for 40 epochs and then set to \u03bb U = 1. Strong supervision includes computing negative log-likelihood L nll for context attention \u03b2 j using supporting facts. Dropout of 0.1 is applied to all recurrent neural networks. Weight decay with coefficient 0.001 is used for the bAbI dataset. Results show no impact on performance with increasing invariants per task. Models tend to ignore extra invariants and only use 1, possibly due to regularisation \u03c8. Invariants learned do not match the data generating distribution from UMLP and UCNN. In training with \u2264 1000 examples, unification binds to correct symbols for predicting answers. Default symbols are omitted for clarity. In task 2, unused variables unify with random tokens biased by position. Task 1 creates one-to-one alignment between predicates and constants. Task 3 forces arity 1 atom to bind with arity 2, creating a one-to-many mapping."
}