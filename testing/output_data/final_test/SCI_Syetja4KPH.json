{
    "title": "Syetja4KPH",
    "content": "Exploration in Deep Reinforcement Learning (DRL) is a key challenge, with the exploration strategy playing a crucial role in learning representations. While the DQN algorithm has improved RL capabilities, it has a statistically inefficient exploration strategy. The RLSVI algorithm offers efficient exploration and generalization through linearly parameterized value functions, but requires hand-designed state representation. This paper proposes a Deep Learning adaptation for RLSVI that learns state representation directly from data. The suggested method for the DQN agent includes a likelihood matching mechanism that adapts to changing representations. It outperforms DQN in five Atari benchmarks, showing competitive results. In Reinforcement Learning (RL), an agent aims to maximize rewards from interactions with an unknown environment by balancing exploration and exploitation. Classic RL algorithms are designed for tabular settings, but for more general settings, hand-designed state representations are used for function approximation. RL algorithms based on linear function approximation have shown stability, data efficiency, and convergence guarantees. They require the learned function to be a linear combination of state representation, which is a constraint due to hand-designed representations. The DQN algorithm revolutionized RL by using Deep Neural Networks for function approximation, allowing policies to be learned directly from raw data and achieving remarkable results across various domains. Despite its success, DQN's exploration strategy is overly simple, using the -greedy approach. This strategy involves taking random actions with a certain probability and optimal actions with the remaining probability, without utilizing observed data for improvement. Thompson Sampling (TS) is an effective heuristic for addressing the exploration/exploitation trade-off in decision-making problems. It randomly takes actions based on the probability it believes to be optimal, using a prior and posterior distribution of model parameters. Randomized Least Squares Value Iteration is an RL algorithm that utilizes TS for decision-making. The Deep Randomized Least Squares Value Iteration (DRLSVI) algorithm combines the exploration mechanism of RLSVI with the representation learning mechanism of DQN. It aims to improve efficiency by integrating the strengths of both algorithms. The DRLSVI algorithm combines DQN for state representation and uses a likelihood matching mechanism to transfer information from old representations. It outperforms DQN and Rainbow algorithms in learning speed and performance on Atari benchmarks. Thompson Sampling is a heuristic for exploration/exploitation trade-off in decision-making problems. Thompson Sampling is a heuristic for balancing exploration and exploitation in decision-making problems. Chapelle & Li (2011) demonstrated the effectiveness of Thompson Sampling compared to other algorithms like Upper Confidence Bound. Agrawal & Goyal (2013) proposed a Thompson Sampling algorithm for the linear contextual bandit problem using Bayesian Linear Regression. Riquelme et al. (2018) suggested incorporating Bayesian Linear Regression on top of the last layer representation. Riquelme et al. (2018) proposed using Bayesian Linear Regression (BLR) on the last layer representation of a neural network for decision-making. They trained a Deep Neural Network (DNN) to learn a representation z, then used BLR to regress values on z for uncertainty estimates. The network is solely used for finding a good representation -z, and the training and updating of BLR can be done independently. This process is computationally expensive and memory-intensive. Zahavy & Mannor (2019) proposed a method to prevent catastrophic forgetting in algorithms with finite memory by matching the likelihood of rewards under old and new representations. In Reinforcement Learning, Strens (2000) introduced \"Posterior Sampling for Reinforcement Learning\" (PSRL), which estimates the posterior distribution over MDPs and finds optimal policies through dynamic programming. Recent work by Osband et al. (2013) guarantees strong expected performance of PSRL in various environments, but its limitation lies in its applicability to small environments. Thompson Sampling in DRL extends the idea of Randomized Least Squares Value Iteration to Model-Free Reinforcement Learning. Bootstrapped DQN uses an ensemble of Q-networks with different data samples to explore and select actions. Various approaches adapt this concept by perturbing the parameter space or providing different priors to ensemble members. In contrast to previous work on Thompson Sampling in DRL, the authors do not alter the DQN agent but utilize its learned representation for RLSVI. They maintain the same network structure, loss, and hyperparameters. Unlike Azizzadenesheli et al. (2018), who modified the network architecture and used double-DQN, the authors do not compensate for changing representations and solve the BLR problem with the same prior each time. The authors utilize the learned representation of a DQN agent for the Randomized Least Squares Value Iteration (RLSVI) algorithm, which is a TS-inspired approach in modeling an environment with discrete time steps using a Markov Decision Process (MDP). The Randomized Least Squares Value Iteration (RLSVI) algorithm combines TS-like exploration and linear function approximation for Model-Free Reinforcement Learning. It samples value-functions from a posterior distribution and acts greedily according to the sampled value-function, guaranteeing near-optimal expected outcomes in episodic settings. RLSVI guarantees near-optimal expected episodic regret without the need for generalization. It can be used as an exploration mechanism for DQN, enhancing capabilities with learned state representation from a neural network. The DQN agent is trained with minor modifications to the architecture and buffer size. The hidden layer is reduced to d = 64. Experience Replay buffer evenly stores actions and transitions in a round-robin fashion. Exploration is done using RLSVI on the last hidden layer of the target network, with changes made to solve a regression problem for every action using DQN's targets. The regression targets y are DQN's targets using the target network predictions. A different Bayesian Linear Regression formulation is used, with \u03c3 2 as a random variable distributed according to the Inverse-Gamma distribution. The posterior distribution can be calculated analytically. Formulating \u03c3 2 as a random variable allows adaptive exploration. The prior in our algorithm plays a central role. The target network's weights are fixed, using last layer activations as state-representation. Every T target training steps, target network is updated with Q-network weights. Posterior distribution needs to be approximated with new representation due to changing Q-network. New Bayesian linear regression problems are solved using N BLR samples when target network changes. Ignoring lost experience can lead to performance degradation from 'Catastrophic Forgetting'. To prevent performance degradation from 'Catastrophic Forgetting', the likelihood of the Q-function in old and new representations is matched. Gaussian moments are used to align the Q-function likelihood in the new representation with the old one. Expectation prior is set using the last layer weights of the DQN for the new representation. Covariance prior is determined for Bayesian linear regression problems. The last layer weights of the DQN are used as the expectation prior for the new representation. Covariance prior is established by using N SDP samples from the experience replay buffer. The goal is to match the covariance of the likelihood in the new representation to the old one. This is achieved by solving a Semi-Definite Program (SDP) using CVXPY. To speed up learning, approximate sampling is used by sampling weights for every action every T sample time steps. To speed up learning, weights are sampled for every action every T sample time steps. The bottleneck of the algorithm is solving the SDP, with a running time of 10-50 seconds per SDP using Intel's Xeon CPU E5-2686 v4 2.30 GHz. The algorithm's complexity depends on the dimension of the representation, the number of samples used, and the desired accuracy. The algorithm's performance is evaluated on a chain environment with additional actions, leading to the \"Augmented Chain Environment\". Results from 5 Atari games in the ALE are reported, with a focus on cumulative episodic regret. The Q-network used is an MLP with 2 hidden layers, and results are averaged across 5 runs. In the augmented chain environment, the agent's performance is compared to standard DQN using -greedy exploration. Results show that -greedy has linear regret, while the new algorithm achieves much lower regret. Modeling \u03c3 2 as a random variable leads to lower regret compared to using constant values. Modeling \u03c3 2 as a random variable leads to lower regret compared to constant variants. Choosing the appropriate size for \u03c3 2 results in better performance, but it doesn't adapt. The algorithm may \"over-explore\" as uncertainty decreases, leading to inferior regret compared to adaptive versions. Our method, which constructs priors by likelihood matching, reduces the forgetting phenomenon compared to other variants that do not match the likelihood. The results show the superiority of our method over the one-moment method and no-prior approach, supporting our claim. Additionally, an unbounded memory algorithm was tested on a small toy problem, showing promising results. The unbounded memory algorithm (green) doesn't show degradation in performance, confirming the bounded ER buffer causes the forgetting phenomenon. Increasing buffer size may prevent catastrophic forgetting in DRLSVI. In a Chain environment experiment, our algorithm (blue) outperforms variants with different buffer sizes, avoiding performance degradation seen in other algorithms. Without likelihood matching, catastrophic forgetting occurs regardless of buffer size. When the buffer lacks experience with non-optimal actions, quick degradation happens. The algorithm can re-learn the optimal policy initially, but regret eventually becomes linear. This pattern was consistent across experiments on 5 Atari games. The evaluation of the model involved time steps and standard evaluation every 250k training steps. The results were based on the average episode return using the learned Q-network with a -greedy policy. Comparison was made with DQN and Rainbow algorithms, showing that the method explored faster than DQN and was competitive with Rainbow. The Deep Q-Networks (DQN) algorithm successfully combined Deep Learning with Reinforcement Learning. A Deep Learning adaptation to RLSVI was presented, learning state representation directly from data. Results were not compared with Azizzadenesheli et al. (2018) due to architectural differences. The method's properties were demonstrated in experiments, showing promise for future improvements in complexity and running time. The algorithm uses a high-dimensional state space and a Convolutional Neural Network (CNN) to approximate the optimal Q-function. It maintains two networks: the Q-network with weights \u03b8 and the target network with fixed weights \u03b8 target. The Q-network takes a state s as input and outputs Q-values for different actions. The target network is used to construct targets based on the Bellman equation, which are used to train the Q-network using Stochastic Gradient Descent (SGD). The weights of the target network are updated every fixed number of time steps. The DQN algorithm uses Experience Replay to optimize network weights by collecting tuples <s, a, r, s'> into a buffer. It is an off-policy algorithm that explores the environment using an \u03b5-greedy strategy. The Q-network with weights \u03b8 and target network with fixed weights \u03b8target are used to approximate the optimal Q-function. The target network's weights are updated every fixed number of time steps."
}