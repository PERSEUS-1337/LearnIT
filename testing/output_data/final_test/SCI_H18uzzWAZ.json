{
    "title": "H18uzzWAZ",
    "content": "Profiling cellular phenotypes from microscopic imaging for drug development involves capturing morphological cell features from images to quantify similarities between different drugs. The challenge lies in separating relevant biological signal from nuisance variation, such as embedding vectors being more correlated for cells cultured and imaged in the same week. The batch of experiments conducted represents the domain of the data. The experiments conducted aim to adjust image embeddings to forget domain-specific information while preserving relevant biological data. By minimizing a loss function based on distances between embeddings across domains, the transformed embeddings maintain the geometric structure and enhance biological signal while reducing domain-specific information. The framework involves inputs (e.g. images) and a mapping function to a low-dimensional space. The experiments aim to adjust image embeddings to remove domain-specific details while retaining biological information. A mapping function sends inputs to a low-dimensional space, generating embeddings that can be compared for semantic similarities. The goal is to find a map that adjusts the embeddings so that the distribution for a given label is independent of the domain. The distribution of adjusted embeddings for a label is domain-independent, maintaining meaningful distances between inputs with different labels. The map F is used for phenotypic profiling of cells, extracting embeddings from images to reveal similarities among biological stimuli. Different methods can be used to extract embeddings, such as engineered features or a Deep Metric Network pre-trained on consumer photographic images. Currently, using image embeddings to differentiate the effects of treatments on cell features faces challenges due to uncontrollable variations in experiments. Cell imaging experiments are organized into batches with sample plates containing wells where treatments are applied. The 'domain' in this context refers to hierarchical levels, with embeddings for cells with a specific treatment being closer together. Our approach aims to minimize nuisance variation in embeddings by transforming the embedding space in a domain-specific way to reduce variation across domains for a given treatment. This addresses challenges in differentiating treatment effects on cell features due to uncontrollable experiment variations. Our goal is to introduce a flexible framework to minimize variation across domains for a given treatment. We use a metric function to construct an optimization problem, with the Wasserstein distance showing significant improvements. Our approach transforms all embeddings equally, incorporating information from all domains without fixed 'target' and 'source' distributions. This allows us to learn transformations from multiple replicates of a treatment across different domains. The text discusses using replicates of a treatment across different domains to learn transformations. It mentions using the Cramer distance as an alternative to the Wasserstein distance for faster training. The goal is to minimize variation across domains by finding maps that transform embedding vectors. The text discusses finding maps A d to transform embedding vectors, aiming to minimize variation across domains. The 1-Wasserstein distance measures the similarity between probability distributions on a metric space. The Wasserstein distance measures the minimal cost of a transportation plan between probability distributions, making it difficult to discern the origin of samples. It is more appropriate than classifiers for comparing distributions, as it captures meaningful features like displacement between distributions. The Wasserstein barycenter, a distribution that minimizes Wasserstein distance between multiple distributions, is used to remove nuisance variation in biological experiments. This method transforms distinct distributions into a common one while minimally perturbing them. Our method is based on using separate maps for each domain and including the average Wasserstein distance among transformed distributions in the loss function. This is achieved by minimizing a loss function containing pairwise Wasserstein distances and using early stopping or adding a regularization term to achieve better results compared to ResNet. Another possible formulation aligns more closely with the idea of the Wasserstein barycenter. The Wasserstein distance is advantageous as it avoids vanishing gradients during training, unlike metrics based on KL-divergence. It must be approximated and is related to the maximum mean discrepancy. In our application, we update kernel parameters during training using a method based on neural networks. To estimate the Wasserstein distance, a neural network is trained based on the ideas in BID8 and BID13 for domain adaptation. The Wasserstein function is optimized with a Lipschitz constant constraint. Embeddings for the dataset are preprocessed to have mean zero and an identity covariance matrix. The impact of nuisance variation is modeled by affine transformations, treating variations as small, random drug-like perturbations. Domain-specific transformations map input embeddings to transformed embeddings, formulated as penalized approximations to the Wasserstein distance between domains. In this paper, the regularization term R(\u03b8 T ) is used to preserve the geometry of original embeddings. Two methods are explored: neglecting R and relying on early stopping, or specifying R as described. The transformation \u03b8 T parameterizes an affine transformation, with different parameter choices possible. The Frobenius norm, 2 norm, and embedding dimensionality are considered in the formulation. The text discusses the use of regularization weights \u03bb M and \u03bb b, Wasserstein distance approximation between embeddings of domains, gradient penalties, and Lipschitz functions in optimization. In optimization, a soft constraint on the norm of the gradient is imposed using a penalty based on the gradient of the Wasserstein function. The penalty term is weighted by a parameter \u03b3, with a value of \u03b3 = 10 found to work well. Intermediate points are chosen randomly to approximate the gradient, denoted by J t,di,dj. To approximate the Wasserstein distance, a gradient penalty is imposed on the norm of the gradient of the Wasserstein function. The penalty is applied along paths connecting intermediate points between two probability distributions. Unlike previous methods, the penalty is only enforced when the gradient norm exceeds 1, which has shown better performance in practice. The objective is to maximize over \u03b8 W to find the optimal transport plan. The objective is to find DISPLAYFORM5 by maximizing the Wasserstein distance over \u03b8 W. The approach involves transforming a minimax problem into a minimization problem using a 'gradient reversal' technique. Embeddings are generated using a method described in BID1 and applied to the image set BBBC021v1 from the Broad Bioimage Benchmark Collection. This dataset includes cells prepared on 55 plates across 10 batches, imaged in three color channels, with control cells treated with DMSO and other cells with various drug compounds. The corresponding embeddings for each cell image are computed. The embeddings for cell images are computed using a method described in BID1. A subset of treatments from 38 compounds, each belonging to one of 12 known mechanism of action groups, is evaluated. The cosine similarity matrix between pairs of selected treatments for TVN embeddings shows clustering based on compound and mechanism of action. A flowchart describes the procedure used to generate and remove nuisance variation from image embeddings. The method uses F to map color images into a 192-dimensional embedding vector. Nuisance variation removal is done by WDN. Evaluation metrics measure biological signal preservation and nuisance variation removal. Embedding vectors should group compounds with the same MOA closely. Mean embeddings for each treatment in each domain are computed to assess grouping. The method computes the mean embeddings for each treatment in each domain and finds the nearest neighbors not belonging to the same compound or batch. The metric is the average portion of neighbors with the same MOA as the mean embedding. Nearest neighbors are based on cosine distance to avoid in-domain correlations. This k-NN metric is a generalization of existing methods and can be used for cluster validation measures. In cluster validation, the Silhouette index measures how well compounds group together in embedding space. It compares distances within a cluster to distances to other clusters, with higher values indicating better clustering. Another metric evaluates the forgetting of domain-specific nuisance information. The text discusses a metric that measures the forgetting of domain-specific nuisance information by training classifiers to predict the batch for each treatment. Linear and random forest classifiers are evaluated, and accuracy should decrease if nuisance variation is corrected. Hyperparameters can be selected through cross-validation to avoid overfitting. The procedure involves applying a leave-one-compound-out cross-validation method to determine the optimal time step for early stopping. The k-NN MOA assignment metrics are evaluated for each compound, and hyperparameters are optimized over compounds with known MOAs. The optimal time step is found to be 28000. The optimal time step for early stopping is determined to be 28000, regardless of the compound. Standard errors of metrics are estimated using a nonparametric bootstrap method, with 200 repetitions. Embedding transformations are kept close to the identity transformation. Wasserstein functions are approximated using a network with softplus and affine layers. The softplus layer followed by a scalar-valued affine transformation is used in a network to approximate Wasserstein functions. Softplus loss is chosen for less noisy estimates and to prevent neuron deactivation. The softplus layer dimension is 2, optimization is done with stochastic gradient, and minibatch size is fixed at 50. Separate RMSProp optimizers are used for parameter classes \u03b8 T and \u03b8 W. Pre-training for 20000 time steps is done before alternating between adjusting \u03b8 T and optimizing \u03b8 W. The training procedure involves adjusting \u03b8 T for 40 time steps and optimizing \u03b8 W for a single time step alternately. Results are compared to TVN and CORAL methods. CORAL aligns embedding coordinates by matching covariance structures. Regularized covariance matrices are computed with a regularizer \u03b7 = 1. Other variations of the training procedure are discussed in subsequent sections. The embeddings transformed by WDN outperform those preprocessed by TVN and generated by the CORAL method in preserving geometry. WDN also shows better performance in k-NN MOA assignment metrics compared to CORAL. Additionally, WDN embeddings exhibit higher batch classification accuracy for linear and random forest classifiers compared to TVN embeddings. Our method outperforms TVN and CORAL in batch classification accuracy, indicating nuisance variation removal. Regularization with early stopping or regularization weights improves network performance. k-NN MOA assignment metrics are presented for different regularization weights and early stopping. Regularization with early stopping or different weights can improve network performance, but may also remove relevant biological signals. Using early stopping simplifies the process but may limit performance due to fewer parameter choices. It is important to find the optimal balance between nuisance variation removal and preserving biological signals. The text discusses the impact of early stopping on optimization and how it compares to regularization weights in improving network performance. It mentions that early stopping produces better results in some cases but a more thorough search over regularization weights could yield similar outcomes. Additional experiments are conducted by varying hyperparameters, such as increasing the minibatch size, to assess their effect on model performance. The architecture of the network estimating pairwise Wasserstein distances becomes more complex with additional hidden layers and nodes per layer. Results show stable curves for k-NN MOA assignment metrics over time steps. A neural network is used to remove specific domain information from embedding vectors while preserving space geometry and enhancing k-NN MOA metrics using Wasserstein distance. The approach can handle diverse embedding distributions without assumptions. Our method utilizes information from treatment replicates across different domains, assuming controls match compounds' transformations. It is expected to be more effective with many replicates for simultaneous alignment, leading to better generalizability. The approach requires choosing free parameters for optimal performance. Our approach involves cross validation across compounds to determine free parameters for regularization or early stopping. The silhouette index for different methods is shown in Table 3, with WDN and CORAL improving cohesion. Potential future directions and limiting issues are also discussed. The cost function can be modified to resemble finding the Wasserstein barycenter by comparing transformed distributions to original distributions. This approach avoids the 'shrinking to a point' issue and does not require regularization or early stopping. However, the new cost function did not show better performance for the specific dataset. An alternative regularization term penalizing differences from the identity transformation could be considered, such as penalizing changes in pairwise distances within a specific domain to preserve biological signal. In-domain variations carry biological signal that we want to preserve using a regularization term. Wasserstein functions were approximated with simple nonlinear functions, but better results may be achieved with more sophisticated functions. Transformations could be generalized to a more diverse class of functions, such as residual networks. Fine-tuning the Deep Metric Network instead of training separate networks could be beneficial. Weighing the various Wasserstein distances against each other could improve results, especially when dealing with imbalanced distributions. The text discusses the challenges of weighing a regularization term against Wasserstein loss terms and suggests hierarchical application of the method. It also mentions the potential for better results by modifying the metric used for Wasserstein distance computation. Additionally, it compares the performance of WDN and CORAL in preserving the geometry of the embedding space. The text discusses the challenges of regularization in WDN and CORAL methods for preserving embeddings. A heatmap illustrates the cosine similarity matrix between treatments, showing how compounds with the same MOA cluster together in the embedding space."
}