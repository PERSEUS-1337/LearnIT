{
    "title": "H1eZ6sRcFm",
    "content": "In this paper, a new VAE model for text is proposed, addressing the issue of decoder ignoring encoder information without weakening its capacity. The model utilizes a multimodal prior distribution, a modified encoder, and multi-task learning to generate well-conditioned sentences. The multimodal prior distribution also enhances the interpretability of acquired representations in generative text models for NLP. Variational Autoencoders (VAEs) were proposed for sentence generation, but faced issues like \"posterior collapse\" due to training with \"teacher forcing\". This led to the KL term converging to zero and encoder input being ignored. In response to issues faced by Variational Autoencoders (VAEs) in sentence generation, a new approach is proposed. The focus is on addressing the problems of posterior collapse and the limitations of using LSTM as the encoder in existing models. Two modifications are introduced in a new architecture for textual VAEs to overcome these challenges. The proposed new architecture for textual VAEs addresses issues like posterior collapse and limitations of using LSTM as the encoder. It introduces a multimodal prior distribution and modifies the encoder to improve performance. The modified encoder in the new architecture for textual VAEs combines attention mechanism and Bag-of-Words encoding to improve performance. The experiments show that modifying the encoder alone is not sufficient for better results, and other parts of the model need to be adjusted as well. The self-attention encoder captures grammatical structure while Bag-of-Words encoding captures semantic content. Multi-task learning is added to help the model acquire meaningful latent variables without weakening the decoder. The addition of multi-task learning significantly improves the quality of output text without causing posterior collapse. The model outperforms baselines on BLEU score, showing well-conditioned generated texts. Each component of the multimodal prior distribution captures grammatical or contextual features, enhancing interpretability. BID1 and BID16 propose solutions to the problem of posterior collapse in textual VAEs. The usage of Bag-of-Words for textual VAEs aims to prevent posterior collapse by controlling the decoder's capacity. Different approaches like using a dilated CNN decoder, deconvolutional layer, and multimodal prior distribution have been proposed to modify prior distributions of VAEs. Some models also suggest using discrete latent variables for text modeling. Models for text modeling have been proposed, with some requiring discretization like Gumbel-Softmax and Vector Quantization for training discrete autoencoders. A multimodal prior distribution can serve as a smoothed autoencoder model with discrete latent variables. Variational autoencoders for text generation involve training an RNN language model to predict the next word based on previous words, and models conditioned on a deterministic latent vector have also been suggested. The BID1 propose a new language model using Variational Autoencoders to capture probabilistic latent variables of global features. Variational Autoencoders learn Variational Bayes through gradient descent with an encoder and decoder parameterized by neural networks. The model maximizes the evidence lower bound instead of the intractable marginal probability. The reparameterization trick can be used for training with gradient descent. The previous work introduced a language model using Variational Autoencoders for text generation. Training VAEs for text generation faces challenges like \"posterior collapse,\" where the KL term approaches zero, leading to loss of input information in latent variables. To stabilize training, teacher forcing is used to provide the ground truth word to the decoder. This technique can be applied to textual VAEs with a simple LSTM-based language model. In this paper, modifications are proposed to improve textual VAEs without limiting the decoder's capacity. These modifications are detailed in chapters 3.2, 3.3, and 3.4. The model uses a standard normal distribution as the prior and a normal distribution as the posterior. The paper proposes modifications to improve textual VAEs by using a multimodal distribution as the prior and an unimodal distribution as the posterior to avoid posterior collapse. This idea is motivated by recent VAE models for image and video generation. The challenge lies in deciding what kind of distribution to use for the multimodal prior. In the context of improving textual VAEs, a multimodal prior distribution called VampPrior has been proposed. This model uses trainable pseudo-inputs to form the prior distribution, allowing for unsupervised clustering and capturing specific features of sentences. The components of the multimodal prior distribution also create a hierarchical structure within the representation space. Recent research into text generation has shown that simple LSTMs may not have enough capacity to encode information from the entire text. To address this, a proposed approach involves concatenating the representation from a self-attention encoder with Bag-of-Words information. This division of roles between self-attention and Bag-of-Words has shown success in experiments, with the attention mechanism being a popular model for encoding text with LSTMs. However, due to VAEs having fixed size probabilistic latent variables, a recently proposed method is used to adapt the attention mechanism for variable size representations. The self-attention BID12 method encodes variable length input into a fixed length representation using an attention mechanism. The fixed length representation is obtained by summing up hidden states of bi-directional LSTM based on attention weights. This model is effective for text generation tasks. The self-attention method encodes variable length input into a fixed length representation using an attention mechanism. Hidden states are summarized with attention weights calculated by weight matrices. A fixed sized representation is acquired by summing up hidden states. Parameters are trained with gradient descent. Previous research shows the effectiveness of Bag-of-Words in NLP tasks. Using a Bag-of-Words input for text generation tasks can help stabilize training compared to LSTMs and self-attention encoders. Multitask learning in NLP tasks, such as multi-lingual training, can improve performance by enabling better intermediate representations. Additionally, a recently proposed model for encoding chemical structure has been introduced. The text discusses the use of multi-task learning to improve the quality of embedded representations in VAEs for text. It highlights the benefits of predicting words in output text as a simple task to enhance model performance and alleviate the issue of posterior collapse. The model is compared with two others, showing similar configurations. The text compares different encoder models for embedding text into a 128-sized vector. The self-attention encoder outperforms the LSTM and Bag-of-Words encoders in a sequence-to-sequence autoencoder model based on BLEU score, but it has a higher false negative rate. The self-attention encoder has a higher false negative rate compared to LSTM in predicting words. It is good at acquiring sentence structure but struggles with embedding all information. Self-attention and Bag-of-Words are used for the encoder in language modeling, showing improvements with multi-task learning and multimodal prior distribution. Changing the encoder alone does not affect results, but incorporating Bag-of-Words input improves scores with multi-task learning. The self-attention encoder, when used with Bag-of-Words input, improves scores. Additionally, a multimodal prior distribution enhances the self-attention encoder's performance over the LSTM encoder. Training the VAE encoder, especially the self-attention encoder, is challenging without overall model improvements. Our model, incorporating self-attention and Bag-of-Words as the encoder, outperforms baselines significantly. The text discusses sampling from the posterior distribution of a model using different inputs for the self-attention and Bag-of-Words encoders. It analyzes the relationship between the two encoders by generating sentences and examining attention weights. The generated sentences in Table 3 have similar grammatical structures to the input of the self-attention encoder, with nouns strongly influenced by the Bag-of-Words encoder. Additionally, attention weights of the self-attention encoder show which parts of a sentence the encoder focuses on. The text discusses the maximum attention weight assigned to words by the self-attention encoder, showing a preference for words determining sentence structure over nouns. It also highlights how attention weights are similar for sentences with shared grammatical structure. The model successfully acquires sentence representations with a multimodal prior distribution aiding in unsupervised clustering. The clustering reveals commonalities in grammatical structure or topic among sentences allocated to different components. A new method is presented to interpret the global structure of the acquired representation space. The acquired representation space is analyzed by visualizing clusters of components in a multimodal prior distribution. Two clear clusters are identified, with cluster 1 sharing grammatical structure and topics related to computer, politics, and culture, while cluster 2 shares topics of politics or human relationships. Components in each cluster have their own unique characteristics. The model acquires a hierarchical structure of sentences through analysis of components in the multimodal prior distribution. Multimodal prior distribution helps understand the representation space easily. Future research should focus on controlling multimodal prior distribution. The text discusses the use of a multimodal prior distribution in improving a model with a simple LSTM decoder. It also mentions theoretical justification for the multimodal prior distribution to prevent posterior collapse. The objective function used can be minimized without latent variables under certain assumptions. The text proposes breaking assumptions in VAEs by using a multimodal prior distribution to prevent posterior collapse. This approach aims to address limitations of restricting decoder capacity and the need for hyper-parameter search. The text discusses minimizing the Kullback-Leibler divergence between Gaussian mixture distribution p and normal distribution q in VAEs to prevent posterior collapse. It suggests using a multimodal prior distribution to avoid the need for modifying the decoder. The model is trained to ensure KL divergence is 0, forcing the decoder to satisfy a specific condition. Modifying the model may lead to the decoder learning conditioned distributions for each z. Experimental results support this hypothesis, using 10,000 sentences for testing and validation with a maximum length of 60 words. The model utilizes self-attention. The model uses self-attention and Bag-of-Words in the encoder, with a LSTM for the decoder. Hidden state size is 256, word embedding size is 256, and latent variables size is 128. Self-attention encoder parameters are d = 350 and r = 30. Word dropout of 0.4 is applied to input text for the decoder. The model modifies without restricting decoder capacity, using word dropout for smoothing. For non-VAE text generation, word dropout is used in the model. The encoder and decoder are pretrained with sequence-to-sequence text generation for a multi-prior distribution model. Comparisons are made with different numbers of components [50, 100, 500, 2000], showing performance is not highly affected by this hyperparameter. Overfitting and high complexity are concerns with many components in the prior distribution, so the model is evaluated with 500 components. Comparison is made with models from BID1 and BID30, using similar configurations. For the BID30 model, SCMM-VAE is used with pretraining of the encoder and Adam BID10 optimizer. The best performance is achieved with a learning rate of 5 \u00d7 10 \u22124 and \u03b2 1 of 0.5. KL weight annealing is done from 0 to 1 until epoch 30, followed by 80 epochs with learning rate decay. Semi-supervised learning methods with VAEs follow the same scheme as BID30, using LSTM initialized with an autoencoder and a language model. The structure of semi-supervised models using VAEs is adopted from BID30, where the topic of a sentence is used as a label for training the discriminator. Our model for semi-supervised learning does not differ from baselines, as it can generate proper sentences without label information. This aligns with the idea that the best models for language modeling and semi-supervised learning are different. Additional samples are provided in Table 7, including topics like the death penalty, violence, and traditional education. Table 8: Sampling from posterior distribution of our model when different texts are input to self-attention and Bag-of-Words encoders. The topics include providing the holy rabbit, going to a police officer, paying for a home, the president of the United States, and the Jewish religion cutting food. The Jewish religion involves cutting food. Components in Table 8 generate sentences with common or diverse structures. Component 1, 22, and 77 have similar grammatical structures in their sentences. The Jewish religion involves cutting food. Components in Table 8 generate sentences with common or diverse structures. Properly acquiring grammatical structure lowers reconstruction loss. Sentences from components 60, 68, and 83 are on sports, computer (music), and politics respectively, with diverse structures."
}