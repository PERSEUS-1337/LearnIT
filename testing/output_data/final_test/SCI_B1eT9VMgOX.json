{
    "title": "B1eT9VMgOX",
    "content": "In this paper, a systematic taxonomy for clustering with deep learning is proposed, along with a review of methods in the field. A new approach based on the taxonomy is introduced, addressing limitations of previous work. Experimental evaluation on image datasets demonstrates state-of-the-art clustering quality and improved performance in some cases. Clustering is a fundamental unsupervised machine learning problem with applications in data separation and automatic data labeling. Clustering algorithms are crucial for tasks like data labeling and data visualization. The performance of these algorithms depends on the input data, requiring different similarity measures and separation techniques. Dimensionality reduction and representation learning are often used alongside clustering to create more clustering-friendly representations. Recent research has shown that deep neural networks can learn non-linear mappings to transform data for better clustering. One of the main contributions is the formulation of a taxonomy of methods using deep learning for clustering, facilitating the overview of existing methods and the creation of new ones. A new method is proposed that combines advantageous properties of existing methods, using an autoencoder-based approach for learning better clustering-friendly data representations. The training involves two phases, with the second phase incorporating a loss function combining reconstruction loss and a clustering-specific loss. In the second phase, the network model is optimized, and clustering assignments are updated. The paper discusses the taxonomy of clustering with deep learning, related methods, and a new method based on this taxonomy. Successful clustering methods with deep neural networks focus on representation learning using DNNs and inputting these representations into a specific clustering method. The curr_chunk discusses the options for neural network training procedures, including the main neural network branch, deep features for clustering, and different types of losses. It also mentions cluster updates and re-running clustering after network training. The main branch of the neural network is used to transform inputs into a latent representation for clustering, with architectures like Multilayer Perceptron (MLP) commonly used for this purpose. Architectures like Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), and Deep Belief Network (DBN) are used for clustering and feature extraction in neural networks. DNNs serve for clustering as mappings to better representations, with features drawn from different layers of the network. The choice of using one layer or several layers in a network affects the richness of the representation and the complexity of semantic representations. Non-clustering loss functions can enforce constraints on the learned model, with options including no additional non-clustering loss functions. The network model is constrained by clustering loss requirements. Autoencoders consist of an encoder and decoder, where the encoder maps input to a latent space and the decoder reconstructs the input. Autoencoders can learn useful representations in cases of different output dimensions or injected noise. Autoencoders can be used for dimensionality reduction and reconstruction tasks. The reconstruction loss measures the distance between the input and its reconstruction. This loss function ensures that important information is preserved. In addition, training samples with targets can be used in non-clustering loss to encourage meaningful feature extraction. The clustering loss functions are specific to the clustering method and the learned representations' clustering-friendliness. Options include no clustering loss, k-Means loss to ensure even distribution around cluster centers, and using a neural network for changing input representation. A neural network is trained with a loss function to minimize the distance between data points and cluster centers. Cluster assignment hardening involves using soft assignments of data points to clusters, with a Student's t-distribution as the kernel for measuring similarity. This enforces soft cluster assignments for better clustering quality. The cluster assignment hardening loss enforces stricter probabilities for assignments by approaching an auxiliary distribution. This aims to improve cluster purity, emphasize high confidence assignments, and prevent distortion in the hidden feature space. The Kullback-Leibler divergence is used to measure the divergence between the two probability distributions. The balanced assignments loss enforces balanced cluster assignments by minimizing the probability of assigning data points uniformly across clusters. The locality-preserving loss aims to push nearby data points together to preserve cluster locality. The curr_chunk discusses different loss functions used in representation learning and clustering. It includes group sparsity loss inspired by spectral clustering, cluster classification loss using cluster assignments as class labels, and agglomerative clustering. The curr_chunk introduces an agglomerative clustering loss function for meaningful feature extraction in network layers. It merges clusters with maximum affinity in each step, optimizing affinity in the latent space through network training. The curr_chunk discusses the optimization of network parameters using a clustering loss function in combination with a non-clustering loss function. The weighting between the two functions, denoted by \u03b1, is a hyperparameter that can be adjusted during training. Methods to assign and schedule the values of \u03b1 include pre-training with \u03b1 set to 0 and fine-tuning with \u03b1 set to 1. The curr_chunk discusses the use of clustering loss in training neural networks, with a focus on adjusting the weighting parameter \u03b1. Different strategies for setting \u03b1 during training, such as joint training and variable scheduling, are explored. Additionally, the types of clustering methods are briefly mentioned. Hierarchical and partitional (centroid-based) clustering approaches are discussed in the curr_chunk. Hierarchical clustering builds a hierarchy of clusters, while partitional clustering groups data points based on cluster centers. Agglomerative clustering and k-means are the dominant methods in deep learning for clustering. Cluster assignments and centers are updated during network training. Cluster assignments can be updated jointly with the network model as probabilities or alternatingly in a separate step. The update process depends on the number of iterations and the frequency of updates in the clustering algorithm. After training converges, the network should have learned a mapping for easier clustering. Re-running clustering after training allows for clustering on similar datasets. Representation mapping on another dataset similar to the one used can lead to better clustering results after training. This is because older representations may be suboptimal, resulting in improved clustering performance post-training. Clustering with deep neural networks has gained interest due to the success of supervised deep learning. Clustering with deep neural networks has gained interest in recent years, with various approaches utilizing DNNs for preprocessing clustering inputs. DEC is a promising method that uses autoencoders and k-means for clustering, with a focus on pretraining the model using input reconstruction loss. The method involves pretraining the neural network with input reconstruction loss and fine-tuning it using cluster assignment hardening loss. Clusters are refined iteratively with high confidence assignments. The method has shown good results and is used as a reference for new methods. DCN is another autoencoder-based method that uses k-means for clustering. It pretrains the network with autoencoder reconstruction loss and jointly trains it with a combination of reconstruction loss and k-means clustering loss. The DBC method, similar to DEC but using convolutional autoencoders, alternates between network training and cluster updates. It outperformed DEC on the MNIST dataset due to its use of convolutional layers. JULE utilizes a convolutional neural network for representation learning and a hierarchical clustering approach. The CCNN method uses a hierarchical clustering approach for representation learning, with a period hyper-parameter that alters training behavior. It showed superior results on the MNIST dataset compared to other methods, but lacks a non-clustering loss which may be a disadvantage. The CCNN method utilizes a hierarchical clustering approach for joint clustering and representation learning. It initializes cluster centers with features from random images and updates them using k-Means. The CCNN parameters are updated based on assigned labels and predictions from the softmax layer. Additionally, other approaches in clustering with deep learning have been explored, such as using a standard autoencoder without clustering loss functions. In BID8, joint training was performed with various loss functions including an autoencoder reconstruction loss, a locality-preserving loss, and a group sparsity loss. Another similar work by BID4 added a balanced assignments loss to the clustering loss to prevent degenerate solutions. Multiple other methods like BID18, BID5, BID31, BID1, BID15, BID27, and BID2 also exist, utilizing neural networks for feature extraction and clustering. The curr_chunk discusses the development of a new clustering method using neural networks and a convolutional architecture for image datasets. The method involves pretraining with an autoencoder and utilizing a combination of taxonomy features to improve clustering results. The network training for the new clustering method involves two phases: pretraining with an autoencoder reconstruction loss and joint optimization of the autoencoder loss and cluster assignment hardening. This approach differs from previous methods by combining reconstruction loss with cluster assignment hardening, eliminating the need for alternating between joint training and clustering updates. The network training for the new clustering method involves two phases: pretraining with an autoencoder reconstruction loss and joint optimization of the autoencoder loss and cluster assignment hardening. This approach eliminates the need for alternating between joint training and clustering updates. The network output is used as input for the k-means method to produce final clustering results. Evaluation is done using clustering accuracy (ACC) and normalized mutual information (NMI) metrics. Training the network involved trying different architectures, network sizes, and tuning hyper-parameters. The learning hyper-parameters used include a learning rate of 0.01 with a momentum of 0.9, batch normalization, and L2 regularization. Experiments were conducted on MNIST and COIL20 datasets, showing clustering performance in terms of accuracy and NMI for various DNN approaches. The proposed algorithm performs comparably, if not better, than other methods. The proposed algorithm performs comparably, if not better, than other state-of-the-art approaches. Visualizations of clustering spaces at different training stages show the method results in more clustering-friendly spaces than the original image and autoencoder spaces. A taxonomy for clustering with deep learning is presented, along with a summary of methods in the field and their specific use of the taxonomy, facilitating the generation of new methods. The new method presented in this study builds upon a taxonomy for clustering with deep learning, allowing for the creation of clearer and more effective methods by combining different building blocks. It outperforms previous approaches, achieving state-of-the-art performance in some cases."
}