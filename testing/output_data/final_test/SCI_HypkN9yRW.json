{
    "title": "HypkN9yRW",
    "content": "We present a dynamic architecture with a differentiable forking mechanism for leveraging logical information in problem data structures. Our DDRprog model, applied to CLEVR Visual Question Answering, outperforms previous approaches with higher accuracy in fewer epochs and parameters. By directly modeling question logic and forking subprocesses, our architecture excels in tasks like counting and integer comparison. Deep learning is data-driven and can be applied to various tasks like visual question answering, scene recognition, language modeling, speech recognition, and translation. Neural architectures have been used to model different data structures, but convolutional and recurrent building blocks were designed with general notions of spatial and temporal locality. When hard logical assumptions are present, it is challenging to incorporate them into the model effectively. Incorporating discrete logic into deep learning models is challenging, especially when hard logical assumptions are present. While some success has been achieved for specific data structures, additional structure must be learned implicitly or available at both train and test time. Approaches like StackRNN, recursive NN, and TreeRNN allow for explicit tree structures during training and testing but have limitations in supervision. Our objective is to develop a general framework for differentiable, discrete reasoning over data structures, including stacks and trees. Our approach is flexible to differing degrees of supervision and demonstrates improved results when structural assumptions are available at test time. The framework is presented in the context of Neural Module Networks (NMN) and Neural Programmer-Interpreters (NPI). The framework presented allows for differentiable, discrete reasoning over data structures like stacks and trees. It has been applied in various architectures for visual question answering, introducing novel behaviors to improve model differentiability. The model selects modules based on loss gradient, enabling a gradient pathway for visual state influence. The architecture presented enables differentiable reasoning over data structures like stacks and trees, utilizing a novel forking mechanism for logical operations. It achieves strong performance on all CLEVR subtasks, leveraging program annotations for high-level reasoning tasks. The architecture presented enables differentiable reasoning over data structures like stacks and trees, achieving strong performance on all CLEVR subtasks. Leveraging additional program annotations, the model improves accuracy on important tasks, motivating more complex tasks over knowledge graphs. The objective is not just to increase accuracy on CLEVR but to enhance architectures' ability to leverage additional supervision for general visual reasoning. The architecture presented enables differentiable reasoning over data structures like stacks and trees, achieving strong performance on all CLEVR subtasks. Prior work on CLEVR is largely categorized by dynamic and static approaches. IEP and N2NMN both generalized the original neural module networks architecture and used functional annotations in CLEVR to predict a static program, while RN and FiLM are static architectures that incorporate implicit reasoning modules. In contrast, the new architecture uses program annotations to explicitly model the question structure and execute the corresponding functional representation. Our architecture enables differentiable reasoning over data structures like stacks and trees, achieving strong performance on all CLEVR subtasks. DDRstack is introduced as a second application of the framework, showcasing improved performance on questions requiring higher level operations. The reverse Polish notation (RPN) expression evaluation task is used to demonstrate the framework's ability to incorporate relevant stack structures differentiably. Despite quantitative differences, the RPN task is structurally similar to CLEVR VQA. The DDR framework combines discrete modular behavior with forking behavior inspired by NPI to leverage structural information in input data. It resolves differentiability issues and achieves moderate improvement on CLEVR and success on RPN tasks where LSTM fails to generalize. The CLEVR dataset includes 100k images with 1 million question/answer pairs, containing high-quality 3D renders of scenes with geometric objects. The dataset avoids biases and provides a program representation for each question, allowing for complex reasoning without external knowledge of natural images. The CLEVR dataset contains 100k images with 1 million question/answer pairs, featuring 3D renders of scenes with geometric objects. The dataset enables complex reasoning without external knowledge of natural images. Many objects are red or spheres, represented by a tree with branches for filtering red and spheres, followed by a binary union operation and a final count. Neural architectures have surpassed human accuracy on the task, motivating more complex synthetic tasks involving logical inference over general knowledge graphs. The future of visual Turing tests and training sets for such tasks require further iteration. The reverse Polish notation (RPN) expression evaluation dataset introduces a task where sequences of tokens in RPN are evaluated to produce a single real valued answer. This task simplifies the problem by removing the need to consider the order of operations, resembling a hash function for large expressions. The reverse Polish notation (RPN) expression evaluation dataset simplifies the problem by removing the need to consider the order of operations, resembling a hash function for large expressions. The objective is to make stronger structural assumptions about the problem and create an architecture to leverage them, modifying the problem to produce the sequence of answers to all intermediate expressions in the answer labels. The framework is incomparable to StackRNN, as it assumes the stack structure of the problem is available to the architecture. The dataset for reverse Polish notation expression evaluation consists of 100k train, 5k validation, and 20k test expressions with 11 numbers and 10 operations. A 20k expression generalization set with 30 numbers is also provided. The purpose of the DDR framework is to incorporate structured information into a neural reasoning architecture for better performance. Our framework integrates structured information into a neural reasoning architecture, enabling complex reasoning without additional supervision. It combines discrete logic with differentiable training and can work with various data structures. The framework merges the design patterns of IEP and NPI, allowing the model to learn program primitives and execute programs predictively. This approach is more versatile than using IEP or NPI alone, with a non-trivial differentiable forking operation for combining them effectively. Our approach, referred to as DDRprog and DDRstack for VQA and RPN tasks respectively, integrates structured information into a neural reasoning architecture. It allows the model to learn program primitives and execute programs predictively, addressing the limitations of existing architectures. The controller produces an index corresponding to a neural module at each timestep, enabling flexible problem supervision. The architecture adaptations require standard encoders to handle mixed language and visual data, with detailed pseudocode, visual representation, and subnetwork details provided. Our approach, DDRprog and DDRstack for VQA and RPN tasks, integrates structured information into a neural reasoning architecture. The model learns program primitives and executes programs predictively. The network uses LSTM and ResNet encoders for language and visual states, with a recurrent highway network (RHN) controller. The controller outputs softmax predictions for neural modules. The model integrates structured information into a neural reasoning architecture by using neural modules that are executed predictively. The architecture includes a differentiable forking mechanism and a small classifier network for module prediction. This approach allows the model to observe intermediate function operations on CLEVR, providing an advantage over static program compilation. The architecture includes a differentiable forking mechanism to handle logical branching operations in CLEVR programs. This mechanism allows the model to process each branch of the program with its own network copy. Our architecture includes a differentiable forking mechanism to handle logical branching operations in CLEVR programs. Each branch of the program is processed by a network copy with its own state. The fork module must differ from a standard unary module to pass original ResNet features to the subprocess in addition to the current visual state. The architecture includes a variant of the binary module that merges original ResNet features with the current visual state. The DDRstack model consists of an LSTM controller, 4 learnable binary modules, and an explicit stack for processing tokens in a fixed expression parse tree structure. DDRstack is a neural analog to RPN expression evaluation algorithm, with NUM and OP tokens processed differently. The model includes a baseline LSTM and a stack for processing tokens. The DDRstack model uses a baseline LSTM with a stack behavior for processing tokens. Predictions are made in the last n timesteps, and the model has 9M parameters. Ground truth program labels are passed during training but only used on the validation set for model selection. After training on a single GTX 1080 TI, our model achieves a state-of-the-art accuracy of 98.3 percent after 52 epochs. It predicts program cells with 99.98 percent accuracy and increases consistency across all tasks on the CLEVR dataset, which is crucial for high-level visual reasoning. The CLEVR dataset is considered the best proxy task for high-level visual reasoning due to its discrete program annotations. Despite achieving a modest improvement in raw accuracy with a smaller architecture, competitive models include RN, FiLM, IEP, and our architecture. Count and Compare Integer questions are identified as the hardest tasks, with the IEP result showing models do not ignore these question types. DDRprog achieves strong performance on both unary and binary tasks, with a significant improvement over the previous state-of-the-art on the Count subtask. Compared to IEP, DDRprog is 4x smaller and outperforms it on the challenging Count subtask. The model shows at least a 2x improvement on all unary tasks and closely matches binary performance. The lack of similar gains on binary task performance is attributed to the use of a singular fork module for cross-communication during prediction. Our model outperforms RN in all categories of reasoning, achieving a 2.6x reduction in overall error. It excels in Count operations and Compare Integer subtask, showing significant improvements. The effectiveness of high epoch counts on model performance remains uncertain. Our model, trained for 1000 epochs, outperforms FiLM and achieves a 4x improvement over it on Compare Integer questions. It eliminates Count deficiency and is 4x smaller than IEP. FiLM struggles with complex binary question structures, while excelling in Compare Attribute tasks. Our model outperforms FiLM on Compare Integer questions, eliminates Count deficiency, and is smaller than IEP. FiLM struggles with complex binary question structures but excels in Compare Attribute tasks. Our model expands neural architectures to leverage discrete logical and structural information, while FiLM is more applicable to low-supervision natural image tasks. In our architecture, we use a hidden dimension of 32 resulting in 17k parameters. Training with Adam at a learning rate of 1e-3, we achieve a test L1 error of 0.17 after 63 epochs. Comparing with a pure LSTM baseline with 9k parameters, we see a test error of 0.28. Increasing the LSTM hidden dimension to 128 (255k parameters) yields a test error of 0.24 after nearly 3000 epochs. Testing on sequences of length n = 10 and n = 30, both models predict values for the entire expression and all subproblems, with accuracy results shown in FIG2. The LSTM model fails on the RPN task, as shown in FIG2. While both small and large LSTM baselines initially match our model's performance on n = 10 dataset, the performance gap grows from n = 6 to n = 10. The n = 30 dataset highlights the failure, with the LSTM's performance significantly worse on the first few subproblems compared to the original task. This discrepancy is due to the question formatting, where leading subproblems do not correspond to the leading tokens of the question. The LSTM model struggles with the RPN task, failing to learn the stack structure and showing rapid error increase. In contrast, our model incorporates the stack assumption, resulting in a smooth generalization curve. The LSTM's lack of explicit knowledge of the problem structure hinders its performance compared to our model. The LSTM baseline struggles with the RPN task due to its lack of explicit knowledge of the problem structure. In contrast, our model incorporates the stack assumption, resulting in a smooth generalization curve. While StackRNN showed better generalization than LSTM on simpler tasks, it is unlikely to scale to RPN due to its complexity. Our dynamic approach efficiently incorporates additional supervision to achieve a dramatic increase in performance and generalization, unlike StackRNN which struggles to generalize to RPN. The DDR framework enables high-level reasoning in neural architectures by leveraging structural information and resolving differentiability issues with discrete logical data. Our work combines the modeling capabilities of IEP/NMN and NPI through a differentiable forking mechanism, demonstrating efficacy in two applications. DDRprog shows improvement over previous state-of-the-art on CLEVR with increased consistency and reduced model size. DDRstack succeeds on RPN where a larger LSTM fails to generalize. The architecture aims to refine versatility, including accurate modeling of the fork module. It enables modeling of complex data structures across various problems, promoting higher-level tasks in neural reasoning. DDRprog introduces architectural details of subnetworks and provides fine-grained layer details. The source code will be released upon publication. ResNetFeaturizer is utilized with specific operations like Add, ReLU, InstanceNorm, and Concatenate. The model includes filters with different sizes, colors, materials, and shapes, enabling accurate modeling of complex data structures for neural reasoning tasks. The program analyzes the number of small purple rubber objects behind a green metallic cylinder compared to those in front of a tiny matte block. The predicted answer is no for the former and yes for the latter."
}