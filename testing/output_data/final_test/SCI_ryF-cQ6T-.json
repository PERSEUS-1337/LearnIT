{
    "title": "ryF-cQ6T-",
    "content": "The study explores the use of tensor networks (TNs) and deep learning architectures for image recognition. Two-dimensional hierarchical TNs are trained using a MERA-based algorithm, overcoming scalability issues. This approach reveals connections between quantum many-body physics, quantum information theory, and machine learning. The TN states encode image classes into quantum many-body states, maintaining unitarity during training. The study investigates quantum features of tensor network (TN) states for image recognition, suggesting they could characterize image classes and machine learning tasks. This work could also identify quantum properties of artificial intelligence methods. Quantum theories and technologies have seen significant progress in solving complex problems using quantum many-body systems. Tensor networks are powerful tools for studying quantum many-body systems due to their exponential growth in the Hilbert space. The use of Tensor Networks (TNs) has revolutionized the study of quantum many-body systems by providing a more efficient ansatz to describe quantum states. TNs have enabled the numerical treatment of complex physical systems and have been compared to deep learning in terms of automated feature extraction and pattern recognition. This has led to significant advancements in both quantum theories and artificial intelligence methods. At a theoretical level, there is a connection between deep learning and the renormalization group, linking holography and deep learning. Neural networks can represent quantum states and have been used for dimensionality reduction and handwriting recognition. TNs have been applied to solve machine learning problems by transferring classical information into a product state in a Hilbert space. This allows for image classification into predefined classes, similar to the challenges faced in quantum many-body systems. The text discusses the use of tree tensor networks (TTN) in image recognition, inspired by quantum physics applications. TTN is more suitable for two-dimensional images compared to matrix product state (MPS). The algorithm aims to efficiently lower the exponentially large space of image possibilities to a polynomial one. The algorithm based on tree tensor networks (TTN) is inspired by quantum physics and aims to efficiently encode image classes as quantum many-body states. Testing on MNIST and CIFAR databases shows comparable accuracies to convolutional neural networks. Results suggest that a growing bond dimension leads to overfitting, and the representation in different layers changes abstraction levels similarly to deep learning models. The text chunk discusses how tree tensor networks (TN) can represent image classes as quantum many-body states, with the highest level of abstraction allowing for clear class separation. Fidelities between TN states from different classes are low, and entanglement entropy indicates class difficulty. TN is defined as a group of tensors contracted in a specific way, representing classical and quantum systems. MPS is a famous example of TN, written as a higher-dimensional vector. The MPS representation of physical states uses physical and virtual bonds to describe the physical Hilbert space. MPS is limited in 2D systems due to the area law of entanglement entropy. Tree tensor networks provide a hierarchical structure for 2D states to avoid the complexity of a large number of indexes. To represent tensors in a TN, symbolic and graphic conventions are used. Tensors are denoted by bold letters without indexes, with elements represented by specific symbols. Vectors and matrices are first- and second-order tensors with one and two indexes respectively. When multiplying tensors, common indexes are contracted. In the graphic representation, tensors are blocks connected by bonds, with dummy indexes shown as shared bonds connecting different blocks. This convention simplifies equations and operations involving tensors. In a Tensor Train Network (TTN), each local tensor has one upward index and four downward indexes. The TTN has a hierarchical structure and can be naturally built for 2D systems. An upward index is added to the top tensor for supervised machine learning labels. Data preparation involves mapping N scalars to the tensor product of N normalized vectors. The feature function in a Tensor Train Network transforms N scalars to a d N-dimensional vector space. The classification output is obtained by contracting this vector with the TTN, where the maximum value position determines the predicted image class. The cost function to be minimized can be the square error. The cost function to be minimized is the square error, defined as DISPLAYFORM7 where J is the number of training samples. Inspired by MERA BID35, a highly efficient training algorithm is derived. The central idea is to impose that \u03a8 is orthogonal, optimizing it in the valid subspace for classification. In MERA, tensors called isometries satisfy TT \u2020 = I, compressing space to a lower dimension. The cost function is the contraction of one TN, efficiently computed for a unitary transformation. This approach connects to condensed matter physics and quantum information theory. The MERA approach in condensed matter physics and quantum information theory involves isometries that compress space while maintaining orthogonality. Renormalization group flows are implemented by these isometries, ensuring unbiased compressions of the Hilbert space. Tensors in the TTN are updated alternately to minimize the cost function. The environment tensor E [k,n] is calculated by contracting Eq. (6) to find the cost function f = \u2212Tr(T [k,n] E [k,n]). The optimal point solution is T [k,n] = VU\u2020, with V and U from the singular value decomposition E [k,n] = U\u039bV\u2020. The update of one tensor involves calculating the environment tensor and its singular value decomposition. Tricks are used to speed up computations, saving intermediate results and normalizing vectors. The strategy for building a multi-class classifier involves the one-against-all classification scheme. The one-against-all classification scheme in machine learning involves training one TTN for each class to recognize if an image belongs to that class. The output is a two-dimensional vector, with a fixed label for a positive answer. For P classes, there are P TTNs. To classify image data, each pixel is mapped to a d-component vector using a feature map. The TTN was built with five layers using the MERA-like algorithm to train the model on the CIFAR-10 dataset, consisting of 60,000 32x32 RGB images in 10 classes. The images were transformed to grayscale for easier training, resulting in a trade-off of less information available for learning. The MERA-like algorithm was used to train a binary classification model investigating machine learning and quantum features. Input and virtual bonds greatly impact the representation power of TTNs, with the input bond determining learnability. TTNs represent a decomposition of a complete contraction, with the virtual bond determining approximation accuracy. The TTNs utilize quantum many-body theory to achieve higher entanglement, leading to better representation power. The deep learning network's feature extraction layers help in separating classes effectively. Visualizing the representation in two dimensions shows clear class separation, making classification easier. The TTNs show a similar pattern to deep learning, with clear separation at the highest level of abstraction. In testing TTNs on the MNIST dataset, increasing bond dimension improved training accuracy but decreased testing accuracy due to overfitting. Optimal bond dimension is crucial as it controls model complexity. Using a one-against-all strategy, a 10-class model was built for classification. Using the one-against-all strategy, a 10-class model was built to classify input images efficiently with minimal values of d TAB0 to reach 95% training accuracy. Quantum state representations were utilized to study images and related issues, starting with a cost function derived from Eq. (6) and understood through the notion of fidelity. The cost function in Eq. (6) is based on fidelity, measuring the distance between vectorized images and TTN states. The goal is to minimize this distance for optimal encoding. To avoid local minima, the one-against-all strategy is proposed for classifying images efficiently. The one-against-all strategy is used for classifying images efficiently based on fidelity between TTN states and vectorized images. The classification is determined by finding the \u03a6 [p] with maximal fidelity with the input image, without the need for orthogonality among {\u03a6 [p] }. The fidelity describes differences between quantum states encoding different image classes, with some cases showing small F p p values indicating approximate orthogonality. Results show large fidelity values in the TN due to data processing methods. The TTN renormalizes input vectors based on spatial locations, potentially creating a network model for image classes. Future investigations will explore these concepts further. Another key concept discussed is entanglement in quantum mechanics, a form of correlations. Entanglement in quantum mechanics is a key characteristic that distinguishes quantum states from classical ones. It is measured by the entanglement spectrum and entropy, which determines the minimal dimensions needed for precision in Tensor Networks. In image recognition, entanglement entropy signifies how much information of one part of an image can be gained by knowing the rest, essential for predicting image details using trained TTNs. Entanglement entropy in image processing using trained TTNs can recover damaged or compressed images. The entanglement spectrum is computed for each class in the MNIST dataset, showing entanglement between different parts of the images. The entanglement spectrum is determined by the singular values of the matrix M = L \u2020 T [K,1] in the TTN, where all tensors are orthogonal. The entanglement in TTNs is determined by the orthogonal tensors, with M having four indexes representing renormalized image space. Entanglement entropy is computed by splitting the system along x or y, showing varying predictability for different image parts. TTNs are effective for image recognition, with scalability and high precision, but learnability depends on input representation power. The learnability of the TTNs model depends on input bonds, while virtual bonds determine approximation. Hierarchical tensor networks abstract similarly to deep neural networks. Fidelity helps distinguish classes, and entanglement entropy characterizes problem difficulty. Future plans include fidelity-based unsupervised training and using entanglement entropy for accuracy characterization."
}