{
    "title": "rJxWxxSYvB",
    "content": "In artificial neural networks trained with gradient descent, the weight transport problem arises when the same weights are used for processing stimuli and calculating gradients during backward passes. This challenge has led to proposals like the feedback alignment algorithm, which uses random backward weights. However, for large networks, random weights may not be effective. A solution to this problem can be achieved through introducing discontinuity in a spiking system. A spiking system can solve the weight transport problem in neural networks. The algorithm used is a special case of an estimator for causal inference in econometrics. Empirical results show rapid approximation of backward weights to forward weights, improving learning performance on tasks like Fashion-MNIST and CIFAR-10. This simple learning rule allows neurons to establish correct backward connections, addressing the weight transport issue. Behavioral improvements in people and animals suggest that synaptic updates must correlate with the loss function gradient. The brain estimates gradients for synaptic updates by using feedback connections that mirror feedforward weights. This process is crucial for optimal learning in algorithms like REINFORCE and AGREL. In algorithms like AGREL, learning is optimal when feedforward and feedback connections are symmetric. Weight symmetry is crucial for biologically realistic gradient estimation algorithms. Solutions to the weight transport problem have been proposed for biological models. Several solutions have been proposed for biological models to address the weight symmetry problem, including hard-wired sign symmetry, random fixed feedback weights, and learning to make the feedback weights symmetric. Learning weight symmetry is considered more biologically feasible and leads to better training results compared to other methods. However, some proposed solutions do not work well in practice and rely on biologically unrealistic assumptions. Learning weight symmetry in biological models is a causal inference problem that can be addressed using methods from the causal inference literature, such as regression discontinuity design (RDD). This approach allows for estimating causality without the need for randomization or control of variables. Regression Discontinuity Design (RDD) is a promising method for estimating causal effects in biological neural networks by leveraging the discontinuity introduced by a threshold. It has been shown to produce unbiased estimators of causal effects in systems of spiking neurons. RDD can potentially solve the weight transport problem in spiking neural networks by using a learning rule for feedback synaptic weights. The algorithm developed for spiking neural networks leverages a neuron's spiking discontinuity to infer causal effects on downstream neurons. By aligning feedback synapse weights symmetrically with feedforward weights, it overcomes the weight transport problem. This leads to better weight symmetry and learning in neural networks compared to other algorithms. The novel algorithm for solving the weight transport problem in spiking neural networks leverages discontinuous spiking to align feedback synapse weights with feedforward weights. This approach improves weight symmetry and learning compared to other algorithms. Feedback alignment, while successful in shallow networks, struggles in deeper networks and complex computer vision tasks. Continuously updating feedback weights to match the sign of reciprocal feedforward weights can bridge the gap in learning performance between feedback alignment and gradient descent. In order to improve learning over feedback alignment, researchers have proposed a method to make feedback weights more symmetric to feedforward weights. This involves minimizing a cost function that ensures weight alignment, with a focus on the third term. The approach is based on the work of Lansdell & Kording (2019) and aims to minimize the third term by leveraging neurons' ability to estimate their causal effect on a global reward. In Kording (2019), neurons estimate their causal effect on a reward signal using RDD and a piecewise linear model. This technique is modified to infer the effect of spiking on downstream neurons, leading to a learning rule for feedback weights that improves training. The primary contribution is demonstrating that spiking neurons can accurately estimate the causal effect on downstream neurons. In this study, spiking neurons accurately estimate the causal effect on downstream neurons using a piece-wise linear model of feedback. A learning rule for feedback weights encourages weight symmetry, improving training and test accuracy in deeper networks. The approach aligns feedforward and feedback weights in a spiking neural network model, enhancing performance on various datasets. The RDD algorithm aligns feedback weights in spiking networks by training them using leaky integrate-and-fire (LIF) neurons at the start of each training epoch of a convolutional neural network. During each epoch, feedback weights are trained in the LIF network and then transferred to the convolutional network for training feedforward weights. The LIF network undergoes a 90-second training phase during the feedback training, with a subset of neurons receiving driving input to spike. The spiking network used for RDD feedback training closely matches the activity of the convolutional neural network, validating the approach of using a separate non-spiking network for training feedforward weights. During RDD feedback training, neurons in the network receive driving input to spike, enforcing sparse firing patterns. Neurons simulate post-synaptic responses and can also receive driving input instead of synaptic inputs. Total feedforward input to a neuron is defined by feedforward weights from other neurons. Neuron i is defined by feedforward weight Wij from neuron j, with voltage vi evolving based on leak and dendritic conductance constants. Input drive ui is modeled, and if vi passes threshold \u03b8, it spikes and resets to vreset = -1. Spiking inputs from downstream layer create post-synaptic responses q, used in causal effect estimation. An RDD window is initiated when voltage approaches threshold \u03b8, lasting T = 30 ms in simulated time. At the end of a 30 ms RDD window, the maximum input drive u and change in feedback \u2206q are used to fit a piece-wise linear model. Parameters are updated using gradient descent for linear regression to estimate the causal effect of neuron i spiking on neuron k's activity. The weight at the feedback synapse is then updated accordingly. The weight at the feedback synapse is updated based on a scaled version of \u03b2 ik, ensuring stability in the feedback weights between layers. The alignment of feedback weights with reciprocal feedforward weights is measured to assess causal effect estimates. Feedback alignment somewhat increases sign alignment during training but is ineffective in earlier network layers. The addition of an RDD feedback training phase greatly increases sign alignment between feedback and feedforward weights in all layers of the network, especially in earlier layers. RDD algorithm surpasses the current state-of-the-art algorithm for weight alignment by Akrout et al. It also adjusts feedback weights to match both sign and magnitude of reciprocal feedforward weights, improving weight alignment. The RDD feedback training phase improves weight alignment by adjusting feedback weights to match the sign and magnitude of feedforward weights. The learning rule minimizes the alignment cost function, leading to better weight alignment compared to feedback alignment. The RDD feedback training phase improves weight alignment by adjusting feedback weights to match the sign and magnitude of feedforward weights, leading to better weight alignment compared to feedback alignment. This improvement is evident in the network's performance on various datasets, showing backprop-level accuracy on both train and test sets. The algorithm presented updates feedback weights in a network of spiking neurons, leveraging spiking discontinuity to estimate causal effects between neurons. It enforces weight alignment and minimizes a loss function, improving learning performance on Fashion-MNIST and CIFAR-10 datasets. This approach shows promise in solving the weight transport problem in deep learning models. The algorithm updates feedback weights in a network of spiking neurons to estimate causal effects between neurons, potentially capturing polysynaptic effects. It aligns weights and minimizes loss function, improving learning performance on datasets. Future work should test the algorithm in networks of explicitly excitatory or inhibitory neurons. The algorithm updates feedback weights in a network of spiking neurons to estimate causal effects between neurons using an exponential kernel function. Weight scaling ensures spike rates in the LIF network stay within an optimal range for the RDD algorithm to converge quickly. The RDD feedback training paradigm involves providing driving input to subsets of neurons in each layer of a network of LIF neurons. This input is created using a Poisson process with a rate of 200 Hz. After 30 s, the driving input stops for that layer and the process repeats for the next layer. The training process on Fashion-MNIST and CIFAR-10 involved random cropping, flipping, and batch normalization. A comparison was made between sign alignment using the RDD algorithm and the Akrout et al. algorithm, with feedback weight updates based on mean firing rates between layers. The spike rates in the LIF network are compared with activities in the convolutional network when both are fed the same input, showing a strong correlation."
}