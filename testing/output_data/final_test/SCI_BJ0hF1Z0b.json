{
    "title": "BJ0hF1Z0b",
    "content": "Our work demonstrates the feasibility of training large recurrent language models with user-level differential privacy guarantees at minimal predictive accuracy cost. By incorporating user-level privacy protection into the federated averaging algorithm, we show that achieving differential privacy on datasets with a large number of users primarily increases computation rather than reducing utility. Our private LSTM language models maintain similar performance to un-noised models when trained on a large dataset. Recurrent models like LSTM and RNNs are essential in language modeling for various applications. Training on data from the right distribution is crucial due to the wide variation in language usage. Language data can be privacy-sensitive, containing personal information like passwords and messages. Language models need to balance accuracy with privacy concerns. Recent attacks on neural networks highlight the risk of memorizing unique patterns in training data. The main goal is to ensure privacy protection in trained models without compromising quality. The focus is on training models for next-word prediction in a mobile keyboard, using differential privacy to train on actual mobile usage data. The paper applies differential privacy to model training using user-adjacent datasets, providing formal guarantees of user-level privacy. Guidelines for parameter tuning when training complex models with privacy guarantees are offered based on extensive experiments. The goal is to ensure privacy protection in trained models without compromising quality, focusing on next-word prediction in a mobile keyboard. Differential privacy (DP) provides a formalization for releasing information from private data, ensuring limited knowledge for adversaries. It applies to machine learning by allowing the public release of model parameters with strong privacy guarantees. Adjacent datasets are defined based on the application context. Adjacent datasets in differentially private machine learning are typically defined at the example level, where two datasets are considered adjacent if one can be obtained by adding or removing a single training example from the other. However, for tasks like language modeling, protecting individual examples is insufficient as each word contributes to the training objective. Therefore, in this work, the focus is on protecting entire user histories in the training data using the definition of differential privacy. The definition of differential privacy is applied to protect user histories in the training set by ensuring user-level privacy through an appropriate adjacency relation. Model training that satisfies this privacy concept with respect to user-adjacent datasets ensures that the presence or absence of any specific user's data in the training set has an imperceptible impact on the parameters of the learned model, making it difficult for an adversary to infer the use of any specific user's data in the training process. The private algorithm discussed relies on FederatedAveraging (FedAvg) and the moments accountant for privacy protection in training deep networks on user-partitioned data. FedAvg, introduced for federated learning, involves training a shared model while keeping training data on users' devices. When training deep networks on user-partitioned data, local computation is essential to minimize communication rounds. FedAvg is effective even in datacenters with differential privacy, forming per-user updates averaged for the shared model. FederatedSGD uses large-batch SGD with \"microbatches\" from distinct users. FedAvg and FedSGD are iterative procedures for federated learning. Modifications are made to achieve differential privacy, including using random-sized batches, clipping per-user updates, and using different estimators for the average update. These changes apply to both algorithms, with FedAvg being the focus while showing that results also extend to FedSGD. The pseudocode for DP-FedAvg and DP-FedSGD includes different estimators for average updates and adding Gaussian noise. Randomly sampling users with bounded L2 norm is crucial for low privacy loss. The moments accountant is used to bound total privacy loss, making training on large datasets attractive. When estimating average quantities in variable-sized samples, the sensitivity of the query is affected. Weighted databases with user-associated rows and weights influence the final outcome. The main loop for DP-FedAvg and DP-FedSGD involves sampling users with probability q and using different user update functions. The moments accountant API is referenced for privacy control. The text discusses bounded-sensitivity estimation of per-user vectors in the context of privacy protection in federated learning. Two estimators are considered, one unbiased and the other matching the exact function with sufficient weight. Sensitivity is controlled by the query function, and a lower-bound on the denominator is necessary for privacy. If user data has bounded norm, the sensitivity of the estimators is bounded. The text discusses clipping strategies for multi-layer models in the context of privacy protection in federated learning. When user vectors are gradients from a neural network, updates need to be clipped to enforce a bound before applying functions. Clipping can be done for each layer separately due to vastly different L2 norms in updates. Clipping strategies are discussed for multi-layer models in the context of privacy protection in federated learning. Two strategies are considered: flat clipping and per-layer clipping with a hyper-parameter S that needs to be tuned. Clipping introduces bias, and adding Gaussian noise scaled to the sensitivity of the estimator can provide a privacy guarantee. The Moments Accountant of BID0 provides tighter privacy bounds for the sampled Gaussian mechanism compared to using ( , \u03b4)-DP bound with the advanced composition theorem. It upper bounds the total privacy cost of T steps of the Gaussian mechanism with noise N (0, \u03c3 2 ) for \u03c3 = z \u00b7 S, where z is a parameter, S is the sensitivity of the query, and each row is selected with probability q. Given a \u03b4 > 0, the accountant gives an \u03b5 for which this mechanism satisfies ( , \u03b4)-DP. The moments accountant from BID1 computes privacy loss using noise scale z = \u03c3/S and steps T. It leverages amplification via sampling for easier privacy attainment with larger datasets. Table 1 shows varying privacy guarantees based on key parameters. Increasing dataset size makes achieving high privacy levels easier. The dataset size impacts the privacy cost of a single query but not significantly for multiple queries. In evaluating DP-FedAvg for training an LSTM RNN in a mobile keyboard, the study varied noise, clipping, and the number of users per round to understand how privacy impacts model quality. Increasing the number of users per round has a small privacy cost as long as it remains a small fraction of the total dataset size. The sensitivity of an average query decreases as the number of users per round increases, allowing for noise levels that do not adversely affect the optimization process. The study evaluated DP-FedAvg for training an LSTM RNN in a mobile keyboard by varying noise, clipping, and the number of users per round to understand privacy's impact on model quality. Experimental results show that FedAvg offers better privacy-utility trade-offs than FedSGD, extending to larger dictionaries with minimal parameter tuning. The LSTM language model architecture aims to predict the next word in a sequence, utilizing a variant of the LSTM recurrent neural network trained on a fixed dictionary. The model uses n-gram models as a baseline for neural language models, with a focus on reducing size for mobile deployment. Evaluation is based on AccuracyTop1 metric. The dataset used is a large public dataset of Reddit posts, preprocessed to 763,430 users with 1600 tokens each, ensuring user-level privacy. In this section, the dataset was preprocessed to 763,430 users with 1600 tokens each. A small test set of 75122 tokens was used for evaluation. The impact of changes for achieving differential privacy in FedAvg was examined, providing a roadmap for applying differentially private training to new models and datasets. The FedAvg algorithm was used for these experiments. In experiments using the FedAvg algorithm, changes to the estimator and user sampling did not significantly affect the convergence rate. The algorithm processed 80 tokens per batch, with 1600 tokens per user in 20 batches per round. In experiments with the FedAvg algorithm, the impact of flat and per-layer clipping on convergence rate was investigated. Per-layer clipping distributed the budget equally across 11 parameter vectors. Adding Gaussian noise to the average update showed minimal loss in convergence early on, but a larger effect later. The experiments, sampling 100 users per round, did not provide a meaningful privacy guarantee. The sensitivity of estimatorf f was calculated as 0.2. In experiments with the FedAvg algorithm, the impact of flat and per-layer clipping on convergence rate was investigated. The sensitivity of estimatorf f was calculated as 0.2. To ensure differential privacy, noise was added based on the sensitivity of the estimator. By adjusting parameters such as the L2 norm bound and noise scale, differential privacy guarantees can be achieved, albeit at a higher computational cost. After investigating the impact of flat and per-layer clipping on convergence rate in FedAvg experiments, models were trained with different numbers of expected users per round to compare accuracy. Two models with S = 15 and \u03c3 = 0.012 were trained, one with C = 100 and one with C = 1250, showing almost identical accuracy curves. The second model achieved (1.97, 10 \u22129)-differential privacy after 3000 rounds, while the first model had a vacuous privacy guarantee. This observation allows for estimating the utility of models trained with larger q for better privacy guarantees without actually training them. Using the accuracy of models trained with C = 100, we can estimate the utility of private models trained with larger C. Experiments with varying levels of clipping S and noise \u03c3 at C = 100 show that models with \u03c3 = 0.003 and S = 15 are only slightly worse than the baseline model. Training with C = 5000 expected users per round achieves (4.6, 10 \u22129)-differential privacy. This approach allows for estimating the utility of LSTMs trained with differential privacy for different datasets and values of C. As training progresses, adjusting noise and clipping levels can improve model performance. Using smaller \u03c3 and S values as training advances can be effective. Comparing DP and non-DP models, while DP-FedAvg may not significantly impact accuracy, it can qualitatively affect predictions. The study evaluated the impact of noise on model predictions by analyzing the bias towards common words. The results showed that better models tend to use fewer common \"head\" words, with little difference observed from changing parameters until model quality is compromised. In this work, an algorithm for user-level differentially private training of large neural networks was introduced, focusing on a complex sequence model for next-word prediction. The algorithm was empirically evaluated on a realistic dataset, showing that private training is possible with minimal loss in utility but increased computation cost. This approach, combined with federated learning, offers significant privacy guarantees for real-world applications. Future work includes designing private algorithms for automating the tuning of clipping/noise tradeoff and applying the method to a wider range of model families and architectures. The algorithm introduced focuses on user-level differentially private training of large neural networks for next-word prediction. It aims to reduce computational overhead and extend to different model families. The proof of Lemma 1 shows bounds on the estimator, while Theorem 1 verifies correct privacy loss moments and composability. The algorithm focuses on user-level differentially private training of large neural networks for next-word prediction. It ensures correct privacy loss moments and composability by bounding the L2-norm of user updates. The training process involves selecting a vocabulary and using a fixed noise variance for privacy. Training a private language model from scratch involves discovering frequent words in the corpus and using a pre-selected dictionary of 10,000 words. The recurrent language model maps words to embedding vectors and composes them with the previous state to emit a new state vector and output embedding. The output embedding is scored against the vocabulary to compute a probability. The language model uses a shared embedding for input and output tokens, reducing model size by 40% for mobile applications. Word embeddings are constrained to have a fixed L2 norm of 1.0 to improve convergence time. The model has 1.35M trainable parameters. For efficient training, public proxy data like books or Wikipedia can be used initially, followed by differentially-private training on private data. Focus is on training a single model shared by all users, with potential for on-device personalization. Starting from random initialization with purely private data is a more challenging scenario addressed in experiments. The text discusses the personalization of models for each user by incorporating additional context like user information as a feature vector. The model's performance is evaluated using AccuracyTop1 and related metrics across various tasks and datasets, including the Reddit Comments Dataset. The goal is to limit the influence of any single author on the final model. The text discusses personalizing models for each user by incorporating user information as a feature vector. The model's performance is evaluated using AccuracyTop1 across various tasks and datasets, including the Reddit Comments Dataset. To limit author influence, a training set with users having at least 1600 tokens is used. The dataset chosen allows for reproducibility and understanding the impact of differential privacy. The text discusses comparing sampling strategies and estimators in private training. FedSGD is more noise-sensitive than FedAvg due to smaller updates. Experiments with FedSGD used B = 1600 and C = 50. Reasonable values for private training suggest the need for 1500 users per round. FedAvg might choose 1000 users per round. The ratio of clipping level to noise is crucial. The text discusses the impact of clipping and noise on private training using FedAvg and FedSGD. FedSGD requires more iterations to reach equivalent accuracy compared to FedAvg. Increasing the number of iterations significantly increases the privacy cost. Larger dictionaries do not significantly improve accuracy and do not require changing clipping and noise parameters. Despite having more parameters, experiments with adding an explicit L2 penalty on model updates did not show a positive effect in decreasing the need for clipping."
}