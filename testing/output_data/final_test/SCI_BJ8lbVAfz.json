{
    "title": "BJ8lbVAfz",
    "content": "Self-organizing principles have been a focus in early learning models but are rarely seen in deep learning architectures. A network model incorporating self-organizing maps into a supervised network shows how gradient learning leads to a self-organizing learning rule. This model is robust and applicable to various areas, resembling biological learning systems. Machine learning advancements, particularly with deep neural network models, have been made possible by faster computer technology and algorithmic improvements. Advancements in deep learning have enabled the execution of complex tasks that were previously impossible due to data volume or complexity. Learning good internal representations is crucial in deep learning, with early breakthroughs coming from unsupervised pretraining and fine tuning. While research focuses on efficiency and performance, there is a lack of understanding in the formation of internal representations in hierarchical neural networks. Restricted Boltzmann Machines and Autoencoders are used in constructing hidden layers of models like Deep Belief Networks and Deep Boltzmann Machine. In modern deep learning theories, the role of self-organizing mechanisms in hierarchical neural networks is not well understood. Topographical self-organization, observed in biological neural networks, may provide insights into learning and self-organization in artificial neural networks. A network combining self-organization aspects with a supervised model for classification is proposed, modifying the Restricted Radial Basis Function Networks with a softmax output layer trained on a crossentropic cost function for a probabilistic interpretation of class membership. The Softmax Restricted Radial Basis Function Networks (S-rRBF) is a modified network that combines unsupervised self-organization and supervised learning into a single learning mechanism. It shows compatible performance with other deep network architectures and emphasizes robustness over achieving the best performance in different applications. This highlights the importance of flexibility in learners. The proposed architecture of Softmax Restricted Radial Basis Function Networks (S-rRBF) demonstrates the emergence of self-organizing structures from supervised gradient learning. It offers insights into the relationship between unsupervised and supervised learning, showcasing internal representations in the competitive layer compared to other mapping techniques like t-SNE. The network can be scaled to deeper layers for tackling complex learning problems. The S-rRBF is a hierarchical neural network with one hidden layer, adopting a softmax output layer and cross entropy cost function. It integrates self-organization with supervised learning, offering a new perspective on artificial neural networks. The S-rRBF is a hierarchical neural network with one hidden layer that integrates self-organization with supervised learning. It uses a neighborhood function to generate output from hidden neurons, similar to Radial Basis Function Networks. The S-rRBF neural network uses a neighborhood function to generate output from hidden neurons. The network is trained to minimize cross entropy by adjusting connection weights to increase the probability of predicting the correct class. The S-rRBF neural network adjusts connection weights to increase the probability of predicting the correct class, utilizing a self-organizing process during supervised training. The S-rRBF neural network undergoes self-organization in the internal layer during supervised training, linking topological self-organization and supervised learning in a single model. This reveals the feasibility of a topographical structure for internal representation in hierarchical supervised learning of neural networks. The S-rRBF neural network undergoes self-organization in the internal layer during supervised training, where the direction of self-organization is regulated by the weight connections. Positive self-organization occurs when the weight leading to the output neuron associated with the true class is larger, while negative self-organization moves the reference vector away from the input. This process differs from SOM as it is label-free. The self-organization process in S-rRBF neural network is label-oriented, unlike SOM which is label-free. It is semi-supervised and does not require exact output error information. The network undergoes positive or negative self-organization based on weight connections. The term e \u2212 X(t)\u2212Wn(t) 2 triggers a dropout effect, resulting in a sparse network. The S-rRBF was tested on various machine learning benchmark problems. The S-rRBF neural network was tested on various machine learning benchmark problems and compared against three deep learning models. Results showed that S-rRBF generally compares favorably with the best performing deep model. Internal representations of S-rRBF for some benchmark problems were also shown. The internal representation of the S-rRBF for the Iris problem is compared to SOM and t-SNE in 2-D visualizations. The markers in the maps represent different classes, with overlapping data shown as \u00d7. The size of the marker indicates the number of data it represents. The S-rRBF's representation differs as it considers the topological self-organization during the learning process. The S-rRBF's internal representation is context-dependent, regulated by data labels. It reflects problem separability well, with high classification performance for easy problems. In contrast, Bank Marketing Data poses a challenge with overlapping data from contrasting classes. S-rRBF shows clear separation of classes, indicating a nice topographical representation. The Fashion MNIST dataset has the same characteristics as the traditional MNIST dataset but does not show clear class representations in its internal representations. The research demonstrates the possibility of building a hierarchical neural network with self-organizing topographical internal representations. The research shows that topographical self-organization can emerge as a result of supervised learning. The two processes are part of a single learning mechanism, distinguished by the layers where they occur. The internal self-organization is not fully unsupervised, but is influenced by the connection weights relative to the true label of the input. Experimental results indicate comparable classification performance to standard supervised networks. The model's performance is comparable to existing conventional models for diverse benchmark applications. Its 2-dimensional internal layer provides visual information on learning representations and can be expanded into deep networks. The visualization of internal layers offers new insights for machine learning."
}