{
    "title": "HkgSk2A9Y7",
    "content": "Large mini-batch parallel SGD is commonly used for distributed training of deep networks. Stochastic Gradient Push (SGP) utilizes a gossip algorithm called PushSum for approximate distributed averaging, allowing for more loosely coupled communications in high-latency scenarios. Although SGP introduces noise in the gradient, it converges to a stationary point of non-convex objective functions. Empirical validation shows the potential of SGP, such as completing 90 epochs of training ResNet-50 on ImageNet using 32 nodes with 8 GPUs per node in around 1.5. SGP completes 90 epochs on ImageNet in 1.5 hours with 10Gbps Ethernet, close to AllReduce SGD's accuracy. DNNs are crucial in image recognition and NLP, with SGD as the main training method due to computational demands. Efficient methods for training DNNs in large-scale computing environments involve a parallel version of SGD, where worker nodes compute local mini-batch gradients and calculate an inter-node average gradient using ALLREDUCE communication primitive or a central parameter server. The use of a parameter server for gradient aggregation can create bottlenecks and points of failure, while ALLREDUCE avoids these issues by computing the exact average gradient in a decentralized manner. Decentralized and inexact versions of SGD are explored to reduce computational overhead in distributed training, as exact averaging algorithms like ALLREDUCE can be problematic in high-latency or high-variability platforms. Various decentralized optimization algorithms leveraging consensus-based approaches have been proposed to address this issue. Recent work in decentralized optimization algorithms focuses on inexact distributed averages using less-coupled message passing algorithms. These methods allow for non-blocking, time-varying, and directed communication, unlike traditional static and symmetric approaches. This approach aims to reduce computational overhead in distributed training, especially in high-latency or high-variability platforms. This paper introduces Stochastic Gradient Push (SGP), a blend of SGD and PUSHSUM for distributed training of deep neural networks. Theoretical analysis shows SGP converges for smooth non-convex objectives. Experimental evaluation on ResNets trained on ImageNet with up to 32 nodes and 256 GPUs demonstrates its effectiveness. Key contributions include the first convergence analysis for SGP with smooth non-convex objectives, showing convergence to a stationary point at a rate of O 1/ \u221a nK. SGP achieves a convergence rate of O 1/ \u221a nK and outperforms ALLREDUCE SGD in high-latency scenarios, showing 88.6% scaling efficiency. It matches ALLREDUCE SGD in validation accuracy for up to 8 nodes and remains close for larger networks. In low-latency scenarios, SGP matches ALLREDUCE SGD in running time and exhibits 92.4% scaling efficiency. Compared to other approaches, SGP runs faster and produces models with better validation accuracy. The network of n nodes cooperates to solve the stochastic consensus optimization problem, aiming to minimize the average loss of a DNN with local data distributions. Nodes must reach a consensus on the solution while communicating to access information about objective functions at other nodes. The problem involves distributed training using large mini-batch parallel stochastic gradient descent. The network of n nodes cooperates to solve the stochastic consensus optimization problem using distributed training with large mini-batch parallel stochastic gradient descent. Nodes compute local stochastic mini-batch gradients and use ALLREDUCE for averaging gradients. An alternative approach using the PUSHSUM gossip algorithm for approximate distributed averaging is explored. The PUSHSUM algorithm is used in gossip algorithms for distributed averaging. Nodes stack initial vectors into a matrix for computing the average vector. Sparse mixing matrices are used to reduce communications, inspired by Markov chains theory. The ergodic limit of the chain is reached under certain conditions. The PUSHSUM algorithm in gossip algorithms for distributed averaging uses column-stochastic matrices for convergence. Symmetric matrices have practical implications but require caution to avoid deadlocks. Each node maintains an additional scalar parameter to update values, ensuring convergence to a limit. The stochastic gradient push (SGP) method for solving equations involves computing de-biased ratios at each node using PUSHSUM iterations. Each node maintains model parameters, PUSHSUM weights, and de-biased parameters for convergence. The stochastic gradient push (SGP) method involves local SGD steps followed by PUSHSUM for distributed averaging. Gradients are evaluated at de-biased parameters and used to update the PUSHSUM numerator. Communication occurs with messages containing PUSHSUM numerator and denominator values. The method focuses on sparse mixing matrices for low communication overhead. SGP involves local SGD followed by PUSHSUM for distributed averaging. Gradients update the PUSHSUM numerator. Communication includes PUSHSUM values. Sparse mixing matrices reduce communication overhead. SGP is equivalent to parallel SGD with identical initial values. Theoretical guarantees assume smooth, non-convex objectives. Assumptions include bounded variance of gradients and data distribution similarity. Mixing matrices are associated with graphs. SGP involves local SGD followed by PUSHSUM for distributed averaging with sparse mixing matrices reducing communication overhead. The algorithm converges if certain conditions are met, as shown by Theorem 1. The proof of Theorem 1 in Appendix C provides precise expressions for constants C and q, showing a linear speedup in the number of nodes with a sufficiently large number of iterations. Theorem 2, under the same assumptions, further demonstrates convergence as the number of iterations grows. Theorem 1 states that as K increases, de-biased variables z converge to the node-wise average x(k), leading to convergence to a stationary point at each node. Various methods like quantizing gradients and multiple local SGD steps aim to accelerate distributed training of DNNs by balancing exact and approximate distributed averaging. These methods introduce noise into SGD to reduce communication overhead and improve training speed. Combining quantized, inexact, and infrequent averaging approaches can lead to a trade-off between training speed and accuracy. Previous works have explored consensus-based methods for large-scale DNN training, with some focusing on synchronous consensus-based versions of SGD. Unlike other methods, PUSHSUM involves asymmetric message passing, providing the first convergence analysis for a PUSHSUM-based method in the smooth non-convex case. The decentralized parallel SGD (D-PSGD) method proposed in Lian et al. (2017) uses message passing that is inherently blocking. It requires more communication per iteration compared to PUSHSUM-based SGP. The convergence of SGP is shown to converge to a stationary point, similar to D-PSGD. Experimental comparison in Section 5 shows both methods find solutions of comparable accuracy. SGP is faster than ALLREDUCE SGD and D-PSGD in large-scale distributed computing environments with up to 256 GPUs. It outperforms D-PSGD in validation accuracy and is slightly less accurate than SGD when using a large number of GPUs. In a large-scale distributed computing environment with up to 256 GPUs, SGP is faster than ALLREDUCE SGD and D-PSGD. It outperforms D-PSGD in validation accuracy but is slightly less accurate than SGD with a large number of compute nodes. Experiments were conducted on 32 DGX-1 GPU servers with 8 NVIDIA Volta-V100 GPUs each, using different communication scenarios. The 1000-way ImageNet classification task was used as the experimental benchmark, training a ResNet-50 model. In a large-scale distributed computing environment with up to 256 GPUs, SGP is faster than ALLREDUCE SGD and D-PSGD. It outperforms D-PSGD in validation accuracy but is slightly less accurate than SGD with a large number of compute nodes. Experiments were conducted on 32 DGX-1 GPU servers with 8 NVIDIA Volta-V100 GPUs each, using different communication scenarios. The 1000-way ImageNet classification task was used as the experimental benchmark, training a ResNet-50 model. The implementation of SGP involves modifying it to use Nesterov momentum and leveraging the efficient NVLink interconnect within each server. Each node computes a local mini-batch in parallel using all eight GPUs and implements a local ALLREDUCE for efficient communication. In high-latency experiments, inter-node averaging is achieved using PUSHSUM over Ethernet after transferring the model to host memory. With a local mini-batch size of 256 samples per node, a single Volta DGX-1 server can perform around 4.384 mini-batches per second. The ResNet-50 model size requires 3.5 Gbit/s for transmitting one copy per iteration in the high-latency scenario with nodes communicating over 10Gbit/s Ethernet. In high-latency scenarios, communication becomes a bottleneck when a single 10 Gbit/s link carries traffic between multiple pairs of nodes. SGP outperforms synchronous and decentralized approaches like D-PSGD and ALLREDUCE SGD in terms of total training time, taking less than 1.6 hours for 32 nodes compared to 2.6 and 5.1 hours, respectively. Appendix B.2 shows that all nodes converge to models with similar training and validation accuracy using SGP. The best validation accuracy decreases for D-PSGD and SGP as the number of nodes increases. SGP's performance drops by 1.2% compared to SGD on 32 nodes, possibly due to noise from distributed averaging. Changing node connectivity can improve this issue. SGP has better validation accuracy than D-PSGD for larger networks. Comparison with AD-PSGD, an asynchronous method, is provided in TAB1. AD-PSGD is an asynchronous implementation of D-PSGD, providing speedups in training time without degrading accuracy. It runs slightly faster than SGP but with lower validation accuracy. Combining AD-PSGD with SGP using the PUSHSUM protocol could further speed up training. Future work could explore this promising approach. In experiments with SGD and SGP over InfiniBand 100Gbit/s, timing differences are minimal. Additional experiments on 32, 64, and 128 GPUs show SGP time-per-iteration remains constant with more nodes, while AllReduceSGD time increases. Communication graph topology impacts SGP validation performance with Ethernet 10Gbit/s. Increasing the number of neighbors in the communication graph improves the accuracy of SGP but also increases training time. SGP with 2 neighbors achieves better accuracy compared to SGP with 1 neighbor and gets closer to SGD's accuracy. However, the increased communication also leads to longer training times. Despite this, SGP with 2 neighbors is still faster than SGD and D-PSGD while maintaining better accuracy than SGP. SGP 2-neighbors is faster than SGD and D-PSGD, with better accuracy than SGP 1-neighbor. The proposed Stochastic Gradient Push method improves scaling efficiency in distributed deep learning by computing in-exact averages at each iteration. SGP converges to a stationary point at an O 1/ \u221a nK rate and achieves a linear speedup with respect to the number of nodes. Empirical results show SGP can be up to 3\u00d7 faster than traditional ALLREDUCE SGD over high-latency interconnect. SGP can be up to 3\u00d7 faster than traditional ALLREDUCE SGD over high-latency interconnect, matches the top-1 validation accuracy up to 8 nodes (64GPUs), and remains within 1.2% of the top-1 validation accuracy for larger-networks. Using a time-varying directed graph to represent inter-node connectivity, nodes communicate with peers at varying distances. Each node periodically transmits messages to specific peers based on their distance in the graph. Using a time-varying directed graph, nodes communicate with peers at varying distances by transmitting messages to specific peers based on their distance in the graph. Each node cycles through the same set of peers, transmitting to two peers from the list at each iteration in a full-duplex manner. In experiments where nodes communicate with peers at varying distances, each node sends messages to specific peers based on their distance in the graph. In one-peer-per-node experiments, each node sends to one neighbor at every iteration, with balanced communication load. In two-peer-per-node experiments, each node sends to two neighbors with three non-zero entries in each column of the matrix. In D-PSGD experiments, nodes in a time-varying undirected bipartite exponential graph send and receive messages from specific peers based on their node number. Odd-numbered nodes send messages to even-numbered nodes and wait for a response, while even-numbered nodes wait to receive a message from an unknown peer and send a response back. This setup is consistent with previous experimental setups. The directed exponential graph with SGP is used for distributed averaging, where nodes compute the average using the PUSHSUM algorithm. The convergence rate is related to the second-largest singular value of the matrix. Cycling through neighbors in the directed graph is proposed for the scheme. After proposing cycling through neighbors in the directed exponential graph for distributed averaging using the PUSHSUM algorithm, it is shown that after k = log 2 (n \u2212 1) iterations, all nodes reach the average due to the graph's excellent mixing properties. Comparatively, cycling through edges of a complete graph results in nodes being much further from the average. Stochastic matrix design is also considered, where nodes randomly sample one neighbor to send to at each iteration. Random schemes for distributed averaging in decentralized optimization are not as effective as deterministically cycling through neighbors in the directed exponential graph. With randomized schemes, communication load is not balanced, and nodes may not receive the same number of messages at every iteration. This impacts optimization as averaging occurs less quickly, affecting the overall process. When averaging occurs less quickly in decentralized optimization, nodes evaluate local gradients at different points, injecting noise into the process. Minimizing floating-point operations in each iteration by using uniform mixing weights for all neighbors can lead to worse performance in terms of errors. The choice of mixing weights in decentralized optimization affects spectral properties of the gossip algorithm. Each node runs gossip and computation threads to maximize resource utilization. Communication threads are used for inter-node network I/O, gossiping messages between nodes. When using InifiniBand-based communication, nodes communicate parameter tensors using GPUDirect RDMA to avoid unnecessary transfers. Each node initializes its model on a GPU and sets its push-sum weight to 1. Communication threads handle message transmission and aggregation, while computation threads update model parameters and push-sum weights. The communication threads handle message transmission and aggregation, while computation threads update model parameters and push-sum weights. The model parameters are converted to de-biased estimates for computing stochastic mini-batch gradients, which are then applied back to the biased model. Gossip is performed on biased model parameters, and the updated model parameters are asynchronously copied back for communication. The biased model parameters are updated using stochastic gradients with nesterov momentum. The learning rate, schedule, momentum, and weight decay follow recommendations from previous studies. Different settings are used for experiments with 32, 64, 128 GPUs compared to 256 GPUs. In the 256 GPU experiment with two peers-per-node, the original learning-rate and schedule are used. Nodes eventually converge to similar validation errors, achieving consensus in representing the same function. Scaling up the number of nodes results in fewer global iterations. When scaling up from 4 to 32 nodes in training Resnet50 on ImageNet using SGP over Ethernet 10Gbit/s, there is a reduction in total iterations by a factor of 8. Despite this, the validation and training accuracy of the 32 node runs remain close to that of the 4-node runs. Performance variability across nodes is observed during training, with initial divergence in validation errors among nodes. The Resnet-50 model on different nodes have validation errors that diverge initially but eventually converge. Experiments on 32, 64, and 128 GPUs show that SGP time-per-iteration remains constant with increasing nodes, while ALLREDUCE SGD time increases. SGP exhibits high scaling efficiency on Ethernet 10Gbit/s and Infiniband 100Gbit/s. SGP shows better scaling with increasing network size and is more robust to high-latency interconnect. The convergence rate analysis is divided into three parts, presenting upper bounds, proving Lemma 8, and providing proofs for Theorems 1 and 2. Preliminary results are extensively used in the analysis. The presentation of stochastic gradient push (SGP) in Algorithm 1 is from a local node perspective, but the update rule at the k-th iteration can be viewed globally. Matrices are defined to represent the values of all nodes at the k-th iteration. The 6th step of SGP can be expressed using these matrices, with T as the transpose of matrix Pk. A known result from control literature allows bounding the distance between de-biased parameters at each node and the node-wise average, assuming B-strongly connected mixing matrices P(k). Based on the assumption of B-strongly connected mixing matrices P(k), after \u2206B consecutive iterations, the product has no non-zero entries. Lemma 3 states that under mixing connectivity assumption, there exists a constant \u03bb and q such that for all nodes and iterations, a specific condition holds. This lemma is derived from Theorem 1 in BID1 and Wolfowitz's proof. Lemma 4 provides a bound for stochastic gradient under certain assumptions. Lemma 5 establishes another inequality under different assumptions. Bounds for various quantities are derived, leading to a final combined inequality. After deriving bounds for various quantities in Lemmas 4 and 5, a final combined inequality is obtained by grouping terms together. Lemma 6 provides a bound for the quantity Q(k)i, and Lemma 8 is presented as the main lemma for convergence analysis. Lemma 8 is the main lemma for convergence analysis, on which the proofs of Theorems 1 and 2 are based. It simplifies expressions involving expectations with respect to the random variable \u03bei. The conditional expectation E[\u00b7] is defined before presenting the proof of the lemma. Lemma 8 simplifies expressions involving expectations with respect to the random variable \u03bei. Let Assumptions 1-3 hold and let C > 0 and q \u2208 (0, 1) be defined constants. The random sequence {Xk} produced by the algorithm satisfies a certain inequality. The proof involves rearranging terms and substituting \u03b3 = nK. This leads to bounding terms based on the total number of iterations K. The proof of Theorem 2 involves using Lemma 6 and assumptions from Theorem 1 with stepsize \u03b3 = nK, where Big O notation accounts for all constants in the setting."
}