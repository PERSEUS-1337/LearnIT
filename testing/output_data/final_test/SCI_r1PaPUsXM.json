{
    "title": "r1PaPUsXM",
    "content": "This paper provides a rigorous analysis of trained Generalized Hamming Networks (GHN) and reveals that stacked convolution layers in a GHN are equivalent to a single wide convolution layer. This equivalence has theoretical implications related to the universal approximation theorem. Additionally, the use of deep epitomes in GHNs allows for network visualization and direct feature extraction without the need for regularized optimizations. The lack of comprehensive understanding of neural networks has led to various visualization techniques being developed to decipher their blackbox nature. These visualizations help in understanding the learning process and optimizing network performance by exploring the relations between input data and neuron activations. Visualization techniques are used to understand how information flows through neural networks. However, a critical question arises regarding the applicability of conclusions drawn from analyzing sample inputs to new data. To address this, an independent visualization tool is needed, but the complexity of neuron outputs makes this challenging. ReLU activations are input data dependent and can be removed from the generalized hamming network (GHN) based on fuzzy logic principles. This simplified network architecture allows for the analysis of neuron interplay based on connection weights only. Stacked convolution layers can be merged into a single hidden layer without considering inputs from previous layers. The equivalent weights of the merged GHN, known as deep epitome, are computed analytically without the need for learning or optimization processes. Deep epitomes from different layers can be applied to new data to extract hierarchical features in one step. Despite recent success, neural networks have been criticized for their blackbox nature. Neural networks, such as BID3, offer accurate approximations but lack a clear explanation of their workings. Various interpretations have been attempted, including logic inferencing in BID24 and the universal approximation framework in BID6. Training deep neural networks remains a trial-and-error process. Fuzzy neural networks (FNN) in the early 1990s aimed to combine neural networks with fuzzy logic for interpretability. Neural networks have also been used to generate membership functions and fuzzy inference rules. The mission to understand neural networks is ongoing, with different theories and approaches being explored. In the new millennium, the use of fuzzy neural networks (FNNs) with membership functions and fuzzy inference rules remains active. However, FNNs have been largely overlooked in the machine learning community, except for recent advancements like the Generalized Hamming Network (GHN) which demonstrated state-of-the-art performance on various tasks. This paper explores merging multiple stacked convolution layers into a single wide convolution layer, showing the preference for deep network structures over shallow ones. The paper discusses the preference for deep network structures over shallow ones, citing the universal approximation theorem. It also mentions the ability to reduce depth and increase width of network architecture while maintaining or surpassing accuracies of deep CNNs and residual networks. Various network visualization techniques are actively being developed to interpret high-level features and learn hierarchical convolutional features. The paper discusses the preference for deep network structures over shallow ones, citing the universal approximation theorem. Various network visualization techniques are actively being developed to interpret high-level features and learn hierarchical convolutional features. Visualization methods based on the analysis of examples raise questions about their applicability to new data. The concept of \"deep epitome\" is reminiscent of previous methods that extract essential elements to model and reconstruct images. The paper discusses the deep epitome of a trained GHN, utilizing the generalized hamming distance to quantify similarity between neuron inputs and weights. The method involves estimating a \"smooth\" mapping between epitome and input image pixels. The representation of neuron computing (w \u00b7 x + b) proposed by BID8 follows a specific condition analytically, leading to generalized hamming networks (GHN). In the context of fuzzy logic, GHD quantifies the equivalence between inputs x and weights w. The ReLU activation function in the GHN framework sets a minimal hamming distance threshold on neuron outputs. BID8 argued that ReLU activation is not essential in GHNs as bias terms are analytically set. Removing ReLUs had negligible impact on easy MNIST classification but slightly prolonged learning for CIFAR10/100. The paper focuses on GHNs without ReLUs for deriving deep epitomes from convolution layers. BID8 suggested analyzing GHNs using fuzzy logic inference rules, but no details were provided. The text discusses unraveling a deep Generalized Hamming Network (GHN) by merging convolution layers into a single hidden layer. It explains the convolution operation in terms of generalized hamming distance (GHD) and combining multiple convolution operations across different layers. The focus is on GHNs without ReLUs for deriving deep epitomes from convolution layers. The text also mentions extending the derivation to higher dimensions and provides examples of GHNs trained for image classifications. The hamming outer product is a non-linear pseudo outer product that is permutation equivalent. It can be extended to multiple tuples and has the property of associativity. The hamming convolution operation sums up corresponding entries and remains the same across partition subsets. The hamming convolution operation is non-linear and non-associative, inherited from the hamming outer product. It can be extended to multiple tuples and accumulates GHDs through convolutions. The conversion from outer products to convolution is non-invertible. The number of epitomes must match for inputs A and B, and the notation * denotes hamming convolution between epitome banks. The convolution of epitomes is discussed, with special cases and notation explained. It is shown that computing the convolution of tuples in stacked layers is possible without recovering individual entries. The non-linearity of hamming convolutions makes computing the composite challenging. The epitome of a hamming is introduced to illustrate this operation. The epitome of a hamming convolution is defined as a set of N pairs E = (g n , s n ), where g n is the summation of GHD entries from hamming convolutions, s n is the number of summands, and N is the length of the epitome. A normalized epitome has s n = 1 for all n, and can refer to input data x or neuron weights w. The summation of GHD entries g n is abstractly defined, and the outer product may operate on arbitrary tuples. The generalized hamming distance approach treats neuron weights as fuzzy templates and sets bias terms analytically. It induces an information enhancement and suppression mechanism, with the arithmetic mean operator used for aggregating evidences in fuzzy sets. The gradient of g(x, w) with respect to x is 1 \u2212 2w, leading to either enhancement or suppression of information in x based on w values. Prominent feature pixels in weights w gradually attain large values, influencing classification, while obscure pixels near 0.5 have minimal impact on the final decision. The experimental results in Section 4 support the information compression interpretation by BID30. The subsection discusses merging multiple hamming convolution operations into a single-layer deep epitome. Theorem 10 states that a generalized hamming network is equivalent to a bank of epitome, called deep epitome, computed by applying composite hamming convolution iteratively. The hamming convolution operation requires the same number of epitomes in the previous layer and channels in the current layer. Inputs are represented as normalized epitomes obtained by recursively applying equations to previous layer outputs. The non-linearity of hamming outer products complicates proving the associativity of epitome convolution. Equations (4) and (5) are crucial for setting bias terms in generalized hamming networks. The generalized hamming networks (GHN) set bias terms analytically instead of using optimization. Fuzzy logic inferencing with deep epitomes involves comparing input x with deep epitomes d using a fuzzy logic rule. Output scores in y indicate the degree of fuzzy equivalence between x and shifted d at different locations. This inferencing rule is applicable to single layer neuron weights or composite deep epitomes. It was proven that a single hidden layer can manifest the universal approximation theorem. The universal approximation theorem states that a single hidden layer network with non-linear activation can approximate any decision function, but it may be too large and fail to generalize correctly. Theorem 10 shows that a simplified single hidden layer network can be constructed from a trained GNH, providing a concrete solution. Deep epitomes extracted from GNHs trained with MNIST, CIFAR10/100 classification allow for visualization of hierarchical features during the learning process. Hierarchical features are learned online during the learning process, forming geometrical structures early on. Efforts are made to refine features for improved details, with a majority of pixel values remaining small while a small fraction accumulates large values to form prominent features over thousands of iterations. The observation of sparse features in deep epitomes is interpreted in terms of sparse coding and fuzziness. Neuron weights with low fuzziness correspond to prominent features, which decrease during the learning process. The reduced fuzziness aligns with minimizing classification errors, although it is not explicitly minimized. The internal representation of deep epitomes is input data independent, as seen in the learning of constellations of strokes in MNIST handwritten images. The deep epitomes in neural networks involve the extraction of hierarchical salient features using fuzzy templates and generalized hamming distance. This approach allows for the simplification of extremely deep convolution networks by extracting deep layer features in one step, potentially reducing computational and algorithmic complexities. The unique characteristic of GHNs is that deep layer features can be extracted without relying on features from previous layers. This novel network representation, called deep epitome, offers a promising avenue for future research exploration. In this paper, a novel network representation called deep epitome is proposed, equivalent to stacked convolution layers in generalized hamming networks (GHN). The representation provides a constructive manifestation for the universal approximation theorem, stating that a single layered network can approximate any decision functions with desired accuracy. Deep structures are essential for decomposing optimization problems into manageable sub-problems. Trained deep GHNs can be simplified for interpretability and reduced complexities. The paper introduces the concept of deep epitome, a network representation similar to stacked convolution layers in generalized hamming networks. It allows for the computation of deep epitomes at all convolution layers without using input data, enabling the extraction of hierarchical features in one step. The normalized epitome encodes the fitness between learned templates and inputs, offering a fuzzy logic interpretation that may shed light on the black box of deep learning. The paper introduces the concept of deep epitome, a network representation similar to stacked convolution layers in generalized hamming networks. It allows for the computation of deep epitomes at all convolution layers without using input data, enabling the extraction of hierarchical features in one step. The normalized epitome encodes the fitness between learned templates and inputs, offering a fuzzy logic interpretation that may shed light on the black box of deep learning. The hamming outer product is non-linear and defined as a pseudo outer product with certain properties, including associativity and iterated operation. The hamming convolution, denoted *, is a binary operation that sums up corresponding hamming outer product entries. It is non-linear, non-associative, and can be extended to multiple tuples. The partition subsets remain the same, and the operation is non-associative due to the summation of GHDs. The hamming convolution, denoted *, is a binary operation that sums up corresponding hamming outer product entries. It is non-linear, non-associative, and can be extended to multiple tuples. An epitome consists of a set of N pairs E = (g n , s n ), where g n denotes the summation of GHD entries from hamming convolutions, s n the number of summands, and N is the length of the epitome. A normalized epitome has s n = 1 for all n. Given two tuples x and y, eq. (4) allows one to compute summation of all hamming outer product elements. The convolution of epitomes E c = E a * E b is computed by merging elements of two epitomes via spatial convolution. This operation is associative, as proven by Theorem 6, and is crucial for deriving deep epitomes that factor out inputs x from subsequent convolutions with neuron weights w. The summation of two epitomes E c = E a E b is defined by element-wise summation, where the sizes of the input epitomes must be the same. This operation can be extended to multiple epitomes and is illustrated in Figure 2. The composite convolutions of multiple epitome banks are associative, as proven in Proposition 9. The associative property of composite convolutions of multiple epitome banks is proven in Theorem 10, leading to the concept of a deep epitome. This deep epitome is computed by applying hamming convolution iteratively to individual layers of epitomes, with specific requirements for valid operations. Deep epitomes are shown at layers 1, 2, and 3 for a GHN trained with MNIST classification at iterations 100 and 10000. Pseudo colour images represent features outputs for input RGB colour channels."
}