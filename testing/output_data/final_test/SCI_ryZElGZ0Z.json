{
    "title": "ryZElGZ0Z",
    "content": "The ability of an agent to discover its own learning objectives is crucial for artificial general intelligence. Breakthroughs in autonomous decision making and reinforcement learning have focused on clear goals, but learning extramural sub-tasks and auxiliary predictions can enhance task learning, transfer of learning, and the agent's representation of the world. A framework for discovery involves curating a collection of predictions to construct the agent's world representation, emphasizing stability over convergence. The importance of stability over convergence in developing an adaptive, regularized algorithm for discovering useful predictions autonomously is highlighted. The concept of representing an agent's knowledge as predictions has a history in machine learning, with predictive state representations (PSRs) being a method for modeling dynamical systems by predicting interactions between an agent and its environment. The PSR formalism offers a minimal collection of core tests and predictions as an alternative to learning the one-step latent dynamics of the world in a POMDP. General Value Functions (GVFs) introduced by BID23 allow for specifying and learning large collections of predictions using reinforcement learning value functions, enabling multi-step state contingent predictions. The agent's state representation is constructed from predictions and is useful for reward maximization tasks. GVFs allow for learning auxiliary tasks to improve the state representation, enabling the agent to make use of predictive representations. The text discusses the importance of discovering General Value Functions (GVFs) for autonomous agents to improve their state representation using predictive representations. Previous works manually specified GVFs, but autonomous agents need to discover them. Discovery methods have focused on options, which are small sub-tasks within an environment, rather than predictive representations. Our approach is inspired by representation search methods for neural networks, aiming to discover General Value Functions (GVFs) for predictive representations. The cascade correlation network learning approach proposes new hidden units over time to incrementally grow the network, showing superior performance in online supervised learning tasks. In this paper, a curation framework for GVF discovery is demonstrated, with algorithms for proposing and pruning GVFs. A set of GVF primitives is proposed for representation search, along with a regularized updating algorithm for pruning less useful GVFs. The approach aims to maintain stability in the representation while demonstrating the ability to prune less useful GVFs. The paper introduces a curation framework for discovering Generalized Value Functions (GVFs) and demonstrates the ability to prune less useful GVFs and corresponding predictive features. It explores the utility of GVF primitives as predictive features in partially observable domains, aiming to facilitate further development in predictive representations for long-term outcome prediction in an agent learning environment. The paper introduces a curation framework for discovering Generalized Value Functions (GVFs) and demonstrates the ability to prune less useful GVFs and corresponding predictive features. GVFs provide a predictive representation to overcome partial observability in agent learning environments. A General Value Function (GVF) consists of a target policy, discount function, and cumulant. The value function is defined as the expected return, where a GVF prediction could provide probabilities for different outcomes. The paper introduces a curation framework for discovering Generalized Value Functions (GVFs) to overcome partial observability in agent learning environments. GVFs consist of a target policy, discount function, and cumulant. The approach decouples the representation specification from the updating algorithm, allowing for the use of predictions to address partial observability without maintaining observation histories. GVF networks use predictions to address partial observability without storing observation histories. The prediction vector is computed based on current observations and previous predictions. Work has been done on learning the prediction function, including off-policy learning with GTD algorithms. In off-policy learning with GTD algorithms, GVF networks are used to estimate value functions for a policy from different behavior policies. In partially observable domains, tracking is crucial for accuracy even when the distribution is stationary. A new updating approach for GVF networks is proposed to address the non-stationary problem faced by agents in such settings. In off-policy learning with GTD algorithms, GVF networks are utilized to estimate value functions for a policy from various behavior policies. A new updating approach for GVF networks is proposed to tackle the non-stationary problem faced by agents in partially observable domains, focusing on the dynamical system treatment of the learning system with stochastic weight updates. The update in GTD algorithms for GVF networks involves a normalization constant dependent on features and target norm. It ensures descent direction reflection and stability in the system with a control input \u03b1 for weight evolution. The stepsize \u03b1 influences the update on each time step, maintaining a small norm for stability. AdaGain algorithm is introduced to estimate the stepsize \u03b1 for a dynamical system with off-policy learning updates and regularization. The update includes both TD and GTD, with concrete examples provided for TD(0). The algorithm ensures stability by adjusting the stepsize based on the underlying state dynamics and target weights. The GTD(\u03bb) algorithm for learning value functions involves using small, fixed stepsizes and stochastic gradient descent to minimize the update. It considers updates for vectors not dependent on w t, with a recursive relationship defined for the update norm and other parameters. The GTD(\u03bb) algorithm for learning value functions involves using small, fixed stepsizes and stochastic gradient descent to minimize the update. A recursive relationship is defined for the update norm and other parameters in DISPLAYFORM5 and DISPLAYFORM6, allowing for approximation of the stochastic gradient update without storing all previous samples. The gradient estimate for \u03c8 t\u22121 is based on \u03c8 t\u22121 computed on the last time step t \u2212 1, providing a reasonable estimate for the current stepsize despite accumulating inaccuracies in past gradients. The GTD algorithm uses small stepsizes and stochastic gradient descent to update value functions. An exponential moving average is used to forget outdated gradients and focus on more recent ones. Regularized GTD updates can reduce variance and weight on less useful features. Proximal updates with a regularizer and regularization parameter can be used to add regularization to GTD. The proposed regularizer, the clipped 2 regularizer, is chosen for its ability to prevent bias from shrinkage and allow high magnitude weights to be learned. The clipping threshold of 2 facilitates pruning without introducing non-differentiability, while still reducing the magnitude of less useful features. AdaGain with regularization (AdaGain-R) reduces weights on less useful features, proving its utility for pruning features and proposed GVFs. Tested on a robot platform with fixed image features, additional experiments in micro-worlds show its effectiveness in GVF prediction. The robotic platform used is a Kabuki rolling robot with an ASUS XtionPRO RGB and Depth sensor, receiving new images every 0.05 seconds. The agent receives new images every 0.05 seconds and constructs the state by sampling 100 random pixels. Each pixel's RGB values are tiled with 4 tiles and 4 tilings on each color channel, resulting in 4800 bit values. The goal is to learn the value function for a policy that always moves forward, with a cumulant of 1 when hitting a wall and a discount of 0.97 except when hitting the wall. The GVF is learned off-policy using GTD(\u03bb) and AdaGain-R with the same experience from the behavior policy. Results are averaged over 7 runs to determine if AdaGain-R can learn in this environment and reduce feature magnitudes. In this environment, AdaGain-R effectively prunes features by generating a large collection of GVFs and refining the representation through replacement. The algorithm tracks return accurately and reduces error over time. AdaGain-R only has significant magnitude on half of the features, demonstrating its utility in the curation framework. To enable generation of GVFs for effective predictive representation, GVF primitives are introduced as modular components. Basic types for discounts, cumulants, and policies are proposed, including myopic, horizon, and termination discounts. These components allow for the definition of structures within networks of GVFs. In the context of introducing GVF primitives for predictive representation, various types of cumulants and policies are explored, including stimuli cumulants, compositional cumulants, random cumulants, random policies, and persistent policies. These components are essential for defining structures within networks of GVFs. In the context of introducing GVF primitives for predictive representation, various types of cumulants and policies are explored. A GVF could consist of a termination discount, an inverted stimuli cumulant for observation one, and a persistent policy with action forward. Experiments are conducted on a discovery approach for GVF networks in Compass World BID20, a partially observable grid-world with different colored walls. The observation vector is five-dimensional, consisting of an indicator bit if the color is observed or not. The performance of the learned GVF network is tested. The performance of the learned GVF network is tested for predicting the probability of seeing specific colors in the environment. The GVFs are randomly generated from primitives, and stimuli cumulants are based on observations of 0 or 1. The GVF network, using randomly generated primitives, accurately predicts difficult GVFs with as few as 100 GVFs in the network. Pruning the bottom 10% of GVFs every two million steps and replacing them improves performance. Compositional GVFs are beneficial for later learning stages. The study proposed a methodology for discovering GVF networks to learn predictive representations, using an algorithm called AdaGain for stability and pruning. Results show the effectiveness of this curation strategy. The system could be improved in terms of learning, generation, and pruning approaches, with potential for further development. In developing a learning strategy, the importance of treating predictive representation as a dynamic system is emphasized. Unlike standard supervised learning, these representations cannot be turned off and on as they progressively build accurate predictions. A focus on stability of the predictive system was proposed, with an algorithm derived to learn a stepsize as a control input to stabilize the system. More complex control inputs, such as one based on the current agent state, could be considered for a more reactive approach, requiring a more complex stability analysis from control theory. The discovery experiments involved a slow process of building predictive representations over millions of steps, with a simplistic pruning strategy. More informed pruning approaches could lead to quicker learning of compact GVF networks. The long-term endeavor of building such representations should be a focus, with potential exploration of scaffolding techniques to enhance learning efficiency. The discovery experiments involved building predictive representations over millions of steps with a simplistic pruning strategy. More informed pruning approaches could lead to quicker learning of compact GVF networks. The system did not treat compositional GVFs differently when pruning, leading to a rise in prediction error at about 80 million steps. There is potential to design other GVF primitives that may be amenable to composition, considering approaches to deal with partial observability such as using history. Predictive approaches like PSRs and TD networks have been shown to overcome issues with short history in determining distance from a wall. PSRs compactly represent state through action-observation sequences and probabilities. PSRs can be extended with options and core tests, and represented as GVF networks with myopic \u03b3 = 0. Core tests involve computing probabilities of observations given action sequences. The text discusses how GVF networks can be used to predict sequences of observations given sequences of actions. It contrasts GVF networks with PSRs and TD networks, highlighting their different approaches to making predictions. TD networks with options were introduced to generalize to temporally extended actions. Options have terminating conditions and a fixed discount during execution. GVFs allow for more general discount functions. TD networks also have a condition function and can learn action-values. The key differences between GVF networks and TD networks lie in their approach to predicting outcomes. The key differences between GVF networks and TD networks lie in how question networks are expressed and answered. GVF networks are easier to specify using the language of GVFs, making it simpler to apply learning algorithms. TD networks have algorithmic extensions not covered by GVFs, such as TD networks with traces. The proximal operator for a function R with weighting \u03b1\u03b7 is defined as 2 + \u03b1\u03b7R(u), allowing for proximal gradient updates even with nonconvex regularizers like the clipped 2 regularizer. The proximal operator for a function R with weighting \u03b1\u03b7 is defined as 2 + \u03b1\u03b7R(u), allowing for proximal gradient updates even with nonconvex regularizers like the clipped 2 regularizer. The derivation of AdaGain with a proximal operator is similar to the derivation of AdaGain without regularization, with the only difference in the gradient of the weights. Using a subderivative in stochastic gradient descent update seems to perform well. The proximal operator for a function R with weighting \u03b1\u03b7 is defined as 2 + \u03b1\u03b7R(u), allowing for proximal gradient updates even with nonconvex regularizers like the clipped 2 regularizer. AdaGain-R has specific parameters for step size, regularization, and meta-stepsize. An experiment was conducted in a six state cycle world domain to investigate the system's ability to ignore dysfunctional predictive features within the network. In a six-state cycle world, the agent progresses deterministically with one state having an observation of one. Seven GVFs are defined for the GVF network to learn, with six corresponding to the cycle states. The first GVF predicts the observation bit for the next time step, while the second GVF predicts the prediction of the first GVF. The second GVF's target for the next time step is '0', aiming to predict the observation bit in two time steps. The cumulant for the third GVF is the second GVF's prediction for the next time step. Improved stability is shown in Figure 4 with the addition of components to the system. AdaGain can maintain stability without a nonlinear transformation, but a more aggressive small meta-step size is needed. Regularization significantly enhances stability and convergence rate. The seventh GVF is utilized more aggressively after Phase 1, and a nonlinear transformation allows the system to react quickly once the seventh GVF is accurate. Without regularization, weights are more spread out, but performance remains consistent. Regularization enhances stability and convergence rate in the system. The addition of the seventh GVF allows the network to reach zero error, making it suitable for testing. In experiments testing algorithms, the addition of the seventh GVF enables stable learning even when perturbed with noise. AdaGain adjusts step size for other GVFs to maintain stability. The addition of the seventh GVF in experiments enables stable learning with noise perturbations. AdaGain adjusts step sizes for other GVFs to maintain stability, preventing instability seen in previous work. The system effectively handles dysfunctional features and demonstrates the ability to prioritize more useful GVFs in the compass world domain. In experiments with GVFs defined in BID20, 155 GVFs produce noise sampled from a gaussian. 20 GVFs are pruned every two million steps based on feature weights. Handcrafted expert GVFs contain useful information and should be utilized more. AdaGain-R removes dysfunctional GVFs first, and pruning expert GVFs doesn't damage representation until the penultimate prune. Pruning dysfunctional or unused GVFs is not harmful to the learning task. Instability at the end of learning can be overcome by generating new GVFs to replace pruned ones. To generate new GVFs to replace pruned ones and prune based on network size used as representation."
}