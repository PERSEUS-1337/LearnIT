{
    "title": "rJl_NhR9K7",
    "content": "Recent work has focused on using the Variational Autoencoder (VAE) framework to discover interpretable representations of data in an unsupervised way. Methods like beta-VAE simplify variational inference but can lead to over-pruning and over-orthogonalization. This paper proposes modifying the probabilistic model to encourage structured latent variable representations by employing a rich prior distribution that breaks rotational symmetry. The proposed prior in the study addresses the trade-off between reconstruction loss and disentanglement in models like beta-VAE and TCVAE. It significantly improves both disentanglement and reconstruction quality over existing methods. Recent interest in unsupervised learning of disentangled representations aims to identify latent variables for true factors of variation and enhance interpretability. The \u03b2-VAE concept, which re-weights terms in the ELBO objective, has inspired much of this work. The modified ELBO objective in BID4; BID5 addresses the trade-off between disentanglement and reconstruction loss in models like \u03b2-VAE and TCVAE. By increasing the \u03b2-weight of the KL-term, the relative weight of the reconstruction loss term is decreased, leading to lower KL-divergence from the prior but higher reconstruction loss. This trade-off is crucial for optimizing latent encodings in the learned model. In section 2.4, a higher weight for the KL-term can amplify biases in variational inference. Many approaches to disentangling deep generative models face a foundational contradiction due to the standard model's lack of identifiability. The current state of the art approaches balance reconstruction loss and disentanglement of the latent representation. Variational inference techniques exhibit bias towards orthogonal effects on data components. In response to biases in variational inference, a novel approach using rotationally asymmetric distributions for the latent prior is proposed to remove rotational ambiguity. This method aims to achieve a higher level of disentanglement in the latent space, allowing for independent subspace analysis and overcoming the trade-off between disentanglement and reconstruction loss in existing approaches. The proposed prior distribution enhances disentanglement in latent space, surpassing existing approaches like \u03b2-VAE and \u03b2-TCVAE. Previous work on variational inference in deep generative models and modifications to learning objectives are discussed, highlighting biases in variational inference. Training of the Variational Autoencoder involves optimizing the evidence lower bound (ELBO) over the empirical distribution. The evidence lower bound (ELBO) is optimized by a deep learning model with parameters \u03b8 and sampled encoder z l \u223c q \u03c6 (z|x). The approximate posterior q \u03c6 (z|x) is modeled as a multivariate Gaussian with diagonal covariance matrix. \u03b2-VAE modifies the ELBO objective by penalizing the KL-divergence with a free parameter \u03b2 > 1 to encourage disentangled representation. The parameter \u03b2 introduces a trade-off between reconstruction loss and disentanglement in representations. A new variant called \u03b2-TCVAE decomposes the ELBO into three parts, allowing for higher MIG scores compared to other models like InfoGAN and FactorVAE. The authors propose excluding the mutual information term when reweighting the KL-term with the \u03b2 weight. They introduce a quantitative evaluation score for disentanglement, the mutual information gap (MIG), which measures the difference in mutual information between the highest and second highest correlated underlying factor. Recent work has focused on learning interpretable representations, with Chen et al. (2016) presenting a GAN variant that maximizes mutual information between observation and a subset of latent variables. The approach optimizes a lower bound of mutual information, with various learning objectives proposed. Some research suggests finding a simpler data representation instead of a perfect generative model. Semi-supervised approaches require knowledge of underlying data factors. The \u03b2-VAE has different interpretations, enhancing statistical understanding. VI enhances statistical biases to produce disentangled representations by biasing parameter estimates away from maximum likelihood towards settings that reduce statistical dependence between latent variables in the posterior. This bias leads to learned components being biased towards orthogonal directions in the output space. The bias introduced by Variational Inference (VI) towards orthogonal directions in the output space can lead to learned components being pruned out, especially in more complex models where true posterior dependencies are significant. In complex models, biases from Variational Inference can lead to component pruning due to dependencies. In \u03b2-VAE with \u03b2 > 1, biases grow, affecting optimization and leading to components being pruned out or set to be orthogonal. In contrast to standard methods, an approach for unsupervised learning of disentangled representations is proposed using specific prior distributions for identifiable and interpretable models. These priors are not rotationally invariant, allowing interpretability of the latent space. Independent Component Analysis (ICA) is used to factorize a distribution into non-Gaussian factors, avoiding ambiguities of latent space rotations. The approach for unsupervised learning of disentangled representations proposes using non-Gaussian distributions as priors for latent variables to avoid rotation ambiguities. A generalized version of Independent Component Analysis (ICA) utilizes exponential power distributions to represent generative factors, with the choice of leptokurtic or platykurtic distributions influencing latent dimension orientations. The use of leptokurtic and platykurtic priors in unsupervised learning influences the orientation of latent dimensions for encoding the (x,y) location of sprites in the dSprites dataset. Leptokurtic distributions favor diagonal projections, while platykurtic distributions align with orthogonal projections. This choice impacts the spatial representation of sprites in the dataset. The choice of prior distributions, such as leptokurtic and platykurtic, influences the orientation of latent dimensions in unsupervised learning. Leptokurtic priors favor diagonal projections, while platykurtic priors align with orthogonal projections, impacting the spatial representation of sprites in the dataset. The normal distribution is a special case of L p -spherically symmetric distributions, with the only L 2 -spherically symmetric distribution having independent marginals. The text discusses using symmetric distributions as prior distributions for p(z) in experiments. It introduces a more general prior, the family of L p -nested symmetric distributions, to include independence between subspaces and dependencies within them. The text also mentions a visualization of this distribution as a tree structure and provides a formula for the surface area of the L p -nested sphere. The family of L p -nested distributions allows a generalization of ICA called independent subspace analysis (ISA), which uses a subclass of L p -nested distributions defined by functions of a tree structure. Sinz & Bethge showed that the subspaces become independent when using a radial distribution, interpreted as a generalization of the Chi-distribution. The proposed latent prior in ISA-VAE allows for defining independent subspaces in the latent space, with the ELBO-objective estimated using Monte-Carlo sampling. This approach can also be combined with the \u03b2-TCVAE method to compute the index code mutual information. BID5 propose a Monte-Carlo sampling approach called minibatch-weighted sampling for sampling from the generative model's prior distribution. During training, sampling from the approximate posterior q\u03c6 remains a multivariate Gaussian distribution, allowing for the application of the reparameterization trick. Experiments show that the proposed prior supports unsupervised learning of disentangled representation even with the unmodified ELBO objective (\u03b2 = 1). The influence of the proposed prior distribution on disentanglement and reconstruction quality is evaluated on the dSprites dataset. The ISA prior leads to a disentangled representation even without modifying the ELBO. Combining the ISA-model with \u03b2-TCVAE results in a high disentanglement score. Combining the ISA-model with \u03b2-TCVAE achieves a high disentanglement score of MIG = 0.54, the highest reported for dSprites. The MIG metric measures how well latent dimensions are separated and their interpretability, outperforming other disentanglement metrics. The MIG metric is more effective for quantifying reconstruction quality than the ELBO. Comparisons are made with \u03b2-TCVAE and \u03b2-VAE in experiments using the same architecture as in previous work. The ISA-layout is chosen following the practice of Sinz et al. (2009b). The ISA-layout, following Sinz et al. (2009b), supports unsupervised learning of disentangled representations. The latent space is interpretable with \u03b2 = 1, achieving a high disentanglement score of MIG = 0.54 when combined with \u03b2-TCVAE. This prior encourages disentangled representations, leading to the highest reported score for dSprites. The proposed prior encourages disentangled representations, as shown in a quantitative comparison between ISA-VAE and ISA-TCVAE with their counterparts \u03b2-VAE and \u03b2-TCVAE. Varying the parameter \u03b2 between 1 and 6, the proposed approaches achieve higher disentanglement scores compared to the original approaches, even when choosing the best value of \u03b2 for each method. The proposed ISA-VAE and ISA-TCVAE approaches achieve higher MIG scores and better reconstruction quality compared to their respective counterparts, even for smaller values of \u03b2. The trade-off between disentanglement and reconstruction loss is improved with the proposed prior, as shown in the comparison with \u03b2-VAE and \u03b2-TCVAE. The proposed ISA-TCVAE approach improves reconstruction quality significantly while enhancing disentanglement compared to the baseline method \u03b2-TCVAE. The structured prior used enables the definition of independent subspaces in the latent space, leading to higher quality reconstructed images. The proposed prior in training deep generative models enhances interpretability of the latent space by combining it with existing approaches for unsupervised learning. Varying parameters and using lepto-and platykurtic distributions lead to different representations in the latent dimensions, with platykurtic priors achieving higher MIG scores. Platykurtic priors with x and y coordinates achieve higher MIG scores. Different values of parameters and layouts were tested to study their impact on reconstruction quality. The configuration with p0 = 2.1, p1 = 2.2, and l1 = 4 showed the best results for both ISA-VAE and ISA-TCVAE models. The ISA-TCVAE model involves sampling auxiliary variables from a Dirichlet distribution and generating images through latent traversals along different axes. Reconstructed images for various models like \u03b2-VAE and ISA-TCVAE are compared. The ISA-TCVAE model involves sampling auxiliary variables from a Dirichlet distribution and generating images through latent traversals along different axes. The images reconstructed with \u03b2-VAE and \u03b2-TCVAE appear noisier compared to the ISA prior, which allows for more detailed reconstruction of the heart shape. The models were trained using the Adam optimization algorithm with a learning rate parameter of 0.01. The architecture of the encoder includes linear layers with Tanh activation functions. The encoder and decoder architecture is similar to BID5. Toy examples reveal biases in variational methods, such as VI breaking degeneracy in factor analysis models and \u03b2-VI returning identical fits regardless of \u03b2 setting. VI biases away from true component directions in over-complete ICA models, while \u03b2-VI with \u03b2 = 5 prunes components. The \u03b2-VAE optimizes the modified free-energy with respect to parameters and variational approximation. When M = 1, the \u03b2-VAE attaches replicated observations to each latent variable for standard variational inference. In real applications, \u03b2 is typically set to a value greater than one, reducing the effective data points per latent variable. The \u03b2-VAE modifies the dataset by reducing the effective data points per latent variable to less than one. Standard VI is then applied to these modified data. In the factor analysis model, the true posterior is Gaussian, and the true log-likelihood of the parameters is defined. The posterior in the factor analysis model is Gaussian, with the mean being a linear combination of observations. Mean-field variational inference returns a diagonal Gaussian approximation to the true posterior. The recognition weights and posterior variances are the same for all data points. The free-energy includes a reconstruction term and a KL regularization term. Different objective functions and posterior distributions are considered to reason about parameter estimates. In factor analysis, mean-field variational inference returns a diagonal Gaussian approximation to the true posterior. The weights are rotated to compute log-likelihood and free-energy, leading to factorized posteriors when orthogonal. The weights are rotated to compute log-likelihood and free-energy in factor analysis. Different settings of the weights lead to factorized posteriors, with orthogonal settings preferred by the free-energy. Variational inference recovers the same weight directions as the PCA solution. In factor analysis, different weight settings break symmetry in log-likelihood, leading to orthogonal latent components with factorized posterior distributions. The optimal solution for variational inference involves retaining two orthogonal components and setting the magnitude of the third component to zero, resulting in a tight bound with no KL cost. In factor analysis, different weight settings break symmetry in log-likelihood, leading to orthogonal latent components with factorized posterior distributions. The optimal solution for variational inference involves retaining two orthogonal components and setting the magnitude of the third component to zero, resulting in a tight bound with no KL cost. In a model example, varying \u03b1 from 0 to 1/2 affects the free-energy, with changing \u03b2 reducing fluctuations but not altering directions. Increasing \u03b2 in the pruning experiment increases uncertainty in the posterior, akin to reducing observations or increasing observation noise. The behaviors introduced by the \u03b2-VAE seem relatively benign and possibly helpful in the linear case, breaking the degeneracy of the maximum likelihood solution sensibly. In deep generative models, the linear case of breaking the degeneracy of the maximum likelihood solution involves selecting solutions with orthogonal components and removing spurious latent dimensions. This behavior is beneficial in the PCA case but may not hold in real-world examples where the posterior distribution over latent variables does not factorize. In deep generative models, breaking the degeneracy of the maximum likelihood solution involves selecting orthogonal components and removing spurious latent dimensions. However, in real-world examples, the posterior distribution over latent variables may not factorize. Variational free-energy and \u03b2-generalization can bias estimated parameters away from maximum-likelihood settings towards factorized Gaussian posteriors. Applying VI and the \u03b2 free-energy method to ICA, we investigate biases in VI and \u03b2-VI by fitting data using the true generative model without amortized inference. The linear independent component analysis generative model uses Student-t distributions for latent variables. The free-energy in mean-field variational inference is computed using Gaussian distributions for each factor. The reconstruction term is similar to PCA, while the KL divergence involves differential entropy and cross-entropy with the prior. The code is slow due to the cross-entropy term for each latent variable, requiring multiple numerical integrations. Gradients are computed using autograd to find a global maximum of the free-energy. The dataset is defined using a sparse Student's t-distribution with v = 3.5, three latent components, and a two-dimensional observed space. Variational inference is used to find components that are more orthogonal than the true directions, reducing dependencies in the posterior. The bias in variational inference can lead to components being forced into an orthogonal setting, hindering the discovery of true component directions. This approach may sometimes yield meaningful structure but is not generally reliable, as it can entangle true components in the posterior and move away from the correct solution. The \u03b2-VI generalization exacerbates this undesirable bias."
}