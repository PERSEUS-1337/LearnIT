{
    "title": "BJgd7m0xRZ",
    "content": "Anomaly detection in unlabeled data identifies non-conforming data points, including those from malicious attacks. One-Class Support Vector Machines (OCSVMs) are effective but can be compromised by sophisticated adversaries. To enhance security, a defense mechanism based on data contraction is proposed, adding uncertainty to the OCSVM learner to thwart adversaries. Anomaly detection involves identifying non-conforming data points, known as anomalies or outliers, in various applications such as network intrusion detection and fraud detection. Adversarial perturbations can affect the separating margin of OCSVMs, but by contracting data in low dimensional spaces, adversarial samples can be successfully identified. The proposed method improves OCSVMs performance significantly by 2-7%. Anomaly detection systems, like One-Class Support Vector Machines (OCSVM), are crucial for identifying malicious attacks that could disrupt operations. While OCSVMs are effective against random noise, deliberate alterations by adversaries can significantly degrade their performance. These systems need periodic retraining in evolving data environments, which may allow adversaries to inject malicious data and weaken the algorithms' decision-making capabilities. Adversaries can inject malicious data to undermine learning algorithms like OCSVMs, aiming to avoid detection or decrease system performance. They manipulate training data to distort the algorithm's learning, making it favorable to them. Sophisticated adversaries can attack in various ways, making it challenging to provide a comprehensive analysis. The key question explored is how to enhance OCSVMs' resistance against adversarial attacks targeting training data integrity through distortions. Adversarial attacks target the integrity of training data through distortions, forcing the learner to model favorably. Securing machine learning systems against such attacks is crucial due to increased automation. Perturbations by adversaries in image recognition can lead to mis-classifications with high confidence. For example, altering a \"S-T-O-P\" sign to be classified as \"Speed Limit 45\" by a self-driving vehicle poses risks. Utilizing a nonlinear data projection algorithm is the goal to address these challenges. Utilizing a nonlinear data projection algorithm to increase attack resistance of OCSVMs against adversarial attacks. Nonlinear random projections facilitate large-scale, data-oriented decisions by reducing optimization parameters. Recent work shows improvements in training and evaluation times without compromising accuracy. Selective nonlinear random projections can enhance attack resistance of OCSVMs under adversarial conditions. Utilizing nonlinear random projections can enhance the attack resistance of OCSVMs against adversarial attacks. The original data distribution properties are preserved in the projected dataset with minor perturbations. The learner gains an additional layer of security as it becomes difficult for the adversary to guess the projection mechanism. The length of the weight vector in the transformed space is affected by the distortion made by the adversary and the nonlinear random projection. In this paper, it is proven that the weight vector w * p 2 is bounded above for OCSVM trained on nonlinearly transformed datasets. The study focuses on increasing attack resistance against adversaries through nonlinear data transformations, providing unpredictability and security advantages to the learner. The proposed approach for anomaly detection involves randomized kernels to add a layer of unpredictability for security. BID13 introduced Random Kitchen Sinks (RKS) to approximate kernel functions in a low-dimensional feature space. Rahimi and Recht mapped data to a low-dimensional space using a randomized feature map for kernel value approximation. In contrast to previous methods like RKS and R1SVM, our work focuses on random projections for efficient anomaly detection using randomized, nonlinear features with a linear kernel. This approach aims to reduce training and evaluation times without sacrificing accuracy. Our work explores random projections as a defense mechanism for OCSVMs under adversarial conditions, a novel approach not seen in existing research. Adversarial learning has sparked various studies in the machine learning community, with different models like AD-SVM introduced to address attacks. While we draw insights from these models, our defense mechanism differs significantly, focusing primarily on unsupervised anomaly detection. Our work focuses on unsupervised learning using OCSVMs and kernels, different from previous works using binary SVMs. Recent research has shown the vulnerability of deep neural networks to adversarial data. We introduce a unique framework combining adversarial learning, anomaly detection with OCSVMs, and randomized kernels, aiming to address the potential dangers posed by adversarial attacks in machine learning systems. In the context of unsupervised learning with OCSVMs and kernels, our work delves into adversarial learning for anomaly detection. The adversary disrupts the learning process by modifying training data to hinder the decision-making of the learner. The training dataset is distorted by perturbations, making it challenging for the learner to differentiate between normal data and adversarial distortions. The adversary's goal is to compromise the integrity of the input data, posing a threat to the learning system. The adversary can determine D based on its knowledge of the learning system, with the learner projecting data to a lower dimensional space in response. Nonlinear transformations are applied to minimize the effectiveness of attacks, with details on selecting a good transformation provided in Section 3.3. This approach approximates nonlinear kernels like the Radial Basis Function, reducing computational and memory overheads. The anomaly detection problem is addressed using the OCSVM algorithm, which separates training data with a maximal margin in a transformed space. The dual form of the OCSVM algorithm is written in matrix notation, with \u03b1 as the Lagrange multipliers vector and \u03bd as a parameter defining bounds on support vectors and outliers. The margin of the optimal separating hyperplane is given by \u03c1/w^2, where w = \u03b1^T C. The OCSVM algorithm separates training data with a maximal margin in a transformed space. In integrity attacks, the adversary minimizes the margin by injecting data maliciously. Targeted attacks involve smuggling specific anomalies across the margin using displacement vectors. The adversary uses displacement vectors to push anomalies towards normal data in the OCSVM algorithm, injecting them into the training set to be classified as normal data points. The severity of the attack is controlled by a parameter, s attack, which determines the distance from the normal data cloud. An anomaly data point closer to the normal data cloud is a moderate attack, while one farther away is severe. Attack severity distorts digits visually, with the attacker choosing target points for distortion. Computational effort and data distribution knowledge are crucial. The attacker uses the normal data cloud centroid as the target for distorting anomaly data points. The adversary can distort anomaly data points by adding \u03ba ij to x ij in the original feature space. Different attacks can be orchestrated by adjusting the percentage of distorted anomaly data points and the severity of the distortion. Increasing the severity of the attack moves anomaly data points farther away from the normal data cloud, making the attack more noticeable. The OCSVM is trained using the entire dataset as normal, but precautions can be taken to minimize the effects of distortions by contracting the data to a lower dimensional space using random projections. This introduces uncertainty to the adversary-learner problem, giving the learner an advantage from a security perspective. However, the unpredictability of random projections can be a caveat in reducing data dimensionality. To increase attack resistance in a learning system, a projection that conceals adversarial distortions should be used. The learner must select a projection that contracts the training set and maximizes the margin between data points in the transformed space. This approach is motivated by a generalized version of Dunn's index. The proposed compactness measure aims to identify suitable projection directions in a one-class problem by calculating the compactness of projected data for multiple random projections of the training data. The projection with the highest compactness value is selected for best attack resistance. An anomaly detection algorithm seeks to identify the smallest hypersphere containing the training data set in either the original dimensional space or a transformed space. The adversary's objective is to maximize or minimize the radius in this scenario. The learner aims to minimize the attack's effects by maximizing the radius of the minimum enclosing hypersphere. The effects of the adversary's perturbations on the margin of separation of the OCSVM are analyzed, with a focus on the distance between the hyperplane and the origin. An upper bound on the vector of weights is derived analytically to address the learner's inability to demarcate perturbations from normal training data. Based on the assumptions provided, an upper bound on the weight vector of a OCSVM trained on a nonlinearly transformed dataset without adversarial distortions is derived. The adversary's small and positive distortions are analyzed, with a focus on minimizing the effects on the margin of separation. The rationality of the adversary in refraining from large distortions is also considered. Theorem 1 discusses the primal solution of the OCSVM optimization problem in the projected space. The primal solution of the OCSVM optimization problem in the projected space is analyzed in the presence of adversarial distortions. The defender can tighten the upper bound of the weight vector by reducing the dimensionality of the dataset. Empirical validation on benchmark datasets shows the effectiveness of the proposed defense mechanism. The performance of OCSVMs with nonlinear random projections is compared in the presence of an active adversary conducting a directed attack by distorting data. Single-class datasets are generated from MNIST BID12, CIFAR-10, and SVHN, with anomalies pushed closer to normal data in test sets for evaluation. The study compares the performance of OCSVMs with nonlinear random projections in the presence of an active adversary distorting data. Single-class datasets from MNIST, CIFAR-10, and SVHN are used, with anomalies pushed closer to normal data in the test sets for evaluation. The datasets are normalized, and the learner selects dimensions based on local intrinsic dimensionality values. Nonlinear transformations are applied, and the resulting model is evaluated using the transformed test sets. The study evaluates the performance of OCSVMs with nonlinear random projections in the presence of an active adversary distorting data. The OCSVM is trained with a linear kernel, and the model is evaluated using test sets. The \u03bd parameter is kept fixed across all experiments to assess the interplay between adversarial distortions and OCSVM performance. The classification performance of OCSVMs trained on nonlinearly transformed data is reported using the f-score against test sets C and D. The study evaluates the performance of OCSVMs with nonlinear random projections in the presence of an active adversary distorting data. OCSVMs trained on nonlinearly transformed data show 2-7% higher classification performance compared to those trained on original feature space. The f-scores decrease between train C |test D and train D |test D, indicating OCSVMs trained on clean data can identify adversarial samples better. This suggests OCSVMs are vulnerable to integrity attacks, allowing adversaries to manipulate models. By reducing dimensions in OCSVMs, f-scores initially increase but then decrease, indicating improved detection of adversarial samples. Graphs show a significant improvement in detecting adversarial samples under the proposed approach, with up to 31% improvement on MNIST. Reducing dimensions in OCSVMs improves detection of adversarial samples initially, but performance declines beyond a certain threshold due to loss of useful information. The effectiveness of the upper bound derived in Theorem 1 is consistent and tighter under dimension reduction. OCSVMs are vulnerable to adversarial attacks on integrity. The paper investigates using random projections to increase OCSVM model robustness against integrity attacks. Performance declines when reducing dimensions beyond a threshold, but projected spaces show comparable performance with less computational burden. The study focuses on OCSVM performance in lower dimensions under adversarial conditions and the impact of nonlinear random projections on model robustness. The study explores the impact of nonlinear random projections on OCSVM model robustness against adversarial perturbations. Results show that OCSVMs can be significantly affected by adversaries accessing the training data. The approach enhances security by making it difficult for adversaries to guess the learner's details, making the learning system more secure. The proposed approach focuses on data contraction in the projected space to enhance security against adversarial attacks. Future work includes investigating the compatibility with other learning algorithms and determining the optimal number of dimensions for data transformation. The study also aims to explore game-theoretical formulations for anomaly detection and adversarial learning under dimensionality reduction techniques, as well as studying gradual malicious data injection attacks. The proposed approach aims to enhance security against adversarial attacks by focusing on data contraction in the projected space. The matrices and variables used in the approach are defined, and a proof for Theorem 1 is provided. The optimization problem is a minimization problem in the projected space with adversarial distortions. The optimization problem in the projected space aims to minimize distortions caused by adversarial perturbations. The primal solution w * p is defined as the optimal solution without any distortion."
}