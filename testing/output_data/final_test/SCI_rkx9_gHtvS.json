{
    "title": "rkx9_gHtvS",
    "content": "We propose an interactive classification approach for natural language queries, involving asking users additional questions to improve accuracy and handle ambiguous queries. This method uses a policy controller to determine when to present questions and maximize information gain. The system can be bootstrapped without interaction data by relying on non-interactive crowdsourcing tasks. This approach effectively balances the number of questions asked with final accuracy. The proposed interactive classification approach involves asking users additional questions to improve accuracy and handle ambiguous queries. This method uses a sequence of binary and multiple choice questions to interact with users and clarify input, enhancing the final prediction. Our system aims to streamline natural language interactions by deciding whether to ask more questions or finalize predictions after a user query. Unlike previous methods requiring full interaction data, we focus on bootstrapping with minimal annotation effort, crucial for applications like virtual assistants. Our approach utilizes Bayesian decomposition to efficiently compute and select the next steps in the interaction process. Our system streamlines natural language interactions by efficiently computing the next question based on posteriors. A policy controller decides whether to ask more questions or finalize predictions. Natural language annotations are collected to model class label distributions and user responses, reducing the need for heavy annotation. The article discusses an interactive classification system for bird identification and FAQ suggestion tasks. The system uses natural language queries and clarification questions to provide accurate labels. The system aims to streamline interactions and reduce the need for heavy annotation. The system uses natural language queries and clarification questions to improve classification accuracy for bird identification and FAQ suggestion tasks. Interactions with users significantly increase accuracy, with a relative boost of 40% to 65% compared to no-interaction baselines. The goal is to classify a query into one class label through interactions, showing over 100% improvement in accuracy with at most five turns of interaction. The system aims to classify a query into one class label through interactions, treating classification label, interaction question, and user response as random variables. Different values of these variables are denoted using subscripts and superscripts. Interactions start with the user providing an initial query, followed by the system asking questions and the user responding. Two types of questions are considered: binary and multiple choice. The system classifies queries into class labels through interactions involving binary and multiple choice questions. Interactions involve the user providing a query, the system asking questions, and the user responding. The model uses a parameterized distribution over class labels, a question selection criterion, and a policy controller to make decisions at each time step. The policy controller decides whether to return the best label or ask a question to gather more information. The model uses interactions to classify queries into class labels by asking questions to maximize information gain. Crowdsourced non-interactive data is used to bootstrap model learning, involving initial queries and text tags assigned by annotators. This data is converted into question-answer pairs for training. The dataset includes information on 'Android operating system', 'Windows operating system', or 'Not applicable'. The data is used to train a text embedding model, create a user simulator, and train a policy controller. Evaluation involves classification accuracy and the trade-off between accuracy and interaction turns. Recent work has used human feedback to train natural language processing models in dialogue learning, semantic parsing, and text classification. The curr_chunk discusses the use of user interaction in model prediction, contrasting with collecting user feedback post-prediction. It mentions incorporating human feedback in reinforcement learning and highlights the attention on language-based interaction in various areas like visual question answering and SQL generation. The curr_chunk focuses on simplified user interaction for efficient data annotation in various tasks like SQL generation and question answering, contrasting with more complex dialogue-based approaches. It mentions prior works related to question selection methods. The curr_chunk discusses the use of entropy reduction criteria in question selection for image search and object identification. It also mentions a Bayesian decomposition of joint distribution and modeling of natural language. Additionally, it contrasts its approach with a learning-to-ask model and highlights the use of entire interaction history in training the model. Our model is trained using crowdsourced annotations, while (Rao & Daum\u00e9 III, 2018) uses real user-user interaction data. Our method models natural language descriptions of classification targets, questions, and answers, maintaining a probability distribution over labels. We aim to help users achieve realistic goals with minimal interaction effort. The model predicts classification labels based on user interactions and question selection using Bayes rule. It assumes user responses depend only on the current question and target label, simplifying the probability decomposition. This approach reduces the task to modeling label probability given the initial query and user response probability based on the chosen question and label. The system selects questions to maximize interaction efficiency by computing information gain on classification labels. This factorization allows separate learning of question selection and user response probability. The system efficiently computes information gain on classification labels by iteratively updating probability distributions using natural language descriptions of questions, answers, and labels. This approach reduces the need for heavy annotation and improves the model in low-resource scenarios. The text discusses the probability of predicting labels based on text inputs and the estimation of answers given questions and labels using linear combinations and parameterized estimations. It also mentions using question-answer annotations to estimate probabilities and smoothing partially observed counts using learned encodings. The text discusses smoothing observed counts with encoding S(\u00b7) and estimating parameters \u03c8 through pre-training. It involves creating text pairs (u, v) for training the scoring function S(\u00b7) and using gradient descent to minimize cross-entropy loss. Negative sampling approximates the sum over all labels in Y. Parameters are fine-tuned using reinforcement learning during policy controller training. The user simulator provides queries, responds to clarification questions, and judges system responses. The user simulator in the system responds to clarification questions, judges label correctness, and selects a target label to start interactions. Sampling from belief distributions adds natural noise, with the interaction ending when the system receives a response. The system's user simulator responds to questions, judges label correctness, and selects a target label to initiate interactions. The policy controller decides whether to ask another question or conclude the interaction, balancing exploration and exploitation. During training, the policy controller is tuned using a reward function that incentivizes correct target predictions and penalizes wrong predictions and excessive questioning. Data for the FAQ task is collected through a crowdsourcing process, and initial queries and tags are obtained for each FAQ document. The system utilizes Amazon Mechanical Turk for data collection and re-purposes an existing dataset for the Birds domain. The system collects initial queries and tags for FAQ documents through crowdsourcing, encouraging diverse utterances and natural language tag annotations. Domain experts define possible tags, which can be phrases or single words describing the document's topic. Some tags are combined into categorical tags, while others remain as binary tags. The system uses deterministic templates to convert tags into questions for FAQ documents. Workers collect user responses to these questions, ranking tags based on relevance to the target and showing only the top 50 to workers. The workers collect user responses to questions generated from tags, ranking them based on relevance to the target and showing only the top 50. The FAQ dataset used contains troubleshooting documents from Sprint's technical website, with data split into training, development, and test sets. Only the queries and tag annotations from the training documents are used for pre-training and learning the policy controller. The second set of experiments involves using the Caltech-UCSD Birds (CUB-200) dataset, which includes 11,788 bird images for 200 different bird species. Each image is annotated with visual attributes and attribute values. Attributes with value count less than 5 are considered categorical tags, resulting in 8 categorical questions. The remaining attributes are treated as binary tags. Additionally, each image has 10 image captions describing the bird, which are used as initial user queries. The study uses image captions as initial user queries and bird species as labels. The data is noisy, making user interactions challenging. The images are not used for model training but for grounding during human evaluation. The full model is compared against baseline methods including No Interact, Random Interact, and Static Interact. The study evaluates a model using image captions as user queries and bird species as labels. The model considers various variants, including different termination strategies and disabling certain estimators. Evaluation involves testing against a user simulator and a real human, measuring classification performance using Accuracy@k. The study evaluates a model using image captions as user queries and bird species as labels. Evaluation involves testing against a user simulator and a real human, measuring classification performance using Accuracy@k. Annotators interact with proposed or baseline models through a web-based interface, providing ratings on interaction quality. A fast recurrent neural network is used to encode texts, with the policy controller receiving rewards for correct and incorrect predictions. The study evaluates a model using image captions as user queries and bird species as labels. Evaluation involves testing against a user simulator and a real human, measuring classification performance using Accuracy@k. Annotators interact with proposed or baseline models through a web-based interface, providing ratings on interaction quality. The model receives rewards for correct and incorrect predictions, with results compared against various baselines using Accuracy@{1, 3}. Our model and its variants outperform baselines in accuracy for FAQ Suggestion and Bird Identification tasks, showing substantial gains with just a few turns of interaction. The No Interact (Neural) baseline achieves 38% Accuracy@1 for FAQ Suggestion and 23% for Bird Identification, while the No Interact (BM25) baseline performs the worst. Our full model achieves 79% Accuracy@1 for FAQ Suggestion and 49% for Bird Identification. The full model outperforms baselines in accuracy for FAQ Suggestion and Bird Identification tasks with just a few turns of interaction. The policy controller trained with reinforcement learning is effective, as shown by the significant improvement over alternative termination strategies. The importance of modeling natural language for efficient interaction is confirmed by the results. Trade-off between classification accuracy and number of turns is illustrated in Figure 2. Our model achieves a significant accuracy boost for FAQ Suggestion and Bird Identification tasks by leveraging human feedback. The approach outperforms baselines across all interaction numbers, with an average of 3 interaction turns for each model variant. The human evaluation results show an Accuracy@1 of 30% for FAQ and 20% for Bird tasks without any interaction. Our full model achieves the best performance with an Accuracy@1 of 59% for FAQ Suggestion and 45% for Bird Identification. Users rate our model as more rational, and human evaluation shows it handles real user interaction effectively despite being trained with non-interactive data. Additional details for human evaluation and example interactions are included in Appendix A.3. Figure 3 displays the learning curves of our model with the policy controller trained with different turn penalties. Interesting exploration behavior is observed during the initial training episodes, with relatively stable accuracy achieved afterward. The models end up using different numbers of expected turns due to the choice of parameters. Our method for interactive classification involves users providing natural language queries and the system asking simple questions to fill in missing information. We use information theory to select the best question at each turn and show how the system can be bootstrapped without interaction data. Results demonstrate that our approach outperforms baselines significantly, and we provide a new annotated dataset for future research. One main challenge in the collection process is familiarizing workers with target documents. A two-step qualification task is set up to ensure good quality annotation. Workers write paraphrases with complete information and then generate queries. After two rounds of tasks, 25 workers are selected to collect initial queries. To ensure target-tag annotation quality, a pretrained model ranks tags for selection. The workers need to complete annotation on three targets without selecting any negative tags. Tags are ranked based on tag-document relevance using a pretrained model. The top 50 tags for each document are split into five lists for annotation by separate workers. The higher ranked tags are more likely related to the target. The text describes the implementation of a text encoder and policy controller network for different tasks. It uses a single-layer bidirectional Simple Recurrent Unit (SRU) for the FAQ suggestion task and a two-layer bidirectional SRU for bird identification. The encoder uses pre-trained fastText word embeddings, while the policy controller is a two-layer feed-forward network with specific parameters. The network utilizes a hidden layer of 32 dimensions with ReLU activation, taking the current step and top-k belief probabilities as input. Training involves initial and paraphrase queries, with an encoder trained on 16K target-query examples. Pseudo-queries are generated by combining existing queries with tags, improving classification performance. Using tags instead of initial queries enhances classification, achieving an Accuracy@1 of 76% when concatenating user queries and tags as input. The system achieves 79% accuracy with only 5 tags in the full model. Each interaction session starts with presenting a user scenario to the annotator, who inputs an initial query and answers follow-up questions. Prediction accuracy, system rationality, and user satisfaction are evaluated through interactions with human judges. User scenarios are designed for each target, and at the end of each interaction, the predicted FAQ and ground truth are presented for user feedback. The system achieves 79% accuracy with only 5 tags in the full model. Each interaction session starts with presenting a user scenario to the annotator, who inputs an initial query and answers follow-up questions. Prediction accuracy, system rationality, and user satisfaction are evaluated through interactions with human judges. User scenarios are designed for each target, and at the end of each interaction, the predicted FAQ and ground truth are presented for user feedback. During the interactions, users rate naturalness and rationality on a scale of \u22122 (strongly disagree) to 2 (strongly agree). The interface for the bird identification task is similar to the FAQ suggestion task, where users describe a bird image to find its category, analogous to writing an initial query. Users can reply 'not visible' if part of the bird is hidden, stopping further questions from the same label group. The bird identification task is challenging due to fine-grained categories, with images looking almost identical but belonging to different classes. The system improves Accuracy@1 from 20% to 45% after less than 3 turns of interaction. Annotators reported that predicted images are sometimes almost identical to true images. Human evaluation results show improvements in FAQ suggestion and bird identification tasks compared to baselines. The proposed model and baselines for FAQ and Bird systems are evaluated based on the number of turns of questions asked. Performance metrics include initial and final Accuracy@1, while user experience is measured in terms of naturalness and rationality. High values are desired in the diagonal part of the evaluation matrix."
}