{
    "title": "H1I3M7Z0b",
    "content": "WSNet is a new approach for learning compact and efficient deep neural networks by sampling from a compact set of learnable parameters. This weight sampling method promotes weights and computation sharing, allowing for the efficient learning of smaller networks with competitive performance in audio classification tasks. WSNet is a novel method for creating compact and efficient deep neural networks for audio classification. Through weight quantization, models can be up to 180 times smaller and 16 times faster than traditional baselines without sacrificing performance. This addresses the challenges of overfitting and high resource consumption in deep neural networks, making them more suitable for devices with limited memory and power. This paper introduces WSNet, a Weight Sampling deep neural network, which enforces parameter sharing among filters to learn compact and efficient deep networks. WSNet significantly reduces model size and computation cost, achieving over 100\u00d7 smaller size and up to 16\u00d7 speedup with negligible performance drop or even better performance than conventional networks. WSNet is parameterized by layer-wise condensed filters for direct sampling in convolutions, reducing parameters compared to conventional CNNs. An integral image method is used to decrease computation cost in training and inference, enabling weight sampling with different filter sizes. Extensive experiments show WSNet's efficacy in acoustic scene classification and music detection. WSNet significantly reduces model size and computation cost while maintaining high classification accuracy in acoustic scene classification and music detection tasks. The approach can be extended to 2D CNNs in future work. SoundNet CNN is notable for sound classification due to training on large-scale unlabeled sound data using visual information as a bridge. It is widely used in audio classification tasks like music detection. WSNet achieves comparable or better performance than SoundNet with a significantly smaller size and faster speed by adopting deep model compression techniques such as weight pruning, filter decomposition, hashed networks, and weight optimization. WSNet is a deep model compression technique that can significantly reduce model size by learning compact representations for both convolution and fully connected layers from scratch. Unlike other methods that suffer from performance drops or require training a teacher network in advance, WSNet achieves this without sacrificing performance. WSNet can reduce model size and computation latency through factorization, quantization, and grouped pruning methods. Compared to other efficient model architectures like Inception and Residual models, WSNet stands out for its smaller model size and faster computation. WSNet introduces a new model design strategy for 1D CNNs, utilizing weight sampling along the spatial and channel dimensions to obtain parameters from a more compact representation. This approach is more flexible and generalizable compared to existing models like MobileNet, Flattened networks, and ShuffleNet. WSNet introduces weight sampling along spatial and channel dimensions, denser weight sampling, and an integral image method for acceleration in training and inference. The 1D convolution layer takes input feature map F and produces output feature map G with spatial length (T), input channel (M), and number of filters (N). The convolution kernel K has shape (L, M, N) where L is kernel size. WSNet introduces weight sampling along spatial and channel dimensions to learn more compact DNNs. The convolution operation is computed using a condensed filter \u03a6, leading to a more efficient network structure. The compactness of the learned layer in WSNet is compared to conventional CNNs with independently learned weights. Weight sampling is demonstrated in two dimensions: spatial and channel dimensions. WSNet introduces weight sampling along spatial and channel dimensions to learn compact DNNs with a more efficient network structure. This approach reduces the number of parameters in the network, preventing overfitting and making it suitable for computation resource-constrained platforms. The weight sampling is demonstrated in spatial and channel dimensions to generate condensed filters for convolution operations. The weight sampling method in WSNet aims to address overfitting and local minimums by efficiently reusing weights among filters. Filters are sampled from a condensed filter in each convolutional layer, improving compactness along spatial dimensions. The weight sampling strategy in WSNet aims to learn compact deep models with negligible loss of accuracy. A channel sharing strategy is proposed to achieve more compact networks without limitations imposed by filter size. By repeating weight sampling along the channel dimension, WSNet achieves significant model size reduction without performance drop. Channel sharing in WSNet allows for maximum compactness up to M, enabling the learning of much smaller models without significant performance drop. This strategy can be applied to fully connected layers as well, reducing the size of learnable parameters. For example, spatial sampling can be performed for the fully connected layer \"fc1\" to achieve a more compact representation. Compared to convolutional layers, which typically have small filter sizes, this approach offers a way to learn aggressively smaller models. WSNet utilizes channel sharing to achieve maximum compactness up to M, allowing for smaller models without performance loss. Fully connected layers can also benefit from spatial sampling for a more compact representation. The training process is similar to conventional deep convolutional networks, with weights in the convolutional kernel sampled from a condensed filter. The gradient calculation for the filter is based on the position mapping of tied weights in the kernel. In open-sourced machine learning libraries like TensorFlow, WSNet's performance can be affected by aggressively decreasing the size of condensed filters. To enhance learning, more filters can be sampled for layers with reduced sizes by using a smaller sampling stride. Denser weight sampling has been shown to improve WSNet's performance, but it also adds extra parameters and computational cost. Denser weight sampling in WSNet is mainly used in lower layers with small filter numbers to reduce computational redundancies. Channel sampling can further reduce the size of added 1x1 convolution layers. The computation cost in a conventional convolutional layer is high due to computational redundancies in WSNet. The proposed integral image method aims to eliminate redundancy in convolution and speed up WSNet by efficiently computing inner products between input feature maps and condensed filters. This method calculates the inner product map P, which stores inner products between input feature map rows and condensed filter columns, resulting in faster convolution computations. The integral image method speeds up WSNet by efficiently computing inner products between input feature maps and condensed filters, eliminating redundancy in convolution. The integral image I, with the same size as P, allows for direct retrieval of convolutional results in O(1) time complexity. The n-th filter lies in the spatial range of (nS, nS + L \u2212 1) in the condensed filter \u03a6. Comparisons of computation costs between WSNet and conventional architectures are discussed in Section 3.4. The n-th filter in the condensed filter \u03a6 lies in the spatial range of (nS, nS + L \u2212 1). Calculating G takes T N times using Eq. (8). The computation cost of the integral image method is based on Eq. (6) \u223c Eq. (8). Zero padding allows convolutional results for padded areas without using Eq. FORMULA11 DISPLAYFORM6. The theoretical acceleration ratio is based on Eq. (5), Eq. FORMULA13, and Eq. (2). A variant method is adopted to boost computation efficiency in WSNet. The efficiency of WSNet is improved by a variant method using integral image techniques, reducing computational costs by wrapping channels of F and accumulating values with intervals along its channel dimension. This method reduces the overall computation cost as demonstrated in Eq. (10) and (8). The computational cost of warping channels in WSNet is reduced, leading to a significant acceleration compared to the baseline. The integral image method utilized in WSNet takes advantage of weight sampling properties, eliminating redundant computations without affecting performance. A detailed analysis of experimental results is presented in this section. WSNet is proven effective in learning compact and efficient networks through extensive ablation studies. It improves classification performance over baseline networks with models 100\u00d7 smaller. Even with models 180\u00d7 smaller, WSNet achieves comparable performance. Additionally, WSNet provides 2\u00d7 \u223c 4\u00d7 acceleration compared to baselines with models more than 100\u00d7 smaller. Datasets used include MusicDet200K, ESC-50, UrbanSound8K, and DCASE for fair comparison with previous literature. MusicDet200K aims to assign binary labels to samples indicating music presence. MusicDet200K has 238,000 annotated sound clips, with 200,000/20,000/18,000 split for train/val/test sets. 70% of samples are labeled as \"non-music\". ESC-50 consists of 2000 environmental recordings with 50 classes, divided into 5 folds for cross-validation. Each recording yields 10 sound clips of 1-second length with 0.5-second overlap. UrbanSound8K and DCASE BID45 are datasets used for sound classification tasks. UrbanSound8K contains 8732 short recordings of urban sound sources, while DCASE BID45 is used for acoustic scene classification. In UrbanSound8K, 8 clips are extracted from each recording with a time length of 1 second and time step of 0.5 seconds. In DCASE BID45, each sample is a 30-second audio recording with 10 acoustic scene categories. During training, 12 sound clips are evenly extracted from each 30-second audio recording, with a time length of 5 seconds and time step of 2.5 seconds. WSNet is evaluated based on model size, mult-adds, and classification accuracy. Two baseline networks are used for comparison, one with 7 convolutional layers and 2 fully connected layers. The effectiveness of WSNet on both types of layers is demonstrated. The state-of-the-art SoundNet BID2 was modified by adding pooling layers to improve performance. This modified SoundNet was used as a baseline on public datasets. Weight quantization was applied to reduce the size of WSNet by linearly quantizing weights to q bins. Each bin's size was calculated based on the range of weights, requiring log2(q) bits to encode the index. Weight quantization was applied to WSNet to reduce model size by a factor of 32 log 2 (q). Unlike previous methods, weight quantization in WSNet is applied after training. The baseline network used on MusicDet200K consists of convolutional layers followed by ReLU, batch normalization, and pooling layers. Pooling layers have a stride of 2 and padding strategies preserve size. After training, weight quantization in WSNet reduces model size without losing accuracy. WSNet is implemented in Tensorflow using the Adam optimizer with a fixed learning rate of 0.001 and a momentum term of 0.9. Dropout layers with a ratio of 0.8 are used after each fully connected layer. Ablation analysis investigates the effects of WSNet components on model size, computational cost, and accuracy. Results are compared in TAB2. The study compares different settings of WSNet listed in TAB2, named by symbols S/C/SC\u2020/D/Q. Spatial sampling with different stride values shows improved classification accuracy, with S 2 and S 4 outperforming the baseline. A sampling stride of 8 only slightly decreases accuracy by 0.6%. Spatial sampling allows WSNet to learn smaller models with comparable accuracies. Channel sampling in WSNet allows for the learning of significantly smaller models with comparable accuracies compared to the baseline. Different levels of compactness along the channel dimension (C 2, C 4, C 8) are tested, showing linear reductions in model size without sacrificing accuracy. Channel sampling proves to be effective, with C 8 performing better than S 8 in high spatial compactness scenarios. Weight sampling on both spatial and channel dimensions results in highly compact models (more than 20\u00d7 smaller than baselines) for WSNet. Channel sampling in WSNet allows for learning significantly smaller models with comparable accuracies compared to the baseline. Denser weight sampling enhances learning capability with aggressive compactness. Integral image method consistently reduces computation cost for WSNet models. The computation cost of WSNet models is significantly reduced by 16.4 times due to the extra computation cost brought by the 1\u00d71 convolution in denser TAB2. The comparison with state-of-the-arts on ESC-50 shows that WSNet achieves lower acceleration. Group convolution BID49 can be used to alleviate the computation cost of the added features. Weight quantization can reduce the model size of WSNet by using 256 bins to represent each weight, resulting in a 1/168 reduction in model size with only 0.1% accuracy loss. WSNet can effectively reduce model size when used in conjunction with weight quantization. The comparison of WSNet with other models on ESC-50 is shown in Table 5. The settings of WSNet on ESC-50, UrbanSound8K, and DCASE are listed in Table 5 as well. WSNet can reduce the model size by 25% compared to the baseline. WSNet significantly reduces model size by 25-45 times and improves accuracy by 0.1-0.5% compared to the baseline. It achieves higher computational efficiency by reducing #Mult-Adds by 2.3-2.4\u00d7. After weight quantization, WSNet's model size is reduced to 1/180 of the baseline with only a 0.2% accuracy drop. WSNet outperforms SoundNet by over 10% with 100\u00d7 smaller models. SoundNet BID2, pre-trained on unlabeled videos, achieves better accuracy than WSNet. WSNet achieves better performance by training on unlabeled video data similar to SoundNet BID2. It reduces model size significantly while maintaining accuracy. WSNet outperforms the baseline by 1% in classification accuracy by taking raw wave recordings as input. WSNet outperforms the baseline by 1% in classification accuracy with a 100\u00d7 smaller model. Even with a model 180\u00d7 smaller, WSNet is only one percentage point lower than the baseline, showing its effectiveness. WSNet achieves comparable results to SoundNet BID2 using only provided data. Weight Sampling networks (WSNet) are presented as highly compact and efficient, utilizing a novel weight sampling method to sample filters from condensed filters. WSNet's capacity of learning compact models is verified through experiments on ESC-50 and MusicDet200K, comparing it with baselines by reducing the number of filters in each layer. WSNet outperforms baselines by a large margin across all compression ratios, particularly when the ratios are large. The focus is on WSNet with 1D convnets, showing clear advantages in learning compact models. WSNet's capacity for learning compact models is demonstrated through experiments on ESC-50 and MusicDet200K, outperforming baselines by reducing the number of filters in each layer. The focus is on WSNet with 1D convnets, showcasing clear advantages in learning compact and computation-efficient networks. The extension of WSNet to 2D convnets involves spatial and channel sampling, with filters denoted as K \u2208 R w\u00d7h\u00d7M \u00d7N and condensed filter \u03a6 having the shape of (W, H, M * ). The condensed filter \u03a6 in WSNet has a shape of (W, H, M * ). Extending WSNet to 2D convnets faces challenges due to stronger spatial dependencies and difficulty in using the integral image method for speeding up. Future work will explore more sophisticated methods for applying WSNet to 2D convnets. In future work, the effectiveness of WSNet in image classification tasks using 2D convnets was verified through preliminary experiments on MNIST and CIFAR10 datasets. Comparison with HashNet was done using the same baselines and hyperparameters. Results are listed in TAB11, showing similar performance between WSNet and HashNet on both datasets. WSNet outperforms HashNet in error rates on datasets, demonstrating advantages in learning compact models. Experiment on CIFAR10 with ResNet18 BID19 as baseline shows WSNet achieves 20\u00d7 smaller model size with slight performance drop (0.6%), proving its effectiveness."
}