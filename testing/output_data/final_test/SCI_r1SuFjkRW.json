{
    "title": "r1SuFjkRW",
    "content": "In this paper, the authors propose a method inspired by sequence-to-sequence models to develop policies over discretized spaces for high dimensional continuous control problems. They use neural networks to predict one dimension at a time, allowing for the modeling of Q-values and policies over continuous spaces. This approach leverages the compositional structure of action spaces during learning and enables the computation of maxima over action spaces. Empirical results on a simple example task demonstrate the effectiveness of this method. Reinforcement learning has traditionally been used for discrete action spaces, but a new method has been developed for high-dimensional continuous control problems. This method utilizes neural networks to predict Q-values and policies over continuous spaces, allowing for global search and overcoming local optimization issues. The technique has been applied to off-policy methods, achieving state-of-the-art results on various continuous control tasks. Continuous control reinforcement learning techniques have been developed using deep neural networks for continuous action spaces, but the gains have not been as significant as for discrete action spaces. The difficulty lies in maximizing functions on continuous domains and applying dynamic programming methods. Some approaches aim to borrow characteristics from discrete problems to make maximization and backups easier. One way to leverage these advantages is by discretizing each dimension of continuous action spaces. To address the challenges of maximizing functions on continuous domains in reinforcement learning, a method is proposed to discretize each dimension of continuous action spaces. By leveraging sequence-to-sequence models, the approach avoids creating an exponentially large discrete space of actions. This technique, based on chain rule decomposition, allows for modeling complex probability distributions in high-dimensional spaces without falling into the curse of dimensionality. The method parameterizes Q-values using a decomposition of the joint function into conditional values tied together with the Bellman operator. The method proposed involves fine-grained discretization of individual domains using conditional values tied with the Bellman operator. This allows for modeling complex distributions and performing global maximization in off-policy settings similar to DQN. Empirical results on a multimodal problem show the effectiveness of this approach. Our model utilizes sequential models to predict over action spaces one dimension at a time, using discrete distributions and off-policy learning. The notation used includes observed state (s t), action space (a), and stochastic environment (E). The agent operates in a sequential manner, taking actions, receiving rewards, and transitioning to new states based on unknown dynamics. An episode consists of a sequence of steps, with a termination criterion. The goal is to learn a policy that maximizes future rewards. Q-learning is an off-policy algorithm that learns an action-value function Q (s, a) and a corresponding greedy-policy. It is trained by finding the fixed point of the Bellman operator and can be represented using deep neural networks like Deep Q-Networks (DQN). The DQN parameters, \u03b8, are trained using gradient descent on the error in equation 2. It is important for a parametric form of Q to easily find the maxima with respect to actions, especially in continuous action problems. Existing techniques like NAF and DDPG are used to address this challenge. Sequential DQN (SDQN) is a model proposed to modify the form of the Q-value function while still being able to find local maxima over actions for use in a greedy policy. It decomposes the original MDP model with N-D actions into a similar MDP with sequences of 1-D actions, creating a 2-layer hierarchy of MDPs. The upper layer contains the original environment, while the lower layer contains the transformed MDP. Both models the same environment and are combined by noting equality of Q values at certain states and performing bellman backups. Sequential DQN (SDQN) introduces a 2-layer hierarchy of MDPs by transforming the original N-D action MDP into a new MDP with sequences of 1-D actions. This new MDP consists of states u st k, where u st k = (s t , a 1:k ) represents the original state s t and a history of actions. Transitions in this new MDP involve computing steps in the N-D environment and receiving new states, rewards, and resetting actions. The Sequential DQN (SDQN) introduces a 2-layer hierarchy of MDPs by transforming the original N-D action MDP into a new MDP with sequences of 1-D actions. This transformation reduces the N-D actions to a series of 1-D actions, allowing for the application of Q-learning. However, this approach increases the number of steps needed to solve the transformed MDP and makes learning a Q-function considerably harder due to overestimation and stability issues. The Sequential DQN introduces a 2-layer hierarchy of MDPs by transforming the original N-D action MDP into a new MDP with sequences of 1-D actions. To address overestimation and stability issues, Q-values for both MDPs are learned simultaneously, with bellman backup from lower to upper MDP. Consistent Q-values are maintained by defining discounting such that the lower MDP has zero discount except when the environment changes state. This allows for equality in Q-values across both MDPs during one step in the environment. The Sequential DQN introduces a 2-layer hierarchy of MDPs by transforming the original N-D action MDP into a new MDP with sequences of 1-D actions. Q-values for both MDPs are learned simultaneously, with bellman backup from lower to upper MDP. Q U and Q L are parameterized as neural networks and trained using TD-0 learning. Q L is learned by enforcing soft equality with MSE when Q U and Q L should be equal. Target networks and double DQN can be used for increased stability during training. The DQN BID15 model is used for training Q U and Q L for stability. Exploration is done using epsilon greedy or Boltzmann exploration. Q U is a MLP with inputs as state and actions, while Q L has two parameterizations - a recurrent LSTM model and a version with shared weights. The lower MDP uses N separate models, Q i, which are feed forward neural networks that switch based on the state index. This weight separation leads to more stable training, especially in complex domains like vision-based control tasks. In practice, fully untied weights were found to be effective. In practice, fully untied weights were effective for stable training in complex domains. Architecture exploration for these models is ongoing. Our work aims to learn policies over large discrete action spaces and approximate value functions over high dimensional continuous action spaces effectively. In (Dulac-Arnold et al., 2015), a strategy using action embeddings and k-nearest neighbors was proposed to reduce scaling of action sizes. BID23 utilizes a hypercube layout for actions, enabling a logarithmic search for the optimal action. Their method is similar to SDQN, constructing a Q-value from sub Q-values. Pazis & Lagoudakis (2009) and BID22 also propose similar transformations for continuous action MDPs. Our approach involves iterative refinement of action selection, contrasting with independent sub-plane maximization. In the realm of continuous state and action environments, specialized solutions have been developed to address overestimation errors when using function approximators. Various deep reinforcement learning approaches such as TRPO and A3C utilize stochastic policies parameterized by Gaussian distributions, while NAF relies on a quadratic advantage function for closed form optimization. Some methods structure networks to be convex in actions but non-convex in states, offering innovative solutions for continuous state and action problems. In reinforcement learning, sequential policies have been used to handle large action spaces like neural architectures and word sequences. Different methods like hierarchical/options based approaches offer ways to factor action spaces, refining action selection over time. Sequential models are effective for tasks generated in a sequential process, improving sample efficiency. Sequential models like DDPG and SDQN are efficient in language modeling but may struggle outside the policy area. DDPG converges quickly to a local maximum, while SDQN initially has high variance but converges to the global maximum as the Q surface estimate improves. BID4 introduced a deep learned sequence-to-sequence model for language modeling. In a deterministic environment with a 2D action space, our algorithm shows effectiveness with a multimodal reward distribution. Our reward function is a multimodal distribution with suboptimal and optimal modes. We use exploration strategies like \u03b5-greedy and local sampling. DDPG utilizes local optimization to learn a policy based on Q values predicted by a critic. Gradient descent is used for learning the policy due to the flexible nature of the Q distribution. In reinforcement learning, gradient descent is used as a local optimization algorithm to learn a policy. However, it can lead to sub-optimal policies due to local maxima in policy space. Deep learning methods typically avoid local minima or maxima in high dimensional parameter spaces, but in RL, the relatively small dimensional action space makes it more likely to encounter these issues. For example, in the hopper environment, algorithms like DDPG may learn to balance instead of moving forward and hopping. In contrast, SDQN can represent the Q surface more accurately and optimize the policy globally, leading to convergence to the optimal policy. In reinforcement learning, deep learning methods like SDQN can avoid local maxima in policy space, enabling convergence to the optimal policy. Unlike DDPG, the Q surface learned can be based on off-policy data, preventing the policy from getting stuck in local maxima. Sampling more on-policy data points results in faster training and improved convergence rates. Experimental evaluations on various continuous control tasks show the effectiveness of these models. The study conducted a hyperparameter search on models in the humanoid environment from OpenAI gym suite BID6. They evaluated the best performing runs with 10 random seeds to assess consistency and performance. The method outperformed DDPG in achieving good policies quickly. Qualitative analysis was done using the best reward averaged over 25,000 steps with evaluations every 5,000 steps across 10 random seeds for stability assessment. Our algorithm outperforms DDPG in off-policy continuous control, achieving better performance on four out of five tested environments. We have the flexibility to choose the number of discretization bins, allowing for potential performance gains with different neural network architectures and hyperparameters. See Figure 4 for results. Results from testing the hyper parameter configuration for half cheetah show that SDQN performs well with bin amounts greater than 4, but not with less than 4 bins. Testing the effect of action order on the same hyper parameters revealed that ordering does not significantly impact performance. Our approach focuses on factoring and sequentially selecting action at each stage using 1-D discretized action spaces. Utilizing a mixture of logistic units can speed up training and satisfy the need for a closed form max. Learning to factor or group action sets can address issues with prespecified ordering of actions, especially for problems with larger action dimensions. The work focuses on utilizing sequential prediction and discretization in an off-policy RL algorithm for continuous control. The method decomposes the model into a hierarchy of Q functions and demonstrates effectiveness on illustrative tasks. The SDQN algorithm decomposes the model into a hierarchy of Q functions and is effective on various tasks, including complex continuous control tasks. Visualization of the sub-DQN in the hopper environment shows the network's ability to be agnostic to certain actions during specific parts of the gait. The SDQN algorithm decomposes the model into a hierarchy of Q functions and is effective on various tasks, including complex continuous control tasks. In the hopper environment, the network learns that certain actions are better during critical parts of the gait to avoid bad results like tripping. The Q distribution for most states in the hopper walk cycle is flat, indicating that small changes in actions have little impact on future rewards. In the hopper environment, critical states exist where selecting the correct action value is crucial for performance. The algorithm is trained with soft constraints to maintain or improve expected rewards. Q surfaces are analyzed globally, showing non-smooth surfaces in both sequential Q and Q U. The Q surface for Hopper is non-smooth and noisy in some regions, but this does not affect the final policy performance. Future work will explore soft Q-learning techniques to smooth the representations. The dimensions of the autoregressive model show differences in noise levels, indicating the order of action dimensions matters. The model struggles to learn sharp features in the a1 dimension, leading to noisy and artifact-filled Q U form. Key hyperparameters affecting performance were learning rates, reward scale, and discount factor. Exploration on less than 10% of transitions was found to be optimal. The model structure used also significantly impacted performance. In experiments, the model structure had a significant impact on performance, particularly when using tied versus untied weights. Future work aims to reduce hyperparameters and study their effects more thoroughly. The LSTM parameterization and untied weight variant were examined, with a focus on the untied weight variant's parameter search process. The computation involves a fully connected layer with an \"embedding size\" followed by a 2 hidden layer MLP with \"hidden size\" units, resulting in an output of \"quantization bins\" width with no activation."
}