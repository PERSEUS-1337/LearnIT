{
    "title": "r1hsJCe0Z",
    "content": "The study focuses on semantic code repair, aiming to predict bug locations and fixes without relying on unit tests. A contextual repair model is trained on real-world code with injected bugs. The framework uses a two-stage approach, generating repair candidates with rule-based processors and scoring them with a neural network architecture called Share, Specialize, and Compete. The architecture for automatic code repair involves generating a shared encoding of source code, scoring repair candidates with specialized network modules, and normalizing scores for competition. The model achieves 41% accuracy in predicting correct repairs with a single guess, outperforming an attentional sequence-to-sequence model. The focus is on fixing semantic bugs, where program behavior deviates from the intended functionality. The focus is on fixing semantic bugs, which are code issues that deviate from the intended program behavior. This work targets bugs that can be identified and fixed by experienced programmers without running the code or deep contextual knowledge. Unlike previous approaches, this model does not rely on unit tests for training or testing, requiring the ability to infer the intended purpose from source code before proposing repairs. Our proposed task focuses on semantic code repair without unit tests, requiring models to deeply understand the code to propose repairs. The repair model was trained on a large corpus of Python projects with injected bugs and tested on real and synthetic bug sets. An attentional sequence-to-sequence architecture was initially evaluated but deemed unsuitable due to lack of direct competition between repair candidates at different locations. The proposed approach for semantic code repair involves a two-stage process using a neural framework called Share, Specialize, and Compete (SSC) network. This framework includes encoding the input code snippet, using specialized neural modules for each repair type to score repair candidates, and normalizing raw scores for competition. Our SHARE network is an evolution of neural code completion and summarization systems, focusing on learning a semantic understanding of code snippets. The SPECIALIZE modules build on this to identify and fix specific bug types. The framework has applications in sequence transformation scenarios like natural language grammar correction, machine translation post editing, source code refactoring, and program optimization. This work addresses semantic program repair without unit tests, inferring functionality from the code, adding to existing literature on program repair. Our work adds to the existing literature on program repair by discussing techniques such as Neural Syntax Repair and Statistical Program Repair. Neural Syntax Repair involves training neural networks to correct syntax errors in code, while Statistical Program Repair uses genetic programming techniques to propose program modifications. Prophet learns a probabilistic model to rank patches for errors based on human patches and hand-engineered program features. Our neural model learns program representations for repairing semantic bugs, unlike hand-engineered features. Previous work focused on tasks like code completion and renaming, while rule-based analyzers for Python handle different issues. Our goal is to predict bug locations and fixes without unit tests or specifications. The text discusses the creation of training data for a repair model that focuses on fixing common Python bugs. It mentions the challenge of obtaining a large dataset of buggy/repaired code snippets and the use of synthesized bugs for training. The model is shown to generalize well to real bugs despite being trained on synthesized data. The training data for the repair model was created by downloading Python projects from GitHub with permissive licenses. Each code snippet was extracted as an AST without surrounding context. Snippets with 5 to 300 nodes were kept, averaging 45 nodes and 6 lines of code. The data was divided into training, test, and validation sets at the repository level. The data for the repair model was divided into training, test, and validation sets at the repository level to avoid overlap. 2,900,000 training snippets were extracted, with 2,000 each for test and validation. Four classes of semantic repairs were considered, including VarReplace, CompReplace, IsSwap, and ClassMember. Bugs were synthesized based on these categories. Bug synthesis for the repair model involved replacing random variables in code snippets to create training data and a synthetic test set. Real bug evaluation was done by mining Git commit history for projects from Github, focusing on commits with specific keywords related to bugs. The process aimed to avoid biases in the dataset by limiting extraction to commits with one line changes containing bug-related words. The commit contained a word from the list \"bug, error, issue, exception, fix\". Filtered commits to keep those corresponding to bug types, resulting in 926 buggy/repaired snippet pairs. The small number of extracted snippets does not reflect the true frequency of bugs during development. The goal of program repair is to transform a buggy snippet into a repaired snippet using a baseline attention sequence-to-sequence neural network BID4. The model outperformed baseline methods like n-gram language models or classifiers by converting buggy/repaired ASTs to tokenized source code. The neural net vocabulary had 50,000 tokens with a 1.1% OOV rate. LSTMs were 512-dimensional, and decoding was done with a beam of 8. The accuracy on the Single-Repair Synth-Bug Test set was 26% for exact matches and 41% when predicting the correct repair along with other changes. The accuracy of the model is 41% when predicting the correct repair along with other changes. However, using a sequence-to-sequence architecture for semantic code repair has weaknesses such as the burden of constructing the entire output sequence and the lack of fair competition between potential repairs at different locations. An alternative approach is considered where repairs are iteratively applied to the input snippet for each bug type. The proposed approach involves manually written candidate generators that suggest all possible repairs of a given type without heuristic pruning. A repair candidate is a specific fix at a location, while a repair instance includes all candidates at that location. The statistical model determines the correct repair using code context, with each instance having a reference label for the correct candidate. The statistical model used to score repair candidates is a Share, Specialize, and Compete (SSC) network, which encodes the input AST using a neural network. The encoding is conditioned only on the AST itself, serving as a shared representation for the next component. The AST is encoded with a bidirectional LSTM for efficiency, following a depth-first traversal. The AST structure is encoded using embeddings for node position, type, relationship with parent, and surface form string. These tokens are fed into a bidirectional LSTM, producing a vector representation H. The shared component handles most neural computation independently of repairs, contrasting with encoding each repair candidate AST with an RNN. The repair candidate AST is scored using a specialized network module for each repair type. Each module takes a shared representation H and a repair instance R with multiple candidates, producing scores for each candidate. Two module types are used: Multi-Layer Perceptron (MLP) Module for fixed label sets and another type for different repair generators. The Pooled Pointer Module addresses the challenge of predicting variables for VarReplace in code repair by encoding each candidate variable using pointers to its usage in the AST. Separate weights are learned for each repair type, and a scalar score is produced for each repair candidate. The Pooled Pointer Module encodes repair candidates using pointers to their usage in the AST. Two approaches for normalizing scores are considered: Local Norm (softmax per repair instance) and Global Norm (single softmax for all candidates). The SSC model is trained on Synth-Bug Train data for 30 epochs with hidden dimensions set to 512 and embedding size to 128. A dropout of 0.25 is applied to the output of the SHARE component. The model was trained on Synth-Bug Train data for 30 epochs with hidden dimensions set to 512 and embedding size to 128. A dropout of 0.25 was used on the output of the SHARE component. Hyperparameter tuning was performed on the Synth-Bug Val set. The evaluation included snippets with one bug each, both artificial and real bugs. Real-Bug Test set had 926 buggy/fixed snippet pairs from GitHub commit logs. The average snippet in the Real-Bug Test set had 31 repair locations and 102 total repair candidates. The accuracy metric was used to evaluate Single-Repair results on Synth-Bug and Real-Bug test sets. The SSC model outperforms the attentional sequence-to-sequence model on Synth-Bug, even with global and local normalization having similar performance. On Real-Bug Test, SSC model still significantly outperforms the seq-to-seq baseline, with lower absolute accuracy. Preliminary human evaluation was done to understand Real-Bug Test results. In a preliminary human evaluation, example predictions from the Real-Bug Test set are shown. The data was re-generated to include 0, 1, 2, or 3 bugs per snippet in the multi-repair setting. A threshold-based approach is used to determine the number of repairs to predict per snippet in this scenario. The system uses a threshold-based approach to determine the number of repairs to predict per snippet. Results show a moderate decrease in F-score from 85% to 81% between 1-repair and 3-repair settings. The system is 82% accurate at predicting when a repair is needed. The system is 82% accurate at predicting when a repair is needed. A human evaluation was conducted with experienced Python programmers using 4 evaluators and 30 snippets each from the Real-Bug Test set. Evaluators typically spent 2-6 minutes per snippet. The SSC model accuracy is higher on this subset than on the full set. The SSC model accuracy is higher on a subset of snippets compared to human evaluators, achieving 60% accuracy versus 37%. A second evaluation with humans showed 76% accuracy when considering only the top four repair candidates generated by the model. This suggests that the model may outperform humans due to learning from a large dataset. The SSC model outperforms humans in accuracy on a subset of snippets, achieving 60% versus 37%. Humans achieved 76% accuracy when considering the top four repair candidates generated by the model, indicating the model's potential to learn from a large dataset. The model demonstrates strong accuracy in a challenging set, showing its ability to generalize to new snippets and learn high-level repair concepts. The SSC model shows the ability to learn high-level patterns like \"y.x = x\" and \"if (x c 1 y...) elif (x c 2 y...)\", enabling it to generalize to new data. However, it struggles with program intent inference compared to humans due to a lack of sub-word representation. In future work, exploring character-level encoding of value strings for modeling lexical similarity by the network is planned. The SSC model's advantage lies in learning from a large amount of data, enabling it to detect subtle patterns that may be missed by humans. For example, the model can easily distinguish between \"if (x.version_info <= y)\" and \"if (x.version_info < y)\" based on frequency in the training data. The neural model presented a novel architecture for specialized network modules to model transformation types in semantic code repair. It achieved high accuracy and learned sophisticated repair patterns from training data. Future plans include expanding to cover more bug types and applying the model to other tasks. The application of a pooled pointer module was used to predict variable replacement scores. The neural model introduced a specialized architecture for semantic code repair, achieving high accuracy by learning complex repair patterns. It plans to expand to cover more bug types and apply the model to other tasks. The application of a pooled pointer module predicts variable replacement scores based on representations computed by the SHARE module. The neural model for semantic code repair achieves high accuracy by learning complex repair patterns. The model's accuracy remains reasonably high even for functions with over 100 repair candidates. The Python abstract syntax tree provides rich semantic information about code snippets. The model's reliance on information from the AST significantly improves performance, with the SSC model outperforming the attentional sequence-to-sequence baseline by a large margin (78.3% repair accuracy compared to 26%)."
}