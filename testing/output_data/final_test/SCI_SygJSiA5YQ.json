{
    "title": "SygJSiA5YQ",
    "content": "The weak contraction mapping is a self mapping with a unique fixed-point, and a Cauchy sequence iteration. A gradient-free optimization method based on this mapping is proposed for global minimum convergence, overcoming challenges faced by gradient-based methods. Initial point quality and derivative calculation are crucial for successful convergence. In gradient-based methods, the domain is divided into subsets based on local minima, converging to one local minimum depending on the initial point. A mapping T:X \u2192 X in a metric space (X,d) is contractive if d(T(x), T(y)) \u2264 qd(x, y) for q \u2208 [0, 1), nonexpansive if q \u2208 [0, 1], and Lipschitz continuous if q < \u221e. The gradient-based methods are usually nonexpansive, with multiple fixed points in the presence of many local minima in the objective function. In optimization, nonexpansive mappings have drawbacks compared to contractive mappings. The Banach fixed-point theorem is powerful for solving systems, but the condition for contraction mappings can be too strict. The paper aims to extend the theorem to weak contraction mappings, which always map to a subset of their domain and admit a fixed-point. The concept of weak contraction mapping and its fixed-point is discussed in detail in this section. Weak contraction mapping always maps to a subset of its domain and yields a fixed-point, coinciding with the global minimum of the objective function. Definition 1 defines the metric space (X, d, D) where the metric measurements d and D are defined. The weak contraction mapping on a complete metric space X yields a unique fixed-point x* in X. By defining a sequence {x_n} as x_n = T(x_n-1), the mapping converges to a limit x* in X. By choosing large enough m, n, the sequence {x_n} converges to a point x* in X. If there exists another fixed-point y, both x* and y are elements in X. The range of weak contraction mapping shrinks over iterates, leading to a Cauchy sequence. The sequence of iteration is a Cauchy sequence that converges to the global minimum of objective function f(x) if f(x) has a unique global minimum point. Lemma 1.5 states that x* is the global minimum point of the function if there is a unique global minimum point. The weak contraction map is easier to implement in optimization problems compared to the contraction map. The sequence {x_n} must move closer to each other for every step to converge to the fixed-point. To find the global minimum point of the objective function f(x), a weak contraction mapping T: X \u2192 X is used. The mapping is implemented by starting with an initial point x_0, calculating the height L = f(x_0), mapping to another point inside the contour, and updating the searching point by averaging the roots of f(x) = L on the contour. Repeat this process until convergence. The contour-based optimization algorithm utilizes a root-finding algorithm to find n roots of the equation f(x) = L. The roots are distributed widely over the contour to prevent concentration in one area. A smaller q value leads to a higher rate of convergence. The equation f(x) = L represents the intersection of the objective function and a hyperplane. Averaging the roots helps to approximate the minimum position on the contour. Averaging the roots on a contour is an effective way to map near the centroid, with a trade-off between the number of roots and convergence rate. The global minimum point is the fixed-point of the iteration and the contour size decreases during the process, eventually converging to the minimum point of the function. In this study, the lower space X \u2264 is decomposed into convex subsets by checking if two roots belong to the same subset based on function values along segments. If the function values are always lower than the contour's level, the roots belong to the same subset; otherwise, they are in different subsets. This method helps in decomposing roots into different convex subsets. The method involves decomposing roots into convex subsets by checking function values along segments between roots. Random points are used to determine if points are higher than contour's level. This helps in mapping points inside a contour and moving hyperplanes downwards. The algorithm decomposes roots into convex subsets, calculates averages within each subset to find the lowest point, repeats iterations until convergence, and returns the global minimum. Tested on the Ackley function, it successfully converges to the global minimum at (0,0). The optimization method is robust to local minima and achieves global convergence. The stochastic contour-based optimization method decomposes roots into convex subsets, calculates averages to find the lowest point, and iterates until convergence. The weak contraction mapping ensures global minimum convergence regardless of initial point position, making it a robust optimization algorithm. The advanced optimization algorithm with weak contraction mapping aims to enhance modern calculations."
}