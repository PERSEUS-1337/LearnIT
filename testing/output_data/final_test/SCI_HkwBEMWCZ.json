{
    "title": "HkwBEMWCZ",
    "content": "Skip connections are crucial in training deep neural networks by breaking permutation symmetry of nodes and eliminating singularities that slow down learning in the loss landscape. Skip connections in neural networks break permutation symmetry of nodes, reduce singularities, and improve training by sculpting the loss landscape. They are extra connections between nodes in different layers that skip nonlinear processing, leading to faster learning in deep networks. In this paper, the authors explore how skip connections improve training in deep neural networks by eliminating singularities in the loss landscapes. These singularities, such as elimination, overlap, and linear dependence, hinder learning in shallow networks. Skip connections in deep neural networks eliminate singularities in loss landscapes, improving training by ameliorating learning slow-down caused by these singularities. Three types of singularities in fully-connected layers are discussed: elimination, overlap, and linear dependence, all related to model non-identifiability. Linear dependence singularities only occur in linear networks, while the other two can arise in non-linear networks as well. Skip connections in deep neural networks eliminate singularities in loss landscapes, improving training by addressing elimination, overlap, and linear dependence singularities in fully-connected layers. These singularities are related to model non-identifiability, with linear dependence singularities unique to linear networks. Skip connections in deep neural networks break elimination, overlap, and linear dependence singularities by ensuring units remain active and distinct, even when connections become zero or weights become identical. This improves training by addressing model non-identifiability and adding orthogonal inputs to units. The simplified two hidden unit model analyzed in Wei et al. (2008) shows singularity causing degenerate manifolds in the loss landscape, leading to plateaus in learning dynamics. On-singularity plateaus result from random walk behavior of SGD along stable segments, while near-singularity plateaus are more relevant in practical cases. In practical cases, near-singularity plateaus slow down learning dynamics near overlap manifolds, affecting even initial locations outside stable segments. This phenomenon becomes more pronounced with higher hidden unit dimensions, reducing model dimensionality. Linear dependence singularities in linear networks have implications for learning in non-linear cases as well, as shown in a toy single-layer nonlinear network. Learning along a linear dependence manifold, represented by m, is slower than other directions like the norm of the incoming weight vector J c. This issue worsens with depth in deep linear networks. Singularities along linear dependence directions reduce model dimensionality and slow down learning. Experiments with deep fully-connected networks compared architectures with and without skip connections. The residual and hyper-residual architectures introduce skip connections between layers, inspired by DenseNet. Skip connections in hyper-residual architecture are fixed, while in DenseNet they are learned. All networks in the experiments had L = 20 layers. In the experiments, networks with 20 hidden layers and 128 hidden units in each layer were trained on the CIFAR-100 dataset using the Adam optimizer. The biases were initialized to 0 and weights were initialized with Glorot normal scheme. Training accuracy was monitored to understand the impact of singularities on the loss landscape. Degeneracy was measured by estimating the eigenvalue density of the Hessian during training. During training, the eigenvalue density of the Hessian was estimated for different network architectures. The spectral density was analyzed using Skilling's method and a mixture density model, revealing the fraction of degenerate and negative eigenvalues. Validation was done on smaller networks with 14K parameters. The fraction of degenerate and negative eigenvalues was analyzed for different network architectures during training. A linear relationship was found between actual and estimated fractions, with the hyper-residual architecture being the least degenerate and the plain architecture the most degenerate. The differences between architectures were more pronounced early in training, with a potential crossover later on. The hyper-residual architecture has the highest training speed, while the plain architecture has the lowest. Model degeneracy increases training difficulty, and skip connections reduce degeneracy. Training 100 plain networks on CIFAR-100 with different initializations showed the impact of stochasticity on model variability. The worst performing networks on CIFAR-100 were more degenerate and closer to elimination singularities, overlap singularities, and were more linearly dependent. Skip connections were found to reduce degeneracy. A malicious initialization scheme was introduced to investigate the benefits of skip connections. The malicious initialization scheme was used to test the benefits of skip connections by subtracting the identity matrix from the weight matrices. Despite the small adverse effect on performance, it was found that skip connections do not solely rely on favorable initialization for their benefits. This challenges previous explanations based on linear models and reveals that skip connections shift the landscape in deep linear networks, rather than eliminating singularities. In nonlinear networks, skip connections eliminate singularities, improving training by altering the loss landscape around these \"ghosts.\" The success of skip connections suggests that other methods to eliminate singularities could also enhance training. In nonlinear networks, skip connections eliminate singularities, improving training by altering the loss landscape. For each layer, random target biases are drawn from a Gaussian distribution with an l2-norm penalty on learned biases deviating from targets. Positive \u00b5 values decrease unit thresholds, reducing elimination singularities and increasing response dimensionality. Setting \u00b5 = 0 and \u03c3 = 0 corresponds to standard l2-norm regularization, which does not eliminate singularities. In experiments optimizing hyperparameters \u00b5, \u03c3, and \u03bb through random search, 30-layer fully-connected feedforward networks were trained on CIFAR-10 and CIFAR-100 datasets. The residual network outperformed the plain network, with an emphasis on singularity elimination through bias regularization. The BiasReg scheme improves performance over the plain network by eliminating singularities, but the BiasL2Reg does not. Despite both networks breaking degeneracies, the residual network is more effective. Gradient norms also play a role in learning speed, where the residual network outperforms the BiasReg network. The gradient norms in the residual network do not diminish in earlier layers, solving the vanishing gradients problem. As training progresses, the gradient norms are larger for the residual network compared to the plain or BiasReg network. Skip connections boost gradient norms near singularities, reducing learning slow-down. Adding a batch normalization layer in the BiasReg network alleviates vanishing gradients near singularities, improving performance. Skip connections should disambiguate hidden units to prevent slow learning. Mathematically, maximal disambiguation requires orthogonal skip connectivity matrices to break permutation symmetry and prevent linear dependencies between hidden units. Random dense orthogonal matrices outperformed identity skip connections in CIFAR-10 and CIFAR-100 datasets, as units can be deactivated due to ReLU nonlinearity, effectively eliminating skip connections for certain inputs. The use of dense orthogonal skip connections reduces the potential for singularity in network units compared to identity skip connections. This ensures disambiguation of units at different layers even when some are deactivated, as shown in CIFAR-100 dataset analysis. The study analyzed the impact of decreasing the orthogonality of skip connectivity matrices on network performance. Starting from a random dense orthogonal matrix, the number of orthonormal vectors was gradually reduced, leading to a deterioration in performance. This reduction affected the permutation symmetry-breaking and linear-dependence-breaking capacities of the skip connectivity matrix. The results for 128 hidden units showed that matrices with higher orthogonality performed better. The study found that more orthogonal skip connectivity matrices led to better performance for 128 hidden units. The less orthogonal matrices suffered from the vanishing gradients problem, impacting their performance. To address this, skip connectivity matrices with eigenvalues on the unit circle were designed with varying degrees of orthogonality. The study manipulated the orthogonality of vectors by adjusting the covariance matrix, controlling the correlations between vectors. This was done by setting the eigenvalue spectrum of the covariance matrix, with \u03c4 as the parameter controlling orthogonality. The effects of this manipulation were observed while keeping the eigenvalue spectrum of the skip connectivity matrix on the unit circle. The study manipulated the orthogonality of skip vectors to observe performance differences. Results show that more orthogonal skip connectivity matrices outperform less orthogonal ones, even when the eigenvalue spectrum is fixed. The benefits of skip connections are attributed to the elimination of singularities, which contributes to their success alongside other factors. The study showed that skip connections are effective in dealing with vanishing gradients and singularities in deep networks, which can impact training performance. The experiments suggest that singularities from non-identifiability play an independent role in training difficulties, as shown by diverging performances correlated with distance from singular manifolds. Vanishing gradients alone cannot explain the differences in performance observed. The experiments demonstrate that vanishing gradients alone cannot account for the differences in performance between identity skips and dense orthogonal skips. Spectrum-equalized non-orthogonal skips may have larger gradient norms but perform worse than orthogonal skips. The BiasReg results also challenge the explanation based on vanishing gradients. Shallow residual nets show better accuracy and less degeneracy compared to shallow plain nets, with gradient norms and accuracy correlated with distance from overlap manifolds. Norms and accuracy are strongly correlated with distance from overlap manifolds in shallow nets. Malicious initialization experiment with residual nets suggests benefits of skip connections cannot be solely explained by well-conditioning or improved initialization. Skip connections alleviate training difficulties in deep nonlinear networks by addressing the \"shattered gradients\" problem. This problem is distinct from vanishing gradients and is partly responsible for training challenges in deep nonlinear networks. The shattered gradients problem is unique to non-linear deep networks, unlike vanishing/exploding gradients and degeneracy issues. Results are based on experiments with fully connected networks, but similar challenges exist in convolutional neural networks due to limited receptive field sizes and weight sharing. Although permutation symmetry is reduced in convolutional networks, it is not entirely eliminated. Our method of singularity reduction through bias regularization (BiasReg; FIG5) indirectly puts a prior over unit activities, reducing permutation symmetry. Prior regularization schemes aim to improve generalizability and interpretability of learned representations by favoring decorrelated or clustered responses. These schemes can also be understood from a singularity elimination perspective. The results suggest that over-parametrization and redundancy in large neural network models may lead to optimization difficulties, contrary to previous arguments. The ambiguity in the terms \"over-parametrization\" and \"redundancy\" plays a role in this apparent paradox. Over-parametrization increases the model's capacity by offering multiple ways to fit the data, while the degeneracies discussed in the paper reduce the model's capacity, making optimization challenging. The results suggest that over-parametrization and redundancy in large neural network models may lead to optimization difficulties. It is recommended to reduce degeneracies in a model for better optimization, but they may help achieve better generalization performance once training performance saturates. Future work could explore the trade-off between harmful and beneficial effects of degeneracies. The derivative of the cost function with respect to a weight between layers is affected by connections between output and input units. When units have the same incoming weights, the derivatives become identical, leading to a degenerate Hessian matrix. This results in non-identifiable parameters, emphasizing the importance of reducing degeneracies in neural network models for better optimization. In neural networks, when weights become non-identifiable due to overlap singularity, the Hessian matrix becomes singular. Similarly, linear dependence singularity occurs when units become linearly dependent, leading to a close to singular Hessian. This highlights the importance of reducing degeneracies in neural network models for better optimization. When a presynaptic unit is effectively killed, the Hessian matrix becomes singular, as the outgoing connections are no longer identifiable. This is known as an elimination singularity. In the residual case, the factors on the right-hand side of the equation change, eliminating overlap and elimination singularities. In hyper-residual networks, singularities are eliminated by ensuring non-zero inputs and linearly independent units. Skip connections use matrices of orthonormal vectors for better performance. Training data in CIFAR-10 and CIFAR-100 is augmented. In CIFAR-10 and CIFAR-100, training data is augmented by adding mirrored versions of each image, resulting in a total of 100,000 training images. Test data remains unaltered with 10,000 images. Learning dynamics in a network with 3 input, 3 hidden, and 3 output units are simulated, focusing on the norms and unit-vector directions of the parameters. The flow field is empirically confirmed to be generic. Skilling's moment matching method is used to estimate the eigenvalue spectra of the Hessian by computing non-central moments of the density. The Hessian's spectral density is estimated using the eigenvalues, showing unbiased estimates as N approaches infinity. The Hessian's spectral density is estimated using the eigenvalues, showing unbiased estimates as N approaches infinity. The products in m k do not require explicit computation of the Hessian, instead, they can be efficiently computed. Moments of the Hessian are estimated and fitted with a parametric density model using fully-connected feedforward networks. The moments of the Hessian are estimated and fitted with a parametric density model using a skew-normal distribution with 4 parameters. Parameters were fitted using a grid search method due to difficulties with gradient-based methods. The Hessian moments were estimated and fitted with a skew-normal distribution using 100 parameters for 10^8 configurations. Validation was done on smaller networks trained on CIFAR-100 with PCA reducing input dimensionality. Eigenvalue fractions were calculated for degenerate and negative values in residual vs. plain networks. Residual networks outperform plain networks with 16 hidden units in each layer, showing better training and test performance. They are less degenerate and have more negative eigenvalues. Results are consistent with deeper and larger networks. Validation on 400 independent plain networks reveals that the best-performing ones are less degenerate and have more negative eigenvalues compared to the worst-performing ones. The best networks have more negative eigenvalues and less overlap in hidden units compared to the worst networks. They also have slightly larger weight norms during training. The results are consistent with those of deeper and larger networks. Additionally, a mixture model was used to estimate fractions of degenerate and negative eigenvalues in plain networks. The analysis focuses on networks with a high fraction of degenerate eigenvalues. Using 10-layer plain networks with 32 hidden units, a correlation between actual and estimated fractions was observed. Linear networks were then used to study the effects of skip connections on learning dynamics. In linear residual networks with identity skip connections, the input-output mapping is defined. In hyper-residual linear networks, all skip connection matrices are assumed to be the identity. The connectivity matrices can be rectangular, with the identity matrix interpreted as a rectangular identity matrix for zero-padding. The learning dynamics of a three-layer network can be expressed by differential equations. The learning dynamics of a three-layer network with skip connections can be described by differential equations, facilitating cooperation and competition between vectors connecting hidden layers to input and output modes. Adding skip connections between adjacent layers alters the learning dynamics, especially in scenarios with only two input and output modes. The learning dynamics of a three-layer network with skip connections can be described by differential equations, showing changes in dynamics when using residual weight matrices and orthogonal matrices for input and output modes. The dynamics in a network with skip connections evolve faster due to added orthonormal vectors, leading to an advantage over a plain network even when initial norms are equalized. Cooperative and competitive terms are orthogonal in the residual network, enhancing performance. In linear three-layer networks, the singularity of the Hessian leads to gradient descent on an energy function. Weight initialization and learning rate settings differ between plain and residual networks. Skip connections between layers enhance convergence by ensuring orthogonality of cooperative and competitive terms. In linear networks with more than three layers, orthogonal skip connections prevent degeneracy but shift singularities. Residual networks exhibit overlap and elimination singularities similar to plain networks with certain variable changes. In linear networks with skip connections, the energy function changes based on the type of network. Residual networks exhibit overlap and elimination singularities similar to plain networks with certain variable changes. The effect of skip connections on the phase portrait of a three-layer network is illustrated in FIG5. The dynamics of learning in linear networks with skip connections are influenced by the position of the saddle point. Identity skip connections in residual networks move the saddle point closer to the origin, while hyper-residual networks move it further away. Deeper networks show more pronounced effects, with the hyper-residual architecture demonstrating a clear advantage in learning dynamics over the plain and residual architectures. The Hessian in reduced linear multilayer networks with skip connections becomes degenerate when mode strengths are equal, but the hyper-residual architecture shifts these degeneracies in parameter space by adding constants. The covariance matrix of eigenvectors is generated using a random orthogonal matrix and a diagonal matrix of eigenvalues. The correlation matrix is found using the diagonal matrix of variances. The skip connectivity matrix is constructed using Cholesky decomposition. Eigenvectors of the skip connectivity matrix are not orthogonal unless \u03c4 = 0. Larger \u03c4 values result in more correlated eigenvectors. The matrix has the same eigenvalue spectrum as the randomly generated orthogonal matrix. The experiment compared plain and residual networks with two hidden layers and 16 units each. The residual network performed slightly better on both training and test data, had less degeneracy, more negative eigenvalues, larger gradients, and less overlap between hidden units compared to the plain network. Gradient norms closely tracked mean overlap and network degeneracy. The experiment compared plain and residual networks with two hidden layers and 16 units each. The residual network performed slightly better on both training and test data, had less degeneracy, more negative eigenvalues, larger gradients, and less overlap between hidden units compared to the plain network. Results suggest that the degeneracies caused by the overlaps of hidden units slow down learning, consistent with the symmetry-breaking hypothesis and results from larger networks."
}