{
    "title": "HyxPx3R9tm",
    "content": "In this work, a technique called variational discriminator bottleneck (VDB) is proposed to stabilize adversarial learning models by constraining information flow in the discriminator. This method effectively balances the generator and discriminator performance by modulating the discriminator's accuracy and maintaining informative gradients. The VDB shows significant improvements in various applications of adversarial learning algorithms. The VDB improves adversarial learning models by constraining information flow in the discriminator, leading to better performance in dynamic continuous control skills and GAN training for image generation. In this work, a simple regularization technique is proposed for adversarial learning, which limits the information flow from inputs to the discriminator using a variational approach. Our approach to stabilizing adversarial learning involves constraining the mutual information between input observations and the discriminator's internal representation, encouraging overlap between data and generator's distribution. This adaptive method automates noise magnitude selection, limiting discriminator accuracy for effective gradient maintenance. The main contribution of this work is the variational discriminator bottleneck (VDB), an adaptive stochastic regularization method for adversarial learning that substantially improves performance across different application domains. The method can be easily applied to various tasks and architectures, such as learning acrobatic skills from mocap data and dynamic continuous control skills from raw video demonstrations. The text discusses the use of adversarial imitation learning and the effectiveness of the technique for inverse reinforcement learning. It also applies the framework to image generation using generative adversarial networks, showing improved performance in many cases. The recent surge in adversarial learning techniques, driven by the success of generative adversarial networks, is highlighted. The challenges of training adversarial models, such as instability and balancing, are also mentioned. Adversarial models face challenges in balancing the discriminator and generator interaction. Various techniques like alternative loss functions, regularizers, and task-specific designs have been proposed to improve stability and convergence. One approach involves applying an information bottleneck to the discriminator to enhance feedback to the generator. Applying an information bottleneck to the discriminator enhances feedback to the generator, allowing it to focus on improving differences between real and fake samples. Adversarial techniques have been used in inverse reinforcement learning and imitation learning to train policies without explicitly recovering a reward function. Leveraging a discriminator instead of a reward function can be beneficial for imitating skills where reward functions are challenging to engineer. Our method improves upon adversarial techniques in training policies without explicitly engineered reward functions. The variational discriminator bottleneck, based on the information bottleneck, enhances generalization by minimizing irrelevant information in the input. This approach produces results comparable to state-of-the-art methods using manually designed reward functions. The variational information bottleneck enhances generalization in deep models by approximating compression effects. It combines techniques from VAEs and GANs to encourage similarity between latent encoding and prior distribution. Instance noise is commonly used in modern architectures, but explicitly enforcing an information bottleneck can also be effective. The variational information bottleneck enhances generalization in deep models by approximating compression effects. It combines techniques from VAEs and GANs to encourage similarity between latent encoding and prior distribution. Explicitly enforcing an information bottleneck leads to improved performance over simply adding noise in various applications like GANs, inverse RL, and imitation learning. The bottleneck encourages the model to focus on the most discriminative features, reducing overfitting and preventing the model from exploiting data idiosyncrasies. The variational information bottleneck enhances generalization in deep models by enforcing an upper bound on the mutual information between the encoding and the original features. This regularization technique improves performance by focusing only on discriminative features and preventing overfitting. The method introduces a variational information bottleneck in a GAN framework to improve model generalization. By adding an encoder to the discriminator and applying a constraint on mutual information, the model becomes less prone to overfitting and more robust to adversarial examples. The framework introduces a variational discriminator bottleneck (VDB) in a GAN model to improve generalization. By enforcing a specific mutual information budget between x and z, the model becomes less prone to overfitting and more robust to adversarial examples. Incorporating a VDB in a GAN model as a VGAN improves generalization by enforcing a mutual information budget between x and z. The encoder models a Gaussian distribution in latent variables Z, and the generator's objective excludes the KL penalty. The discriminator is a single linear unit followed by a sigmoid. The VDB helps improve robustness to adversarial examples. The VDB in a GAN model enforces a mutual information budget between x and z. By adding continuous noise to discriminator inputs, distributions with disjoint support can have continuous support everywhere. Adjusting the noise variance dynamically using a learned encoder and information bottleneck helps ensure convergence. The VDB in a GAN model enforces a mutual information budget between x and z by adjusting noise variance dynamically. This prevents perfect differentiation between distributions, allowing for smoother decision boundaries and more informative gradients for the generator. The VDB in a GAN model enforces a mutual information budget between x and z by adjusting noise variance dynamically. This prevents perfect differentiation between distributions, allowing for smoother decision boundaries and more informative gradients for the generator. The gradient of the generator will be non-degenerate for a small enough constraint, ensuring that coefficients are always bounded below in the VDB. This result is presented in Appendix A. VAIL uses a discriminator to differentiate between target and agent policies, guiding the agent to mimic target states. The discriminator also acts as the reward function, encouraging the agent to visit states indistinguishable from demonstrations. A VDB can be incorporated into the discriminator, with the reward for the policy specified by the discriminator. This method is referred to as variational adversarial. Variational adversarial imitation learning (VAIL) uses a discriminator to guide the agent to mimic target states. A new algorithm, variational adversarial inverse reinforcement learning (VAIRL), can be derived from VAIL. VAIRL operates similarly to GAIL but with a different discriminator form. Fu et al. show that under certain restrictions, the learned reward function can be used to train policies in environments with different dynamics. In VAIRL, stochastic encoders are introduced to modify functions of the encoding. The objective is reformulated to include joint distribution of successive states from a policy. Learning curves comparing VAIL to other methods for motion imitation show improved performance. VDB enables agents to learn complex motion skills from a single demonstration in imitation learning. The VDB allows agents to learn complex motion skills from a single demonstration, including visual demonstrations like video clips. It also enhances the performance of inverse RL methods, which reconstruct a reward function from demonstrations to perform tasks in new environments. The method is versatile and effective for unconditional image generation, not limited to control tasks. The goal is to train a simulated character to mimic mocap clips from human actors, tracking target states at each timestep. The discriminator architecture greatly affects performance on complex skills in a 34 degrees-of-freedom humanoid character. The encoding Z is 128D with an information constraint of I c = 0.5 and a dual stepsize of \u03b1 \u03b2 = 10 \u22125. All policies are trained using PPO. VAIL can reproduce various skills, including dynamic flips and complex contacts, from a single demonstration. Comparisons are made with other techniques like state-only GAIL and GAIL with instance noise. The discriminator architecture significantly impacts performance on complex skills in a 34-degree-of-freedom humanoid character. Various techniques like GAIL with instance noise and GAIL with a gradient penalty are compared, with a model combining VAIL and GP also being trained. Performance is evaluated based on the average joint rotation error between the simulated character and the reference motion. VAIL outperforms previous adversarial methods in training policies for complex skills in a humanoid character. VAIL-GP achieves the best performance overall. Adding instance noise without the KL constraint leads to worse performance as the network can learn a latent representation that negates the noise effects. VAIL demonstrates comparable performance to handcrafted reward without manual engineering, producing motions resembling original demonstrations. Previous methods struggle with reproducing complex skills like backflips and spinkicks. Despite more demonstration data, behavioral cloning fails to replicate skills. Our method achieves better results in motion imitation compared to prior work. The goal of the agent is to directly imitate skills from video demonstrations, making manually engineering rewards impractical. VAIRL learns a smoother reward function for reliable transfer of skills, unlike AIRL which overfits to specific tasks. Performance on flipped test mazes shows VAIRL's mean return is more consistent. The VAIL method successfully learns to imitate skills from video demonstrations, outperforming GAIL and pixel-loss methods. Visualizations of the VDB gradient and discriminator saliency maps are provided for further investigation. The VAIL method outperforms GAIL in learning skills from video demonstrations. VAIL's discriminator shows structured attention to image patches, with larger gradients compared to GAIL. Adaptive \u03b2 updates improve learning speed, with dual gradient descent achieving consistent performance. VAIRL can recover reward functions from demonstrations and re-optimize them to train new policies in the same environment or transfer behavior to different environments. It was applied to C-maze and S-maze tasks, showing results in Figure 7. The expert navigates from start to target positions, and the recovered reward is used to train new policies for solving tasks. VAIRL can recover reward functions from demonstrations and re-optimize them to train new policies in the same environment or transfer behavior to different environments. In the C-maze, plain AIRL sometimes overfits and fails to transfer, while VAIRL with VDB learns a smoother reward function for better transfer. In the S-maze, AIRL is unstable in learning a reward function, but VAIRL performs better without a gradient penalty and improves further with it. The performance of VAIRL drops on both tasks when the KL constraint is disabled. The performance of VAIRL drops on both tasks when the KL constraint is disabled (\u03b2 = 0), suggesting that improvements from the VDB cannot be solely attributed to noise introduced by the sampling process for z. Experiments on image generation with VGAN are conducted on CIFAR-10, CelebA, and CelebAHQ datasets, comparing to stabilization techniques like WGAN-GP, instance noise, spectral normalization (SN), and gradient penalty (GP). Performance is measured using Fr\u00e9chet Inception Distance (FID), showing consistency with human evaluation. The methods compared on CIFAR-10 include vanilla GAN, instance noise, VGAN, VGAN -GP, and VDB. VGAN remains stable during training, outperforming vanilla GAN and instance noise. Combining VDB and gradient penalty achieves the best performance with an FID of 18.1. Samples of generated images can be seen in FIG6. Refer to Appendix E for more experimental details and results. The variational discriminator bottleneck (VDB) is a regularization technique for adversarial learning that shows significant improvements over previous methods on challenging tasks. While promising results have been achieved for video imitation, primarily with synthetic scenes, extending the technique to real-world videos is an exciting direction for future work. Additionally, a more in-depth theoretical analysis of the method is suggested to derive convergence and stability results. The main theorem (Theorem A.1) introduces the concept of a variational discriminator bottleneck (VDB) in adversarial learning. It outlines constraints and the gradient passed to the generator, emphasizing the importance of the encoder's mapping function and the optimal discriminator. The analysis hints at the potential for extending the method to average constraints in future research. The main theorem introduces the variational discriminator bottleneck (VDB) in adversarial learning, emphasizing constraints and gradients for the generator. It shows that the gradient of the generator points towards data distribution and away from generator distribution, with coefficients bounded by a positive constant as input-dependent variance decreases. The proof presented accounts for noise injected into the latent space of the VDB, assuming an input-independent variance. It discusses the distribution of embeddings under real data and generator, optimal discriminator, and gradient passed to the generator. The gradient drives the generator's samples towards embeddings of points from the dataset weighted by their likelihood under real data. The theorem shows that the coefficients in the embedding may be small, leading to vanishing gradients for the generator. It also proves that C(I c ) is a continuous monotonic function that approaches a positive value as I c approaches 0. This result is based on the KL divergence between the encoded distributions and a prior distribution. The distance between encoded distributions shrinks with I c, ensuring that coefficients on vectors are at least as large as C(I c). The KL divergence bounds the embedding vectors' length and the likelihood is bounded below by a lower bound. A lower bound and an upper bound on \u03c3^2 can be derived from the KL constraint. The reparameterization trick is used to combine VDB with gradient penalty. The coefficient w GP weights the gradient penalty in the objective for different tasks. It is applied only to real samples. The goal of motion imitation tasks is to train a simulated agent to mimic a demonstration provided in the form of a mocap clip. The character's body configuration is represented by link positions and velocities, with a phase variable \u03c6 to synchronize motion progress. The policy \u03c0(a|s) samples actions for PD controller poses at joints, modeled with a 3-layered network. The value function has a similar architecture with a linear output unit. The policy is queried at 30Hz during physics simulation. The policy is trained using PPO with specific stepsizes for different components. Gradient descent with momentum 0.9 is used for all models. The PPO clipping threshold is set to 0.2. Evaluation is done with a maximum horizon of 20s, with early termination triggered by torso-ground contact. The discriminator is modeled with a phase-functioned neural network to capture time-dependency. The phase-functioned neural network (PFNN) models the time-dependency of reference motion by using phase-dependent weights to determine network parameters. In the implementation, 5 sets of parameters are used, with linear interpolation between them for each phase value. Performance comparison of VAIL with other methods for motion imitation is shown in learning curves. The phase-functioned neural network (PFNN) is used to model the time-dependency of reference motion with 5 sets of parameters and linear interpolation. VAIL's performance is compared to other methods for motion imitation using learning curves. The phase-functioned neural network (PFNN) is compared to fully-connected networks as discriminators. PFNN shows significant performance improvements, especially for complex skills like dance and backflip. Phase information is crucial for performance, with PFNN outperforming fully-connected nets. The accuracy of discriminators trained using different methods is illustrated in FIG0. Discriminators trained via GAIL quickly differentiate between samples, even with instance noise. VAIL without the KL constraint slows progress but reaches near perfect accuracy with more samples. Enforcing the KL constraint constrains the discriminator's performance to around 80% accuracy. The dual gradient descent update enforces the VDB constraint effectively. In video imitation tasks, a simplified 2D biped character is used to avoid depth ambiguity. The video imitation tasks use a 2D biped character with 12 degrees-of-freedom for demonstrations. The agent aims to imitate the motion shown in the video without access to the original reference motion. Two maze tasks are evaluated, including the C-maze with continuous 2D action space and ground truth reward formulation. The S-maze is a larger variant of the C-maze environment with added reward shaping to encourage passing between walls. Policy networks for all methods were two-layer ReLU MLPs with 32 hidden units per layer. Reward and discriminator networks had 32-unit mean and standard deviation layers. Environments will be released with VAIRL implementations. The TRPO BID40 agent was trained on ground truth reward for 200 iterations to generate expert demonstrations. IRL and imitation methods were then trained on a maze for 200 iterations using TRPO as the policy optimizer. Discriminator updates were done between policy updates using Adam. VAIRL runs used a target KL of 0.5 for the C-maze and a more complex S-maze was used for testing. The C-maze and S-maze were used for training and testing, with VAIRL showing smoother reward function recovery compared to baselines. Different hyperparameters were used for testing the two mazes, with VAIRL performing better in matching the ground truth reward. The addition of a gradient penalty enhances the reward effect for both AIRL and VAIRL, especially in S-maze. Combining the gradient penalty with a variational discriminator bottleneck leads to a smooth reward increase as the agent approaches its goal. Further experiments on image generation and details of the setup are provided, including the use of non-saturating objectives and FID computation on samples. RMSprop with a fixed learning rate is used for all experiments, and a variational discriminative bottleneck is implemented as a 1x1 convolution on the final embedding space. For image experiments, a Gaussian distribution over Z is used with a mean and diagonal covariance matrix. Adaptive \u03b2 update with a dual stepsize of \u03b1 \u03b2 = 10 \u22125 is employed. Instance noise is added to the final embedding space of the discriminator. Resnet-based architecture is used for CIFAR-10, while the same architecture is used for CelebA and CelebAHQ. Models are trained from scratch at full resolution."
}