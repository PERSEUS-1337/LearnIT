{
    "title": "H1l_gA4KvH",
    "content": "The problem addressed is generating configurations for optimal material nano-pattern design that satisfy physical constraints. This involves balancing thermal resistance, electrical conductivity, and mechanical stability for efficient thermoelectric design. The approach leverages posterior regularization and formulates the problem as sampling from a Gibbs distribution. Challenges arise from the complex nature of the physical constraints, which are obtained through solving non-linear PDEs. To address this, Surrogate-based Constrained Langevin dynamics for black-box sampling is introduced, utilizing two surrogate approaches. The Zero-Order Langevin method approximates gradients in Langevin Sampling, while the second approach uses deep neural networks for efficient sampling. Both methods are proven to converge for log-concave and smooth target distributions. They are effective in designing optimal nano-porous material configurations with low thermal conductivity and mechanical stability. These approaches are useful for real-world design problems with multiple constraints, such as computational material design. In material design, constrained black-box sampling is approached as a Gibbs distribution on a compact support. This problem is solved using constrained Langevin dynamics with gradient-free methods, building on zero-order optimization techniques. In material design, constrained black-box sampling is tackled as a Gibbs distribution on a compact support using constrained Langevin dynamics. To address computational challenges, a Surrogate Model Based Langevin dynamics approach is proposed, involving learning an approximation of the gradient of the potential of the Gibbs distribution. This method aims to improve the mixing of the Langevin dynamics towards the target Gibbs distribution by focusing on learning the gradient rather than the potential itself. Our main contributions include casting the problem of generating samples under constraints in the black-box setting as sampling from a Gibbs distribution. We introduce Constrained Zero-Order Langevin Monte Carlo and Surrogate Model Based Projected Langevin Monte Carlo, using deep neural networks or reproducing kernel spaces to learn the gradient of the potential of the Gibbs distribution. The study introduces Constrained Zero-Order Langevin Monte Carlo and Surrogate Model Based Projected Langevin Monte Carlo for generating samples under constraints in a black-box setting. It emphasizes the importance of approximating the gradient of the potential using Hermite and Taylor learning. The proposed methods are showcased for designing nanoporous configurations with improved thermoelectric efficiency, focusing on optimizing pore locations for minimal thermal conductivity and desired mechanical stability. The study focuses on satisfying equality and inequality constraints in a black-box optimization setting, where functions can only be evaluated point-wise without access to their gradients. Lagrangian parameters are used to relax the objective, similar to previous frameworks but with a focus on bounded domains and a black-box setting. The solution to the constraint satisfaction problem in a black-box setting involves sampling from a Gibbs distribution defined on a compact support. This distribution, also known as Boltzmann distribution, is obtained using Langevin dynamics. The material configuration is determined by pore locations, material properties, and responses to heat or stress flows. In Bayesian optimization, the focus is on finding a point that meets constraints, while in this study, the goal is to find a distribution of candidate samples that satisfy constraints. The paper assumes a uniform distribution on a compact domain, with gradients being zero on the domain's support. Sampling from the distribution involves a mixture of black-box sampling on constraints. In this section, we discuss sampling from a distribution using a mixture of black-box and white-box sampling techniques. Assumptions include \u2126 being a convex set containing a Euclidean ball, with the projection onto \u2126 denoted as P \u2126 (x). The function U is assumed to be convex, \u03b2-smooth, and have bounded gradients. The Total Variation (TV) distance between two measures \u00b5, \u03bd is defined as follows. In unconstrained Langevin Dynamics, sampling from a Gibbs distribution with unbounded support can be done via the Langevin Monte Carlo (LMC) algorithm. Constrained Langevin Dynamics aims to sample from a distribution within a specific set, with Projected Langevin Monte Carlo (PLMC) being a variant that shows mixing properties towards the stationary distribution. PLMC consists of a single iteration of LMC followed by a projection on the set \u2126 using the operator P \u2126. Proximal Langevin Dynamics, introduced by Brosse et al. (2017), uses ProxLMC with a step size \u03b7 and regularization parameter \u03b3. ProxLMC performs an ordinary LMC with updates converging to the target Gibbs distribution \u03c0. In the context of Proximal Langevin Dynamics introduced by Brosse et al. (2017), the current study presents two strategies for approximating the gradient of the potential in a black-box setting. The first strategy involves borrowing ideas from derivative-free optimization, while the second strategy entails learning a surrogate deep model to approximate the gradient of the potential. The Surrogate Projected Langevin Dynamics and Surrogate Proximal Langevin Dynamics are introduced as methods to replace the potential gradient with a surrogate gradient for optimization purposes. The study introduces Surrogate Projected Langevin Dynamics and Surrogate Proximal Langevin Dynamics as methods to approximate the gradient of the potential in a black-box setting. The main theorems focus on bounding the total variation distance between the trajectories of the surrogate dynamics and the true dynamics. Theorems are based on techniques in Stochastic Differential Equations and Kullback-Leibler divergence. The distribution of random variables X and Y obtained by iterating PLMC and S-PLMC are denoted as \u00b5 PLMC K and \u00b5 S-PLMC K respectively. Similarly, the distributions obtained by iterating ProxLMC and S-ProxLMC are denoted as \u00b5 ProxLMC K and \u00b5 S-ProxLMC K. The convergence of Surrogate Constrained LMC to the Gibbs distribution is proven under certain assumptions. In zero-order optimization, the Gaussian distribution is considered. In zero-order optimization, Gaussian smoothed potential with i.i.d. standard normal vectors is considered. Constrained Zero-Order Projected LMC and Zero-Order Proximal LMC are defined for log-concave densities with compact support. The uniform approximation of gradients in expectation allows for the application of Zero-order Constrained Langevin to approximate the Gibbs distribution. In zero-order optimization, a neural surrogate model is introduced to estimate the gradient of the potential of the Gibbs distribution. This is necessary for the convergence of constrained Langevin dynamics. The computation cost of zero-order constrained Langevin is high, so the neural surrogate model serves as an alternative to the true potential gradient. In zero-order optimization, a neural surrogate model is used to estimate the gradient of the potential of the Gibbs distribution, which is crucial for the convergence of constrained Langevin dynamics. To reduce computational costs, surrogate neural network models are trained to approximate PDE outputs and gradients in material design problems. This approach, known as Hermite Learning, involves regressing the value of the property \u03c8 and matching zero-order gradient estimates using a function class H \u03b8. In deep learning, Jacobian matching is used to optimize objectives efficiently with deep neural networks. When using inexact zero-order gradients, there is an additional bounded numerical error. Constructing the dataset for Jacobian matching remains expensive due to the need for multiple queries for each point. In deep learning, Jacobian matching is utilized to optimize objectives efficiently with deep neural networks. The construction of the dataset for Jacobian matching is costly, requiring 2n + 1 queries of the PDE solver for each point. The Taylor learning framework of gradients is leveraged to learn a surrogate potential and gradient consistent with the first-order Taylor expansion. Mukherjee & Zhou (2006) and Wu et al. (2010) propose objective functions to parameterize the surrogate potential and its gradient using deep learning techniques. The Taylor learning approach leverages first-order approximations in local neighborhoods to construct a surrogate model without the need for zero-order estimation of gradients. Both Hermite and Taylor learning provide theoretical guarantees when the approximation function space is an RKHS. The surrogate model LMC is defined by replacing in the constrained Langevin dynamics. In Hermite and Taylor learning, theoretical guarantees are provided for the approximation of the target function \u03c8. Assumption D requires a learned surrogate G on the training distribution \u03c1 \u2126 to hold for all intermediate distributions in Langevin sampling. This allows for an \u03b5-approximation of the target Gibbs distribution in terms of total variation distance. Assumption D on the -approximation of the gradient can be achieved for a large enough training set N in Hermite and Taylor learning. Surrogate constrained LMC, referred to as x-PLMC or x-ProxLMC, involves zero-order optimization with Gaussian smoothing. Previous work on non-convex zero-order optimization is related, but our approach differs in handling compact density. In our setting, the density has a compact support, requiring the use of projected LMC and Proximal LMC. Mirror Langevin sampling by Hsieh et al. can also be utilized. Gradients can be estimated using gradient distillation and learning gradients methods. Other approaches like score matching and dual embeddings can also be leveraged. Estimating gradients can be done using Stein's method or maintaining a surrogate of the gradient as in Stein descent. Optimization methods commonly used for material design include Bayesian Optimization (BO) and Genetic Algorithms. BO involves estimating the black-box function using a probabilistic surrogate model, often a Gaussian process, and maximizing an acquisition function to determine where to sample next. BO can be extended to address optimization under black-box constraints, as proposed by Hern\u00e1ndez-Lobato et al. Genetic Algorithms are also utilized in optimal material design. Genetic Algorithms (GA) are commonly used for generating material samples with desired properties and handling optimization under constraints. However, GA can be limited by requiring a large number of function evaluations and getting stuck in local optima. Deep reinforcement learning techniques, such as Deep Q-networks, have also been used for optimizing molecules under specific constraints. The advantage of the framework discussed is the ability to obtain a distribution of optimal configurations without relying on pre-existing datasets. In this section, the usability of a black-box Langevin sampling approach for designing nano-porous configurations is demonstrated. Surrogate models are used to learn the potential function, with four different variants compared. The surrogate-based Langevin Monte Carlo method is shown to generate new samples under thermal and mechanical constraints. The sample quality is compared between surrogate and zero-order approaches. A dataset of 50K nano-porous structures is used to train the surrogate models, with each structure being 100nm \u00d7 100nm in size and containing 10 pores. The study constructs nano-porous structures with 10 square pores of 17.32nm side length. Using OpenBTE and Summit solvers, thermal conductivity \u03ba and von Mises stress \u03c3 values are obtained for each structure. Pore locations serve as input features for surrogate models, with additional features like pore-pore distances considered for thermal conductivity. More details on PDEs and solvers are provided in Appendices B and C. The study uses nano-porous structures with square pores to obtain thermal conductivity and von Mises stress values. Pore locations are input features for surrogate models, with pore-pore distances considered for thermal conductivity calculations. Feed-forward neural networks with specific architecture and activation functions are used to model the surrogates efficiently. The networks are trained on objective functions with an Adam optimizer and fine-tuned through grid-search to select the best models. The study uses nano-porous structures with square pores to obtain thermal conductivity and von Mises stress values. Pore locations are input features for surrogate models, with pore-pore distances considered for thermal conductivity calculations. Feed-forward neural networks with specific architecture and activation functions are used to model the surrogates efficiently. The networks are trained on objective functions with an Adam optimizer and fine-tuned through grid-search to select the best models. Black-box Langevin sampling is proposed for designing nano-configurations under thermal conductivity and mechanical stability constraints, with comparison metrics including minimum value of \u03ba and Monte Carlo estimates for \u03ba and \u03c3. The study focuses on designing nano-configurations with low thermal conductivity for high thermo-electric efficiency. Using Gibbs distribution, 20 samples are obtained from the target distribution \u03c0(x) using black-box Langevin MCs. Surrogates like regression, Taylor-Reg, Taylor-1, and zero-order are used with projection or proximal update. Summary statistics are shown in Table 1, with regression-PMLC and regression-ProxLMC representing the sampling methods. The study focuses on designing nano-configurations with low thermal conductivity for high thermo-electric efficiency. Surrogates like regression, Taylor-Reg, Taylor-1, and zero-order are used with projection or proximal update. The regression-based method is less effective due to lack of implicit objective for approximating gradients. Taylor-Reg and Taylor-1 demonstrate effectiveness in approximating gradients and achieving lower thermal conductivity. Taylor-1-ProxLMC and Zero-order-PLMC perform similarly in terms of minimum achieved, but Taylor-1 offers 17x speed up over zero-order methods. Taylor-2 works similarly to Taylor-1. In the study, nano-configurations are designed to achieve low thermal conductivity for high thermo-electric efficiency. Multiple constraints are considered, including thermal conductivity and mechanical stability provided via von Mises stress. The goal is to minimize thermal conductivity while keeping the von Mises stress below a certain threshold. The framework involves sampling from a Gibbs distribution and relaxing the inequality constraint to a Hinge loss term on von Mises stress. In this framework, the inequality constraint is relaxed to the Hinge loss term on von Mises stress. Results are summarized in Table 2, showing improved \u03ba and reduced \u03c3 simultaneously. The approach effectively samples new configurations under multiple competing constraints, as demonstrated by the statistics of 20 new samples obtained. Surrogate-Based Constrained Langevin Sampling was introduced for black-box sampling from a Gibbs distribution on a compact support. Two approaches were studied: zero-order methods and learning gradient approximations using deep neural networks. Convergence proofs were shown in the log-concave and smooth case. The learned surrogate model Langevin had a good tradeoff between computation and approximation power. The method was applied to nano-material configuration design with promising results. Zero-order gradient approximations showed superior performance. Surrogate gradient methods utilize neural networks to model gradients efficiently, offering a 15x speedup over zero-order methods. The networks have 4 hidden layers with ReLU activation and sigmoid output, trained using Adam optimizer. The networks are fine-tuned with grid-search to select the best models for comparison. Surrogate models are evaluated based on how well they generalize on a hold-out test set using RMSE. Taylor-Reg generalizes better and converges faster than Taylor-1 and Taylor-2 for predicting \u03ba, while all methods perform similarly for predicting \u03c3. Figure 3 illustrates the sample complexity of Taylor-Reg, Taylor-1, and Taylor-2 for \u03ba prediction, showing Taylor-Reg outperforming the others. Z-Hermite learning was not included in the comparison due to the high cost of obtaining necessary data, but a separate study demonstrated the effectiveness of Z-Hermite surrogate LMC. The study demonstrated the effectiveness of Z-Hermite surrogate LMC on a smaller data with OpenBTE version 0.9.55. Results show the model's ability to learn the gradient of \u03ba(x) and estimate entropy based on nearest neighbor data. A trade-off between computation and accuracy was observed, with zero-order PLMC and ProxLMC achieving low thermal conductivity but high computational costs. In contrast, deep surrogate models are more time-efficient but slightly less optimal. A hybrid algorithm is proposed to balance these trade-offs. The proposed hybrid method combines zero-order and Taylor-1 surrogate models to balance accuracy and computation trade-offs. The hybrid algorithm alternates between using gradients from both models, achieving lower thermal conductivity than Taylor-1 PLMC while running faster than zero-order PLMC. Further improvements can be made by collecting zero-order gradients and updating the surrogate with Z-Hermite learning. The text discusses a hybrid PLMC algorithm that alternates between zero-order and Taylor-1 surrogate gradients to train a network. It also mentions using the Boltzmann transport equation for heat transport at the nanoscale, with coefficients determined by the bulk MFP distribution. The algorithm aims to balance accuracy and computation trade-offs, achieving lower thermal conductivity than Taylor-1 PLMC while running faster than zero-order PLMC. In this work, the gray model is assumed where all phonons have the same mean free path (MFP) of \u039b 0 = 10 nm. The thermal conductivity is computed iteratively for each phonon direction over an unstructured mesh. Periodic boundary conditions are applied along the unit cell with a temperature difference of \u2206T = 1 K along the x-axis. The effective thermal conductivity is calculated using Fourier's law, with the heat flux J = (\u03ba bulk /\u039b 0 ) T (\u039b 0 )\u015d n. The mechanical stress in the simulation is modeled using linear elasticity equations with periodic boundary conditions. The von Mises stress is calculated to ensure mechanical stability and prevent material plasticity. The stress needs to be below the yield stress of the material for the simulation to be accurate. The stress in the simulation must be lower than the material's yield stress. For mechanical simulation, the SUMIT code was used. Optimality conditions are set for q and x in \u03a9. The proof involves Projected Langevin and continuous processes defined by interpolation of X and Y. The data processing inequality is applied."
}