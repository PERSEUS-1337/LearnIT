{
    "title": "rJxoi1HtPr",
    "content": "Humans can learn and deploy various skills to handle daily situations, while neural networks have limited memory capacity, hindering their ability to learn multiple skills without forgetting them. In this work, a model with a growing memory capacity is proposed to bridge neural networks with human-like learning capabilities. The model shows improved adaptation skills in a continual learning task based on language modeling, where it can recover faster after a switch in input language or domain. The comparison is made to human brains' ability to store massive amounts of information in long-term memory. The curr_chunk discusses how humans can retain vast amounts of information in their long-term memory, even if it is not always relevant. It highlights the challenge of \"catastrophic forgetting\" in neural network models and the need for models with growing memory capacity to mimic human learning capabilities. The curr_chunk emphasizes the importance of developing models with growing internal capacity to address catastrophic forgetting in neural networks. Previous approaches relied on fixed-sized networks, leading to eventual capacity limitations. In contrast, the proposed models aim to grow their capacity to prevent forgetting without the need for an explicit task identification signal. In this paper, a recurrent neural network is introduced that can grow its memory by creating new modules as training progresses, aiming to address catastrophic forgetting in neural networks without the need for an explicit task identification signal. Our model introduces a multilingual/multidomain language modelling task with switching domains to adapt to the current context. It includes character-based and word-based benchmarks in different languages and domains, where models autonomously discover switches. Experimental results show faster domain switching compared to other neural networks, with a general model adaptable to various neural network architectures. Neural networks can be developed with unbounded memory by allowing them to grow in layers or architecture. Early work focused on reducing computational complexity, but now the goal is to increase memory capacity. This strategy may be limited as all units remain fully connected, forcing the network to access all memories simultaneously. Neural architecture search and neuro-evolution offer different growth models for learning systems. Stack-RNNs allow models to create new units for flexible memory but are limited to toy problems and struggle with retrieving distant memories quickly. The proposed Growing Long-Term Memory Network (GLTMN) focuses on sparse growth by limiting the number of modules kept alive at any given time. This network is composed of modules working together to compute the output through a weighted combination of their predictions. The strategy aims to prevent linear time complexity increase as the network grows. The Growing Long-Term Memory Network (GLTMN) utilizes a Mixture-of-Experts architecture with short-term memory (STM) and long-term memory (LTM) modules. The LTM grows incrementally while the STM has a fixed number of modules. Only the STM modules are trained on incoming experience, with mixture weights encompassing both LTM and STM. The GLTMN model utilizes a Mixture-of-Experts architecture with STM and LTM modules. STM modules are trained on incoming experience, with mixture weights encompassing both LTM and STM. Modules in STM are consolidated into LTM based on recent experience, and LTM modules can be reinstated back into STM. The overall size of the model is kept constant by removing LTM modules when a maximum size is reached. The GLTMN model uses a Mixture-of-Experts architecture with STM and LTM modules. Modules in STM are initially all modules, while LTM modules are added based on recent experience. The system computes predictions by combining outputs of all modules with mixture weights. This model can be viewed as a Product of Experts, eliminating the need to normalize individual model outputs. The GLTMN model utilizes a Mixture-of-Experts architecture with STM and LTM modules. Predictions are computed by combining module outputs with mixture weights, eliminating the need to normalize individual model outputs. The system consolidates a STM module into LTM every T processed examples based on recent activity. Memory management is optimized by adapting weights multiple times for the mixture weights and only once for the modules. The GLTMN model uses a Mixture-of-Experts architecture with STM and LTM modules. The system consolidates a STM module into LTM every T processed examples based on recent activity. Memory management is optimized by adapting weights multiple times for the mixture weights and only once for the modules. The model has a memory limit of n = 30 modules, with the highest mixture weight module in LTM being reinstated into STM for further training. Despite the memory limit, the model's potential to allocate new memory is unbounded. The GLTMN model utilizes a Mixture-of-Experts architecture with STM and LTM modules. It does not assume a specific type of architecture and can be applied to various network types. In experiments, the model is evaluated using LSTM architecture for an online language modeling task. The model reduces to a Products of Experts when LTM is not used. The model utilizes a Mixture-of-Experts architecture with independent weight vectors. An LSTM network computes the weights in a PoE model. The experimental setup tests the model's ability to adapt to a continuous stream of circumstances and develop a wide range of skills. The model is evaluated in lifelong language modeling tasks where context dictates required skills. In a Mixture-of-Experts architecture with independent weight vectors, an LSTM network computes weights in a PoE model. Two lifelong language modelling tasks are introduced, featuring conflicting learning signals when transitioning between languages or domains. The model aims to adapt to non-i.i.d. data continuously, requiring a good performance in transitioning while maintaining overall language modelling proficiency. In a Mixture-of-Experts architecture with independent weight vectors, an LSTM network computes weights in a PoE model. The approach is not adequate here, adopting an online learning paradigm where the model receives an instance x t, makes a prediction \u0177 t, and incurs a loss L(\u0177 t, y t). The goal is minimizing cumulative loss in a multilingual dataset with English, French, Spanish, German, and Czech character sets. The dataset was carefully constructed to include only linguistically valid character sets to prevent non-linguistic noise interference. The character vocabulary for the experiments was limited to characters appearing more than 100 times in the corpus, resulting in a vocabulary of 215 characters. Another dataset included English text from various sources, with a vocabulary size of 58K words. The corpus was split into fragments from different languages or domains with lengths randomly sampled from an exponential distribution. Additionally, a multilingual dataset was created with randomly alternating 1M and 10M-character-long sequences. For the multilingual dataset, sequences were extracted with varying lengths from exponential distributions. The multi-domain dataset also followed a similar procedure with sequences of different lengths. The GLTMN model used 30 LSTM modules with two layers and 200 hidden units each. Performance was compared to a single LSTM network to measure the advantage of the mixture model. The study compared different LSTM models on multilingual and multi-domain tasks. They evaluated a double-layered LSTM with 1300 units, independent LSTMs for each domain/language, and a PoE model with mixture coefficients produced by a simple LSTM module. Different approaches were tested to measure the advantage of mixture models over single modules. The study compared various LSTM models on multilingual and multi-domain tasks, including a double-layered LSTM with 1300 units, independent LSTMs for each domain/language, and a PoE model with mixture coefficients. Hyperparameters were tuned for all models, with the best-performing strategy using 20 modules for the STM. The goal was to measure if the growing model provided any advantage in recovering learned information while maintaining competitive performance. Metrics proposed for evaluation included online perplexity. The study compared various LSTM models on multilingual and multi-domain tasks, including a double-layered LSTM with 1300 units, independent LSTMs for each domain/language, and a PoE model with mixture coefficients. Hyperparameters were tuned for all models, with the best-performing strategy using 20 modules for the STM. The goal was to measure if the growing model provided any advantage in recovering learned information while maintaining competitive performance. Metrics proposed for evaluation included online perplexity, post-switch confusion, and plots illustrating the process. The experimental results for multilingual and multi-domain tasks show that the PoE model with LSTM weights performs slightly better in overall online perplexity. However, the GLTMN model excels at recovering after changes in distribution, performing almost on-par with the PoE model. GLTMN excels at recovering after distribution changes, outperforming stand-alone LSTM and PoE models. It spikes on the first batch, expected as weights are optimized per batch. In multi-domain tasks, transferring knowledge is more effective than in multilingual tasks. Enhancing neural networks with a modular structure improves general performance, with the LTM component aiding in faster recovery after task switches. Weight vectors were analyzed in multilingual and multi-domain tasks, showing modules working in tandem. Modules could be in LTM or STM at any given time, with effective knowledge transfer in multi-domain tasks. In multilingual and multi-domain tasks, modules are allocated to different domains with limited transfer between languages. Weight vectors show modules working together, with clearer clustering in multilingual tasks compared to multi-domain tasks. Knowledge transfer aids in task performance. Approaches to address catastrophic forgetting in neural networks have been categorized into two branches: fixed network size with adjusted learning procedures, and systems that allow for new modules to handle new tasks. The work discussed aligns more closely with the latter approach, similar to mixture of experts systems like the product of experts approach. The text discusses models with unbounded memory for continual learning, including the product of experts approach. Other models like memory networks and neural turing machines have shown that structured memory aids in learning longer dependencies. The proposed model saves algorithms learned by modules in memory, enhancing interaction between recent and remote memory. The text discusses the interaction between recent and remote memory in neural-network-assisted language modeling. It borrows terms associated with memory consolidation and reinstatement, adapting problems to a life-long learning setup. Some models extend their memories to support fast-changing statistics from the recent past, while others work towards the multilingual setting. In a life-long learning setup, different languages are treated as tasks, requiring flexible memory capabilities for artificial intelligence. A method based on growing modules over time is proposed, with memories consolidated into long-term memory. Two lifelong language modeling tasks are introduced, one character-based and multilingual, and the other word-based on multiple languages. The Growing LTM model is effective in adapting quickly without sacrificing performance in language modeling tasks. It is flexible and can be used with any neural network architecture. The system shows promise in various domains such as robotics and image recognition. Future research can explore mechanisms that utilize input data structure to associate it with relevant models. In future research, exploring mechanisms for neural networks to decide when to grow their memory structures and enable communication between memories through a central routing mechanism is essential. This will pave the way for developing more general learning systems and enhance the adaptability of neural networks in various domains."
}