{
    "title": "rylHspEKPr",
    "content": "We introduce property signatures as a representation for programs and specifications for machine learning algorithms. A property is a function that describes a simple property of a function, such as comparing input and output lists. By evaluating a list of properties for a function, we can generate a property signature. This signature can be guessed for a function based on input/output pairs, leading to potential applications in program synthesis. Experimental results show that property signatures can significantly improve program synthesis efficiency. Program synthesis is a longstanding goal in computer science research, dating back to the 1940s and 50s. Deep learning methods have shown promise in automatically generating programs from input-output examples. Representing programs and specifications in a way that supports learning is crucial for delivering on this promise. Just as computer vision benefits from convolutional neural networks and natural language processing benefits from LSTMs, ML techniques for computer programs will benefit from architectures with a suitable inductive bias. Program synthesis benefits from architectures with a suitable inductive bias. A new representation for programs and their specifications is introduced, based on using simpler programs. This includes the concept of a property, which computes a boolean function of the input and output of another program. Properties can be used to synthesize programs from input-output examples, such as checking list lengths or element values. The outputs of properties can be concatenated into a vector, creating a representation called a property signature. Property signatures, a new representation for programs and their specifications, can be used for machine learning algorithms in program synthesis. They serve as the first layer of a neural network and can be applied to various problems like algorithm induction, code readability, and program analysis. The paper introduces property signatures as a way to featurize programs and specifications, demonstrating their use in a machine-learning based synthesizer for a general-purpose programming language. The paper introduces property signatures as a new representation for programs and specifications in machine learning algorithms for program synthesis. It demonstrates how property signatures can improve synthesizer performance by automatically learning useful property signatures for functions, leading to a significant increase in program synthesis efficiency. The program synthesized using the property signatures method is based on a specialized programming language called Searcho. This language is designed for rapid execution in large-scale distributed searches. The system releases the programming language, runtime environment, search infrastructure, machine learning models, and training data for future research. The synthesized program returns a sub-list of elements in a list that are distinct from their previous value. In Inductive Program Synthesis, the goal is to synthesize a program meeting a given specification. In Inductive Program Synthesis, the focus is on Programming by Example (PBE) to synthesize a program meeting a given specification. PBE involves providing input/output pairs for the target program. Existing PBE systems include Winston (1970), Menon et al. (2013), and Gulwani (2011). A sample PBE specification could be a list of input/output pairs like [(1, 1), (2, 4), (6, 36), (10, 100)], where the goal is to find a function that squares the input. The challenge lies in ensuring the synthesis procedure recovers the 'best' or 'simplest' program satisfying the specification. The focus of program synthesis is often on domain-specific languages, but there are challenges in using general-purpose languages like C++ or Python due to complexity and execution issues. To address this, a Turing complete programming language called Searcho has been created specifically for program synthesis, aiming to overcome these technical obstacles. Searcho is a Turing complete programming language designed for program synthesis. It is compiled to bytecode and runs on the Searcho Virtual Machine. The language is strongly typed with algebraic datatypes and includes a library of 86 functions. This language and library are larger than those used in previous neural program synthesis work. The experiments in this paper involve using a machine learning model to enhance the performance of a baseline synthesizer on human-constructed PBE tasks. The model is trained to do premise selection for a symbolic search procedure, focusing on integers and lists of integers within a specific range. Applying this technique to a given PBE specification may be challenging due to its constraints and limitations. The paper presents illustrative programs in Haskell syntax to enhance the readability of the techniques discussed. These programs, referred to as properties, operate on input/output pairs to extract specific functions such as checking for duplicates or ensuring input/output lengths match. These functions can be applied to PBE specifications to improve the performance of a baseline synthesizer. The paper presents illustrative programs in Haskell syntax to enhance readability. These programs, known as properties, operate on input/output pairs to extract specific functions. Three programs are analyzed, each yielding a Boolean result based on the given examples. The 'property signature' [True, True, False] is derived from these results, providing insights into the input list manipulation required. The paper discusses creating program properties in Haskell to extract functions from input/output pairs. These properties are used to narrow down the search for program synthesis. The properties are represented as Boolean functions and are used to generate a signature for input list manipulation. The properties' output values are used to train a neural network for program specification representation. The paper introduces program properties in Haskell to extract functions from input/output pairs. These properties, represented as Boolean functions, are used to create a signature for input list manipulation. The properties' outputs are utilized to train a neural network for program specification representation. The function \u03a0 is defined to categorize properties as AllTrue, AllFalse, or Mixed. Computing the property signature for a program may be challenging, so an estimated signature is computed based on a small set of input-output pairs. This estimate provides an under-approximation of the true signature of the program. When estimating property signatures for synthesis using PBE, we can 'featurize' specifications without the definition of the function. To select useful properties, random properties are generated and pruned based on their ability to distinguish between programs. Occam's razor is applied to keep the shortest property among those giving the same value for every test program. The text discusses the use of property signatures in training a premise selector to predict library function usage. By computing estimated property signatures for functions based on input-output examples, a feedforward network can predict the frequency of library function appearances in a program. This premise selector can also identify useful properties for model predictions. Experiments show that property signatures enable the synthesizer to generate programs it previously could not. The text discusses the use of property signatures in training a premise selector to predict library function usage. By computing estimated property signatures for functions based on input-output examples, a feedforward network can predict the frequency of library function appearances in a program. This enables the synthesizer to generate programs it previously could not, with broader utility in representing various types of functions and reducing dependency on example distribution. Additionally, synthesized programs can be used for semantic searches and as new properties themselves. The baseline synthesizer works by filling in typed holes in a program based on the specification. The synthesizer starts with an empty 'hole' and fills it in all possible ways allowed by the type system. Programs are ordered by cost, which is a sum of individual operation costs. The synthesizer is given a configuration at the beginning of the procedure. The synthesizer is given a configuration at the beginning of the procedure, consisting of pool elements that can fill in holes in the program. Each pool element is a rewrite rule for a library function in Searcho, with correctly-typed holes for arguments. The configuration specifies a subset of pool elements for use during search, informed by machine learning. Training data is generated from a test corpus with 14 program types, sampling configurations and generating training examples. The synthesizer generates training programs for 14 program types by sampling configurations and generating up to 10,000 distinct programs for each type. A test set of 185 human-generated programs of varying complexity is constructed, including tasks like computing the GCD and fibonacci numbers. Test functions do not appear in the training set. The neural network architecture predicts the frequency of pool elements in the output. Due to a large number of elements, random sampling is used to optimize the search process. Multiple configurations are sent to a synthesis server for parallel processing based on the model's predictions. The baseline synthesizer solved 28 test programs on average, while with property signatures, it solved an average of 73 test programs much faster. The synthesizer composed functions with other functions, using property signatures to speed up the synthesis process. The program uses property signatures to speed up the synthesis process by guessing the signature for unknown functions and looking them up in a cache of previously computed functions. This approach significantly improved the number of test programs solved compared to the baseline synthesizer. The experiment showed that property signatures can be useful in speeding up the synthesis process by predicting the signature of a function from other signatures. This approach was tested on a dataset of random functions, demonstrating the potential of using machine learning models for this purpose. The experiment demonstrated the effectiveness of property signatures in predicting function signatures using a neural network model. The model achieved 87.5% accuracy on a test set, showcasing its ability for logical deduction on properties. This suggests the potential of property signatures in guiding program synthesis research. The composition predictor model accurately predicts function properties, with values like AllTrue and Mixed. Prior work on program synthesis includes studies by Gottschlich et al. (2018) and others. Function properties are similar to those in Property Based Testing, popularized by the QuickCheck library. Our properties operate on input/output pairs, related to proving theorems using machine learning. Existing work on synthesis borrows ideas from programming language design and uses refinement types for program specifications. Property signatures are a compromise between refinement types and dependent types, allowing for specifications impossible to express in refinement types. Researchers have used machine learning to synthesize and understand programs. Menon et al. (2013) introduced the idea of features, which are hand-crafted and applied in a limited domain. Balog et al. (2016) differs from Menon et al. in the use of a more expressive DSL and a larger set of allowed component functions. Their machine learning method does not work as effectively. In contrast to Balog et al. (2016), the researchers in this work have introduced the concept of properties and property signatures to synthesize programs that were previously not achievable with a baseline method. Their approach allows for the synthesis of programs with 14 different function types, compared to Balog et al.'s limited set of integers and lists of integers. The test cases in Balog et al. were generated from an enumerative synthesizer, guaranteeing quick synthesis, while the human-generated test cases in this work posed challenges in synthesizing programs within a reasonable timeframe. The researchers introduced property signatures to synthesize programs that a baseline method could not achieve. They have open sourced their code to accelerate future research. The top-down synthesizer used in this work iterates until a satisfying program is found or time runs out. The cost of a partial program is calculated based on its elements and filling holes. The section discusses special operations like tuple construction and lambda abstraction in the baseline synthesizer. It includes a formal description of the basic synthesis algorithm and an example trajectory of partial program expansions to generate the swap function for tuples of integers. The experiment compares different approaches in program synthesis. The experiment compared premise selection using Property Signatures to an algorithm from (Balog et al., 2016). Modifications were made to the experimental procedure to accommodate DeepCoder's limitations with handling integers and lists of integers. Function types were restricted based on (Balog et al., 2016), and random inputs were regenerated to fit DeepCoder's integer range. After modifying random inputs to fit DeepCoder's integer range, random training functions were generated and filtered based on input-output pairs. A test suite of 32 functions was created by modifying examples in the test set. A model was trained to predict functions using embeddings of input-output pairs, with special characters used to separate inputs for functions with multiple inputs. The study compared the synthesis techniques of DeepCoder, Property Signatures, and Random Baseline. DeepCoder synthesized an average of 3.33 test programs, Property Signatures synthesized 16.33, and Random Baseline synthesized 3 programs on average. This large gap in results aligns with existing literature on synthesis techniques. In our experiment, Property Signatures are less sensitive to distribution shifts compared to DeepCoder-esque techniques. This allows for improved performance on arbitrary programs and input types, even if they do not outperform DeepCoder on certain test sets."
}