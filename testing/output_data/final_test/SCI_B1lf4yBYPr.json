{
    "title": "B1lf4yBYPr",
    "content": "Existing deep learning approaches for visual feature learning extract more information than necessary, compromising privacy. This research introduces a novel trust score metric for deep learning models, a model-agnostic framework to improve trust scores by suppressing unwanted tasks, and a benchmark dataset called PreserveTask. The study evaluates and enhances the trust scores of five popular deep learning models. The trust scores of popular deep learning models like VGG16, VGG19, Inception-v1, MobileNet, and DenseNet were measured and improved, with Inception-v1 having the lowest score. The study also demonstrated the framework's results on the color-MNIST dataset and its applications in face attribute preservation. The primary goal of artificial intelligence is to mimic human intelligence, especially with the advancement of deep learning models. Data and model governance frameworks are crucial for protecting data and model meta information. The importance of data and model governance frameworks lies in controlling the sharing of information between entities and protecting against adversarial attacks. Models can learn private information from data, leading to unintended intelligence. Techniques like transfer learning encourage models to generalize and extract generic features for multiple tasks. For example, a classifier trained to detect shapes can also predict size and location using extracted features. A shape classifier can predict the size and location of objects in an image, making it more intelligent. In privacy-preserving applications, data and visual attributes need to be kept private from the model. For example, a deep learning model trained to predict gender from a face image can also predict age and identity. Debiasing techniques aim to remove bias in learning specific tasks. The research motivation is to study if a model can be trained to perform only specific tasks, removing bias in learning tasks like shape classification. The goal is to ensure the model learns only selected tasks from input images and unlearns others like color and size. The challenge lies in curating a balanced image dataset for multi-class classification tasks. The research proposes a framework to measure trust scores of DL models and improve them during training. It introduces a class-balanced multi-task dataset called PreserveTask with tasks like shape, size, color, location, and background color classification. Additionally, a novel metric is used to compare the trustworthiness scores of popular DL models like VGG16 and VGG19. The research introduces a framework to measure and compare trust scores of popular DL models like VGG16, VGG19, Inception-v1, MobileNet, and DenseNet. It proposes a model-agnostic solution to improve trust scores during training by preserving certain tasks on the same image. Experimental analysis shows that the framework enhances the trust score, particularly for Inception-v1. Practical applications are demonstrated using colored MNIST dataset and face attribute preservation with datasets like Diversity in Faces (DiF) and IMDBWiki. The research discusses k-anonymity preservation and attribute suppression in face recognition. Previous studies focused on masking sensitive information from video feeds and preserving the identity of face images through de-identification techniques. Studies aimed to extract necessary meta information without revealing identity to enhance face as a usable biometric. The research focuses on face de-identification and attribute suppression techniques to protect sensitive information in face recognition. Previous studies have explored methods to anonymize face gender information while preserving identity. Techniques such as perturbing input data and using generative adversarial networks have been proposed to suppress attributes. The study by Jayaraman et al. (2014) decorrelated visual attributes using negative gradients in the model. These approaches aim to enhance the usability of face recognition while maintaining privacy. The research focuses on debiasing models in face recognition by preserving specific attributes while suppressing others. Existing research primarily focuses on data perturbation to mitigate bias, but there is a gap in techniques for model perturbation without altering input data. The research aims to untangle shared tasks in deep learning models to enable them to focus on specific tasks. A benchmark dataset is needed to evaluate the privacy-preserving capacity of DL models. The dataset should involve multiple tasks on the same image with varying numbers of classes to study classification task complexity. The research focuses on untangling shared tasks in deep learning models by suppressing multiple classification tasks using random class labels. The dataset should be noise-free and class balanced to avoid complexities that could affect classification performance. Publicly available datasets like LFW, CelebA, IMDB-Wiki, AwA 2, and CUB have multiple binary classification tasks and one nonbinary classification task. The research focuses on creating a new PreserveTask dataset for benchmarking models in multi-task classification. The dataset includes five different classification tasks with multiple objects and labels, inspired by the CLEVR dataset. The primary objective is to maintain task privacy and ensure high performance on individual tasks. The PreserveTask dataset consists of five classification tasks: Shape, Color, Size, Location, and Background Color. Each task has specific categories, and the images are 256x256 colored images. There are 1260 variations with 50 training and 10 testing images for each, totaling 63,000 training and 12,600 testing images to ensure class balance. The PreserveTask dataset aims to maintain class balance across tasks by creating a dataset with minimal noise. It serves as a benchmark for different frameworks in shared task learning. The dataset consists of five classification tasks with specific categories and a total of 63,000 training and 12,600 testing images. Further extensions may include adding more real-world objects. Location prediction classifiers can be trained on top of feature f1, which contains more information about the object than just its shape. Techniques exist to suppress the model from learning certain attributes or tasks, such as size, by applying negative loss or gradient to specific features. This requires the information about the tasks to be suppressed to be available during training. In a proposed framework, the model agnostic approach of suppressing task learning allows for direct application to any deep learning model. By generating random n-class labels in the gradient reversal branch, the model can suppress other classification tasks. Multiple gradient reversal branches can be built to suppress all possible classification tasks, training the DL model with a custom loss function. The proposed framework allows for suppressing other classification tasks in any deep learning model by training it with a custom loss function. The model is evaluated against different tasks in the PreserveTask dataset to extract the trust score. The ideal deep learning model should provide 100% accuracy on trained tasks and random accuracy on other tasks. Trust score is calculated based on how closely the model's performance matrix aligns with the ideal matrix. The trustworthiness of a trained DL model is computed based on the deviation of task performance from the ideal matrix. The trust score ranges from 0 to 1, with a score above 0.9 considered highly desirable. Using random labels for unknown tasks can improve trustworthiness, as shown in empirical analysis. The trust score of a trained DL model is crucial, with scores above 0.9 being highly desirable. Experimental results demonstrate the impact of changing non-diagonal elements on trust scores, showing the sensitivity of the metric. The experiments were conducted using the PreserveTask dataset to analyze the proposed framework. The experiments were performed using the PreserveTask dataset with a popular deep learning model, Inception-v1, achieving 99.98% accuracy for shape classification. The model's features were extracted and used to train neural network classifiers for predicting size, color, location, and background color of objects, with accuracies of 97.29%, 51.25%, 99.98%, and 92.05% respectively. The performance in size, location, and background prediction tasks was notably high. The experiments with the Inception-v1 model showed high accuracy in shape classification. However, color prediction performance was low due to the independence of shape and color tasks. Training the model on one task and predicting others resulted in poor performance. Trust score of the model was 0.7530, indicating its limitations. Similar experiments with other deep learning models were also conducted. The trust scores of various deep learning models including Inception-v1, VGG16, VGG19, MobileNet, and DenseNet are compared in Figure 5. Inception-v1 and DenseNet have the lowest trust scores, while MobileNet has the highest. The question of whether models need to be highly intelligent for privacy preservation is raised. Experiments are conducted to suppress known tasks during training, with one task preserved and one suppressed using the Inception-v1 model. The experimental setting involves preserving one task and suppressing another using the Inception-v1 model. The gradient reversal layer helps unlearn the suppressed task while learning the preserved task. Comparisons are made with a customized negative loss function to evaluate performance. Results show that using different task suppression mechanisms affects color prediction performance, with random labels reducing accuracy regardless of the preserved task being shape or size. The experimental setting involves preserving one task and suppressing another using the Inception-v1 model. The gradient reversal layer improves trust scores, and random labels are used to prevent memorization of a single suppression task. The experimental setting involves preserving one task and suppressing another using the Inception-v1 model. Random labels are used to prevent memorization of a single suppression task, resulting in reduced prediction performance for certain tasks. Using random labels for task suppression in the Colored MNIST Dataset resulted in comparable trust scores to using known labels, with improved performance compared to the baseline trust score of a DL model. The MobileNet model was trained from scratch to achieve a baseline trust score of 0.756, and after applying task suppression with random labels and gradient reversal based training, the performance improved significantly. In the framework for task suppression with random labels and gradient reversal training, MobileNet model trust scores increased to 0.824. The TSNE plot shows that suppressing the color prediction task scatters features of 'red' and 'cyan' colored images. In the Diversity in Faces (DiF) Dataset, gender and pose classification tasks were considered, with a trust score of 0.7497 achieved using Inception-v1 model on a subset of 39296 images with equal class balance. In the IMDB-Wiki dataset, gender and age classification tasks were performed using DenseNet model, achieving a trust score of 0.7846. After applying task suppression and known class labels, the trust score increased to 0.7883. With random unknown class labels, the trust score further increased to 0.7860. Our framework for measuring and improving a DL model's trust has practical applications in face recognition and gender recognition systems. A model-agnostic framework was showcased for measuring and improving trustworthiness from a privacy preservation perspective. A novel benchmark dataset called PreserveTask was created to evaluate a DL model's capability in suppressing shared task learning, opening up further research opportunities. Popular DL models like VGG16, VGG19, Inception-v1, DenseNet were experimentally evaluated. The study experimentally showed that popular DL models like VGG16, VGG19, Inception-v1, DenseNet, and MobileNet exhibit poor trust scores and surpass their intended intelligence levels. A case study on face attribute classification was conducted using Diversity in Faces (DiF) and IMDB-Wiki datasets. The supplementary material includes detailed hyper-parameters for model reproducibility and additional analysis and visualizations. Five baseline deep learning models were utilized: Inception-v1, VGG16, VGG19, DenseNet, and MobileNet. The experiments used popular DL models like VGG19, DenseNet, and MobileNet. Data was z-normalized before training. Standard architectures were borrowed from Keras library. Models were trained with categorical cross-entropy and Adam optimizer. A two hidden layer neural network was used as a classifier with specific architecture and activation functions. The study utilized popular DL models like VGG19, DenseNet, and MobileNet, with data z-normalized before training. Categorical cross-entropy and Adam optimizer were used for training, with specific parameter values. Validation data accounted for 20% of the dataset, and the model was trained for 100 epochs with early stopping. A batch size of 32 was used for faster computation, and experiments were conducted using a 1 \u00d7 K80 GPU. Additional analysis, visualizations, and charts were included in this section for better comparison. Trust scores for various DL models and suppression techniques were presented, showing differences in model performance. After applying various suppression techniques, the trustworthiness of the Inception-v1 model on the PreserveTask dataset improved, even when using random labels for unknown tasks. Performance matrix heat-maps were generated after suppressing known and unknown tasks using negative loss and GR layer, showcasing the shared task performance. Trust scores obtained in the Diversity in Faces (DiF) dataset were also highlighted. Trust scores in the Diversity in Faces (DiF) dataset improved for the Inception-v1 model after applying various suppression techniques, including using random labels for unknown tasks."
}