{
    "title": "ryevtyHtPr",
    "content": "Image segmentation involves grouping pixels belonging to the same object or region by solving the \"insideness\" problem. Deep Neural Networks have excelled in segmentation tasks, but their approach to insideness remains unclear. This study focuses on analyzing DNNs solving the insideness problem for closed curves, showing that both feed-forward and recurrent architectures can implement solutions, with recurrent networks being more adept at learning general solutions. Deep Neural Networks (DNNs) excel in image segmentation tasks, but their approach to the \"insideness\" problem is unclear. Recurrent networks are better at learning general solutions for this problem, highlighting the need for new training strategies. Recent advancements in Deep Neural Networks (DNNs) have led to the development of more advanced segmentation modalities and applications, such as segmenting individual object instances, understanding spatial relationships like containment, and generating realistic images. Insideness plays a crucial role in these tasks, especially when there are limited cues available besides object boundaries. In this paper, the focus is on investigating insideness-related representations in DNNs for segmentation, specifically isolating insideness from other components. The study demonstrates that DNNs with dilated convolutions and convolutional LSTMs can effectively solve the insideness problem for any given curve. The study shows that DNNs with dilated convolutions and convolutional LSTMs can effectively solve the insideness problem for any curve. However, standard training strategies do not lead to general solutions, except for a recurrent network with a specific training strategy designed for long-range relationships. The study demonstrates that DNNs struggle with learning elemental visual tasks, despite advancements in architectures to capture long-range dependencies. The challenges with insideness are attributed to poor generalization rather than optimization, as networks can solve the task for a given dataset. Training strategies play a significant role in capturing long-range dependencies effectively. The study introduces a new paradigm to analyze insideness-related representations in DNNs using synthetic stimuli with closed curves. This approach avoids mixing insideness with other image segmentation components found in natural images. The notation X \u2208 {0, 1} N \u00d7N represents an image or matrix of size N \u00d7 N pixels, with X i,j denoting the value at position (i, j). This notation is used for indexing elements in images and matrices throughout the paper. The study introduces a new paradigm to analyze insideness-related representations in DNNs using synthetic stimuli with closed curves. In the figures, white and black represent 0 and 1. The closed curve in the image is a digital version of a Jordan curve without self-crosses or self-touches. The segmentation of X is denoted as S(X) \u2208 {0, 1} N \u00d7N, where the pixels in F X can have a value of 0 or 1. The value of S(X) i,j can be 0 or 1, determined by the input image itself. The number of digital Jordan curves is vast, even for small image sizes. Insideness is a global problem, not local, making simple pattern matching impossible. A shallow neural network can solve the insideness problem, but the number of units needed may be too large. In this Section, two DNN architectures are introduced to solve the insideness problem: a feed-forward with dilated convolutions and a ConvLSTM. Dilated convolutions capture long-range dependencies crucial for segmentation. These architectures can be implemented in practice and are based on the ray-intersection method. The ray-intersection method for determining the insideness of a pixel is based on the fact that a ray alternates between inside and outside regions when crossing a curve. The parity of the total number of crossings determines the pixel's region, with odd parity indicating inside and even parity indicating outside. Enumerating all possible intersections of a ray and curve, only horizontal rays are considered, with five cases for intersection. The insideness of a pixel is determined by the parity of the total number of crosses when a ray intersects a curve. Horizontal rays are considered, with odd parity indicating inside and even parity indicating outside. Dilated convolutions, also known as atrous convolutions, involve convolutions with upsampled kernels. Dilated convolutions, also known as atrous convolutions, involve convolutions with upsampled kernels that enlarge receptive fields without increasing parameters. Implementing equation 3 with dilated convolutions in a neural network is demonstrated in App. B, using multiple layers to capture information across the image. The number of dilated convolutional layers is logarithmic to the image size, N. The use of dilated convolutions with kernels of different sizes, including 1xN and 3x3, is shown to solve the insideness problem in neural networks. A two-layer convolutional network with 1x1 kernels can implement the parity function. ConvLSTM with a single 3x3 kernel is sufficient for handling long-range dependencies in segmentation architectures. ConvLSTM with a single 3x3 kernel can solve the insideness problem by utilizing internal back-projection of the LSTM. This method involves expanding the outside region from the image borders and blocking the expansion when reaching the curve, preventing the outside region from intruding inside the curve. The solution to the insideness problem involves expanding the outside region from the image borders and blocking the expansion when reaching the curve. This iterative process uses expansion and blocking operations to determine the inside and outside regions of the curve. The algorithm for solving the insideness problem involves expanding the outside region from the image borders and blocking the expansion when reaching the curve. This process uses expansion and blocking operations to determine the inside and outside regions of the curve. The output gate expands hidden representations using a 3x3 kernel, and the blocking operation is implemented using element-wise product with a Boolean not operation. In Fig. 2b, computations for ConvLSTM weights are shown, with a key component being the use of a value denoted as q to saturate non-linearities. Setting q = 100 can solve the insideness problem for all curves in datasets, even with just one kernel. Multiple stacked ConvLSTM networks can implement the coloring method by setting unnecessary ConvLSTMs to implement the identity. The kernel can implement the coloring method by setting unnecessary ConvLSTMs to perform the identity operation and unnecessary kernels to 0. Networks with lower complexity than LSTMs can solve the insideness problem, but are rarely used. A convolutional recurrent network with one sigmoidal hidden unit per pixel and a 3x3 kernel can also solve the insideness problem for any given curve. Experiments on synthetically generated Jordan curves aim to predict if each pixel is inside or outside the curve. The experimental setup is described, and the generalization capabilities of DNNs trained in a standard manner are analyzed. Three algorithms are introduced to generate Jordan curves for datasets, with images generated for training, validation, and testing. The datasets are constructed to meet specific constraints, with dissimilar images used for testing and validation sets. Examples of curves for each dataset are shown in Fig. 3a. The experimental setup includes the generation of datasets with different types of curves such as Polar, Spiral, and Digs. Each dataset has specific characteristics like vertices, spiral growth, and random thickness. Evaluation metrics are defined based on the problem statement. Evaluation metrics are used to assess the performance of network architectures in classifying pixels in the Jordan curve F X. Metrics include per pixel accuracy and per image accuracy. Architectures evaluated include dilated convolutional DNN and its variants like Ray-intersection network. The network architectures evaluated include Ray-intersection network, CNN, UNet, ConvLSTM, and 2-LSTM with different initialization methods. The ConvLSTM models are tested for their ability to color images by expanding the outside region. The parameters are initialized using Xavier initialization. The ground-truth consists of insideness for each pixel in the image. Cross-entropy with softmax is used as the loss function. Using a weighted loss improves the accuracy of the networks. Using a weighted loss with a hyperparameter \u03b1 improves network accuracy. Different values of \u03b1 (0.1, 0.2, 0.4) are tested along with batch sizes and learning rates. Networks are trained for at least 50 epochs and tested for the highest validation set accuracy. Thousands of networks are trained per dataset, with detailed results in Appendix G. Testing accuracy is reported for the best hyperparameters. The networks trained on the Polar dataset show varying testing accuracy, with Dilated, 2-LSTM, and UNet achieving close to 100%, while Ray-int. and 1-LSTM perform poorly. Optimization issues similar to previous cases are noted. Networks with ConvLSTMs require two LSTMs for high accuracy, raising questions about stochastic gradient descent performance. Per pixel accuracy is generally high, with focus shifting to per image accuracy reporting. The networks Dilated, 2-LSTM, and UNet achieved high accuracies when trained on the Polar dataset. However, they did not generalize well to datasets with more vertices. Only when trained on 24-Polar did they generalize across all Polar datasets. Similar conclusions were drawn for UNet. Further analysis included training on different datasets and testing on various datasets to assess generalization capabilities. The networks tested on 24-Polar and Spiral datasets generalized well within the same family but not across different families. Training on a more varied dataset did not always improve cross-dataset accuracy. Overfitting was observed in some cases, despite attempts to address it with weight decay regularization. The lack of generalization in the networks trained on 24-Polar and Spiral datasets is visualized to understand why the representations do not generalize well. Units in different layers are tuned to local or global features of the curves in the training set, rather than capturing long-range dependencies. Weight decay regularization did not improve accuracy in addressing this issue. The lack of generalization in networks trained on 24-Polar and Spiral datasets is due to overfitting to local features instead of capturing long-range dependencies. Visualizing feature maps shows the network expands borders and curves differently from theoretical solutions, leading to poor generalization. Training with large-scale natural image datasets is evaluated to address generalization issues. Training with large-scale natural image datasets addresses generalization issues by using state-of-the-art architectures like DEXTR for instance segmentation and DeepLabv3+ for semantic segmentation. However, these methods struggle to determine insideness for curves, even after fine-tuning. The coloring routine lacks long-range relationships but can capture them by applying multiple times. The standard training strategy enforces ground-truth to improve accuracy. The coloring routine simplifies learning insideness by focusing on a 3x3 neighborhood operation. It enforces ground-truth at each step, rather than waiting until the last step. Irrelevant inputs are set to 0 during learning, and an architecture search was conducted to optimize the process. The architecture search led to the creation of Coloring Net, a convolutional recurrent neural network with a sigmoidal hidden layer and backprojection to the hidden layer. It successfully solves the insideness problem and reaches 0 training error about 40% of the time. The network is applied to images of Jordan curves and compared with 2-LSTM and Dilation networks. The Coloring Net, compared to 2-LSTM and Dilation networks, demonstrates superior cross-dataset accuracy in Spiral and Digs datasets with less than 1000 training examples. This highlights the potential of decomposing learning for routine emergence. DNNs with dilated convolutions and convolutional LSTM can effectively solve the insideness problem for any curve, specializing in detecting curve characteristics in the training set. The research shows that simple recurrent networks can learn the coloring routine with significantly less data compared to other methods. This reductionist approach aims to improve image segmentation and understanding complex spatial relationships. The study introduces a procedure to determine the minimum number of Jordan curves in an image. The research introduces a procedure to derive a lower bound of the number of Jordan curves in an image using grid graphs and adjacency rules for black and white pixels. Digital Jordan curves are defined as cycles in a grid graph, and the numbers of all cycles in grid graphs of different sizes were computed up to 27 \u00d7 27 vertices. The research utilizes grid graphs to derive lower bounds for the number of digital Jordan curves in an image. By upsampling and padding, they ensure that the curves do not contain border pixels, resulting in a lower bound estimation. The study uses grid graphs to establish lower bounds for the number of digital Jordan curves in images. By employing upsampling and padding techniques, they ensure the curves exclude border pixels, leading to a more accurate estimation. Additionally, a feed-forward convolutional DNN is introduced, with parameters replicating equation 3, and a layer within the network is optimized using multiple dilated convolutions. The smallest CNN identified for implementing the ray-intersection method consists of 4 layers, with the first two computing X(i, j) \u00b7 X(i + 1, j) and the last two determining the parity. The study introduces a feed-forward convolutional DNN with parameters replicating equation 3. The network consists of 4 layers, with the first two computing X(i, j) \u00b7 X(i + 1, j) and the last two determining the parity. The architecture uses a convolutional layer with a 2 \u00d7 1 kernel to detect intersections in binary images. The study introduces a feed-forward convolutional DNN with parameters replicating equation 3, consisting of 4 layers. The network uses a 2 \u00d7 1 kernel to detect intersections in binary images, with a Table 1 showing lower bounds of digital Jordan curves in N \u00d7 N images. The Ray-Intersection Network utilizes a 1 \u00d7 N kernel equivalent to multiple dilated convolutional layers, with the second layer summing over the products of each ray. The Ray-Intersection Network utilizes a 1 \u00d7 N kernel to capture long-range dependencies of insideness. The third and fourth layers calculate the parity of each unit's value using a DNN introduced by Shalev-Shwartz et al. (2017). The Ray-Intersection Network uses a parity network with convolutions on all units in H (2). The network has 3N/2 kernels in the third layer and uses a 1 \u00d7 1 kernel in the third and output layers. The DNN is feasible in practice with O(N) kernels and no more than 4 convolutional layers with ReLUs. Additionally, a layer with a 1 \u00d7 N kernel is equivalent to several layers of dilated convolutions with a 3 \u00d7 3 kernel size. The dilated convolution is defined as applying the kernel in a sparse manner every d units. It allows for long-range sums using a series of dilated convolutions with a 3x3 kernel. Dilated convolutions with factors of 2 can accumulate entries of the ray, with dilation factors being powers of 2. Networks with these dilation factors are common in practice. DNNs with dilated convolutions can solve the insideness problem and are implementable, with the number of layers and kernels growing logarithmically and linearly with image size. The parity of each unit's value can be calculated using a DNN introduced by Shalev-Shwartz et al. The network introduced by Shalev-Shwartz et al. (2017) calculates the parity of integers bounded by a constant C. It has hidden units with ReLUs and one output unit indicating if the input is even. The complexity of the shape is expressed by the maximum number of times a horizontal ray can cross F X. There is a difference in input handling between the network in Shalev-Shwartz et al. (2017) and the one used in the paper, where the sum is done through dilated convolutions before using the network. The network introduced by Shalev-Shwartz et al. (2017) calculates the parity of integers bounded by a constant C using hidden units that threshold at specific values. The output layer combines these units to yield the parity function for even numbers. A ConvLSTM can implement the coloring routine by performing expansion and blocking operations on an image. The method can be achieved with a network as small as one ConvLSTM with one kernel, or multiple stacked ConvLSTM with more kernels. Unnecessary kernels can be set to 0, and ConvLSTM not needed should implement the identity operation. The ConvLSTM implements the identity operation using sigmoids as nonlinearities in a convolutional recurrent network with one hidden layer. The network's output pixel is determined by the hidden state of the pixel and its 4-neighbourhood. The sigmoid outputs should asymptotically approach 0. The ConvLSTM implements the identity operation using sigmoids in a convolutional recurrent network with one hidden layer. To ensure the coloring routine doesn't fade, the sigmoid outputs should asymptotically approach 0 or 1. The algorithm generates curves using polar coordinates and randomly selected vertices connected by straight lines. The Polar datasets consist of curves with randomly generated vertices at varying distances from the center. The Spiral dataset features curves created from a random walk with segments extending in random directions and lengths, while ensuring no self-intersections occur. In the Digs Dataset, random rectangles are generated with random \"digs\" of varying thicknesses added sequentially without crossing previous digs. The hyperparameters for all architectures are reported, including the Dilated Convolution DNN with specific kernel sizes and number of layers. The architectures compared in the study include Ray-intersection network, Dilated architecture, UNet, and Convolutional LSTM. Different hyperparameters and layer configurations are tested to analyze the effectiveness of dilated convolutions. The study compares different architectures including Ray-intersection network, Dilated architecture, UNet, and Convolutional LSTM. The Convolutional LSTM models tested include 1-LSTM, 2-LSTM, and 2-LSTM without initialization. Weight decay was added to all layers for regularization. Dilated architecture trained on 24-Polar and Spiral datasets achieved less than 95% test accuracy despite close to 100% accuracy in the training set. The study compared different architectures including Ray-intersection network, Dilated architecture, UNet, and Convolutional LSTM. Weight decay was used for regularization in all layers. Overfitting was observed in experiments with weight decay values between 10^-5 to 1, except for a weight decay of 1 where training did not converge. The CNN did not exhibit overfitting. Dilated architecture required 25 layers compared to 9 layers for Dilated, but adding more layers did not improve accuracy. The study also evaluated DEXTR and DeepLabv3+ for solving the insideness problem in Instance Segmentation. The study evaluated DeepLabv3+ architecture for solving the insideness problem in segmentation. The model utilizes dilated convolution and Atrous Spatial Pyramid Pooling for feature extraction. It is pretrained on PASCAL VOC 2012 and fine-tuned on Polar and Spiral datasets. Different parameters such as output stride, loss weight, and learning rates were adjusted during training. The study fine-tuned the DeepLabv3+ network on Polar and Spiral datasets, achieving good pixel predictions but struggling with border areas. The DEXTR model was also used for segmentation, requiring 4 user-marked points for accurate results."
}