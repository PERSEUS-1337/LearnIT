{
    "title": "SJevPNShnV",
    "content": "Our study explores how a recurrent neural network integrates long- and short-term memory mechanisms to solve tasks. We found that sequential recall is achieved through oscillatory dynamics in addition to asymptotic attractors for long-term memory. This suggests that different memory mechanisms can coexist in a single neural network, with implications for artificial intelligence and neuroscience applications. In this study, a dynamical system analysis of a trained RNN on tasks involving short-term memory and delayed output during classification is performed. Gating units in LSTM and other architectures aim to address long-scale temporal learning issues, but it is not well understood how these units combine information to solve complex tasks requiring memory of past inputs. Attractor networks and orthogonal transformations are used in RNN solutions for tasks requiring memory and delayed output. The interaction between attractor dynamics and rotational transformations in tasks needing long-term memory and sequential recall is explored using slow-point analysis techniques. The study explores how attractor dynamics and oscillatory transients in low-dimensional subspaces interact to support timed, sequential computations in recurrent neural networks. The findings provide insights into solving complex temporal tasks and their impact on biological circuits. The research replicates and builds upon previous work on memory tasks in artificial RNNs, focusing on the interaction of memory mechanisms with delayed recall. The study investigates the interaction of memory mechanisms in recurrent neural networks, specifically focusing on delayed recall. A single-layer RNN with hyperbolic tangent activation function is used, with 100 neurons and standard optimization algorithms. The network architecture is explored to verify the original findings and analyze attractor dynamics for sequential computations. The update rule for hidden states and outputs in the network is defined for training using the Adam optimization algorithm. Only specific matrices were trained, and the neural circuit behavior was analyzed using tools from dynamical system theory. Slow point analysis was used to identify approximate fixed points in the system. A Lyapunov function was defined to determine the fixed points and their types based on the Jacobian matrix eigenvalues. The neural network's ability to match and maintain the last input for each channel while preventing crosstalk is studied using a delayed 3-bit flip-flop task. This task involves two memory mechanisms and network dynamics with three independent channels. The network must also introduce a time delay to study long term memory and sequential recall interaction. The neural network is tested on a delayed task to study memory mechanisms and network dynamics, requiring a response delay and state change delay. Analysis of the network dynamics reveals slow points and fixed points, with observations replicated from previous research. The characteristic \"cube\" pattern is observed in the network dynamics. The neural network exhibits a \"cube\" pattern where each vertex represents an output state, with attractors at the corners and saddle points on the edges and faces. These attractors encode specific output states and implement long-term memory. Saddle points guide the dynamics during output state switches, with stable subspaces separating basins of attraction. This configuration aligns with previous findings. The neural network exhibits a \"cube\" pattern with attractors at the corners encoding specific output states. The configuration aligns with previous findings and shows trajectories in phase-space with stable fixed points. The neural network exhibits a \"cube\" pattern with attractors at the corners encoding specific output states. In the delayed task, inputs must be treated in a subspace orthogonal to the output space to ensure no output change is expressed before the delay has elapsed. This leads to different hidden dynamics compared to immediate output tasks. The study focuses on investigating hidden dynamics with added delay in a neural network. Standard PCA and slow-point analysis were not informative, so triggered activation averages are used. A single type of transition is characterized from state (-1, -1, -1) to state (-1, -1, 1). Trajectories of RNN states before and after the switch are averaged to obtain a short average trajectory. The study investigates hidden dynamics in a neural network by analyzing a single type of transition from state (-1, -1, -1) to state (-1, -1, 1). Trajectories of RNN states before and after the switch are averaged to obtain a short average trajectory, revealing rotating dynamics in the hidden-layer activation space. The PCA projection in FIG1 shows a decomposition resembling Fourier modes of a step function, with neurons locking into precise frequencies. The subspace for rotational transformations is mostly orthogonal to attractor separatrices of different memorized outputs. This rotating dynamics phenomenon has been observed in artificial neural networks during sequential recall. The eigenspectrum of the matrix W in artificial neural networks reveals rotating speeds in different subspaces. The network without delay has three eigenvalues close to the real axis. The switch from output (-1,-1,-1) to output (-1,-1,1) for the delayed network was studied using a triggered average and plotted according to the first principal components. The eigenvalues of the recurrent connection matrix W for the studied RNNs show rotating dynamics in the delayed network, with large imaginary parts indicating oscillatory dynamics for sequential recall tasks. In this study, it was found that long-term memory and sequential recall can be easily implemented by a simple RNN through rotational dynamics localized around the origin. This periodic activity may serve as a \"precision timing\" mechanism that can be combined with learned computations, enabling the introduction of small delays in recurrent neural circuits. Future work could investigate the mechanism of delayed recall in conjunction with different computational tasks, leading to emergent oscillations enabling transient dynamics. This geometric understanding may aid in faster training by promoting transverse rotations in initialized networks. The observed oscillatory mechanism aligns with brain dynamics, where neurons exhibit low-dimensional activity patterns and engage in oscillatory activity at the population level. The network in the study is a one-layered recurrent neural network with 100 neurons and three real-valued matrices representing connections. The activation function used is the hyperbolic tangent, commonly seen in studies on neural networks due to its similarity to biological spiking neurons. The study focuses on a one-layered recurrent neural network with 100 neurons and three matrices representing connections. It explores the use of discrete-time networks in artificial intelligence, which are easier and faster to implement than continuous networks. This approach aims to bridge the gap between neural networks for neuroscience and artificial intelligence. The study explores training a one-layered recurrent neural network with 100 neurons using randomly set weights matrices. To simplify training, the network is \"unfolded\" to consider only a certain number of steps back in time. A 10 time-steps deep unfolded network was chosen for training. Investigation of Memory in RNNs reveals slow points with more than three unstable dimensions, marked by red triangles. The network's capacity is stretched to the limit, affecting long-term memory tasks."
}