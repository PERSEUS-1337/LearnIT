{
    "title": "H1eUz1rKPr",
    "content": "We propose a novel approach to learning permutation invariant representations by predicting the size of the symmetric difference between pairs of multisets. Our model uses multiset representations based on fuzzy set theory, where elements are vectors on the standard simplex. By predicting the symmetric difference as the l1-distance between these representations, we show improved performance compared to DeepSets-based approaches. Additionally, our model learns meaningful representations that map objects of different classes to distinct standard basis vectors. Recent work has shown the benefits of permutation invariant models aligned with set-based input tasks, such as point-cloud classification, estimating cosmological parameters, collaborative filtering, and relation extraction. The containment relationship between sets, particularly intersection, is often used as a measure of relatedness. For example, comparing keywords for two documents may involve modeling that {currency, equilibrium} describes a more specific set of topics. The text discusses the containment order as a partial order on sets and its extension to multisets. It highlights the challenge of representing multisets in a way that respects this order, including modeling relations between individual objects. The goal is to learn representations that induce desired multiset relations, such as money \u2248 currency and balance \u2248 equilibrium. In this paper, a measure-theoretic definition of multisets is presented to formally define the \"flexible containment\" notion. The theory allows for learning representations of multisets and their elements, considering richer information about the relationship between two multisets beyond containment. In this paper, a measure-theoretic definition of multisets is presented to learn representations of multisets and their elements by using the sizes of their symmetric differences or intersections. The goal is to predict relationships between unseen pairs of multisets, showing that it allows for predicting containment relations. The model is compared to DeepSets-based approaches and demonstrates the importance of capturing these relations. Recent research has focused on learning functions on sets, with Zaheer et al. (2017) introducing DeepSets to approximate set functions using permutation invariant pooling functions. Other approaches include set autoencoders by Probst (2018) and set representations learned by comparing input sets to trainable \"hidden sets\" by Skianis et al. (2019). However, these methods involve computationally expensive matching problems. Vendrov et al. (2015) and Ganea et al. (2018) explore modeling partial orders on sets. Recent research has focused on learning functions on sets, with various approaches introduced by different researchers. Vendrov et al. (2015) and Ganea et al. (2018) model partial orders on objects using geometric relationships in Euclidean and hyperbolic space. Nickel and Kiela (2017) embed hierarchical network structures in hyperbolic space based on hyperbolic distance. These approaches are unified under the framework of \"disk embeddings\" by Suzuki et al. (2019). Other work takes a probabilistic approach to representing hierarchical relationships, such as the Order Embeddings by Lai and Hockenmaier (2017). In modern research, various approaches have been proposed to model relationships between objects. Lai and Hockenmaier (2017) probabilistically formulate Order Embeddings by modeling joint probabilities as cone intersections. Vilnis et al. (2018) represent entities as \"box embeddings,\" where containment of one box inside another signifies order relationships. Vilnis and McCallum (2015) suggest modeling words as Gaussian distributions to capture entailment and generality, extended to mixtures of Gaussians by Athiwaratkun and Wilson (2017). Fuzzy set theory, introduced by Zadeh (1965), defines fuzzy sets using membership functions, with operations like intersection defined based on these functions. In modern fuzzy set theory, intersection is defined via a t-norm function. The intersection of two fuzzy sets A and B is determined by the membership function. Recent literature extends fuzzy set theory to multisets using a membership function that counts the appearances of an object with a certain membership. The goal is to learn to represent and predict containment between multisets. In modern fuzzy set theory, intersection is defined via a t-norm function, extending to multisets using a membership function that counts object appearances. A multiset is defined by a membership function mapping objects to the number of times they occur, with different subsets of non-negative reals dictating the type of multiset. The cardinality of a multiset is defined with respect to a measure on a universe of objects. The cardinality of a multiset is defined with respect to a measure on a universe of objects. The cardinality is viewed as a density of a measure on the universe, and operations on multisets are defined based on their membership functions. The intersection and union of multisets follow standard T-norm and T-conorm functions. The intersection and union of fuzzy multisets are defined using standard T-norm and T-conorm functions. Different T-norms cannot be easily used for defining multiset operations. Containment for multisets is formally defined, which differs from traditional definitions in literature. This formulation is more suitable for machine-learning applications and is related to real-valued multisets in formal logic and axiomatic set theory. A multiset transformation from one universe to another, denoted as T : \u2126 * \u2192 U *, allows for a more flexible notion of containment between multisets A and B. This flexibility is based on the similarity between objects in the universes \u2126 and U, which is externally provided by observations of subset relations. The transformation T captures this similarity, enabling a more nuanced understanding of containment relationships. The multiset transformation T from \u2126 to U allows for a flexible notion of containment between multisets A and B. This is based on the similarity between objects in the universes \u2126 and U, where subsets are observed in U * rather than in \u2126 * . The mapping T assigns tags or labels to each multiset in \u2126 * , enabling a nuanced understanding of containment relationships. The multiset transformation T assigns tags to objects in \u2126, allowing for a nuanced understanding of containment relationships. T induces a pushforward multiset transformation that preserves cardinalities. This transformation relates pairs of multisets over \u2126 and maps them to a \"latent\" universe U. The multiset transformation T from \u2126 to a \"latent\" universe U aims to predict relations for unobserved pairs of multisets. However, the setup faces challenges as containment between two multisets A and B may not hold. Instead, richer relations beyond containment can be observed between multisets. The multiset transformation T from \u2126 to a \"latent\" universe U aims to predict relations for unobserved pairs of multisets. Regardless of whether T (A) or T (B) is a subset of the other, we can determine overlap by looking at the size of their intersection or symmetric difference. This information can help us understand containment between multisets and their cardinalities. The learning task involves predicting containment relations between multisets by determining overlap through the size of their intersection or symmetric difference. The multiset transformation T aims to predict relations for unobserved pairs of multisets in a latent universe U. The error on these predictions indicates how well the flexible notion of containment is captured. The task involves observing samples (A, B) from a universe \u2126 embedded in R^d, with a multiset transformation T that preserves cardinalities. The task involves predicting containment relations between multisets by determining overlap through intersection or symmetric difference. The multiset transformation T aims to predict relations for unobserved pairs in a latent universe U, minimizing squared error in predicted overlaps. The task involves observing samples from a training distribution D over pairs of multisets in \u2126, with T preserving cardinalities. The learning task involves predicting containment relations between multisets by determining overlap through intersection or symmetric difference. The model aims to provide representations of multisets in a Euclidean space, allowing for calculations of overlap size. The target universe [k] helps in defining a representation function for this purpose. The natural representation function \u03a8k for finite universe [k] allows for intuitive indicator vector representations of multisets. Operations defined via membership functions can be performed coordinate-wise, with the cardinality of a multiset S \u2208\u00db * given by the sum of entries in \u03a8k(S). Proposition 4.1.4 states that for multisets R and S over [k], |R S| = ||\u03a8k(R) \u2212 \u03a8k(S)|| 1 and |R \u2229 S| = min{\u03a8k(R), \u03a8k(S)}, applied coordinate-wise. This representation function is used to train the model for predicting containment relations between multisets. The natural representation function \u03a8k is used to train the model for predicting containment relations between multisets. Proposition 4.1.4 suggests preferring the size of the symmetric difference over the size of the intersection as the training signal. The gradient of the 1-distance depends on both representations \u03a8k(R) and \u03a8k(S) in each coordinate, while the coordinate-wise minimum can only depend on one representation in each coordinate. The hypothesis class of models should contain all pushforward multiset transformations and potentially be restricted to those preserving cardinalities. Learning directly over all pushforward transformations is not feasible. Unfortunately, learning over all pushforward transformations is not feasible due to the difficulty and non-differentiability of the problem. Instead, a probabilistic approach is taken, where a probabilistic universe transformation is defined as a map from one universe to a space of probability measures. This approach allows for smoothly parametrizable transformations, leading to the definition of an expectation multiset transformation. The expectation multiset transformation L is well defined and preserves cardinalities. It is induced by a probabilistic universe transformation, allowing for learnability even when T is an expectation transformation. The question remains on how to parametrize the model. The outstanding question is how to parametrize representations \u03a8k(L(A)) for any given A \u2208 \u2126. When A is a multiset sampled from the training distribution D, the representation can be written as \u03a8k(L(A)) = x\u2208A \u03a8k((x)), where each x occurs m_A(x) times in the sum. This simplifies learning a map from \u2126 to the probability simplex in R^k. The representation of vectors in Rk is ensured to be in the probability simplex by applying a function component-wise. The model utilizes the softplus function for differentiability. The losses are defined in terms of these representations. The experimental results are presented to compare the effectiveness of the symmetric difference and intersection in practice. The model compares two approaches in terms of error on tasks and predicting containment. The importance of the precise model definition is explored, with a connection made to the DeepSets model. The balance in model properties is discussed based on theory motivating the model. The authors demonstrate that their model can learn any permutation invariant function, such as the size of the intersection or symmetric difference of multisets. They use two baseline models, \"unrestricted multisets\" and \"DeepSets\", in their experiments. They investigate the effectiveness of their multiset operations on representations by replacing terms in their losses with a learnable function. This new prediction is akin to a second DeepSets model, chosen for its permutation invariance. They also test the quality of their parametrizations of the multisets operations. In Section 5.4, the authors explore the effectiveness of their parametrizations of multisets operations through a \"cross-wiring\" scheme. They also analyze the learned representations of elements from the dataset MNIST, consisting of handwritten digit images. Training models on pairs of multisets, they randomly generate sets with cardinality in the range [2, 5] to avoid learning from singletons. The authors trained models on pairs of multisets randomly generated from the MNIST dataset, with cardinality in the range [2, 5]. The predicted cardinality of the symmetric difference or intersection was calculated using multiset representations. Adam was used to minimize squared error, with a learning rate of 5 \u00d7 10 \u22125 chosen through grid search. The models were trained on multisets generated from the MNIST dataset with cardinality [2, 5]. Evaluation included multiset sizes from [2, 20] with images from the test set. Models were evaluated on 3 \u00d7 10 4 multiset pairs using a LeNet-5 neural network architecture. Six types of models were examined for performance on various tasks. The study evaluated different models on predicting cardinality using multisets generated from the MNIST dataset. Results showed that models with learned multiset operations had higher prediction errors compared to theoretically-motivated parametrizations. The benefit of using theoretically-motivated definitions was evident, but it did not guarantee their superiority for the task. Comparing models from the expectation-transformation model to DeepSets, there is a significant decrease in performance. The error in predicting intersection size is consistently about twice as small as predicting symmetric difference size, contrary to expectations. Further exploration is needed on this intriguing gap. Additionally, models are compared on predicting containment relations between sets, a crucial task. In the experiment, pairs A and B are sampled to predict containment relations using Theorem 3.2.6 and Proposition 3.2.4. The models achieve over 96% accuracy, with a noticeable difference in performance for models trained on the intersection task. The models trained on the intersection task perform worse than those trained on symmetric difference. This suggests that the task itself may not be the best way to capture containment relations. To test the intrinsic nature of the cardinality operations, two experiments are conducted using different prediction functions. In the second experiment, models trained to predict intersection cardinality using a different prediction function show higher mean-absolute errors compared to the \"correct\" prediction functions. The learned object representations approximate standard basis vectors, indicating appropriate point-mass probabilities for each object's label. However, in cases where the true size of U is unknown, the restricted representations may be undesirable. The restricted representations in the model may be undesirable due to pinched nature, especially with discrepancies in object number and dimension. The unrestricted multiset model shows more balanced clusters, but clusters for d = n are less well-separated. The DeepSets model lacks interpretable representations. The regular multiset model performs well on containment prediction tasks even with small values of k. The model trained on multisets of sizes [2, 5] generates MNIST images of zeros, ones, and twos. A novel task involves predicting the size of symmetric differences between pairs of multisets. This approach is motivated by a measure-theoretic concept of \"flexible containment.\" By learning representations based on symmetric differences, the model achieves high accuracy in predicting containment relations. Future work may explore clustering n objects based on multiset difference sizes. In Appendix H, it is shown that n \u2212 1 specific multiset comparisons are sufficient to recover clusters. The latent multiset space U can be learned. Multiset union and addition are defined differently. Multiset difference is challenging due to the absence of a complement notion for multisets. A \\ B is defined as {1, 2} by removing common elements from A and B. A fuzzy set A over a universe \u2126 is defined by a function m A : \u2126 \u2192 [0, 1], mapping each x \u2208 \u2126 to its membership degree in A. Fuzzy set operations can be defined using this concept, such as multiset difference, which subtracts counts with a minimum value of zero. Symmetric multiset difference can be expressed as (A\\B) \u222a (B\\A), where addition or union can be used interchangeably. Fuzzy set operations are defined using element-wise fuzzy logical operations, including T-norms which generalize conjunction. T-norms are compatible with classical logic, where 0 is \"false\" and 1 is \"true\". Strong negators generalize logical negation, with the standard being n(x) = 1 - x. S-norms, or t-conorms, generalize disjunction and have the identity element as 0. The text discusses fuzzy set operations using T-norms and S-norms, which generalize conjunction and disjunction, respectively. De Morgan's laws are generalized with complementary s-norms. The intersection of fuzzy sets is defined using T-norms, while the complement and union are defined using strong negators and s-norms. The text also presents a proof for a proposition regarding multisets. The proof shows that |A B| = |A| + |B| - 2|A\u2229B| for multisets A and B over the same universe \u2126, using a dominating measure \u03bb. The elements of \u2126 are divided into subsets based on the cardinalities of A and B. The proof demonstrates that the cardinality of the intersection of multisets A and B can be expressed as |A B| = |A| + |B| - 2|A\u2229B| using a dominating measure \u03bb. This is achieved by dividing the elements of \u2126 into subsets based on the cardinalities of A and B. The restricted multiset model trained on multisets of sizes [2, 5] generates three-dimensional representations of test-set MNIST images. The problem of clustering n objects given n-1 symmetric set difference sizes can be solved with n-1 specific queries. The problem of clustering n objects with n-1 symmetric set difference sizes can be solved with n-1 specific queries. This is done in two steps, first by identifying k disjoint subsets of U, and then confirming these subsets are the clusters C1, ..., Ck. The clusters form a tetrahedron in a three-dimensional representation of test-set MNIST images. The first step involves logarithmically \"splitting\" U with queries that recursively divide the sets in half. In the process of clustering objects, sets are recursively split in half to form disjoint sets C1, ..., Ck. The second step involves verifying that objects within each set belong to the same cluster by comparing consecutive pairs. This requires a total of n - 1 queries to confirm the clusters."
}