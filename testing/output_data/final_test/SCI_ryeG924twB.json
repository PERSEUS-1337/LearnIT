{
    "title": "ryeG924twB",
    "content": "Existing works in deep Multi-Agent Reinforcement Learning (MARL) focus on coordinating cooperative agents, but in real-world scenarios, agents are often self-interested. Leaders must provide bonuses for efficient coordination, known as expensive coordination. The challenges include predicting followers' behaviors and dealing with complex interactions. This work addresses these issues through an event-based deep RL approach, modeling the leader's decision-making process as a semi-Markov Decision Process. This work proposes a novel multi-agent event-based policy gradient to learn the leader's long-term policy, design follower-aware and follower-specific attention modules, and reduce followers' decision space for faster training. Experiments show superior performance compared to existing methods in resource collections, navigation, and the predator-prey game. In real-world scenarios, agents are often self-interested, such as taxi drivers who prefer high-demand areas for more rewards. Forcing selfless contributions can lead to short-term gains but long-term inefficiency and dissatisfaction. Similarly, government initiatives to promote investment in poverty areas may reduce company profits for societal fairness. In real-world scenarios, agents are often self-interested, such as taxi drivers who prefer high-demand areas for more rewards. Forcing selfless contributions can lead to short-term gains but long-term inefficiency and dissatisfaction. The fairness of society may reduce company profits, leading to companies leaving when forced to invest. To achieve coordination among followers and leader's goals, providing bonuses like extra pay for serving customers in rural areas or government subsidies for investing in poverty areas is essential. This paper addresses the large-scale sequential expensive coordination problem with a novel RL training scheme, different from previous works focusing on static decisions. The curr_chunk discusses the limitations of existing works in addressing sequential decisions in large-scale games. M 3 RL is highlighted as a related work that focuses on leader-follower dynamics but simplifies the problem by only considering rule-based followers. The expensive coordination problem is identified with two critical issues. In the expensive coordination problem, two critical issues are identified: 1) the leader's long-term decision process and 2) the complex interactions between the leader and followers. These issues are addressed through an abstraction-based deep RL approach in this work. In addressing the expensive coordination problem, a novel event-based policy gradient is proposed to model the leader's decision-making process as a semi-Markov Decision Process. Additionally, a leader-follower consistency scheme is introduced to predict followers' behaviors accurately, and an action abstraction-based policy gradient algorithm is suggested to accelerate the training process for followers. Our proposed action abstraction-based policy gradient algorithm simplifies the interaction between the leader and followers, accelerating the training process. Experiments show that our method outperforms state-of-the-art methods in resource collections, navigation, and predator-prey scenarios. This work is related to leader-follower RL, temporal abstraction RL, and event-based RL, addressing the issue of expensive coordination in leader-follower scenarios. Our work focuses on computing the leader's policy against RL-based followers in complex scenarios, leveraging temporal abstraction RL. This is in contrast to existing methods that struggle with RL-based followers. Our proposed framework simplifies leader-follower interaction and outperforms state-of-the-art methods in various scenarios. Our leader's decision process differs from existing methods by naturally forming an intermittent decision process, eliminating the need for a two-level decision process. A novel training method is introduced based on the leader's nature. A novel training method is introduced for single-leader multi-follower Stackelberg Markov Games (SMG), focusing on event-based RL & Planning. Previous studies leverage events to capture important elements during episodes, depicting events as leader actions and environment feedback. This approach aims to learn the long-term leader's policy in SMG scenarios. In single-leader multi-follower Stackelberg Markov Games (SMG), the leader assigns goals and bonuses to followers. Transition and reward functions are defined, along with discount factor and joint policies for leader and followers. The leader's policy is based on previous actions and current state, while followers' joint policy depends on leader's action and current state. The proposed framework introduces a novel training scheme for a leader policy in Stackelberg Markov Games. The leader's utility is defined by the policy profile, while followers aim to maximize their own utilities. The trajectory is a sequence of state, leader's action, and followers' actions. The framework addresses training a leader policy against rule-based and RL-based followers in an expensive coordination problem. The curr_chunk discusses addressing issues in the leader's decision-making process and interactions with followers in an expensive coordination problem. It proposes a semi-Markov Decision Process model for the leader, a follower-aware module for predicting follower behaviors, and an action abstraction-based policy gradient method for followers to simplify decision-making and accelerate training convergence. The curr_chunk describes event-based trajectory optimization for the leader in a semi-MDP framework. It introduces a modified option structure to model the leader's decision process and follower interactions, focusing on one-step option-state transition functions with decay. The low-level policy for followers is not included, and the finite time horizon is emphasized with a gamma value of 1. This modified option aims to capture the leader's long-term decision-making. The curr_chunk discusses the leader's policy gradient in optimizing utility within a multi-agent option-state transition framework. It focuses on the deterministic occurrence of leader actions in sampled trajectories and defines event sets to represent these actions. The leader's new actions within a trajectory are further defined to track changes in their decision-making process. The curr_chunk explains the probability of the leader's actions within a trajectory and how it relates to maximizing accumulated rewards. It also discusses the probability of the entire trajectory, including followers and state transitions. The goal is to select actions that maximize rewards. The curr_chunk discusses the policy gradients for the termination function and leader's policy function in maximizing accumulated rewards. It introduces Proposition 1, which shows the relationship between the leader's commitment to a new action and the policy gradients. The leader's policy function updates infrequently due to commitment to the same action, leading to sample inefficiency. An alternative policy indication function, Event-Based Policy Gradient (EBPG), updates more frequently than sparse EBPG. Dense EBPG is preferred as it updates the leader's policy function more frequently, allowing for learning from negative rewards. The dense Event-Based Policy Gradient (EBPG) updates the leader's policy function more frequently, providing more signals to correct actions. Experiments show that dense EBPG is better than sparse EBPG. However, it is still challenging for the leader to make long-term action decisions based on current state information due to followers' changing behaviors. New modules and training schemes are introduced to capture followers' behaviors and global state changes, using neural networks to learn state representation. To capture followers' behaviors accurately, three modules are designed: leader-follower consistency, follower-aware module, and attention mechanism for sequential decision making in non-zero-sum games. Previous methods focusing on predicting opponents' behaviors are not directly applicable to this case. SMG is not zero-sum. Assumptions include limited action and state space, smooth reward function for leader action, and consistency in policies. Leader-Follower Consistency proposition states that small changes in leader's policy result in slight changes in followers' policies if game regularization and policy bound are met. The leader's policies in SMG must be updated only when followers reach the best response policy to ensure convergence. However, with RL-based followers, ensuring this is challenging due to the need for extensive training time, which is impractical. To speed up training, an action abstraction approach inspired by Poker is suggested. In SMG, followers' actions are grouped into a meta policy inspired by Poker and action abstraction RL. This simplifies decision-making as followers only need to choose a meta action. The original game is transformed into a meta game, making it easier to solve. The policy for each follower is defined based on the augmented state and a high-level meta action. The lower-level policy for choosing primitive actions is assumed to be rule-based and deterministic. For example, in a navigation task, the meta policy could be selecting a landmark to explore while the lower policy could be a specific route planning algorithm. The leader's loss functions are designed using an actor-critic structure with added regularizers for performance enhancement. Maximum entropy and L2 regularization are implemented for the leader's policy function. Imitation learning is used for the predicted action function, along with baseline functions to reduce variance. The followers in the study utilize g (c t ) and \u03c6 b (c t ) to reduce variance, with a structure based on actor-critic design and action abstraction policy gradient. The leader-follower learning rate follows the two time-scale principle. Tasks evaluated include resource collection, multi-bonus resources, navigation, and predator-prey scenarios based on the SMG framework. The leader can choose bonuses levels in different scenarios such as resource collection, navigation, and predator-prey. Followers must navigate landmarks and capture moving preys for rewards. The difficulty increases with changing follower combinations in each episode. The method is evaluated against the M3RL baseline. The study evaluates a new method against the M3RL baseline. Different ablations are tested for the leader and follower parts using various followers. Hyper-parameters are set for the implementation in Pytorch, with specific learning rates for the leader's critic and followers. The study evaluates a new method against the M3RL baseline, with specific learning rates for the leader's critic and followers. The optimization algorithm used is Adam. Training takes less than two days on a NVIDIA Geforce GTX 1080Ti GPU. Hyper-parameters are set for the loss function and exploration rates. Results show that the new method outperforms the state-of-the-art method in all tasks, demonstrating sample efficiency and fast coverage. Our method demonstrates sample efficiency and fast coverage, outperforming the state-of-the-art in multi-bonus resource collections and navigation tasks. Ablation studies show that components like attention and EBPG enhance performance. In RL-based scenarios, our approach surpasses baseline methods, especially with action abstraction, showing twice the reward in predator-prey games. The experiment confirms the robustness of our method to noise in follower actions. Our method demonstrates robustness to noise in follower actions, achieving higher total rewards and incentives compared to the state-of-the-art method. Our method outperforms the state-of-the-art in interacting with followers, Sparse EBPG is compared to dense EBPG showing the latter's superiority, attention visualization reveals capturing of followers for bonus assignment, two time-scale training scheme proves effective, and dynamic committing interval enhances performance. This paper introduces a novel RL training scheme for Stackelberg Markov Games with a single leader and multiple self-interested followers. The method outperforms fixed committing intervals, rewards followers effectively, and performs well with varying numbers of RL-based followers. The full results are available in Appendix D. The paper introduces a novel RL training scheme for Stackelberg Markov Games with a single leader and multiple self-interested followers. It includes an event-based policy gradient for the leader's policy, a follower-aware module, and an action abstraction-based policy gradient algorithm for followers. Experimental results show significant improvement over existing methods. The proposed method focuses on self-interested agents in SMGs, providing a new scheme for multi-agent systems with different roles. It contributes to hierarchical RL by offering a non-cooperative training scheme between high-level and low-level policies. Additionally, it introduces a novel policy gradient method for temporal abstraction structures. Future directions include exploring multiple cooperative/competitive leaders and self-interested followers. The curr_chunk discusses the leader-follower scheme in different scenarios such as the labor market and hierarchical organizations. It mentions adversarial attacks on the SMG model and expresses gratitude to contributors. The parameters for the termination function and leader's policy are also mentioned. The utility for the leader is explained in terms of taking new actions. The curr_chunk discusses the REINFORCE trick and the rule of derivations in the context of the termination function. It simplifies the equation using trajectory probabilities and introduces a piece-wise function for the proof of policy gradient. The curr_chunk introduces the policy gradient for the leader's actions in a multi-agent system, highlighting its sparsity and the update process. It also discusses the representation of the leader's action probability and the Lipschitz continuity of the reward function. The reward function is Lipschitz continuous with respect to \u03c9. The consistency is established under certain conditions. A baseline function is added to reduce variance in the event-based policy gradient for the leader. Successor representation is used for expected baseline functions. Two baseline neural network functions with parameters\u03b8 g and\u03b8 b are trained through minimizing the mean square error to reduce variance in the event-based policy gradient for the leader. The leader can formulate the gradient using the baseline policy gradients and leverage imitation learning to learn the action probability function p k a. Experimental scenarios are illustrated in Figure 5, detailing environments where the leader and followers collect resources, with followers having their own preferences that may differ from the leader's. Bonuses are used to incentivize followers to obey the leader's instructions. The leader incentivizes followers with bonuses based on their preferences for different resources. There are 4 types of resources and each agent has different preferences. The leader has 2 types of bonuses and 4 goals (each resource). The tasks include Multi-Bonus Resource Collections and Modified Navigation, where the leader and followers navigate landmarks with different preferences. In a Modified Predator-Prey task, the leader and followers catch preys with different preferences. Each follower has its own preference, which may differ from the leader's. Preys do not disappear when caught and can exist until the game ends. There are 8 types of preys, each with different preferences for the agents. The leader has 2 types of bonuses and 8 goals, with each prey serving as a goal. The leader in the Modified Predator-Prey task has 2 types of bonuses and 8 goals, with each prey serving as a goal. The rewards for the leader and followers are defined separately, with the leader needing to pay bonuses immediately after signing a contract. The followers in the Modified Predator-Prey task receive a reward for completing tasks based on their preferences. A penalty is imposed if they betray the leader by not agreeing to work after signing a contract. The network is modified to have a follower-specified attention mechanism instead of a vanilla one. Our method in the Modified Predator-Prey task involves follower-specific weights to indicate importance. The output for g and b is calculated sequentially to measure leader-follower coordination. Our method outperforms the state-of-the-art, especially with EBPG. Without EBPG, our method's performance is worse, showing the effectiveness of our novel policy gradient. In the navigation environment, the performance of our method diminishes without follower-specified attention, highlighting the importance of attention in certain scenarios. An ablation study in the predator-prey task evaluates the robustness of our method to noise, testing the impact of dense event-based policy gradient compared to sparse event-based policy gradient. The study evaluates the performance of Sparse Event-Based Policy Gradient (sparse EBPG) compared to dense EBPG, showing that the dense method improves the leader's performance. Additionally, sparse EBPG with follower-specified attention performs better than without, indicating the importance of attention in stabilizing training with sparse signals. The attention mechanism plays a crucial role in improving performance by identifying important followers for the leader to take actions. Ablation study shows that a higher learning rate for followers compared to the leader results in better performance. The two time-scale update scheme (TTSU) stabilizes the training process and improves reward curves. The two time-scale update scheme (TTSU) stabilizes the training process and improves performance by enhancing the leader's coordination with followers, resulting in higher rewards for RL followers compared to other methods. The leader's coordination with followers using the two time-scale update scheme improves performance and rewards for RL-based followers. The method outperforms state-of-the-art approaches, with action abstraction policy gradient being crucial for convergence. Adding modules enhances performance, with the method incorporating all modules achieving the highest reward."
}