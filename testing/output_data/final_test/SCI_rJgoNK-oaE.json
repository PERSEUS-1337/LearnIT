{
    "title": "rJgoNK-oaE",
    "content": "Validation is a key challenge in ensuring safe autonomy, with simulations often being either too simple or too complex. Approximate validation methods, like adaptive stress testing (AST), are necessary to find failures without unsafe simplifications. This paper introduces AST theory and provides examples of validation problems formulated for AST. The challenge lies in validating autonomous robotic systems operating in uncertain, real-world environments where interactions with various actors, including humans, occur amidst uncertainty in perception, prediction, and control. Simplifying scenarios by limiting non-agent actors and range is a common approach due to high-dimensional complexities. One approach to approximate validation in ensuring safe autonomy is adaptive stress testing (AST), which casts the validation problem as a Markov decision process (MDP) and uses reinforcement learning algorithms to identify the most-likely failure of a system in a scenario. This method helps uncover possible failure modes of an autonomous system for addressing, providing a bound on the likelihood of failures. Adaptive stress testing (AST) is a method that identifies failure modes of autonomous systems for addressing. It requires accurate models of all actors and is susceptible to local convergence. AST allows failures to be identified in simulation for complex systems in high-dimensional spaces. The latest methodology for using AST includes example validation scenarios formulated as AST problems. Reinforcement learning algorithms are applied to efficiently find solutions in simulation, with an RL-based solver outputting Environment Actions as control inputs to the simulator. The simulator uses reinforcement learning algorithms like Monte Carlo tree search or trust region policy optimization to find the most likely failure in a system by maximizing the probability of a trajectory ending in a specific subset of the state space where events of interest occur. The simulator uses reinforcement learning algorithms to maximize the probability of a trajectory ending in a specific subset of the state space where events of interest occur. Functions like INITIALIZE, STEP, and ISTERMINAL interact with the simulator. The reward function must be structured with parameters like \u03b1 and \u03b2f(s) to find the most-likely failure. The network takes actions to move the pedestrian close to the car early in training to find collisions quickly. The action reward function is proportional to log probabilities. An optional training heuristic is given at each timestep, with different cases for trajectory termination. The user should set a large penalty for non-terminal time-steps to optimize reward in scenarios where an autonomous system needs validation. Three scenarios are presented with examples of how they could be formulated as an AST problem, with further details in Appendix A. Cartpole is a classic test environment for continuous control. The Cartpole test environment is used for continuous control algorithms, with a neural network control policy trained by TRPO. The goal is to prevent the bar on the cart from falling over by controlling the horizontal force applied to the cart. The reward function encourages the system to push closer to failure states. Autonomous vehicles interacting with pedestrians at a crosswalk. Imperfect sensors, collision event, environment action vector, reward function with distance heuristic and Mahalanobis distance function. Encourages moving pedestrian closer to car for faster training speeds. The reward function uses the Mahalanobis distance function to ensure robustness of the Airborne Collision Avoidance System (ACASX) in identifying system failures during near mid-air collisions. The simulator involves sensor, aircraft, and pilot models to simulate scenarios with multiple planes in the same airspace. The reward function for adaptive stress testing uses \u03b1 = \u221e and no heuristics, with g(a) = log P (s t | s t+1 ). This approach aims to find failures in autonomous systems in simulation without reducing scenario complexity, essential for ensuring safety and reliability. The cartpole scenario from Ma et al. BID7 is illustrated in Figure 2. The cartpole scenario from Ma et al. BID7 involves a control policy using a neural network to prevent the bar from falling over. The system failure is defined by specific conditions. In the autonomous vehicle scenario from Koren et al. BID4, the vehicle and pedestrian's initial positions and velocities are described. The autonomous vehicle policy, based on the intelligent driver model BID10, involves a scenario where a pedestrian and a vehicle with specific initial positions and velocities need to cross paths. Using AST, various failures in the ACASX system were identified, including a near mid-air collision (NMAC) shown in Figure 4."
}