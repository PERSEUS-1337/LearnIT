{
    "title": "B1EiIsCctm",
    "content": "Generative models with discrete and continuous latent variables struggle to train using traditional log-likelihood maximization. Optimal Transport framework of Wasserstein Autoencoders can effectively train these models without modifications to the objective function. The discrete latent variable is fully leveraged by the model, providing significant control over generation. This approach offers powerful unsupervised learning capabilities. Learning with generative latent variable models like Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) offers a powerful way to uncover the underlying structure in large, unlabeled datasets. VAEs provide a stable lower bound on log likelihood and an encoding distribution, but may produce blurry samples. GANs, on the other hand, generate sharp samples but have less stable training. A new approach based on minimizing the Optimal Transport distance between generative models has also emerged. The OT approach to generative modeling aims to improve capabilities with discrete and continuous latent variables, showing potential for more effective training compared to traditional VAE methods. Discrete latent-variable models are crucial for unsupervised learning due to their prevalence in natural datasets, despite being more challenging to train than continuous models. Training discrete latent-variable models can be challenging, even with a small number of mixtures. In a study on a Gaussian-mixture latent-variable model (GM-LVM), authors struggled to train their model on MNIST using variational inference without modifying the VAE objective. The model tended to collapse the discrete latent variational distribution, posing a problem in the unsupervised setting. However, in the semi-supervised version of the same problem, the discrete latent could be learned once labeled samples were available. The OT approach to training generative models induces a weaker topology on distributions, enabling easier convergence than VAEs. GM-LVMs can be trained in the unsupervised setting on MNIST using a hierarchical generative model with discrete and continuous latent variables. The generative model in this work involves a GM-LVM with categorical distribution and continuous latent variables. When trained as a VAE, it is referred to as a GM-VAE, and when trained as a Wasserstein Autoencoder, it is called a GM-WAE. Training GM-LVMs as GM-VAEs in the traditional VAE framework can be challenging due to the discrepant scales between the latent variational distribution and the generative distribution, leading to unbalanced structures in the model. The generative model learns around the structure of the generative distribution, galvanizing the discrete distribution early in training. By choosing a variational distribution to mirror the prior, the ELBO can be written with terms depending on q D (k|x). The second term is much smaller than the first, leading to shutting off k values with large reconstruction losses. This is shown in Figure 1 where the reconstruction loss decreases within the first 10 training steps. The reconstruction loss decreases early in training by shutting off certain values of k in q D (k|x), leading to a shift in q C (z|k, x) towards a more useful distribution for the generative model. This transition results in improved reconstructions from the model after a few thousand training steps. After a few thousand training steps, reconstructions from the GM-VAE model resemble MNIST digits. The generative model learns to use continuous latent variables, but the discrete distribution q D (k|x) does not revive certain k values, leading to a penalty in the reconstruction term of the ELBO. This difficulty in leveraging the structure of latent variables in GM-VAEs is evident in the flat discrete KL and columns corresponding to shut off k values never repopulating. The difficulty in training GM-VAEs lies in the restricted convergence of distributions indexed by training steps, potentially due to the per-data-point nature of the ELBO objective and the impact of the KL divergence term on learning the variational distribution. This issue may be addressed by using an objective function inducing a weaker topology to facilitate convergence of sequences in GM-LVMs. Approaching the training of GM-LVMs using the OT framework, specifically the Wasserstein distance as the objective, may help converge to a distribution utilizing its discrete latent variable. Minimizing the 2-Wasserstein distance between the data distribution and the GM-LVM is a key focus, with parametrized distributions reducing the search space for the infimum. This approach provides an upper bound on the true 2-Wasserstein distance, particularly when the conditional distribution is deterministic. Modeling the \"variational\" distribution q(z, k|x) to mirror the prior structure, a relaxed version of W \u2020 2 is introduced, equivalent to the original distance when \u03bb \u2192 \u221e. Using Maximum Mean Discrepancy (MMD) with IMQ kernels, which have fatter tails than classic kernels, provides an unbiased U-estimator for the distance on the space of densities. The choice of bandwidth for the IMQ kernels in the GM-WAE model can be varied to reduce sensitivity, allowing for the computation of MMD between continuous distributions. This overcomes the limitation of VAEs in using deterministic generative models, as the Wasserstein distance allows for parametrization of the generative density. The generative density p G (x|z) is parametrized as a deterministic distribution x|z = g \u03b8 (z) using a deep neural network. Gradient-based minimization is enabled by parametrizing q(z, k|x) with neural networks. The GM-WAE can overcome the limitations of VAEs by utilizing the topology induced by the Wasserstein distance on the space of distributions. The GM-WAE can overcome VAE training issues by utilizing the Wasserstein distance on the space of distributions, which avoids the problem of optimizing distributions at the individual data-point level. This approach allows for more efficient movement of one distribution onto another without demanding similarity conditioned per data point. The text discusses using an aggregated posterior to allow flexibility in learning data-point specific information while matching the prior on aggregate. It also mentions using OT techniques to train GM-LVMs with simple neural network architectures on MNIST data. The prior is a mixture of Gaussians representing the 10 digits in MNIST with a non-informative uniform prior over the mixtures. The mean and covariance of each mixture are fixed before training. Choosing dim(z) = 9 worked well, with equidistant \u00b5 0 k and identical \u03c3 0 k. The variational distribution uses neural networks, while the generative model employs a deterministic approach with specific network architectures. The convolutional filters in the layer have sizes of 5x5, except for the last layer which is 1x1. Batch normalization and ReLU activation functions are used after each hidden layer. The optimization is done with Adam using a learning rate of 0.0005. The implementation of GM-WAE can reconstruct MNIST digits well from latent variables. The encoding process involves determining the mode via discrete latent and then drawing the continuous. The GM-WAE reconstructs MNIST digits well by encoding input through discrete latent modes and drawing continuous encodings. It leverages the structure of the prior to maintain multiple modes, unlike GM-VAE. The smooth manifold structure of GM-WAE's latent variables is evident in reconstructions and linear interpolations between prior modes. The GM-WAE reconstructs MNIST digits well by encoding input through discrete latent modes and drawing continuous encodings. It leverages the structure of the prior to maintain multiple modes, showing smoothness in the learned latent manifold and matching modes of the prior. The quality of samples is related to the encoder networks' ability to match the prior distribution, resulting in credible handwritten digits near the modes of the prior. The VAE objective may lead to collapse of certain k values in the discrete variational distribution, affecting reconstructions. Initializing VAE with trained WAE parameters initially produces high-quality samples, but reconstructions deteriorate after iterations and do not improve with further training. Learning curves show this deterioration. The continuous KL term in the VAE objective is much larger than the reconstruction term, leading to performance deterioration. The per-data-point nature of the KL in VAE destroys the reconstructions, making them less customized and blurrier. The GM-WAE is compared against GM-VAE in terms of reconstruction loss, with GM-WAE using deterministic generative models. Reconstruction loss curves are shown in FIG1, along with GM-VAE's losses with different rescaling factors for the KL terms. GM-WAE is not trained to minimize this loss but achieves competitive results. The GM-WAE outperforms its VAE counterpart in reconstructing MNIST digits, as shown in FIG1. Results indicate that GM-WAE, despite its lower complexity, offers competitive performance and more control over generation and inference. The GM-WAE outperforms the VAE in reconstructing MNIST digits, offering more control over generation and inference. The GM-WAE can reconstruct data and generate new samples meaningfully from the prior distribution. The discrete distribution q D (k|x) learns to assign each discrete latent value k to a different class of digit, although not perfectly. This is because the GM-WAE focuses on reconstructing data rather than encoding it in a specific way. The GM-WAE achieves nearly 70% accuracy on digit-class assignment in the discrete encoder, showing overlap between different values of k and digit classes. The model assigns digit-class labels based on the maximum average discrete latent for each class, with 5s, 8s, and 9s showing distinct clusters. The GM-WAE achieves close to 70% accuracy on the test set, while the best GM-VAE variant only reaches around 30%. Visualizations show distinct clusters in the latent variables, although they do not fully align between the prior and the samples. The GM-WAE model achieves high accuracy on the test set by effectively using both discrete and continuous variational distributions, maintaining proximity between the variational distribution and the prior. It is well-suited for data with discrete classes and continuous variation, unlike VAE models which fail to preserve the discrete-latent structure. The Wasserstein distance successfully trains a mixture-of-Gaussians model in the WAE framework, leveraging its discrete-continuous latent structure fully. Promising results on MNIST demonstrate the additional control available with both discrete and continuous latent variables, motivating further study in Optimal Transport in generative modeling."
}