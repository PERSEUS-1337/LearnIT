{
    "title": "r1e3WW5aTX",
    "content": "Identifying relations between words is crucial for understanding human languages and NLP tasks. Unsupervised operators like vector offset can recover specific relationships between words, but learning generic relation representations from word embeddings is still unclear. By treating relation representation as a supervised learning problem, we use a neural network to map word embeddings to relation representations. Evaluations show that the penultimate layer of the neural network acts as a good representation for word relations. Different types of relations exist. Representing relations between words is crucial for various NLP tasks. Two main approaches exist in the literature for this purpose, one involving statistical analysis of a text corpus and the other using lexical patterns to derive vectors representing word relations. The elements of the vector correspond to the co-occurrence of words with specific patterns in the corpus. The holistic approach measures relational similarity between word pairs based on co-occurrence patterns in a corpus, while the second approach computes relations from pre-trained word embeddings using relational operators. Sparsity is a challenge for the holistic approach due to the need for sufficient co-occurrence data. Prediction-based word embedding methods represent individual words with low-dimensional vectors. Word embeddings represent individual words with low-dimensional vectors, optimizing language modeling objectives. Prior work has shown that word embeddings encode structural properties related to semantic relations, with the PairDiff method accurately solving analogical questions. This compositional approach applies linear algebraic relational operators to word representations, sparking renewed interest in word embeddings. Several unsupervised methods, including PairDiff, 3CosAdd, and 3CosMult, compose relation representations using word embeddings. Concerns have been raised about word embeddings capturing relational structural properties, with PairDiff performing well on the Google analogy dataset but struggling with other relation types. Studies have shown that semantic relations are captured less accurately compared to syntactic relations, and word embeddings are unable to detect paradigmatic relations like Hypernym, Synonym, and Antonyms. In response to limitations of unsupervised relation composition methods, the possibility of learning supervised methods is considered. A parametrised operator is proposed to accurately represent relations between words without altering their embeddings. The Multi-class Neural Network Penultimate Layer (MnnPl) is introduced as a simple and effective method for computing relation representations from word representations. The Multi-class Neural Network Penultimate Layer (MnnPl) is a parametrised operator that accurately represents relations between words using a nonlinear feed-forward neural network. It outperforms unsupervised relational operators and generalizes well to unseen relations. The Multi-class Neural Network Penultimate Layer (MnnPl) accurately represents relations between words using a nonlinear feed-forward neural network. It outperforms unsupervised relational operators and generalizes well to unseen relations. Relations between words can be classified into contextual and lexical types. Contextual relations exist within a specific context, while lexical relations hold independently of context. Word embeddings map words to real-valued vectors to represent lexical relations. Word embeddings learning methods map words to vectors representing meanings. BID25 showed relations between words can be represented by vector-offset. Research on relational operators from word embeddings sparked. Using pre-trained word embeddings for relations is computationally attractive. Relational operators do not require co-occurrence contexts, unlike holistic approaches. This is beneficial as the number of possible word pairings grows exponentially. In the compositional approach, pre-trained word embeddings allow for computing relation representations between any two words without re-learning. This is advantageous for relational search engines, as it avoids the need to precompute relation representations for all user queries offline. Compositional methods differ from Knowledge Graph Embedding (KGE) methods like TransE, DistMult, and CompIE. In the compositional approach, pre-trained word embeddings are used to compute relation representations between words without re-learning. This method is advantageous for relational search engines, as it eliminates the need to precompute relation representations for all user queries offline. Compositional methods for relation representation differ from Knowledge Graph Embedding (KGE) methods like TransE, DistMult, and CompIE. Methods for relation representation in analogy completion tasks, such as 3CosAdd and 3Cos-Mult, involve computing distances between words in a common vector space to find missing word pairs. These methods consider the relation between word pairs (a, b) and (c, d) by calculating cosine similarity scores. This approach differs from Knowledge Graph Embedding methods like TransE and DistMult. The 3CosAdd and 3CosMult methods calculate cosine similarity scores for word pairs (b, d), (a, d), and (c, d). These methods do not create a representation for the relation between a and b. PairDiff is considered the best operator for analogy completion, while elementwise multiplication is best for link prediction in knowledge graphs. PairDiff has been proven to be the best linear unsupervised operator for relation representation. The distance between word embeddings in a semantic space significantly affects the performance of PairDiff in analogy completion. The inner-product method for measuring relational similarity between word-pairs can lead to issues by simplifying the task into measuring lexical similarities between individual words. The inner-product method for measuring relational similarity between word-pairs can lead to issues by simplifying the task into measuring lexical similarities between individual words. When using pre-trained GloVe word embeddings with PairDiff as the relation representation, lexical similarities between individual words can result in higher relational similarity scores, even if the implicit relation between the words is more similar in a different word-pair. PairDiff was evaluated using semantic and syntactic relations in the Google dataset, but recent works have shown limitations in detecting certain relation types such as hypernymy, synonymy, and antonymy. Unsupervised operators proposed in the literature are fixed and cannot capture all actual relations between words. Our proposed supervised relational composition method uses a neural network to better represent relations between words, learning from datasets like SemEval 2012 Task2 and Google. It trains a multi-class classifier to classify different relation types, addressing limitations in detecting relation types like hypernymy, synonymy, and antonymy. The use of unsupervised relation composition operators like PairDiff and vector concatenation was explored in various studies to represent word-pairs for classification tasks. However, these operators were found to not effectively capture relational properties but rather retained information from individual words for prediction by classifiers. PairDiff was deemed inadequate for inducing hypernym relations, as observed in different relation types analysis. The paper aims to learn a parametrised function to accurately represent relations between two words using their pre-trained word embeddings. The function outputs a relation representation from the input word representations, which can be in the same or different vector space. The dimensionality of word and relation representations can vary, and higher-order relation representations like matrices or tensors could be included, but this would increase computational complexity. The paper proposes a supervised relation composition operator using a feed-forward neural network with hidden layers and a softmax layer. Different nonlinear activation functions are experimented with in the hidden layers. The neural network is trained on a dataset of word-pairs with relations to predict the relation given the concatenated pre-trained word embeddings. The paper introduces a method called Multi-class Neural Network Penultimate Layer (MnnPl) to learn relation representations between word pairs. The neural network is trained to predict relations using pre-trained word embeddings and is evaluated on out-of-domain relation prediction and measuring relational similarities between word pairs. The paper introduces the Multi-class Neural Network Penultimate Layer (MnnPl) for relation prediction and measuring relational similarities between word pairs. It discusses relational training datasets, input word embedding models, experimental setup, comparison with baseline methods, and experiments on in-domain and out-of-domain relation prediction tasks. The degree of relational similarities is measured using manually assigned scores, evaluated on datasets like BATS and DiffVec 2 BID41. The BATS dataset contains 4 main relation types (semantic and syntactic) with 10 sub-relation types each, totaling 2,000 word-pairs. DiffVec has 15 main relation types with 36 subcategories, totaling 12,452 word-pairs. Different word embedding models like CBOW, Skip-Gram, and GloVe were used for comparison, all trained on the ukWaC corpus. The study utilized the ukWaC corpus BID9, a web-derived English corpus with 2 billion words. Word embeddings were trained using GloVe, CBOW, and SG models with specific parameters. LSA was also used to generate word embeddings. A co-occurrence matrix was created with the 50k most frequent words to address data sparseness. The study utilized the ukWaC corpus to train word embeddings using various models. Singular value decomposition was applied to reduce dimensionality, and word embeddings were normalized for improved results. Stochastic Gradient Descent with Momentum was used for optimization. The study used the ukWaC corpus to train word embeddings with various models and applied Singular Value Decomposition for dimensionality reduction. Stochastic Gradient Descent with Momentum was used for optimization. The model was implemented using TensorFlow with dropout regularization. Hyperparameters were validated using the SAT 374 multiple choice analogy questions dataset. The optimal configuration included two hidden layers with tanh activation function and a regularization coefficient of 0.001. The model was trained until convergence on the validation dataset and consistently performed well in evaluations. The relation representations produced by MnnPl were compared against several baselines. The study compared relation representations produced by MnnPl against various unsupervised and supervised baselines. Unsupervised baselines included PairDiff, Concatenation, elementwise addition, and elementwise multiplication. Supervised baselines included Supervised Concatenation and Super-Diff, which used weight matrices and bias vectors for relation representation computation. The study compared relation representations produced by MnnPl against various unsupervised and supervised baselines. In addition to supervised operators, the bilinear operator proposed by BID15 is used as a supervised relation representation method. The operator involves pairwise interactions between matrices and projection matrices for contributions towards the relation. The operators are trained using a margin-based rank loss objective to minimize distances between analogous pairs and maximize distances between non-analogous examples. Training instances are generated from a set of related word pairs. The study compared relation representations produced by MnnPl against various unsupervised and supervised baselines, including the bilinear operator proposed by BID15. Training instances are generated by pairing related word pairs to create positive and negative examples for a balanced binary labeled dataset. Regularization is applied to different supervised relation composition operators based on the SAT validation dataset, with no regularization for SuperConcat and Super-Diff operators due to decreased accuracy on SAT questions. The study evaluated relation representation methods by comparing MnnPl with unsupervised and supervised baselines, including the BLin operator. Regularization was applied to the BLin operator with a coefficient of 0.1 on the tensor A, while P and Q were not regularized. Evaluation focused on the ability to generalize to unseen relations in an out-of-domain prediction task. Training is not needed for unsupervised operators. The study compared MnnPl with unsupervised and supervised baselines, including the BLin operator, to evaluate relation representation methods. Regularization was applied to the BLin operator with a coefficient of 0.1 on tensor A. Evaluation focused on generalizing to unseen relations in an out-of-domain prediction task. Training is not required for unsupervised operators. The study evaluated relation representation methods by comparing MnnPl with unsupervised and supervised baselines, including the BLin operator. Regularization was applied to the BLin operator with a coefficient of 0.1 on tensor A. Evaluation focused on generalizing to unseen relations in an out-of-domain prediction task. The cosine similarity between representations for stem pairs and word-pairs in D t was measured to determine correct matches. The (micro-averaged) classification accuracy of the test sets was computed as the evaluation measure. Mean Average Precision (MAP) was used to measure the relation representation quality. To evaluate relation representation methods, Mean Average Precision (MAP) is measured. Near vs. far analogies are used to analyze similarities between word pairs, showing that detecting near analogies is easier than far analogies using word embeddings. This analysis helps in understanding the learned relation representations. The accuracy of relation representation methods is evaluated by measuring Mean Average Precision (MAP). Near analogies are easier to detect than far analogies using word embeddings. Attributional similarity is crucial in representing word relations accurately. The accuracy of relation representation methods is evaluated using Mean Average Precision (MAP). MnnPl consistently outperforms other methods in accuracy and MAP scores. CBOW embeddings show the best scores for two datasets. MnnPl can accurately represent relations in an in-domain setting as well. The evaluation of relational operators on different relation types in the BATS dataset shows that MnnPl consistently outperforms other methods for both semantic relation types. PairDiff performs worse for lexicographic relations but excels in encyclopaedic relations. ADD achieves the second best accuracy for encyclopaedic relations, while Concat follows MnnPl in MAP scores. PairDiff is biased towards attributional similarity between head and tail words in the embedding space, which is aligned for encyclopaedic relations but not for lexicographic relations. This results in poor performance for lexicographic relations compared to MnnPl, which reports the best results. PairDiff is biased towards attributional similarity between words in word-pairs. Test cases in the DiffVec dataset are grouped into lexical-overlap and lexical-nonoverlap categories to evaluate this bias. The average 1-NN classification accuracy drops significantly from lexical-overlap to lexical-nonoverlap for PairDiff, indicating a performance difference compared to MnnPl. In an in-domain setting, the performance of relation representation operators is evaluated on relational instances belonging to relation types used in the training set. 5-stratified folds cross-validation is implemented using 1-NN and MAP. The in-domain experiment setting is similar to the out-of-domain experiment, except in the latter, R is split into source and target relation sets. Detailed results for in-domain evaluation are presented in TAB6. In an in-domain evaluation, relational similarity scores are measured between word-pairs using suitable relation embeddings. The dataset used for this task is inspired by SemEval-2012 task 2 dataset. Human judgments of relational similarity are reflected in the scoring of word-pairs with similar relations. For example, pairs like (cupboard, dishes) and (kitchen, food) are rated higher in relational similarity compared to pairs like (cupboard, dishes) and (water, ocean). Multiple patterns can express instances of relations, such as \"X holds Y\" or \"Y in the X\". The BID3 dataset contains 6,194 word-pairs representing 20 semantic relation subtypes. Relational similarity scores were calculated using cosine similarity between relation vectors. Supervised methods were trained on BATS due to limited overlap with Chen datasets. Pearson correlations were measured for embedding models and relational representation methods. The proposed MnnPl model showed better correlation with human ratings compared to supervised and unsupervised baselines across all relations. The Fisher transformation test confirmed the statistical significance of MnnPl's correlations at the 0.05 level. The Concat baseline had a stronger correlation coefficient than PairDiff, while Add and Mult were stronger than PairDiff for SG and LSA embeddings. CBOW embeddings outperformed others in measuring relational similarity for out-of-domain relation prediction tasks. Obtaining accurate scores for relational similarity requires qualified fine-grained relation embeddings. The study focused on learning relation embeddings from word embeddings using parametrised operators, specifically highlighting the effectiveness of the MnnPl model in accurately representing relations between words. The results showed that MnnPl outperformed other baselines in measuring relational similarity and generalised well to out-of-domain relations despite limited training instances. Our analysis questions the effectiveness of unsupervised relation composition operators in discovering relational structures in word embeddings. We demonstrate that simple supervised operators can accurately recover relational regularities. Our work aims to inspire the NLP community to explore more sophisticated supervised operators for extracting information from word embeddings. Additionally, recent research has shown that relying solely on distributional word embeddings for lexical relations like hypernyms is insufficient, highlighting the benefits of using holistic (pattern-based) approaches. The holistic approach uses lexical contexts for relation detection, while the compositional approach relies on embeddings. Future work aims to unify these two approaches for relation representations."
}