{
    "title": "By4oPeX9f",
    "content": "In this paper, data statements are proposed as a design solution and professional practice for natural language processing technologists. The adoption of data statements can address scientific and ethical issues related to using data from specific populations in technology development. Data statements can help reduce exclusion and bias in language technology, improve precision in research claims, protect companies from public scrutiny, and create language technology that aligns with users' linguistic preferences. As technology becomes more prevalent in society, it is crucial for technologists to consider the impact of their design decisions on people, including those who may be indirectly affected by the systems. This paper focuses on the potential adverse effects of natural language processing (NLP) technology, such as systems that do not work for specific subpopulations or reinforce biases present in training data. There are both scientific and ethical reasons to be concerned about these issues. The paper discusses the importance of integrating ethical considerations into the everyday practice of natural language processing (NLP) technology. It introduces a new professional practice called data statements, which aim to improve engineering and scientific outcomes while promoting ethically responsive NLP technology. Data statements provide context about datasets to help developers and users understand how experimental results generalize, how software should be deployed, and what biases may be present in systems built on the software. In this paper, the focus is on data statements for NLP systems, which should be included in various NLP writings to address ethical issues and improve generalizability and reproducibility in the field. The paper focuses on data statements for NLP systems to enhance generalizability and reproducibility in the field. It includes defining terms, discussing the need for data statements in NLP, proposing detailed data statements, discussing bias mitigation, and making recommendations for implementation and tech policy. An NLP dataset is a collection of speech or writing with annotations indicating linguistic structure and labels classifying aspects of the speakers' intentions. Data statements promote ethical practice and sound science in NLP systems, emphasizing the importance of distinguishing between scientific soundness and ethical issues. Multi-modal data sets combine language and video to identify sarcastic language. Annotators assign annotations to raw data, sometimes using NLP tools for a first pass corrected by human annotators. Curators play a role in dataset creation. Curators play a role in dataset creation by selecting data, creating search terms, and designing interview questions. Stakeholders impacted by a system can be direct (developers, speakers, annotators, curators) or indirect. Algorithms in NLP include rule-based and machine learning approaches. In NLP, algorithms can be rule-based or machine learning. NLP systems are software that process natural language using trained algorithms. Bias in computer systems refers to systematic discrimination. Bias in computer systems can result from systematic discrimination, which can lead to unfair outcomes. This bias can stem from preexisting biases in society, technical constraints, or the application of a system designed for one context to another. Training data limitations can also lead to ethically problematic biases in NLP systems. Language data can learn pre-existing biases from speakers, including biases about gender, race, ethnicity, and religion. Machine learning algorithms can amplify these biases, leading to real-world consequences. For example, sentiment analysis systems may rate reviews of Mexican restaurants more negatively due to associations with negative sentiment in the data. Pre-existing biases can be trained into NLP systems, leading to emergent bias. Systems perform better for speakers whose characteristics match training data. Mitigating biases in datasets is crucial for ethical NLP development. The characteristics of datasets impact system performance and measurement in NLP. Academic NLP papers often discuss dataset annotation but may overlook genre and language details. Initiatives like OLAC and FLaReNet aim to improve language resources. Initiatives like OLAC and FLaReNet prescribe metadata for language resources to aid in discoverability. However, detailed characterization of speakers and annotators is lacking. Data statements should be included in NLP publications and system documentation to address this gap. Data statements in NLP publications should be detailed and concise. Long-form versions should be included in academic papers or system documentation for each dataset. Privacy considerations may limit the information included, but permission can be sought from annotators and speakers. The proposed schema for long and short form data statements includes curation rationale, selection criteria for texts, and the importance of seeking permission from annotators and speakers before collecting and publishing information. Older datasets can be updated with long-form data statements for citation purposes. The curation rationale is important for large datasets to help users understand the text systems trained with them. Language variety and speaker demographics should also be described with specific tags and information. Speaker demographic characteristics, including variation in pronunciation, prosody, word choice, and grammar, correlate with identities constructed by bilingual Mandarin speakers. Transfer from native languages can impact non-native language production. Recording quality and other relevant information should be specified in data statements. The data statements for datasets should include short form summaries in publications, covering main points from the long form statements. Speaker demographic characteristics impact language production, and recording quality should be specified in data statements. The data statements for datasets should include short form summaries in publications, covering main points from the long form statements. It is important to address the needs laid out in \u00a73 and describe both long and short versions of data statements. Best practices for writing data statements are expected to emerge as the field gains more experience. In some cases, full specification of all information may not be feasible, such as when precise demographic information is unavailable in web text datasets. Providing ranges rather than precise values may be preferable in certain situations, like to protect annotator privacy. Best practices for describing demographic characteristics can be found in resources like the American Psychological Association's Manual of Style. The importance of considering data anew for NLP work is emphasized, with a focus on examining dataset characteristics in relation to NLP goals. Data statements should be written at or near dataset creation, as illustrated with two case studies involving Twitter data labels and an interview collection. The Hate Speech Twitter Annotations collection consists of labels for \u223c19,000 tweets collected by BID55 and BID54 for studying hate speech detection in tweets. The dataset can be accessed at https://github.com/zeerakw/hatespeech. BID55 scraped Twitter data using contentious terms chosen through crowd-sourcing and intuition to analyze the effectiveness of models trained on the annotations. The dataset for Hate Speech Twitter Annotations consists of labels for around 19,000 tweets collected by researchers. The data was gathered using contentious terms chosen through crowd-sourcing and intuition. The tweets were collected via the Twitter search API in late 2015, with at least Australian and US mainstream Englishes included. Over 1500 different Twitter accounts are part of the dataset, with no direct demographic information obtained from the speakers. The dataset for Hate Speech Twitter Annotations includes tweets collected in late 2015, with contentious terms chosen through crowd-sourcing. The dataset consists of labels for around 19,000 tweets from over 1500 different Twitter accounts, with no direct demographic information obtained from the speakers. The tweets likely come from both younger and older adult speakers, majority of whom likely identify as white. Gender distribution and socioeconomic status of the speakers are not available. The dataset for Hate Speech Twitter Annotations includes tweets collected in late 2015, with contentious terms chosen through crowd-sourcing. The dataset consists of labels for around 19,000 tweets from over 1500 different Twitter accounts. The annotations include information about the demographic of annotators, with 1,065 crowdworkers recruited primarily from Europe, South America, and North America, and expert annotators recruited for their understanding of intersectional feminism. Expert annotators ranged in age from 20-40, included 3 men and 13 women, and had various ethnicities and native languages. The dataset for Hate Speech Twitter Annotations includes tweets collected in late 2015, with labels for around 19,000 tweets from over 1500 different Twitter accounts. Expert annotators represented different income levels and languages. Tweets were published between April 2013 and December 2015, with 23% reacting to a specific Australian TV show. The tweets were informal, written language of up to 140 characters per tweet, intended for either other viewers of the show or the general Twitter audience. Racist tweets aimed to provoke confrontational exchanges. The dataset for Hate Speech Twitter Annotations includes tweets collected in late 2015, with labels for around 19,000 tweets from over 1500 different Twitter accounts. Expert annotators represented different income levels and languages. Racist tweets aimed to provoke confrontational exchanges. The predominant topics for racist tweets were Islam and Islamophobia, while for sexist tweets, the topics were the TV show and people making sexist statements while claiming not to be sexist. The dataset can be downloaded from http://www. tribunalvoices.org. The VRT project, funded by the United States National Science Foundation, is part of a research program on developing multi-lifespan design knowledge. It is independent from the ICTR, the United Nations, and the government of Rwanda. Interviewees had an opportunity to review and redact any material that was misspoken or revealed confidential information. The dataset includes interviews with professionals in international justice from various nationalities. The interviewees consist of 13 women and 36 men, all adults, working in roles such as judges, prosecutors, and support roles like communications and prison wardens. The interviews are conducted in English and French, with some requiring interpretation. The dataset includes interviews with professionals in international justice from various nationalities. The 7 interviewers are information and legal professionals from the US, while the interpreters are language professionals employed by the ICTR. The initial transcription was outsourced to a professional company for accuracy. The interviews with professionals in international justice were conducted in Autumn 2008 at the ICTR in Arusha, Tanzania and in Rwanda. The transcripts were reviewed multiple times for accuracy by bilingual doctoral students. The speech situation was semi-structured, with interviewers starting with prepared questions. Accuracy in transcription was crucial due to the sensitivity of the topic and the high political status of some interviewees. The interviews conducted with professionals at the ICTR in Autumn 2008 aimed to provide reflections on their experiences and insights for the people of Rwanda and the global public in the future. Professionals from various tribunal organs were interviewed, with a focus on making the interviews widely accessible. The video interviews were recorded in high definition. The video interviews at the International Criminal Tribunal for Rwanda (ICTR) were recorded in high definition with some background noise. The data consists of transcripts from 49 spoken interviews in English and French with personnel at the ICTR, focusing on their experiences and reflections on international justice. The interviewees are adults working in international justice, the interviewers are information or legal professionals, and the transcribers are highly educated and fluent in English and French. Data statements are essential for communicating dataset characteristics and should be developed alongside datasets. Retrospective data statements may be necessary for pre-existing datasets without proper documentation. Curation rationales influence the types of texts included in datasets, as seen in the case of Hate Speech Twitter Annotations. Data statements are designed to mitigate bias in systems using data for training and testing by enabling assessment of gaps in speaker populations represented in the data. They help identify populations not included in testing and training datasets, reducing the risk of emergent bias. Data statements are crucial for identifying biases in systems trained on data that may not represent them well. They help diagnose and mitigate pre-existing bias, as seen in Speer's example of Mexican restaurants and sentiment analysis. By providing information on the training data used, data statements enable more informed system development, deployment, and audits by users. While they do not solve the entire bias problem, data statements serve as a critical infrastructure for addressing bias issues. In system design, making choices to enable access for disabled individuals is crucial to uphold the principle of universal access. Similarly, committing to addressing bias in NLP technology is essential for mitigating bias issues. In NLP technology, addressing bias is crucial for the field's progress. Data statements are key to explicit dataset characteristics, enabling evaluation and consideration of potential impacts over time with value scenarios. Value scenarios are essential in NLP technology to address bias and consider potential impacts over time. An example is Big U Hospital collaborating with the CS Department to create DiseaseAlert, improving patient outcomes by alerting staff to emerging infectious diseases. Big U Hospital's DiseaseAlert system, initially successful in improving patient outcomes by detecting local health trends, faced challenges when implemented at City Hospital in Abuja, Nigeria. Despite efforts to localize the system using tweets from Abuja, City Hospital found the system leading to unnecessary tests and inaccurate results, damaging their reputation. Big U, puzzled by the discrepancy in performance, had to address City Hospital's complaints about the system's poor performance. The developers of City Hospital's DiseaseAlert system discovered that a third-party language ID component trained on US and UK English text was causing misclassification of tweets from Abuja. This led to relevant tweets being discarded during processing, impacting the system's performance. The lack of critical information in the documentation hindered the localization process, highlighting the need for data statements in the system's training and test sets. The lack of data statements in the documentation for all system components, including third-party components, hindered the City Hospital IT staff from recognizing potential limitations of DiseaseAlert. This led to misclassification of tweets from Abuja due to the language ID component being trained on US and UK English text. If data statements had been included, biases could have been identified and addressed before deployment, enabling better system adaptation. In 2022, 'Data Statement' is a standard section in NLP research papers. Concerns remain about language communities not included in the field's data catalog, leading to potential bias. National funding supports a project to identify knowledge gaps by comparing data statements with surveys of spoken languages. The project reveals disparities in resources for different language varieties. The study identifies language varieties lacking resources, leading to a list of underserved populations. This prompts the NLP community to broaden the data catalog, prioritize languages, and fund collaborative projects. As a result, bias in the catalog is reduced, allowing for more comprehensive experiments and technology development. The NLP community recognizes limitations in existing data catalog, leading to underserved language communities. Data statements can help address this issue by broadening the data catalog, reducing bias, and improving research reliability. The potential negative outcome of data statements as a barrier to research can be mitigated with standardized guidelines proposed by ACL in 2026. The ACL proposes standardized data statements for research papers in 2028 to facilitate automated meta-analysis. However, a decline in papers from underrepresented regions and new dataset reporting prompts an investigation by an ad hoc committee. The ACL proposes standardized data statements for research papers in 2028 to facilitate automated meta-analysis. A survey reveals two causes for paper rejections: missing data statements and researchers opting for existing datasets. The ACL executive plans a mentoring service and considers relaxing standards to encourage dataset creation. Negative impacts are foreseen, but can be mitigated through other practices. It is recommended that data statements be widely used before standardization. The value scenarios described here emphasize the importance of widespread adoption of data statements and the need for mentoring and community engagement to further ethical discourse in the field. These scenarios illustrate how data statements can effectively mitigate bias in NLP systems, supported by related work in medicine, transparency in AI datasets, and algorithmic impact statements. In medicine, guidelines like CONSORT aim to improve reporting of clinical trials by providing a checklist for authors. The inclusion of eligibility criteria for participants is crucial for assessing the study's relevance to patients. The inclusion of demographic data in research can facilitate further work through meta-analyses, enabling the detection of health inequities. Several groups are working on proposals regarding bias and AI, such as 'datasheets for datasets' and 'dataset'. The MIT Media Lab proposes 'dataset nutrition labels' and 'Ranking Facts' widgets by Yang et al. The datasheets proposal is similar to ours, inspired by computer hardware specs. Both schemas include sections on dataset creation motivation. The datasheets aim to cover all dataset types for machine learning, while our schema has more specific requirements. Gebru et al. and other groups propose algorithmic impact statements to clarify data quality control for input sources in the field of NLP. Multiple starting points for this discussion will make it more fruitful. Data statements are proposed as a practical tool for NLP technologists to mitigate potential harms of technology development. They aim to address data quality control, representativeness of samples, and assumptions or limitations. Practical uptake of data statements will require coordinated efforts within the field to promote adoption. The primary cost for writers is time, with writing a data statement estimated to take 2-3 hours based on case studies. Collecting demographic information for data statements may require additional time and planning, potentially leading to projects needing approval from institutional review boards. Writers may face space constraints when including data statements in publications, while reviewers and users must carefully assess the information provided to determine dataset suitability. Data statements provide critical information for dataset adoption, saving time compared to diagnosing biases in deployed systems. NLP technologists are key stakeholders in data statement technology and engaging them in development will promote uptake. Professional organizations should convene a working group on data statements to ensure they meet NLP technologists' needs. The Association for Computational Linguistics is convening a working group on data statements to develop best practices for producing informative data statements, including steps before collecting a dataset, writing concise statements, and addressing privacy concerns. This work may draw from practices in other fields like medicine and psychology. The working group on data statements is developing best practices for creating informative data statements, drawing from fields like medicine and psychology. They recommend creating training materials, establishing mentoring networks, and implementing field-level policies to support the use of data statements. Funding agencies could require data statements in management plans, conferences and journals could provide additional space for data statements. Transparency of datasets and systems is crucial for accountability and justice. Without data statements, citizens are deprived of the ability to challenge government decisions made by automated systems, undermining due process. Building systems that are fair and representative is also a concern. Data statements are crucial for building systems that are fair and representative. They enable developers to make informed choices about training sets and flag potential underrepresented populations. Data statements also help diagnose systemic unfairness in system performance and provide transparency for government and advocacy groups. Long-form data statements should be a requirement in system documentation for academia, industry, and government. Inclusion of long-form data statements should be a requirement for ISO and other certification. Short-Form Data Statements Required for Academic and Other Publication, caution must be exercised to ensure this requirement does not become a barrier to access for some researchers. Secrecy concerns may also arise in some situations. As consumers of datasets or products trained with them, NLP researchers, developers, and the general public should use systems only if there is access to the information proposed to be included in data statements. Researchers and developers have an obligation to consider the ethical implications of their work and integrate such thoughts into regular practice. One specific proposal to address exclusion and bias in language technology is the practice of including 'data statements' in all publications and documentation for NLP systems. Including 'data statements' in all publications and documentation for NLP systems is proposed to address bias and exclusion in language technology. This practice aims to foreground how data represents the world, promote research on diverse datasets, and encourage researchers to consider stakeholder values. It may take time to develop the skill of writing data statements, but it is crucial for building systems that work for diverse populations and avoid teaching computers based on limited world views. Data statements are crucial for the scientific community, industry, and the public as they enable precise claims, targeted experiments, conscientious software development, and equitable NLP system development."
}