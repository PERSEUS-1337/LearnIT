{
    "title": "SJx63jRqFm",
    "content": "Intelligent creatures can independently explore environments and acquire skills without supervision. In this paper, the proposed method \"Diversity is All You Need\" (DIAYN) learns skills without a reward function by maximizing an information theoretic objective using a maximum entropy policy. It demonstrates the unsupervised emergence of diverse skills like walking and jumping on robotic tasks. The method can learn skills that solve benchmark tasks without receiving the true task reward, and pretrained skills can be used for parameter initialization in downstream tasks. Unsupervised skill discovery can improve exploration and data efficiency in reinforcement learning. Learning skills without reward has practical applications in reinforcement learning, as it can improve exploration and data efficiency. Intelligent creatures can explore environments and learn useful skills without supervision, which can later be used to quickly achieve specific goals. Skills discovered without reward can also serve as primitives for hierarchical RL in long horizon tasks. Unsupervised learning of skills can reduce the need for human feedback in task learning. Designing a reward function to elicit desired behaviors from an agent is challenging. Unsupervised skill discovery helps determine tasks in unfamiliar environments. Autonomous acquisition of skills without rewards is a difficult problem. Skills discovered without rewards can be used as primitives for hierarchical RL in long horizon tasks. In this paper, a method for learning diverse skills with deep RL in the absence of rewards is proposed. The objective is based on mutual information to enable RL agents to autonomously discover distinct skills that explore large parts of the state space. These skills are useful for hierarchical reinforcement learning and imitation learning. The goal is to train skills that maximize coverage over possible behaviors, even if some skills may initially perform useless actions. The paper proposes a method for learning diverse skills in deep RL without rewards. The objective is to train skills that are distinguishable and as diverse as possible to explore the environment effectively. This approach aims to push skills away from each other, making them robust to perturbations and enabling behaviors like running forward, backflips, skipping backwards, and face flops. The paper introduces a method for learning diverse skills in deep RL without rewards. It focuses on maximizing an information theoretic objective to achieve unsupervised emergence of various skills like running and jumping. Additionally, the method demonstrates the ability to solve benchmark tasks without receiving true task rewards and shows how these skills can be adapted quickly for new tasks. The paper presents a method for learning diverse skills in deep RL without rewards, focusing on maximizing an information theoretic objective for unsupervised emergence of skills like running and jumping. It demonstrates the ability to solve tasks without true rewards and shows how these skills can be quickly adapted for new tasks. The method focuses on maximizing mutual information between states and skills, enabling task-agnostic learning. Previous work has explored connections between RL and information theory, developing maximum entropy algorithms. Tools from information theory have also been applied to skill discovery, emphasizing empowerment of a hierarchical agent through maximizing mutual information between states and skills. The method aims to maximize mutual information between latent skills and trajectories, using maximum entropy policies to ensure skill diversity. It differs from previous work by fixing the prior distribution over skills and preventing collapse to a few skills. The DIAYN algorithm aims to maximize mutual information between latent skills and trajectories by updating the discriminator and skill to visit diverse states. This method differs from previous work by preventing collapse to a few skills and ensuring skill diversity with maximum entropy policies. In contrast to previous work on diversity maximization in reinforcement learning, the focus is on acquiring complex skills with minimal supervision to improve efficiency. The objective is to derive a general, information-theoretic approach that can be applied to any RL task without manual design of distance metrics. This method aims to learn many diverse policies, scaling to more complex tasks due to algorithmic design choices. Our method for unsupervised skill discovery, DIAYN (\"Diversity is All You Need\"), focuses on learning skills in an unsupervised RL paradigm. The aim is to acquire skills that can be used for various tasks by dictating the states the agent visits. This approach scales to complex tasks due to algorithmic design choices, such as off-policy RL and conditioning the discriminator on individual states. Our method for unsupervised skill discovery, DIAYN (\"Diversity is All You Need\"), focuses on learning skills in an unsupervised RL paradigm. The aim is to acquire skills that can be used for various tasks by dictating the states the agent visits. This approach scales to complex tasks due to algorithmic design choices, such as off-policy RL and conditioning the discriminator on individual states. Second, skills are distinguished using states, not actions, to account for actions that do not affect the environment. Exploration is encouraged by incentivizing diverse skills that act randomly. The objective is constructed using information theory notation, with random variables S and A for states and actions, respectively, and a latent variable Z conditioned on the policy. The objective of our method, DIAYN, is to maximize mutual information between skills and states (I(S; Z)) to ensure skills control the states visited. We also minimize mutual information between skills and actions given the state (I(A; Z | S)) to focus on states distinguishing skills. By maximizing entropy of the mixture policy (H[A | S]), we aim to have a high entropy prior distribution over skills. This objective is optimized with respect to policy parameters (\u03b8) to encourage exploration and diverse skill acquisition. In our approach, we ensure high entropy by fixing p(z) to be uniform. We aim for easy inference of skill z from the current state and randomness in skill behavior. To approximate p(z | s), we use a learned discriminator q \u03c6 (z | s). By replacing p(z | s) with q \u03c6 (z | s), we obtain a variational lower bound on our objective. DIAYN is implemented with soft actor critic (SAC) to learn a policy conditioned on the latent variable z, maximizing entropy over actions. DIAYN introduces entropy regularization in the objective function G, scaling the entropy regularizer by \u03b1 = 0.1 for a balance between exploration and discriminability. It uses a categorical distribution for sampling skills during unsupervised learning, rewarding the agent for visiting easily discriminable states. Unlike previous adversarial methods, DIAYN forms a cooperative game to avoid instabilities. The DIAYN optimization problem aims to evenly partition states between skills, with each skill having a uniform distribution. Empirical results show DIAYN's robustness to random seed variations, with minimal impact on learned skills and downstream tasks. The evaluation compares DIAYN to prior work, analyzing skills, training dynamics, and addressing issues from previous methods. In the second half, we demonstrate how skills learned by DIAYN can be applied to various tasks, outperforming baselines. Experiments range from 2D navigation to complex tasks like ant locomotion. DIAYN learns multiple distinct skills for tasks like inverted pendulum and mountain car without rewards. View videos 3 and code 4 for more details. DIAYN learns multiple distinct skills for solving tasks without rewards. Applied to continuous control tasks like half cheetah, hopper, and ant, it demonstrates a diverse set of primitive behaviors. Skills include running, flipping, jumping, walking in curved trajectories, balancing, hopping, and diving. Evaluation of skills throughout training shows increasing diversity. The mountain car experiment in FIG12 demonstrates the diversity of skills learned without rewards. The discriminator in DIAYN operates at the state level, allowing for skills that do not overlap entirely. RL agents may take actions initially yielding no reward to reach high-reward states, as shown in FIG1. In the DIAYN environment, skills are learned that exit the hallway to be distinguishable, without learning the prior p(z) to avoid the \"Matthew Effect\". Comparing DIAYN and VIC on the half-cheetah environment, VIC converges quickly to sampling only a few skills, while DIAYN discovers more diverse skills by fixing the distribution over skills. The DIAYN algorithm discovers diverse skills without a reward function, paving the way for various applications in reinforcement learning. These applications include adapting skills for maximizing rewards, hierarchical RL, and imitation learning. DIAYN can serve as unsupervised pre-training for more efficient finetuning of task-specific policies, similar to pre-trained models in computer vision. The learned skills can be used to directly maximize task rewards by selecting the skill with the highest reward and further finetuning it using the task-specific reward function. Unsupervised pretraining with DIAYN enables efficient finetuning of task-specific policies, improving learning speed on various tasks like half cheetah, hopper, and ant. The critic networks initialized with pseudo-rewards from pretraining aid in speeding up learning process. Unsupervised pretraining with DIAYN accelerates learning on various tasks like half cheetah, hopper, and ant. Hierarchical RL faces challenges with motion primitives, but DIAYN discovers task-agnostic skills that can enhance hierarchical RL. A simple extension to DIAYN for hierarchical RL outperforms baselines on challenging tasks. The meta-controller in hierarchical RL chooses skills for the next k steps. A 2D point navigation task was used to test the algorithm, showing increased rewards with more skills. DIAYN outperforms VIME by effectively partitioning the state space with learned skills. DIAYN outperforms state of the art RL algorithms on challenging tasks like the cheetah hurdle and ant navigation, showing superior performance in handling sparse rewards and obstacles. This experiment highlights the effectiveness of unsupervised skill learning in addressing exploration challenges in RL. DIAYN can effectively handle exploration challenges in RL by scaling to tasks with more than 100 dimensions. By biasing DIAYN towards discovering specific skills through conditioning the discriminator on a subset of the observation space, it can learn skills relevant to the task, such as changing the center of mass in the ant navigation task. Incorporating prior knowledge, as shown in the \"DIAYN+prior\" result, can further enhance DIAYN's performance in discovering useful skills for hierarchical tasks. DIAYN can effectively handle exploration challenges in RL by scaling to tasks with more than 100 dimensions. By biasing DIAYN towards discovering specific skills through conditioning the discriminator on a subset of the observation space, it can learn skills relevant to the task. Incorporating supervision can boost performance on hierarchical tasks, as seen in expert trajectories DIAYN imitations. The key takeaway is that DIAYN, primarily an unsupervised RL algorithm, can incorporate supervision when available to improve performance. Imitation learning replaces existing policies with differentiable ones for easier updates in response to new constraints. Given an expert trajectory, a learned discriminator estimates the skill that generated it. This approach is evaluated on half cheetah tasks. DIAYN is a method for learning skills without reward functions, enabling diverse skill acquisition for complex tasks. It can quickly adapt to new tasks, solve complex tasks hierarchically, and imitate experts. By simplifying tasks into a set of useful skills, DIAYN facilitates learning and can be combined with methods for enhancing observation space and reward functions. DIAYN is a method for learning skills without reward functions, enabling diverse skill acquisition for complex tasks. It can quickly adapt to new tasks, solve complex tasks hierarchically, and imitate experts. The skills produced by DIAYN might be used by game designers to allow players to control complex robots and by artists to animate characters. The log p(z) term in Equation 3 is a baseline that does not depend on the policy parameters \u03b8, so one might be tempted to remove it from the objective. In DIAYN, log(z) pseudo-reward is used to encourage the agent to stay alive by subtracting the log p(z) baseline. This ensures the reward function is non-negative, preventing the agent from ending the episode early. Analytic solutions can be computed for simple environments like a gridworld with N \u00d7 N dimensions. The DIAYN approach uses log(z) pseudo-reward to keep the agent alive by subtracting the log p(z) baseline. This prevents early episode termination. The distribution of states visited for each skill should match its stationary distribution. Optimal policies evenly partition the state space, as shown for the 2-skill case. The state distributions shown in FIG8 are optimal for the DIAYN objective with no entropy regularization. Policies exist that achieve these distributions, as illustrated in the figure. The skills partition the states, allowing for inference of the skill from the state. The un-regularized DIAYN objective is maximized with a set of two skills that partition the state space. In the regularized objective, an even partition is not perfect. The regularized objective in DIAYN aims for an even partition of states, which is not perfectly optimal but approaches optimality as the gridworld size increases. Skills with specific state distributions achieve the DIAYN objective within a factor of O(1/N), where N is the gridworld size. The objective prefers skills that partition states with short borders and correspond to bottlenecks. In DIAYN, the objective is to partition states into sets with short borders, favoring bottleneck states. Increasing model capacity to 300 hidden units was necessary for learning. Increasing model capacity to 300 hidden units was necessary for learning diverse skills. Skill initialization was compared to random initialization using the same model architecture. Skill z is concatenated to the current state to pass to Q function, value function, and policy. Learning stability was improved by scaling the maximum entropy objective by \u03b1 = 0.1. The cheetah hurdle environment is a modification of HalfCheetah-v1 with added boxes. The ant navigation environment is a modification of Ant-v1, with goals at the corners of a square with a side length of 4 meters. The ant receives a reward when its center of mass is within 0.5 meters of the correct goal. The maximum possible reward is +5. The hierarchical algorithm was applied to the agent's state to reach a goal, with DIAYN skills covering the state space sufficiently. The policy only needed to choose a single skill to complete the task. The plot shows the diversity of skills increasing without becoming deterministic policies. The stability of DIAYN to random seed was demonstrated by repeating an experiment for 5 random seeds, showing little effect on training dynamics. Entropy regularization with varying \u03b1 values in a 2D point mass scenario led to more diverse skills, aiding exploration in complex state spaces. Increasing \u03b1 resulted in skills visiting a wider range of states, enhancing exploration capabilities. Discriminating between skills became challenging with further increases in \u03b1. In a study on DIAYN, increasing \u03b1 values led to more diverse skills in a 2D point mass scenario, aiding exploration in complex state spaces. Discriminating between skills became difficult with further increases in \u03b1. Skills learned without task rewards were evaluated with the task reward function, showing a wide range of rewards and diversity in learned skills. Standard model-free algorithms trained directly on task rewards typically converge to specific scores on different tasks. Skills learned without rewards were compared to random skills, demonstrating a diverse distribution of rewards. The study on DIAYN showed that increasing \u03b1 values led to more diverse skills in a 2D point mass scenario, aiding exploration in complex state spaces. Learned skills without task rewards displayed a wide range of rewards and diversity, with some skills receiving rewards based on their actions. Skills that ran in place or did backflips received negative rewards, while skills that ran forward received higher rewards. Random skills that did nothing received a task reward near 1000 in the benchmark ant task. In complex environments, DIAYN explores effectively by learning diverse locomotion primitives in standard RL benchmark environments like half-cheetah, hopper, and ant. Despite never receiving any reward, the half cheetah and hopper learn skills that move forward and achieve large task rewards, while the ant learns skills that move freely in the XY plane. In the appendix, we visualize the objective throughout training and evaluate all skills on three reward functions: running, jumping, and moving. DIAYN optimizes a collection of policies for diverse exploration, unlike previous work that finds a single policy for exploration. In contrast to previous work, DIAYN optimizes skills without a reward function and performs well on tasks like running, jumping, and moving. Comparing to VIC, we learn the distribution over states induced by skill s to optimize the objective. Learning p(z) involves optimizing the objective using Lagrange multipliers, resulting in a reduction in the effective number of skills sampled from the skill distribution throughout training. This effect is observed in tasks like inverted pendulum and mountain car, with the number of skills dropping to less than 10 regardless of the size of z. In this section, the effect of learning p(z) rather than keeping it fixed is discussed. The entropy of p(z) is compared throughout training, showing a significant drop in the effective number of skills when p(z) is learned. This supports the claim that learning p(z) results in the agent learning fewer diverse skills. The effective number of skills decreases during training when learning p(z). Skills learned for inverted pendulum and mountain car without rewards are visualized, showing multiple distinct skills for solving tasks. Skills include balancing at different positions and oscillating back and forth with varying characteristics. In one trajectory, the agent's X coordinate is plotted over time for inverted pendulum and mountain car tasks. Balancing skills for the pendulum show diversity in positions, frequencies, and magnitudes. Skills for mountain car vary in reward levels, with different strategies for climbing the mountain. Some skills start moving backwards to gain speed, while others start forwards and then turn around. Skills also differ in when they turn around and their velocity. Figures 20, 21, and 22 display more skills learned without reward. The text discusses using a learned discriminator to estimate which skill was most likely used in a trajectory. It involves optimizing a problem where each skill induces a distribution over states. The goal is to project the expert distribution over states onto a family of distributions over states. This is achieved by solving an equation iteratively over skills. The expert visits BID5 and generates synthetic trajectories using a different random seed. The closest DIAYN skill is retrieved for each expert trajectory. Evaluating q \u03c6 (\u1e91 | \u03c4 * ) estimates the imitation accuracy. This information is valuable for predicting imitation success before execution, especially in safety critical settings. Comparisons are made with three baselines, including a \"low entropy\" variant. Our method is compared to three baselines in terms of task performance. Variational Intrinsic Control combines a \"low entropy\" baseline and a \"learned p(z)\" baseline. The \"few skills\" baseline learns only 5 skills while others learn 50. Results across 600 imitation tasks show that our method consistently achieves the lowest trajectory distance. The discriminator's score is a good predictor of task performance, as the distance between expert and imitation decreases with increasing score. Our method, compared to three baselines, consistently achieves the lowest trajectory distance in task performance. Learning maximum entropy skills and fixing the distribution for p(z) enabled our method to scale to more complex tasks."
}