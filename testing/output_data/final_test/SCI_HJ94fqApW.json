{
    "title": "HJ94fqApW",
    "content": "Model pruning is a technique to enhance the efficiency of deep learning by reducing computational requirements. This paper introduces a channel pruning method for speeding up computations in deep convolutional neural networks. The approach involves an end-to-end stochastic training process to make certain channels constant, followed by pruning those channels from the network by adjusting biases. This method differs from traditional approaches by focusing on simplifying the computation graph without relying on the assumption that smaller-norm parameters are less informative. Our approach involves pruning constant channels from the original neural network by adjusting biases, resulting in a compact model that can be quickly fine-tuned. This method prioritizes critical computations in deep learning pipelines and has shown competitive performance in image learning benchmarks. Some computations in a trained model can be safely removed without degrading performance, accelerating efficiency and reducing overfitting effects. Methods for determining which computations to prune can be categorized from learning or computational perspectives. In proposing a new method for CNN optimization, the focus is on viewing computations as a network flow through different layers. By introducing \"gates\" at each channel, the goal is to control the information flow and remove less relevant channels to improve efficiency. The method proposed for CNN optimization involves using \"gates\" at each channel to control information flow and improve efficiency. This approach does not add parameters or change the computation graph of the existing CNN, making it easy to build multiple compact models with different performance levels in a single round of training. This simplifies the process of selecting a balanced model for deployment. In this paper, a method is proposed for optimizing CNNs by using \"gates\" at each channel to control information flow and improve efficiency. The approach does not add parameters or change the computation graph of the existing CNN, simplifying the process of selecting a balanced model for deployment. The only applicability constraint is that batch normalization should be used in the CNN, and the method is validated across different benchmarks. Reducing the structural complexity of deep learning models has been a long-studied topic in the neural network and deep learning communities. Previous works have focused on sparsifying weights or feature maps in a network, with recent efforts exploring structured sparsity for vector components. Regularization-based pruning techniques require sensitivity analysis per layer, adding extra computations. Pruning techniques involve sensitivity analysis per layer, but their method uses global rescaling without sensitivity estimation. The approach focuses on enforcing sparsity on the scaling parameter \u03b3 in batch normalization layers to address potential issues with regularization-based pruning techniques. This method restricts pruning operations to batch-normalized convolutional layers, blocking sample-wise information passing through certain channels. The recent work by BID8 and Network-Slimming BID15 focuses on removing unimportant channels and sparsifying scaling parameters in convolutional layers. Our proposed algorithmic approach based on ISTA and rescaling trick improves optimization robustness and speed. It is unclear how other methods would handle the \u03b3-W rescaling effect and if they can be applied to large pre-trained models like ResNets and Inceptions. In linear regression, using a tractable norm to regularize parameters helps in selecting important features by comparing their norms after training. This approach is widely accepted in statistics and machine learning communities. Regularization techniques like lasso and ridge regression require normalization of predictor variables to select important features in linear models. Failure to normalize can lead to ineffective results. Nonconvex learning poses challenges for regularization, with issues like lack of control over weights' norms in different layers. Fine-tuning regularization methods is crucial to avoid limitations in model performance. Reparameterization patterns in deep linear networks subject to least square with Lasso regularization can lead to inefficiencies in gradient-based learning. Pre-trained models augmented with new norm-based parameter regularization may experience rapid increases in gradient updates. Theoretical gap in achieving parameter sparsification for deep learning models with norm-based parameter regularization. Batch normalization not compatible with weight regularization, as shown in an example of penalizing l1 or l2 norms of filters in convolution layers followed by batch normalization. Uniform scaling of weights has no effect on output. Many existing works claiming to use Lasso, group Lasso, or thresholding to enforce parameter sparsity in deep learning models have theoretical gaps. The issue of minimizing weight norms of multiple layers together and choosing proper penalties for each layer remains unclear. Some strategies involve projecting weights to the surface of a unit ball, but this leads to non-convex feasible parameter sets, making optimization for data-dependent pruning methods more challenging. Many heuristic algorithms in neural net pruning do not naturally generate sparse solutions. Thresholding is often used to set parameters to zeros, raising questions about preserving neural net functionality and setting thresholds across layers based on their contribution to performance and resource consumption. Our approach focuses on enforcing sparsity of parameters in CNN-scale parameter \u03b3s in batch normalization layers. We aim to make the channel importance comparable across different layers by measuring the magnitude values of \u03b3. Additionally, we avoid reparameterization effects across layers by batch-normalizing subsequent convolution layers. While our work lacks a strong theoretical guarantee, working with normalized feature inputs and their regularized coefficients together may lead to a more robust approach. Our channel pruning technique focuses on finding less important channels using sparsity inducing formulation in convolution with batch normalization. By setting certain elements in \u03b3 to zero, constant image channels can be pruned while maintaining network functionality. If the subsequent convolution layer lacks batch normalization, the values are absorbed into the bias term. The bias term is adjusted by setting elements in \u03b3 to zero, preserving network functionality. If no padding is used, the approximation is equivalent. Pruning is done by setting more elements in \u03b3 to zero, making deployment of the pruned model easier. Fine-tuning may be necessary to address performance degradation after pruning. In channel pruning, the \u03b3 parameter acts as a \"dam\" in a channel-to-channel computation graph of a CNN, controlling the flow of information between nodes. This approach complements the traditional method of reducing information through feature map transformations during forward propagation. Our approach introduces a method to block information at each channel by forcing its output to be constant using ISTA. Despite the gap between Lasso and sparsity in non-convex settings, ISTA (BID2) remains a useful sparse promoting method, especially when used carefully. Specifically, we adopt ISTA in the updates of \u03b3s, projecting the parameter at every gradient descent step to a potentially more sparse one. This serves as a \"flood control system\" in our end-to-end learning process. The ISTA method acts as a \"flood control system\" in end-to-end learning, with each \u03b3 functioning like a dam. When \u03b3 is zero, information is blocked, while \u03b3 = 0 allows information to pass through in geometric quantities proportional to \u03b3. Scaling \u03b3 and W l+1 affects the output x l+2 without changing it, but also scales the gradients \u2207 \u03b3 l and \u2207 W l+1 l. The parameter dynamics of gradient learning with ISTA depend on the scaling factor \u03b1. If \u03b1 is large, optimization of W l+1 progresses slower than that of \u03b3. The method involves computing sparse penalties for each layer, training a neural net with ISTA on \u03b3, and post-processing to remove constant channels. The ISTA method in end-to-end learning acts like a flood control system, with \u03b3 controlling information flow. The parameter dynamics of gradient learning with ISTA depend on the scaling factor \u03b1. The method involves rescaling weights in subsequent layers, fine-tuning with stochastic gradient learning, and adjusting hyper-parameters like learning rate \u00b5, sparse penalty \u03c1, and rescaling factor \u03b1. Proper \u03b1 selection is crucial for faster sparsification progress in ISTA. Larger \u00b5 leads to faster convergence and sparsity progress, while larger \u03c1 results in a more sparse model. Choosing \u03b1 from {0.001, 0.01, 0.1, 1} is recommended for pretrained models. When tuning parameters for pretrained models, it is recommended to choose \u03b1 from {0.001, 0.01, 0.1, 1} to warm up sparsity progress. Select \u03c1 to make cross-entropy and regularization losses comparable, choose a reasonable learning rate, and set \u03b1 based on the average magnitude of \u03b3s in the network. Monitoring these quantities during training and terminating iterations when they plateau is crucial for successful parameter tuning. If the Lasso-based regularization loss decreases linearly in the first few epochs, it may indicate unsuccessful parameter tuning. We experiment with the CIFAR-10 dataset using ConvNet and ResNet-20 architectures, resizing images to 32x32 and pre-processing them. The study focuses on reducing channels in ConvNet to convert an over-parameterized network into a compact one. The study aims to convert an over-parameterized network into a compact one by reducing channels in a 4-layer convolutional neural network. Model A is trained from scratch with specific parameters and then pruned to create a smaller network with better test accuracy. Model B is created from the pre-trained Model A, slightly less accurate but with one third fewer parameters. The study converts an over-parameterized network into a compact one by reducing channels in a 4-layer convolutional neural network. Model B, derived from pre-trained Model A, is slightly less accurate but with one third fewer parameters. Experimental observations show that reducing channels improves generalization performance, and a tradeoff between test accuracy and model efficiency is achieved. The study focuses on pruning channels in the ResNet-20 model for CIFAR-10 benchmark. Model A is trained from scratch with a 37% parameter reduction and 1% accuracy loss. Model B, derived from Model A, has a higher penalty with comparisons shown in Table 2. The approach is also tested on pre-trained ResNet-101 for image classification. The study explores pruning channels in the ResNet-101 model for image classification. Two pruned models are created with different parameter sizes and fine-tuned, resulting in minimal increase in error rates. Top-1 error rates are summarized in TAB2. Our model v2 achieved a compression ratio of more than 2.5 while maintaining error rates 1% lower than other state-of-the-art models. Training from scratch allows for proper adjustment of scales for \u03b3 and W, while pre-trained models may require additional steps for \u03b3 to warm up. Adopting the rescaling trick with a smaller \u03b1 value helps in such cases. By adopting the rescaling trick with a smaller \u03b1 value, it is possible to skip the warm-up stage and quickly start sparsifying \u03b3s. For instance, while training ResNet-101 from scratch may take over a hundred epochs, pruning can be completed in 5-10 epochs followed by a few more for fine-tuning. This approach is particularly useful when pruning channels in over-parameterized models, such as pre-trained networks fine-tuned for different tasks like image segmentation. In an image segmentation experiment, a neural network model combines an inception-like branch and a densenet branch to output a binary mask for a 224 \u00d7 224 image. The inception branch locates foreground objects, while the densenet branch refines object boundaries. Pruning channels in both branches results in a model that saves 86% parameters and 81% flops. The pruned model is fine-tuned and compared with the base model using Mean IOU as the evaluation metric. The pruned model improves over the base model on four out of five test datasets, with a 2% to 5% increase in Mean IOU. However, it performs worse on the DUT-Omron dataset. The proposed model pruning technique simplifies the computation graph of a deep convolutional neural network by updating the \u03b3 parameter in batch normalization using ISTA. Our method utilizes a \u03b3-W rescaling trick for model pruning in convolutional neural networks, avoiding numerical difficulties seen in other regularization-based approaches. Empirical validation shows its effectiveness in creating compact CNN models, with most channels in bottom layers kept and top layers pruned."
}