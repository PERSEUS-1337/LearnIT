{
    "title": "SyeZIkrKwS",
    "content": "The convolution operator in convolutional neural networks (CNNs) is computationally expensive. Various methods have been proposed to make CNNs more efficient, such as designing lightweight networks or compressing models. Despite efficient network structures like MobileNet or ShuffleNet, redundant information between convolution kernels still exists. To address this issue, a dynamic convolution method named DyNet is proposed in this paper, which can generate convolution kernels adaptively based on image contents. DyNet significantly reduces computation cost while maintaining performance, with reductions of 40.0%, 56.7%, 68.2%, and 72.4% FLOPs for ShuffleNetV2 (1.0), MobileNetV2 (1.0), ResNet18, and ResNet50 respectively, on ImageNet. DyNet accelerates inference speed of MobileNetV2, ResNet18, and ResNet50 on CPU platform. It also reduces FLOPs by 69.3% while maintaining Mean IoU on segmentation task. CNNs have evolved over the years, requiring significant computation resources for convolution operations. Recently, there has been a focus on building lightweight and efficient deep models for mobile devices, with methods categorized into efficient network design (MobileNet, ShuffleNet) and model compression techniques (pruning, factorization). Handcrafted efficient network structures have also been designed. In the context of building lightweight and efficient deep models for mobile devices, handcrafted efficient network structures have been designed. However, significant correlations among convolutional kernels lead to redundant calculations and make compression difficult. By linearly fusing convolution kernels to generate dynamic kernels, noise-irrelevant features can be extracted without cooperation, reducing computation cost remarkably. In this paper, a novel dynamic convolution method named DyNet is proposed to reduce computation cost in convolution layers. DyNet consists of a coefficient prediction module and a dynamic generation module, which predict coefficients of fixed convolution kernels and generate dynamic kernels. The method is simple to implement and can be used as a plugin for any convolution layer. Experimental results on networks like MobileNetV2, ShuffleNetV2, and ResNets show that DyNet reduces FLOPs significantly while improving accuracy on ImageNet. DyNet reduces FLOPs for MobileNetV2, ResNet18, and ResNet50 by 54.7%, 67.2%, and 71.3% respectively, with minor changes in accuracy. It also accelerates inference speed on CPU platform. Efficient network design is crucial for computer vision tasks, with various approaches like GoogleNet, SqueezeNet, and Xception being explored. Efficient network design for computer vision tasks includes approaches like bottleneck networks, depth-wise separable convolution, channel shuffling, model compression, and knowledge distillation. Redundancy between convolution kernels and redundant computation are still challenges in designing small networks. DyNet is a more effective method for reducing computation in efficient networks compared to pruning-based methods. It can significantly reduce FLOPs in models like MobileNetV2 with minimal accuracy drop. Dynamic convolution kernel generation is relevant in computer vision and NLP tasks, with approaches like directly generating kernels based on feature maps. Our proposed method aims to reduce redundant calculations in convolution by predicting coefficients for linearly combining static kernels, achieving real speed up for CNN on hardware. This approach differs from previous work by focusing on efficiency rather than model expressiveness. Theoretical analysis and correlation experiments demonstrate that dynamically fusing several kernels can reduce correlations among convolutional kernels. In the NLP domain, methods have been developed to generate input-aware convolution filters that can adapt to input sentences of varying lengths. The DyNet method aims to reduce redundant computations in convolution by predicting coefficients for combining static kernels efficiently. This approach focuses on efficiency rather than model expressiveness, unlike previous methods. It utilizes dynamic convolution to improve adaptivity and flexibility in language modeling. The proposed DyNet based architectures include Dy-mobile, Dy-shuffle, Dy-ResNet18, and Dy-ResNet50. Previous works have shown that convolutional kernels are naturally correlated in deep models. In deep models, convolutional kernels are naturally correlated. Most works aim to reduce correlations by compressing, but efficient networks like MobileNets are still hard to prune due to significant correlations. These correlations are crucial for maintaining performance by obtaining noise-irrelevant features. By dynamically fusing kernels, noise-irrelevant features can be achieved without redundant kernel cooperation. The paper proposes a dynamic convolution method to fuse multiple kernels into a dynamic one based on image contents. It includes a trainable coefficient prediction module and a dynamic generation module to achieve this. The goal is to learn kernel coefficients that create a dynamic kernel without redundant cooperation. The prediction module predicts coefficients based on image contents using a global average pooling layer and a fully connected layer with Sigmoid activation function. The dynamic generation module corresponds to fixed and dynamic convolution kernels for dynamic convolution layers. The coefficients \u03b7 i t are obtained for dynamic kernels generation. Batch based training scheme is not suitable for training dynamic convolution due to different convolution kernels for each input image. Feature maps are fused based on coefficients during training. MobileNetV2, ShuffleNetV2, and ResNets are equipped with dynamic convolution, resulting in Dy-mobile, Dy-shuffle, Dy-ResNet18, and Dy-ResNet50. Dynamic kernels can extract noise-irrelevant features independently, allowing for reduced channels in base models while maintaining performance. In Dy-mobile, the original MobileNetV2 block is replaced with a dymobile block. The channels for all three convolution layers are set to C out, and groups = C out /6 for dynamic depthwise convolution. FLOPs are reduced from 6C^2HW to C^2HW in the first dynamic convolution layer. The first dynamic convolution layer in the dy-mobile block reduces FLOPs from 6C^2HW to C^2HW. The second layer maintains FLOPs at 6CHW \u00d7 3^2, while the third layer also reduces FLOPs to C^2HW. In the dy-shuffle block, channels are split into left-branch and right-branch with a 3:1 ratio. In Dy-ResNet18 and DyResNet50, channels are reduced by half in dynamic convolution layers of each residual block. Two linear layers are used to decrease parameters due to large input channels. Data augmentation includes random cropping and flipping for training dynamic neural networks with SGD strategy. Parameters like batch size, initial learning rate, weight decay, and momentum are set at 2048, 0.8, 5e-5, and 0.9 respectively. We set batch size, initial learning rate, weight decay, and momentum as 2048, 0.8, 5e-5, and 0.9 respectively, with label smoothing at 0.1. DyNet is evaluated on ImageNet with 1.28 million training images and 50K validation images. The proposed networks are trained on the training set and compared with state-of-the-art networks under a mobile setting. The dynamic convolution significantly reduces computation cost while capturing dynamic kernels. The proposed dynamic convolution reduces computation cost while capturing dynamic kernels. Experiments on MobileNetV2 show that DyMobileNetV2 outperforms MobileNetV2, with correlation values of the generated dynamic kernels being smaller than fixed kernels. The proposed dynamic convolution reduces computation cost while capturing dynamic kernels. Experiments on MobileNetV2 show that DyMobileNetV2 outperforms MobileNetV2, with smaller correlation values for the dynamic kernels compared to fixed kernels. The redundancy of dynamic kernels is demonstrated in Figure 6, showing a distribution with values between -0.1 and 0.2, indicating less redundancy compared to fixed convolution kernels. Speed analysis on hardware performance is also conducted. The inference speed of DyNet on CPU platform was analyzed using Caffe, with a focus on the acceleration ratio with larger input sizes. Results showed that DyNet achieved reduced latency with larger input sizes, making it more effective in scenarios where networks perform better with larger inputs. DyNet is more effective in scenarios where networks perform better with larger inputs. The training speed on GPU platform is slower due to feature map fusion. DyNet reduces FLOPs by 69.3% while maintaining MIoU on Cityscapes validation set compared to Dilated FCN with ResNet50. Dynamic convolution outperforms static convolution in experiments. The study compares Dy-mobile (1.0) and Dy-shuffle (1.5) with Fix-mobile (1.0) and Fix-shuffle (1.5) networks, showing absolute classification improvements of 5.19% and 2.82% respectively. The dynamic kernel retains representation ability effectively, with the group size (g t) not affecting computation cost but influencing network performance. Ablative study on g t sets it as 2, 4, and 6 for Dy-mobile (1.0). The study conducted ablative analysis on the group size (g t) for Dy-mobile(1.0) networks, with g t set as 2, 4, and 6. Results showed that larger g t improved performance as more kernels cooperated for noise-irrelevant features. A comparison between g t = 1 and g t = 6 on Dy-mobile(1.0) and Dy-ResNet18 revealed a decrease in accuracy by 2.58% and 2.79% respectively, indicating that the enhancement in dynamic networks was not solely due to the attention mechanism. The motivation for dynamic networks comes from convolutional operations, where kernels cooperate for noise-irrelevant features. The space expanded by the kernels is denoted as \u2126, and if trained until noise space \u03a8 is a subset of \u2126, noise-irrelevant features can be obtained through kernel cooperation. Dynamic networks are motivated by convolutional operations where kernels work together for noise-irrelevant features. The white output function is decomposed with \u03b2 and x, assuming normalized norms. When there is no noise, the output is determined by linearly combining convolution outputs. To reduce computation cost, a dynamic kernel fusion approach is proposed, allowing for noise-irrelevant output with a single convolution operation. The coefficients are determined by the input image and subsequent layers. The proposed dynamic convolution in Eq. 10 determines coefficients \u03b2 based on input, enabling noise-irrelevant output. By simplifying equations and utilizing vertical vectors, the output function is expressed as a linear combination of convolution outputs. This approach reduces computation costs and allows for noise-irrelevant features with a single convolution operation. The coefficients are determined by the input image and subsequent layers."
}