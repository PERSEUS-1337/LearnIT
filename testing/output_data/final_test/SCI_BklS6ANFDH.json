{
    "title": "BklS6ANFDH",
    "content": "Semantic dependency parsing involves finding bi-lexical relationships with graph-structured representations. A semi-supervised learning approach based on the CRF autoencoder framework is proposed, using a discriminative neural parser as the encoder and a generative neural model as the decoder. The model shows significant improvement over the supervised baseline in identifying predicate-argument relationships within sentences. Semantic dependency parsing involves extracting high-level structured semantic information from sentences using directed acyclic graphs (DAG). Different from syntactic dependency parsing, semantic dependencies allow a word to have multiple predicates. Supervised models for semantic dependency parsing have transition-based or graph-based parsing mechanisms. However, a limitation is the scarcity of labeled data resources for training these models. The scarcity of labeled data for semantic dependency parsing (SDP) limits scale and diversity. Annotation of SDP graphs is costly, especially with deep learning's data hunger. To address this, semi-supervised SDP learning from labeled and unlabeled data is explored. Unsupervised and semi-supervised SDP research is lacking compared to supervised approaches. Existing successful unsupervised and semi-supervised methods do not have tree-shape restrictions on parsing results. In this work, an end-to-end neural semi-supervised model is proposed for learning a dependency graph parser for semantic dependency parsing (SDP). The model leverages both labeled and unlabeled data, using a Conditional Random Field Autoencoder framework and an encoder based on a supervised model by Dozat & Manning (2018). The model proposed is an end-to-end neural semi-supervised approach for learning a dependency graph parser for semantic dependency parsing. It utilizes a CRF Autoencoder framework with an encoder based on a supervised model by Dozat & Manning (2018) and a generative decoder using a recurrent neural network language model. The model is arc-factored, allowing for tractable learning and parsing, and a unified learning objective is defined to leverage both labeled and unlabeled data. Our model, based on the CRF autoencoder framework, outperforms the state-of-the-art supervised baseline on the SemEval 2015 Task 18 Dataset. It does not involve sampling, ensuring better stability, and benefits from different amounts of unlabeled data. The CRF autoencoder aims to reconstruct the input with an intermediate latent structure, maximizing the conditional reconstruction probability. The model, based on the CRF autoencoder framework, surpasses the state-of-the-art supervised baseline on the SemEval 2015 Task 18 Dataset. It focuses on reconstructing the input with an intermediate latent structure to maximize conditional reconstruction probability. The input is a natural language sentence represented by a sequence of words, with a special token TOP at the beginning. The model, based on the CRF autoencoder framework, surpasses the state-of-the-art supervised baseline on the SemEval 2015 Task 18 Dataset by reconstructing the input with an intermediate latent structure to maximize conditional reconstruction probability. The input sentence is represented by a sequence of words, with a special token TOP at the beginning. The encoder produces a dependency parse graph of the input sentence, while the decoder generates a word sequence. The encoder computes the probability of generating the dependency parse graph given the sentence, and the decoder computes the probability of reconstructing the sentence conditioned on the parse graph. The conditional distribution is specified by the encoder and decoder in combination. The model focuses on maximizing the conditional reconstruction probability P(\u015d|s) by considering dependency arc predictions. The encoder, based on the model of Dozat & Manning (2018), uses a multilayer bi-directional LSTM to generate contextualized representations of words, POS tags, and lemma embeddings in the input sentence. Arc-label predictions are left for future work. The model utilizes a multilayer bi-directional LSTM to create contextualized word representations, which are then processed by feedforward neural networks to generate vectors for word dependencies. A biaffine function is applied to word pairs to determine arcexistence scores, allowing for the computation of arc presence likelihood in a sentence. The model uses a sigmoid function to calculate arc-absence probability in dependency parsing. The probability of generating a dependency parse graph from the encoder is computed. The generative decoder incorporates dependency relationships and parse graphs, inspired by a Graph Convolutional Network. Semantic dependency parsing allows for multiple dependency heads for a word. The proposed approach involves generating a word multiple times, each time conditioned on a different head in a dependency graph. This method aims to simplify the decoder structure for better inference and learning tractability. The proposed approach involves generating words multiple times, each time conditioned on a different head in a dependency graph. Parameters are shared among all generators. The generative probability of each word is computed with and without dependency arcs. The decoder in the proposed approach generates word representations based on different dependency heads in a graph. Parameters are shared among all generators, and generative probabilities are computed with and without dependency arcs. The decoder in the proposed approach generates word representations based on different dependency heads in a graph. Parameters are shared among all generators, and generative probabilities are computed with and without dependency arcs. In the model, the probability of generating sentences from a dependency graph is computed by taking the geometric mean of probabilities for all sentences. The final decoding probability is defined using parameters of the encoder and decoder to parse a sentence by maximizing the probability of the sentence and its parse. The proposed approach involves finding a parse graph Y that maximizes the probability P(\u015d = s, Y |s) for a given sentence s. The parsing algorithm has a time complexity of O(m^2) for a sentence with m words. The model is trained using supervised and unsupervised loss functions, with a balance between the two determined by a tunable constant \u03c1. The supervised loss is computed for labeled sentences (s, Y*), where Y* is the gold parse graph. The model is trained using supervised and unsupervised loss functions, with a balance between the two determined by a tunable constant \u03c1. Gold parses provide labels for each dependency, and a purely supervised module is used to model dependency labels. The unsupervised loss maximizes the conditional reconstruction probability for unlabeled sentences. The model can be trained end-to-end on a dataset containing both labeled and unlabeled sentences. The performance of the model is evaluated on the English corpus of the SDP 2014 & 2015 for Broad Coverage Semantic Dependency Parsing. The model is evaluated on the English corpus of the SDP 2014 & 2015 for Broad Coverage Semantic Dependency Parsing using Unlabeled F1 score (UF1) and Labeled F1 score (LF1) metrics. The encoder hyper-parameters are adopted from Dozat & Manning (2018), with words occurring less than 7 times treated as UKN. The decoder has one layer of uni-directional LSTM with a hidden size of 600. The model uses a single layer of uni-directional LSTM with a hidden size of 600. The output sizes for FNN (dec\u2212head) and FNN (dec\u2212pre) are both 400, activated by a tanh(\u00b7) function. The loss function is optimized by the Adam+AMSGrad optimizer, with hyper-parameters \u03b2 1 , \u03b2 2 kept the same as Dozat & Manning (2018). The training time for one batch with the autoencoder is 2-3 times longer than Dozat & Manning (2018) due to the extra decoder. In the first experiment, a fixed amount of labeled data is used, with more unlabeled data continuously incorporated into the training set. Experimental results show that our model outperforms the baseline in the purely supervised setting. The model is trained on both labeled and unlabeled data, with a proportion increasing from 0% to 90% of the complete dataset. Long sentences are removed to reduce running time and memory usage. The advantage of our model is more significant on the out-of-domain test set. In the semi-supervised setting, our model shows improved performance with an increasing size of unlabeled data up to 30%. However, little improvement is observed beyond 40% unlabeled data, suggesting a possible upper limit. The model outperforms supervised models significantly with a 0.1:9.9 proportion of labeled and unlabeled data. In the semi-supervised setting, the model's performance improves with more labeled data, but the advantage of the semi-supervised model diminishes. The model shows strong generalizability on out-of-domain tests. Evaluation is done on three representations: DM, PAS, and PSD, with hyper-parameters slightly tuned based on previous experiments. 10% of sentences are used as labeled data, and the rest as unlabeled data. The experiment follows a semi-supervised approach with 10% of sentences labeled and the rest unlabeled. Different word representations are examined, showing significant improvement of the semi-supervised model over supervised baselines on both DM and PSD tasks. The experiment showed improvement of the semi-supervised model over supervised baselines on DM and PSD tasks, but not on PAS representation. Previous work on unsupervised or semi-supervised dependency parsing has focused on tree-structured parsing. Our decoder, inspired by previous work, handles parse graphs and is arc-factored. It adopts the CRF Autoencoder framework for semantic dependency parsing, unlike existing approaches focused on different semantic representations. In this work, a semi-supervised learning model for semantic dependency parsing using CRF Autoencoders is proposed. The model consists of a discriminative neural encoder for producing a dependency graph based on an input sentence, and a generative neural decoder for input reconstruction. This approach promises end-to-end learning in an arc-factored fashion. The model for semantic dependency parsing works in an arc-factored fashion, offering end-to-end learning and efficient parsing. It outperforms the baseline in both full-supervision and semi-supervision settings, showing further improvements with unlabeled data. Experimental results and hyper-parameters are provided, along with stability testing on DM annotation. To test the stability of the model, the experiment from Section 4.4 was repeated three times on the DM annotation with different sampled datasets. Experimental results on three randomly sampled datasets are shown in Table 5, comparing the supervised model of Dozat & Manning (2018) with our model trained on labeled data only and our model trained on both labeled and unlabeled data."
}