{
    "title": "B1GySqOojm",
    "content": "End-to-end automatic speech recognition (ASR) evaluates performance by measuring word-error rate (WER). This paper analyzes an ASR model combining word-and-character representation in a multi-task learning framework, showing improvement in WER. Character-level supervision helps the model interpolate between recognizing frequent and shorter words. Recent works have shown that multi-task learning (MTL) on the word-and character-level can improve the word-error rate (WER) of common end-to-end speech recognition architectures. MTL can be interpreted as learning an efficient mapping from audio signals to character outputs, allowing for simpler decoding mechanisms and reduced prediction time. However, word-level models face challenges with out-of-vocabulary words and decreased training examples per label class. In this work, the focus is on character-level MTL models like CTC, with potential benefits for other models like RNN Transducers or Encoder-Decoder models. The study shows that training a word-level model from scratch on a small dataset is feasible, and performance can be enhanced with character-level supervision. The study focuses on character-level MTL models like CTC, showing benefits for other models. It discusses the challenges of training word-level models from scratch and the use of MTL for joint training on multiple levels of granularity. The study discusses the benefits of character-level MTL models like CTC and the challenges of training word-level models. Different MTL methods improve CTC baseline by combining losses in parallel or hierarchical structures. Orthogonal works explore minimizing WER directly. The right granularity of word decomposition is a challenging problem, with options for fixed decompositions or optimizing alignments and decompositions jointly. In natural language processing, the study explores the inductive bias learned through multi-task learning for speech recognition using the CTC loss function. The CTC loss function defines the relationship between audio input and groundtruth transcripts, enabling efficient computation of gradients. The output representation allows the model to process the input data effectively. The output representation of the CTC loss function enables the model to transcribe any word without a specified alignment. Character-level CTC models can benefit from external language models to improve accuracy. Using a word-level alphabet ensures correct spelling but may result in unknown tokens for rare words. Label sparsity is a challenge in word-level models. Label sparsity is a challenge in word-level models, leading to overfitting. To address this, a word-character-level model combines word and character-level information using an MTL loss. The MTL loss combines character-level CTC loss and word-level CTC loss with a hyperparameter \u03bb. Setting \u03bb to 1 gives equal weight to both losses, but other choices may improve performance. Alternatively, the weight could be estimated based on uncertainty or gradient norm of each loss term. We experimented with different approaches to weight the loss terms, but did not see a significant performance improvement over equally-weighted loss. Our models were trained using a convolutional architecture based on Wav2Letter, which avoids iterative computation over time and performs well in terms of WER. Experiments were conducted on news articles from the Wall Street Journal dataset, focusing on word frequency and length. The character-level model used 32 characters including a space and blank token for output. The character-level model used 32 characters, including a space and blank token for output. The word-level model had an alphabet of 9411 units with an OOV rate of 9% on the training set and 10% on the validation set. The MTL model shared every layer between word and character levels, except the last. Greedy decoding was used for output, with a heuristic to replace unknown tokens. Word and character-only models were also trained using the Adam optimizer. The experiments involved training word and character-only models using the Adam optimizer with specific parameters. Batch normalization and dropout were applied, and input data was transformed into spectrograms. The results showed that multi-task learning (MTL) converged faster with a lower Word Error Rate (WER) compared to single-task character-level models. The experiments showed that multi-task learning (MTL) outperformed single-task character-level models in terms of Word Error Rate (WER). A word-level-only model achieved similar performance to the character-level baseline. Combined decoding slightly improved WER and eliminated unknown-token predictions, enhancing transcript readability. The model's preference bias was characterized based on previous research findings. The model's preference bias is characterized by showing which examples are easy to classify in the particular representation each model is learning. The distribution of recognized words changes during training, with the word-level model biased towards recognizing common words first. The model's bias towards recognizing common words first is evident in both word-level and character-level models. The word-level model covers all words regardless of length early in training, while the character-level model focuses on shorter words initially. The MTL model combines both biases to learn a more uniform distribution across word frequency and length. The study explores combining word-level and character-level models in ASR to improve performance. Training on a word-level with character-level supervision in MTL shows noticeable improvements. The addition of character-level supervision interpolates between recognizing more frequent words and shorter words, leading to a more uniform distribution across word frequency and length. The study suggests combining word-level and character-level models in ASR to enhance performance. Character-level supervision in MTL training results in improved recognition of infrequent but crucial long words, especially in challenging datasets like medical communication. Further analysis of word distributions in terms of pitch, noise, and acoustic variability could offer more insights."
}