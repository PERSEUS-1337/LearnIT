{
    "title": "B1l6qiR5F7",
    "content": "Natural language is hierarchically structured with smaller units nested within larger units. The standard LSTM architecture lacks bias towards modeling a hierarchy of constituents. The proposed ON-LSTM adds an inductive bias by ordering neurons, achieving good performance on various language tasks. The underlying structure of language is tree-like, with rules determining how words form meaningful units. The human brain can implicitly acquire this structure, leading to interest in neural network approaches for latent structure induction. Integrating a tree structure into a neural network language model is important for hierarchical representation and abstraction levels. Deep neural networks use levels of abstraction to model language composition effects and improve generalization. Techniques inferring tree structures for natural language sentences have gained attention. Predicting latent tree structures can guide word semantics composition and aid in next word prediction. Grammar induction, the task of learning syntactic structure from raw data without expert-labeled data, remains a challenge. Recent attempts have faced issues such as inducing trivial structures or struggling with Reinforcement Learning for branching policies. Recurrent neural networks have been effective for language modeling. In this work, ordered neurons are introduced as a new inductive bias for recurrent neural networks, aiming to improve language modeling by capturing latent tree structures in language. Introducing ordered neurons as a new inductive bias for recurrent neural networks, promoting differentiation in storing long-term and short-term information. Proposing the cumax() activation function to allocate neurons effectively. Designing the ON-LSTM model biased towards tree-like composition operations, achieving good performance in language modeling. The ON-LSTM model shows good performance in language modeling, unsupervised constituency parsing, syntactic evaluation, and logical inference tasks. It aligns with syntax principles better than previous models and outperforms standard LSTM models in capturing long-term dependencies and generalizing to longer sequences. Previous work has also leveraged tree structures for natural language tasks. In a study by Bowman et al. (2016), they used guidance from a supervised parser to train a stack-augmented neural network. Recent findings suggest that introducing structure information into LSTMs is beneficial, with RNNGs outperforming LSTMs on certain tasks. Tree-structured models have shown to be more effective for tasks generated by recursive programs, indicating that hierarchical structures may be helpful. The task of learning grammar structures from data, known as grammar induction, remains a challenge. Previous work has incorporated syntactic structure in language modeling, with recent attempts using neural models for algorithmic tasks. The Parsing-Reading-Predict Networks (PRPN) model introduced self-attention to compose previous states for parsing, aiming to align learned structures with gold-standard parse trees. The PRPN model uses self-attention to compose previous states based on a learnt \"syntactic distance\" corresponding to parse tree depth. Alternative models like Clockwork RNN and multi-scale hierarchical recurrence have been proposed to capture hierarchies in data. Our work proposes a soft relaxation of dropout using cumax() activation and conditions update masks on input for sequential data processing. Unlike Clockwork RNN and nested dropout, our model can adapt the structure to observed data instead of imposing a predefined hierarchy on hidden representations. The model introduces ordered neurons to dynamically reallocate dimensions of the hidden state to each node in a tree structure, allowing for representation of information at different time-scales. The model introduces ordered neurons to represent information at different time-scales in a tree structure. High-ranking neurons contain long-term global information that lasts for several time steps. Visualization shows different update frequencies of hidden state neurons. The new RNN unit, ON-LSTM, introduces ordered neurons to represent information at different time-scales in a tree structure. High-ranking neurons encode long-term global information, while low-ranking neurons encode short-term local information. The model controls the update frequency of neurons in a data-driven manner, with some neurons updating more frequently than others. The ON-LSTM introduces ordered neurons to represent information at different time-scales in a tree structure. It controls the update frequency of neurons in a data-driven manner by introducing a new activation function that enforces an order for updating neurons. This function splits the cell state into two segments, allowing the model to apply different update rules. The ON-LSTM introduces ordered neurons to represent information at different time-scales in a tree structure. It controls the update frequency of neurons by splitting the cell state into two segments: the 0-segment and the 1-segment. The model applies different update rules on the two segments to differentiate long/short-term information. The variable d represents the split point between the two segments, and the probability of the k-th value in g being 1 is computed using a cumulative distribution function. In practice, a continuous relaxation is used to compute gradients due to the inclusion of a discrete variable in the computation graph. The ON-LSTM model introduces ordered neurons to represent information at different time-scales in a tree structure. It uses master forget and input gates to control update operations of cell states. The new update rule is defined based on the properties of the cumax() activation. The master gates serve as high-level control for erasing and updating information in the model. The ON-LSTM model utilizes master forget and input gates to manage the updating of cell states, with ordered neurons representing information at different time-scales in a tree structure. The forget gate erases information stored in previous cell states, while the input gate controls the writing mechanism of the model based on the content of the current input. The model's update rule is defined using the cumax() activation function, with master gates serving as high-level control for information processing. The ON-LSTM model uses master gates to manage cell state updates, with ordered neurons representing information at different time-scales. The product of the master gates represents the overlap of input and previous information. The model allows for fine-grained operations within blocks controlled by forget and input gates. The master gates focus on coarse-grained control, with the same dimensions as hidden states for computational efficiency. In practice, setting ft and \u0129t as Dm = DC dimensional vectors, where D is the dimension of hidden state and C is a chunk size factor, significantly reduces the number of extra parameters needed for the LSTM. Neurons within each C-sized chunk share the same master gates. The proposed model is evaluated on tasks such as language modeling, unsupervised constituency parsing, targeted syntactic evaluation, and logical inference. Language modeling evaluation is done by measuring perplexity on the Penn TreeBank task. Our model, a three-layer ON-LSTM with 1150 units in the hidden layer and an embedding size of 400, outperforms the standard LSTM in the Penn TreeBank task. Dropout values were manually selected for optimal performance, with a weight-dropout of 0.45 applied to recurrent weight matrices. The total number of parameters increased slightly to 25 million, including additional matrices for master gates computation. The ON-LSTM model outperforms the standard LSTM in the Penn TreeBank task with the same number of layers, embedding dimensions, and hidden states units. The master gates control information storage in neurons, improving performance without skip connections or parameter increase. The model is tested on the WSJ10 dataset and WSJ test set for unsupervised constituency parsing. The WSJ10 test set includes sentences from the PTB dataset, while the WSJ test set uses the same sentences as the PTB test set. To infer the tree structure of a sentence from a pre-trained model, hidden states are initialized with the zero vector. The top-down greedy parsing algorithm is used for unsupervised constituency parsing. The ON-LSTM model achieves state-of-the-art unsupervised constituency parsing results on the WSJ test set. The first and third layers do not perform as well, possibly due to being too focused on capturing local information. ON-LSTM shows better generalization and robustness towards longer sentences compared to previous models. It also provides strong results for phrase detection, including ADJP, PP, and NP. The ON-LSTM model achieves state-of-the-art unsupervised constituency parsing results on the WSJ test set, showing better generalization and robustness towards longer sentences. It also provides strong results for phrase detection, including ADJP, PP, and NP, which could benefit various downstream tasks. Targeted syntactic evaluation tasks have been proposed in BID38 to evaluate language models on structure-sensitive linguistic phenomena. The ON-LSTM model outperforms the baseline LSTM on long-term dependency cases, while the baseline LSTM performs better on short-term ones. The models were trained with specific parameters and evaluated on the WSJ test set using different random seeds to calculate the average F1 score. The ON-LSTM model shows strong results for phrase detection, such as ADJP, PP, and NP, indicating its potential for various downstream tasks. The ON-LSTM model outperforms the baseline LSTM on long-term dependency cases, while the baseline LSTM performs better on short-term ones. Results for NPI test cases show high variance across different hyper-parameters. ON-LSTM achieves better perplexity on the validation set and performs well on logical inference tasks with a vocabulary of six words and three logical operations. The ON-LSTM model outperforms the baseline LSTM on long-term dependency cases and logical inference tasks. The model requires predicting the correct label for a pair of sentences using an RNN encoder and a multi-layer classifier. The RNN models had 400 units in one hidden layer, an input embedding size of 128, and a dropout of 0.2 between layers. The ON-LSTM model, with an embedding size of 128 and a dropout of 0.2, outperforms the standard LSTM on logical inference tasks. It achieves nearly 100% accuracy on short sequences and shows better performance on longer sequences. The ON-LSTM model demonstrates superior generalization on structured data with varying lengths compared to standard LSTM. It incorporates ordered neurons, a new recurrent unit with a unique gating mechanism and activation function, cumax(\u00b7). The model outperforms on logical inference tasks, achieving close to 100% accuracy on short sequences and showing improved performance on longer sequences. Additionally, a tree-structured model like RRNet BID24 can achieve strong performance on datasets with ground truth structure input. The ON-LSTM model introduces a new activation function cumax(\u00b7) and mechanism for tree-like composition operations in recurrent neural networks. It performs well on unsupervised constituency parsing and language modeling tasks, showcasing its ability to capture the latent structure of natural language. Additionally, the model excels in handling long-term dependencies and logical inference tasks."
}