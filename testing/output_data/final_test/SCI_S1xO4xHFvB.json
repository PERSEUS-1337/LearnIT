{
    "title": "S1xO4xHFvB",
    "content": "Compressed forms of deep neural networks are crucial for deploying large-scale models on resource-constrained devices. Atomic Compression Networks (ACNs) propose a novel architecture that repeats a small set of neurons in subsequent layers, achieving significant compression rates compared to traditional neural networks with minimal loss in accuracy. Our method achieves significant compression rates (up to 1116\u00d7 reduction) with minimal loss in accuracy (0.15% to 5.33%), allowing for sub-linear model complexities and enabling deep ACNs with fewer parameters than logistic regression. The universe is made up of matter, formed by atoms which define chemical elements and bond to create molecules. In Machine Learning, neurons are like atoms in intelligence, forming complex models. Unlike physical matter, neural networks do not reuse neurons across layers but optimize weights independently for each neuron. This new paradigm for constructing deep neural networks is inspired by nature. Proposing a new paradigm for constructing deep neural networks called Atomic Compression Networks (ACNs), which repeat a fixed set of neurons to achieve unprecedented compression in network parameters without compromising prediction quality. Deep neural networks (DNN) excel in various domains like computer vision and natural language processing, but face challenges in resource-constrained environments. The text discusses a novel compression paradigm for neural networks called Atomic Compression Networks (ACNs), which repeat a fixed set of neurons to achieve significant compression in network parameters. This approach aims to address fragility concerns in existing neural compression techniques and has shown promising results in real-world vector datasets and computer vision tasks. The curr_chunk discusses various approaches to building deep neural network models using modular neural networks, such as End-to-End Module Networks, Compositional Recursive Learner, Routing Networks, Modular Networks, and recent work on the recursive formulation of the Routing Network. These models utilize modular transformations and conditional computation to improve network architecture. The recent work by Cases et al. (2019) focuses on the recursive formulation of the Routing Network, applying modules consecutively. Unlike MNN, our approach uses smaller neurons and reuses components within the same network level. Zaremoodi et al. (2018) extend the RN model by using a router in a gating mechanism to control input to shared RNN modules, similar to a Mixture of Experts model. Neural Architecture Search (NAS) (Zoph & Le, 2016) is concerned with automatically discovering high-performing neural networks. Neural Architecture Search (NAS) focuses on automatically discovering high-performing neural network architectures for various tasks using search approaches like (Neuro-)Evolution and Reinforcement Learning. A sub-field of NAS involves searching for neural modules called cells or blocks that can be reused in network architecture, although the discovered cells are typically more complex than single neurons. Network compression techniques like weight pruning have been used to prevent overfitting by reducing network complexity. Various approaches such as iterative weight pruning and filter pruning based on different norms have been proposed in the literature. While some methods require time-consuming architecture search, our approach can be trained from scratch in one go, offering a more efficient solution. Weight pruning techniques have been explored by various researchers, including L 0 -norm regularization, channel pruning based on activation effects, importance score calculation using Taylor expansion, adaptive pruning schemes for weight sub-groups, and Bayesian approaches. Different colored arrows represent input masks for neurons in the network, with independent output layers. Factorization methods are also used to create low-rank approximations. Recent research explores factorization methods for producing low-rank approximations of weight matrices in DNNs. Various techniques such as vector quantization, squeeze modules, Tensor-Train format, and Kronecker product are used for compression. Hybrid models combine pruning, quantization, Tucker decomposition, and Bayesian matrix factorization for efficient model compression. The teacher-student approach, a variant of compression methods for DNNs, involves distilling knowledge from a fully trained deep teacher model to a smaller student model. Different approaches like regularization and quantization techniques are used in designing and training the student model. ACNs offer the advantage of avoiding training large over-parameterized architectures prone to overfitting, saving time and computational resources. Training smaller models directly by recursively-stacked neurons allows for efficient training with less data. Unlike other approaches, our model shares neurons with an arbitrary number of weights across layers, enabling large compression without degrading prediction performance. Additionally, networks can be trained with reduced numerical precision of parameters as a complementary method. Approaches to train networks with reduced numerical precision of parameters have been explored, leading to compression with only marginal accuracy degradation. Courbariaux et al. (2016) took this idea further by constraining weights and activations to +1 or -1. Neurons can be seen as arbitrary functions applied stochastically with the same weights. Different functions form a global set of parameters available to the model, including an output layer that is not reused elsewhere in the network. The naive approach to recursion involves repeating whole layers with shared weights, resulting in rectangular-shaped neural architectures. In contrast, the neuron level ACN reuses the most atomic components of a neural network, allowing the same neuron to be reused multiple times within a layer. The neuron level ACN reuses neurons within a layer by randomly sampling connections from the previous layer, with shared trainable parameters but unique connection masks for each neuron. The forward pass through an ACN neuron layer involves projecting the mask indices from previous layers. The input masks can be reproduced on the fly at inference time by storing the seed of the pseudo random generator used for sampling. The ACN can reproduce input masks on the fly during inference, with rare cases of elements not being forwarded. A study on a synthetic curve fitting problem compares ACN to a simple FC network, both with similar parameters. The regression task involves fitting a function with a hyper-parameter grid search for optimal performance. The study compares an ACN to a simple FC network in a synthetic curve fitting problem. ACN achieves better fit in terms of MSE than the FC baseline, even with fewer parameters. The ACN can reproduce input masks on the fly during inference. The ACN outperforms the FC network in fitting non-linear functions with fewer parameters by utilizing function composition. This allows for deeper architectures and improved fitting capability. Performance was evaluated on various datasets, detailed in the appendix. The datasets used in the study are detailed in appendix A.1, selected from the OpenML-CC18 benchmark. The model is compared to six baseline models, including FC, RER, and TensorNet, each with their own unique characteristics and performance evaluations. Details on hyper-parameters and training setup can be found in appendix A.2. The model TensorNet builds on a low-rank quantization method using Tensor-Train format. It outperforms HashedNet by 1.2%. Bayesian compression method BC uses sparsity-inducing priors to prune unimportant neurons. Molchanov et al. use variational dropout to achieve compression and accuracy. In experiments, pruning gates are introduced after batch normalization layers to achieve different levels of network compression based on importance scores. Results of the study on real-world datasets are shown in figure 3, while table 1 displays results for image datasets. Performance metrics and number of parameters are averaged over multiple runs with different random seeds. The ACN architecture consistently outperforms all other models on nine real-world datasets, even for models with up to 10,000 parameters. This demonstrates the efficiency of ACN in terms of parameters and training data required. The ACN architecture shows superior performance on real-world datasets, even with up to 10,000 parameters, indicating its efficiency in terms of parameters and training data required. The recursive nature and parameter sharing throughout the architecture positively impact predictive efficiency compared to the number of parameters. The ACN architecture demonstrates superior performance on real-world datasets, even with up to 10,000 parameters, showcasing its efficiency in terms of parameters and training data needed. When compared to a large optimal FC network, our model achieves compression rates ranging from 88 to over 1100 times. Our model achieves compression rates of 88 up to more than 1100 times with a loss in test accuracy kept at around 0.15% up to 5.33%. ACNs realize higher compression rates than FC with smaller accuracy loss in 5/9 cases. On image datasets, ACN consistently outperforms all other models for up to 100,000 parameters. The ACN model with 4091 parameters outperforms a linear model with 7850 parameters by 1.8% on various datasets. The LayerNet performs best for < 500,000 parameters, while the FC network is optimal for more than 500,000 parameters. Despite lower accuracy on CIFAR10 compared to CNN, dedicated FC networks achieve around 56% accuracy. The model sacrifices processing time for higher compression. The ACN model sacrifices processing time for higher compression. An ablation study compares ACN with parameter-sharing and recursion on a neuron-level with the LayerNet architecture. Results show that recursion on neuron-level is more effective for competitive models with high compression, outperforming LayerNet in most cases. However, LayerNet has a beneficial regularization effect on large models, especially on image datasets. The Atomic Compression Networks (ACN) model achieves high compression rates with minimal loss in accuracy compared to other baselines. Future work includes incorporating skip connections, extending the model to CNNs, and exploring the combination of ACN with NAS methods for further optimization. Additionally, dataset details are provided for datasets selected from the OpenML-CC18 benchmark. The datasets selected for the study had more than 3000 instances and 50 original features. The train/val/test split was 0.64/0.16/0.2 for image datasets and split randomly for others. No additional transformation or data augmentation was applied beyond the original datasets. The datasets included bioresponse, HAR, InternetAds, isolet, nomao, optdigits, and spambase. Given the computational demands of deep learning architectures, a full hyper-parameter search is infeasible for real-world datasets. Instead, specific deep architectures are used with a small parameter grid including learning rate, batch normalization, and dropout probability. Each model type is evaluated with eight different architectures per dataset, varying the number and width of hidden layers. The size of the set of modules M varies between 3 and 5 for LayerNet and between 16 and 512 for ACN. Input dimensions D range from 2 to 128 depending on the maximum width of hidden layers. TensorNet varies tensor-train layers and decomposition length across architectures per dataset. Pruning approaches BC and TP involve training large FC architectures, then pruning in small steps to select the best architecture within each parameter bin. Models are trained for 25 epochs on batches of size 64 for real-world datasets and 128 for image datasets. In order to minimize loss, stochastic gradient descent with Adam is used for training models on real-world datasets and image datasets. The learning rate is halved every 10 epochs, except for Tp where SGD with momentum is used. Gradient norm is truncated to a maximum of 10 to prevent exploding gradients. Models are implemented using PyTorch framework, except for TensorNet which uses Tensorflow. Results are shown in table 5, with additional results in table 6 for sparsification baselines. The additional sparsification baselines in table 6, using L1-regularization and iterative hard thresholding, perform similarly to the small FC baseline. They outperform FC and ACN on the splice dataset in bins with the highest parameter count. These baselines were trained and evaluated following the same protocol as other models."
}