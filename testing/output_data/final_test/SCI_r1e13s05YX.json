{
    "title": "r1e13s05YX",
    "content": "Deep neural networks are effective at approximating complex functions through training with data and gradient descent. A proposed method integrates calls to existing black-box functions into the training of a base neural network by approximating their functionality with a differentiable neural network. This drives the base network to comply with the black-box function interface during optimization, allowing for seamless integration of black-box functions at inference time. The proposed method integrates black-box functions into training by replacing differentiable estimators with non-differentiable counterparts. This \"Estimate and Replace\" paradigm allows for end-to-end training of a neural network to compute black-box inputs without intermediate labels. The integrated model generalizes better and learns more efficiently than fully differentiable models or RL-based methods. DNNs can be decomposed into functions, some learned and some precise. This approach, like in Semantic Parsing and Question Answering tasks, can lead to superior solutions. Matching network outputs to existing functions' inputs is crucial. These existing functions are referred to as black-box functions (bbf). For example, decomposing a number comparison task into two steps can leverage the solved problem in symbolic computation. The proposed approach, called Estimate and Replace, involves using a black-box estimator to approximate a non-differentiable black-box function during training. This allows trainable components to be optimized using gradient-based methods without the need for intermediate labels. The black-box function is used as an oracle to train the estimator, which is then replaced with the original function during inference. In this work, the challenges of training a solution with trainable components and black-box functions are addressed. The lack of prior knowledge on input distribution and dealing with near-perfect function approximation are key issues. The problem formulation, network architecture, training procedures, experiments, and benefits of the modular approach are discussed in different sections. In this work, the problem of training a DNN model to interact with black-box functions is addressed. The goal is to fit a target function given a dataset and oracle access to the black-box function. An argument extractor function is used to extract arguments from the input domain, which are then passed to the black-box function to produce the final result. The Estimate and Replace approach presented in this section enables training a DNN to interact with non-differentiable black-box functions. The model, EstiNet, consists of an argument extractor and black-box estimator modules that learn to extract arguments and estimate the black-box function, respectively. The black-box estimator serves as a differential estimator during optimization, fitting the black-box functionality directly. EstiNet is a model that fits the black-box functionality directly by using the black-box function as a label generator during training. It eliminates the need for intermediate labels and shows better generalization than an end-to-end neural network model. EstiNet uses a modular architecture with module reuse and model interpretability. Adapters are used to adapt the argument extractor's output to the black-box function input and the black-box function's output to the final output label. EstiNet is a modular architecture with two modules: the argument extractor and the black-box estimator. Each module is trained separately using its own input-label pair samples and loss function. The model is optimized with two distinct loss functions - the target loss and the black-box loss. The argument extractor is optimized with the target loss using the task's dataset, while the black-box estimator is optimized with the black-box loss using the black-box dataset. The black-box dataset is generated by sending input samples to the black-box function and recording the output as labels. Two ways of generating input samples are discussed: offline sampling and online sampling. Multiple training procedure options are available due to having two independent datasets and loss functions. Offline training involves training the black-box estimator using offline sampling. In offline training, the black-box estimator is first trained using offline sampling. Its parameters are then fixed and loaded into the EstiNet model to train the argument extractor with the task's dataset and target loss function. However, offline training can lead to noisy training due to distribution differences between the offline black-box dataset and the actual inputs received by the argument extractor during training. On the other hand, online training aims to address this distribution difference problem by jointly training the argument extractor and black-box estimator using target loss and black-box loss, respectively. The black-box estimator is trained with the black-box dataset generated via online sampling. In hybrid training, the black-box estimator is initially trained offline but not frozen, then loaded into the EstiNet model for parallel training with the argument extractor. This approach aims to solve the cold start problem by using a black-box dataset for training via back-propagation, replacing the need for intermediate labels. The black-box estimator in hybrid training can hinder the argument extractor's learning by becoming overly confident, leading to small gradient updates and complicating training. This issue is different from Multi-Task Learning. In hybrid training, the black-box estimator's confidence can hinder the argument extractor's learning. To address this, Entropy Loss and Label Smoothing Regularization are used to encourage less confident distributions. The text discusses the validation of a proposed solution for improving generalization and learning efficiency in machine learning models. Four experiments are presented to test the approach, showing better performance without using intermediate labels. The experiments demonstrate the ability of the method to learn a decomposition solution and perform well with less training data. The EstiNet model is designed to answer greater-than/less-than logical questions on real numbers by extracting arguments and operators, and using a black-box estimator for logic operations. Performance comparison with a baseline model shows the effectiveness of the EstiNet model in learning the task decomposition and black-box function interface. The EstiNet model generalizes better than the baseline in online training. The accuracy difference increases as training data decreases. The Estimate and Replace approach requires only 5% of the data for 0.97 accuracy. The second experiment compares the approach to an Actor Critic-based RL algorithm on the Image-Addition task using MNIST images. The NALU cell proposed by BID26 shows better generalization in solving the task. EstiNet model outperforms previous solutions in solving the Image-Addition task by using an argument extractor layer and a black-box estimator. The argument extractor classifies MNIST images without intermediate labels, while the black-box estimator utilizes LSTM and NALU cell to output a regression number. The model must generalize to unseen sequence lengths for accurate results. Comparing EstiNet with an end-to-end NALU model shows superior performance. The EstiNet model achieves high accuracy in classifying MNIST images without intermediate labels, showcasing its ability to generalize to any sequence length. It outperforms a plain NALU-based end-to-end network and significantly surpasses an AC-based RL agent in learning efficiency. The EstiNet model successfully solves a non-differentiable image-lookup task by replacing the black-box estimator with the original black-box function. The task involves a k-dimensional lookup table and MNIST images with corresponding digits. Results show the model's ability to generalize to the black-box function. The EstiNet model demonstrates the ability to generalize to the black-box function by replacing the lookup table at inference time without retraining. Results show accuracy for different model configurations on the MNIST test-set, with a decrease in performance when replacing the lookup table at test mode. The EstiNet model shows high performance in inference mode by replacing the black-box estimator with an unseen function. The Image-Lookup task validates the need for confidence regularization. Experimental results demonstrate the correlation between over-confidence and small gradients. The Estimate and Replace approach is applied to a challenging task combining logic and lookup operations, showcasing generalization ability on database tables. Different training modes are compared in this experiment. The experiment compares offline, online, and hybrid training modes using a table-based question answering dataset. An EstiNet model is constructed with an argument extractor layer to perform logical operations on relevant columns. Results show that offline training leads to low model accuracy and inference performance. The experiment compares offline, online, and hybrid training modes using a table-based question answering dataset. Fixing estimator parameters during end-to-end training may hinder model fitting to the training set. Online training led to significant improvement in inference performance, with hybrid training further enhancing performance. End-to-end learning requires large datasets and works well for specific tasks like neural machine translation. Other architectures like Neural Turing Machine and Neural Programmer allow end-to-end training for multiple tasks. Neural networks like BID19 and BID26 use differentiable components for specific operations, but may lack accuracy and scalability. Program induction involves constructing a program based on input for interaction with black-box functions. Semantic parsing can transform natural language queries into logical forms for program execution. Recent works in program induction, such as BID20, focus on training with query-answer pairs instead of query-program pairs. Other approaches include neural network-based program translation (BID16) and learning programs from execution traces (BID22, BID4). Reinforcement learning is used to select actions for executing operations, with works like BID30 proposing extensions to Neural Turing Machines. Recent works in program induction, such as BID20, focus on training with query-answer pairs instead of query-program pairs. Reinforcement learning is used to select actions for executing operations, with works like BID30 proposing extensions to Neural Turing Machines. To overcome the difficulty of discrete selections necessary for interfacing with an external function, the Estimate and Replace approach substitutes the gradient with an estimate using RL, achieving state-of-the-art results in Semantic Parsing and Question Answering. This approach provides immediate interpretability benefits by supporting model composability, allowing for the division of the model into components that can be interpreted individually. Model composability contributes to interpretability by allowing a natural language processing model to interface with a WordNet service for additional features. This interaction provides insight into the model's decision-making process. Additionally, composability enables reusability of well-defined module functionality, overcoming limitations of black-box neural network predictions. EstiNet architecture and Estimate and Replace process aim to improve learning in neural network prediction by addressing limitations like poor generalization, low efficiency, under-utilization of optimal functions, and the need for intermediate labels. The two-step approach involves estimating the black-box functionality for training and then replacing it with the concrete function during inference. This allows for end-to-end training using gradient-based optimization and generated labels. The network model uses gradient-based optimization with labels generated from the black-box function during training to improve stability and reduce sample complexity compared to policy gradient methods. Leveraging the black-box function at inference time leads to better generalization than end-to-end neural network models. The approach involves a modular neural network for added interpretability and reusability benefits, with future work focusing on tasks solvable with a single black-box function. Training a neural network to interact with black-box functions is crucial for advancing artificial intelligence. This involves computing final predictions over a set of black-box functions, requiring an additional network output module. The ability to extract the right information from the right knowledge source is key to AI progress. The Image-Addition and Image-Lookup tasks utilize the MNIST dataset. The implementation of Image-Addition and Image-Lookup tasks in AI involves using the MNIST dataset. The tasks utilize a black-box function that performs a sum operation on sequences of digits. The architecture includes convolutional layers, max-pooling, and a fully-connected layer implemented in PyTorch. The argument extractors share parameters and output MNIST classifications for each image in the sequence, while the sum estimator is an LSTM. The architecture for the Image-Addition and Image-Lookup tasks in AI involves using the MNIST dataset. The sum estimator is an LSTM network followed by a NALU cell, while the lookup estimator consists of fully-connected layers with ReLU activations. Training includes a hybrid procedure with confidence regularization to stabilize learning. Target losses are cross-entropy and squared distance for lookup and addition, respectively. The LSR loss involves H, p, q, y, and y for MNIST regressions. The loss includes online and threshold entropy components. The RL environment uses fixed-length episodes with MDP modeling MNIST dataset samples as states and agent actions for rewards based on label errors. The agent model used A3C as the learning algorithm with two convolutional layers and fully connected layers. The experiments were implemented in TensorFlow on synthetic datasets for Text-Logic and Text-Lookup-Logic tasks. The TLL task involved generating a table-based question answering dataset. The TLL dataset involves a table-based question answering task where a DNN extracts parameters from a query to perform non-differentiable logic on a table column. For example, to answer a query about countries with more than 7 gold medals, the DNN extracts the argument, accesses the gold medals column, and executes the greater than operation to find the relevant countries. The TLL dataset involves a table-based question answering task where a DNN extracts parameters from a query to perform non-differentiable logic on a table column. The black-box function interface for solving the TLL task requires basic logic functions like equalto, less-than, greater-than, max, and min. Each function defines an API with inputs and outputs to select rows based on specific queries in the table. The TLL data consists of tables with column names and entities like countries, teams, or products, with quantitative properties in subsequent columns. The TLL dataset involves generating tables with entity names and values using random selection and uniform distribution. Two sets of questions were created using 5 functions, with 20,000 train questions and 4,000 test questions. Input representations include words, numbers, queries, and tables. Word pieces are used to represent words, and exact numerical values are crucial for decision-making. The text discusses the representation of numbers and queries in the EstiNet model using float32 scheme BID12 and LSTM model BID9. The approach involves representing numbers as Boolean vectors and queries as matrices of word embeddings encoded into vector representations. The EstiNet TLL model utilizes three types of argument extractors in its architecture. The EstiNet TLL model utilizes three types of argument extractors in its architecture: operation, argument, and column selectors. These selectors are implemented as classifiers using a simple, fully connected network. The predicted class matrix C is used to make selector predictions. The EstiNet TLL model uses selectors to make predictions based on a class matrix C. The selectors transform soft selections into hard selections using Gumbel Softmax. Five estimators are used, each implemented with a transformer network encoder BID27, to estimate logic operations. Each estimator consists of multiple layers with attention heads and feed forward networks. The EstiNet TLL model utilizes selectors and layer normalization, followed by linear transformation and Gumbel Softmax. The task involves selecting tokens from a sentence with greater-than or less-than questions. The architecture includes two selectors for floating numbers and a classification for greater-than or less-than comparisons."
}