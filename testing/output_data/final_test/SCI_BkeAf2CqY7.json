{
    "title": "BkeAf2CqY7",
    "content": "Federated learning is gaining attention as an emerging field with strict constraints on model efficiency and communication costs. A new framework based on variational dropout is proposed to learn a sparse model and reduce gradient exchanges during training, showing superior performance in model compression and communication. Federated Learning is a machine learning approach gaining attention for its applications in mobile scenarios. It allows geographically distributed devices to collaboratively learn a shared model without sharing private data. This approach uses distributed stochastic gradient descent and a parameter server to coordinate the training process. Federated learning involves devices computing gradients locally, aggregating them on a server, and updating model parameters collectively. It differs from distributed learning in datacenters due to constraints on model efficiency and communication for mobile devices. In federated learning, communication constraints on mobile devices require reducing communication costs. A proposed framework sparsifies model parameters and gradients exchanged between server and devices to accelerate training. The framework proposed for federated learning involves sparsifying model parameters and gradients to reduce communication costs and improve computational efficiency on mobile devices. Experiment results demonstrate significant model compression and communication reduction without accuracy loss. Our work introduces a federated learning framework that achieves model sparsification and communication reduction in a unified manner, addressing the statistical challenge in a mobile setting. It is related to gradient compression in distributed SGD, with approaches like TernGrad and QSGD focusing on reducing gradient communication overhead. In BID2, QSGD allows dynamic trade-off between accuracy and communication cost by choosing gradient bit width. Previous works aimed to reduce communication cost by transmitting only important gradients, such as using structured updates or magnitudes as indicators. Our work introduces a dropout parameter as an importance indicator for stochastic gradient transmission. Additionally, our work is related to model sparsification, aiming to reduce computational intensity by pruning redundant model parameters. The authors in BID5 developed a parameter pruning method to sparsify deep neural networks without losing prediction accuracy. Their approach integrates model sparsification and communication reduction in a single optimization framework based on distributed training data. Inspired by Bayesian neural networks, their method considers uncertainty in weight estimates to be more robust to overfitting. Variational inference approximates the posterior distribution of w using a parametric distribution q \u03c6 (w). Variational Dropout is an effective regularization method for deep neural networks. The proposed efficient federated learning framework is based on unified gradient and model sparsification. Gradients transmitted from each device to the central server are sparse, and the shared model learned from distributed private data is sparse. Gaussian noises are added to the weights with a fixed dropout rate. Dropout is connected with Bayesian neural networks, allowing for a different dropout rate to be learned for each individual weight. Each weight has a Gaussian distribution parameterized by variance and enforced to be greater than zero. The first term in the approximation can be done by Monte Carlo. The authors propose using variational dropout to sparsify Bayesian neural networks by pruning weights with high dropout rates, maintaining predictive accuracy. This is achieved by approximating terms using Monte Carlo estimation and a prior distribution, leading to a constant term with a sigmoid function. The authors propose using variational dropout to sparsify neural networks in federated learning, inspired by the need to prune weights with high noise levels. They describe a standard federated learning framework with synchronous SGD and a parameter server coordinating gradients from geographically distributed devices. The proposed method uses variational dropout to sparsify neural networks in federated learning. It involves aggregating gradients from decentralized devices and updating local models with averaged gradients. The proposed federated learning framework utilizes variational dropout to sparsify neural networks. It aims to have all N devices collectively learn a sparse model while reducing communication costs. The server initializes a base model and learning rate, distributing them to all devices before training begins. The federated learning framework utilizes variational dropout to sparsify neural networks. All N devices start with the same initial parameters. During each iteration, devices select mini-batches from their local datasets and compute gradients. Communication overhead is reduced by using dropout parameters to identify important information. The federated learning framework uses variational dropout to sparsify neural networks. Dropout parameters are utilized to reduce communication overhead by identifying important gradients for transmission to the server. Dropout rates correspond to noise in weights, with higher dropout parameters indicating less important model parameters. Each device can identify important gradients using a (0,1)-matrix based on dropout parameters. The federated learning framework utilizes variational dropout to sparsify neural networks. Dropout parameters help reduce communication overhead by selecting important gradients for transmission to the server. The (0,1)-matrix M(\u03b1i) becomes highly sparse during training, leading to sparse gradients (g\u03b8i, g\u03b1i). This sparsity reduces the total number of exchanged gradients. The server averages the uploaded gradients (\u1e21\u03b8, \u1e21\u03b1) and sends them back to the devices for updating the model parameters with a preset learning rate \u03b7. In experiments with federated learning, it was observed that \u03b1 is negatively correlated with \u03b8. Variants of the algorithm were created to reduce communication costs by optimizing \u03b1 locally on each device without uploading or downloading its gradients. This strategy leverages the synchronization of \u03b8 during SGD iterations to ensure consistency across devices. Only important model parameters are uploaded, while dropout parameters are omitted. In experiments with federated learning, variants of the algorithm were created to reduce communication costs by optimizing \u03b1 locally on each device without uploading or downloading its gradients. Dropout parameters are omitted in the implementation. The experimental results on Algorithm 1 are shown in the supplementary material, while the results on the algorithm variants are presented in the experimental section. The performance of the framework was evaluated on three deep neural networks and five datasets fitting the federated learning setting. The standard distributed learning framework was used as the baseline, and experiments were conducted on NVIDIA 1080Ti GPUs. In experiments with federated learning, the framework was evaluated on three deep neural networks and five datasets. The experimental settings were consistent for the baseline and proposed framework, with a mini-batch size of 128 and Adam BID8 as the optimizer. Each dataset was divided into N parts for N devices, emulating real-world scenarios. Pre-training on ImageNet BID4 was done before training on the datasets. Both baseline and proposed framework were run to full convergence for fair comparison. Top-1 test accuracy and model sparsity were examined with 4 devices. TAB1 shows the accuracy achieved by both frameworks. Our framework achieved 1.4% to 7.2% model sparsity with comparable accuracy to non-sparse models across different datasets and models. The framework also reduced communication costs during training, taking more epochs to converge compared to the baseline. Our framework achieves high model sparsity with comparable accuracy, reducing communication costs during training. It takes more epochs to converge compared to the baseline due to the larger variance in sending sparse gradients. The efficiency of our framework is demonstrated by the quick drop in transmitted gradients at the beginning of training, showing the ability to learn sparse models in a federated setting. The key question is whether the communication cost reduction offsets the extra epochs required for convergence. Our approach reduces communication costs by 12.2% to 22.2% compared to the non-sparse framework, offsetting the overhead caused by extra epochs. This success is attributed to the fast sparsification speed of our approach, as shown in FIG1. In experiments, the framework shows scalability to 8 and 16 devices without accuracy loss. It achieves 3.3% to 7.7% non-zero weights and reduces communication costs to 14.7% to 22.2%. Our framework demonstrates scalability to 16 devices with 4.4% to 8.4% non-zero weights and communication cost reductions to 16.1% to 26.3% compared to non-sparse models. The percentage of non-zero weights and communication reduction slightly increase as the number of devices increases due to data distribution challenges. The proposed framework shows scalability to 16 devices with reduced non-zero weights and communication costs compared to non-sparse models. However, using random initialization weights leads to decreased accuracy and challenges in Bayesian dropout training. The paper presents a federated learning framework based on variational dropout for efficiently learning deep neural networks from distributed data. Experiments show high model sparsity with 2.9% to 7.2% non-zero weights and reduced communication costs from 12.0% to 18.2% with no accuracy loss. The framework scales well with an increasing number of devices and future work includes examining performance in asynchronous distributed SGD setting. In this section, the performance of Algorithm 1 is compared with different settings, including using pretrained weights from Imagenet, training from random initialization weights, and sharing gradients of \u03b1 using pretrained weights. The accuracy comparison for 4, 8, and 16 devices is summarized in TAB6 and TAB7. The goal is to further reduce communication costs by not sharing \u03b1 in the experimental setting. The accuracy comparison between using pretrained weights and random initialization weights shows that sharing \u03b1 does not affect accuracy when using pretrained weights. However, when the model is initialized with random weights, there is a 2-3% drop in accuracy. The percentage of non-zero weights is higher when sharing \u03b1 with pretrained weights, indicating a more sparse model after training. The total communication percentage of three settings is compared in FIG4. Sharing \u03b1 during training doubles the communication cost compared to not sharing. Training from random initialized weights results in the lowest communication cost due to fewer non-zero weights."
}