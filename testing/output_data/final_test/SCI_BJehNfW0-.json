{
    "title": "BJehNfW0-",
    "content": "The foundational paper by Goodfellow et al. (2014) suggested that GANs can learn the target distribution with large deep nets, sample size, and computation time. However, a recent theoretical analysis by Arora et al. (2017) raised doubts about this when the discriminator has bounded size, showing that mode collapse can occur. The current paper introduces a novel test using the birthday paradox to estimate support size, revealing that well-known GANs approaches can learn distributions with low support. Additionally, it explores encoder-decoder GAN architectures like BiGAN/ALI for learning more meaningful features. The current paper discusses the limitations of encoder-decoder training objectives in addressing mode collapse in Generative Adversarial Networks (GANs). It questions whether GANs can truly learn the target distribution they are trained with and highlights the challenges in preventing the learning of meaningless codes for data. The paper also mentions the traditional approach of training a generator deep net to produce realistic outputs and a discriminator deep net to provide feedback for improvement. The paper discusses the limitations of encoder-decoder training objectives in addressing mode collapse in GANs and questions if GANs can truly learn the target distribution. Standard analysis shows that with large generator and discriminator, training can succeed in learning the distribution closely. However, GANs do not provide an estimate of distributional fit like perplexity score, leading researchers to use qualitative tests to evaluate performance and prevent memorization of training data. The GAN training mode is tested for memorization by checking image similarity to training set, generating interpolating images, and examining meaningful directions in latent space. Tests like log-likelihoods and Annealed Importance Sampling reveal distribution mismatch. BID8 proposed a method to balance sample quality and diversity in GANs, but lacks a clear definition of sample diversity. Recent theoretical analysis suggests that GAN training can approach optimality even if the generator hasn't fully learned the distribution. The target distribution, like all possible human faces, has a large support set, while the training objective can be close to optimal with a smaller discriminator size. The paper discusses the possibility that the discriminator in GANs may struggle to distinguish a diverse target distribution from a trained distribution with small support. This failure mode differs from the generator memorizing training samples. The analysis suggests that real-life GANs training may avoid such issues due to unknown properties of SGD or hyper-parameter choices. Further experimental investigation is needed. The current paper introduces a new test for the support size of trained distributions in GANs, revealing mode collapse issues in popular training methods. It also proves limitations of encoder-decoder frameworks like BiGAN and ALI, which require learning both inference and generative mechanisms. The paper introduces a new test for support size in GANs, revealing mode collapse issues in training methods. It also highlights limitations of encoder-decoder frameworks like BiGAN and ALI in learning meaningful codes/features. The proposed birthday paradox test for GANs suggests that batches with duplicate images indicate a small support size. However, the test may not be definitive due to nonuniform distributions. This highlights a potential failure mode in GANs training. Theorems 1 and 2 provide insights on probability distributions and collisions in sampling. The distribution's support size can be inferred from consistent collisions in batches, indicating a major component with limited support size. This is crucial for accurate sampling. In the GAN setting, accurately estimating the support size of a distribution with n modes requires \u2126(n/ log n) samples, making it impractical for human examination. Despite the infinite support size in continuous distributions, the birthday paradox test still works for near-duplicates. By selecting the 20 closest pairs in a finite sample and visually identifying potential duplicates, the test was conducted on CelebA and CIFAR-10 datasets. For faces, Euclidean distance in pixel space is used as a similarity measure, while for CIFAR-10, a discriminative CNN is pre-trained for classification and the top layer representation is used as an embedding. The Euclidean distance in the embedding space is then used as a similarity measure. These metrics provide upper bounds on the support size of the distribution. Some GANs and other methods apply noise to training and generated images, useful for computing perplexity scores. Noised images are blurry and the birthday paradox test is not effective for them. Experiments work best with GANs generating sharper, realistic images. The birthday paradox test is done with Euclidean distance as the similarity measure, aiming for faces that look like doppelgangers. The experiments with GANs aim to generate realistic images that resemble doppelgangers. DCGAN and MIX+DCGAN models show a high probability of producing duplicate images in a batch of 800 samples. The support size of the distribution is estimated to be around 640,000, similar to the size of the training set. The experiments with GANs show that ALI (or BiGANs) have a more diverse distribution compared to DCGAN and MIX+DCGAN models. Collisions occur with 50% probability with a batch size of 1200, suggesting a support size of a million. This supports the idea that the bidirectional structure in ALI prevents mode collapse. The support size could be near-linear in the capacity of the discriminator, allowing for a small support distribution to fool the best discriminator. Further investigation is needed to understand how generator nets exploit this training \"loophole\" in real-life scenarios. In a test with discriminator size, DCGANs were built with larger discriminators while keeping other parameters fixed. The discriminator had 5 layers with increasing output channels. Results suggest that diversity of the learnt distribution grows near-linearly with discriminator size. On CIFAR-10, Euclidean distance in pixel space is not informative, so a classifying CNN was used. The text discusses the use of a Stacked GAN model for generating high-quality images for a similarity test. It highlights the impact of training with noisy samples on the test results and compares the performance of DCGAN and Stacked GAN models in generating realistic images. The Stacked GAN model, trained by conditioning on class labels, is found to produce the most real-looking images with high diversity within each class. The batch sizes needed for duplicates are shown in Table 1. Duplicate samples are visually inspected for similarity to training images, revealing lack of diversity in GANs. Adversarial Feature Learning setup involves a generator G and an encoder E. The generator G and encoder E in the BiGAN setup aim to align the distributions of latent variables and data samples. The goal is for G(z) and E(x) to be jointly distributed as p(z, x), the true generator and encoder distributions. The BiGAN objective is formulated using min-max adversarial training. The BiGAN setup involves aligning distributions of latent variables and data samples using a generator G and encoder E. The objective is formulated through min-max adversarial training, with key components including empirical distribution, random seeds, and a concave measuring function. The image distribution \u00b5 is assumed to consist of noised images, while the seed distribution \u03bd is typically a spherical zero-mean Gaussian. In the context of BiGAN setup, the generator G and encoder E align distributions of latent variables and data samples through min-max adversarial training. The discriminator is assumed to be L-lipschitz, with the support size of the generator's distribution depending on its capacity. The theorem states that for certain discriminators, the encoder E has low complexity and extracts noise from input x, while the generator G has a small-support distribution. The BiGAN setup involves aligning distributions of latent variables and data samples through min-max adversarial training. The discriminator's capacity affects the support size of the generator's distribution. The argument shows that the discriminator cannot distinguish between a generator sampling from \u00b5 and one memorizing a subset of random images in \u00b5. The encoder's role in the noise model is crucial, defining the distribution of noised images \u00b5. The main idea is to prove the existence of the generator/encoder pair through a probabilistic construction. The encoder extracts noise from the noised image, while the generator produces a distribution of support size m. The generator is designed to sample from the image distribution and map a sample z to a corresponding image x. The set of samples x*i is random, leading to the generator's output being random as well. The generator G(z) produces a distribution over generators, with one satisfying Theorem 3. It can be implemented using a ReLU network of complexity O(md). The proof involves non-colliding sets of samples from \u03bd, ensuring the encoder matches the expectation of \u03c6(D(x, E(x))) to fool the discriminator. The concentration of E G E z\u223c\u03bd \u03c6(G(z), z) around this expectation is crucial. The concentration argument in proving the generator's effectiveness involves calculating the expectation of a function over non-colliding sets of samples. By considering the randomness in both the generator and the sets, we can analyze the concentration of the function. The paper discusses the concentration of the generator's effectiveness in terms of T and G, using Markov's inequality to show that most encoders satisfy a certain condition. It highlights gaps in current GAN research, particularly in addressing mode collapse and the existence of bad solutions in the optimization landscape. The new Birthday Paradox test provides a benchmark for testing the diversity of images in a distribution, revealing that current GAN approaches suffer from mode collapse. Experiments suggest that the size of the distribution's support scales with discriminator capacity, leading researchers to consider GANs more for feature learning than distribution learning. The encoder-decoder GAN architectures aim to improve diversity by encouraging the generator to use meaningful encodings of images. However, these architectures still face challenges such as mode collapse and the inability to guarantee meaningful codes. Theorems 1 and 2 are proven in this section, showing the probability of collisions among samples and defining collision time in a discrete distribution. The text discusses estimating the uniformity of a distribution P using collision time and a surrogate for uniformity. It mentions using DCGANs on a dataset to extract collision candidates due to training constraints. The text discusses the uniformity of a distribution using collision time and a surrogate for uniformity. It mentions using CNN classifier on a single-category dataset to identify corrupted samples with noise patterns. The distribution puts significant probability on noise patterns, implying under-fitting. 43 out of 900 generated images had fixed noise patterns, indicating a 5% probability. The text questions the diversity of distributions learned by earlier methods like VAEs. The text discusses the diversity of distributions learned by VAEs and introduces a method using feedback from a discriminator for training the generator net. It mentions challenges in running the birthday paradox test due to unrealistic VAE samples. The notation for image and code distributions is introduced for further analysis. The text introduces a method using feedback from a discriminator for training the generator net. It discusses building a LL \u03c6-net for discriminators and bounding their size. Theorem 3 is mentioned along with a finite-sample version in Corollary D.1. The text discusses reducing expectations over non-colliding sets and applying McDiarmid's inequality. It also mentions building a LL \u03c6-net for discriminators and bounding their size. The text discusses reducing expectations over non-colliding sets and applying McDiarmid's inequality. It also mentions building a LL \u03c6-net for discriminators and bounding their size. Over the points in the net, Pr T,G (\u2203D, |R D,T,G | \u2265 /2) \u2264 exp(\u2212\u2126(p log(\u2206/ ))."
}