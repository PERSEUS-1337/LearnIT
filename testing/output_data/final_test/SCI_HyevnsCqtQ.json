{
    "title": "HyevnsCqtQ",
    "content": "With the rapid growth of deep neural networks (DNNs), research on network model compression like weight pruning has been extensive. This work introduces Integral Pruning (IP) technique, integrating activation pruning with weight pruning to improve execution efficiency. IPnet, the resulting network, balances sparsity between activations and weights, saving 71.1% to 96.35% of computation cost with minimal impact on testing accuracy. Deep neural networks (DNNs) have shown advantages in various applications, leading to demands in data storage and processing. DNN pruning, including zeroing-out redundant weight parameters, is explored to reduce model size while maintaining performance quality. Zero-skipping technique on sparse weight parameters can further save computation costs. Many DNN accelerator designs leverage these techniques. The approach of leveraging the zero-activation pattern of ReLU for activation sparsity in DNN accelerator designs cannot be extended to other activation functions like leaky ReLU. Weight or activation pruning alone cannot optimize inference speed due to the need to minimize computation costs, especially in convolution layers which dominate DNN inference time. In this work, the integral pruning (IP) technique is proposed to minimize the computation cost of DNNs by pruning both weights and activations. The experiment shows that the zero-activation percentage decreases after weight pruning, potentially affecting accelerator designs. IP learns dynamic activation masks by attaching activation pruning to weight pruning. The proposed IPnet technique balances sparsity between activations and weights, improving execution efficiency. It can achieve up to 5.8\u00d7 activation compression rate, 10\u00d7 weight compression rate, and eliminate 71.1% \u223c 96.35% of MACs. Compared to BID4, IPnet can further reduce computation cost by 1.2\u00d7 \u223c 2.7\u00d7. Weight pruning is an effective compression technique for reducing model size and computation cost in neural networks. It involves pruning redundant weights by adding a regularization term to the loss function and then pruning weights below a certain threshold. Finetuning is done to recover accuracy loss, and the process can be iteratively carried out to find the optimal balance between model compression and accuracy. This approach is highly effective, especially for fully-connected layers. Weight pruning is a compression technique in neural networks that removes redundant weights from conv layers. Techniques like group Lasso regularization and filter ranking are used to eliminate redundant groups and filters. Activation sparsity is also utilized in DNN accelerator designs to reduce off-chip memory access and computation cost. Techniques like zeroing out small activations are explored to improve activation sparsity. The curr_chunk discusses methods to regulate and stretch activation sparsity in neural networks, including adaptive dropout and winners-take-all autoencoder techniques. These methods aim to improve regularization in deep neural networks by focusing on activation magnitude. The proposed IP integrates weight pruning and activation sparsification to reduce DNN computation cost and accelerate inference. It involves two steps - activation pruning concatenated with weight pruning to mask off unimportant information. The goal is to keep only important connections and activations to minimize computation cost. Technical details in model quality control will be explained, focusing on accuracy. The weight pruning stage involves masking out weight parameters below a threshold and updating weight masks for finetuning. Techniques include determining thresholds based on weight distribution and focusing on specific layers vulnerable to pruning. Multiple rounds of finetuning are needed for optimal results. The weight pruning stage involves multiple rounds of pruning-finetuning recursions to search for optimal weight sparsity. Activation pruning focuses on masking out activations with small magnitude to reduce computation cost. Dynamic masks are learned in the activation pruning stage to select winners based on different input classes. The winner rate is defined as the number of winners divided by the total activation number. Activation pruning involves determining the winner rate per layer through analysis of activation pruning sensitivity. The model with dynamic activation masks is finetuned to recover accuracy drop without iterative mask updating. Deeper layers tolerate larger activation pruning strength, and not all layers share the same winner rate. Testing on a validation set is done to analyze activation pruning sensitivity. The dynamic activation pruning method increases activation sparsity while maintaining model accuracy. The solution for determining threshold \u03b8 in Equation (2) for activation masks involves a canonical argpartition problem to find top-k arguments in an array. This can be solved in linear time O(N) through recursive algorithms, where N is the number of elements to be partitioned. To speed up algorithms, threshold prediction can be applied on the down-sampled activation set by selecting top-\u03b1k elements. For DNN training, a dropout layer is commonly added after large fc layers to prevent over-fitting. Neuron activations are randomly chosen in the feed-forward phase, and weight updates are only applied to selected neurons in the back-propagation phase, updating a random partition of weight parameters in each training iteration. Although the activation mask selects a small portion of activated neurons, a dropout layer is still necessary to prevent over-fitting. In the weight pruning stage, it is recommended to use the same optimizer as the original model with a reduced learning rate. For activation pruning, Adadelta is usually preferred. The dropout layer is suggested to be modified with a specific setting to regulate dropout strength. Different optimizer requirements are found for weight pruning and activation pruning. During activation pruning, Adadelta is recommended for sparse weight updates by adapting the learning rate for each weight parameter. Finetuning involves updating only a small portion of weight parameters with a reduced learning rate. The models are implemented in TensorFlow and verified on various datasets using IPnets. The compression results of IPnets on activations, weights, and MACs are summarized in TAB0 compared to the original dense models. IPnets achieve a 2.3\u00d7 \u223c 5.8\u00d7 activation compression rate and a 2.5\u00d7 \u223c 10\u00d7 weight compression rate, requiring only 3.65% \u223c 28.9% of MACs in dense models. The accuracy drop is minimal, and in some cases, IPnets achieve better accuracy than dense models. Our method can learn sparser activations and weights, saving computation and outperforming other approaches. The ReLU function brings intrinsic zero activations for MLP-3, ConvNet-5, and AlexNet in experiments. Non-zero activation percentage increases in weight-pruned models, undermining weight pruning efforts. Activation pruning can remedy activation sparsity loss and prune more activations compared to dense models. IPnets reduce more MACs compared to WP models, showing improvement in compression rates. The model configuration details for MLP-3 on MNIST are discussed, with two hidden layers having 300 and 100 neurons. The amount of MACs is calculated with a batch size of 1, and the non-zero activation percentage is averaged from random samples. The model size is compressed through weight pruning and further reduced with IP, maintaining high accuracy. IP is also applied to a 5-layers CNN on CIFAR-10, showing promising results in reducing MACs. The ConvNet-5 model on CIFAR-10 achieved 86% accuracy with two conv layers and three fc layers. By applying IPnet, 59.6% of weights and 56.4% of activations were pruned, reducing MACs by 27.7% with only a 0.06% drop in accuracy. The computation bottleneck is in conv layers, similar to AlexNet on ImageNet, where IP was also applied. AlexNet achieved 57.22% top-1 accuracy on the validation set. The computation bottleneck in AlexNet, similar to ConvNet-5, is in conv layers, consuming over 90% of total MACs. Deeper layers show more pruning strength on weights and activations, with conv5 reducing MACs by 10\u00d7. ResNet-32 consists of 1 conv layer, 3 residual units, and 1 fc layer, with rapid increases in filter numbers and weight amounts. The last fc layer can be disregarded in terms of weight volume and computation cost. The original model achieves 95.01% accuracy on CIFAR-10 with 7.34G MACs per image. Weight and activation pruning reduce hyperparameter exploration space. IP reduces activation percentage to 38.6%. Model size compressed 3.1\u00d7 with 86.3% MACs avoided and <0.5% accuracy drop. Activation distribution shifts towards zero after IP. The activation distribution after IP shows activations near zero being pruned out, with a focus on removing small negative values. Kept activations are trained to be stronger with larger magnitude. Static activation pruning is widely used in efficient DNN accelerator designs. The threshold for activation pruning in IP is dynamically set based on winner rate and activation distribution layer-wise. Comparison between static and dynamic pruning is done on ResNet-32 for CIFAR-10 dataset. Static pruning setup assigns a threshold in the range of [0.07, 0.14] for leaky ReLU. In static pruning setup, leaky ReLU is assigned a threshold in the range of [0.07, 0.14], affecting activation sparsity patterns. Dynamic threshold settings based on winner rates can improve accuracy under the same sparsity constraint. Finetuning with dynamic activation masks can recover accuracy loss, as shown in experiments. Weight pruning strength varies per layer, requiring sensitivity analysis for proper activation pruning strength layer-wise. The proper activation pruning strength layer-wise is crucial for maintaining accuracy in models like AlexNet and ResNet-32. Deeper conv layers can handle sparser activations, with layer conv1 being most sensitive to pruning. Finetuning with dynamic activation masks can help recover accuracy loss. Threshold prediction on down-sampled activation sets can accelerate the process of selecting activation winners. In this paper, IP combining weight pruning and activation pruning is proposed to minimize computation cost in DNNs. Experimental results show significant reductions in computation cost for various models on MNIST, CIFAR-10, and ImageNet datasets. The proposed method achieves a 2.3\u00d7 -5.8\u00d7 activation compression rate and a 2.5\u00d7 -10\u00d7 weight compression rate, outperforming weight pruning by 1.2\u00d7 -2.7\u00d7. IPnets are designed for DNN accelerator with efficient sparse matrix storage and computation units, meeting constraints from memory space and computing resources. The proposed method achieves significant reductions in computation cost for various models on MNIST, CIFAR-10, and ImageNet datasets. It meets constraints from memory space and computing resources in embedded systems."
}