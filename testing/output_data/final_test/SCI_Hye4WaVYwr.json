{
    "title": "Hye4WaVYwr",
    "content": "We compare model-free reinforcement learning with model-based approaches, focusing on the expressive power of neural networks for policies, $Q$-functions, and dynamics. Theoretical and empirical evidence suggests that in certain MDPs, model-based planning can approximate the optimal policy better than neural network parameterization. A simple multi-step model-based bootstrapping planner (BOOTS) is applied to enhance policy strength. Empirical results demonstrate the effectiveness of BOOTS in improving policy optimization algorithms. Applying BOOTS on model-based or model-free policy optimization algorithms at test time improves performance on MuJoCo benchmark tasks. Model-based deep RL algorithms offer better sample efficiency than model-free algorithms for continuous control tasks. Model-based deep RL algorithms can be categorized into model-based policy optimization algorithms and model-based planning algorithms. In this paper, the theoretical comparison between model-based and model-free reinforcement learning (RL) in continuous state space is explored through the lens of neural network approximability. The focus is on the representation power of neural networks for Q-function, policy, and dynamics, and how both types of algorithms utilize this expressivity. The main finding highlights the differences in how model-based and model-free RL leverage neural networks. The study explores the differences in approximability of Q-function, policy, and dynamics in model-based and model-free reinforcement learning using neural networks. It shows that even in one-dimensional continuous state space, the optimal Q-function and policy can be much more complex than the dynamics, leading to sub-optimal performance in deep RL algorithms utilizing neural networks. The study compares model-based and model-free reinforcement learning algorithms using neural networks. Model-free algorithms like DQN and SAC, as well as model-based algorithms like SLBO and MBPO, fail to converge to optimal rewards due to lack of expressivity. Model-based planning algorithms, on the other hand, should not suffer from this issue as they use learned dynamics. Even with a constant number of pieces, a partial planner can enhance policy expressivity by planning for k steps and using a Q-function for the remaining steps. Real-world tasks likely have a more complex optimal Q-function and policy than dynamics. A model-based few-steps planner on top of a parameterized Q-function can outperform the original Q-function by introducing additional expressivity through planning. Empirical verification on MuJoCo benchmark tasks shows significant gains by applying a model-based planner on Q-functions learned from model-based or model-free policy optimization algorithms. Our contributions include constructing continuous state space MDPs with complex Q-functions and policies, empirically showing the complexity of randomly generated piecewise linear MDPs, and demonstrating the limitations of model-free RL algorithms on these MDPs. We propose a model-based bootstrapping planner (BOOTS) to improve performance on MuJoCo benchmark tasks. The curr_chunk discusses the performance of a model-based RL algorithm on MuJoCo benchmark tasks, outperforming previous state-of-the-art in the humanoid environment. It compares with prior theoretical work on model-based RL in tabular cases and deep neural networks, focusing on expressivity versus sample efficiency. The study highlights differences in sample complexity and space complexity between model-based and model-free algorithms in tabular MDP settings. The curr_chunk discusses the sample complexity differences between model-based and model-free RL algorithms, particularly in the Linear Quadratic Regulator setting. It highlights a gap in sample efficiency due to the dimensionality of state space supervision. The curr_chunk discusses the limitations of model-free algorithms in deep reinforcement learning due to exact parameterization requirements and sparse rewards. It also mentions model-based RL algorithms like MBPO, STEVE, and MVE that use variants of model-free algorithms on learned dynamics. Q-learning-based policy optimization algorithms are modern extensions of the Dyna framework. SLBO is a model-based algorithm using TRPO in the learned environment. Various approaches exploit dynamics for model-based planning, such as generating extra data for planning implicitly, combining probabilistic models with planning, using Sequential Monte Carlo methods, training a Q-function for lookahead planning, and using random shooting as the planning algorithm. Various approaches in policy optimization algorithms involve using dynamics to improve performance, including random shooting, distilling policies from trajectory optimization, training adversarially robust policies, reformulating as a meta-learning problem, and learning dynamics and value functions for predicting future rewards. Some methods focus on enhancing learned dynamics models, often using an ensemble of models. The Markov Decision Process (MDP) is a key concept in reinforcement learning, consisting of state space, action space, transition dynamics, reward function, and discount factor. An RL agent aims to find the optimal policy that maximizes the expected total reward. The value function is crucial in determining the optimal policy. The value function V \u03c0 and optimal value function V satisfy the Bellman equation and Bellman optimality equation for policy \u03c0. We focus on continuous state space, discrete action space MDPs with deterministic dynamics. There exist MDPs with simple dynamics but complex Q-functions and policies in one-dimensional continuous state space. Learning Q-functions parameterized by neural networks requires an exponential number of samples for optimal expected total reward. The Q-function is more complex than the dynamics, even in random MDPs. Visualization of dynamics, reward functions, and optimal Q-function approximation are illustrated. Lipschitz dynamics can also be considered for similar properties. In the infinite horizon case with 0 < \u03b3 < 1 discount factor, an \"effective horizon\" H = (1 \u2212 \u03b3) \u22121 is defined. Assuming H > 3 as an integer, an MDP M H is constructed with dynamics represented by piecewise linear functions and a reward function. The transition function for a fixed action is also piecewise linear with at most 3 pieces. The initial state distribution is uniform over the state space [0, 1). The construction of the MDP in the previous section involves piecewise linear functions with at most 3 pieces. The dynamics can be modified to be Lipschitz, and key properties are used in the proof of Theorem 4.2. The optimal policy and Q-function are complex, with the policy being a piecewise linear function with an exponentially increasing number of pieces. The optimal Q-function and value function are fractals that are not continuous. The optimal policy for the MDP has a specific number of pieces. The optimal policy for the MDP has a specific number of pieces, with the Q-function being a fractal. The dynamics shift the binary representation of states with some addition, satisfying specific equations. The optimal policy for the MDP involves shifting the binary representation of states to create more 1's for higher future rewards. The policy should choose action 0 if the (H + 1)-th bit of s is already 1, and flip the bit if it is not. The optimal policy involves shifting the binary representation of states to create more 1's for higher future rewards. Action 0 is chosen if the (H + 1)-th bit of s is already 1, otherwise action 1 is taken to flip the bit. The optimal value function is computed by executing the optimal policy, and a formal proof is provided in Appendix B.1. It is shown that there is no neural network approximation of Q or \u03c0 with a polynomial width. In this section, it is proven that no neural network approximation of Q or \u03c0 with a polynomial width is possible. A piecewise linear policy \u03c0 must have a large number of pieces to achieve near-optimal rewards. Constant depth neural networks with polynomial width cannot approximate the optimal policy with near optimal rewards. The number of pieces of the policy is bounded by twice the number of pieces of Q when there are two actions. In the setting of Theorem 4.3, if a policy \u03c0 is near-optimal with \u03b7(\u03c0) \u2265 0.92 \u00b7 \u03b7(\u03c0), then Q has at least \u2126(exp(cH)/H) pieces. The proof of Theorem 4.3 suggests that any polynomial-pieces policy behaves suboptimally in most states, leading to suboptimality. Additionally, an exponential sample complexity lower bound is provided for Q-learning algorithms with neural networks. The Q-function occurs not only in crafted cases but also more robustly in (semi-) randomly generated MDPs. The Q-function occurs more robustly with a decent chance for randomly generated MDPs. An empirical approach is taken to visualize Q-functions for random dynamics using piecewise linear and Lipschitz dynamics. The DQN algorithm is used with a finite-size neural network to learn the Q-function. Two methods are designed to generate random or semi-random piecewise dynamics with at most four pieces. In the second method, SEMI-RAND, more structure is introduced in the generation process to increase the chance of observing the phenomenon. The functions have 3 pieces with shared kinks and more fluctuations in the outputs at the kinks. The reward for both methods is r(s, a) = s, \u2200a \u2208 A. Figure 1 illustrates the dynamics of the generated MDPs from SEMI-RAND. The optimal policy and Q-functions in Appendix D.1 can be complex with a large number of pieces. Results show that a significant fraction of MDPs generated from RAND and SEMI-RAND methods have policies with more pieces than the dynamics. This suggests that the Q-function complexity can exceed that of the dynamics. For more details, refer to the empirical results in Appendix D.2. Model-based policy optimization methods suffer from a lack of expressivity when the Q-function or policy are too complex for neural network approximation. This leads to sub-optimal rewards, as verified in experiments using DQN, SLBO, and MBPO on randomly generated MDPs. The optimal policy for a specific MDP has 765 pieces, and the optimal Q-function has about 4 \u00d7 10 4 pieces. In experiments, DQN with various neural network widths fails to achieve optimal rewards. However, using BOOTS + DQN with even k = 3 planning steps shows near-optimal reward, indicating improved policy expressivity. The Q-function Q and learned dynamics f are used in Algorithm X for testing. If f is not available, it is learned from data in R. Using zero-th order optimization algorithms like cross-entropy method or random shooting, near-optimal policy cannot be found even with 2-14 hidden neurons and 1M trajectories. Enlarging the Q-network improves DQN algorithm performance at convergence. SLBO and MBPO fail to achieve near-optimal rewards due to the inability to approximate the optimal policy and value function. In Sections 4.2 and 4.3, it is shown that complex Q-functions or policies can hinder model-free or model-based policy optimization algorithms. Model-based planning algorithms are not affected by this limitation as the final policy is not represented by a neural network. A few-steps planner can enhance the expressivity of the Q-function, as proven in Theorem 5.1. A k-step model-based bootstrapping planner is applied on existing Q-functions during testing, inspired by theoretical results. This approach is similar to MCTS in AlphaGo, utilizing learned dynamics for one-dimensional MDPs or continuous control tasks in MuJoCo. In AlphaGo, the learned dynamics are used to handle the continuous state space. Test policies for MBSAC and SAC use deterministic policy for better performance. Applying the Bellman operator with learned dynamicsf for k times gives a bootstrapped version of Q. Algorithm 1, called BOOTS, explains how to apply the planner on top of any RL algorithm with a Q-function. The optimal Q-function can be represented with fewer pieces using B k f [Q] compared to representing it directly with Q. Theorem 5.1 states that using a bootstrapped policy \u03c0 boots k,Q,f (s) with a constant-piece piecewise linear dynamicsf and a 2 H\u2212k+1 -piece piecewise linear function Q achieves optimal total rewards. This approach provides a 2 k gain in expressivity compared to directly approximating the optimal Q-function with a piecewise linear function. The performance improvement is significant in realistic settings like randomly-generated MDPs and the MuJoCo environment. BOOTS algorithm is implemented on random piecewise linear MDPs with various planning steps and learned dynamics. The planner enumerates all possible future action sequences, and bootstrapping with a partial planner improves sample-efficiency and performance. BOOTS is applied on MuJoCo environments with SAC algorithm, showing optimal rewards and sample-efficiency. More details are in Appendix D.3. In Figure 4, BOOTS+SAC and BOOTS+MBSAC are compared with SAC and MBSAC on Gym Ant and Humanoid environments, showing BOOTS can enhance existing baselines. BOOTS has limited impact on simpler environments due to less complex Q-functions. Figure 5 compares BOOTS+MBSAC and BOOTS+MBPO with other algorithms on the humanoid environment, demonstrating superior performance. The study highlights a significant neural network representation gap for Q-function, policy, and dynamics. Our model-based bootstrapping planner BOOTS improves performance in synthetic settings and MuJoCo environments. Open questions include generalizing results to high-dimensional state space, analyzing optimal Q-function pieces, and measuring neural network complexity. Challenges include longer test times and planning efficiently in high-dimensional dynamics with long horizons. Our algorithm MBSAC uses SAC to optimize the policy and Q-function by mixing real and virtual data generated by learned dynamics model. It outperforms MBPO in wall-clock time and performs similarly on Humanoid but slightly worse in other environments. Initialize replay buffer B with n init steps of interactions with environments by a random policy, pretrain dynamics on replay buffer data. Sample initial state s 0, iterate for n iter steps: Perform action a t \u223c \u03c0 \u03b2 (\u00b7|s t ), update state s t+1, add transition to B. Optimize dynamicsf \u03b8 with mini-batch from B, sample real and start data B real and B start, update policy \u03c0 \u03b2 and Q \u03d5 using SAC. For Ant and Humanoid environments, modifications were made to the observation space to compute rewards from actions and observations. The policy network used an MLP with ReLU activation and two hidden layers of 256 units. The dynamics model utilized 2 Fixup blocks with 500 hidden units each, leading to more accurate models compared to convolution layers. The model training algorithm used non-squared loss instead of MSE loss. The study utilized non-squared 2 loss for model training and explored planning with oracle dynamics in various environments. Results showed smaller improvements of BOOTS compared to MBSAC and SAC in Cheetah and Walker environments. Planning with oracle dynamics revealed the impact of inaccurate learned dynamics on performance. Figures 6 and 7 present mean rewards, standard deviation, and relative gains of BOOTS. Planning with oracle dynamics generally improved performance, but to varying degrees. The study explored planning with oracle dynamics in various environments, showing smaller improvements of BOOTS compared to MBSAC and SAC in Cheetah and Walker environments. Planning with oracle dynamics generally improved performance, but to varying degrees. Experimenting with different planning horizons showed slightly higher total rewards for both MBSAC and SAC, with planning horizon k = 16 not working well due to errors in the dynamics. The expressivity of Q-functions varies depending on the environment, raising interesting questions on how and when to use learned dynamics for planning. The solution to Bellman optimal equations is unique, verifying that V and \u03c0 satisfy certain conditions. By defining \u03b5 = 2(\u03b3), we can confirm the proposed solution. For different cases of s (H+1), we determine the values of \u03c0 (s) and s. Ultimately, we prove the validity of Eq. (11) for a fixed parameter H. The proof of Theorem 4.3 involves showing that if a policy \u03c0 has a certain number of pieces, then a specific inequality holds. This is based on the advantage decomposition lemma and the behavior of \u03c0 in different states. Lemmas B.3 and B.4 demonstrate that suboptimal states cannot be avoided due to the distribution of states. This leads to a large suboptimal gap for policy \u03c0. Corollary B.2 shows that the suboptimal gap of policy \u03c0 is significant. By using the advantage decomposition lemma, it is proven that for near-optimal policies, the gap is at least \u2126 (exp(cH)/H). Lemmas B.3 and B.4 provide further insights into the behavior of policy \u03c0 in different states, leading to a large suboptimal gap. The construction defines intervals where policy \u03c0 acts unanimously, discarding non-uniform behavior. Statement (b) is satisfied by the density definition. Verification of statements (a) and (c) involves the inverse of Markov transition T \u03c0. In the context of defining intervals for policy \u03c0, the text discusses the preservation of density by T \u03c0 and the need for an exponentially large number of samples in Q-learning algorithms. An exponentially large number of samples is required in Q-learning algorithms. Q-learning with Oracle is a stronger computation model that directly fits Q functions with supervised learning. Theorem B.5 proves a sample complexity lower bound for Q-learning algorithms. The Q-learning with Oracle algorithm requires exponentially many samples to find a policy with high accuracy. The proof exploits the sparsity of the solution found by the minimal-norm tie-breaker, showing that there are at most O(n) non-zero neurons in the solution. The proof shows that the minimal-norm solution has at most O(n) non-zero neurons in a two-layer ReLU neural net. The norm of the neural net is defined, and the Q-learning with oracle algorithm finds the solution through supervised learning. Theorem B.7 states that for n = o(exp(cH)/H), the greedy policy has a high accuracy. The minimal-norm solution to Eq. (24) has at most 32n + 1 non-zero neurons. The proof of Theorem B.7 shows that the greedy policy has at most 64n + 4 pieces for a Q-function with 32n + 2 pieces. Lemma B.8 is based on merging neurons with the same x i values. The proof involves replacing neurons with similar x values and merging neurons with different intercepts between data points, resulting in a decrease in norm. The proof involves merging neurons with similar x values and intercepts between data points, resulting in a decrease in norm. Between two data points, there are at most 34 non-zero neurons in the minimal norm solution. In the minimal norm solution, there are at most 15 non-zero neurons with specific conditions on their values. In the proof of Theorem 5.1, the true trajectory estimator and optimal action sequence are defined. An extension to the construction ensures Lipschitz dynamics with an action space A = {0, 1, 2, 3, 4}. Effective horizon H = (1 \u2212 \u03b3) \u22121 defines an MDP M H with \u03ba = 2 \u2212H. The effective horizon H = (1 \u2212 \u03b3) \u22121 defines an MDP M H with dynamics and reward function. The optimal policy \u03c0 for M H is defined along with the corresponding optimal value function. A piecewise linear policy \u03c0 with near optimal reward must have at least \u2126 (exp(cH)/H) pieces. The dynamics where f (s, a) = 0 or f (s, a) = 1 may affect the behavior. The dynamics where f(s, a) = 0 or f(s, a) = 1 can disrupt the uniform state distribution. Triggering the clip in an interval incurs a high cost in suboptimality gap. The RAND method generates random kinks and values in MDPs with less structure. State and action spaces are defined, with kink positions and values generated randomly. The SEMI-RAND method adds structures to MDP dynamics to increase complexity in optimal policies. State and action spaces are defined with fixed kink positions and fluctuating values. Initial state distribution is uniform, and 10 3 1-dimensional MDPs are randomly generated. The initial state distribution is U(0, 1) and 10 3 1-dimensional MDPs are randomly generated with a constant number of pieces in dynamics. The histogram shows that the optimal policy tends to have more pieces than the dynamics. The Q-network is a fully connected neural net with varying hidden-layer width, using SGD optimizer with learning rate 0.001 and momentum 0.9. Other details include a replay buffer size of 10^4, target-net update frequency of 50, batch size of 128 in policy optimization, and a behavior policy that exponentially decays from 0.9 to 0.01. The MBPO algorithm uses 2 loss functions for model-learning and Soft Actor-Critic (SAC) for policy optimization. In the policy optimization step, Soft Actor-Critic (SAC) is used with specific parameters such as hidden neurons, optimizer, temperature, and rollout steps. For the model-learning step, PPO is used instead of TRPO, with its own set of parameters including hidden neurons, optimizer, and policy optimization steps. The behavior policy follows a -greedy policy according to the current policy. The behavior policy is -greedy according to the current policy network, with an exponential decay from 0.9 to 0.01. The Model-based Planning algorithm involves learning dynamics from sampled trajectories and planning with the learned dynamics. Parameters include 32 hidden neurons in model-net and Adam optimizer with learning rate 0.001. Bootstrapping follows a similar training time behavior as DQN algorithm, with 64 hidden neurons in the Q-net and planning horizon variations. In this section, technical lemmas are presented. Lemma E.1 states that for A, B, C, D \u2265 0 and AC \u2265 BD, a certain inequality holds, which is strict when BD > 0."
}