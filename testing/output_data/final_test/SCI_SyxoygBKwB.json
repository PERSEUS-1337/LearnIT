{
    "title": "SyxoygBKwB",
    "content": "CopyCAT is a targeted attack that lures deep reinforcement learning agents into following an outsider's policy. It is fast and effective on Atari 2600 games in a read-only setting where the adversary can only attack the agent's observation. This attack is focused on sequential control systems using deep neural policies, different from previous methods targeting neural classifiers. The work developed methods to attack neural classifiers with adversarial examples, successful at fooling deep networks with high-dimensional input-data like images. Previous works used these examples to break neural policies, but they are not applicable in real-time settings and rely on strong assumptions. A new attack is proposed to target deep reinforcement learning agents by luring them into following an outsider's policy in a read-only setting. CopyCAT is an algorithm designed to take control of neural policies by altering observations of the environment rather than the agent's inner state. It is targeted towards matching a neural policy's behavior with an arbitrary policy and consists of pre-computed state-independent masks. This attack can be implemented in real-time settings without requiring additional time at inference. The attack model (Papernot et al., 2016) focuses on altering the agent's observations to manipulate its behavior, targeting a specific policy. The attack must be fast enough for real-time use and involves writing in the agent's memory of past observations. This approach differs from untargeted attacks aimed at preventing the agent from taking its preferred actions. In contrast to previous works targeting specific policies, the attack model aims to manipulate the agent's behavior by altering its observations. This can involve training a target policy to control an autonomous vehicle or any agent with high-dimensional inputs. The vulnerability of autonomous agents is exposed as they can be lured into following catastrophic behaviors. In Reinforcement Learning (RL), an agent interacts with a dynamic environment to learn optimal control through a Markov Decision Process. The agent's policy is trained to maximize expected discounted return, with the value function denoting the policy's effectiveness in achieving this goal. Value-based algorithms in deep reinforcement learning use the value function or action-value function to compute policies. Deep RL utilizes neural networks for function approximation to handle large state spaces, with actor-critics directly parametrizing their policy with neural networks. Adversarial examples were initially introduced in supervised classification contexts. Adversarial examples were introduced in supervised classification for generating inputs that deceive classifiers. The Fast Gradient Sign Method (FGSM) is a common method for creating adversarial examples by maximizing the loss of the classifier. It can be turned into a targeted attack by optimizing for a specific label. These attacks can be computed for different norms, such as L \u221e and L 2. An L2 attack can be turned into iterative-FGSM by taking multiple gradient steps. Deep RL agents can be deceived using untargeted FGSM to prevent preferred actions or targeted FGSM to induce specific actions, but real-time feasibility is a concern. This requires direct modification of the agent's inner state and neural policy input. CopyCAT is an attack designed to fully control an agent's behavior by using pre-computed additive masks to drive the policy to follow a specified target policy. It can be used in real-time settings without additional computation, making it a universal attack on raw observations. CopyCAT gathers data from the agent by observing its interactions with the environment to create a dataset of observations. The objective is for CopyCAT to work with read-only access to the agent's inner workings. The dataset D consists of observations, not states, and if successful, CopyCAT can transfer to unseen observations. The learned attack is universal and can move the entire support of observations. The learned attack is universal and can move the entire support of observations in a region of R N where \u03c0 chooses a precise action. An adversarial example targeted towards label \u0177 is created by maximizing log P(\u0177|x) subject to constraints. CopyCAT is designed to work with any observation it is applied to, using masks to lure \u03c0 into taking specific actions. The method is optimized by alternating between gradient steps with adaptive techniques. CopyCAT is a full optimization method for creating adversarial examples to fool neural classifiers. It uses adaptive learning rate and projection steps onto the L \u221e -ball of radius, with parameters for constraints on attack norms. The attack can be applied to make a policy follow a target policy by inferring actions at each time step. The vulnerabilities of neural classifiers were highlighted by Szegedy et al. (2013), leading to the development of adversarial examples. The method from Huang et al. (2017) uses fast-gradient-sign method to create adversarial examples to attack deep reinforcement learning agents. However, this approach is not feasible in real-time as it requires crafting a new attack at each time step and modifying the inner state of the agent directly. On the other hand, the approach of Lin et al. (2017) reduces the number of attacked states but still relies on heavy optimization schemes that are not practical for real-time settings. The curr_chunk discusses various methods for adversarial attacks on deep reinforcement learning agents, including targeted attacks and methods to reduce agent performance. Different approaches have been proposed, such as adversarial training and environment generation for studying worst-case scenarios. The focus is on how adversaries can take control of a neural network. The curr_chunk focuses on building an attack targeted towards a specific policy \u03c0 target, measuring the success rate of the attack by comparing it to the agreement rate between the policies. The goal is to test CopyCAT's ability to influence the behavior of the attacked agent, showing that a high success rate does not guarantee the desired behavior. The policies involved are trained and frozen, with no learning of neural policies. The agent's behavior is tested by comparing the cumulative reward obtained under attack to the target policy's reward. If the attacked policy matches the target policy's behavior, it indicates successful behavior manipulation. The behavior induced by \u03c0 under attack matches the behavior induced by \u03c0 target, as cumulative reward is used to monitor an agent's behavior. The goal is not to improve DQN's performance through attacks, but to show that its behavior can be fully manipulated. Cumulative reward is used as a proxy for monitoring behavior under attack in real-time targeted attacks. We aim to build a real-time targeted attack and compare our algorithm to baselines applicable to this scenario. The fastest state-of-the-art method involves applying targeted FGSM at each time step to compute a new attack. CopyCAT is faster at inference and only attacks observations, not complete states. Comparing CopyCAT to a version of the method where the attack is computed only with respect to the last observation. The FGSM attack is computed based on the last observation, similar to CopyCAT. The gradient is adjusted to maintain fairness in comparison. CopyCAT and FGSM-L \u221e have similar parameters, with CopyCAT having an additional regularization parameter for lower energy attacks. Full optimization-based attacks are not practical for sequential decision making. Sticky actions are used to introduce stochasticity in the problem. Sticky actions are always turned on to introduce stochasticity in the problem. DQN uses a stack of four observations as its inner state for Atari games. Trajectories generated by \u03c0 are used to fill D with 10k observations for learning the masks of \u2206. CopyCAT has an extra parameter \u03b1 that influences the L2 norm of the attack. FGSM-L\u221e computes an attack \u03b7 of maximal energy, with CopyCAT producing |A| masks for a given \u03b1. The largest L2 norm of the masks is shown in Fig. 1 for varying \u03b1. The attack produced by CopyCAT has lower energy than FGSM-L\u221e for a given \u03b1, especially significant for higher values. The analysis focuses on the agent's behavior under attack, measuring attack success rate and cumulative rewards. In an experiment, CopyCAT's ability to lure an agent into specific behavior is tested by measuring attack success rate and cumulative rewards. The attack is successful if the agent's behavior matches a target policy with higher rewards. Graphs show the success rate and rewards for different parameter values. In experiments, CopyCAT is tested to lure an agent into specific behavior by measuring attack success rate and rewards. There is a gap between high success rate and matching behavior of \u03c0 target. CopyCAT with certain parameters can achieve a high success rate but may not match the target policy behavior. With specific parameters, CopyCAT can consistently induce the agent to follow the behavior of \u03c0 target. Comparisons are made to the targeted version of FGSM. In experiments, CopyCAT is tested to lure an agent into specific behavior by measuring attack success rate and rewards. The success rate and average cumulative reward under attack are compared between CopyCAT and FGSM in real-time and read-only settings. The success rate and cumulative reward are plotted against the L2 norm of the attack for FGSM and the largest L2 norm of the masks for CopyCAT. The standard deviation is only plotted for the attack success rate due to the intrinsic noise of CopyCAT. CopyCAT successfully lures the agent into desired behavior, with over 99% of actions matching the target. FGSM achieves a 75% success rate in turning actions into targeted ones but fails to match behavior with the target. CopyCAT outperforms FGSM in achieving higher cumulative rewards under attack. CopyCAT is a simple algorithm designed to manipulate neural policies by luring them into desired behavior using a finite set of masks. It outperforms FGSM in achieving higher cumulative rewards under attack. The effectiveness of universal masks in manipulating policy behavior in real-time settings was demonstrated in Atari games. Future work includes developing algorithms that can maintain normal behavior when attacked or detect and respond to attacks in sequential-decisionmaking settings. Another direction is testing targeted attacks on neural policies in black-box scenarios without access to network weights. Attacks on neural policies in black-box scenarios are challenging, especially for targeted adversarial examples. Reinforcement learning may face more difficulties than supervised learning due to less interpretable representations and higher variability between different random seeds. Different policies trained with the same algorithm can result in varying decision boundaries. Transferring targeted examples may require training imitation models to compute transferable adversarial examples. Only a subset of results is shown in Sec. 5, with explanations and interpretations provided. HERO, Space Invaders, and Air Raid are included in the results. In this appendix, additional experiments are conducted to study the proposed approach. The attacked agent was a trained DQN agent, while the target policy was a trained Rainbow agent. FGSM is able to lure an untrained DQN into following the target policy as well as CopyCAT. Trained networks are more robust to adversarial examples, showing it is easier to fool an untrained network. Trained networks are more robust to adversarial examples. CopyCAT can lure the agent into following the target policy. Black-box targeted attacks involve transferring adversarial examples between different models without access to weights. Universal attacks may transfer better between models, enhancing CopyCAT for black-box settings. In the black-box setting, CopyCAT trains multiple masks efficiently against any convex combination of proxy models' predictions. Masks are computed by maximizing over 100 epochs on the dataset D. The highest performing masks are selected based on competition accuracy computed by querying the attacked model \u03c0 on states built from consecutive observations in D. In the black-box setting, CopyCAT trains masks against proxy models to attack the model \u03c0. Results show success rates and cumulative rewards over new episodes. Each dot represents an attack tested, with the maximal norm of the attack on the x-axis and corresponding values of \u03b1 on the y-axis. The proposed black-box attack is effective, although less efficient than its white-box counterpart. Reinforcement learning has led to great improvements for games or robots manipulation but struggles with realistic-image environments. This paper focuses on weaknesses of reinforcement learning agents and presents a proof-of-concept for universal adversarial examples on ImageNet. Brown et al. (2017) previously showed universal attacks using a patch, while this method uses an additional mask. The study presents a universal attack on VGG16 targeted at \"tiger shark\" label, using a patch on the image. The attack is trained on a small dataset and tested on ImageNet validation set. The network is from Keras pretrained models and attacked in a white-box setting. After 200 epochs, the train accuracy is 90% and test accuracy is 88.44%. The experiment validated the existence of universal adversarial examples on realistic images, expanding CopyCAT's scope beyond Atari-like environments. Adversarial examples are shown to be a property of high-dimensional manifolds, making it easier for CopyCAT to find universal adversarial examples on higher dimensional images."
}