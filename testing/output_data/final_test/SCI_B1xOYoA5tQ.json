{
    "title": "B1xOYoA5tQ",
    "content": "Deep models in computer vision are vulnerable to adversarial examples due to one-hot encoding. A new approach using multi-way encoding makes models more robust against attacks, achieving state-of-the-art results on benchmark datasets. This approach also poses challenges in detecting stolen models. Deep learning models are vulnerable to adversarial attacks, including black-box attacks that rely on perturbing input based on the gradient of the loss function. This added \"noise\" can fool a model without being visually evident to a human, raising concerns about model security. The conventional deep classification frameworks make models vulnerable to black-box attacks by correlating gradients with respect to input. To increase adversarial robustness, a multi-way encoding representation replaces one-hot encoding with real number encoding in a higher-dimensional space. This approach expands the number of possible gradient directions, making it harder for adversaries to perform attacks. Multi-way encoding replaces one-hot encoding with real number encoding in a higher-dimensional space, increasing the number of possible gradient directions to improve adversarial robustness. This approach makes it challenging for adversaries to perform targeted or untargeted attacks, enhancing model's robustness in white-box attack scenarios. Experimental results on benchmark datasets demonstrate the benefits of multi-way encoding. Trains a model to misclassify by relaxing and increasing encoding dimensionality for better adversarial robustness. Demonstrates the benefits of multi-way encoding in higher-dimensional space for improved model robustness against attacks. The text discusses the use of 3D multi-way encoding to reduce the transferability of watermarked images, making it harder to detect stolen models. It highlights the vulnerability of traditional 1of K mapping to adversarial gradients and proposes a novel solution using multi-way encoding to improve model robustness against attacks. Additionally, it demonstrates how this approach can be used to attack model watermarking schemes. In attacking model watermarking schemes, various adversarial attacks and defenses have been explored, including gradient-based attacks like the Fast Gradient Sign Method (FGSM) and the Basic Iterative Method (BIM). These attacks aim to craft adversarial examples by manipulating input gradients to fool classifiers. PGD, also known as the iterative version of FGSM, is shown to be stronger when starting from randomly chosen points within allowed perturbation. State-of-the-art adversarial defenses focus on gradient masking to make it harder for adversaries to find useful gradients. However, methods like BID12 and BID7 are robust against BPDA attacks without relying on obfuscated gradients. BID12 and BID7 use conventional one-hot encoding, while the proposed approach suggests a higher dimensional multiway encoding to obstruct adversarial gradient search. Other attempts include using alternate output encodings for image classification in deep models by BID22 and BID16. In this section, we introduce a new output encoding scheme called multi-way encoding to enhance model robustness against adversarial attacks. The approach involves using a target model g(x) and a substitute model f(x) for generating black-box attacks. Adversarial examples x adv are created using gradient-based methods like FGSM and PGD, perturbing input x based on the attack strength \u03b5. This results in x adv being a translated version of x further away from its original vicinity. Adversarial attacks involve creating x adv, a translated version of x further away from its original vicinity, to increase the likelihood of misclassification. This is achieved by moving x towards a specific target class, using the adversarial class as the ground truth during back-propagation. The most widely used setup for deep classification networks involves encoding the output activation of neurons and calculating softmax predictions and cross-entropy loss. The proposed multi-way encoding method in this work is Random Orthogonal (RO) output vector encoding generated via Gram-Schmidt orthogonalization. For a classification problem of k classes, a codebook C RO is created, breaking away from the 1-of-K encoding. The loss function used is between the output of the encoding-layer and the RO ground-truth vector, instead of softmax and cross-entropy. In a multi-way encoding setup, the loss function is between the encoding-layer output and the RO ground-truth vector. Mean Squared Error (MSE) Loss is used for classification, resulting in a larger number of possible gradient directions. This reduces the likelihood of an adversary causing misclassification by selecting a harmful direction. The use of a 1-of-K encoding and softmax-cross entropy classifier limits the directions a point can move in, as depicted in Figure 1. In multi-way encoding, the gradient space is less constrained compared to 1-of-K encoding, resulting in a larger number of possible gradient directions. This reduces the likelihood of misclassification by adversaries. Additionally, multi-way encoding increases robustness by adding dimensionality to the gradients. We combine multi-way encoding with adversarial training for added robustness in solving the canonical min-max problem against PGD attacks. Experiments are conducted on benchmark datasets including MNIST, CIFAR-10, CIFAR-100, and SVHN, each with specific characteristics and sizes. SVHN BID13 is an image dataset for recognizing street view house numbers. Black-box and white-box attacks are defined in the context of model architecture and encoding. The impact of output encoding layer dimension on classification accuracy is tested using RO multi-way encoding on the MNIST dataset under different attack scenarios. The study analyzed the impact of output encoding layer dimension on classification accuracy using RO multi-way encoding. It was found that using different encodings for the source and target models resulted in less correlated gradients, leading to better accuracy in white-box attacks. In experiments using RO encoding with dimension 2000 and \u03b2 = 1000, the study delves into the effectiveness of multi-way encoding on adversarial robustness. Models A and C, based on LeNet-like CNNs, were trained on MNIST using different output encodings. Adversarial examples were generated using FGSM with an attack strength of 0.2. The study explores the impact of output encoding on adversarial robustness. Adversarial examples were created using FGSM with an attack strength of 0.2. Target models showed vulnerability to attacks from substitute models with the same encoding, leading to lower accuracy. This highlights the importance of hiding output encoding for improved model robustness. The study emphasizes the importance of hiding output encoding to enhance model robustness against adversarial attacks. Results show that using different output encodings can significantly impact the model's vulnerability to attacks, with RO encoding demonstrating better accuracy and robustness compared to 1of K encoding. Further ablation studies are presented in Appendix A. The study highlights the impact of using different output encodings on model vulnerability to adversarial attacks. Gradients of sign(\u2207 x Loss(f (x))) and sign(\u2207 x Loss(g(x))) are used to create adversarial examples. Results show that RO encoding is more robust compared to 1of K encoding. Black-box FGSM attacks of varying strengths are demonstrated for different encodings in substitute models. Using RO encoding for source and target models increases robustness to adversarial attacks, maintaining higher accuracy as attack strength increases. Multi-way encoding enhances robustness in black-box attacks compared to 1of K encoding for targeted and untargeted attacks on benchmark datasets. Targeted attacks manipulate gradients to misclassify examples, while black-box attacks are generated from substitute models trained with 1of K encoding. The experiment results show that models using RO encoding are more resilient to black-box attacks compared to 1of K encoding. RO consistently leads to higher classification accuracy for untargeted attacks and lower attack success rates for all benchmark datasets. The experiment results demonstrate that RO encoding leads to higher classification accuracy for untargeted attacks and lower attack success rates for all benchmark datasets. Adversarial training involves injecting adversarial examples into the training data of the target model to enhance its robustness. Black-box attacks are generated from an independently trained substitute model, with a mix of clean and adversarial examples used for training in some cases. In this study, adversarial examples were generated and compared for MNIST, CIFAR-10, and CIFAR-100 datasets using the experimental setup of BID12 and BID2. State-of-the-art defense methods BID12 and BID7 were evaluated, with a focus on using LeNet for MNIST and WideResNet for CIFAR-10. Results were also presented for CIFAR-100 and SVHN datasets. The combination of multi-way encoding formulation with adversarial training achieved state-of-the-art robustness against white-box and black-box attacks, while improving accuracy on clean datasets. In this study, an attack was introduced for detecting stolen models by adding a watermark to sample images and deliberately causing misclassifications. The attack involves fine-tuning the stolen model using multi-way encoding to make it more challenging to detect. Adversarial examples become less transferable when the encoding of substitute and target models is different. The experiment followed the CIFAR-10 setup. The attack involves fine-tuning stolen models using different encoding methods to reduce transferability of adversarial examples. Results show that fine-tuning with RO encoding significantly decreases watermarking detection accuracy, making it comparable to models trained from scratch. Fine-tuned models benefit from pre-trained weights of stolen models. Results on CIFAR-10 dataset show that fine-tuning with 1of K encoding yields 87.8% watermarking detection, while using multi-way RO encoding results in only 12.9% detection. The accuracy of fine-tuned models benefits from pre-trained weights of the stolen model. Ablation studies further investigate the effectiveness of RO encoding. The text discusses the use of RO encoding, softmax layer, and cross-entropy loss in a network model. It compares the performance of different target models using FGSM attacks and measures input gradient correlation. Results show that RO achieves the highest accuracy and lowest input gradient correlation with substitute models. The study compares the correlations of convolutional layers in different target models using RO and 1ofK encoding. Results show higher correlations in 1ofK models compared to RO models, even with the same output encoding. The correlations between RO and 1ofK models are also low."
}