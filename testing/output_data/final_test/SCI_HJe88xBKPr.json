{
    "title": "HJe88xBKPr",
    "content": "Reduced precision computation is a key area addressing the 'compute gap' in deep learning applications. Deep neural network training has shifted to 16-bit precision for improved performance and energy efficiency. This paper introduces a method to train DNNs using 8-bit floating point representation, achieving state-of-the-art accuracy across various datasets and workloads. An enhanced loss scaling method is proposed to improve error propagation in the reduced subnormal range of 8-bit floating point. The success of Deep Learning models has led to the adoption of larger neural networks, increasing computational requirements exponentially. Techniques like reduced precision computation using 8-bit floating point representation have been proposed to improve error propagation and address gradient noise, resulting in slightly higher validation accuracy compared to full precision baseline. The requirements for training Deep Learning models have been growing exponentially, outperforming Moore's Law and hardware capabilities. Research is focusing on reducing numeric precision to improve compute efficiency without sacrificing model accuracy. Recent studies show that deep neural networks can be trained using 16-bit precision with minimal impact on validation accuracy. State-of-the-art training platforms now support 16-bit precision for improved performance. Recent research has focused on training deep neural networks at lower precision, including 8-bit integer formats. However, training with 8-bit integers has been challenging due to limited dynamic range. Wang et al. (2018) have shown that 8-bit floating representation can be used for training convolutional neural networks with specialized hardware. Our paper extends the state of the art in 8-bit floating point (FP8) training by proposing a scalable solution that eliminates the need for specialized hardware, enabling efficient MAC designs with higher compute density. We demonstrated state-of-the-art training results using 8-bit floating point representation across multiple datasets and workloads. Additionally, we propose an enhanced loss scaling method to compensate for the reduced subnormal range of 8-bit floating point. In the pursuit of improving compute efficiency, researchers have experimented with various numeric formats and hardware implementations for deep learning training. Studies have shown that deep neural networks can be trained with minimal loss in accuracy using 16-bit fixed point representation, half-precision floating point, and dynamic fixed point formats. Researchers have also proposed an enhanced loss scaling method and stochastic rounding technique to address gradient noise for better model generalization. In recent studies, neural network training has shifted to 16-bit hardware for improved performance. Various methods have been explored to reduce precision requirements in deep neural networks, such as using 1-bit and 2-bit convolutions for weights and activations. However, these methods have resulted in a loss of validation accuracy. Recent studies have focused on reducing precision requirements in deep neural networks, with methods like 1-bit and 2-bit convolutions. Wang et al. (2018) successfully trained Resnet-50 using 8-bit floating point format with specialized hardware. While previous studies attributed training issues to accumulator size, further research on larger networks suggests these issues are independent of accumulator size. The impact of using FP8 numeric format on training is being studied. The impact of using FP8 numeric format on training is being studied, focusing on maintaining a high precision accumulator (FP32). The choice of bit-level representation of floating point has a significant impact on the effectiveness of the numerical format, especially at low bit-width representations. Maintaining a higher dynamic range for effective propagation of error gradients can lead to values that are too few and scattered for gradient computations. After considering the impact of using FP8 numeric format on training and the importance of maintaining a high precision accumulator (FP32), a decision was made to use s=1,e=5,m=2 format for 8-bit floating point representation. Each GEMM/convolution operation involves two input tensors in 8-bit format, producing a 32-bit output that needs to be down-converted back to 8-bit. Rounding during down-conversion is crucial for numeric accuracy recovery. Different rounding modes were studied for their impact on training performance. In Section 3.2, the precision settings for compute operations in mixed precision training are discussed. The 'GEMM' operator is a key compute kernel used in deep neural networks for various passes. Quantization nodes perform down-conversion and rounding on 32-bit output from GEMM operators to 8-bit floating point format. Weight, activation, error, and gradient tensors are quantized to 8-bit floating point format in training experiments. The data flow during optimization and weight update steps is shown in Figure 1b. In the optimization path, L2-regularization is added to cross entropy and loss is scaled before back propagation. Weight gradients are computed and converted to 8-bit floating point format. Gradients are re-scaled using the loss scale parameter and passed to the momentum optimizer. The final gradients are applied to the master weights stored in half-precision format, which are up-converted to 32-bit during update and then converted back to half-precision after the update. This process ensures precision in weight updates. The proposed 8-bit floating point format has a smaller subnormal range compared to half-precision floating point. Previous studies have shown that loss scaling technique can successfully train neural networks. The dynamic range comparison between full-precision (FP32), half-precision (FP16), and the proposed 8-bit floating point (FP8) formats is shown in Table 1. The text discusses the use of dynamic loss scaling methods for training convolution networks in FP8 format. Results show that Resnet-50 converges at a scaling factor of 10,000, while the Transformer model uses a standard dynamic loss scaling method. The text discusses using loss scaling methods in TensorFlow for the GNMT model to address gradient overflows and underflows. The algorithm adjusts the loss scale value to prevent 'NaN' errors during gradient computation. By setting a minimum threshold for the loss scale value, outliers causing overflows are ignored to maintain a higher loss scale value. The algorithm adjusts the loss scale value to prevent 'NaN' errors during gradient computation by setting a minimum threshold. It enhances the algorithm with additional checks to ignore spurious overflows and reduce the interval between loss scale updates to recover from any drop in value. Reduced precision methods can introduce noise affecting convergence and accuracy of deep neural networks. Rounding techniques in quantization can help regulate this noise, with the choice of rounding method significantly impacting numeric accuracy. Stochastic rounding has been effective for training neural networks with low-precision fixed point formats. Different rounding methods showed varying convergence results at different loss scales. Dynamic loss scaling with increasing minimum threshold for the scaling factor is used to prevent 'NaN' errors during gradient computation. The impact of rounding methods on model convergence and generalization is explored in this section. RNE rounding method was effective for smaller networks like Resnet-18 but led to overfitting in ResNet-50. Dynamic loss scaling is used to prevent 'NaN' errors during gradient computation. The increased validation error is caused by noisy error gradients during early epochs, leading to unconstrained growth in model parameters. This behavior is exacerbated by increased regularization loss, with ResNet-18 showing a gradual increase in L2 regularization loss and ResNet-50 starting with high L2-loss due to initialization of low fan-in 1x1 convolutions. The initialization of ResNet-50 leads to noisy behavior in early iterations, requiring stochastic rounding for better performance. Additional experiments with different forms of regularization show that RNE is ineffective in regulating quantization noise, causing unconstrained growth in model parameters. Unlike deterministic rounding techniques, stochastic rounding is essential for ResNet-50. Stochastic rounding is used to regulate quantization noise in Resnet-50, improving regularization effectiveness. A TensorFlow platform emulates 8-bit floating point properties for training experiments. The training framework updates the graph by inserting quantization nodes for convolution and GEMM kernels. Residual networks Resnet-34/50 were successfully trained on Imagenet-1K dataset using this method. 8-bit training experiments involved quantizing certain layers while maintaining others at half-precision to minimize accuracy loss. The first convolution layer requires a minimum of eight mantissa bits for accurate representation of image data. The master copy of weights is stored in half-precision to speed up weight update operations. Identical batch size and hyperparameters are used for both full precision and 8-bit training experiments. Validation accuracy achieved by convolution networks on imagenet-1K dataset is summarized in Table 2. The text chunk discusses training a 8-layer GNMT encoder/decoder LSTM model and a 6-layer Transformer-big translation network using 8-bit floating point format for MatMul operations. Activation functions like tanh and sigmoid use half-precision data type. The experiments match single precision baselines in terms of BLEU scores. The text chunk discusses training a 8-layer GNMT encoder/decoder LSTM model and a 6-layer Transformer-big translation network using 8-bit floating point format for MatMul operations. The results show state-of-the-art accuracy across multiple datasets and workloads. The proposed solution eliminates the need for stochastic rounding hardware, reducing costs. In the critical compute path, Wang et al. (2018) proposed reducing cost and complexity of the MAC unit by using 8-bit numeric format for large scale neural network training. Solutions for gradient underflow and quantization noise include enhanced loss scaling and stochastic rounding."
}