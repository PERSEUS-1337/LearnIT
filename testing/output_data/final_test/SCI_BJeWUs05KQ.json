{
    "title": "BJeWUs05KQ",
    "content": "The use of imitation learning for complex tasks with multiple modes or hierarchical structures can be challenging. Previous work has shown that learning separate policies for each mode or sub-task can improve performance. A new algorithm based on generative adversarial imitation learning framework automatically learns sub-task policies from unsegmented demonstrations by maximizing information flow between sub-task latent variables and generated trajectories. This approach connects with the Options framework for learning hierarchical policies. In this work, an imitation learning framework is developed to learn sub-task policies from unsegmented activity demonstrations. The framework also learns a macro-policy for switching between sub-tasks, allowing for shared learning benefits. Learning sub-task specific policies has the benefit of shared learning and specialization over a restricted state space. Previous works in imitation learning focus on modeling variability in each sub-task policy using a latent variable inferred through high mutual information with expert demonstrations. However, scaling this approach to real-world scenarios with isolated demonstrations for each sub-task is challenging. In this work, the aim is to learn sub-task policies directly from unsegmented activity demonstrations using a causal graphical model. The model represents a Dynamic Bayesian Network and helps in transitioning between sub-tasks efficiently. The approach involves expert demonstrations represented by state-action pairs, enabling the learning of policies for completing different sub-tasks. The aim is to learn sub-task policies from expert demonstrations using a causal graphical model. The model transitions between sub-tasks efficiently by representing Dynamic Bayesian Network. The approach involves state-action pairs to learn policies for completing different sub-tasks, focusing on maximizing directed information flow from trajectories to latent factors. The approach involves maximizing directed information flow from trajectories to latent factors in a causal graphical model, providing a better upper bound on information flow compared to mutual information. It connects with learning sub-task policies using imitation learning and the options framework in a generative adversarial imitation setting. Our approach combines hierarchical policies with generative adversarial imitation learning to overcome issues like compounding errors in behavior cloning. Key contributions include extending existing frameworks for learning sub-task specific policies and demonstrating the segmentation of expert demonstrations in experiments on discrete and continuous state-action spaces. Our approach combines hierarchical policies with generative adversarial imitation learning to segment expert demonstrations into meaningful sub-tasks and learn sub-task specific policies for complex high-dimensional control tasks. Generative Adversarial Networks (GAN) use a generator network to represent the agent's policy and a discriminator network to differentiate between expert and agent policies. InfoGAIL and InfoBID16 introduce a latent variable to separate behaviors in demonstrations and enforce high mutual information between the variable and state-action pairs. In InfoGAIL, a modified objective is proposed to model intra-trajectory variations using latent codes corresponding to sub-tasks within a demonstration. The options framework is discussed, where options consist of sub-policies, termination policies, and activation policies. This framework has been widely studied in the context of MDPs. In the context of MDPs, the options framework has been extensively studied in RL literature. One challenge is automatically inferring options without supervision. Various approaches, such as multiple-instance learning and graph-based algorithms, have been used to identify bottleneck states. Instead of discovering bottleneck states, a unified framework using an information-theoretic approach is proposed to automatically find relevant option policies. This approach formulates the options framework as a probabilistic graphical model, treating options as latent variables learned from expert data. The options framework in MDPs involves learning option policies as latent variables from expert data. These policies are optimized using the Expectation-Maximization algorithm. The proposed approach extends this to multiple levels of option hierarchies and allows for the selection of continuous actions in states where no options apply. Additionally, the method can be expanded to multi-level hierarchies and hybrid categorical-continuous macro-policies. The work proposes a behavior cloning approach using connectionist temporal classification to maximize the joint likelihood of sketch sequences and sub-policies without requiring task sketches as input. Prior research in robot learning has focused on learning motion primitives from unsegmented demonstrations, which can be used for policy improvement through reinforcement learning. The proposed approach in the current text chunk utilizes reinforcement learning for policy improvement, segmenting expert demonstrations using different models, and learning an appropriate policy over extracted options to induce novel behaviors. This is in contrast to prior research focusing on learning motion primitives for policy improvement through reinforcement learning. The current text chunk discusses using graphical models to learn sub-task policies from unsegmented expert demonstrations, where each demonstration is represented by state-action pairs and corresponding latent variables. This approach differs from previous methods that only use a single latent variable to model expert sub-task demonstrations. The text chunk discusses maximizing mutual information between demonstrated state-action pairs and latent variables for sub-task demonstrations. This is combined with an adversarial loss for imitation learning. The proposed method aims to generate trajectories that maximize performance. In this work, the focus is on maximizing directed information flow from trajectories to latent sub-activity variables instead of mutual information. This approach replaces dependence on \u03c4 with a dependence on the trajectory generated up to current time t. The method involves using an approximate posterior distribution to learn a posterior distribution over the next latent factor c given the latent factors discovered so far. The approach involves maximizing directed information flow from trajectories to latent sub-activity variables by learning a posterior distribution over the next latent factor c. This is achieved by using a variational auto-encoder (VAE) to estimate the prior distribution and sampling latent variables using the Gumbel-softmax trick. The VAE pre-training step involves using the Gumbel-softmax trick to obtain samples of latent variables from a categorical distribution. The objective is to maximize the lower bound of the probability of trajectories to train the VAE. The weights of the network q are fixed from the VAE pre-training step when optimizing the Directed-Info GAIL loss. The VAE encoder uses the current state and previous latent variable to produce the current latent variable. The decoder reconstructs the action using the current state and latent variable. The Directed-Info GAIL approach combines VAE pre-training with sub-task policies to learn from expert trajectories. Our approach combines behavior cloning and generative adversarial imitation learning, using GAIL to learn robust policies without compounding errors. Conditioning GAIL on latent codes from behavior cloning prevents mode collapse in GANs. Results show segmentation of sub-tasks, learning sub-task conditioned policies, and combining them to achieve task objectives. The proposed Directed-Info GAIL approach improves over the policy learned from the VAE pre-training step in a grid world environment. The agent's goal is to reach an apple using sub-task policies learned by the approach. The Directed-Info GAIL approach improves the policy learned from VAE pre-training in a grid world environment. Sub-task policies learned by the approach guide the agent to reach an apple, with semantically meaningful navigation plans represented. The agent utilizes different latent variables for different sub-tasks, successfully combining navigation strategies to achieve objectives. The approach involves using macro-policy switches between latent codes to reach objectives. Experimentation is done in 5 continuous state-action environments, including Circle-World where the agent learns to draw circles in different directions. The state is represented by (x,y) coordinates and actions by unit vectors. Expert trajectories include actions for clockwise and anti-clockwise directions. The study explores multi-modal expert trajectories for different actions in various states, aiming to disambiguate phases in trajectories. Experiments on Pendulum, Inverted Pendulum, Hopper, and Walker environments in OpenAI Gym are conducted to test scalability. The approach aims to identify action primitives to aid successful task completion, comparing results with GAIL and supervised behavior cloning using a VAE. Expert trajectories are generated by training an agent with Proximal policy optimization. Using a VAE, expert trajectories were generated by training an agent with Proximal Policy Optimization. The method successfully segmented demonstrations into intuitive sub-tasks, allowing for the composition of new behaviors not seen in the expert data. The learned sub-task policies could be combined to perform tasks in different directions, as demonstrated in the Circle-World environment. The method successfully segmented demonstrations into intuitive sub-tasks, allowing for the composition of new behaviors. The sub-task policies can be utilized as a library of primitive actions, providing a significant benefit over monolithic policies. Results on the Pendulum environment show the network associating different latent variables with different sub-tasks based on state characteristics. In higher dimensional environments like Hopper and Walker, similar results were observed. Our proposed method identifies basic action primitives in higher dimensional continuous control environments like Hopper and Walker. It learns to assign separate latent variable values for different action primitives such as jumping, mid-air, and landing phases, improving performance over VAE pre-training. The approach shows comparable performance to existing methods. Our approach addresses compounding errors and segments demonstrations into sub-tasks with composable policies. Results on FetchPickandPlace-v1 show competitive performance. Mean returns over 100 episodes are compared using different methods. Further analysis and visualizations are provided in the Appendix. Videos of results on Hopper and Walker environments can be viewed at the provided link. In experiments on the FetchPickandPlace-v1 task in OpenAI Gym, agents trained using Directed-Info GAIL and baseline GAIL approaches failed to successfully complete the task despite correctly segmenting expert demonstrations. The robot would reach the object but struggle to grasp it in both approaches. Despite correctly segmenting expert demonstrations, agents trained using Directed-Info GAIL and baseline GAIL approaches fail to grasp the object in the FetchPickandPlace-v1 task. Stronger supervision may be needed to teach the agent the subtle action of grasping, as no other work has successfully trained GAIL on this task. The policy is additionally trained to minimize the L2 distance between the policy action and the expert action on states in the expert demonstrations. The discriminator and policy gradients are computed using Directed-Info GAIL loss, along with a batch of states from the expert demonstrations to train the policy. Our proposed approach, Directed-Info GAIL + L2 loss, outperforms baselines in training the policy. The reward function penalizes incorrect movements more than failed grasps, leading to a qualitative improvement in performance. Sample videos show a significant difference in success rates between our method and the baseline. Visit https://sites.google.com/view/directedinfo-gail/home#h.p_qM39qD8xQhJQ for more details. Our proposed method outperforms the baseline by providing the agent with sub-task codes to disambiguate similar states before and after grasping. Learning separate sub-task policies improves imitation learning in complex tasks with a hierarchical structure. The algorithm infers latent sub-task policies from unlabelled expert demonstrations by modeling imitation learning as a directed graph. It uses directed information in a generative adversarial framework to learn sub-task and macro policies, with theoretical connections to hierarchical reinforcement learning. Experiments show the method can segment expert demonstrations, learn sub-task specific policies, and combine them into a macro-policy. By maximizing directed information instead of mutual information, a posterior distribution over the next latent factor is learned given the discovered latent factors and trajectory. The objective is to train the policy network with a fixed q from VAE pre-training and minimize over the policy \u03c0. Parameters for VAE pre-training and Gumbel-Softmax distribution are set accordingly. In the Circle-World experiment, an additional loss term is added to penalize the number of times certain actions are taken. Adding a smoothing penalty term to the VAE pre-training loss helps the network segment expert trajectories more intuitively, reducing the number of latent variable switches. This prior encourages the network to switch between latent variables only when necessary, leading to better segmentation results. VAE pre-training loss introduces priors and biases towards human notion of sub-tasks. Low dimensional representations in Hopper and Walker environments cover 90% of variance in states, showing segmentation of trajectories based on similarity. The dimensionality of the sub-task latent variable affects the approach, with results showing similar action primitives even with a higher dimensional context. Context values for the state-space embedding are visualized, showing similarity to previous context values. The proposed approach can infer appropriate sub-task representations independent of the dimensionality of the context variable, as shown by the visualizations of context values obtained by PCA."
}