{
    "title": "BkxP2mnq8S",
    "content": "Compressed sensing aims to recover a structured signal $x$ from limited noisy linear measurements $y \\approx Ax$. Recent work has shifted focus from sparsity in a known basis to structure coming from a generative model $G: \\R^k \\to \\R^n$. Two results show the challenge of this approach, matching upper bounds for compressed sensing from $L$-Lipschitz generative models, requiring roughly $\\Omega(k \\log L)$ linear measurements for sparse recovery. Generative models generalize sparsity as a representation of structure by constructing a ReLU-based neural network that contains all k-sparse vectors in its range. In compressed sensing, structured signals x \u2208 R n are learned from limited linear measurements y \u2248 Ax. Linear measurements are easy in various settings like streaming algorithms, single-pixel cameras, genetic testing, and MRIs. The unknown signals x are compressible, requiring fewer words to describe. Compressed sensing relies on a formal notion of signal structure, often using sparsity to recover estimates x * of x from linear measurements y = Ax. In compressed sensing, structured signals x \u2208 R n are learned from limited linear measurements y \u2248 Ax. The focus is on achieving recovery guarantees with 3/4 probability using k-sparse vectors. Deep convolutional neural networks have been successful in learning richer models for signal structure, surpassing the limitations of sparsity. Generative models like GANs and VAEs can be used for compressed sensing by achieving recovery guarantees using only a few measurements. This approach, known as \"function-sparse recovery,\" shows that the recovered vector is nearly as good as the nearest point in the generative model's range. The main theorem states that this method is tight for any setting of parameters. The paper discusses the existence of a Lipschitz function G that requires a minimum number of linear measurements for recovery in generative models. It highlights the necessity of additive error \u03b4 in general Lipschitz model recovery compared to sparse recovery. Additionally, it introduces a Lipschitz neural network Gsp that contains all k-sparse vectors. The paper presents two key results: a lower bound for compressed sensing relative to a Lipschitz generative model and a generative model capable of containing all sparse vectors with a minimal number of measurements. The results demonstrate the power of generative-model representation over sparsity in structure recovery. The paper discusses techniques related to communication complexity and function-sparse recovery algorithms. It presents a generative model that includes a large, well-separated set of points and demonstrates the ability to identify points from a given set using the recovery algorithm. The proof involves showing how to identify points from Ax using the function-sparse recovery algorithm. The proof involves recovering z from y = Az by rounding z to the nearest element of Z with minimum distance R/ \u221a 6. By repeating the process, we can recover z t to z 1, learning t lg |Z| = \u2126(tk log(Lr/R)) bits total. If entries of A are rational numbers with bounded numerators and denominators, each entry of Az can be described in O(t + log n) bits. To address issues with the theorem statement, z is perturbed with additive Gaussian noise before discretizing Az. The algorithm requires recovering all t vectors with a certain probability, which can be achieved by using a reduction from the augmented indexing problem. This problem involves Alice sending Bob a message so that Bob can output a specific element with a certain probability, requiring a certain amount of communication. The lower bound states that the matrix A must have a certain size to satisfy the probability condition. The algorithm requires constructing a large, well-separated set Z within an n-dimensional ball of radius R. A function-sparse recovery algorithm is used with a Lipschitz function G. Certain binary linear codes yield points that are mutually far apart. An O(L)-Lipschitz mapping is constructed to a subset of these points. The algorithm constructs a mapping from R^k to R^n by applying a function coordinate-wise. This results in points in R^n lying in a ball of radius R but potentially being R/\u221a3k close. To address this, an error correcting code is used to select points that are mutually R/\u221a6 far apart. Additionally, a sparsity-producing generative model is created by mapping R^2 to positive 1-sparse vectors using unbiased ReLUs. The algorithm constructs a mapping from R^k to R^n by applying a function coordinate-wise. This results in points in R^n lying in a ball of radius R but potentially being R/\u221a3k close. To address this, an error correcting code is used to select points that are mutually R/\u221a6 far apart. Moreover, unbiased ReLUs are used to create a sparsity-producing generative model by mapping R^2 to positive 1-sparse vectors. This allows for the production of neural networks with disjoint activation ranges, enabling the recovery of k-sparse vectors with nonnegative coordinates. The algorithm constructs a mapping from R^k to R^n using a function coordinate-wise. A lower bound on communication complexity implies a lower bound on the number of bits used to represent Ax. Discretizing the measurement matrix by rounding does not change the resulting measurement significantly. It is assumed that the measurement matrix A is well-conditioned and orthonormal. The algorithm constructs a mapping from R^k to R^n using a function coordinate-wise. For well-conditioned matrices A with orthonormal rows, we can discretize the entries without changing the behavior significantly. This is achieved by rounding the matrix to b bits per entry, resulting in a matrix consisting of m orthonormal rows. The Augmented Indexing problem involves two parties, Alice and Bob, communicating to find a specific index in a string. The communication cost is minimized in this problem. The well-known theorem related to well-separated points is used to prove Theorem 2.1. Theorem 2.1 states that a large set of well-separated points can be obtained in the image of a Lipschitz generative model. By proving a k = 1 analog, it is shown that there exists a set of points in R^n of size 2^\u2126(n) with pairwise distances between 1/3 and 2/3. This result is then extended to arbitrary k while achieving the parameters in Theorem 2.1 through the use of an O(L)-Lipschitz map. The text discusses constructing a large subset of well-separated points in R^n using error correcting codes. By considering a prime subset A of points in P, a Reed-Solomon code is used to create a subset X with points that disagree in k/2 coordinates. This results in a distance of in 2-norm, expanding the set G = g \u2297k. The text discusses constructing a large subset of well-separated points in R^n using error correcting codes. By considering a prime subset A of points in P, a Reed-Solomon code is used to create a subset X with points that disagree in k/2 coordinates, resulting in a distance of in 2-norm. This leads to a lower bound for function-sparse recovery, proving Theorem 2.2 with specific conditions on points Z and G. The Augmented Indexing problem is then solved on instances of size d with communication cost O(m log n). Alice splits her string y into log n chunks, uses y j as an index to choose x j, and transmits A x to Bob. Bob can compute j from his input i, reconstruct x j+1 to x log n, and compute A z. Bob then computes A(w - s) to recover the correct point x j. Bob uses a more complex technique by choosing another vector u from B n (R/D j) and computing A(w - s - u) = A w - Au. He then runs the estimation algorithm A on A(w - s - u) to obtain \u0175. The distribution of w - u is independent of A, so running the recovery algorithm on A(w - u) would work with probability at least 3/4. Therefore, with probability at least 2/3, \u0175 satisfies the recovery criterion for w - u. This ensures that the distance between D j x j and \u0175 is strictly bounded by R/2 \u221a 6. Bob can correctly identify x j with probability at least 2/3 by using the message A x. The communication cost of this protocol is O(m \u00b7 log n), where m = \u2126(min(k log(Lr/\u03b4), n)). The set of all k-sparse vectors in R n is contained in the image of a 2 layer neural network, showing that function-sparse recovery is a generalization of sparse recovery. There exists a 2 layer neural network G with a simple construction, defining two gadgets G. The text discusses the construction of a 2-layer neural network G with two gadgets, focusing on the output nodes and internal nodes defined by ReLU activation. Trigonometry is used to show that certain conditions lead to specific outputs of G. The proof of Theorem 2.3 is also mentioned. The text discusses the construction of a 2-layer neural network G with two gadgets, focusing on the output nodes and internal nodes defined by ReLU activation. Trigonometry is used to show that certain conditions lead to specific outputs of G. The proof of Theorem 2.3 involves generating 1-sparse vectors using copies of G, which can be represented by a neural network with 2 layers. The algorithm from Theorem 3 allows for the recovery of the correct k-sparse vector using O(kd log(nk)) measurements, requiring only O(k log n) linear measurements for 2 / 2 (k, C)-sparse recovery."
}