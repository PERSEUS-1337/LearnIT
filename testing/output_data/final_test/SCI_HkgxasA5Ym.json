{
    "title": "HkgxasA5Ym",
    "content": "Obtaining reliable uncertainty estimates for neural network predictions is a challenge. Bayesian neural networks offer a solution, but specifying their prior is still open. Standard normal priors in weight space may lead to weak regularities, causing unforeseen generalization. Noise contrastive priors (NCPs) are proposed to improve uncertainty estimates by training the model to output high uncertainty for data points outside the training distribution. NCPs use input and output priors to add noise and create wide distributions. They are compatible with models that output uncertainty estimates, easy to scale, and provide reliable uncertainty estimates during training. Bayesian neural networks with Noise Contrastive Priors (NCPs) improve uncertainty estimates by preventing overfitting outside of the training distribution. This method shows scalability on the flight delays dataset and enhances predictions in real-world scenarios. Reliable uncertainty estimates are crucial in healthcare and for autonomous agents exploring their environment. Bayesian analysis provides a principled approach to modeling uncertainty in neural networks by placing a prior over the network's weights and biases. This helps in separating epistemic uncertainty (missing knowledge) from aleatoric noise (inherent randomness) in data generation. Active learning aims to acquire labels in regions of high uncertainty but low noise. Noise contrastive priors (NCPs) are introduced to address challenges in specifying priors for neural networks. NCPs encourage uncertainty outside of the training distribution through a loss in data space, aiming to improve generalization on out-of-distribution inputs. Noise contrastive priors (NCPs) are compatible with models representing functional uncertainty as random variables, easy to scale, and provide reliable uncertainty estimates for improved active learning performance. Unlike weight priors, data priors allow for expressing informative assumptions about input-output relationships, such as expressing high uncertainty on unfamiliar examples. The prior over a labeled data set {x, y} can be on x and another variable in the model, representing uncertainty. It takes the form p prior (x, y) = p prior (x) p prior (y | x). To prevent overconfident predictions, a good input prior should include OOD examples. A good output prior should be a high-entropy distribution. An out-of-distribution classifier model uses a binary auxiliary variable to determine if an input is OOD. Generating OOD inputs is challenging. OOD data is challenging to represent due to the need for uniformity in both input and complement distributions. An algorithm inspired by noise contrastive estimation is used to approximate the complement distribution with random noise. The hypothesis is that encouraging high uncertainty output near the boundary of the training distribution will extend to the entire OOD space. This eliminates the need to sample arbitrary OOD inputs, focusing instead on points close to the training distribution boundary. Noise contrastive priors enforce data priors on both input and output variables to prevent overconfident predictions. Noise contrastive priors enforce data priors on both training inputs and perturbed inputs by noise, using methods like randomly flipping features or adding Gaussian noise. Hyperparameters tune how far from the boundary we sample and the output uncertainty. The output mean determines the default prediction outside the training distribution. The noise contrastive prior method enforces data priors on training inputs and perturbed inputs by noise, aiming to make the model uncertain while generalizing to out-of-distribution inputs. The loss function includes a term representing a data prior, with a hyperparameter \u03b3 controlling the trade-off. This approach can be interpreted as inducing a function prior through the prior predictive distribution. The distribution p(\u03b8 |x,\u1ef9) shapes the belief over weights to make predictions highly variable outside of the training distribution. The prior acts as \"pseudo-data\" similar to classical work on conjugate priors, encouraging learning parameters that capture both training and prior data well. This method can be combined with other priors, such as the standard normal prior in weight. In this section, Noise Contrastive Priors (NCP) are applied to a Bayesian neural network (BNN) trained via variational inference. The NCP adds uncertainty to the model on out-of-distribution (OOD) inputs, extending the Bayes by Backprop (BBB) model with a NCP on the mean predicted by the neural network for a regression task with heteroskedastic modeling. The Noise Constrastive Priors (NCP) are applied to a Bayesian neural network (BNN) trained via variational inference. The weight prior is used only for the output layer to predict the mean, representing the model's epistemic uncertainty. The NCP is placed on the distribution of the mean, resulting in a loss function. The KL-divergence of the NCP loss can be computed analytically for the linear output layer. The NCP loss function applies weight regularization to regress network weights to a standard normal prior, improving generalization in-distribution. It encourages OOD generalization by matching the mean distribution to the output prior, resulting in high uncertainty on OOD inputs. NCP is sufficient as a prior for the BNN, with an alternative interpretation in the appendix. Priors for neural networks include entropic and hierarchical priors. Prior works have explored entropic and hierarchical priors for neural networks. Recent studies have introduced networks with latent variables to address uncertainty disentanglement. Additionally, general-purpose weight priors based on approximating Gaussian processes have been proposed. Some works have focused on priors for compression and model selection. NCPs take a functional approach by imposing explicit regularities in terms of the network's inputs and outputs, avoiding explicit beliefs over parameters for classification tasks. Classic methods involve adding noise to inputs for improved generalization, while output regularization is based on the maximum entropy principle. Regularization methods like label smoothing, entropy penalties, virtual adversarial training, and mixup aim to improve generalization within the training distribution. Classifying out-of-distribution inputs is a simple approach for neural network uncertainty, with noise contrastive estimation being a core method. Recent advancements include using GANs to generate out-of-distribution samples. These methods are orthogonal to NCPs, which focus on improving uncertainty estimates outside of the training distribution. NCPs focus on uncertainty estimates for active learning in regression tasks, using Bayesian principles and evaluating on various datasets. The method scales to large datasets and is sensitive to input noise. The implementation uses TensorFlow Probability and compares neural network models in an active learning setting. The models trained with NCP show decreasing test RMSE and NLPD, while baseline models overfit. The deterministic network quickly overfits, while the Bayesian neural network predicts mean and variance with weight uncertainty. Bayes by Backprop (BBB) is a Bayesian neural network trained via gradient-based variational inference with a standard normal prior in weight space. It is compared to Out-of-distribution classifier with noise contrastive prior (OCD+NCP) for active learning, where new data points are selected to maximize expected information gain. The objective function is higher where the model has high epistemic uncertainty and predicts low aleatoric noise. The text discusses using a softmax distribution to acquire labels with diversity, estimating aleatoric noise and epistemic uncertainty in Bayesian neural networks, and using OOD probability for the classifier model. Visualization experiments are conducted on a 1-dimensional regression task with a sine function. The text discusses active learning on a 1-dimensional regression task with a sine function, where models trained with NCP achieve lower NLPD on the test set. The distributional shift between training and testing data requires reliable uncertainty estimates to avoid mispredictions for OOD inputs. The text discusses using two layers of 200 hidden units, a batch size of 10, and a learning rate of 3 \u00d7 10 for all models in active learning. Models with Noise Contrastive Priors (NCP) outperform other baseline models in preventing overfitting and overconfident generalization. The study uses a flight delay dataset with 8 input variables and 700K training examples. The study uses a flight delay dataset with 700K training examples and 100K test examples. The test set has a distributional shift, and models with Noise Contrastive Priors (NCP) show better generalization. Bayesian neural network with NCP achieves lower RMSE than without, and uncertainty-based models outperform deterministic models. In addition to active learning experiments, a passive learning run on all 700K data points of the flights dataset was conducted to explore the scalability of NCP. The experiment showed that NCP is robust to input noise variations and consistently improves RMSE for tested noise sizes, yielding the best NLPD for noise sizes below 0.6. Our models significantly improve state of the art performance on the flights data set by using noise contrastive priors (NCPs) for neural networks. NCPs encourage network weights to capture high uncertainty on out-of-distribution inputs, leading to strong improvements over baselines and scalability to large regression tasks. Active learning for regression tasks benefits from the uncertainty estimates provided by NCPs. In future work, NCPs could be applied to settings like image classification and learning with sparse data. NCPs encourage uncertainty on out-of-distribution inputs and can be an alternative to Gaussian process priors. They are used in a Bayesian neural network model to capture function uncertainty. Another approach involves making explicit predictions about whether an input is out-of-distribution. The OOD probability of x is determined by p(o = 1 | x). If o = 0, the model outputs the neural network prediction, otherwise, a fixed output prior is used. The neural network weights \u03b8 are estimated with a point estimate, and the classifier prediction p(o | x, \u03b8) captures uncertainty. The NCP p(o | x, \u03b8) = \u03b4(o = 1 | x, \u03b8) assumes noised-up inputs to be OOD. During training, {x, y} and o = 0 are observed, and the loss function is determined analogously to a Bayesian neural network model. The OOD classifier model uses a single neural network with two output layers parameterizing Gaussian and binary distributions. The Bayes by Backprop model with NCP adds forward KL-divergence to the loss, reparameterizing the KL in weight space as a KL in output space. This allows specifying a prior in output space instead of weight space. Neural nets do not satisfy the assumption of a continuous and 1-1 mapping with respect to parameters, leading to multiple parameter vectors with the same predictive distribution. A compact reparameterization of the neural network would address this issue."
}