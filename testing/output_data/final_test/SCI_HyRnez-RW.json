{
    "title": "HyRnez-RW",
    "content": "Reading comprehension can be challenging, especially with longer or multiple evidence documents. Existing neural architectures often select a single passage for answers, limiting the ability to leverage multiple mentions of the answer. This work introduces lightweight models combined in a cascade approach to find answers, allowing scalability to larger evidence documents and aggregation of information from multiple sources. Our approach achieves state-of-the-art performance on the TriviaQA dataset by aggregating information from multiple mentions of each answer candidate across documents. Recent progress in reading comprehension is attributed to new large-scale datasets like SQuAD, driving neural approaches that build complex deep models. Short-term memory networks have shown impressive results on SQuAD, but struggle with large evidence texts like in the TriviaQA dataset. Existing approaches truncate documents, limiting access to necessary evidence spread throughout supporting documents. Short-term memory networks excel on SQuAD but face challenges with extensive evidence texts like in TriviaQA. Current approaches truncate documents, hindering access to crucial information spread across supporting documents. Approaches like greedy search for the best 1-2 sentences in a document, requiring access to multiple mentions for accurate answers. In this paper, a novel cascaded approach to extractive question answering is presented, achieving state-of-the-art performance on all tasks and metrics in the TriviaQA evaluation. The model consists of three levels using feed-forward networks applied to an embedding of the input. The first level uses simple bag-of-embeddings representations, the second level incorporates an attention mechanism, and the third level handles answer candidates mentioned multiple times in the evidence document. The novel cascaded approach to extractive question answering consists of three levels of submodels. The third level aggregates mention level representations to build a single answer representation. During training, all classifiers are trained using a multi-loss framework to prevent adaptation between features. The contributions include a non-recurrent architecture for processing longer evidence texts, aggregation of evidence from multiple mentions of answer candidates, and the use of a multi-loss objective. The experimental results show that key elements such as non-recurrent architecture, aggregation of evidence, and multi-loss objective are crucial for achieving state-of-the-art performance in reading comprehension. The model's use of feed-forward networks with fixed length window representations allows for significant parallelization, making it 45\u00d7 faster than recurrent models. Existing approaches typically rely on recurrent neural nets or memory nets with attention mechanisms to align questions with passages, but the model explored in this study offers a more efficient alternative. The BiDAF architecture BID22 is designed for SQuAD but does not scale to TriviaQA due to longer passages. Pointer networks with multi-hop reasoning have been used in various architectures for both datasets. Different approaches involve document truncation and scoring spans in paragraphs separately. This approach differs from existing question-answering architectures. Our approach for question-answering differs from existing architectures by using a cascade of simpler models, feed-forward networks for scalability, aggregating information at the representation level, and leveraging multiple correct answer spans in the document. For the reading comprehension task, we propose a cascaded model architecture with three levels. Submodels in lower levels use simple features to score answer spans, while higher level submodels select the best answer. The model uses simple features to score answer spans, with higher level submodels selecting the best answer using attention-based features and aggregating information from different occurrences of each answer candidate in the document. Each submodel scores potential answer span candidates in the evidence document using bags-of-embeddings. The final objective is a linear combination of all submodels' objectives, with a runtime analysis included. Input consists of question and document word embeddings, with candidate answer spans being consecutive word embeddings from the document. The approach uses candidate answer spans limited to a maximum length l, maintaining unique answer candidate spans and a mapping to corresponding unique answers. The task is to predict a single answer candidate from a set of correct answer strings. A meta-architecture of simple submodels organized in a cascade is described, inspired by the idea of separate classifiers using complementary features. This approach aims to scale linearly with document size and predict the best answer using attention-based features. The architecture consists of multiple submodels organized in a cascade, each returning a score that is fed to the next level. The models define distributions over answer spans and unique candidates. During training, the total loss is an interpolation of losses from each submodel, but during inference, a single prediction is made. The first level takes bags of embeddings as input and contains submodels that consider the span and question together. The first level of the architecture consists of two submodels: one considers the span and question together, while the other includes the local context of the span. Span embeddings are used, with a binary feature indicating if the span contains question tokens. The question + span component predicts the answer using a neural network on fixed length representations. The model uses weighted question embeddings to focus on important words in the question, generated by a two-layer feed-forward network. The scores are normalized to generate an aggregated question vector. The question + span model computes hidden layer representations and scalar scores for span candidates. The span + short context component considers words in the left and right context of a span for semantic modeling. Level 2 submodel aligns question embeddings with sentence embeddings using neural attention. The aligned representations from the decomposable attention model are used to predict if a sentence contains answers. Attention is applied to sentences instead of individual span contexts to manage computational requirements. Level 2 assigns different scores to spans in the same sentence by including level 1 representations. Sentence embeddings are defined as embeddings of words in a sentence, and similarity between question and sentence embeddings is measured to compute a similarity score. Attend and compare steps are utilized in this process. The original vector and its attended vector are concatenated and passed through a feed-forward net to obtain question-aware sentence vector and sentence context-aware question vector. Using these attended vectors as features, along with hidden layers from level 1 and questionspan feature, new scores and hidden layers are computed for level 2. Information is aggregated from candidate answer spans occurring multiple times, with hidden layers passed through a feed-forward net and summed using a unique span map. The hidden layer in level 3 is used to compute the score for each unique span in the document. To handle distant supervision, the approach leverages multiple correct answer occurrences by maximizing the probability of all such instances. The overall objective is to minimize the negative log likelihood of correct answer spans under all submodels, with hyperparameters to weigh the contribution of each loss term. The asymptotic complexity of the approach is briefly discussed. The complexity of the approach involves matrix-vector multiplication and feed-forward network operations. Each submodel has its own complexity level based on question and document lengths. The total complexity is determined by the lengths of the question, document, and spans. The approach's total complexity is O(nl\u03c1 2 + mn\u03c1), with nl and nm terms being parallelizable over the document length (n) and suitable for GPUs. The TriviaQA dataset contains 95k question-answer pairs from online sources, with evidence collected from the web and Wikipedia. Answers have aliases from DBPedia, occurring multiple times in the document. The dataset provides question-answer pairs with evidence from online sources and Wikipedia. Documents are tokenized and truncated to 6000 words and 1000 sentences. Sentences are limited to 50 words, and spans up to length 5 are considered. Cross-sentence spans are discarded for an oracle exact match accuracy of 95% on Wikipedia data. For the Wikipedia domain, training instances are created for each question with associated documents. GloVe embeddings of dimension 300 are used, not updated during training. Out-of-vocabulary words are hashed to random embeddings. Dropout regularization is applied to ReLU layers. Hyperparameters like network size, dropout ratio, learning rate, context size, and loss weights are tuned using grid search. The model was trained with specific hyperparameters like ratio (0.1), learning rate (0.05), context size (1), and loss weights. Adagrad BID6 was used for optimization, and each setting took 2-3 days to train on a single NVIDIA P100 GPU. Results showed state-of-the-art performance on both Wikipedia and web domains, outperforming more complex models. In the web domain, the model also surpassed approaches using multi-layer recurrent pointer networks with specialized memories. Table 2 presents ablations of our model components, with the final approach (3-Level Cascade, Multiloss) achieving the best performance. Using a single loss in level 3 (3-Level Cascade, Single Loss) significantly decreases performance, highlighting the importance of a multi-loss architecture. Combining submodels in level 1 into a single network (3-Level Cascade, Combined Level 1) shows high results but lacks consistency. Results of using smaller components of our model are also shown. The results show that the model without the aggregation submodel performs considerably worse, highlighting the importance of aggregating multiple mentions of the same span. Level 1 only models are the weakest, indicating that attending to the document is crucial for reading comprehension. The difference between smaller components becomes more pronounced in top-k predictions, but the full model outperforms the model without the aggregation submodel, suggesting the former may struggle to distinguish between the best answer candidates. The effect of truncation on Wikipedia indicates the need for more scalable approaches. The study on Wikipedia suggests the importance of scalable approaches for better performance on the TriviaQA task. The model predicts answers that frequently occur in the document and shows a close distribution to the gold answers. A speed comparison with a baseline LSTM model demonstrates the computational advantages of the approach as the document length increases. Our approach outperforms the LSTM baseline in terms of speed, showing a 8\u00d7 faster speed for a document length of 200 tokens and a 45\u00d7 faster speed for a maximum length of 10,000 tokens. The model predicts entities from the question and benefits from evidence context and aggregation for performance. Lower levels of the model make accurate predictions, but confusion between entities of the same type is observed, especially in the lower levels. Aggregating mention representations across the evidence text improves model performance. Our model benefits from evidence context and aggregation for performance, with lower levels making accurate predictions but struggling with entity-type confusion. Mention representations across the evidence text help improve model performance by selecting the correct answer. Context surrounding the span also plays a crucial role in model accuracy. Our model, a 3-level cascaded model for TriviaQA reading comprehension, utilizes feed-forward networks and bag-of-embeddings representations to handle longer evidence documents and aggregated information from multiple occurrences of answer spans. It achieved state-of-the-art performance on Wikipedia and web domains, surpassing complex recurrent architectures."
}