{
    "title": "rkxd2oR9Y7",
    "content": "Adaptive regularization methods modify full-matrix preconditioning for practicality and effectiveness in handling machine learning problems with a large number of parameters. Novel theoretical analysis is also provided. The algorithm GGT utilizes efficient inverse computation of square roots of low-rank matrices for adaptive regularization in non-convex optimization settings. Preliminary experiments show improved convergence rates across various tasks and deep learning benchmarks. Momentum, adaptive regularization, and variance reduction are key acceleration methods in stochastic gradient descent for deep learning. Adaptive regularization methods like AdaGrad and Adam use diagonal matrices for limited adaptive learning-rate adjustments. Full-matrix adaptive regularization, while theoretically promising, is hindered by high computational costs. GGT is a practical solution for full-matrix adaptive regularization, addressing computational challenges by efficiently applying the inverse square root of the second-moment matrix of recent gradients. It allows for better utilization of anisotropic curvature in loss landscapes, showing significant benefits over baselines in ill-conditioned problems. GGT's implementation results in faster training on deep learning benchmarks, particularly in complex landscapes like RNN training. Our algorithm provides theoretical guarantees for convergence to first-order critical points in a stochastic non-convex setting with adaptive regularization. It outperforms SGD in certain scenarios, especially in complex landscapes like RNN training. AdaGrad and Adam are popular adaptive optimizers in deep learning, with Adam being the most widely used for training modern deep models. The paper discusses a full-matrix drop-in replacement for Adam, extending to various variants like RMSprop, Adadelta, and Nadam. The challenge lies in the scalability of these methods due to the storage and inversion of square matrices in high-dimensional models. Matrix sketching has been used to approximate the AdaGrad preconditioner but can be sensitive to noise, resulting in overhead even with fewer model parameters. The authors propose a method using Kronecker products for AdaGrad with full-matrix preconditioners, avoiding the need for sketching or architecture-dependent restrictions. Their algorithm resembles L-BFGS and aims to forget past curvature using an exponential window, leading to a low-rank preconditioning matrix. Recent deep learning applications of second-order methods like BID32 and BID33 are intriguing but not within the scope of this paper. The role of adaptive regularization is a debated topic, with suggestions to switch from Adam to SGD at the end of training for better generalization. Adam's convergence has been scrutinized, but it still performs well in practice despite issues with outlier gradients. Analyses of Adam or AMSGrad are not used in this study. Several parallel works, including BID27, BID51, BID47, BID10, and BID40, have also been referenced. Our work is the first to characterize the advantage of adaptivity in optimization problems. Our main contribution is GGT, an efficient algorithm for full-matrix adaptive preconditioning. GGT utilizes the preconditioner from full-matrix AdaGrad, with gradient history attenuated exponentially like Adam, and truncated to a window parameter r. The mathematical specification of GGT is provided in Algorithm 1 for stochastic optimization. GGT is an adaptive optimizer for stochastic optimization, utilizing a small window of historical gradients for preconditioning. This approach allows for efficient full-matrix adaptive regularization, similar to Adam's coordinate-wise scaling. The algorithm provides the benefits of adaptivity while keeping the computational cost comparable to SGD. The preconditioning matrix in GGT is based on a small window of gradients, implying low rank. GGT computes the inverse square root of the empirical covariance matrix indirectly, leading to efficient implementation and provable guarantees for non-convex optimization. The window parameter r should be chosen based on the available RAM, with r = 200 used in large-scale experiments. The inversion of the large low-rank matrix GG can be done by diagonalizing the small matrix G G, along with GPU-friendly operations. The algorithm, using a cyclic buffer, takes O(dr 2 + r 3 ) time per iteration. The algorithm for storing and updating G t takes O(dr 2 + r 3) time per iteration and O(dr) memory in total. Experimental results show a running-time overhead of \u223c 1.3\u00d7 (CNN) and \u223c 2\u00d7 (RNN) compared to SGD. Practical suggestions for applying GGT include adding momentum to the gradient steps for better performance. In large-scale experiments, momentum with \u03b2 1 = 0.9 is used to compute the update step. Using v t instead of gradients for updating G t provides a small performance boost. Choosing \u03b2 2 = 1 has little impact as long as r T. Decoupling the scalars \u03b5 and 1/\u03b5 in the update step allows for tuning GGT's behavior. Adding a small multiple of the identity matrix to G G before eigendecomposition improves numerical stability. Empirical studies show that adaptive methods are beneficial in ill-conditioned scenarios. The text discusses the benefits of adaptive methods in ill-conditioned optimization problems and the value of limited gradient memory. It also evaluates the performance of GGT on larger-scale deep learning tasks and provides empirical insights on training dynamics in deep learning models. The experiments compare full-and diagonal-matrix adaptive optimizers with SGD in synthetic scenarios. In synthetic experiments, SGD is compared with adaptive optimizers without momentum, regularization, or learning rate schedule. Diagonal AdaGrad accelerates optimization, while full-matrix preconditioning converges the fastest in logistic regression tasks. In synthetic experiments, full-matrix preconditioning with cubic-time matrix inversion converges the fastest. Adding a window slightly improves convergence. An optimization problem highlights the importance of exponentially decaying gradient memory in minimizing the logarithmic barrier function of an anisotropic polytope. The experiments demonstrate how adaptive regularization can improve optimization in small-scale settings. In Section 3.4, the study connects proof-of-concept optimization instances to an empirical study of curvature in more realistic landscapes. The training dynamics of GGT on a deep architecture for computer vision are investigated, using a 26-layer 3-branch residual network with Shake-Shake regularization. This architecture, with low parameter count, achieves state-of-the-art classification accuracy. In each experiment, the architecture with low parameter count achieved state-of-the-art accuracy. Different optimizers were tested with varying results, with GGT consistently outperforming others in training loss. Data augmentation techniques were used, and results are shown in FIG2. The study found that SGD generalized slightly better than other optimizers towards the end of training. The gap was less dramatic compared to previous research due to the use of tuned learning rates and powerful regularization techniques. GGT was observed to reduce this gap slightly. Additionally, it was noted that AdaGrad's learning rate decay is too aggressive, leading to convergence to a poor solution. Further empirical work is needed to explore architectures synergistically tuned to optimizers. Using a low learning rate can improve the training loss curve but worsen generalization by over 3%. Training a 3-layer LSTM for character-level modeling showed significant improvement over baselines. The optimization task for this model may benefit from full-matrix regularization. The state of the art for character-level language modeling is less documented compared to word-level modeling. The end-to-end result of our model is competitive with recurrent models, outperforming Adam, AdaGrad, and SGD in wall-clock time. We experimented with GGT as a replacement for Adam but only saw improvement in the first 20 epochs. Adam performed better than GGT on attention-based architectures for NLP. In this section, insights from synthetic experiments and deep learning benchmarks are unified. Anecdotal observations on the evolution of preconditioner matrices' singular values are provided, with a visualization of the spectrum density of the low-rank preconditioner G t G t during training. The experiment shows that G t G t has a condition number of \u223c 10 3 for CNN and RNN training settings. This visualization offers a new perspective on CNN and RNN landscapes. The visualization of gradient spectra in CNN and RNN landscapes reveals distinct evolution patterns during training. The condition number of the CNN landscape increases towards the end, possibly linked to the low-rank structure of well-trained nets. Recurrent models exhibit a rapidly changing spectral structure early on, suggesting a more complex landscape. The high condition number (\u223c 10^6) correlates with the superiority of full-matrix preconditioning, indicating improved anisotropy. This study is the first to use covariance matrix of recent gradients to analyze changes in loss landscape curvature. In this section, the analysis of GGT is outlined, showing convergence to an approximate first-order critical point faster than SGD in some cases. The study focuses on adaptive regularization's improvement over the worst-case bound of SGD in stochastic optimization of a non-convex function. The adaptive ratio \u00b5 of an algorithm A is defined as DISPLAYFORM0, where x A is the output of A, and x * is a comparator. For AdaGrad, this ratio is always bounded by a quantity independent of T, potentially much smaller, and inversely proportional to the dimension in certain convex optimization problems. This provides a theoretical justification for the speedup of adaptive optimizers, even for strongly convex functions. Theorem 4.1 states that Algorithm 3 outputs an \u03b5-accurate solution for a bounded, Lipschitz, and smooth function with a stochastic gradient oracle. The introduction of the data-dependent adaptivity constant \u00b5 improves the rate of convergence, matching the classic O \u03b5 \u22124 rate. The work focuses on full-matrix adaptive regularization, making it viable for large-scale optimization with the GGT algorithm. The GGT algorithm is a scalable optimization algorithm with full-matrix adaptive preconditioning that accelerates optimization in ill-conditioned loss landscapes. It shows accelerated convergence on deep learning benchmarks and offers adaptive convergence guarantees. Theoretical benefits of adaptive regularization in a non-convex setting are also discussed, aiming to contribute to the large-scale optimization toolbox and deepen the understanding of loss landscapes in deep learning. The curr_chunk discusses the theoretical treatment of GGT in adaptive regularization for non-convex stochastic optimization. It introduces a version of GGT with a hard gradient memory window and presents a theorem on the advantage of adaptivity over SGD in achieving a first-order critical point faster. The setting of stochastic non-convex optimization is defined, with a bound on the number of stochastic gradient calls provided by Theorem A.2. The adaptive ratio \u00b5 controls the convergence rate of GGT. The curr_chunk discusses the adaptive ratio \u00b5 in stochastic optimization of non-convex functions with a bounded-variance stochastic gradient oracle. The objective is to find a first-order critical point with a Lipschitz gradient. The algorithm aims to minimize a smooth convex function by making a reduction to stochastic convex optimization. The adaptive ratio \u00b5 is defined as the ratio of the algorithm's performance in minimizing the convex function. The adaptive ratio \u00b5 in stochastic optimization of non-convex functions with a bounded-variance stochastic gradient oracle is defined as the advantage in convergence rate obtained by the algorithm compared to vanilla SGD. AdaGrad is a popular algorithm for this purpose due to its adaptive regularization. The bounds imply that \u00b5 can be small depending on the optimization problem's geometry. An example is provided for both diagonal and full versions of AdaGrad. In this section, Algorithm 3 is described, utilizing AdaGrad as a subroutine. The analysis uses an idealized version of GGT, replacing the gradient memory mechanism with a hard window. This modification allows for a more informative theory, incorporating the familiar theory of AdaGrad for convex optimization while addressing the need to forget past gradient information in adaptive non-convex optimization. The full-matrix AdaGrad algorithm is restated for clarity. The full-matrix AdaGrad algorithm, introduced by BID13, accumulates the second-moment matrix of all past gradients. Algorithm 3 utilizes AdaGrad as a subroutine and runs between restarts, updating x to be the output of Algorithm 2 on f t (x) for w steps. Algorithm 3 in the context of AdaGrad accumulates second-moment matrix of past gradients. Discrepancies with Algorithm 1 include absence of first-moment estimation and model averaging for theoretical guarantees in optimization. The addition of the \u03bb x \u2212 x t 2 term is also noted. The addition of the \u03bb x \u2212 x t 2 term in Algorithm 3 ensures iterates in each window do not move too far, allowing for a tight analysis for hard-window GGT. It is an artifact introduced to analyze each window as a fixed convex program, utilizing the convex theory of AdaGrad directly. This technique is similar to the one in the algorithm proposed by BID3. Additionally, from a \u03c3-bounded stochastic gradient oracle for f, it is easy to construct one for f t by adding \u22122\u03bbx t deterministically. Theorem A.2 states that for a non-convex function f with bounded second derivatives, the point x returned by Algorithm 3 satisfies E \u2207f (x) \u2264 \u03b5. The text discusses the use of GGT in training neural networks, specifically comparing it to diagonal-matrix adaptive methods. GGT shows comparable performance to SGD, with a slight gap in generalization. Large-scale empirical studies are presented, including training a 19-layer convolutional network on CIFAR-10 without residual connections or batch normalization. GGT shows comparable performance to SGD in training neural networks, with a slight gap in generalization. GGT trains rapidly after decaying the learning rate, but is outperformed by Adam in training and validation loss on a Transformer network for language modeling tasks. The value of using gradient correlations in training attention models appears to be limited."
}