{
    "title": "rJl8FoRcY7",
    "content": "The application of multi-modal generative models using a Variational Auto Encoder (VAE) is a new research focus for sensor fusion and bi-directional modality exchange. This study explores the learned joint latent representation and emphasizes the importance of expressiveness and coherence for multi-modal datasets. A multi-modal VAE is proposed based on the full joint marginal log-likelihood to capture the most meaningful representation for ambiguous observations. Additionally, a technique to generate correlated datasets from uni-modal ones is introduced to address the lack of available properties for multi-modal sensor setups. Auto Encoder (AE), Variational Auto Encoder (VAE), and Disentangled Variational Auto Encoder (\u03b2-VAE) have had a significant impact on generative models and deep reinforcement learning. VAEs encode data into linearly separable latent space features, allowing for generative joint models and zero-shot domain transfer in DRL. A good generative model should not only generate good data but also provide a coherent and expressive latent space representation, crucial for multi-modal approaches in sensor fusion setups. In this contribution, the focus is on investigating the latent space characteristics and quantitative features of existing multi-modal VAEs. A novel approach, M 2 VAE, is proposed for building and training a multi-modal VAE without simplifying assumptions. The generation of correlated multi-modal datasets from uni-modal ones is also discussed, along with connections to sensor fusion and active-sensing. The related work, comprehensive approach, dataset generation, and evaluation are detailed in different sections, leading to a conclusion in the final section. Variational autoencoder (VAE) combines neural networks with variational inference for unsupervised learning of complex distributions. A D-dimensional observation is modeled using a D-dimensional latent vector, with a probabilistic decoder and encoder network. The objective of VAEs is to maximize the marginal distribution, which is intractable. The encoder and decoder are trained jointly to approximate the distribution close to a prior. The model is trained using stochastic gradient variational Bayes to maximize the evidence lower bound of the marginal log-likelihood. Multi-modal Variational Auto Encoders have been used for generative models and feature extraction with different modalities. Conditional VAEs and conditional multi-modal autoencoders aim to improve bi-directional reconstruction. BiVCCA trains two VAEs with interacting inference networks for two-way reconstruction. Models based on variation of information estimate joint distribution with multi-directional reconstruction capabilities. Triplet ELBO is introduced for bi-modal VAEs. Multi-modal stacked Auto Encoders combine latent spaces for reconstruction of missing modalities. Training full multi-modal VAEs is challenging due to the complexity of inference networks. The VAE model is intractable due to the complexity of inference networks. Training the full joint model helps estimate expressive latent embeddings. A new model, JMVAE-Zero, is proposed for datasets with missing modalities, while JMVAE uses uni-modal encoders and a bimodal en-/decoder for training. The VAE model uses uni-modal encoders and a bimodal en-/decoder for training, aiming to build a coherent posterior distribution. Regularization by BID17 focuses on the shape of encoder distributions, leading to constraints on training modalities. In a tri-modal setup, only certain bi-modal encoder combinations can be derived from the VI. The VAE model utilizes uni-modal encoders and a bimodal en-/decoder for training, focusing on building a coherent posterior distribution. In a tri-modal setup, specific bi-modal encoder combinations can be derived from the VI. The objective is to find a meaningful posterior distribution for inference about further actions and to jointly train all permutations of modality encoders. The ELBO for the multi-modal VAE is derived by applying logarithm and Bayes rules to the independent set of observable modalities. The VAE model uses uni-modal encoders and a bimodal en-/decoder for training to create a coherent posterior distribution. In a tri-modal setup, specific bi-modal encoder combinations are derived from the VI. The ELBO for the multi-modal VAE is obtained by applying logarithm and Bayes rules to the observable modalities, leading to the derivation of conditionals and advantages of the technique. The VAE model uses uni-modal encoders and a bimodal en-/decoder for training to create a coherent posterior distribution. In a tri-modal setup, specific bi-modal encoder combinations are derived from the VI. The ELBO for the multi-modal VAE is obtained by applying logarithm and Bayes rules to the observable modalities, leading to the derivation of conditionals and advantages of the technique. To deviate from the common prior, \u03b2-VAE BID3 BID4 regularizers are applied with a constant normalized factor \u03b2 norm = \u03b2 * D * /Dz for balancing input and latent space. The M 2 VAE consists of 2 |M| \u2212 1 encoders and |M| decoders allowing bi-directional exchange of modalities. The VAE model uses uni-modal encoders and a bimodal en-/decoder for training to create a coherent posterior distribution. In a tri-modal setup, specific bi-modal encoder combinations are derived from the VI. The ELBO for the multi-modal VAE is obtained by applying logarithm and Bayes rules to the observable modalities, leading to the derivation of conditionals and advantages of the technique. To deviate from the common prior, \u03b2-VAE BID3 BID4 regularizers are applied with a constant normalized factor \u03b2 norm = \u03b2 * D * /Dz for balancing input and latent space. The M 2 VAE consists of 2 |M| \u2212 1 encoders and |M| decoders allowing bi-directional exchange of modalities. The model allows arbitrary modality combinations with different weights, where major subsets dominate the shaping of the posterior distribution. The VAE model uses uni-modal encoders and a bimodal en-/decoder for training to create a coherent posterior distribution. In a tri-modal setup, specific bi-modal encoder combinations are derived from the VI. The ELBO for the multi-modal VAE is obtained by applying logarithm and Bayes rules to the observable modalities, leading to the derivation of conditionals and advantages of the technique. To deviate from the common prior, \u03b2-VAE regularizers are applied with a constant normalized factor for balancing input and latent space. The M 2 VAE consists of multiple encoders and decoders allowing bi-directional exchange of modalities. The model allows arbitrary modality combinations with different weights, where major subsets dominate the shaping of the posterior distribution. In contrast, creating own multi-modal datasets is exhaustive since training generative models either demand dense sampling or supervised signals to form a consistent latent manifold. We propose a consolidation technique by sampling from superimposed latent spaces of various uni-modal trained CVAEs to generate multi-modal datasets from distinct and disconnected uni-modal sets. Additionally, a bi-modal mixture of Gaussians (MoG) dataset is proposed to show particular behaviors of various VAE approaches. Hebbian learning relies on continuous transformation of objects in observable space. Assumptions in latent manifold learning require coherent manifold and proper factorization for training. Data must exhibit continuous transformation properties for successful training. This assumption is adopted for multi-modal datasets to ensure correlations between observations. Modalities play a crucial role in multi-modal sensor fusion by ensuring correlation and coherence. A technique is proposed to generate new multi-modal datasets from uni-modal sets meeting specific conditions. The VAE's learned posterior distribution matches the desired prior effectively for single class observations. The CVAE builds non-related posterior distributions for each class label, while the \u03b2-VAE learns disentangled latent representations. The approach combines advantages of disentangled and factorized latent representations to superimpose latent manifolds from uni-modal encoders. Latent samples are drawn from the posterior to operate CVAE encoders and generate multi-modal data. MNIST and fashion-MNIST datasets are consolidated into an entangled-MNIST set. A bi-modal JMVAE is trained on the newly generated data to depict dataset properties. The consolidation of uni-modal datasets is challenging due to the difficulty in measuring continuity, resulting in mixed datasets like mixed-MNIST. To compare ELBO fairly, fashion-MNIST is shuffled per class label to create mixed-e-MNIST (me-MNIST). The JMVAE's latent space shows that uni-modal trained encoders have orthogonal variances, while e-MNIST exhibits desired behavior for multi-modal datasets. This approach generates new entangled data for training. Our proposed approach for generating new entangled datasets meets the requirements of multi-modal datasets by investigating a Mixture-of-Gaussians distribution to mimic bi-modal observations. The focus is on the ambiguous resolving properties of the VAE, depicted in an artificial experiment. Multi-modal sensor setups for complementary fusion exhibit similar behavior, rectifying various modalities to achieve a complete view of the scene. The generative process with class labels as factorized latent state representation is mimicked by the MoG-Experiment. The MoG-Experiment mimics the generative process with class labels as factorized latent state representation. Various datasets are used to test the capabilities of the M2 VAE, evaluating the ELBO of different approaches on the e-MNIST dataset. VAE architectures are compared qualitatively and quantitatively, focusing on the impact of parameter sets on the latent space representation. The impact of parameter set (\u03b2 * , \u03b1) on the M 2 VAE is significant. A direct connection (\u03b1 = 1) leads to collapsed classes in the multi-modal latent space. However, with \u03b1 10 \u22122, the encoders find an expressive latent space distribution. High \u03b2 values result in entangled factors, while small normalized \u03b2 norm 10 \u22122 show robust disentanglement. This behavior is similar in the M 2 VAE. The impact of \u03b2 on the M 2 VAE is significant, with small values chosen to relax learning pressure. Diverging \u03b2 parameters result in lower ELBOs, but equal learning pressure is argued for all encoders. Reconstruction loss causes learning of mean representatives of classes. The loss of the M 2 VAE's objective leads to the learning of mean representatives of classes in the observation space. This can result in the collapse of classes to a single mean value in the latent encoding. The ELBO for the observation increases, indicating the quality of the embedding. This insight could be useful for tasks requiring ambiguity resolution, where the ELBO can be used as a signal for epistemic exploration in unsupervised reinforcement learning approaches. FIG3 is qualitatively estimated and can only be compared between encoders of the same approach. The proposed M2VAE showed the most coherent latent space distribution compared to JMVAE-Zero and tVAE. JMVAE-Zero learned similarities between encoders but a new embedding for classes (1,2), while tVAE achieved coherence by training a full multi-modal VAE first. However, the ELBO per embedding did not provide direct conclusions between the encoders. The M2VAE enforces encoders to approximate the same posterior distribution, leading to strong coherence between embeddings. Classes in multi-modal latent embedding collapse to mean values in uni-modal ones. Ambiguous embeddings in M2VAE result in higher reconstruction loss. Class (0) is ambiguously detectable in the embeddings. In multi-modal VAEs, class (0) is ambiguously detectable in uni-modal embeddings. In-place sensor fusion is introduced for distributed active-sensing tasks, where latent space representation can be interpreted as an inverse sensor model. This compressed information can be efficiently transmitted and updated between sensing agents. The VAEs have a natural denoising characteristic that re-encodes information in a better version, shown in Fig. 6. The latent space discrimination is evident with high entropy separating clusters. Initial z values are auto re-encoded, converging to fixed-points in the latent space manifold. The proposed M 2 VAE shows coherent latent spaces, while JMVAE-Zero and tVAE do not. This affects sensor fusion accuracy. The ELBO was estimated to evaluate the models' performance. The proposed M2 VAE achieves the highest Evidence Lower Bound (ELBO) value and learns an expressive latent space distribution. It provides crisp reconstructions when sampling from the latent space for data generation. This work introduces a novel multi-modal Variational Auto Encoder trained on Mixture-of-Gaussian and complex datasets from MNIST and fashion-MNIST. The study introduced a novel multi-modal Variational Auto Encoder trained on datasets from MNIST and fashion-MNIST. They formulated requirements for sensor fusion and developed a technique for learning new datasets, entangled-MNIST. Additionally, they explored in-place sensor fusion in active sensing scenarios using VAEs, which denoise data and exhibit attractor behavior in latent space. The proposed model achieved the highest ELBO values, indicating a good latent representation. Future work will focus on integrating ambiguous resolving characteristics for epistemic exploration. The study introduced a novel multi-modal Variational Auto Encoder trained on datasets from MNIST and fashion-MNIST, achieving high ELBO values. Future work will focus on integrating ambiguous resolving characteristics for epistemic exploration. The proposed approach by BID17 can be extended to multiple modalities. The log-likelihood of three modalities can be expressed using ELBOs and KL divergence. The ELBO L 3M 2.6.1. log can be derived by substituting log-likelihoods as per the expressions in Sec. 3. The marginal log-likelihood for the variation of information (VI) can be expressed as log p(m|M \\ m) for any set M. The joint log-likelihoods of subsets of M can be combined to get log p(M \\ m). The latent space prior in all VAEs is a Gaussian with unit variance, and they sample from a Gaussian variational distribution. The VAEs sample from a Gaussian variational distribution parametrized by encoder networks. Architectures used in the paper are summarized in Tbl. 2. Reconstruction loss for ELBO calculation is binary cross-entropy for e-MNIST and root-mean-squared error for MoG. CVAE for e-MNIST includes convolutional layers, fully-connected layers, and one-hot vector labels. Architecture details are provided in Tbl. 3."
}