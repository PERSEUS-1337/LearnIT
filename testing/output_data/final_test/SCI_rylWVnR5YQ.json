{
    "title": "rylWVnR5YQ",
    "content": "We propose a modification to traditional Artificial Neural Networks (ANNs) by introducing modulators that mimic biological neuromodulators. These modulators allow traditional ANN nodes to adjust their activation sensitivities based on input patterns, leading to context-dependent activation functions. This modification shows significant improvements in Convolutional Neural Networks and Long Short-Term Memory networks. Artificial neural networks (ANNs) like CNNs and LSTMs have various applications in computer vision, natural language analysis, and speech recognition. Collaboration with Psych-Neuro communities has greatly benefited the development of ANNs. Traditional ANN nodes have rigid information processing capabilities compared to real neurons. Popular activation functions include sigmoid, tanh, and ReLU. In this study, a new modification to ANN architectures is proposed by adding modulators to modulate the activation sensitivity of the ANN nodes. This allows for individual biological neurons' capability to conduct complex nonlinear mappings from inputs to outputs to be replicated in deep ANNs. The proposed modification to ANN architectures involves adding modulators to modulate the activation sensitivity of the nodes, replicating biological neurons' capability to conduct complex mappings in deep ANNs. This modification captures neuronal principles such as intrinsic excitability, firing modes, firing rate integration, and neuromodulation, relevant to attention mechanisms and gating mechanisms in LSTM cells. The modulator mechanism adjusts the sensitivities of target ANN nodes in run-time by changing activation function slopes, complementing attention and gate mechanisms. It helps achieve better test stability and performance with simpler models. Two modulation mechanisms were designed for CNNs and LSTMs. The modulator mechanism adjusts the sensitivities of target ANN nodes in run-time by changing activation function slopes, complementing attention and gate mechanisms. It helps achieve better test stability and performance with simpler models. Two modulation mechanisms were designed for CNNs and LSTMs. Specifically for CNNs, a layer-specific modulator is used to modulate the outputs of CNN nodes before activation, similar to neuromodulation in biological systems. This modulation mechanism is further expanded for Densely Connected CNNs by adding modulators to each dense block layer. The modulator mechanism adjusts the sensitivities of target ANN nodes in run-time by changing activation function slopes. In addition to the Cellgate, traditional LSTM cells have three modifying gates (Forget, Input, and Output) that regulate the cell state. The modulating LSTM introduces a new \"modulation gate\" for easier implementation. The modulator mechanism in LSTM cells includes a new \"modulation gate\" for adjusting sensitivities of target nodes. This gate functions similarly to neuronal facilitation and depression, allowing for node-specific sensitivity adjustment. The modulator can take outputs from other CNN layers or LSTM cells as inputs, and each CNN layer has a modulator that maps input into a scalar modulation signal. This signal is then multiplied with the integration of inputs before traditional activation. In experiments with CNNs, modulation was applied to dense block layers in DenseNet models using a sigmoid activation function. Different settings were tested, including vanilla CNNs, vanilla DenseNets, and vanilla DenseNet-lites compared to their modulated counterparts. The modulation affected the transformation activity of subsequent dense block layers. The CNN model consists of 2 convolution blocks with sequential layers, pooling, and dropout. A fully connected layer with 512 nodes is added at the end. The first block has 32 filters of size 3x3, while the second block has 64 filters. A dropout of 0.25 is applied to each block. The vanilla DenseNet used a structure with 40 depth and 12 growth-rate, with a dropout of 0.5. The vanilla DenseNet-lite has a similar structure but with a growth-rate of 10, resulting in 28% fewer parameters. Modulators were added to the CNN, DenseNet, and DenseNet-lite to obtain their modulated versions. The modulated networks have slightly more parameters than their vanilla versions. Experiments were run for 150 epochs on 4 NVIDIA Titan Xp GPUs with a mini-batch size of 128 using data augmentation. The CNN models were trained using the Adam optimization method with a learning rate of 1e-3. The vanilla CNN model starts to overfit after 80 training epochs, while the modulated CNN model, although slightly more complex, is less prone to overfitting. The modulated CNN model, despite being slightly more complex, is less prone to overfitting and outperforms its vanilla counterpart by a large margin. Modulation significantly improves training, validation, and test results for DenseNets. The modulated DenseNet/DenseNet-lite models consistently outperform their vanilla counterparts, with the modulated DenseNet-lite model even surpassing the vanilla DenseNet model despite having fewer parameters. The LSTM experiments involved using the NAMES dataset to classify ethnicity and the SST2 dataset to classify movie reviews as positive or negative. Three versions of vanilla LSTMs were created for fair comparisons. Control 1 had an identical total LSTM cell size, Control 2 had the same number of nodes per layer, and Control 3 had an extra Input gate for an identical number of nodes. For the first experiment, a modulated LSTM with an extra Input gate was compared to control LSTMs in terms of parameters and hyper-parameters. The model was tested on a name categorization dataset with parameters ranging from 4.1 K to 6.4 K. The experiment was repeated 30 times and referred to as Simple-LSTM. The hidden dimension was set to 150, batch size to 5, embedding dimension to 300, initial learning rate to 1e-3, no learning rate decay, Adam optimizer with no dropout, and 100 epochs were collected. The number of parameters ranged from 57.6 K to 90 K. The experiment was repeated 100 times and referred to as Advanced-LSTM. Both modulated LSTMs outperformed control groups in test performance. Statistical significance varied between the two LSTM models. The Advanced-LSTM with tanhshrink modulation consistently showed statistical significance (P<.001) in all conditions, with lowest variance in the modulated condition. The modulation mechanism added new aptitudes for learning and generalization, stabilizing the input-output relationship. The addition of a modulation mechanism to traditional ANNs allows for context-dependent activation functions, leading to consistently improved performance compared to original versions. Modulated models also show reduced overfitting and can perform on par with larger vanilla versions. Future studies will explore the relationship between model parameters and network performance. The interaction between network implementations and Activation Function wrapping for slope-determining neurons is important. Investigating layer-wide single-node modulation on models with parallel LSTM's could be useful. Epigenetics involves gene activation and inactivation influenced by environmental factors, affecting cellular proteins like ion channels. Modulation of these proteins impacts neuron firing and function. These proteins can also influence epigenetic expression. Comparing neuron and node output from various signals can provide insights. Intrinsic excitability affects neuron firing rate and plasticity. Neurons integrate information in Type 1 (continuous) and Type 2 (discontinuous) modes, influenced by interneuron communication. Our modification controls neuron firing modes, allowing for the switch between continuous and binary information encoding. This is analogous to how artificial neural network nodes output neurotransmitters. Our modification enables a network to use previous activity to determine its current sensitivity to input, allowing for both Facilitation and Depression in neurotransmitter release. Neuromodulation plays a key role in this process, as research has shown that the same neuron or circuit can exhibit different responses based on the concentrations of various neuromodulators. The slope of the activation function may be likened to the mechanism of neuromodulation. The slope of the activation function acts as a form of neuromodulation, adjusting the gradient for dynamic back-propagation in Machine Learning. It can impact node output differently based on the activation level, allowing for information extraction beyond the standard range. By implementing a conditional slope based on node input, a wide range of functional outputs can be generated. Injecting noise can help deep neural networks with noisy datasets, acting as a stabilizer for neuronal firing rates. Increased clustering in two-dimensional node-Activation space is observed when the Activation Function slope is dynamic, suggesting noise may improve network performance through stabilization. Nodes in LSTMs and CNNs benefit from added complexity in their input-output dynamic, mimicking the functionality of neuromodulators and enhancing network performance. The text discusses how adding complexity to neural networks can improve performance by mimicking neuromodulators. The exact mechanism is unknown, but further research is needed to explore non-computationally-demanding methods. Additionally, a modulator gate was tested on a three-layered LSTM network with promising results on the Penn-Treebank dataset. The model outperformed template perplexity on the Penn-Treebank dataset, achieving an average of 58.4730 compared to the template average 58.7115. The limited sample size was due to long training times and resources constraints. The lack of control for model parameters requires the assumption that the author fine-tuned network parameters for optimal performance."
}