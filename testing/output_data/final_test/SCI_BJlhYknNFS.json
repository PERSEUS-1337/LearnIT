{
    "title": "BJlhYknNFS",
    "content": "We introduce a more efficient neural architecture for amortized inference by combining continuous and conditional normalizing flows with a principled choice of structure. The gradient flow is derived from the minimally faithful inverse of the underlying graphical model, reducing parameters in the neural network and adaptive integration steps in the ODE solver. This increases throughput at training and inference time without sacrificing performance compared to unconstrained flows. The structural inversion and flow construction are expressed as compilation passes of a probabilistic programming language, applicable to stochastic inversion of models like convolutional neural networks. The flow for amortized inference combines continuous and conditional normalizing flows with a principled choice of structure, reducing parameters in the neural network and adaptive integration steps in the ODE solver. This increases throughput at training and inference time without sacrificing performance. The structural inversion and flow construction are expressed as compilation passes of a probabilistic programming language, applicable to stochastic inversion of models like convolutional neural networks. The pipeline involves three program transformations: translating a generative process into a graphical model, computing its inverse structure, using it as a sparsity pattern for a new neural network architecture, and training the resulting flow with a symmetrized KL loss. The faithful inversion algorithm is applied to obtain a correct dependence structure for the inverse model, minimizing moralizing edges to capture all correlations in the posterior. The pipeline involves transforming a generative process into a graphical model, computing its inverse structure, using it as a sparsity pattern for a new neural network architecture, and training the resulting flow with a symmetrized KL loss. Amortized inference techniques yield efficient posterior approximations by training function approximators on losses defined using the generative model and training data. The general framework for inference amortization involves a neural ordinary differential equation system that transforms a reference distribution to the desired target density. This transformation is parametrized on latent particles, with conditioning achieved by providing a constant control input. Numerical computation at inference time is performed by integrating particle trajectories along the dynamics to approximate posterior samples. The log-probability of each particle must also be integrated to obtain a normalized distribution at the end of the flow. Our work focuses on inference amortization using a neural ordinary differential equation system in the latent space. The flow network architecture is constrained to respect the statistical independence structure by masking the weight matrix with the adjacency of the graphical model. The architecture of the neural network model respects the statistical independence structure by masking the weight matrix with the adjacency of the graphical model. The optimization objective is the symmetrized Kullback-Leibler divergence in expectation over training data. The optimization objective is the symmetrized Kullback-Leibler divergence in expectation over training data, where the forward KL term measures the quality of density estimation on the support of the true posterior, and the reverse KL term incentivizes samples from q \u03a6 to behave similarly to the true posterior. Efficient estimation of this objective is possible when the joint model is available and the variational posterior q \u03a6 is reparametrized. In experiments, a significant performance improvement is observed over using only the forward or reverse KL for training. The different architectures in the study use similar numbers of dimensions for latent spaces, with FFJORD and flows with full connectivity achieving competitive performances. The sparsity structure with faithful inversion trains faster and has fewer parameters, showing a more appropriate inductive bias. A control experiment with randomized sparsity structure performs poorly, highlighting the importance of faithful inversion. The symmetrized loss improves learning signal by combining mode-seeking and mass-seeking behavior. It outperforms reverse KL-based loss for training on complex models like arithmetic circuits. Forward KL quickly saturates on the task. The symmetrized KL provides a faster learning signal compared to the reversed KL, improving to below 10 nats. This is crucial as it optimizes both density estimation and sampling behavior. The CNF can only be trained effectively with the symmetrized KL, showing consistent benefits in all experiments. The text discusses stochastic deconvolution using a flow model with a sparsity pattern. It demonstrates amortized inference for image convolution, showing how output pixels are generated from filter weights and input patches. The inversion structure reveals statistical dependencies between input pixels, filter weights, and output values. The model is trained on randomly cropped data. The inference artifact is trained on randomly cropped real image patches from the MNIST digit classification dataset, amortizing over all possible convolutional filters without weight sharing. A qualitative consistency check is performed by reconstructing outputs from samples of the approximate posterior."
}