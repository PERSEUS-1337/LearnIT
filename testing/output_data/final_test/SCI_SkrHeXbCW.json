{
    "title": "SkrHeXbCW",
    "content": "Nearest neighbor algorithms' performance in high dimensions depends on data structure. With the rise of neural network-learned representations, the interaction between these algorithms and neural networks is studied. The network architecture can significantly impact nearest neighbor efficacy, even with unchanged classification accuracy. Proposed training modifications can improve datasets for nearest neighbor algorithms. Our modifications in learned representations accelerate nearest neighbor queries by 5x, focusing on computational efficiency in processing large databases of images, texts, and videos. The primary goal of learning representations is to enable accurate labeling by classifiers, but additional goals like efficiency are becoming increasingly important. In this paper, the focus is on learning representations for similarity search in large databases of images, texts, and videos. Nearest Neighbor Search (NNS) is a fundamental algorithmic problem with applications in machine learning and data science, including web search, recommender systems, and face recognition. The challenge lies in designing efficient NNS methods that interact effectively with the data. Recent progress in deep learning is replacing hand-crafted feature vectors with learned representations for Nearest Neighbor Search (NNS). This shift leads to higher accuracy and more meaningful results, raising questions about the performance of existing NNS algorithms on these new feature vectors. Recent progress in deep learning is replacing hand-crafted feature vectors with learned representations for Nearest Neighbor Search (NNS). The focus is on whether existing NNS algorithms perform well on these new classes of feature vectors and exploring the design space to learn representations suited for fast NNS. The goal is to preprocess a dataset of points to quickly answer queries about minimizing distances. Theoretical analysis of Nearest Neighbor Search (NNS) algorithms, focusing on locality-sensitive hashing (LSH) for cosine similarity. Analyzing the impact of distance gaps on query time quantitatively. Study on running time of hyperplane LSH in NNS problem. The expected number of candidates considered in a nearest neighbor search grows with the angle \u03b1 between the query vector and the nearest neighbor. Smaller angles result in significantly faster NNS, as shown by concrete numbers. For example, improving the angle from \u03c0/3 to \u03c0/4 for a dataset of size n = 10^6 reduces the number of nearest neighbor candidates from 16K to 1.5K, a 10\u00d7 speed-up. Theoretical analysis shows that smaller angles between query vectors and nearest neighbors result in faster nearest neighbor searches. Empirical comparisons of two state-of-the-art NNS methods, Annoy and FALCONN, demonstrate similar behavior to the theoretical analysis. The cost of approximate nearest neighbor searches is analyzed in terms of distance computations and total query time on a laptop computer. The dataset consists of random unit vectors with queries planted at specific angles. Query times for finding the nearest neighbor are reported with a 90% success rate. The query times of ANN implementations align with theoretical predictions, showing improvement over linear scan baseline. The angle between query and nearest neighbor is crucial for NNS performance. Current ANN algorithms only outperform linear scan at angles around 60 degrees with 90% accuracy. For larger angles, maintaining high accuracy and speed is challenging. Modifications to neural networks are explored to enable faster NNS by focusing on learning representations with small angles. Neural networks in domain X predict labels y \u2208 [C] using training examples. The network consists of weights \u03b8, class vectors {v y }, and a bias vector b. The softmax loss function is commonly used for training. The goal is to achieve a small angle between query and nearest neighbor for faster NNS performance. The training process aims to achieve a small loss by maximizing the correct dot product between \u03c6 \u03b8 (x) and v y, while minimizing other dot products. Three key properties of the loss function are highlighted, including the influence of class vector norms on model predictions, the irrelevance of representation vector norms for 0-1 prediction loss, and the benefit of a small angle between the representation vector and class vector for prediction accuracy. The training process focuses on maximizing the correct dot product between \u03c6 \u03b8 (x) and v y, while minimizing other dot products. It is important to have well-aligned representation vectors with the correct class vectors for better prediction accuracy. The mean correct angle between the representation vector and the correct class vector is a key metric that is improved through modifications. In Section 2, the angle is a key metric that influences speed-ups in ANN algorithms. Modifications are evaluated on medium and larger datasets like multiMNIST with 1,000 to 10,000 classes to confirm intuitions and assess training impacts. In Section 2, the angle is a key metric that influences speed-ups in ANN algorithms. We investigate the impact of three training modifications on datasets with 1,000 to 10,000 classes. Scaling the representation vectors can change the loss function without affecting prediction accuracy. Different options for controlling the norms of the representation vectors are considered, such as Layer Normalization and Scaling. Batch normalization (Ioffe & Szegedy, 2015) ensures mean 0 and variance 1 for each activation, with experiments showing better results in accuracy, convergence speed, and angles compared to other options. Using batch norm does not impact accuracy on the evaluation set. Batch normalization ensures mean 0 and variance 1 for each activation, improving accuracy, convergence speed, and angles. The representation vector in a ReLU-based neural network is restricted to the non-negative orthant, affecting the locations of class vectors in the softmax. To address the issue of class vectors being restricted to the non-negative orthant in softmax, a modification is proposed to remove the last layer ReLU or place it before batch norm. This allows the final representation vector to have both positive and negative coordinates, improving angle accuracy on multiMNIST. Ensuring equal norm for all class vectors in softmax aids in easier NNS problems and better distance measure comparability. The third modification proposed involves constraining the norms of class vectors to be equal, achieved through projected gradient descent. This ensures an angular NNS problem and helps in achieving smaller angles in the model. The second modification in Equation (2) sets the second term to 1, improving the mean correct angle and reducing class vector norms on the multiMNIST dataset. This cumulative improvement results in an angular NNS problem with better conditioning. The training modifications are evaluated on two large datasets, including a multiclass variant of the MNIST dataset. The multiclass dataset multiMNIST_C is created by concatenating images from the MNIST dataset. The average norm of class vectors is reduced and the mean correct angle improves with training modifications on multiMNIST dataset with 10,000 classes. The positive effects are cumulative with previous modifications. Our model architecture is based on the MNIST model with two 5 \u00d7 5 convolutional layers followed by batch normalization, ReLU, and MaxPool. The network is trained with stochastic gradient descent on a single machine with a GPU. Experiments are also conducted on the Sports1M dataset containing about 1 million videos. In Vijayanarasimhan et al. FORMULA5, a multiclass problem is constructed using video frames as examples and video ids as labels. The frames are converted to VGG features using a pretrained VGG16 network, with a fully connected layer of size 1024 and a softmax layer of dimension 256. Training is accelerated using a sampled softmax loss approximation with 8192 samples. The models are trained using a distributed TensorFlow setup with 15 workers. Our models are trained using a distributed TensorFlow setup with 15 workers and one GPU each. The goal of our experiments is to improve the angle between representation vectors and class vectors without sacrificing model accuracy. We also examine the \"well-distributedness\" of class vectors and measure imbalance that affects the run time of an LSH-based nearest neighbor scheme. The results of our experiments on multiMNIST_100K are shown in FIG8. Our training modifications on the multiMNIST_100K dataset significantly decrease the mean correct angle from 40 to 15, resulting in a 30\u00d7 faster softmax evaluation at inference time with no loss in accuracy. On the Sports1M dataset, our modifications improve accuracy and the distribution of points on the unit sphere, leading to better performance after 600K iterations. Our modifications on the Sports1M dataset improve accuracy after 600K steps. Placing batch normalization after the non-linearity is crucial, resulting in a 10\u00d7 improvement in inference speed. The second moment of table loads relative to the ideal value increases slightly, staying around 1.4\u00d7 for the best accuracy configuration. Our experiments show significantly smaller angles between representation vectors and correct class vectors, leading to a tenfold improvement in inference speed without sacrificing overall accuracy. Our study demonstrates modifications to neural network training and architecture that reduce the angle between learned representation vectors and class vectors, leading to a 5\u00d7 speed-up in query times for nearest neighbor algorithms. Future research could explore faster training of large multiclass networks. The FALCONN library shows a 5\u00d7 speed-up in query times for nearest neighbor algorithms, while the Annoy library does not reach relative accuracy 1. Our training techniques result in a softmax more compatible with the kd-tree algorithm, providing faster query times for fixed accuracy. The paper by Liu et al. (2016) focuses on training with larger angular distance gaps but modifies the loss function instead of the training process or network architecture. Their emphasis is on classification accuracy, not fast similarity search. The paper focuses on fast similarity search by evaluating network modifications using state-of-the-art NNS methods. They use a synthetic example to illustrate the effect of their changes on the softmax layer geometry. The network trained has two hidden layers with ReLU non-linearities and is optimized using AdaGrad. The paper demonstrates the impact of network modifications on the softmax layer geometry using a synthetic example with two hidden layers and ReLU non-linearities. Training is done with AdaGrad, leading to pre-softmax vectors with similar norms and improved angle alignment with correct class vectors. The results are visualized in FIG9, showcasing the effectiveness of batch normalization. LSH scheme involves finding the hash bucket of a query vector in multiple tables to collect candidate database points. The goal is to choose parameters m and k to ensure high probability of finding the nearest neighbor with a small number of candidates. The analysis considers the number of hash tables needed to query and the spurious candidate vectors encountered. The probability of a dataset point lying in the same LSH bucket as the query vector is calculated to minimize failure probability. To minimize failure probability in LSH scheme, parameters m and k are chosen carefully. Setting m to 5/p \u03b1 ensures a small failure probability. It is important to set k to be as large as possible for a small candidate set, but not larger than log n to avoid empty hash buckets. The expected number of points in a hash bucket is n/2 k, but this may not bound the expected number of candidates during a query due to correlations. Fast query time is ensured by considering the event of a query landing in a hash bucket with many database points. The expected number of spurious candidates in a hash bucket is analyzed based on the data and query distribution. The table load measure penalizes evenly spread out point sets less than highly clustered ones. This analysis considers the event of a query landing in a hash bucket with many database points. The expected number of points in a hash bucket containing a randomly chosen query point from the database is analyzed. The analysis considers the distribution of data and queries, with a focus on the event of a query landing in a hash bucket with multiple database points. The analysis focuses on the balance of the LSH table for efficient inference, with a formula estimating the number of candidate distance computations based on the angle between vectors. For example, improving the angle from \u03c0/3 to \u03c0/4 reduces hash bucket lookups significantly. In experiments on CelebA dataset with 200K face images of 10K celebrities, a Resnet model with 32 layers is evaluated for representation quality on 8K identities. The goal is to assess generalization to unseen identities, not just unseen images of known celebrities. After evaluating a Resnet model on 8K identities from the CelebA dataset, researchers took 2K unseen identities to compute representations for 20K images. They found that normalizing class vectors significantly improved accuracy and angular gaps. Combining softmax normalization with Batch Normalization-ReLU swap resulted in 74% accuracy, compared to 60% without normalization. The angle between query and nearest neighbor improved by 8 degrees without affecting the balance of evaluation points. Various options for controlling representation vector norms were considered. In a study using the multiMNIST dataset with 1K classes, different normalization techniques like Batch Normalization (BN) and Layer Normalization (LN) were compared. A network with batch norm showed similar angles but better convergence speed and accuracy compared to other options. Grid search over learning rates was done to determine the best configuration. The results are shown in FIG0, indicating batch norm as noticeably superior in terms of accuracy and convergence speed."
}