{
    "title": "HJePXkHtvS",
    "content": "In this work, a deep generative classifier is proposed to detect out-of-distribution samples and classify in-distribution samples by integrating Gaussian discriminant analysis into deep neural networks. Unlike traditional classifiers, this approach models class-conditional distributions as separable Gaussian distributions, allowing for the definition of confidence scores based on the distance between a test sample and the center of each distribution. Empirical evaluations on multi-class images and tabular data show that the generative classifier achieves superior performance. The generative classifier excels in distinguishing out-of-distribution samples and can be applied to various deep neural networks. Out-of-distribution detection is crucial due to the unpredictable behaviors of deep neural networks when faced with data distribution mismatches. Accurately measuring distributional uncertainty in DNNs is a significant challenge in real-world applications. Several recent studies have addressed this issue. Recent studies have focused on detecting out-of-distribution (OOD) samples using confidence scores or Mahalanobis distance without re-training the model. However, existing methods using discriminative classifiers have limitations in distinguishing OOD samples from in-distribution samples in the latent space. The softmax probability values for known classes cause overlap between ID and OOD samples in the latent space, reducing the gap in confidence scores. Existing confidence scores use calibration techniques with varying hyperparameters. A small portion of each test set is used for validation, with optimal hyperparameter values for each test case. This tuning process is not ideal for OOD detection where prior knowledge of test distributions is unavailable. In this paper, a novel objective is proposed to train DNNs with a generative classifier for identifying OOD test samples. The deep generative classifier learns separable class-conditional distributions in the latent space without using OOD samples for validation. This approach calculates the likelihood and confidence of a test sample belonging to each class based on the Euclidean distance from the class-conditional distributions. The proposed classifier, based on Gaussian discriminant analysis, effectively detects out-of-distribution samples and maintains classification accuracy for in-distribution samples in deep neural networks. The objective is derived from Gaussian discriminant analysis, with a distance-based confidence score for OOD sample detection. The objective is to optimize the deep learning model by ensuring that data samples in the same class form independent spheres in the latent space. This involves learning class-conditional distributions that are separable from each other, calculating class-conditional probabilities, and defining terms based on Euclidean distances between data representations and class centers. The objective is to optimize deep learning models by focusing on the distance between samples and their target class centers for accurate modeling of class-conditional distributions. This involves learning separable class-conditional distributions and calculating probabilities based on Euclidean distances. The generative classifier defines the posterior distribution P(y|x) using class-conditional distribution P(x|y) and class prior P(y). In Gaussian discriminant analysis, each class-conditional distribution follows a multivariate Gaussian distribution. To fuse GDA with DNNs, class covariance matrices are fixed to the identity matrix. The posterior probability that a sample belongs to a class is described using class center and bias terms. The objective of the classifier is to maximize the posterior probability for training samples by minimizing the KL divergence between empirical class-conditional distributions and Gaussian distributions. This is done to enforce the GDA assumption and ensure that class centers are the actual class means of training samples. The KL divergence is used to approximate K class-conditional Gaussian distributions by minimizing it for all classes. The regularization effect is controlled by combining the KL term with the posterior term using a \u03bb-weighted sum. The hyperparameter \u03bb is used for training the model and does not need tuning for different test distributions. The proposed classifier in DNNs replaces the fully-connected layer. The proposed classifier in DNNs replaces the fully-connected layer with a distance metric layer, predicting class labels based on the distance from each class center. This \"distance classifier\" or \"generative classifier\" computes confidence scores using class-conditional probabilities, enabling out-of-distribution detection. The proposed distance-based confidence score D(x) in DNNs uses Euclidean distance in the latent space to distinguish between in-distribution (ID) and out-of-distribution (OOD) samples. This approach outperforms the traditional Mahalanobis distance method and does not require additional computations for class means and covariance matrices. It can measure predictive uncertainty with a single DNN inference, showing effectiveness in detecting samples not belonging to the known classes. The DeepSVDD objective trains DNNs to map samples close to the class center in the latent space, forming a hypersphere of minimum volume. For multiclass classification, the DNNs with distance classifier extend DeepSVDD by incorporating K one-class classifiers into a single network. The objective aims to make the classifiers distinguishable and learn each classifier by gathering training samples into their corresponding center. The proposed model extends DeepSVDD by incorporating one-class classifiers for each class to detect out-of-distribution samples. Experimental results show the superiority of the distance classifier over the softmax classifier in ID classification and OOD detection using tabular and image datasets. Regularization term analysis is also provided, with code and datasets available for reproducibility. The datasets GasSensor, Shuttle, DriveDiagnosis, and MNIST are preprocessed and normalized for training and testing. The training set excludes samples from one class to simulate OOD scenarios, while the test set includes all OOD samples and remaining ID samples. Evaluations are done with 5-fold cross validation and MLP with three hidden layers is used for DNNs. The DNNs for training tabular data use a multi-layer perceptron with three hidden layers. The architecture of MLP is the same for both softmax and distance classifiers. The Adam optimizer with a learning rate of 0.01 is used, and the regularization coefficient is fixed at 1.0. Two methods are compared using DNNs optimized for the softmax classifier: a baseline method and a state-of-the-art method based on Mahalanobis distance. The proposed method using Mahalanobis distance outperforms competing methods in classification accuracy and OOD detection metrics. The proposed method using Mahalanobis distance improves OOD detection without compromising ID classification accuracy by explicitly modeling class-conditional Gaussian distributions. The distance classifier shows similar performance to the softmax classifier, and 2D latent space plots illustrate the training and test distributions of the GasSensor dataset. The DNNs in the GasSensor dataset successfully separate ID and OOD samples in the latent space. The proposed distance classifier does not require identical covariance matrices for effective OOD sample detection. ResNet and DenseNet CNNs are used for validation. Experimental results show that ResNet and DenseNet CNNs are trained on three image datasets (CIFAR-10, CIFAR-100, SVHN) with additional OOD samples from TinyImageNet and LSUN. The models are trained using stochastic gradient descent with Nesterov momentum and follow training configurations suggested by previous studies. The regularization coefficient for the distance classifier is set to 0.1. The distance classifier with a coefficient \u03bb of 0.1 shows good generalization for deeper models like ResNet and DenseNet. It outperforms the softmax classifier in ID classification accuracy. The dmlayer, which learns class centers, improves classification power of existing DNNs. The proposed objective helps classify ID samples and identify OOD samples accurately. The regularization term's effects on performance and data distributions in the latent space were investigated. The distance classifier's performance varied with different \u03bb values, affecting ID classification when \u03bb exceeded 10^2. OOD detection was not significantly impacted by the regularization coefficient within the range of (0.1, 10). Visualizations of the 2D latent space showed the training distribution of MNIST for different \u03bb values. The regularization term with different \u03bb values affects the decision boundary and class centers in the latent space. Proper \u03bb values make DNNs place class-conditional Gaussian distributions far apart, improving OOD sample detection. Many studies have focused on measuring uncertainty in model predictions, particularly for non-Bayesian DNNs. Distributional uncertainty arises from differences between training and test distributions, with various approaches attempted for OOD detection. The baseline method defines confidence scores based on softmax probabilities, while ODIN aims to enhance detection reliability. Our approach differs from existing methods by explicitly learning class-conditional Gaussian distributions and computing scores based on Euclidean distance from class centers. This paper introduces a deep learning objective to learn a multi-class generative classifier, combining Gaussian discriminant analysis with DNNs. Our generative classifier combines Gaussian discriminant analysis with DNNs to effectively distinguish OOD samples from ID samples. It outperforms other methods in detecting OOD tabular data and images, and can be easily integrated with different types of DNNs for improved performance."
}