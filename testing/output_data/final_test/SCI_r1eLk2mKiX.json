{
    "title": "r1eLk2mKiX",
    "content": "Network pruning is a common method to reduce the computational cost of deep models. Surprisingly, fine-tuning a pruned model may result in comparable or worse performance than training from randomly initialized weights. This suggests that the efficiency benefit of pruning comes from the pruned architecture itself rather than inherited weights, implying that some pruning algorithms could be viewed as performing network architecture search. The process of network pruning involves training a large model, pruning unimportant weights, and fine-tuning the pruned model to regain accuracy. The belief is that training a large network first is crucial, and fine-tuning the pruned model is more effective than training from scratch. Selecting important weights is a key research topic in pruning methods. In network pruning, x% channels are pruned in each layer automatically, with a, b, c, d% determined by the algorithm. Surprisingly, training the pruned model from scratch can achieve similar or better performance than the traditional three-stage pipeline. This challenges the belief that starting with a large model is necessary for predefined target architectures. For automatically discovered architectures, the efficiency comes from the obtained architecture rather than inherited weights. The results suggest that the value of network pruning methods may lie in identifying efficient architectures and implicit architecture search, rather than preserving \"important\" weights. Pruning methods can target architectures determined by humans or algorithms, with the latter determining how many channels to prune in each layer. Training the pruned model from scratch may be unfair due to its reduced computation requirements. In experiments, small pruned models are trained for fewer epochs than large models to account for reduced computation. Different training methods, such as training from scratch or with a set computation budget, are compared. Various pruning methods are evaluated, including L1-norm based channel pruning and Network Slimming. Results for other pruning methods and transfer learning can be found in the appendix. The curr_chunk discusses channel pruning methods for convolutional networks, including L1-norm based pruning and Network Slimming. Results show that scratch-trained models achieve similar accuracy to fine-tuned models, with Scratch-B models performing slightly better. Network Slimming uses L1-sparsity regularization on scaling factors and automatically discovers target architectures. In this section, the value of automatic network pruning methods lies in searching efficient architectures. Network Slimming is used as an example method, showing that pruned architectures are more parameter efficient, achieving the same accuracy with fewer parameters. The value of automatic network pruning methods is in finding efficient architectures with 5\u00d7 fewer parameters than uniformly pruned architectures. Two design strategies are proposed: \"Guided Pruning\" and \"Transferred Guided Pruning\", which show promising results in designing better architectures. \"Transferred Guided Pruning\" uses patterns from a pruned VGG-16 on CIFAR-10 to design a network for VGG-19 on CIFAR-100, outperforming architectures directly pruned on VGG-19 and CIFAR-100. In summary, training small pruned models from scratch can achieve comparable or higher accuracy levels than models obtained through traditional \"training, pruning, and fine-tuning\" methods. This challenges the belief that over-parameterization is necessary for model performance. Pruning methods are useful when starting with a pretrained large model, as fine-tuning is faster, and obtaining models of different sizes can be done quickly by pruning from a larger model. The value of automatic pruning algorithms lies in finding efficient architectures and providing design guidelines, challenging the belief that over-parameterization is necessary for model performance. This is supported by various studies on filter level pruning methods for deep neural network compression. In ICCV 2017, research on efficient convolutional networks through network slimming, batch normalization for deep network training acceleration, learning features from tiny images, ImageNet database, very deep convolutional networks for image recognition, deep residual learning, and densely connected convolutional networks was presented. In CVPR 2017, Zehao Huang and Naiyan Wang presented data-driven sparse structure selection for deep neural networks. ECCV 2018 featured Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He discussing aggregated residual transformations for deep neural networks. NIPS 2015 showcased Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun's work on Faster R-CNN for real-time object detection. ICCV 2017 included Zhiqiang Shen, Zhuang Liu, Jianguo Li, Yu-Gang Jiang, Yurong Chen, and Xiangyang Xue's research on learning deeply supervised object detectors from scratch. Additionally, Jianwei Yang, Jiasen Lu, Dhruv Batra, and Devi Parikh developed a faster PyTorch implementation of Faster R-CNN in 2017. Lastly, Barret Zoph and Quoc V Le explored neural architecture search with reinforcement learning in ICLR 2017. In ICLR 2017, additional experiment details are provided regarding the setup. The implementation closely follows the original paper's protocols, either adopting existing methods or re-implementing them for similar results. Retraining large models from scratch led to higher accuracy than reported. The relative accuracy drop from unpruned models is reported to accommodate different frameworks and training setups. The appendix provides results of four additional pruning methods and an experiment on transfer learning from image classification to object detection. Scratch-E and Scratch-B generally outperform the fine-tuned model, except for VGG-Tiny where aggressive pruning limits Scratch-E's performance. Despite reduced training budgets, Scratch-B achieves similar accuracy levels. In summary, training small models from scratch can often achieve the same accuracy as large models output by a three-stage pipeline. When given the same computation budget, scratch-trained models may even outperform fine-tuned models. Sparse Structure Selection imposes sparsity regularization on scaling factors during training for pruning structures. Residual block pruning in fully-convolutional network architectures like ResNet-50 results in models with improved accuracy compared to pruned models. Sparse Structure Selection is used for sparsity regularization during training, and non-structured weight pruning focuses on individual weights with small magnitudes to create sparse weight matrices. Before training the pruned sparse model from scratch, the standard deviation of weight initialization is re-scaled based on the remaining non-zero weights in convolution layers. Scratch-E falls short on CIFAR datasets compared to fine-tuned results, while Scratch-B performs on par. On ImageNet, Scratch-B may be slightly worse than fine-tuned results due to task complexity and pruning granularity. Sparsity regularization is used in training large models to aid in the pruning process. In experiments, sparsity regularization was analyzed for Network Slimming. Results showed that both Scratch-B and fine-tuned models performed comparably with and without sparsity regularization. Small pruned models matched fine-tuned model accuracy in classification tasks. Further investigation is needed for transfer learning to other vision tasks. In experiments, sparsity regularization was analyzed for Network Slimming, showing comparable performance between Scratch-B and fine-tuned models with and without sparsity regularization. Small pruned models matched fine-tuned model accuracy in classification tasks. Evaluating the L1-norm based pruning method on the PASCAL VOC object detection task using Faster-RCNN, the study found that models trained from scratch could outperform fine-tuned models under the transfer setting. Two approaches, Prune-C (classification) and Prune-D (detection), were compared, with results showing the effectiveness of Prune-C. The study found that Prune-C outperformed Prune-D in non-structured weight pruning, suggesting that early pruning in the classification stage can prevent the model from getting stuck in bad local minima. Results complement Section 4 and show that starting small models from scratch at the classification stage can lead to further performance improvement. The study compares parameter efficiency of architectures obtained through non-structured pruning methods, showing that pruned architectures are more efficient than uniformly sparsified ones. Different design strategies like \"Guided Sparsifying\" and \"Transferred Guided Sparsifying\" also exhibit higher parameter efficiency. Comparison with traditional architecture search methods is discussed at the end. Recent advancements in network architecture search have introduced new techniques such as using network pruning for architecture search. This approach only requires one-pass training but is limited to exploring sub-networks within a larger network. In contrast, traditional methods like reinforcement learning and evolutionary algorithms can search for a wider range of variations in network architectures. Additionally, there are methods that automate the design of network architectures by pruning channels using reinforcement learning and compressing the architecture. Sharing or inheriting trained parameters has also become popular in the network architecture search literature. In network architecture search literature, sharing trained parameters has become popular. Investigating training from scratch versus using inherited parameters is of interest. Common traits between network pruning and architecture search are observed, with wisdom borrowed from each other. Weight distribution comparison is done for unpruned, fine-tuned, and scratch-trained models using Network Slimming and non-structured weight level pruning on DenseNet-40 and CIFAR-10. Prune ratios of 60% and 80% are used, with results shown in FIG5. The weight distribution of fine-tuned and scratch-trained pruned models differs from unpruned large models, with fewer weights close to zero. This suggests less redundant structures in the pruned architecture, supporting the effectiveness of automatic pruning methods."
}