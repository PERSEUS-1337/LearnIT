{
    "title": "ByeSYa4KPS",
    "content": "We introduce sparse learning, a method for training deep neural networks with sparse weights while maintaining dense performance levels. Our algorithm, sparse momentum, redistributes pruned weights and accelerates training by up to 5.61x compared to other methods. We achieve state-of-the-art performance on MNIST, CIFAR-10, and ImageNet datasets, reducing mean error by 8%, 15%, and 6% respectively. In our analysis, momentum redistribution and growth benefits increase with network depth and size. Sparse weight configurations in deep neural networks train faster and achieve similar errors as dense networks. Training sparse networks without pruning from dense networks is demonstrated in this work. In this work, sparse momentum is developed to train sparse networks that match the performance of dense networks without the need for re-training. The algorithm prunes weights with small magnitude, redistributes weights based on momentum, and grows new weights to fill missing connections. Comparing sparse momentum to compression algorithms shows improved efficiency in training deep neural networks. Sparse momentum is shown to outperform compression algorithms and maintain sparse weights during training on MNIST, CIFAR-10, and ImageNet-1k datasets. For CIFAR-10, AlexNet, VGG16, and Wide Residual Networks require 35-50%, 5-10%, and 20-30% of weights to achieve dense performance levels. Speedups of 2.74x to 5.61x are estimated for sparse convolution compared to 1.07x to 1.36x for dense convolution. Momentum redistribution and growth become more crucial in deeper networks. The importance of momentum redistribution and growth in deep and large networks is highlighted for good ImageNet performance. Work on creating sparse neural networks from dense ones has a long history, with recent focus on memory and computational benefits for mobile devices. The iterative cycle of train-dense, prune, re-train has been influential, with extensions to compressing recurrent neural networks and other models. Various approaches have been used for compressing neural networks, including continuous pruning and re-training, joint loss/pruning-cost optimization, layer-by-layer pruning, fast-switching growth-pruning cycles, and soft weight-sharing. These methods often involve re-training phases that increase training time, but the main goal is to create a compressed model for mobile devices. Other compression algorithms include L0 regularization and Bayesian methods. Sparse neural networks can achieve dense performance levels with a single training run, accelerating training. Methods maintaining sparse weights throughout training through a prune-redistribute-regrowth cycle are related to this work. DEEP-R introduces a Bayesian perspective. DEEP-R introduces a Bayesian perspective and performs sampling for prune and regrowth decisions, but is computationally expensive. Sparse evolutionary training (SET) simplifies prune-regrowth cycles using heuristics, but lacks cross-layer weight redistribution critical for performance. Our method works in a fully sparse setting and is more generally applicable than Dynamic Sparse Reparameterization (DSR) and Single-shot Network Pruning (SNIP). We define sparse learning as training deep neural networks to maintain sparsity while maximizing predictive and run-time performance. In our experiments, we compare against DEEP-R, SET, DSR, and SNIP. In this work, a divide-and-conquer strategy is used to tackle sparse learning in deep neural networks. The approach involves pruning weights, redistributing weights across layers, and regrowing weights. Sparse Momentum is applied at the end of each epoch to remove a percentage of weights with the smallest magnitude and normalize the gradient. The sparse momentum algorithm involves removing a percentage of weights with the smallest magnitude and redistributing them across layers based on momentum. This process aims to improve the efficiency of weight distribution in reducing overall error. When using stochastic gradient descent, weights oscillate between small/large and negative/positive gradients with each mini-batch. To reduce oscillations, averaging gradients over time can help find weights that consistently reduce error. Exponentially smoothing gradients with momentum accelerates optimization by valuing recent gradients closer to the local minimum. Momentum efficiently identifies weights that consistently reduce error in deep neural networks. The algorithm for sparse momentum involves initializing the network with a certain sparsity, removing weights, training the network normally, and applying sparse momentum after each epoch. This includes redistributing, pruning, and regrowing weights to reduce error consistently. See Algorithm 1 for details. The algorithm for sparse momentum involves redistributing, pruning, and regrowing weights to reduce error consistently. In step (a), the momentum magnitude contribution for each layer is calculated by normalizing the mean momentum magnitude of nonzero weights. Step (b) prunes a proportion of weights with the lowest magnitude, while step (c) regrows weights by enabling the gradient flow of zero-valued weights with the largest momentum magnitude. The algorithm for sparse momentum involves redistributing, pruning, and regrowing weights to reduce error consistently. For layers with larger momentum magnitude but dense structure, the prune rate is reduced proportionally to sparsity. The prune rate is decayed after each epoch using a cosine decay schedule, ultimately annealing to zero. Different experimental settings are compared to previous studies for analysis. Experimental settings for training on MNIST and CIFAR-10 include batch sizes, learning rate decay, data augmentations, number of epochs, optimizer parameters, validation set size, and test set evaluation. The study reports standard errors and conducts 10-12 experiments per method/architecture/sparsity level with different random seeds. The study conducts 10-12 experiments per method/architecture/sparsity level using modified network architectures of AlexNet, VGG16, and LeNet-5. Two variations of the experimental setup are considered for ImageNet and CIFAR-10, one partially dense and the other fully sparse. The parameters in the dense layers make up 5.63% of the weights of the ResNet-50 network. The study uses ResNet-50 on ImageNet with specific parameters and training settings. Experiments were conducted on MNIST and CIFAR-10 with tuned prune and momentum rates. ImageNet experiments were run on 4x RTX 2080 Ti GPUs. Our software, built on PyTorch, enables easy adaptation of PyTorch neural networks to sparse momentum networks with masked weights. It outperforms other model compression methods on MNIST, showing consistent strong performance. The software will be open-sourced along with trained models and experimental results. Sparse momentum is competitive with variational dropout for one model but underperforms for another. It achieves equal performance to the LeNet-5 Caffe dense baseline with 8% weights and outperforms Single-shot Network Pruning on CIFAR-10. Sparse momentum also outperforms other methods on MNIST and CIFAR, showing strong performance overall. In general, sparse momentum outperforms other methods on ImageNet for Top-1 accuracy, while dynamic sparse is better for Top-5 accuracy with 20% weights. Sparse momentum remains competitive in fully sparse settings and finds a weight distribution that works well. Test set accuracy with confidence intervals on MNIST and CIFAR at varying sparsity levels is shown for LeNet 300-100 and WRN 28-2. The study analyzes the number of weights needed to achieve dense performance on CIFAR-10 networks. The study analyzes the number of weights needed to achieve dense performance on CIFAR-10 networks by increasing weights by 5% until sparse network performance overlaps with dense performance. Speedup of the model is measured by sparse momentum. Speedup estimates are calculated using theoretical FLOPS reductions and practical empty convolutional channels. The study explores sparse convolution estimates by calculating FLOPS saved and runtime for each convolution operation. Scaling the runtime by FLOPS saved aims to maximize speedups. While fast sparse convolution algorithms exist for coarse block structures, optimal algorithms for fine-grained patterns need development. Additionally, the analysis includes measuring practical speedups achievable with dense convolution algorithms, highlighting the gap in efficiently training sparse networks. The study explores sparse convolution estimates by calculating FLOPS saved and runtime for each convolution operation. Speedups for dense convolution are achieved by scaling each operation based on empty channels. Results on CIFAR-10 show VGG16 networks require fewer weights for dense performance, while AlexNet needs the most. Wide Residual Networks fall in between. Sparse momentum yields significant speedups, especially for Wide Residual Networks. Dense convolution speedups are lower and depend on network width. The study explores sparse convolution estimates by calculating FLOPS saved and runtime for each convolution operation. Speedups for dense convolution are achieved by scaling each operation based on empty channels. Results on CIFAR-10 show VGG16 networks require fewer weights for dense performance, while AlexNet needs the most. Wide Residual Networks fall in between. Sparse momentum yields significant speedups, especially for Wide Residual Networks. Dense convolution speedups are lower and depend on network width. The overhead of sparse momentum is equivalent to a slowdown of 0.973x compared to a dense baseline. Redistribution of weights based on momentum magnitude becomes increasingly important for larger networks, as shown by increased test error from small to large networks without momentum redistribution. Sparse momentum outperforms other sparse algorithms on MNIST, CIFAR-10, and ImageNet, showing that smarter growth algorithms are needed for good performance as network size increases. The study also highlights the importance of weight redistribution based on momentum magnitude for larger networks to prevent increased test error. Sparse momentum outperforms other sparse algorithms on MNIST, CIFAR-10, and ImageNet, showing the need for specialized sparse convolution and matrix multiplication algorithms to enable the benefits of sparse networks. Sensitivity analysis on prune rate and momentum for VGG-D and AlexNet-s models on CIFAR-10 shows robustness of sparse momentum to prune rate variations. The study found that smaller prune rates work slightly better than larger ones, with cosine and linear prune rate annealing schedules performing equally well. Confidence intervals for momentum values between 0.7 and 0.9 overlap, indicating robustness to momentum parameter choice. Sparse momentum is more sensitive to low momentum values (\u22640.6) and less sensitive to high values (0.95) compared to dense control. The study also tested the hypothesis that sparse momentum is more sensitive to deviations from a momentum parameter value of 0.9 than a dense control, finding no evidence to support this claim. Sparse momentum is highly robust to deviations in pruning schedule and momentum parameters. The study compares features of dense and sparse networks statistically and focuses on the overall distribution of features within each layer. Unlike feature visualization, the study emphasizes the magnitude of activity in a channel for discriminators. The study focuses on convolutional channel-activation analysis to measure class specialization in neural networks like AlexNet-s and VGG16-D. It involves passing the training set through the network, aggregating activation magnitudes in each channel for each class, and normalizing to determine specialization for a particular class. The study analyzes class specialization in neural networks like AlexNet-s and VGG16-D by testing the hypothesis that sparse networks have lower class specialization than dense networks. Results show evidence supporting this hypothesis, leading to the rejection of the null hypothesis. The study rejects the null hypothesis, indicating that sparse networks learn features with lower class specialization compared to dense networks. Plots show the difference in feature distribution between sparse and dense networks in various layers. Sparse networks learn more general features useful for a broader range of classes, potentially rivaling dense networks. Further experiments with ResNet-50 in a fully sparse setting also show promising results. In a fully sparse setting, a cosine learning rate schedule and label smoothing of 0.9 are used, with results shown in Table 5. Class-specialization of 0.5 indicates that 50% of activity comes from a single class."
}