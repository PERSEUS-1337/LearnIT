{
    "title": "H1xEtoRqtQ",
    "content": "The paper proposes splitting deep learning models between multiple parties as a scalable technique for shared model governance. It empirically investigates the security guarantee of this technique through the model completion problem, which evaluates how much training is needed to recover a model's original performance. Experiments on supervised learning and reinforcement learning show promising results. The model completion problem is harder in reinforcement learning than in supervised learning due to the unavailability of trained agent's trajectories. Model splitting could be a feasible technique for shared model governance in expensive training settings. Training larger models on more data increases costs and susceptibility to theft, prompting the need for technical solutions to monitor data privacy and misuse. Shared model governance is a challenge in training large neural networks due to the susceptibility to theft. Homomorphic encryption and secure multi-party computation have high overhead, making them impractical for training. A scalable alternative with minimal overhead is proposed for sharing model governance. The method of model splitting is proposed as a scalable alternative for sharing model governance with minimal overhead. It involves distributing a deep learning model between multiple parties, such as Alice and Bob, where each party holds a disjoint subset of the model's parameters. This approach ensures security by preventing theft of the entire model. In this work, the problem of model completion is introduced, where an adversary with access to everything except missing parameters aims to recover the model. The paper formally defines model completion, proposes a metric to measure its difficulty, and presents empirical results in supervised learning (SL) and reinforcement learning (RL) using various networks and environments. After introducing the concept of model completion, the study compares the completion difficulty between residual and nonresidual networks. The paper explores vertical model completion, focusing on completing all parameters in one layer, as opposed to horizontal completion where some parameters are completed in every layer. The goal is to understand the training required to complete a reinitialized layer in a model. The study discusses horizontal model completion, which involves completing some parameters in every layer. Various techniques like dropout and pruning neural networks are explored, with findings suggesting that lower layers are more important. The lottery ticket hypothesis is also mentioned, where only a small subnetwork is crucial for performance. The model completion problem is viewed as transfer learning. The model completion problem involves transfer learning from one task to the same task, sharing only a subset of parameters. It is related to distillation of deep models, where a smaller model with the same performance is sought. Techniques like homomorphic encryption and secure multi-party computation have been applied successfully to machine learning on small datasets. Homomorphic encryption (HE) and secure multi-party computation (MPC) have been successfully applied to machine learning on small datasets like MNIST, BID14, BID32, BID7, BID46, and the Wisconsin Breast Cancer Data set. HE allows computation on encrypted numbers without decryption, enabling training by an untrusted third party in encrypted form. MPC allows sharing numbers across parties for computations without revealing information. HE and MPC offer different tradeoffs for additional security benefits. Homomorphic encryption (HE) and secure multi-party computation (MPC) offer different tradeoffs for additional security benefits. HE incurs a large computational overhead, while MPC incurs a smaller computational overhead but with greater communication overhead. HE provides cryptographic security, while MPC provides perfect information-theoretic guarantees as long as parties do not collude. In some applications, the additional overhead of HE or MPC is worth it for privacy and security, but for scaling shared model governance to large neural networks, model splitting incurs minimal computational and manageable communication overhead. Model splitting for governance offers minimal computational and communication overhead compared to Homomorphic Encryption (HE) and Secure Multi-Party Computation (MPC). The security guarantee is limited by the model completion problem studied in the paper. Two settings are considered: supervised learning and reinforcement learning, where the model's performance is evaluated based on test loss and an agent interacts with an environment to maximize episodic return. The agent's goal is to maximize episodic return by mapping observations to actions through a policy parameterized by a model. The loss function for reinforcement learning is the negative expected episodic return. Training costs are quantified by measuring computational cost during training steps, which are assumed to be constant. Training cost is measured by the number of training steps executed. The training procedure involves optimizing the model parameters \u03b8 through a sequence of steps. The training cost is defined as the number of steps needed for the loss to fall below a given threshold. After training for N steps, the parameters are split into previously trained and freshly initialized ones for a retraining procedure. The retraining procedure aims to complete the model using a fixed set of available procedures. The cost of retraining is determined by the sequence of parameter vectors obtained. Different thresholds are considered for model completion, including partial completion based on relative progress from the untrained model parameters. The hardness of model completion is a relative measure, comparing completion cost to original training cost. It is not relative to the time it took to train the model to its best performance. The model completion hardness is bounded between 0 and 1, with various retraining strategies explored empirically to estimate it. The completion cost is compared to the original training cost, not considering the time taken to train the model to its best performance. Our best results provide an upper bound on model completion hardness. Faster retraining procedures may exist. Three retraining procedures are explored: T1 optimizes \u03b801 and \u03b802 jointly, T2 optimizes \u03b802 only, and T3 overparametrizes missing layers. These procedures vary in hyperparameters but maintain the model structure. The retraining procedure T1 involves overparametrization with a larger number of parameters for faster learning. Parameters are initialized using different schemes like glorot normal, msra, or caffe. Previous research shows the impact of initialization schemes on convergence properties of neural networks. This procedure establishes upper bounds on model completion hardness. In experiments, models are trained to a desired performance level, then reinitialized for retraining. Different models like AlexNet and ResNet50 are trained on ImageNet to minimize cross-entropy loss. AlexNet has eight layers with convolutional and fully connected layers, while ResNet50 has 50 layers with residual blocks. The curr_chunk describes the architecture and retraining procedures of a model with max-pooling, ResNet blocks, and reinforcement learning using A3C, Rainbow, and IMPALA agents. Different learning rate schedules are used during retraining, while hyperparameters remain the same. The curr_chunk discusses the use of distributional RL and noisy nets in a model equipped with a replay buffer. IMPALA is an extension of A3C that uses off-policy corrections and population-based training. The model is trained on Atari 2600 games and DeepMind Lab, treating the list of games/levels as a single learning problem. Levels where agents behave randomly are filtered out to reduce noise in the MC-hardness metric. The experimental results on the hardness of the model completion problem are reported in FIG4, showing different experiments with layers being reinitialized. MC-hardness T(\u03b1) is plotted with error bars indicating standard deviation over multiple runs, with colors representing different values of \u03b1. Further details of the training and retraining procedures for all models can be found in the appendices. In most cases, T 1 is the best retraining procedure. T 1 outperforms T 2 in all settings except for A3C and ResNet-50 retraining. T 2 performs better for A3C starting from the third convolutional layer and for ResNet-50 with \u03b1 \u2264 0.9. T 3, with modified layers, yielded worse results compared to using the same architecture. The results show that T 4 does not have a significant difference in retraining time between different initialization schemes. ResNet50 has lower model hardness compared to AlexNet for certain thresholds, but both models require about 40% of the original training cost to achieve the original performance. Skip connections help retraining for certain thresholds, with an outlier observed at \u03b1 = 0.9. Residual neural networks utilize skip connections across multiple layers. Residual neural networks utilize skip connections across multiple layers, causing features to be additive rather than replacing them. Lower-level and higher-level representations are spread out across the network, making model completion more independent of layer location. In non-residual networks, lower convolutional layers learn simpler, task-independent features. The impact of noise perturbations on deep learning models is higher in lower layers, affecting subsequent activations and gradients in higher layers, slowing down training. The number of parameters in deep models varies greatly between layers, with lower-level layers having significantly fewer parameters than higher-level layers. This variation can affect model completion independently of layer location. In experiments with AlexNet, increasing or decreasing feature maps and fully connected units by 50% did not significantly impact MC-hardness. RL models are harder to complete than SL models, with model completion in SL easier at threshold \u03b1 = 1. Resetting a layer in the model hinders the agent's ability to generate useful experience, requiring re-exploration of the environment during retraining. The model completion hardness becomes easier with access to Rainbow's replay buffer, filled with experience from the fully trained policy. This is supported by the benefits of kickstarting BID39, where a newly trained agent gains access to an expert agent's policy. Our findings suggest that residual networks are easier to complete than non-residual networks, lower layers are often harder to complete than higher layers, and RL models are harder to complete than SL models. Several questions remain unanswered, such as the differences in MC-hardness between layers in Rainbow and AlexNet compared to A3C, the significance of the absolute number of parameters, and the possibility of faster retraining procedures. This definition of model completion hardness allows for the modulation of completion difficulty in model architectures. The text discusses the ease or difficulty of model completion for different types of networks and layers. It mentions that model completion can be costly, with retraining often taking up to 40% of the original training costs. The security implications of model splitting for shared governance are also highlighted as needing further investigation. Our experiments suggest that model splitting could be a promising method for shared governance, offering cost-competitiveness with normal training and inference. The learning schedule for training and retraining procedures for AlexNet is shown in Table 1, with retraining being 50% faster than training. During retraining, the learning schedule is 50% faster than during training for T1 and T2. Reset is performed for the first 5 convolutional layers and 2 fully connected layers. Training and retraining are done for 60e3 batches with batch size of 64 and 2-regularization of 1e-4 using Momentum SGD optimization. The first convolutional layer, first ResNet block, and last fully connected layer are reinitialized for each network section. Each agent is trained on a single Atari level for 5e7 environment steps over 10 seeds. For 5e7 environment steps, over 10 seeds, standard Atari architecture is used with 3 convolutional layers, 1 fully connected layer, and 2 fully connected 'heads'. RMSProp optimizer with specific parameters is utilized for optimization. Certain Atari levels are excluded from reported statistics and MC-hardness calculations due to poor agent behavior. Each agent is trained on a single Atari level for a specific number of environment frames over multiple seeds. In experiments with Montezuma's Revenge, Venture, and Solaris, the same network architecture and hyperparameters from BID19 are used for the first 3 convolutional layers. A single agent is trained on 28 DeepMind Lab levels for 1 billion steps using population-based training with a population size of 12. Two DeepMind Lab levels are excluded due to poor agent behavior. The first convolutional layer is reinitialized for the experiments."
}