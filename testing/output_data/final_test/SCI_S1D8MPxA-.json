{
    "title": "S1D8MPxA-",
    "content": "Weight pruning is an effective method for reducing model size and computation cost without sacrificing accuracy. A new sparse matrix representation using the Viterbi algorithm is proposed, offering a fixed index compression ratio regardless of the pruning rate. Multiple sparse matrix candidates are generated and the one minimizing model accuracy degradation is selected. The Viterbi algorithm is used for model pruning, enabling highly parallelizable processing for efficient hardware implementation. It achieves significant compression in index data storage requirements compared to existing methods, reducing storage by 85.2% in MNIST and 83.9% in AlexNet. This method also outperforms relative index compression techniques, reducing storage by 52.7% in MNIST and 35.5% in AlexNet. Deep neural networks require high-performance parallel computing systems due to the increasing complexity of tasks and data volume. Researchers have suggested pruning as a technique to reduce redundant connections in energy-efficient DNN, achieving a 9\u00d7 to 13\u00d7 reduction in connections. The design of sparse matrix formats significantly impacts computational parallelism in DNN performance. Sparse matrix formats play a crucial role in hardware-friendly DNNs by reducing index storage and enabling parallelizable index decoding. Various formats have been proposed to balance index size reduction and decoding complexity. However, the computation time for sparse matrix-matrix multiplications (SpMM) can be longer than dense matrix multiplication on GPUs due to serialized index decoding and irregular memory access patterns. This can lead to increased latency in models like AlexNet and VGG16. Traditional pruning techniques are more attractive when sparse matrix-vector multiplication (SpMV) can be utilized. The proposed sparse matrix format for DNNs reduces index storage and enables parallelizable decoding. It uses error-correction coding techniques to compress non-zero values, improving performance compared to traditional formats. The proposed scheme for DNNs utilizes error-correction coding to reduce index storage and enable parallelizable decoding. Pruning is done in a restricted manner by constructing a specific sparse matrix format first, followed by a DNN-specific Viterbi encoder generating a sequence of random numbers to indicate parameter survival. The input pattern is used as the sparse matrix index, leading to a deterministic output random number sequence for sparse matrix representation. The Viterbi algorithm is used to minimize accuracy degradation based on a user-defined cost function. It is computationally efficient and can be modified for energy-efficient DNN pruning. The proposed Viterbi decompressor (VD) uses FlipFlops and XOR gates to generate pseudo random number outputs. The proposed Viterbi decompressor (VD) utilizes FlipFlops and XOR gates to generate pseudo random number outputs. A dense matrix is formed after pruning, and an input sequence is applied to VD to generate outputs. The Viterbi-Compressible Matrix (VCM) format has less overhead compared to CSR, with the input sequence becoming the index information. The VD-compressible dense matrix representation is limited, and the pruning method considering VCM may result in limitations. The key to the success of Viterbi Decompressor (VD) is designing a VD that allows diversified parameters to survive and efficiently searching for the optimal input sequence. Achieving a high index compression ratio implies that the possible VD-compressible dense matrix representations need to be uniformly distributed. The goal of VD is to act as a random number generator using the input sequence. The Viterbi Decompressor (VD) acts as a random number generator using the input sequence. Practical ECC techniques with a fixed encoding rate simulate random coding with allowed decoding complexity. VD's randomness is determined by the number of FFs and XOR gates configuration. The design methodology of VD is detailed in Appendix A.1, with a structure similar to the design introduced in BID17. VD for DNN applications requires the pruning rate to be a user-defined parameter. The Viterbi Decompressor (VD) acts as a random number generator with a user-defined pruning rate. VD outputs are connected to binary number comparators to control the pruning rate. A trade-off exists between pruning rate granularity and index compression ratio. The proposed index decoding operation using VD is a parallel process. The proposed index decoding operation with VD is a parallel process with low hardware overhead. Unlike other formats, VCM decoding with VD does not require significant memory overhead and can be done at a fixed rate. This allows for efficient memory bandwidth utilization and increased parallelism. The objective of pruning with VD is to preserve larger value weights by assigning a cost function to each pruning case. Pruning with Viterbi algorithm involves constructing a trellis diagram representing states with FF values. Each transition with an input bit generates multiple output bits, leading to efficient pruning exploration. The Viterbi algorithm involves constructing a trellis diagram to represent states with FF values. Each transition with an input bit produces multiple output bits. The cost function for transitions is computed using branch and path metrics, where the path metric is defined as a reward value instead of a cost value. The path metrics can be normalized to prevent overflow. The Viterbi algorithm aims to maximize branch metric accumulation in path selection, favoring parameters with larger magnitude through a skewed tanh function. Pruning flexibility allows for different cost functions based on pruning approach, following a hidden Markov model. The Viterbi algorithm follows a hidden Markov model with scaling factors S1 and S2 empirically determined. Diversified states are explored by maintaining 50% of '1' and '0' distributions. The target pruning rate is controlled by comparator threshold value THc. THp is determined based on parameter distribution and target pruning rate. The state with the maximum path metric is chosen at the final time index in Viterbi pruning. The Viterbi algorithm selects the state with the maximum path metric at the final time index. The algorithm can be implemented using dynamic programming with a time complexity of O(l \u00b7 2^f), where l is the input sequence length and f is the number of FFs. By inserting a dummy input sequence, every state of the Viterbi Decoder can be reached, allowing for efficient pruning methods. The impact of different Viterbi Decoder configurations and branch metric selections on model accuracy and index compression ratio is analyzed. Weight distribution after pruning and accuracy sensitivity using MNIST are empirically studied. Observations from MNIST are then applied to AlexNet to validate the scalability of the proposed method. Experiments are conducted using a LeNet-5-like convolutional MNIST model with fixed parameters for fast design exploration. Increasing these parameters enhances randomness of VD output and target pruning rate resolution, critical for improving pruning rate with minimal accuracy degradation. The impact of different Viterbi Decoder configurations and branch metric selections on model accuracy and index compression ratio is analyzed. After training, weights are pruned with different NUM v for VD, providing a trade-off between accuracy and index compression ratio. For Conv layers, a low NUM v is desired, while a wide range of NUM v can lead to minimal accuracy degradation for FC layers. MNIST experiments suggest NUM v =8 for Conv layers and NUM v =40 for FC layers for optimal trade-off. The optimal trade-off between index compression ratio and accuracy is achieved by choosing NUM v =8 for Conv layers and NUM v =40 for FC layers. Finding the pruning threshold value (TH p ) that results in the target pruning rate can be iterative, especially with high NUM v. Sensitivity of accuracy to TH p needs to be investigated, as it affects weight distributions and pruning rates. Despite minor discrepancies, retraining processes converge, as shown in Figure 7. The default skip state in Viterbi-based pruning is one, allowing for diverse state exploration and improved pruning quality. Increasing skip states preserves more larger magnitude weights while maintaining pruning parameters. Sparse matrix comparison with MNIST shows the effectiveness of Viterbi-based pruning over magnitude-based pruning. The \"tanh\" function is chosen for branch metric pruning due to its sharpness and low sensitivity. Pruning and retraining processes were compared between magnitude-based and Viterbi-based methods, showing successful accuracy maintenance. The final pruning rate and memory requirements are detailed in the results. The VCM format significantly reduces memory footprint compared to CSR, with a 53.1% reduction in memory size. VCM's index storage is 85.2% smaller than CSR's, making it superior in terms of storage requirement and dense matrix reconstruction. VCM and Viterbi-based pruning methods were tested on the AlexNet model, showing scalability. The VCM format, used with the AlexNet model on ImageNet, achieves a 38.1% reduction in matrix size mainly due to a significant decrease in index storage requirement. The index compression ratio of VCM outperforms other schemes, with a 35.5% reduction in index size compared to a 4-bit relative index scheme. Both VCM format and traditional sparse matrix pruning methods aim to reduce the size of neural networks by increasing sparsity. Various approaches, such as introducing cost biases and minimizing error increase, have been suggested to achieve this goal. Optimal Brain Damage (OBD) restricts the Hessian matrix to prune parameters effectively. Han et al. proposed pruning deep neural networks based on parameter magnitude, achieving significant compression rates without loss of accuracy. Subsequent work further compressed the pruned network using weight sharing and Huffman coding. These methods, while impressive in compression rate, face challenges with irregular sparsity and complex index decoding processes that hinder common hardware utilization. BID10 developed a dedicated hardware accelerator to address the challenges of complex index decoding processes that hinder common hardware utilization. Recent papers have proposed iterative hardware-efficient pruning methods for faster inference speed and smaller model size. BID23 suggested iterative pruning on a feature-map level using a heuristic approach, while another paper used a first-degree Taylor polynomial to estimate parameter importance with reduced computational burden. Coarse-level pruning methods like pruning convolution kernels and feature maps together have been suggested to avoid the need for a sparse matrix format. BID23 introduced coarse-level pruning to avoid sparse matrix format, with a lower pruning rate. BID25 proposed a sparse convolution algorithm for faster inference speed, but only tested on CPUs. BID0 used linear-feedback shift registers for random weight pruning, simplifying hardware structure but lacking selective pruning capability. Variational Dropout allows for training dropout rates layer-wise or weight-wise, enabling high sparsity in deep neural networks. BID21 demonstrated weight-wise Variational Dropout for pruning, achieving high sparsity in a deep neural network for the CIFAR-10 classification task. BID22 and BID20 proposed structured pruning of deep neural networks using new Bayesian models. Deep neural networks can be pruned either neuron-wise or channelwise, achieving state-of-the-art sparsity for the CIFAR-10 task. Various methods like low rank approximations, vector quantization, and HashedNets are used for model compression. SqueezeNet with Fire modules achieves high accuracy with fewer parameters. These methods can be combined for further compression, such as SqueezeNet with Deep Compression achieving 510\u00d7 compression. Efforts to combine existing and new ECC techniques with DNN pruning methods create energy-efficient and high-performance DNNs. The approach is suitable for ASIC or FPGA, but can also be utilized in GPUs through new kernels and libraries. Future considerations may include quantization of non-zero weight values and entropy-related coding design. In the future, considerations for embedding into branch metric or path metric equations are proposed. A new DNN-dedicated sparse matrix format and pruning method using Viterbi encoder structure and algorithm are introduced. Limited choices of pruning results are considered for significant index compression ratio. Pruning result selection is based on Viterbi algorithm with user-defined branch metric equations to minimize accuracy degradation. The proposed sparse matrix, VCM, shows noticeable index storage reduction compared to relative index scheme, allowing for a wide range of applications like SpMM. Sparse matrices can efficiently convert into dense matrices. VD outputs are generated by XOR gates, with 6 possible input candidates called XOR taps. VD Matrix represents VD with 3 XOR taps in each row and a minimum Hamming distance of 4. Increasing XOR taps and minimum Hamming distance improves randomness. VD Matrix generation is based on Algorithm 1. The VD Matrix is generated using Algorithm 1, based on the number of outputs, XOR taps, and minimum Hamming distance. The number of VD outputs increases exponentially, while the number of FFs increases linearly. Implementing VD is not expensive even with a high compression ratio. The number of XOR taps for pruning should always be an even number for optimal performance. In order to enhance controllable target pruning rates, the number of VD outputs needs to be increased. Accuracy degradation can be reduced by increasing the number of states in the trellis diagram. Inserting dummy inputs as an initial sequence can increase the number of available states for indexing. Increasing the number of available states in the trellis diagram can be achieved by adding dummy input sequences with weight parameters. The size of the dummy inputs is insignificant if the number of FFs in VD is much smaller than the number of weight matrix elements divided by R. Additionally, skip states, which skip time indexes in the trellis diagram, can also reduce accuracy degradation. This concept is similar to dummy inputs as it increases the available states in the search. In the context of increasing available states in the trellis diagram, the Viterbi algorithm discards VD outputs based on branch metrics. Magnitude-based pruning preserves larger weight magnitudes, while k skip states discard VD outputs for consecutive time indices. Normalized magnitudes and branch metrics are calculated, showing the importance of tanh in assigning higher importance to parameters with greater magnitude. The Viterbi-based pruning method, using tanh for magnitude-based pruning, is tested on highly sparse DNNs with the Variational dropout-based pruning method BID21. The VCM data is obtained after pruning weights and neurons, reducing storage requirements by 22.6% and 21.0% compared to CSR. Hamming distance, XOR taps, pruning rate, and memory footprint comparisons are also presented. The proposed Viterbi-based pruning method reduces VCM storage requirements by 22.6% and 21.0% compared to CSR. It achieves similar classification accuracy with short retraining times in LeNet-300-100 and LeNet-5-Caffe, showing compatibility with existing pruning methods without the need to modify equations."
}