{
    "title": "BJlrlm8NsH",
    "content": "The reparameterization trick in variational inference is limited by the standardization transformation, but a new model allows for a more general transformation. This model combines the advantages of control variate and generalized reparameterization, with a new polynomial-based gradient estimator that outperforms the reparameterization trick in certain conditions and can be applied to a wider range of variational distributions. Studies on synthetic and real data demonstrate the effectiveness of the proposed model. Our proposed gradient estimator in variational inference reduces gradient variance significantly compared to other methods, leading to faster inference. Machine learning objective functions can often be expressed as expectations, but calculating the exact gradient is impractical, necessitating gradient estimation. Stochastic optimization methods like the reparameterization trick and score function methods are commonly used for this purpose, with recent advances in large-scale machine learning tasks benefiting from these techniques. Our focus is on variational inference tasks in this paper. Our paper focuses on variational inference tasks, aiming to approximate the posterior distribution in probabilistic models by introducing a parameteric family of distribution q \u03b8 (z) and optimizing the Kullback-Leibler divergence. We propose a better optimizer for a larger class of parameteric family of distributions by replacing the parameter-independent transformation in reparameterization trick with a generalized transformation. Our gradient model introduces a generalized transformation-based approach with a velocity field related to the characteristic curve of a sublinear partial differential equation. It extends the G-REP method and offers a more powerful and flexible way to construct gradient estimators. Our gradient model introduces a polynomial-based gradient estimator that is superior to existing methods in various experiments. The paper is organized into sections reviewing SGVI, proposing a generalized transformation-based gradient, introducing the polynomial-based G-TRANS gradient estimator, evaluating performance on synthetic and real data, reviewing related works, and concluding with future work discussions. The text discusses the use of variational inference (VI) to maximize the evidence lower bound (ELBO) by constructing Monte Carlo estimators for the gradient of the ELBO. The score function method and reparameterization trick are popular gradient estimators for VI tasks. The reparameterization trick is a method to transform the variational distribution into a distribution that does not depend on the variational parameter, reducing variance in gradient estimators for variational inference tasks. Variance reduction methods like Rao-Blackwellization and control variates are necessary to improve the performance of gradient estimators. The reparameterization trick transforms the variational distribution into one that does not depend on the variational parameter, reducing gradient estimator variance. This method is applicable to common distributions like Gaussian but challenging for others like Gamma or Beta due to the need for special functions in standardization. The trick results in lower variance gradient estimators compared to the score function method. Theorem 3.1 states that the probability density function of \u03c1, denoted as w(\u03c1, \u03b8), is dependent on the variational parameter \u03b8. The G-TRANS gradient estimator is introduced as a generalized transformation-based gradient with specific velocity fields. The score function and reparameterization gradients are shown to be special cases of the G-TRANS model under certain conditions. The reparameterization trick is a special case of the G-TRANS model, which introduces a generalized transformation-based gradient with specific velocity fields. The variance of the resulting gradient estimator is determined by the unique solution of the transport equation. The G-TRANS model is a special case of the control variate method. The G-TRANS model is a special case of the control variate method, connecting to the reparameterization trick. Theorem.3.1 transforms the unbiased reparameterization procedure by finding the appropriate velocity field. Variational optimization theory can be used to minimize estimate variance, but the Euler-Lagrange equation's solution is impractical due to the integrand containing f(z). Introducing the velocity field offers a more elegant way to construct gradient estimators without computing Jacobian matrices. In the next section, a polynomial-based G-TRANS gradient estimator is introduced, which is superior to existing reparameterization gradient frameworks. The base distribution is assumed to be factorized, and an ad-hoc velocity field family is considered. The resulting gradient estimator is more general than the score function method or reparameterization trick. In this paper, a polynomial-based G-TRANS gradient estimator is discussed for distributions with analytical high order moments like Gamma, Beta, or Dirichlet distribution. The estimator is constructed based on a polynomial velocity field and is different from existing reparameterized gradient frameworks. The polynomial-based G-TRANS gradient estimator outperforms the reparameterization gradient estimator under certain conditions. By choosing a suitable polynomial, a better gradient estimator can be obtained, and adjusting the polynomial coefficients can further enhance performance. In practical situations, it is challenging to estimate coefficients for high-degree polynomials. Therefore, in experiments, only degrees less than 2 are considered. A Dirichlet distribution is used to approximate the posterior distribution for a probabilistic model with a multinomial likelihood and Dirichlet prior, using Gamma distributions to simulate Dirichlet distributions. The problem studied involves multinomial log-likelihood with shape parameters. The study focuses on constructing a polynomial-based G-TRANS gradient estimator for the factorized distribution using a variational Dirichlet distribution with shape parameters. An approximation scheme is used to estimate the derivative of the lower incomplete gamma function, which is crucial for the gamma CDF. The scheme involves using a power series for small values of \u03b1 and z, and central finite difference for large \u03b1. The approximation error is kept below 10^-9 for specific ranges of \u03b1 and z. The study constructs a polynomial-based G-TRANS gradient estimator for factorized distributions using a variational Dirichlet distribution with shape parameters. The approximation scheme for the lower incomplete gamma function is used to reduce gradient variance compared to other methods like IRG and RSVI. Our G-TRANS gradient estimator performs better than the IRG method for large \u03b1 1 values in the Sparse Gamma deep exponential family model with the Olivetti faces dataset. The model includes polynomial coefficient C, shape augmentation B, and optimal concentration \u03b1 = 2. In the experiment, the model consists of 3 layers with different components in each. Various priors and settings are used for the weights and local variables. The RSVI result is reproduced with specific parameters. G-TRANS achieves significant results according to Figure 2. Results show that G-TRANS outperforms ADVI, BBVI, G-REP, and RSVI in accuracy and ELBO improvement. It is faster than IRG initially due to lower gradient variance but loses speed as step size decreases. Research is exploring extending reparameterization tricks to broader distributions. G-REP offers a generalized reparameterized gradient, but the gradient model in this study provides a more elegant expression. Our model presents a more elegant expression of the generalized reparameterized gradient compared to G-REP. It hides the transformation behind the velocity field, avoiding the costly computation of the Jacobian matrix. Unlike RSVI, our gradient estimator can be constructed deterministically. Our gradient estimator can be constructed deterministically, reducing stochasticity and lowering gradient variance compared to RSVI. Our model derives the transport equation for reparameterization gradient through rigorous mathematical deduction, allowing for a more generalized transformation-based gradient compared to existing models like the path-wise derivative. The velocity field in our model only needs to satisfy the unbiasedness constraint, unlike other models that require conformity to the transport equation. The IRG method by Figurnov et al. (2018) differs from the path-wise derivative by using a different approach for multivariate distributions. Other works like Graves (2016) and Knowles (2015) have also attempted to overcome limitations of standard reparameterization, but involve computationally expensive methods not suitable for large-scale variational inference. Schulman et al. (2015) and ADVI (Kucukelbir et al., 2017) have proposed alternative gradient estimation techniques, but with limitations in applicability to general variational inference tasks. Our proposed G-TRANS gradient model extends the reparameterization trick to a larger class of variational distributions, providing a flexible way to construct gradient estimators with lower variance for faster convergence. Future work includes constructing G-TRANS gradient estimators for distributions lacking analytical high-order moments. The G-TRANS gradient model extends the reparameterization trick to variational distributions without analytical high-order moments. Utilizing approximation theory, high-order polynomial functions can effectively approximate test functions with cheap computations. Constructing velocity fields with optimal transport theory is a promising direction. The proof of Theorem 3.1 involves transforming random variables and satisfying constraints, leading to conclusions about derivatives with respect to \u03b8. The proof of Theorem 3.1 concludes with the transport equation for the reparameterization trick, showing independence of the standardization distribution with \u03b8. By considering the unbiased constraint, we simplify the term to focus on (r \u03b8 (z, \u03b8)) 2. The Euler-Lagrange equation simplifies this further, highlighting the intractability in real-world scenarios. The unbiasedness constraint is satisfied if h(z, \u03b8) is bounded, leading to the verification of v \u03b8 ah (z, \u03b8). In the G-TRANS framework, the dual polynomial velocity field v \u03b8 dp leads to a result similar to Proposition 4.2. If Cov(P k \u2202 log q(z,\u03b8) \u2202\u03b8 , (2f \u2212 P k ) \u2202 log q(z,\u03b8) \u2202\u03b8 ) > 0, then the gradient estimator from the dual polynomial velocity field has lower variance than the score function gradient estimator."
}