{
    "title": "r1ltgp4FwS",
    "content": "We focus on temporal self-supervision for GAN-based video generation tasks, emphasizing the importance of temporal relationships in generated data for tasks like video super-resolution and unpaired video translation. Existing methods often use norm losses like L2 for super-resolution, leading to smooth results lacking spatial detail. Our approach introduces a temporally self-supervised algorithm that utilizes temporal adversarial learning to achieve coherent solutions with spatial detail. Additionally, we propose a Ping-Pong loss for improved learning objectives. Our approach introduces a novel Ping-Pong loss to improve long-term temporal consistency in GAN-based video generation tasks. It effectively prevents recurrent networks from accumulating artifacts over time without sacrificing spatial detail. Additionally, we propose metrics to quantitatively evaluate accuracy and perceptual quality of temporal evolution, confirmed through user studies. Applying GANs directly to sequence generation without constraints often leads to strong artifacts due to temporal changes, making conditional video generation tasks challenging. In our work, we propose a novel adversarial learning method for video generation tasks, focusing on spatial content and temporal relationships. This approach is applied to video super-resolution and unpaired video translation tasks, producing realistic results with coherent structures over time. Our work introduces a spatio-temporal adversarial training method for video generation tasks, ensuring realistic and coherent results over time. We employ a novel \"Ping-Pong\" loss for long-term consistency and include metrics for quantifying temporal coherence. Our work introduces a spatio-temporal adversarial training method for video generation tasks, ensuring realistic and coherent results over time. The model outperforms previous work in terms of temporally-coherent detail, quantified with various metrics and user studies. The model can transfer facial expressions between different domains by establishing correct temporal cycle-consistency. Deep learning has shown significant progress in image generation tasks, with adversarial training improving perceptual quality. Adversarial training has been found to enhance perceptual quality in various multi-modal problems such as image super-resolution and image translations. Perceptual metrics are proposed for evaluating image similarity based on semantic features. Video generation tasks aim for realistic results that evolve naturally over time, with recent works in video super-resolution focusing on spatial detail and temporal coherence. Adversarial learning combined with a recurrent structure shows promise for improving temporal modes in video super-resolution. Applying adversarial training with a recurrent structure improves temporal modes in video super-resolution tasks. GANs are commonly used for video translation, but discriminators often only supervise spatial content. To address this, spatiotemporal adversarial training is proposed for video style transfer and video translation tasks, emphasizing the importance of temporal self-supervision for enhancing spatio-temporal correlations without sacrificing spatial detail. Temporal smoothness in video style transfer tasks is enforced using warping techniques, but it can lead to a loss of spatial detail and temporal changes in the output. Current methods for quantifying temporal coherence are sub-optimal, and there is a lack of perceptual metrics for evaluating natural temporal changes. This study addresses this issue by proposing two improved temporal metrics and showcasing the benefits of temporal self-supervision over direct temporal losses. Previous approaches have used adversarial temporal losses to achieve time consistency, but they may not be suitable for videos as they rely on ground truth motions. The curr_chunk discusses the limitations of using realism in temporal changes for videos and introduces a new approach focusing on unpaired translation tasks. It also mentions the use of L2-based time-cycle losses for tracking and optical flow estimation, as well as a PP loss for video generation. The approach aims to improve the quality of results through indirect optimization and direct constraint via generated video content. The generative network described in the curr_chunk produces image sequences in a frame-recurrent manner using a recurrent generator G and a flow estimator F. The generator maps data from domain A to domain B, recursively using previous outputs and motion estimation to align frames. This process improves long-term temporal consistency in video results. The generative network uses two recurrent generators for unpaired tasks like UVT, with one mapping from domain A to B and the other back. A ResNet architecture is used for VSR generator G and an encoder-decoder structure for UVT generators and F. A novel spatio-temporal discriminator D s,t is used to supervise triplets of frames, improving temporal consistency in video results. The proposed D s,t architecture receives two types of triplets: three adjacent frames and corresponding warped frames. Warped frames provide temporal information with aligned content. For VSR tasks, D s,t guides the generator to learn the correlation between LR inputs and high-resolution targets. The proposed D s,t architecture uses LR frames as conditional inputs to guide the generator in learning the correlation between LR inputs and high-resolution targets. The discriminator penalizes unrealistic artifacts in generated images and ensures temporal cycle-consistency between different domains. Our approach uses unconditional spatio-temporal discriminators to establish connections between different domains, focusing on improving spatial features before enhancing temporal correlation. The input for the generator can be a static triplet, a warped triplet, or the original triplet. The discriminator of our approach only takes one triplet at a time, unlike previous methods. The network transitions between different types of triplets during training, with a distribution of (50%,25%,25%). The GAN training results in dynamic goals due to discriminative networks discovering learning objectives. Inputs strongly influence the training process and final results. Modifying inputs in a controlled manner can lead to substantial improvements in training results. The concatenation of frames is a crucial operation for discriminators to understand spatio-temporal data distributions and reduce temporal problems in spatial GANs. The spatiotemporal adversarial loss is essential for preventing blurred structures in multi-modal datasets. Using a single D s,t network can balance spatial and temporal aspects effectively, avoiding inconsistent sharpness and overly smooth results. By extracting shared spatio-temporal features, smaller network sizes are enabled. In adversarial training, generators can converge towards reinforcing spatial features over longer periods, leading to temporal mode collapse. This issue can be alleviated by training with longer sequences, but generators should ideally work with sequences of arbitrary length for inference. To address the problem of recurrent generators working with sequences of arbitrary length for inference, a new Result trained with PP loss is proposed. The PP loss minimizes the L2 distance between ground-truth and generated images to remove artifacts and improve temporal coherence. A bi-directional \"Ping-Pong\" loss is used to construct symmetric PP sequences for natural videos, ensuring that generated results stay close to valid information and are symmetric. The proposed method involves training recurrent generators with extended PP sequences to ensure symmetry in the generated outputs. A PP loss is used to constrain the outputs from both \"legs\" to be identical, removing drifting artifacts and preserving high-frequency details. This approach effectively extends the training data set and serves as a form of data augmentation. The proposed method involves training recurrent generators with extended PP sequences to ensure symmetry in the generated outputs. A comparison in Appendix E disentangles the effects of augmentation of PP sequences and temporal constraints. Results show that temporal constraints are key to suppressing artifacts and allowing models to infer longer sequences. Perceptual loss terms use feature maps from a pre-trained VGG-19 network and in-training discriminators to encourage generator output similarity to ground truth in VSR tasks. The method involves integrating a spatio-temporal discriminator into VSR and UVT tasks. Different discriminator losses are used for each task, along with various loss functions for training the generators. The generators are trained with mean squared loss, adversarial losses, perceptual losses, PP loss, and warping loss. The method integrates a spatio-temporal discriminator into VSR and UVT tasks, using different losses for each task. Generators are trained with mean squared loss, adversarial losses, perceptual losses, PP loss, and warping loss. The effects of temporal supervision are illustrated through ablation studies and training models with different inputs. The study compares TecoGAN model variants to other VSR and UVT models, highlighting differences in temporal information. ENet and CycleGAN are single-image adversarial models, FRVSR and DUF are VSR methods without adversarial losses, and RecycleGAN is a spatial adversarial model with temporal evolution learning. DsOnly improves temporal coherence with framerecurrent connection, but shows high-frequency changes between frames. Adding temporal discriminator (DsDt) enhances coherence, but may lead to drifting artifacts in long sequences due to reinforcement of existing details. The DsDtPP model effectively suppresses drifting artifacts in long sequences by adding self-supervision for long-term temporal consistency. It balances the generator and discriminators to achieve continuous and detailed temporal profiles without streaks from temporal drifting. The TecoGAN model, trained with a Ds,t discriminator, achieves excellent quality with halved network size and reduced resource usage. The TecoGAN model, trained with a single discriminator, shows significant reduction in resource usage compared to using two discriminators. It yields similar quality to DsDtPP with faster and more stable training. A larger generator with 50% more weights was also trained for better detail generation. The focus is on the larger single-discriminator architecture with PP loss as the full TecoGAN model for VSR. Results and comparisons are shown in figures and video results are available in supplemental material. The curr_chunk discusses an ablation study for the UVT task, starting from a single-image GAN-based model and progressing to a model with spatio-temporal discriminators. The study uses renderings of 3D fluid simulations for training data. The study explores video translation from domain A to B using different models. The DsOnly model shows better temporal coherence but struggles with spatio-temporal cycle-consistency. The D s,t model improves cycle-consistency by correlating spatial and temporal aspects. However, without L pp, the model reinforces detail over time undesirably. The full TecoGAN model, including L pp, produces the best results with detailed smoke structures. The TecoGAN model with L pp yields the best results, showing detailed smoke structures and good spatio-temporal cycle-consistency. A DsDtPP model with multiple networks performs similarly but the spatio-temporal D s,t architecture is preferred due to its natural balance of temporal and spatial components, requiring fewer resources. Future work may explore variants like a shared D s,t for both domains. An ablation study on the Obama and Trump dataset also shows promising results. The smoke dataset is used for an ablation study on temporal adversarial learning for VSR. Different inputs are tested for the discriminator network, focusing on the spatio-temporal features of the target domain. The simplest input consists of original, unwarped triplets, leading to a sub-optimal quality baseline model. The baseline model trained using original triplets lacks sharp spatial structures and coherent motions. The vid2vid network improves temporal coherence by using a video discriminator for supervised output sequences. However, training a variant with estimated motions and original triplets does not significantly improve results for unpaired translation tasks. The discriminator still struggles to correlate spatial and temporal features effectively. The TecoGAN model for UVT improves spatial details and motion coherence by incorporating static, warped, and original triplets during training. This approach helps the network learn to extract spatial features first and then build on them to establish temporal features, ultimately improving the correlation between spatial and temporal content. The TecoGAN model enhances spatial details and motion coherence by utilizing original triplets to establish a correlation between spatial and temporal content. The discriminator guides the generator to produce coherent structures, leading to detailed and moving visual results. Quantitative evaluations focus on the VSR task, including user studies and spatial metrics, along with the proposal of novel temporal metrics for assessing temporal coherence. The TecoGAN model improves spatial details and motion coherence by using original triplets to connect spatial and temporal content. Two new temporal metrics are proposed to measure temporal coherence, including tOF and tLP, which aim to provide consistent results over time. The model outperforms others in terms of LPIPS, showcasing its ability to capture semantic similarities. The TecoGAN model introduces two new metrics, tOF and tLP, to measure temporal consistency. tOF measures pixel-wise motion differences, while tLP measures perceptual changes using deep feature maps. Both metrics are crucial for quantifying realistic temporal coherence. Evaluation shows that temporal adversarial models outperform spatial ones, with TecoGAN performing well in capturing spatial detail and temporal coherence. TecoGAN model performs well in capturing spatial detail and achieving good temporal coherence, comparable to non-adversarial methods like DUF and FRVSR. User studies confirm TecoGAN's results are closest to ground truth in VSR tasks. The model also introduces new metrics tOF and tLP to evaluate temporal consistency, showing good tLP scores and temporal coherence on par with RecycleGAN. The proposed GAN architecture and PP loss enable stable temporal functions in VSR, surpassing direct supervision. Results show coherent details and cycle consistency in UVT tasks, reflected in metrics and user studies. Realistic outputs are achieved for natural images, with some limitations in under-resolved faces and text. The method shows stable performance in VSR and UVT tasks, with potential for motion translation. Despite non-linear training, the method remains stable over multiple runs and can serve as a basis for various generative models. Qualitative and quantitative analyses are provided in appendices for further insights. The text discusses the metrics, user studies, network architecture, and performance evaluation of the TecoGAN model for video super-resolution tasks. The model is tested on various datasets and compared with other methods. The TecoGAN model is compared to other methods using the same dataset, showing improved details in down-sampled and captured images. The model is trained for various tasks including translations between smoke simulations and real-smoke captures, narrowing the gap between simulations and real-world phenomena. Evaluation is done using PSNR and LPIPS metrics. The TecoGAN model, evaluated using PSNR and LPIPS metrics, achieves high perceptual quality with a slight decrease in PSNR compared to other models. The model shows improved details in down-sampled and captured images, narrowing the gap between simulations and real-world phenomena. TecoGAN outperforms other methods by more than 40% for LPIPS, showing good perceptual quality and reasonable pixel-wise accuracy. Evaluating temporal coherence in VSR and UVT is challenging without ground-truth motion, as shown by T-diff metric. Pixel-wise motion difference and perceptual changes over time are measured for VSR task, revealing errors in ENet due to flickering content. Bi-cubic up-sampling, DUF, and FRVSR achieve low errors in temporal evaluations. Our DsOnly model produces sharper results than ENet, sacrificing temporal coherence. By incorporating temporal information into discriminators, models like DsDt, DsDt+PP, TecoGAN, and full TecoGAN show improvements in temporal metrics. TecoGAN excels in UVT tasks with sharp spatial features and coherent motion, outperforming previous methods on the Obama&Trump dataset. The importance of evaluating results using a combination of spatial and temporal metrics is highlighted. It is important to consider perceptual changes in temporal evaluations, as shown with proposed temporal coherence metrics tOF and tLP. tOF compares motions instead of image content, making it more robust. The calculation of tLP can work reliably with different perceptual metrics, as demonstrated with the PieAPP metric. The results are visually consistent and reflect motion differences effectively. The conclusions from the evaluation of the PieAPP error function closely match the LPIPS-based evaluation. Results on the Vid4 dataset and Tears of Steel datasets show that the network architecture can generate realistic and coherent detail. The proposed metrics allow for stable evaluation of temporal perception in video sequences. The study conducted user studies for the Vid4 scenes to evaluate temporal perception metrics. Various VSR methods were compared using a 2AFC design, with participants making pair-wise choices while watching synchronized videos. The metrics were found to reliably capture human temporal perception. The study evaluated VSR methods for Vid4 scenes through user studies using a 2AFC design. Participants made pair-wise choices while watching videos, with no control over playback. Scores were computed using the Bradley-Terry model, showing TecoGAN model performing well in three cases and achieving second place in the walk scene due to smoother images and unexpected details. The user study showed that users preferred the TecoGAN output over other deep-learning methods with a 63.5% probability. TecoGAN achieved spatial improvements in all scenes compared to DUF and FRVSR, but had lower scores in temporal metrics for certain scenes. Overall, the metrics confirmed the performance of TecoGAN and matched the results of the user studies. In user studies comparing TecoGAN with other deep-learning methods, TecoGAN was preferred with a 63.5% probability. Results from evaluating CycleGAN, RecycleGAN, and TecoGAN on Obama&Trump datasets showed TecoGAN outperforming the others in preserving original expressions. The y-axis in the evaluation was more crucial, indicating TecoGAN > RecycleGAN > CycleGAN rankings with clear separations. The x-axis assessed if the inferred result matched the general spatio-temporal content of the target domain. The TecoGAN model consistently outperformed other deep-learning methods in user studies, with participants preferring its results. Flow estimation near image boundaries is less accurate, leading to alignment issues during warping. The output of the flow estimation network is less reliable at image boundaries due to a lack of neighborhood information. Objects may move into or out of the field of view near the boundaries. The TecoGAN model outperformed other deep-learning methods in user studies. Flow estimation near image boundaries is less accurate, causing alignment issues during warping. Objects moving into or out of the field of view significantly affect images warped with inferred motion. Curriculum learning is used for UVT discriminators during training. The proposed PP loss in TecoGAN includes Ping-Pang data augmentation and temporal consistency constraint to address issues with streaking artifacts in recurrent models. Another TecoGAN variant was trained without the constraint to show the separate contributions of data augmentation. The VSR generator in TecoGAN includes operations like convolution, transposed convolution, element-wise addition, and densely-connected layers. The architecture consists of sequential residual blocks with different numbers in the generator. Each ResidualBlock contains specific operations to enhance the resolution of input tensors. The VSR D s,t architecture includes operations like convolution, ReLU, and BatchNorm. The discriminators DsDt, DsDtPP, and DsOnly have a similar architecture to D s,t. The flow estimation network F consists of convolutional layers with Leaky ReLU and MaxPooling. The VSR D s,t architecture includes operations like convolution, ReLU, and BatchNorm. The discriminators DsDt, DsDtPP, and DsOnly have a similar architecture to D s,t. The flow estimation network F consists of convolutional layers with Leaky ReLU and MaxPooling. For UVT tasks, a learning rate of 10^-4 is used for training, with the input domain images cropped to 256x256. Additional training parameters are listed in Table 6. L content and L \u03c6 are used to improve convergence, with L content faded out in the first 10k batches and L \u03c6 used for the first 80k and faded out in the last 20k. TecoGAN is implemented in TensorFlow, with only the trained generator network needed for inference after training. Models are evaluated on... The VSR models are evaluated on a Nvidia GeForce GTX 1080Ti GPU, with the larger TecoGAN model showing slightly slower performance compared to FRVSR. The TecoGAN model outperforms the DUF model in terms of performance despite having a smaller size."
}