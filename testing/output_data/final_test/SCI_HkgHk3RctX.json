{
    "title": "HkgHk3RctX",
    "content": "Ranking is crucial in machine learning and information retrieval, focusing on presenting a user with an appealing slate of items by considering interactions between them. Placing an item on the slate influences the selection of other items alongside it. The proposed seq2slate model for ranking predicts the next item to place on the slate based on items already chosen, capturing complex dependencies. The model is trained from weak supervision using click-through data and shows promising results in experiments on ranking benchmarks and a real-world recommendation system. Pointwise rankers trained from click-through data optimize a loss function BID17. This approach scales well for large problems but struggles to capture dependencies between ranked items, especially when limited display area or strong position bias is present. In such cases, presenting a diverse set of items at the top positions may be preferable to cover a wider range of user interests. In learning-to-rank, approaches consider interactions between ranked items during training. Pairwise methods train a classifier to rank items within pairs, while listwise methods consider the full permutation of items. Despite inter-item dependencies in the loss functions, the ranking function remains pointwise. Some work focuses on capturing interactions between items in the ranking scores themselves to influence item placement in the final ranking. In this paper, a general, scalable approach to ranking is presented, which considers high-order interactions. A sequence-to-sequence model is applied to the ranking task, where the input is a list of candidate items and the output is the resulting ordering. The model, called sequence-to-slate, takes into account the order in which the input is processed, and assumes the availability of a base ranker for ordering the input sequence. The curr_chunk discusses using pointer networks, a type of seq2seq model with an attention mechanism, to address the seq2seq problem in ranking. The network is trained end-to-end using weak supervision from click-through data instead of ground-truth rankings. The approach is shown to be useful in learning-to-rank benchmarks. The proposed approach utilizes pointer networks in a seq2seq model with an attention mechanism to address the ranking problem. It aims to predict the output ranking given input items, such as music recommendations for a user query, by maximizing user engagement. The approach is trained using weak supervision from click-through data and has shown utility in learning-to-rank benchmarks. The proposed approach uses pointer networks in a seq2seq model with an attention mechanism to predict rankings based on input items, without fixed output vocabulary. It captures high-order dependencies between items in a ranked list, including diversity and similarity. The model is trained using weak supervision from click-through data and has shown effectiveness in learning-to-rank benchmarks. The seq2slate model utilizes pointer networks to predict rankings based on input items, without a fixed output vocabulary. It consists of an encoder and decoder RNNs with LSTM cells, where the encoder transforms the input sequence into latent memory states and the decoder uses an attention function to output rankings. The attention function in the seq2slate model uses a query vector to produce a probability distribution over the next item to include in the output sequence. This probability is obtained via a softmax over the remaining items and represents the model's focus at each decoding step. The selected item's embedding is then fed into the next decoder step. The model in the seq2slate framework uses a differentiable vector denoted as go in FIG0 for gradient-based learning. It makes no explicit assumptions about item interactions and can capture rich interactions like diversity or similarity. The computational cost of inference is O(n^2) but can also use a cheaper single-step decoder with linear cost O(n). The seq2slate model trains by logging click-through data from an existing ranker. Each training example includes a sequence of items and binary user feedback. The goal is to learn parameters for good rankings. The model uses a differentiable vector for learning and can capture diverse item interactions. Inference cost is O(n^2) but can also use a cheaper single-step decoder with linear cost. The model is trained using click-through data from a ranker to learn parameters for good rankings. Different performance measures can be used to evaluate the quality of rankings. In the standard seq2seq setting, models are trained to maximize the likelihood of target sequences, but in this case, weak supervision in the form of labels is used instead of ground-truth tokens. The seq2slate model can be trained directly from weak supervision labels (e.g clicks) using reinforcement learning to optimize for ranking measures. Policy gradients and stochastic gradient ascent are used to maximize the expected ranking metric obtained by sequences sampled from the model. The gradient is formulated using the REINFORCE update and can be approximated via Monte-Carlo sampling. Supervised learning using labels y is proposed as an alternative to reinforcement learning for optimizing seq2slate model. Per-step loss functions like cross-entropy and hinge loss are considered for assigning scores to input items. To improve convergence in supervised learning with labels y, a smooth version of the hinge-loss function is used, replacing maximum and minimum with smooth counterparts. Loss at each decoding step depends on items already chosen to avoid incurring further loss after a correct prediction. Loss is incurred after a correct label prediction. A smooth version of the hinge-loss function is used in supervised learning with labels y. The loss at each decoding step depends on items already chosen to prevent further loss. Per-step weight w j can improve performance earlier in the sequence. Teacher-forcing is not an option, so the model feeds on its previous predictions. Two policies for producing a permutation during training are sampling and greedy decoding, each with their corresponding losses. The greedy policy selects the item maximizing p \u03b8 at each time step, resulting in a non-continuous loss. The sampling policy draws each permutation from p \u03b8, leading to a differentiable loss. The gradient for the sampling policy is formulated as DISPLAYFORM0 and can be approximated by DISPLAYFORM1. The gradient is formulated as DISPLAYFORM0 and can be approximated by DISPLAYFORM1, with b(x k ) as a baseline. Stochastic gradient descent decreases both sample loss and the probability of high loss samples. This differs from scheduled sampling, which ignores the probability of sampling high loss sequences. The approach includes both terms and applies to training sequence-to-sequence models. Training policies involve minimizing loss via stochastic gradient descent over mini-batches. Performance evaluation is done on ranking tasks, including learning-to-rank benchmark data. In the implementation details, hyperparameters are set based on literature, using mini-batches of 128 training examples and LSTM cells with 128 hidden units. The Adam optimizer is used with an initial learning rate of 0.0003, decayed every 1000 steps by a factor of 0.96. Network parameters are initialized uniformly at random. Regularization techniques include dropout with a probability of 0.1 and L2 regularization with a penalty coefficient of 0.0003. Results are based on supervised training with cross-entropy loss and a sampling policy. Inference metrics are reported for the greedy policy, using an exponential moving average with a decay rate of 0.99 as the baseline. When training the seq2slate model with REINFORCE, R = NDGC@10 is used as the reward function without model regularization. Experiments with a bidirectional encoder RNN did not show significant improvements. Two learning-to-rank datasets were used: Yahoo Learning to Rank Challenge data and Web30k dataset. Performance of seq2slate and baselines on diverse-clicks data was evaluated. The procedure to generate click data involved training a base ranker from raw data. The base ranker is trained using the RankLib package, selecting the best performing model for each dataset. A user cascade model is simulated to generate training data, where items are clicked based on relevance scores. To introduce high-order interactions, a generative process called diverse-clicks is used, where users only click on relevant items that are not too similar to previously clicked items. The study uses a diverse-clicks generative model to simulate user behavior and train the seq2slate model and baseline rankers. The baseline rankers include AdaRank, Coordinate Ascent, LambdaMART, ListNet, MART, Random Forests, RankBoost, and RankNet. The results show that the seq2slate model performs comparably to state-of-the-art models. The study compares the seq2slate model to state-of-the-art models and baseline rankers, showing that seq2slate outperforms all baselines. Visualizations of attention probabilities reveal how the model captures item dependencies. Different training variants are explored, with results reported in TAB2, including rank-gain metrics per example. The study compares the seq2slate model to state-of-the-art models and baseline rankers, showing superior performance. Different training variants are evaluated, with supervised learning performing best. Results show positive rank gain with weakly supervised methods. Training with REINFORCE yields mixed results on different datasets. Performance between greedy and sampling policies during training shows no significant difference. The study compares the seq2slate model to state-of-the-art models and baseline rankers, showing superior performance, especially with supervised learning. In contrast, the one-step decoder in TAB2 performs comparably to the sequential decoder, indicating faster inference time. However, on more complex real-world data, sequential decoding outperforms. Sensitivity to input order is highlighted, showing that seq2seq models are often sensitive to input processing order, as demonstrated in shuffled data results in TAB2. The study demonstrates that seq2slate performs significantly worse in shuffled data, indicating that reranking is easier than ranking from scratch. The model shows adaptivity to different types of interactions, such as the similar-clicks model, with comparable performance to baseline rankers and slightly better results on harder datasets like Web30k. The study shows that the model adapts well to various data interactions, including a ranking problem in a commercial recommendation system. Training with massive click-through logs, the model outperforms the production base ranker in test data, suggesting that sequential decoding captures complex dependencies better. Live experiment results also support the effectiveness of the learned seq2slate model. The study demonstrates that the seq2slate model performs well in a live A/B testing experiment, showing significantly higher click-through rates in top positions. The research builds on the success of seq2seq models in various prediction tasks and focuses on training seq2seq models for the ranking task using weak feedback data. The seq2slate model is a novel approach to training seq2seq models for ranking using deep models and embedding the entire slate of items in the RNN state. This method allows for simultaneous optimization of embeddings and sequence model training. Compared to other approaches, seq2slate uses sequential decoding, which is found to be crucial in certain applications. The seq2slate model is a novel approach to ranking sets of items using deep models and sequential decoding, which is crucial in certain applications. Unlike other methods, it focuses on learning from click-through data rather than full rankings or relevance scores. Santa BID33 introduced a framework for learning permutations using the Sinkhorn operator, but it is not directly applicable to ranking settings without ground-truth permutations. Pointer-networks are particularly suitable for this approach, addressing the challenge of training the model from weak user feedback. The proposed approach aims to improve ranking quality by training the model from weak user feedback. Experiments show scalability and significant ranking improvements. Future work includes exploring Transformer network BID36, beam-search inference, Actor-Critic training, and studying off-policy correction."
}