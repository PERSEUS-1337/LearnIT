{
    "title": "BJgb3k3qpE",
    "content": "The convolutional filters initialized farthest apart using pre-computed Grassmannian subspace codebooks performed well across datasets. This paper aims to share initial results to inspire the deep-learning community to explore classical Grassmannian subspace packing for more efficient ideas. The curr_chunk discusses standard initialization methods for neural networks, specifically focusing on the first convolutional layer of a CNN model for MNIST digit classification. It explains the application of convolutional filter weights to an image patch to extract feature values. The initialization of these filter weights is studied in relation to activation functions and architectures. The curr_chunk provides a plethora of resources detailing best practices for initialization strategies in deep learning frameworks. Common strategies include Lecun, He, Glorot/Xavier, and Orthogonal, with recommendations based on activation functions. Recent works have questioned the efficacy of initialization strategies for specific architectures like deep residual networks. Yang & Schoenholz (2017) critique the dependence of optimal init-variances on network depth and propose a mean field residual networks framework. BID5 emphasizes the efficacy of using swish activation functions over RELU-like functions. In Binary neural networks, Hadamard initialization is showcased as effective. Default initializations for layers and activation types vary among deep learning frameworks, sparking debate among practitioners. Researchers continue to work on improving initialization strategies in deep learning. The paper discusses the need for an optimal initialization strategy in deep learning, building on previous work in Grassmannian subspace-packing. The goal is to prevent convolution filters from learning similar attributes post-training. The authors in 2014 proposed best practices for improving convolutional networks, focusing on different kernel sizes, stride lengths, and feature-scale clipping to prevent dominance of one feature kernel. They demonstrated how smaller strides and filter sizes led to more distinctive features and fewer dead features, aiming for each filter to learn something different efficiently. The curr_chunk discusses the importance of inter-filter diversity in convolutional networks by maximizing the minimum pairwise distance between filters in the Grassmannian N-subspace packing problem. This aims to efficiently utilize computational resources and improve classification performance. The paper focuses on maximizing the minimum distance between k-dimensional subspaces in the Grassmannian N-subspace packing problem, specifically for the line-packing scenario. It involves arranging unit vectors to minimize magnitude correlation, resulting in a codebook matrix with a minimum distance of packing. The Rankin bound provides an upper limit for this distance, and a normalized invariant measure allows for volume computation in G(m, 1). The normalized Haar measure on \u2126 m allows computation of volumes in G(m, 1) to define the density of a line-packing matrix W. Pre-computed repositories exist for the best known packings in complex 3 and real scenarios. The codebook repository is limited up to (m = 16, 1, N = 45). One approach is to construct packings in Grassmannian manifolds using the alternating projection method. The paper explores architectures based on filter-sizes (m) and number of filters (N). In experiments, shallow CNNs with 2 to 4 convolutional layers were tested to capture diverse features. Grassmannian initializations were compared with standard Xavier initialization for first-layer convolutional filter kernels. The goal was to allow the network to learn different combinations and activations of diverse first-layer features. In experiments, shallow CNNs with 2 to 4 convolutional layers were tested to capture diverse features. Grassmannian initializations were compared with standard Xavier initialization for first-layer convolutional filter kernels. The goal was to allow the network to learn different combinations and activations of diverse first-layer features. For single-channel inputs like MNIST, KMNIST, and Fashion MNIST, line packings of (m, 1, N) were used for the first convolutional kernels. This involved finding the best way to 'stab' N lines through a 9-D sphere with a minimum distance between each line maximized. Similar experiments were conducted on ResNets using a standard ResNet-56 model with Xavier initialization and Adam optimizer as baseline. The weights of the first convolutional kernel were initialized as Grassmannian line packings for images with 3 input channels, with N as the number of output channels and N = 3N due to the input channel size. The first layer was initialized using this line packing without biases and trained under both fixed and trainable conditions. In experiments, Grassmannian initializations were compared with standard Xavier initialization for first-layer convolutional filter kernels in shallow CNNs. The first layer was trained under fixed and trainable conditions, showing better test accuracies on different datasets with Grassmannian initializations. Trainable Grassmannians outperformed standard initializations, especially with SGD optimizer, even in deeper networks like ResNets. Adam optimizer showed faster convergence in ResNets but Grassmannians still achieved slight accuracy improvements on the final test-set. The curr_chunk discusses achieving slight accuracy improvements on the final test-set classification score using Adam optimizer after training for 200 epochs. The code is also provided for reproducibility and includes a framework for extracting Grassmannians."
}