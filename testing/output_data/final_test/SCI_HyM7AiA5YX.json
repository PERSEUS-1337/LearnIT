{
    "title": "HyM7AiA5YX",
    "content": "Training deep neural networks traditionally involves using a primary objective like softmax cross entropy for classification and sequence generation. However, this approach mainly focuses on the ground-truth class and overlooks information from incorrect classes. To address this, a new training paradigm is proposed that maximizes the likelihood of the ground-truth class while neutralizing the probabilities of the complement classes. Experimental results across various tasks show that this dual-objective training improves model performance significantly. The experimental results confirm that training with both primary and complement objectives improves the performance of state-of-the-art models across all tasks and makes them more robust to single-step adversarial attacks. Statistical learning algorithms optimize training objectives like likelihood and cross entropy, which is popular for deep neural networks. Cross entropy is used for tasks such as classification and sequence generation. Training with cross entropy aims to minimize the difference between predicted and actual probabilities. The primary objective of minimizing entropy in neural network training has shown success but has limitations in utilizing information from incorrect classes. The model used is ResNet-110 with an embedding representation before softmax operation. Visualization using t-SNE shows cleaner and more separable class clusters in (b) compared to (a), leading to improved classification results. Complement Objective Training (COT) is a new training paradigm that neutralizes predicted probabilities for classes other than the ground truth without compromising the model's primary objective of minimizing entropy. This approach aims to evenly suppress complement classes while maximizing the predicted probability for the ground truth class. In this paper, the authors propose \"complement entropy\" to complement the softmax cross entropy for neutralizing the effects of complement classes. The neural net parameters are updated iteratively to minimize cross entropy and maximize complement entropy. Experimental results confirm that COT improves accuracies for image classification and language understanding tasks. Models trained by COT are also shown to be more robust. The authors introduce \"complement entropy\" to neutralize complement classes in neural network training. This new training algorithm alternates between maximizing cross entropy and complement entropy. Experimental results demonstrate that models trained with this method are more robust to adversarial attacks. The authors introduce \"complement entropy\" to neutralize complement classes in neural network training, optimizing on the complement entropy drives predicted probabilities of complement classes to be neutralized. This helps the neural net generalize better and be more robust to adversarial attacks. The authors introduce \"complement entropy\" to neutralize complement classes in neural network training, optimizing on the complement entropy drives predicted probabilities of complement classes to be neutralized. This helps the neural net generalize better and be more robust to adversarial attacks. The training mechanism involves alternating between primary and complement objectives, with additional forward and backward propagation required for the complement objective, making the total training time 1.6 times longer. The authors introduce \"complement entropy\" to neutralize complement classes in neural network training, optimizing on the complement entropy drives predicted probabilities of complement classes to be neutralized. This helps the neural net generalize better and be more robust to adversarial attacks. The training mechanism involves alternating between primary and complement objectives, with additional forward and backward propagation required for the complement objective, making the total training time 1.6 times longer. They perform experiments to evaluate COT on various tasks and test the model's robustness against adversarial examples. The authors introduce \"complement entropy\" to neutralize complement classes in neural network training, optimizing on the complement entropy drives predicted probabilities of complement classes to be neutralized. This helps the neural net generalize better and be more robust to adversarial attacks. They perform experiments to evaluate COT on various tasks and test the model's robustness against adversarial examples. The model trained by COT is attacked by adversarial examples, and additional efforts for tuning learning rates might be required for optimizers to achieve the best performance. The complement entropy can be modified to balance the losses between the primary and complement objectives. The authors introduce complement entropy to improve baseline models for image classification on datasets like CIFAR-10, CIFAR-100, SVHN, Tiny ImageNet, and ImageNet-2012. They use various baseline models like ResNet-110, PreAct ResNet-18, ResNeXt-29, WideResNet-28-10, and DenseNet-BC-121 with specific training settings. The models are trained using SGD optimizer with momentum, weight decay, and learning rate adjustments over 200 epochs. For training WideResNet-28-10, settings from BID26 are followed with learning rate adjustments at the 60th, 120th, and 180th epoch. No dropout (BID21) is applied. Baseline models for Tiny ImageNet and ImageNet-2012 follow settings from BID27. CIFAR-10 and CIFAR-100 datasets contain colored natural images split into training and testing sets. Pre-processing includes zero-padding, random cropping, and mirroring during training. Original images are used for testing. The COT model consistently outperforms baseline models in terms of classification errors for CIFAR-10 and CIFAR-100 datasets. Significant performance boosts are seen in models like ResNetXt-29, while others like WideResNet-28-10 and DenseNet-BC-121 also show improvements. Testing errors decrease after the 100th epoch, with COT showing consistent better performance compared to the baseline models. The SVHN dataset BID17 contains images from Google Street View, divided into training and testing sets. COT consistently outperforms baseline models, with the biggest improvement seen in ResNet-110 with an 11.7% error rate reduction. Performance improvement from COT stabilizes after the 100th epoch due to a decrease in learning rate. The Tiny ImageNet 1 dataset is a subset of ImageNet BID2 with 100,000 training images and 10,000 testing images across 200 classes. Four state-of-the-art models are used as baselines: ResNet-50, ResNet-101, and ResNeXt-101 (32\u00d74d). Data augmentation techniques are applied during training, and the testing data is augmented with central cropping. Experimental results in TAB4 show that COT consistently improves performance. Experimental results in TAB4 demonstrate that COT consistently enhances the performance of baseline models, specifically ResNet-50. The dataset used for image classification is (Russakovsky et al., 2015), containing 1.3 million training images and 50,000 testing images across 1,000 classes. Data augmentation techniques like random crops and horizontal flips are applied during training, while 224x224 center crops are used for testing. The experimental setup includes a minibatch size of 256, 90 training epochs, and an initial learning rate of 0.1, decayed by dividing 10 at the 30th, 60th, and 80th epoch. COTResNet-50 shows an improvement in the baseline error rate from 24.7 to 24.4. COTResNet-50 improves performance on image classification tasks. COT is also evaluated on NLU tasks like machine translation and speech recognition. Necessitates normalized complement entropy due to large number of target classes. Applies COT on seq2seq model with Luong attention mechanism on IWSLT 2015 English-Vietnamese dataset. The official TensorFlow-NMT implementation experiments with models using greedy decoder and beam search decoder. COT improves testing BLEU scores compared to the baseline NMT model on both decoders. The dataset used consists of 65,000 one-second utterances of 30 different types. The baseline model is referenced from BID27 and undergoes the same pre-processing steps for spectrogram generation. The spectrograms are zero-padded to equalize sample length. VGG-11 BID20 model is selected as the baseline and trained for 30 epochs using SGD optimizer with momentum. COT further reduces error rate by 1.56% compared to the baseline. Adversarial examples result in incorrect model outputs with high confidence. COT generates clear and well-separated class boundaries in embeddings. Models trained with COT are believed to generalize better and be more robust to attacks. White-box attacks are conducted to verify this conjecture. In white-box attacks, Fast Gradient Sign Method (FGSM) is used to create adversarial examples. For baseline models, perturbations are based on the primary gradient, while for models trained by COT, perturbations are based on the sum of the primary gradient and the complement gradient. Perturbation is limited to a maximum value of 0.1. The experiments conducted on FGSM transfer attacks show that models trained using COT are more robust to both white-box and transfer attacks on the CIFAR-10 dataset. The complement gradients in COT models help neutralize incorrect class probabilities, potentially explaining their effectiveness in forming adversarial perturbations. In this paper, Complement Objective Training (COT) is introduced as a new training paradigm that optimizes the complement objective in addition to the primary objective. The use of complement gradients in COT models may explain their robustness to both white-box and transfer attacks. The proposal of complement entropy as the complement objective aims to neutralize incorrect class probabilities. Complement Objective Training (COT) improves model performance and robustness to adversarial attacks. Future studies can explore non-entropy-based complement objectives and broader applications of COT, such as in generative models and object detection. Further investigation is needed on COT's behavior against advanced adversarial attacks. The performance of models trained using Complement Objective Training (COT) is evaluated on the CIFAR-10 dataset under I-FGSM transfer attacks. Models trained with COT show lower classification error compared to others. The experiment uses 10 iterations."
}