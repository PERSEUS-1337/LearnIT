{
    "title": "Skx1dhNYPS",
    "content": "The paper introduces VideoEpitoma, a neural network architecture designed to reduce the computational cost of CNNs for recognizing human actions in videos. It consists of a timestamp selector and a video classifier, with the selector choosing representative timesteps for the video. This approach aims to maintain performance while decreasing computation requirements. VideoEpitoma reduces computation by up to 50\\% for video classification using selected frames as input. The selector improves classifier choices despite being on separate CNNs, achieving state-of-the-art results on datasets like Charades and Breakfast Actions with reduced computation. The efficiency of human visual and temporal information processing allows for quick understanding of videos. Building efficient neural networks can help recognize actions in videos, with success seen in short-range action datasets. Long-range actions may take minutes to unfold. Long-range actions in videos can take minutes to unfold, posing a computational bottleneck due to the sheer number of frames to be processed. Attention mechanisms have been proposed as a potential solution, with applications in various computer vision tasks and language understanding. Soft-attention methods focus on significant visual signals, but still require full processing by neural networks without reducing computation costs. VideoEpitoma is a two-stage neural network proposed for efficient classification of long-range actions in videos. It utilizes neural gating to selectively process video frames, reducing computational costs without compromising performance. The VideoEpitoma model uses a timestep selector with lightweight CNNs to efficiently represent long-range actions. A gating module selects significant timesteps for video summarization, followed by a heavyweight CNN for recognition. This approach reduces computation while maintaining performance in video classification. The timestep selector benefits video classification models by reducing computation costs and improving gating mechanisms. State-of-the-art results are achieved on action recognition benchmarks with significant computational cost reductions. Efficient CNN architectures like MobileNet and ShuffleNet are used for video classification, with methods for pruning least important weights or filters. Efficient CNN architectures like ShuffleNet-3D and MobileNet-3D are used for video classification, with a focus on long-range actions in datasets like Kinetics and UCF-101. These actions can be classified with as little as 10 frames per video, but long-range videos may require up to a thousand frames for accurate classification. Neural architecture search is used to find lightweight models like NasNet-Mobile to reduce computational costs. This paper focuses on reducing the number of video frames needed for action recognition by selecting the most relevant frames. Another approach to reduce computation is through dynamically routing the compute graph of a neural network based on the complexity of input signals. This paper focuses on gating video frames for efficiency in action recognition. Different approaches include gating layers of a CNN, gating convolutional channels, and using a separate gater network. Various methods for frame sampling in short-range videos are discussed, such as learning ranking scores and using a student-teacher model. Frame sampling for long-range actions differs significantly from short-range actions. This paper focuses on frame selection for long-range actions in video classification. The Timestep Selector stage uses a lightweight CNN and a gating module to select important timesteps for video-level feature extraction. The VideoEpitoma model consists of two stages: Timestep Selector and Video Classifier. The VideoEpitoma model consists of two stages: Timestep Selector for timestep gating and Video Classifier for deep video-level representations and classification accuracy. The Timestep Selector selects relevant timesteps based on dominant visual concepts for recognition of long-range actions in videos. The Timestep Selector in the VideoEpitoma model learns concept kernels to efficiently summarize long-range actions in videos by selecting relevant timesteps based on dominant visual concepts like \"Pancake\", \"Eggs\", \"Pan\", and \"Stove\". This process involves transversing through thousands of timesteps and deciding which to consider based on similarity to the learned latent concepts, using a lightweight representation for efficient decision-making. The selector relies on an efficient LightNet to represent timesteps in a long-range video. A Gating Module is used to select relevant timesteps by comparing their features to concept kernels and converting the similarity scores to binary decisions. The Gating Module selects relevant timesteps by applying a modified activation function to the gating value \u03b1, ensuring better timestep gating. The Gating Module selects timesteps based on their importance using a modified activation function. A temporal modeling layer, self-attention, is used for conditional gating, allowing each timestep to be correlated with others in the video before gating. The gating module uses gated-sigmoid for selecting timesteps during training to prevent domination of features. A step-function is used for binary gating at test time. Sparse selection is enforced to choose few timesteps while maintaining classification accuracy. The selector may cheat by predicting gating values just above 0.5 to select all timesteps, as the only loss considered is classification. The VideoEpitoma approach uses L0 regularization on gating values to enforce sparsity on selected timesteps, preventing cheating by the selector. The video classifier, powered by HeavyNet, processes the subset of timesteps chosen by the selector for effective video classification. VideoEpitoma utilizes one layer of self-attention for temporal modeling and a two-layer MLP for classification. CNNs like LightNet and HeavyNet are fine-tuned on the dataset before training. The model is trained with batch size 32 for 100 epochs using Adam optimizer. MobileNetv3 is chosen for LightNet, while HeavyNet experiments with I3D, ShuffleNet3D, and ResNet2D. Gumbel noise and clipped sigmoid are used during training in the gating module. In the test phase, a step-function is used for binary gating values. The Breakfast Actions dataset is used for long-range actions. The Breakfast Actions dataset contains 1712 videos for long-range actions, with 10 classes of making breakfasts. It has video-level annotations and an average video length of 2.3 minutes. Charades is a benchmark for human action recognition with 157 action classes, divided into training videos. Charades dataset consists of 8k, 1.2k, and 2k videos for training, validation, and test splits, covering 67 hours. Each video is around 30 seconds long and labeled with 6 and 9 actions for training and test splits. The dataset meets the criteria of long-range actions and uses Mean Average Precision (mAP) for evaluation. An experiment is conducted on Breakfast dataset to determine if a Timestep Selector based on LightNet features benefits a classifier based on HeavyNet features. The study evaluates the benefit of a Timestep Selector on off-the-shelf CNN classifiers using different sampling methods and measures performance with sampled timesteps from videos. The selector achieves sparse selection of timesteps without degrading classification performance. The study demonstrates that a Timestep Selector can significantly reduce the number of processed timesteps for off-the-shelf classifiers like I3D, ResNet2D, and ShuffleNet3D while maintaining performance. This reduction in computation is more beneficial for efficient recognition of long-range actions compared to reducing the processing of each timestep. The results show that the stand-alone selector can enhance the performance of classifiers, raising the question of training VideoEpitoma end-to-end. Based on experiments with different CNNs, aligning timestep features of LightNet with HeavyNet is crucial for training VideoEpitoma end-to-end. The study shows that a Timestep Selector can reduce processed timesteps for classifiers like I3D, ResNet2D, and ShuffleNet3D, improving recognition of long-range actions. Stand-alone selector enhances classifier performance, prompting the question of end-to-end training for VideoEpitoma. The study demonstrates the importance of aligning timestep features of LightNet with HeavyNet for end-to-end training of VideoEpitoma. The Timestep Selector improves classifier performance by reducing processed timesteps for I3D, ResNet2D, and ShuffleNet3D. Stand-alone selector enhances classifier performance, prompting consideration for end-to-end training. The experiment shows that the gating module in HeavyNet, based on LightNet features, selects relevant timesteps for better classifier performance. Gating irrelevant visual evidence is crucial for recognizing long-range actions. For example, in distinguishing between \"Making Pancake\" and \"Preparing Coffee\", it is important to gate out irrelevant visual cues like \"Knife\" and only consider relevant ones like \"Pan\". This process, known as frame gating, improves action recognition accuracy. The Timestep Selector uses a temporal modeling layer before the gating module to enable correlation between timestep features and video context. Ablation study confirms that conditioning the gating mechanism on both timestep and video context results in better conditional gating compared to only conditioning on timestep features. Frame-conditioned gating leads to small variance in selected timesteps for action categories. The Timestep Selector shows small variance in selected timesteps for action categories, indicating less dependency on context. However, frame-conditioned gating results in more consistent ratios regardless of the category. Performance drops when using the variant of the Timestep Selector that is conditioned only on timestep features, acting as a saliency selector. Context-conditioned gating shows higher variance in selected timesteps, suggesting increased dependency on action category. The experiment compares the variance in gating mechanisms for context and frame selection. Context gating shows higher variance, indicating more dependency on action category. The study explores the tradeoff between accuracy and computation in recognizing long-range actions using VideoEpitoma and compares it with off-the-shelf CNNs like I3D, ResNet2D, and ShuffleNet3D. The conclusion highlights the computational budget differences between VideoEpitoma and competing CNNs. The experiment compares the variance in gating mechanisms for context and frame selection, showing higher dependency on action category for context gating. It explores the tradeoff between accuracy and computation in recognizing long-range actions using VideoEpitoma, MobileNet, ShuffleNet3D, and I3D. The study also compares VideoEpitoma with off-the-shelf CNNs like ResNet2D and highlights the computational budget differences. Additionally, it tests VideoEpitoma against off-the-shelf CNNs for recognizing multi-label action videos of Charades, noting differences in video length and content. VideoEpitoma outperforms ResNet2D in recognizing long-range actions in videos, with a focus on multi-label classifications in Charades. The neural model is designed to efficiently recognize mid-range videos, showcasing its superiority at different time scales. The paper highlights the differences between long-range and short-range actions, influencing the development of VideoEpitoma as a solution for efficient video recognition. The paper introduces VideoEpitoma, a neural model that efficiently selects salient parts of videos to improve off-the-shelf CNN classifiers' performance. It includes a novel gating module for selecting important timesteps and shows enhanced results when trained end-to-end. VideoEpitoma is tested on long-range action benchmarks, demonstrating its efficiency in saving computation while maintaining recognition effectiveness. VideoEpitoma efficiently selects salient video parts to enhance CNN classifier performance, particularly in recognizing long-range actions."
}