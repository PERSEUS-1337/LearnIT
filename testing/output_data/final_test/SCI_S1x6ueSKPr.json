{
    "title": "S1x6ueSKPr",
    "content": "Pre-trained deep neural network language models like ELMo, GPT, BERT, and XLNet have shown top performance in language tasks, but their size is impractical for mobile and edge devices. A significant portion of the model's memory is taken up by the input word embedding matrix. Knowledge distillation techniques have been successful in compressing large models, but struggle with different vocabularies. A new knowledge distillation technique is introduced to train a student model with a smaller vocabulary and dimensions, using a dual-training mechanism. The new knowledge distillation technique compresses the BERT-BASE model by over 60x while maintaining task performance, resulting in a compact language model under 7MB. Experimental results show higher compression efficiency and accuracy compared to other state-of-the-art techniques. Recent research has focused on compressing large pre-trained language models for real-time inference on resource-constrained devices. Techniques include model quantization and knowledge distillation to train smaller student models using knowledge from larger teacher models. Recent research has concentrated on compressing large pre-trained language models for real-time inference on resource-constrained devices. One significant bottleneck overlooked in previous efforts is the input vocabulary size and its corresponding word embedding matrix, which can make up a substantial portion of all model parameters. Distillation techniques are limited in reducing model sizes as they require the student and teacher models to share the same vocabulary and output space. To address this limitation, two novel ideas are presented to enhance knowledge distillation, particularly for BERT models, with the goal of reducing model sizes to a few mega-bytes. The focus is on reducing model sizes to a few mega-bytes by using a smaller vocabulary for the student model during distillation. The model leverages layer-wise teacher parameters to optimize the student model. Dual Training involves feeding a mix of teacher and student vocabulary-tokenized words to align embeddings. The study focuses on compressing neural network models by aligning teacher and student vocabularies for better knowledge transfer. By using dual training and shared variable projections, a highly-compressed student BERT model with a 61.94x compression ratio compared to the teacher model was trained, showing competitive performance in language modeling and downstream tasks. Model compression techniques for neural networks have evolved alongside the popularity of neural networks, aiming to reduce memory-intensive models. Approaches include matrix approximation, parameter pruning/sharing, weight quantization, and knowledge distillation. These methods aim to reduce the number of model parameters by approximating matrix parameters, removing redundant weights and neurons, and quantizing model weights. Model weight quantization techniques focus on mapping model weights to lower-precision integers and floating-point numbers, especially effective with hardware supporting efficient low-precision calculations. Knowledge distillation involves teaching a smaller student model to match the output label distributions from a larger teacher model, offering more modeling freedom. This approach works well for tasks like machine translation and language modeling. In the context of model weight quantization and knowledge distillation, some approaches focus on reducing the number of WordPiece tokens in the model vocabulary for BERT model compression. This involves techniques like dual training and shared projection to teach a smaller student model to match the output label distributions of a larger teacher model. In model weight quantization and knowledge distillation, techniques like dual training and shared projection are used to compress BERT models by reducing the vocabulary size. The teacher model, a 12-layer uncased BERT BASE, with 30522 WordPiece tokens and 768-dimensional embeddings, trains a smaller student model with fewer parameters. The student model is trained from scratch with a smaller vocabulary (5K) and hidden state dimension (e.g., 48), following the general knowledge distillation paradigm. The student model, with parameters denoted by \u03b8s, has a smaller vocabulary and dimensions compared to the teacher model. The vocabulary consists of 4928 WordPieces, with 93.9% overlap with the teacher's vocabulary. The student model is optimized using the teacher model's training objective. The student model's vocabulary is smaller than the teacher model's, with 93.9% overlap. The student model is optimized using the teacher model's training objective, which includes masked language modeling and next sentence prediction tasks. However, due to differences in vocabulary tokenization, the outputs of the two models may not align. This hinders the use of existing knowledge distillation techniques, so alternative approaches are explored. During distillation, a method is proposed to mix teacher and student vocabularies for aligning their output spaces. This involves randomly selecting tokens from a training sequence to segment using the student vocabulary, while others are segmented using the teacher vocabulary. This encourages alignment of representations for the same words in both vocabularies. During distillation, the model learns to predict words from student and teacher vocabularies by aligning their output spaces. This is done through masked language modeling, where words are segmented using both vocabularies to effectively learn student embeddings from teacher embeddings and model parameters. Dual training is performed only for teacher model inputs, with the student model receiving words segmented exclusively using the student vocabulary. Different softmax layers are used for each vocabulary during masked language modeling. Some approaches align the student model's intermediate predictions with the teacher's outputs for better generalization. During distillation, the model aligns student and teacher vocabularies through masked language modeling to learn student embeddings from teacher embeddings and parameters. To minimize information loss, trainable variables in the teacher model are projected into the same space as the student model using projection matrices U and V. In distillation, the model aligns student and teacher vocabularies through masked language modeling to learn student embeddings from teacher embeddings and parameters. Projection matrices U and V are used to project trainable variables in the teacher model into the same space as the student model. The final loss function includes masked language modeling cross-entropy losses for both student and teacher models. The distillation model aligns student and teacher vocabularies through masked language modeling to learn student embeddings from teacher embeddings and parameters. The final loss function includes cross-entropy losses for both student and teacher models. The evaluation of the knowledge distillation approach involves two classes of experiments: one using masked word prediction task for explicit evaluation of the language model, and another involving fine-tuning the language model with task-specific affine layer for implicit evaluation of the learned representations. During distillation, the teacher BERT model is used to train the student BERT language model with the same corpus. Dual training is enabled for teacher model inputs, with a probability set for segmenting words. Shared projection experiments utilize Xavier initialization for projection matrices. Loss weight coefficient is set after tuning, and direct distillation of the teacher BERT language is performed. The study focuses on distilling the teacher BERT language model to obtain a task-agnostic student model, which is then fine-tuned for downstream tasks. The distillation process is optimized using LAMB on Cloud TPUs, with training taking 2-4 days depending on the model dimension. Three variants of the student models are evaluated, including dual training with down-projection or up-projection of teacher model parameters. Training is done for embedding and hidden dimensions of 48, 96, and 192, resulting in 9 total variants. The study focuses on distilling the teacher BERT language model into task-agnostic student models with embedding and hidden dimensions of 48, 96, and 192. A total of 9 variants are trained using a compact 5K-WordPiece vocabulary. The smallest student model has significantly fewer parameters and requires only 1% of the floating-point operations compared to BERT BASE. Evaluation includes a baseline without knowledge distillation (NoKD) and comparison with Patient Knowledge Distillation (PKD) from Sun et al. (2019). The study focuses on distilling the teacher BERT language model into task-agnostic student models with different dimensions. Evaluation includes using the Reddit dataset for word mask prediction accuracy and fine-tuning on tasks from the GLUE benchmark. The study evaluates distilling the teacher BERT language model into task-agnostic student models with different dimensions. Evaluation includes using the Reddit dataset for word mask prediction accuracy and fine-tuning on tasks from the GLUE benchmark, such as Sentiment Treebank (SST-2), Microsoft Research Paraphrase Corpus (MRPC), and Multi-Genre Natural Language Inference (MNLI). The models are fine-tuned for 10 epochs using LAMB with a learning rate of 0.0002 and batch size of 32. The study evaluates distilling the teacher BERT language model into task-agnostic student models with different dimensions, showing improved performance on reading comprehension datasets like SQuAD and RACE. Dual training significantly enhances word prediction accuracy, with SharedProjUp outperforming SharedProjDown. Notably, there is a performance drop with decreasing model dimensions. Results of distilled models, teacher model, and baselines on downstream language understanding tasks are shown in Table 3. The models trained with proposed approaches consistently outperform NoKD baselines, indicating the effectiveness of dual training and shared projection techniques. Our 192-dimensional models, trained with dual training and shared projection techniques, outperform PKD baselines and are competitive with larger models on task accuracy while being significantly smaller. The drop in performance from 192-dimensional to 96-dimensional models is minimal, with the 96-dimensional model achieving higher accuracy than the PKD 6-layer baseline. Even highly-compressed 48-dimensional models perform well, comparable to the 3-layer PKD model. The 192-dimensional models, trained with dual training and shared projection techniques, outperform PKD baselines and are competitive with larger models on task accuracy. The effect of shared variable projection is less pronounced for larger models and more data, potentially reducing the degrees of freedom available to the model. In this work, the up-projection method outperforms down-projection in language modeling and downstream tasks. Issues with input vocabulary size in NLP can lead to longer tokenized sequences, making model training harder. Reducing input vocabulary sizes has been explored in NLP, but not specifically for model compression. In this work, the focus is on classification tasks on shorter texts, which are less affected by input sequence lengths compared to tasks like machine translation. Real-world applications often involve short text inputs, suggesting a need for a better balance between vocabulary size and sequence lengths. The goal is to explore BERT's language modeling capacity and the impact of its large WordPiece vocabulary, rather than fine-tuning teacher models for distillation. The study focuses on improving knowledge distillation for BERT by using a smaller vocabulary and dimensions for student models. A dual training mechanism aligns teacher and student embeddings, while shared variable projection facilitates knowledge transfer. Highly-compressed student BERT models show improved accuracy and compression efficiency in language tasks. Future work may involve combining these methods with existing approaches. Future work may involve combining the approach with existing methods to reduce the number of layers in student models and explore low-rank matrix factorization for transferring model parameters. Considering the frequency distribution of WordPiece tokens during embedding training could further optimize model size."
}