{
    "title": "HkxStk34Kr",
    "content": "In this work, flexible joint distributions are constructed from low-dimensional conditional semi-implicit distributions to improve variational inference accuracy. Recent advances have focused on estimating or bounding KL divergence for complex distributions, enabling inference with hierarchical, semi-implicit, and fully implicit distributions. While existing methods work well for low dimensions, they struggle with higher dimensions. This work specifically focuses on semi-implicit variational inference for structured multi-dimensional distributions. Structured semi-implicit variational inference tightens entropy bounds and evidence lower bounds for multi-dimensional distributions. It captures the multi-modal nature of posterior distributions in deep Gaussian processes and constructs autoregressive semi-implicit models. This method approximates the intractable posterior distribution in probabilistic models by using a reparameterizable and tractable variational distribution. Semi-implicit variational inference (SIVI) extends the framework to mix explicit and implicit distributions, creating a semi-implicit distribution with intractable marginal density. SIVI uses K+1-sample estimates to obtain a lower bound on entropy, allowing for a proper variational objective by bounding the KL-term in the evidence lower bound. Unlike other methods, SIVI does not require access to the density q(). Semi-implicit variational inference (SIVI) approximates a multi-dimensional distribution with a mixture of Gaussian distributions to bound entropy. To address performance degradation with increasing dimensions, a high-dimensional joint distribution is factorized into low-dimensional conditional distributions. This approach reduces the need for modeling high-dimensional distributions while still capturing the joint distribution effectively. In Section 4, two models with a structured joint distribution are discussed. Theorem 1 proves that considering the structure leads to a tighter bound. The structured SIVI bound approximates the marginal distribution with an exponentially large mixture of distributions. SSIVI is applied to deep Gaussian processes, and it is noted that the true posterior does not factorize across layers as assumed. The structured semi-implicit posterior approximation used in SSIVI-DGP eliminates limiting assumptions by employing a fully-factorized Gaussian conditional distribution. This approach allows for the recovery of multimodal posteriors with cross-layer dependencies, as demonstrated in a toy regression problem with two natural modes, Mode A and Mode B. In Figure 1, SSIVI with K=100 and 3000 Adam updates is used. Gaussian conditional distributions are modeled by neural networks with three hidden layers. DSVI converges to one mode, while SGHMC and SSIVI capture all modes and inter-layer dependencies. The structured semi-implicit distribution is implemented using a recurrent neural network. The generative process involves a recurrent neural network with stacked GRU cells to define h(\u00b7, \u00b7), and fully-connected neural networks for \u00b5(\u00b7, \u00b7) and \u03c3 2 (\u00b7, \u00b7) to output mean and log-scale of a Gaussian distribution. All mixing variables are scalar and follow a standard Gaussian distribution. The model is trained to generate samples from a synthetic multi-dimensional structured distribution using SSIVI with 10000 steps of Adam optimization. SSIVI provides a tighter bound compared to SIVI as the number of dimensions increases. The proof for this can be shown similarly to the proof for SIVI by Molchanov et al. The second inequality is proven by rewriting the SIVI bound and expanding the product of sums into a mixture of distributions. This leads to a rewritten SSIVI objective. The SSIVI bound is invariant to permutation of k, and the gap between SIVI and SSIVI bounds is derived. The DGP model is defined with a Gaussian process prior and likelihood training using maximum marginal likelihood. Sparse GP models introduce auxiliary variables for complexity reduction in training. In sparse Gaussian processes, the marginal likelihood is maximized through a lower bound called ELBO. This ELBO simplifies the optimization process by canceling out computationally heavy terms. Deep Gaussian processes are constructed as a chain of multi-output sparse Gaussian processes, with each layer having an output variable and inducing inputs. The joint distribution over these variables is defined similarly to conventional sparse GPs. The training of Deep Gaussian Processes involves maximizing a variational lower bound on the data log marginal likelihood. Unlike sparse GPs, SSIVI for Sparse DGP uses a structured semiimplicit distribution and requires plain MC estimation for the expected log-likelihood. This is due to the explicit conditioning of the variational model on inducing values from the previous layer. The final objective for training SSIVI-DGPs involves modifying the lower bound to account for dependencies of inducing values between layers."
}