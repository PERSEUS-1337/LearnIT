{
    "title": "SJlKDVS2hV",
    "content": "Recurrent neural networks (RNNs) are powerful for modeling sequential data and document-level sentiment classification. Despite their theoretical complexity, trained networks converge to interpretable, low-dimensional representations using a simple mechanism present across RNN architectures. This demonstrates the universal and human-interpretable computations that can arise in recurrent networks. Recurrent neural networks (RNNs) are capable of learning complex relationships in input sequences and applying this structure in a nonlinear manner. Despite being viewed as black boxes, RNNs can converge to low-dimensional representations when trained for document-level sentiment analysis. Analysis techniques reveal that locally linear approximations to RNN dynamics are highly interpretable, involving low-dimensional line attractor dynamics that can store analog values. The mechanism of linear dynamics in RNNs is consistent across various architectures. Four RNN architectures were trained on sentiment classification tasks using different datasets. The best performing networks were analyzed by linearizing the dynamics around approximate fixed points. The study analyzed the hidden states of RNNs trained on sentiment classification tasks using different datasets. The optimization process involved minimizing q with respect to hidden states, sampled randomly from the state activation visited by the trained network. Principal components analysis (PCA) was performed on the RNN states, showing that the top 2-3 PCs explained \u223c90% of the variance in hidden state activity. Untrained networks exhibited higher dimensional distributions of hidden states on the same test set. The study analyzed RNN dynamics in a low-dimensional space for sentiment classification. The RNNs approximated line attractor dynamics, with fixed points forming a 1D manifold aligned with readout weights and all fixed points being attracting and marginally stable. The study analyzed RNN dynamics for sentiment classification, showing that fixed points are attracting and marginally stable. Inputs produce linearly separable effects on the RNN state vector around fixed points. \u223c500 fixed points were identified and projected into a low-dimensional space. The RNN hidden states were fit to a high percentage of variance in fixed points, suggesting the RNN states remain close to the manifold of fixed points. The main axis of variation of the 1D manifold was labeled as m. Fixed points were spread along a 1D curve in PC space, aligned with the readout weights. A low-dimensional approximation using LLE accurately parameterized the 1D manifold of fixed points. The identified fixed points in the RNN were shown to be marginally stable, preserving accumulated information from inputs. A linearization procedure was used to approximate the RNN dynamics near the fixed points, resulting in a discrete-time linear dynamical system. The Jacobian matrices depend on the chosen fixed point for linearization. The Jacobians of the RNN depend on the chosen fixed point for linearization, and can be considered as functions of h*. By reducing the nonlinear RNN to a linear system, the network's response to a sequence of inputs can be estimated analytically. The effect of each input is decoupled from others, allowing us to focus on the effect of a single input x t. The total effect of x t on the final RNN state is determined by the number of time steps between x t and the end of the sequence. The eigenvalues and eigenvectors of the RNN are crucial for understanding its dynamics. The eigenvalues are sorted based on magnitude, with the top modes affecting the network representation. The decay time constant of each mode provides insight into the system's effective memory. FIG1 illustrates the distribution of eigenvalues and time constants at different fixed points. The eigenvalues and eigenvectors of the RNN are important for understanding its dynamics. The decay time constant of the top modes reveals the system's memory capacity. The eigenvalue spectra show that RNN states mainly explore a low-dimensional subspace during sentiment classification, with fast decay for most dimensions. The top eigenmode simplifies the effect of input on network activity. The effect of specific words on the RNN state vector is analyzed by projecting vectors onto a low-dimensional subspace. Positive and negative words push the state in opposite directions, while neutral words have a smaller effect. The input projection captures the magnitude of change induced by words along the eigenmode associated with the longest memory. The input projection onto the top eigenmode reflects the impact of words on the RNN state vector. Positive and negative words show strong correlations with the magnitude of change, while neutral words have minimal effect. The right eigenvector represents the integration direction of x, aligning with the manifold of fixed points in an approximate line attractor. In this work, dynamical systems analysis was applied to understand how RNNs solve sentiment analysis. A simple mechanism of integration along a line attractor was found in multiple architectures trained for the task. The study provides evidence that different network models can converge to similar solutions that may be understood by human practitioners. In sentiment classification, word sequences are processed using a vocabulary and one-hot vectors. Different RNN architectures were considered for prediction, with LSTM having concatenated hidden and cell state vectors. The RNN prediction is evaluated at the final time step using readout weights. The LSTM architecture in sentiment classification uses readout weights to predict at the final time step. Three benchmark datasets were examined: IMDB movie reviews, Yelp user reviews, and Stanford Sentiment Treebank sentences. Models were trained using Adam with a batch size of 64 and a hyper-parameter search over learning rate ranges. The study involved training LSTM models on three datasets using Adam optimization with hyper-parameter search. The accuracies of the final models were compared, and the best performing models were analyzed for each architecture type and dataset. The optimization process involved minimizing a loss function with respect to hidden states using auto-differentiation methods. The study trained LSTM models on three datasets using Adam optimization with hyper-parameter search. The optimization process involved finding approximate fixed points of varying speeds by varying the stopping tolerance for q. Slow points were identified as reasonable places for linearization. Figures were provided summarizing the linear integration mechanism for different architectures and datasets. The study trained LSTM models on three datasets using different architectures (LSTMs, GRUs, Update Gate RNNs, Vanilla RNNs) and found consistent mechanisms across them. The figures provided show the instantaneous effect of word inputs, average effects over different words, PCA projections, and distribution of overlap. Blue arrows represent r1, the top right eigenvector, and the distribution of r1m over all fixed points is shown. The null distribution consists of randomly generated unit vectors of the hidden state size."
}