{
    "title": "rklVOnNtwH",
    "content": "In this paper, a method is proposed to detect out-of-distribution (OOD) samples in classification by extracting uncertainties of features in each layer of deep neural networks (DNNs). This approach outperforms existing methods, achieving state-of-the-art detection performance on various datasets and models. Our method improves detection performance on classification models, increasing AUROC score to 99.8% in DenseNet on CIFAR-100 and Tiny-ImageNet datasets. Deep neural networks excel in tasks like image classification, object detection, and speech recognition. Detecting out-of-distribution samples is crucial for safety in real-world applications. Previous studies have attempted to detect out-of-distribution (OOD) samples using deep neural networks (DNNs), but they may incorrectly classify in-distribution samples as OOD due to low confidence in difficult datasets or models. This leads to poor detection performance. Previous studies have tried to detect out-of-distribution (OOD) samples using deep neural networks (DNNs), but they may misclassify in-distribution samples as OOD due to low confidence in challenging datasets or models, leading to poor detection performance. One reason for this issue is that existing approaches only utilize features near the output layer, which are closely tied to classification accuracy. To address this, we propose incorporating features from both the input and output layers, hypothesizing that uncertainties in input layer features can aid in detecting OOD samples. For instance, in convolutional neural networks (CNNs), filters near the input layer extract features like edges crucial for in-distribution classification, resulting in larger uncertainties for in-distribution inputs compared to OOD inputs. The study addresses the problem of misclassifying in-distribution samples as out-of-distribution (OOD) due to low confidence in challenging datasets or models. Existing approaches overlook uncertainties in features close to the output layer, leading to poor detection performance. The proposed method incorporates uncertainties of latent features close to the output layer, visualized using scatter plots of a CNN on the SVHN dataset. Classes 0, 1, and 2 were used for training, while classes 0, 1, 2, and OOD were used for validation. The contour of the maximum output of the softmax layer was plotted to demonstrate the differences in uncertainty between in-distribution and OOD samples. The softmax output decreases when the model is uncertain about the image's class. Points in scatter plots show combined uncertainties of features, helping classify images. Uncertainties of features close to the output layer are effective for detecting out-of-distribution (OOD) samples. A method called UFEL extracts uncertainties in each layer to detect OOD samples after training a discriminative model. Our proposal, agnostic to model architecture, estimates uncertainties using a reparameterization trick. It visualizes output probability and combined uncertainties in the feature space to discriminate in-distribution and OOD images. The uncertainties help classify images, especially those difficult to classify, by detecting OOD samples. In experiments, UFEL is shown to effectively detect OOD examples with smaller uncertainties compared to in-distribution samples. It outperforms existing methods on datasets like CIFAR-100 and models like LeNet5, remaining robust to hyperparameters. The method builds on the classification confidence approach proposed by Hendrycks & Gimpel (2016) for OOD sample detection without model re-training. Many models have been proposed to detect OOD samples using pre-trained deep classifiers. ODIN, a calibration technique by Liang et al. (2017), uses temperature scaling and adds controlled perturbations to inputs. Other methods like Lee et al. (2018), DeVries & Taylor (2018), and Hendrycks et al. (2018) have also extended the baseline method. Malinin & Gales (2018) explored uncertainty-based methods for classifying in-distribution samples. Malinin & Gales (2018) distinguished between data uncertainty and distributional uncertainty to classify in-distribution samples close to the decision boundary as OOD samples. They estimated the parameter of the Dirichlet distribution using a DNN and trained the model with in-distribution and OOD datasets. Our work models the distribution of the logit of the categorical parameters as a Gaussian distribution, enabling training without an OOD dataset. UFEL is introduced to extract uncertainties of features in each layer for detecting OOD samples without the need for an OOD dataset. It combines various features such as softmax output, categorical parameter distribution, and feature extraction uncertainties from the latent space close to the input layer. These uncertainties are probabilistically modeled, estimated, and combined to improve detection accuracy. The objective function of normal deep classification involves a cross entropy loss function and a DNN. To extract uncertainties of features in each layer, the output is modeled as a Gaussian with parameters dependent on the previous layer's output. The reparameterization trick is used to train the model like regular classification models. The model is trained like regular classification models by combining features extracted in each layer using two methods: summing uncertainties and linearly combining them, or directly and nonlinearly combining them using a CNN. The parameters are chosen through logistic regression and trained with in-distribution and OOD validation samples. The CNN model is trained using features extracted from each layer and tested for detecting OOD samples. Standard datasets like SVHN, CIFAR-10, and CIFAR-100 are used for in-distribution samples, while OOD datasets like Tiny ImageNet, LSUN, iSUN, Gaussian noise, and uniform noise are also employed. Standard augmentation techniques are applied, and 5,000 validation images are used for parameter selection. The CNN model was trained using features from each layer to detect OOD samples. Hyperparameters were tuned using training images from SVHN, CIFAR-10, and CIFAR-100 datasets. Parameters of the CNN were tuned using a separate validation set, and models were tested on in-distribution and OOD datasets. Evaluation metrics included TNR at 95% TPR, AUROC, AUPR, and accuracy. In the experiment, UFEL was compared with baseline methods like Hendrycks & Gimpel (2016) and ODIN (Liang et al., 2017). Different detection metrics and model structures were used, including LeNet5, WideResNet, and DenseNet. Parameters like temperature scaling and perturbations were calibrated for ODIN. The structure for extracting variance parameters differs between LeNet5, DenseNet, and WideResNet. LeNet5 was trained with increased channels for accuracy, using the Adam optimizer for 10 epochs. DenseNet and WideResNet were trained with stochastic gradient descent and Nesterov momentum. Reparameterization tricks were inserted into specific layers for each model. WideResNet used WRN-40-4 and DenseNet used Dense-BC with specific parameters and training epochs. In this section, UFEL was trained with DenseNet-BC for 200 epochs on CIFAR-10 and CIFAR-100 datasets, and 10 epochs on the SVHN dataset. The learning rate was reduced after the 150th epoch. Five experiments were conducted to show UFEL's superior performance compared to baseline methods, its independence from ACC, strong OOD data detection ability, robustness to the number of OOD samples, and overall performance testing. UFEL demonstrates robustness to varying numbers of OOD samples. The fifth experiment evaluates UFEL's performance on unseen OOD datasets, showcasing how uncertainties in CNN layers distinguish in-distribution from OOD data. By combining these features, UFEL achieves state-of-the-art OOD sample detection. Using DenseNet trained on CIFAR-100 with iSUN dataset for OOD images, var 1 and var 3 emerge as strong features that outperform ODIN, highlighting the effectiveness of uncertainty in feature extraction and classification for OOD sample detection. UFEL outperforms baseline and ODIN methods in detecting OOD samples, showing state-of-the-art performance. The model is robust to noise and demonstrates superiority in in-distribution accuracy. CIFAR-100 and LeNet5 datasets particularly showcase UFEL's significant margin of improvement. In this experiment, DenseNet-BC was trained on CIFAR-10 as the in-distribution dataset and TIM as the OOD dataset. The features of the method showed no correlation with in-distribution accuracy. The var 1 feature close to the input layer had the highest ability to detect OOD samples. AUROC plots were generated using SVHN as the in-distribution dataset and TIM, LSUN, and iSUN as OOD datasets with the LeNet5 model, showing discriminative capabilities. In this experiment, UFEL was tested for its ability to discriminate in-distribution and OOD examples using various datasets and changing the number of in-distribution classes. Results show UFEL's robustness to the number of class labels, outperforming baseline and ODIN methods in all cases. This suggests UFEL's effectiveness for small datasets. In this experiment, UFEL's performance was evaluated while changing the number of OOD validation examples. Hyperparameters of ODIN and UFEL were tuned on a separate validation set with varying numbers of OOD images. UFEL (CNN) outperformed other methods in most cases but performed worse than ODIN in some instances due to the need for tuning OOD samples. UFEL (LR) consistently outperformed prior methods with minimal hyperparameters and tuning samples. Figure 5 shows the AUROC plot for changing the OOD dataset, using CIFAR-10 and CIFAR-100 as in-distribution datasets. UFEL outperforms prior work in generalization to unseen OOD datasets, as shown in Table 3. The binary classification method does not generalize well in distinguishing in-distribution datasets and OOD datasets like TIM, LSUN, and iSUN. In this paper, the uncertainties of features extracted in hidden layers are crucial for detecting out-of-distribution (OOD) samples. By combining these uncertainties, the proposed approach achieves state-of-the-art OOD detection performance on various models and datasets. This method has the potential to enhance the safety of classification systems by improving OOD sample detection. Future work could involve using the model in an unsupervised manner to minimize reconstruction error without relying on in-distribution labels. Additionally, combining UFEL with ODIN could further enhance performance, as they are complementary methods. The CIFAR dataset contains 32 \u00d7 32 natural color images with 50,000 training images and 10,000 test images. CIFAR-10 has 10 classes, while CIFAR-100 has 100 classes. The SVHN dataset includes 32 \u00d7 32 color images of house numbers with 604,388 training images and 26,032 test images. SVHN has 10 classes with digits 0-9. The Tiny ImageNet dataset has 10,000 test images from 200 classes. LSUN dataset has 10,000 test images of 10 scenes. The iSUN dataset includes 8,925 SUN images. The images were downsampled to 32 \u00d7 32 pixels and two noise datasets were created: Gaussian noise with values sampled from a Gaussian distribution and uniform noise with values sampled from a uniform distribution. The neural network architecture included Conv2d layers, ReLU activation functions, MaxPool2d layers, and Linear layers with softmax activation for classification."
}