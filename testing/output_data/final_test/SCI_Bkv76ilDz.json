{
    "title": "Bkv76ilDz",
    "content": "Generating complex discrete distributions remains a challenge in machine learning. Discrete Wasserstein GAN (DWGAN) introduces a new model based on the Wasserstein distance between discrete distributions. Experimental results are provided for synthetic and real data from MNIST handwritten digits. The GAN models aim to generate data based on training samples through a two-player game between a generator and discriminator network. Wasserstein GAN improves training by using Wasserstein distance. Generative models are important in natural language processing for learning text distributions. The Discrete Wasserstein GAN (DWGAN) is proposed to address issues with using continuous sample distance for discrete distributions in GAN models. It is based on a dual formulation of the Wasserstein distance and introduces a novel training algorithm and network architecture to enforce dual constraints in optimization. The Generative Adversarial Networks (GANs) model a sample generating distribution through a two-player game between a generator and a discriminator. The GAN approximates the Jensen-Shannon divergence (JSD) between generated and real data distributions. The Wasserstein GAN was proposed as a solution to the limitations of divergence metrics like JSD in GAN optimization. It approximates the dual problem of the Wasserstein distance, providing more robust gradients for training. This involves optimizing over 1-Lipschitz functions, with network weights clipped to ensure k-Lipschitz properties. A variant enforces this property through a gradient penalty in optimization. The Wasserstein GAN approximates the Wasserstein distance between distributions by adding a gradient penalty. It models discrete probability distributions by encoding input vectors in a one-hot representation and using a softmax nonlinearity in the generator's output. The critic network takes the softmax output as input without rounding, and argmax is used to generate valid discrete samples. The discrepancies in the standard Wasserstein GAN for discrete problems arise from differences in continuous and discrete distances, especially with a large number of classes. This difference becomes more pronounced as the number of discrete classes increases. The proposed new GAN architecture is based on the Wasserstein distance between two discrete distributions. The Wasserstein distance is defined as the sample distance between two probability distributions, aiming to correct modeling discrepancies in discrete problems. The Wasserstein distance is a sample distance metric defined as the hamming distance between two discrete probability distributions. The size of the Linear Program problem for generating real-world discrete distributions grows exponentially, making it intractable to solve. Kantorovich duality provides a dual formulation for the Wasserstein distance. The dual formulation of the Wasserstein distance involves maximizing a function f subject to inequality constraints. To approximate this, parameterized functions f w are used, modeled with a neural network. However, constructing a neural network that satisfies the inequality constraints explicitly is challenging. The text discusses designing a neural network architecture to approximate h(x, x ) and satisfy inequality constraints in the context of the dual formulation of the Wasserstein distance. The generator network uses a softmax nonlinearity trick and maps random noise z to a sample in one-hot. The critic network in our model takes inputs from real samples and the generator's output, using a parameterized function to produce an output vector. The architecture of the critic network is illustrated in FIG2. The critic network architecture, shown in FIG2, consists of two sub-networks. One sub-network processes real training samples, while the other processes samples from the generator. Each sub-network has its own intermediate layers, and their outputs are concatenated for further processing. The critic network architecture includes two sub-networks processing real training samples and generator samples. The outputs are concatenated and passed through a fully connected layer to produce a tensor of size n \u00d7 m. A tanh nonlinearity is applied to produce a tensor v with values ranging from -1 to 1, which is then multiplied element-wise by the \"filter\" tensor u. The output is the sum of the element-wise multiplication, yielding a vector of n elements representing h w (y, y ) for each pair of real and generated samples. Additional modifications are made to facilitate network training, including randomly swapping samples from the real training data to enforce the optimum condition h(x, x ) = f (x) \u2212 f (x ). The training algorithm for the proposed architecture involves randomly swapping samples from real data and generator output, applying a scaling factor to the softmax function, and using a specific algorithm for the Discrete Wasserstein GAN. In contrast with continuous GANs, only a few GAN formulations have been proposed for modeling discrete probability distributions. Various techniques like adjusting the Wasserstein GAN, using maximum likelihood, and introducing Boundaryseeking GAN (BGAN) have been employed to address natural language generation tasks and train the generator to produce samples within the discriminator's decision boundary. Other GAN models exploit parametric conditional distributions for discrete cases. The generator outputs a parametric conditional distribution in GAN models. Some models combine adversarial training with Variational Autoencoders to model discrete probability distributions. Evaluating generative models objectively is challenging, with previous research suggesting user studies or proxy measures like perplexity and BLEU. To address these limitations, a synthetic experiment is proposed to objectively evaluate GAN models' performance in modeling discrete distributions. The synthetic experiment aims to objectively evaluate GAN models' performance in modeling discrete distributions by generalizing the classic tic-tac-toe game to include arbitrary players and board sizes. The goal is to model the true generating distribution over valid board configurations, simplifying the rules to include at least one full column, row, and diagonal taken by a player. Examples of valid and non-valid board configurations are shown in FIG3. In the synthetic experiment, GAN models are evaluated in modeling discrete distributions by extending the classic tic-tac-toe game. Valid and non-valid board configurations are illustrated in FIG3. The performance of generative models can be objectively validated by checking if a sample is valid under the real distribution. Metrics are constructed to track the model's performance, including the percentage of valid samples and the average of maximum player's gain for a board configuration. The maximum player's gain is defined as the maximum number of cells taken by a player in a full column, row, or diagonal on a board configuration. Comparing the Discrete Wasserstein GAN model with the standard Wasserstein GAN model on different board sizes, the quality of samples is evaluated based on metrics like the average of maximum player's gain, percentage of unique samples, and percentage of new samples. The Wasserstein GAN model with DWGAN networks outperforms the standard WGAN model on 3-by-3 and 5-by-5 boards with 2 players and 8 players. The DWGAN networks achieve good performance faster, taking less iterations to reach similar performance levels. However, mode collapse issues are observed after achieving top performances. After achieving top performances, the DWGAN network can produce diverse samples with high player gains on a 5-by-5 board in 500 iterations. However, mode collapse occurs after iteration 550, reducing the percentage of unique samples. To address this, a norm penalty is added to the critic network optimization, resulting in 96% valid samples and maintained sample diversity. The Discrete Wasserstein GAN (DWGAN) achieves 96% valid samples and maintains sample diversity by using a norm penalty. MNIST digits discretized to binary values are used for training, generating new digits. The model produces quality digit images similar to a standard Wasserstein GAN. 100 diverse samples are generated before mode collapse occurs. The Discrete Wasserstein GAN (DWGAN) approximates the Wasserstein distance between two discrete distributions with a novel training algorithm and network architecture. Future work includes improving training stability and applying the model to other datasets. Linear programming involves convex optimization with vector variables and matrices. The Lagrange dual function maximizes g(\u03bb, \u03bd) subject to \u03bb 0, and can be written as an LP in inequality form with vector variable \u03bd \u2208 R m. The dual of an LP in inequality form with vector variable \u03bd \u2208 R m is equivalent to the original LP in standard form. Strong duality holds for LPs in standard or inequality form if the primal problem is feasible. The discrete Wasserstein distance between two probability distributions over a finite set X can be defined as a linear program. The dual LP is given with \u03bd = -\u00b5 at the optimum. The dual LP is equivalent to the original LP in standard form with \u03bd = -\u00b5 at the optimum. Example 1 illustrates the dual optimization problem with discrete distributions and sample distance matrices. The optimal transport of mass from P s to P r is determined by the matrix T, with an objective value of 0.3 representing the total mass moved. In the dual LP solution, the optimal transport of mass from P s to P r is determined by matrix T, with an objective value of 0.3. Using Julia v0.5.2 with Knet deep learning framework, functions are used to generate a tic-tac-toe board for synthetic experiments. The code includes training the generator and updating weights based on gradients. The code involves training the generator for synthetic experiments using Julia v0.5.2 with Knet deep learning framework. It includes updating weights based on gradients and logging values for output and gradients."
}