{
    "title": "SkeGURNtDH",
    "content": "MultiGrain is a neural network architecture that generates compact image embedding vectors for multiple tasks. It is trained for classification and instance/copy recognition using self-supervised ranking loss. The embeddings are more compact and achieve high accuracy, outperforming the current state-of-the-art method on ImageNet. Our approach benefits from self-supervision, pooling method, and mini-batches with repeated augmentations of the same image. Image recognition in computer vision involves various approaches optimized for different aspects of the problem, such as recognizing classes, instances, and copies of objects. Specialized image representations are typically used for each case, like in image retrieval to match query images with a database. The goal is to search collections with multiple granularities, matching by class, instance, or copy. MultiGrain is a compact embedding that can solve recognition tasks of different granularities while maintaining or surpassing the accuracy of specialized embeddings. It is obtained by training a Convolutional Neural Network (CNN) jointly on various tasks, with intermediate layers better suited for low-level tasks like instance and copy recognition. Our work focuses on extracting a single global embedding optimized for classification and instance retrieval tasks, integrating different levels of invariance. The MultiGrain architecture allows for learning with only class-level labels through self-supervised learning. MultiGrain architecture enables learning with class-level labels through self-supervised learning, improving classifier performance by forming batches with multiple augmentations of the same image. It incorporates a pooling layer inspired by image retrieval, boosting classification accuracy for high-resolution images and outperforming SoTA on ImageNet for ResNet-50. Most CNNs leverage a trunk designed for classification like Residual networks. Architectural improvements and training on weakly annotated data can improve accuracy. The state of the art on ILSVRC 2012 is held by AmoebaNet-B, but ResNet-50 is commonly used in literature for image classification and instance retrieval. Image search aims to find images similar to a query in a large collection, evaluated for landmark recognition, object recognition, and copy detection. Image retrieval involves instance-level retrieval using embedding vectors to find nearest neighbors in the embedding space. Refinement steps like geometric verification and query expansion are used, but this paper focuses on local image descriptors aggregation. The paper focuses on local image descriptors aggregation for instance-level retrieval. CNNs trained on classification datasets are competitive feature extractors for image retrieval. Architectures for instance search involve modified classification trunks with pooling for spatial locality. The R-MAC image descriptor aggregates regionally pooled features for efficient comparisons between image regions. In contrast to previous works that assume a domain-specific training set, Radenovi\u0107 et al. (2018) introduce a generalized mean pooling method for image retrieval. Multi-task training with CNNs has shown success in various vision tasks, but the design and training process still involve many heuristics. Ongoing research aims to develop more efficient multi-task networks. The ongoing research focuses on efficient parameter sharing, hyper-parameter settings, and data augmentation to improve generalization in multi-task networks. A new batch augmented sampling strategy enhances generalization performance by filling batches with data-augmented copies of the same image, leading to better resource utilization. This approach can boost the effect of data augmentation with a lower number of distinct images per batch. The MultiGrain architecture combines classification and retrieval tasks by optimizing with a classification loss and a retrieval loss. The classification loss uses standard cross-entropy, while the retrieval loss is based on the triplet loss. The MultiGrain architecture combines classification and retrieval tasks. The retrieval loss includes triplet loss and contrastive loss, with optimization depending on hyper-parameters. Wu et al. (2017) proposed a method that normalizes embeddings, samples negative pairs, and uses a margin loss combining contrastive and triplet loss. The MultiGrain architecture combines classification and retrieval tasks, with a joint loss function that includes distance-weighted sampling for image pairs and a combination of losses weighted by a factor \u03bb. The normalization of embeddings allows for the use of Euclidean distance as a cosine similarity, facilitating retrieval tasks without the need for elaborate parameter tuning. For recognition tasks, encoding the entire image into a single vector is crucial. Global spatial pooling operators are used to achieve this, contrasting with local pooling operators like max pooling in CNNs. Recent classification architectures like ResNet and DenseNet utilize average pooling, which is less sensitive to geometric transformations and allows flexibility for images of any size. Image retrieval, on the other hand, requires more localized and fine-grained geometric information. The generalized mean pooling (GeM) layer computes the generalized mean of each channel in a feature tensor computed by a convolutional neural network. It preserves local information for images with cluttered backgrounds and distinctive parts that require identification. The GeM layer computes the generalized mean of each channel in a feature tensor using a parameter p > 0. It is competitive on its own and has been studied in the context of scene recognition/image classification. MultiGrain uses GeM to bridge different worlds and adapt the network to varying image resolution. Repeated augmentations are introduced for training with SGD and data augmentation, forming an image batch by sampling different images and transforming them multiple times. The GeM layer computes the generalized mean of each channel in a feature tensor using a parameter p > 0. Repeated augmentations are introduced for training with SGD and data augmentation, forming an image batch by sampling different images and transforming them multiple times. Instance level ground-truth y ij = +1 iff images i and j are two augmented versions of the same image. RA has lower performance than standard i.i.d. scheme for small batch sizes but outperforms with larger batches due to correlated samples facilitating learning features invariant to augmentations. The Euclidean distance between transformed features is equivalent to the Mahalanobis distance between input descriptors. PCA is trained at the end of CNN training using an external dataset of unlabelled images. Whitening operation \u03a6 can be written as \u03a6(e) = S(e/e - \u00b5). Parameters of the classification layer need to be modified to take whitened embeddings as input. Inducing decorrelation via a loss is insufficient for ensuring feature generalization in image classification. In image classification, resizing and center-cropping input images to a low resolution like 224 \u00d7 224 pixels is common for memory efficiency and faster inference. However, image retrieval requires finer details and typically uses larger input sizes of 800 or 1024 pixels without cropping. MultiGrain is trained at 224 \u00d7 224 resolution but can be evaluated at larger resolutions during testing. During testing, MultiGrain can be evaluated at larger resolutions using a larger pooling exponent to optimize retrieval accuracy. The optimal exponent is determined through a synthetic retrieval task on a subset of ImageNet images, balancing retrieval and classification performance. MultiGrain is built using ResNet-50 as the base architecture, optimized with SGD and a learning rate schedule. Data augmentation includes a standard set of augmentations. The baseline CNN achieves 76.2% top-1 validation error with cross-entropy training and uniform batch sampling. The network achieves high accuracies without regularization terms, data augmentations, or external data. Two settings for the GeM layer are considered: p = 1 or p = 3, with p = 3 improving retrieval performance. The network is trained on 224x224 pixel crops and tested on resolutions of 224, 500, and 800 pixels. The network achieves high accuracies without regularization terms, data augmentations, or external data. Two settings for the GeM layer are considered: p = 1 or p = 3, with p = 3 improving retrieval performance. The network is trained on 224x224 pixel crops and tested on resolutions of 224, 500, and 800 pixels. For resolution s * > 224, the largest side of the image is resized to s * and evaluated without cropping. Margin loss and batch sampling are utilized with m = 3 RA repetitions per batch. Datasets include ImageNet-2012 for training and validation images for classification accuracies. Mean average precision is reported on the Holidays dataset for image retrieval, and accuracy is reported on the UKB object recognition benchmark. The network achieves high accuracies without regularization terms, data augmentations, or external data. The performance is evaluated on the Inria Copydays dataset with 10k distractor images from YFCC100M. PCA whitening transformations are computed from 20k images from YFCC100M. Larger images are used at test time with resolutions greater than 224, varying the exponent values. Figures show classification and retrieval accuracy at different resolutions for different exponent values. At resolution s = 224, the best pooling exponent for classification is p = 3. Larger scales require p > 3. Cross-validation on IN-aug determines values for \u03bb, set at 0.5 for experiments. MultiGrain nets are trained at s = 224 with p = 1 or p = 3. Evaluation at resolutions s = 224, 500, 800 selects the same exponent p for classification. The MultiGrain model shows significant improvement in classification performance compared to the baseline ResNet-50 model. The improvement is attributed to RA batch sampling, retrieval loss, p = 3 pooling, and expanding resolution to 500. These factors contribute to the model achieving 78.6% top-1 accuracy. The MultiGrain model integrates AutoAugment for improved accuracy in CNNs, achieving 78.2% top-1 accuracy at resolution s=224. This outperforms AA alone and mixup. Increasing the test resolution further improves performance. The GeM pooling significantly increases the accuracy of off-the-shelf networks. Different pooling methods were compared, including Fisher vectors, neural codes, RMAC, and GeM. GeM was fine-tuned at a higher resolution with multi-scale input processing for improved results. Top-1 accuracy of 83.6% was achieved with PNASNet-5-Large, showing a 0.9% improvement over the original model. MultiGrain nets improve accuracies on all datasets compared to ResNet-50 baseline for similar resolutions, with repeated augmentations being a key factor. Results are compared with previous studies without additional training data, showing favorable performance at resolution 500 for retrieval tasks. MultiGrain nets significantly reduce processing time compared to higher resolutions, making them ideal for large-scale or low-resource vision applications. Results at resolution 500 show slightly lower performance with margin loss, attributed to limited transfer from the IN-aug task. MultiGrain embeddings excel in image classification and instance retrieval, utilizing a classical CNN trunk with a GeM pooling layer. The pooling layer allows for increased image resolution at inference time while maintaining a small resolution during training. MultiGrain embeddings demonstrate strong performance in both classification and retrieval tasks, setting a new state of the art. MultiGrain sets a new state of the art in classification and retrieval tasks. The approach will be open-sourced, with additional experiments detailed in appendices. These include augmentation sampling, GeM pooling effects, loss weighting, data-augmented batches, hyper-parameters, and ablation results. MultiGrain ingredients improve accuracy of pre-trained ConvNet with minimal additional training cost, achieving best reported classification results on imagenet-2012. The algorithm used in MultiGrain involves sampling batches with repeated augmentations and computing the loss on positive and negative pairs. The effect of changing the GeM pooling exponent on activation maps is visualized, focusing on a single class (racing car) and its corresponding activation map. By setting p = 3, the car is detected with high confidence and without spurious detections. Boureau et al. (2010) analyze the impact of the tradeoff parameter \u03bb between the classification and margin loss terms. \u03bb = 0.5 means that the classification has slightly more importance. The impact of the tradeoff parameter \u03bb on classification performance is analyzed. \u03bb = 0.5 gives slightly more weight to classification at the beginning of training, with the classification term becoming dominant towards the end. \u03bb = 0.1 results in poor accuracy, while \u03bb = 0.5 shows higher performance than \u03bb = 1. Margin loss improves classification accuracy, with \u03bb = 0.5 giving the best results at resolutions of 224 and 500 pixels. The image on the left shows channel 909 at different resolutions with GeM parameters. In the full resolution version, car locations are clearer, and p * = 3 reduces noisy detections. Training the architecture with data-augmented batches improves performance, even without ranking triplet loss. The validation accuracy of the network trained with data-augmented batches and batch sampling shows improved performance in the long run, reaching higher final accuracy. This benefit extends even to a pure image classification task, raising questions about its general applicability beyond specific architectures and training methods. Further analysis with a linear model and synthetic classification task supports this hypothesis. The study explores the benefits of using different data-augmented instances of the same image in one batch during training. Two batch sampling strategies, uniform sampling, and paired sampling, are considered. The experiment shows that the repeated augmentation scheme can enhance optimization and generalization in certain cases. The repeated augmentation scheme in training provides optimization and generalization benefits, reinforcing the impact of data augmentation. Resnet-50 features are extracted using GeM pooling and trained with cross-entropy, data augmentation, and uniform batch sampling. Additional results and an ablation study for the MultiGrain architecture show significant improvements over using just activations. The choice of not using triplet loss can be as effective in some cases, but may limit the embedding's versatility. The AutoAugment data augmentation does not transfer well to retrieval tasks due to its specificity to Imagenet classification. Learning AutoAugment specifically for retrieval tasks could help but may result in less general embeddings. Data augmentation is a limiting factor for multi-purpose embeddings, as improving for one task like classification can hurt performance for other tasks. Top-1/top-5 validation classification accuracies obtained by finetuning for higher evaluation scales from off-the-shelf networks are shown in Table E.1. In this section, additional classification results using pretrained networks like NASNet, SENet, and PNASNet are presented. A strategy for evaluating classifier networks trained with GeM pooling at different resolutions and exponents is discussed, which can be applied to pretrained networks as well. The evaluation scale is determined by finetuning the parameter p* using stochastic gradient descent on training images from ImageNet. This strategy has a limited memory footprint and converges quickly on a few thousand training samples. Finetuning is done with SGD using batches of 4 images and momentum 0.9. For fine-tuning, 50,000 images are selected from the training set with one pass on a reduced dataset using pretrained convnets. Different scales and choices of p* are tested, showing improved performance compared to standard evaluation. GeM pooling with p > 1 is found to be crucial for scale insensitivity and better performance at larger resolutions. Training from scratch with p > 1 pooling, repeated augmentations, and margin loss could further enhance results. Future work will focus on running training experiments on large networks with augmentations and margin loss, which is more expensive."
}