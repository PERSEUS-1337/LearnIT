{
    "title": "SJcKhk-Ab",
    "content": "Successful recurrent models like LSTMs and GRUs use gating mechanisms to improve learning of temporal dependencies and address gradient issues. Learnable gates in a model provide quasi-invariance to time transformations in input data. A new chrono initialization method enhances learning of long term dependencies in LSTMs and GRUs. Handling long term dependencies in temporal data has been a classical issue in recurrent networks due to the vanishing gradient problem. Feedback connections and gating mechanisms in models like LSTMs and GRUs help alleviate this issue. Using orthogonal weight matrices is another proposed solution, but it may come with computational overhead or limitations in representational power. The paper discusses how enforcing invariance to time transformations in data leads to gate-like mechanisms in recurrent models, deriving part of LSTM and GRU architectures. It also provides guidelines on initializing gate biases based on the range of time dependencies to be captured. The paper explains the benefits of setting the bias of the forget gate of LSTMs to 1 or 2 for good performance in capturing medium term dependencies. It also discusses the importance of being resilient to time rescaling in sequential learning tasks, showing improvements with long-term dependencies and minimal gains with short-term dependencies. In sequential learning tasks, it is important for recurrent models to be able to learn from time-warped input data as easily as from regular data. A class of models is considered invariant to time warping if they can handle changes in time, such as rescalings or accelerations, without affecting their behavior on the data. This allows for better performance in capturing medium to long-term dependencies in the input sequence. The text discusses the importance of recurrent models being able to learn from time-warped input data. It explains how gating mechanisms in the model are linked to handling time transformations, such as linear time rescaling. The text also delves into the continuous-time setting and describes how a time-rescaled model satisfies certain conditions. The time-rescaled model, when translated to a discrete-time model, becomes a leaky RNN. By taking the Taylor expansion of the recurrent model, a leaky model with a learnable parameter > 0 can represent input data in the same way as the original model. Learning the global characteristic timescale of the problem is crucial in this context. The global characteristic timescale of the problem is important for recurrent networks to be resilient to various time transformations of inputs, such as time warpings. Invariance to time warpings requires models to represent input data with any time warping function, allowing them to learn from different time scales effectively. To achieve time warping invariance, models must represent Equation (6) for any time warping, which is unknown a priori and needs to be learned. Ordinary recurrent networks are not invariant to time rescalings or warpings, so a learnable function representing the derivative of the time warping is introduced. This function, such as a recurrent neural network, defines a class of recurrent models that is quasi-invariant to time warpings, depending on the learning power of the function. To achieve time warping invariance, models must represent Equation (6) for any time warping, which is unknown a priori and needs to be learned. A specific model, like neural networks of a given size, can only represent a specific class of time warpings, providing quasi-invariance. The network will retain information about the inputs at time 0 for a duration of the order of magnitude of 1/ (0). The solution of FORMULA4 for \u210e( ) = \u2212 ( \u2212 0 ) \u210e( 0) retains information from the past < 0 for a time proportional to 1/ . This model is an extension of the RNN model providing invariance to time warpings, with input gating and forget gating. Time warpings are increasing by definition, and the model learns based on the data seen so far. The model (8) introduces input gating and forget gating for invariance to global time warpings, allowing each unit to have its own local contraction/dilation function for flexibility in handling different timescales in the signal. The model introduces input and forget gating for invariance to global time warpings, allowing each unit to have its own local contraction/dilation function for flexibility in handling different timescales in the signal. This yields a model with activation and incoming parameters of unit, resembling the evolution equation of cell units in LSTMs and hidden units in GRUs. The forget gate is tied to the input gate, saving parameters but not systematically compared. The Taylor expansion is valid when the derivative of time warping is not too large. The discrete-time gated models are invariant to time warpings that stretch time but not to those that make things happen too fast. The use of a sigmoid for the gate function aligns with constraints > 0 and < 1. It is reasonable to use a model with memory lying in the same temporal range as the sequential data's temporal dependencies. The biases of the gates greatly impact the values of ( ) over time, with values centered around ( ). To capture long-term dependencies, LSTM gate biases should be initialized with a uniform distribution. This is referred to as the chrono initialization. The chrono LSTM initialization is tested against the standard initialization on various tasks, outperforming standard LSTM initialization on synthetic tasks and competing on real world problems. It is also tested on next character prediction on the Text8 dataset and next word prediction on the Penn Treebank dataset. Single layer LSTMs with different sizes are used for experiments, except for word level prediction where a 10 layer deep recurrent highway network is utilized. Various recurrent architectures are compared on a task involving time warpings to test their robustness. Time-warped tasks are created by repeating each character a certain number of times in the input and output sequences. The task involves time-warped sequences with fixed or variable warping, using single layer LSTMs or a 10-layer deep recurrent highway network. The train dataset consists of 50,000 length-500 randomly warped random sequences with a size-10 alphabet. The goal is to minimize cross entropy in predicting the next character of the output sequence. The study compares three recurrent architectures (RNNs, leaky RNNs, and gated RNNs) with 64 recurrent units in predicting the next character of the output sequence using time-warped sequences with fixed or variable warping. The gated RNNs utilize LSTM-lite with tied input and forget gates. The study compares three recurrent architectures (RNNs, leaky RNNs, and gated RNNs) with 64 recurrent units in predicting the next character of the output sequence using time-warped sequences with fixed or variable warping. Gated architectures significantly outperform RNNs with moderate warping coefficients. Leaky RNNs solve uniform time warpings but struggle with variable warpings. Gated RNNs outperform RNNs and leaky RNNs in predicting the next character with time-warped sequences. They are quasi invariant to general time warpings and achieve perfect performance in both setups. Optimization for synthetic tasks is done using RMSprop with specific parameters. The copy task assesses a model's ability to remember information for long durations using an alphabet of 10 characters. The curr_chunk discusses a dataset where characters are randomly selected from the alphabet, with a signal character indicating the network to provide outputs. The target sequence involves remembering an input sequence for a specific number of timesteps. LSTM models with 128 units are used for tasks like the copy task, where the model needs to predict characters from a set range. The LSTM models with different initialization methods are compared on tasks like the copy task and variable copy task. The results show that chrono initialization outperforms standard initialization, especially on the variable copy task. The adding task is also mentioned in the context. The adding task involves two input sequences, one with numbers and the other with positions to add, aiming to predict the sum. LSTMs with 128 hidden units are used, with chrono initialization significantly speeding up learning, converging 7 times faster for certain values. The self loop feedback gating mechanism in recurrent networks is derived from first principles, with gated connections regulating local time constants. Chrono initialization, a method for initializing gate biases in LSTMs, shows benefits in handling long term dependencies. Additional experiments test the generalization capacity of different recurrent architectures on warping tasks. The generalization capacity of different recurrent architectures is tested on a warping task with varying warps. Results show that all networks have good generalization, with gated RNNs performing better but decreasing faster with longer warps. Plain RNNs have lower accuracy but more consistent performance. Additionally, pixel level classification tasks like MNIST and pMNIST are discussed. The model evaluates standard and chrono initialization on MNIST and permuted MNIST datasets using LSTMs with 512 hidden units. Results show chrono initialization performs better on permuted MNIST with a best validation result of 96.3%, compared to standard initialization at 95.4%. The text discusses the comparison between chrono and standard initialization on the character level text8 dataset BID17 using LSTMs with 2000 units. Chrono initialization outperforms standard initialization on the validation set by a small margin. On the character level text8 dataset BID17, chrono initialization slightly outperforms standard initialization in compression rate. This difference is consistent across two independent runs, suggesting that short term dependencies play a role in the performance difference. Additionally, chrono initialization shows resilience in more complex models like deep RHN networks on the word level Penn Treebank dataset BID21. The max bias is set to 11 based on previous research. Test results show a test perplexity of 6.54, similar to BID25."
}