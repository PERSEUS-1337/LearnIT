{
    "title": "SJl8J30qFX",
    "content": "Interpretability in machine learning has traditionally focused on local explanations, which explain why a model made a specific prediction for a sample. However, these explanations lack information about the overall behavior of the model. To address this, the proposal suggests using model distillation to learn global additive explanations that describe the relationship between input features and model predictions. These global explanations, in the form of feature shapes, offer more expressive insights than feature attributions. Through experimentation, it is demonstrated that global additive explanations can effectively describe model behavior and provide valuable insights into models like neural networks. The focus has shifted to developing global explanations that describe the overall behavior of a model, complementing local explanations. While not as accurate on individual samples, global explanations provide insights into feature trends and help understand the importance of features and detect unexpected patterns in training data. Model distillation techniques BID7 BID24 are proposed to learn global additive explanations for approximating the prediction function of a model F(x). The output is a set of feature shapes that offer insights into complex model behavior, particularly for fully-connected neural nets trained on tabular data. These global explanations complement local explanations by illustrating feature relationships. The contributions include proposing global additive explanations for complex models like neural nets, using model distillation with powerful additive models to learn feature shapes, and comparing feature shapes to other global explanation methods in terms of fidelity, accuracy, and interpretability. The approach involves using model distillation with powerful additive models to understand how inputs affect the model, based on previous research threads. Global additive explanations have been used to analyze inputs to complex models and decompose prediction functions into lower-dimensional components. The methods decompose the model using numerical or computational methods. The approach involves using model distillation with powerful additive models to understand how inputs affect the model. Instead of decomposing the model using numerical or computational methods, the proposal is to learn F using model distillation. This involves choosing flexible base learners such as splines and bagged trees to create global additive explanation models like SAT and SAS. The distillation setup to learn these models is described in Section 2.2. Higher order components can increase the accuracy of F, but make interpretation more difficult. Higher order components can increase the accuracy of F, but make interpretation more difficult. Examples of this expression are shown in Section 4.1, and the utility of adding higher order components when present in F is discussed in Section D.2. There has been work on approximating classifier models with interpretable models like trees and rule lists, but less on regression models. Rule lists for regression lack implementations in state-of-the-art models. Model distillation is used to learn these models. Model distillation simplifies the process of training teacher neural nets by requiring only labeled data. Generalized additive models can approximate a wide range of classification and regression models. Additive explanations are shown to be more interpretable than decision trees. The teacher models are fully-connected neural nets trained with specific optimization techniques and hyperparameters. Training student additive explanation models involves finding optimal feature shapes to minimize the error between teacher and student models. Different models are used, including FNNs with 2-hidden layers and 512 hidden units per layer. The optimization details for additive explanation models involve using cyclic gradient boosting for trees and cubic regression splines for splines. The models can be visualized as feature shapes to show the relationship between input features and predictions. Feature shapes, such as monotonic or concave/convex shapes, are learned from original data without distillation. They are different from feature attributions and are more expressive. A user study showed that humans can understand and use feature shapes effectively. Feature shapes are more expressive than feature attributions, describing the contribution of a feature across its entire domain to the model. Both global and local feature attributions can be derived from feature shapes. Humans can derive feature attribution from feature shapes, suggesting that an explanation of a model's prediction can be viewed as a model itself. Explanation models can be quantitatively evaluated for fidelity. In evaluating model explanations, fidelity and accuracy are key factors. Different methods like BID37 and BID40 focus on local fidelity, while BID29 evaluates global accuracy using prototypes. Baselines include additive explanations and interpretable models obtained through distillation. Partial dependence is a classic global explanation method estimating feature impact on predictions. The global explanation method estimates how predictions change as feature x j varies. It involves querying the neural net with new data samples generated by setting the value of their x j feature to z. The method constructs an additive function G through Taylor decomposition and defines the attribution of feature i as \u2202F (x) \u2202xi x i. SHAP is a state-of-the-art local explanation method that decomposes predictions additively between features using a game-theoretic approach. The text discusses a game-theoretic approach using the python package by the authors of SHAP to provide global explanations by averaging local attributions at unique feature values. The global attributions obtained are plotted as feature shapes and validated on synthetic data with known ground-truth shapes. The approach is then evaluated on real data against other explanation methods and interpretable models. A user study is designed to assess the interpretability of the feature shapes. In Section 4.2.2, a user study is designed to evaluate the interpretability of feature shapes. Further validation is done with controlled experiments on real data in Section 4.4, where data is simulated from synthetic functions to recover feature shapes. The study focuses on how predicted feature shapes differ for neural nets of varying capacities trained on the same data. In a study evaluating interpretability of feature shapes, synthetic functions from BID25, BID18, and Tsang et al. (2018) were used. Noise features x9 and x10 were added to samples with no effect on F1(x). Two teacher neural nets, 2H-512,512 and 1H-8, predicted F1 using all ten features. The high-capacity 2H neural net had a test RMSE of 0.14, while the low-capacity neural net had a test RMSE of 0.48. Global additive explanation models SAT and SAS were generated for each neural net, with faithful reconstruction RMSE values. This suggests that student methods accurately represent the teacher and are likely similar to each other. The RMSE error of teacher models on all samples is compared to samples where the predicted feature shapes agree or disagree with the ground truth shape. The feature shapes of global explanation models SAT and SAS closely match the ground-truth shapes, with the 2H model showing better alignment. However, limitations are observed in capturing sharp changes in the ground-truth shapes. The limitations of a 2-hidden-layer neural net are highlighted, achieving 0.14 test RMSE. SAT and SAS' feature shapes for the 1H neural net capture the gist but not the details of the ground-truth function. The methods fit what the teacher model has learned, matching ground-truth shapes when the teacher model is accurate. Feature shapes are used to predict inaccurately predicted samples by the teacher model. The learned feature shapes are evaluated against the ground truth using the teacher model. Lower teacher error is expected in agreement regions and higher error in disagreement regions. A statistical analysis confirms significant differences in errors between the two regions. The study used a Mann-Whitney-Wilcoxon rank sum test to show that teacher error is higher when feature shapes do not match ground truth. An augmented version of F1 was designed to analyze interactions in teacher predictions, showing that learning interactions is more challenging. The study compared feature shapes with and without interactions, showing that interactions can change feature shapes. Despite interacting with x1, the feature shape of x2 remained unchanged. The interaction term x1x2 has an expected value of zero, not affecting the feature shape. The expected value of |x3|2|x4| when x3 \u223c U(\u22121, 1) is 1/(2|x4| + 1), modifying the feature shape. The study compared feature shapes with and without interactions, showing that interactions can change feature shapes. Five data sets were selected for analysis, including Bikeshare, Magic, LendingClub, FICO, and pneumonia data. 2H neural nets showed the most accuracy gain over 1H neural nets on Bikeshare, Loan, and Magic. Results for 1H neural nets are in the Appendix. SAT and SAS yielded similar results in terms of accuracy and fidelity, with SAT being more accurate in some cases like Magic, and SAS having the edge in others like FICO. Trees are locally adaptive smoothers better able to adapt to sudden changes in input-output relationships than splines, but this also gives them more capacity to overfit. Trees tend to have more jagged feature shapes than splines, especially in regions with fewer points. gSHAP is recommended for local explanations, while global model distillation methods perform better for producing global explanations. Fidelity of SAT compared to other interpretable models on Bikeshare and Pneumonia data is shown in Figure 5, with SAT performing well in terms of accuracy and fidelity. The study compares decision trees (DT) and sparse L1-regularized linear model (SPARSE) trained using scikit-learn. Results are presented based on a model-specific parameter K controlling model complexity. SPARSE performs poorly in terms of accuracy and fidelity compared to DT. Explanation methods using sparse linear models are used for local, not global, explanations and only for classification, not regression. Trees start to match the accuracy of SAT. The study compares decision trees (DT) and sparse L1-regularized linear model (SPARSE) trained using scikit-learn. Trees start to match the accuracy of SAT at depth K = 6 (64 leaves). However, interpretability is hindered by depth, and they do not always perform as well as SAT. For example, on Pneumonia, a depth K = 12 tree achieved lower accuracy than SAT. The user study results are shown in Table 4. The study compared decision trees (DT) and sparse L1-regularized linear model (SPARSE) for interpretability. A subgroup discovery algorithm (S-RULES) was used for regression but did not generate disjoint rules. S-RULES were found to be less faithful than SPARSE on Pneumonia. A user study with 50 subjects showed that feature shapes could be understood and used by humans, comparing them to DT, SPARSE, and S-RULES. Individuals familiar with machine learning concepts such as if-then-else structures, scatterplots, and equations were studied using explanation models to answer questions on inferential and comprehension tasks. The study compared the accuracy of decision trees and SAT models, with the depth 4 tree showing promising results. The study compared the accuracy of decision trees and SAT models, with the depth 6 tree showing an RMSE of 0.98 for SAT and 1 for DT-6. DT-4 used five features: Hour, Year, Temperature, Working Day, Season. Feature shapes were displayed for SAT based on these five features. Subjects were randomly assigned to see outputs from different models in stages, including smaller versions with only the two most important features. The study also examined the understanding and use of feature shapes by humans. The study compared decision trees and SAT models, with SAT-5 subjects outperforming DT-4 in ranking feature importance. While 0% of DT-4 subjects predicted the correct order of all 5 features, 45% of SAT-5 subjects did. Common mistake was inverting the ranking of Season and Working Day. One SAT-5 subject noted demand increases during morning and evening commuting hours. Demand for bike usage is highest during morning and evening commuting hours, with a flat trend during working hours. SAT-5 subjects were better at detecting peak patterns during these times compared to DT-4 subjects. SAT-5 subjects also excelled at computing prediction changes with temperature variations and identifying errors in the data related to seasonal bike demand. The study compared the performance of different models in predicting bike demand based on features like Hour and Temperature. Smaller models showed better performance in certain tasks, such as predicting feature order and computing prediction changes. However, simplifying the models led to a trade-off between ease of use and accuracy. Increasing the number of features from 2 to 5 significantly impacts the time needed to interpret models. The SAT model sees a 60% increase in time, while the DT model experiences a 166% increase. Interpreting deeper trees leads to more mistakes. The subgroup-rules and sparse linear models are also interpretable but less accurate. The SPARSE model is the fastest to interpret at 5.1 minutes on average but hides interesting patterns. For example, all subjects identified an increasing relation between demand and hour, with 83% predicting the exact increase amount. The SAT-5 and DT-4 models provided unique insights that other models did not. S-RULES was difficult to interpret, with subjects struggling to predict feature importance and changes in predictions. Despite non-disjoint rules, 66% of subjects identified peak demand during rush hour. Feature shapes were crucial for human understanding and performance in interpreting models. In this paper, interpretable representations allowed humans to outperform decision trees, sparse linear models, and rules in ranking feature importance and identifying patterns. Feature shapes were easier to understand than big decision trees. Small decision trees and sparse linear models were better at calculating prediction changes but less accurate. Global additive explanations were validated on real data by modifying labels on Bikeshare dataset. On Bikeshare dataset, label modification was done by adding 1.0 to the label for samples with humidity between 55 and 65. A 2H neural net was retrained on the modified data to learn feature shapes. The feature shapes of the new neural net should be similar to the original net except for a \"bump\" in the humidity feature range. Data modification involved expert discretization, such as transforming continuous variables like body temperature into bins like normal, mild fever, etc. In an experiment, additive explanation models were tested to see if they could detect discretizations in features learned by a neural net without direct access to the discretized features. Student models were trained using original un-discretized features as input and neural net outputs as labels. Feature shapes of two features in Pneumonia data were compared between teachers trained on continuous data and teachers trained on discretized features. Our approach captures expected discretization intervals in the data and presents a method for interpreting complex models like neural nets trained on tabular data. The method, based on distillation with high-accuracy additive models, offers clear advantages over other approaches. Global additive explanations are not meant to compete with local or non-additive explanations, but to show different interpretable representations work effectively. Our approach focuses on interpreting complex models like neural nets trained on tabular data by capturing expected discretization intervals. Global additive explanations are valuable for tasks requiring quick understanding of feature-prediction relationships, but not suitable for raw image inputs. The method can be applied to any classification or regression model, including random forests and CNNs. The feature shapes for features x1 to x9 of F2 show that x9, a noise feature, has little importance. Table A1 compares accuracy and fidelity of global explanation models for 1H and 2H neural nets. 1H nets are easier to approximate but less accurate on test data. FICO data shows better fidelity for 2H explanations. The fidelity of the gGRAD explanation to the 1H neural net is worse than other methods on the Magic data. Individual gradients of the 1H neural net show extreme predictions for some samples. The gGRAD explanation generates extreme predictions for samples not faithful to the teacher's predictions, especially in overfitted neural nets. AUC is less affected by extreme values compared to RMSE. Feature shapes are used to check for monotonicity in domains like credit scoring. The neural net's feature shapes are used to impose monotonicity on 16 features, with 15 showing the expected trend except for \"Months Since Most Recent Trade Open\" which increased instead of decreasing monotonically. This discrepancy was confirmed through global explanations and a quick experiment sampling values across the feature's domain. The majority of samples had predictions that increased as a feature increased, confirming a monotonically increasing pattern in the neural net. Visualizing neural net training: 3 features showed expected patterns, but \"Months Since Most Recent Trade Open\" increased instead of decreasing monotonically. Visualizing neural net training reveals insights into underfitting, overfitting, and different training scenarios. The video demonstrates how a neural net learns on a medical dataset, showing feature shapes before, at, and after early-stopping point as it progresses from underfit to optimally fit to overfit. The main cause of overfitting is not just increased non-linearity in the fitting function, but also other factors. In neural net training, overfitting is caused by unwarranted growth in model confidence as logits become more extreme. Adding pairwise interactions improves model performance, as seen in Bikeshare data where RMSE decreases from 0.98 to 0.60. An interesting interaction is observed between \"Time of Day\" and \"Working Day\" in bike rental demand patterns. The demand for bike rentals peaks during midday from 10am-4pm, with interactions with temperature. A global explanation method must incorporate these interactions for high-fidelity explanations. Higher-order components are added to the global additive explanation to achieve this."
}