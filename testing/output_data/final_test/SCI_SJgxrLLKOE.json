{
    "title": "SJgxrLLKOE",
    "content": "Engineered proteins have the potential to solve various problems in biomedicine, energy, and materials science, but designing successful proteins is challenging due to the complex relationship between protein sequence and 3D structure. The task of finding a viable protein design is known as the inverse protein folding problem. Generative models for protein sequences conditioned on a graph-structured design target have been developed to capture complex dependencies efficiently. This approach focuses on long-range sequence dependencies but local 3D spatial relationships, improving upon prior parametric models. The goal is to automate the creation of protein molecules with specific structural and functional properties using deep generative models. The field of protein design has made significant progress in the past two decades, including the creation of novel 3D folds, enzymes, and complexes. However, current practices often involve multiple rounds of trial-and-error, with initial designs frequently failing. Challenges arise from the bottom-up approach that relies on energy functions and sampling algorithms. An alternative top-down framework for protein design involves learning a generative model for protein sequences based on a target structure represented as a graph. This approach aims to automate the invention of protein molecules with specific properties. Our model enhances sequence models with graph-based 3D structure descriptions, capturing higher-order dependencies efficiently. Leveraging sparse and localized graph and self-attention in 3D space, we achieve linear computational scaling. This approach benefits from protein science insights, enabling representational flexibility and inductive bias. Our model combines sequence models with graph-based 3D structure descriptions, allowing for flexible representation and improved generalization performance in protein engineering and design. Our model captures the joint distribution of the full protein sequence by incorporating long-range interactions from the structure, unlike previous works that focus on independent predictions or specific positions. Additionally, there has been significant progress in developing deep generative models for protein sequences within individual protein families. Several groups have achieved promising results using protein language models to learn sequence representations that transfer well to supervised tasks. Conditional generative modeling can facilitate adaptation to specific parts of structure space, but language models are bottlenecked by the limited number of evolutionary 3D folds. Evaluating protein language models with structure-based splitting of sequence data reveals challenges for unconditional language models in assigning likelihoods to sequences from out-of-training folds. Deep models of protein structure have been proposed to create 3D structures for sequence design. A non-parametric approach to protein design decomposes target designs into substructural motifs queried against a protein database. The model extends the Transformer to capture sparse relational information between sequence elements. Our model incorporates graph-structured self-attention to represent protein structure as an attributed graph with node and edge features. This approach allows for different variations in macromolecular design, including 3D considerations. The text discusses the macromolecular design problem, focusing on the need for a graph representation of coordinates that is invariant to rotations and translations while being locally informative. This is motivated by limitations of current graph neural networks. The text introduces invariant and locally informative features for graph representation in the macromolecular design problem. It involves augmenting points with orientations to define a local coordinate system and deriving spatial edge features from rigid body transformations. The text introduces features for distance, direction, and orientation in graph representation for macromolecular design. It includes positional embeddings based on relative positioning of nodes and utilizes quaternion representations for rotations. In contrast to the absolute positional encodings of the original Transformer, the text introduces relative positional encodings in BID29. It combines structural and positional encodings to create an aggregate edge encoding vector. Node features include dihedral angles embedded on a 3-torus, while edge features include contacts and hydrogen bonds based on distance and electrostatics. The Structured Transformer model introduced in this work incorporates relational information by using local attention for each node's k-nearest neighbors in 3D space. Edge features are included to embed spatial and positional dependencies in deriving attention, extending the Transformer model to spatial contexts. Our model extends the Transformer to spatially structured settings by incorporating edge features to embed dependencies in deriving attention. The joint distribution of the sequence given structure is decomposed autoregressively, with the encoder computing node embeddings from structure-based features and the decoder predicting the next amino acid based on the preceding sequence and structural embeddings. The model extends the Transformer to spatially structured settings by incorporating edge features for attention. The encoder computes node embeddings from structure-based features, while the decoder predicts the next amino acid based on the sequence and structural embeddings. The component allows the head to attend to separate subspaces of the embeddings via learned query, key, and value transformations. The encoder yields refined embeddings as it traverses multiple layers, with the topmost layer producing the output. The decoder module follows the same structure as the encoder but with augmented relational information. The decoder module in the model has a structure similar to the encoder but includes relational information for causally consistent access to preceding sequence elements. It utilizes self-attention and position-wise feedforward modules with a hidden dimension of 128. The concatenation and masking structure ensure that sequence information flows only from preceding positions to the current position, allowing for attention to subsequent structural information. The dataset used for evaluating model generalization across protein folds was based on the CATH hierarchical classification of protein structure. It included 18025 chains in the training set, 1637 chains in the validation set, and 1911 chains in the test set, with a cluster-split at the fold level. The models were trained using specific parameters and early stopping based on validation perplexity. The focus was on evaluating model performance using perplexity per letter of test protein folds. Protein sequences with the same 3D structure may have similar designs, and likelihood-based evaluations were conducted. Simple models of protein sequences were used to provide context for protein perplexities. The dataset used for evaluation included training, validation, and test sets based on the CATH hierarchical classification of protein structure. The average perplexity per letter of protein profiles in Pfam 32 was found to be around 11.6, indicating potential usefulness for functional protein sequences. There was a significant gap between unconditional language models and models conditioned on structure, with test perplexities for structure-independent models barely better than null letter frequencies. Protein language models trained on specific 3D folds struggle to predict sequences of unseen folds, impacting protein engineering. Structured Transformer model achieved a perplexity of \u223c7 on the test set, emphasizing the importance of local orientation information in protein structure. Comparison with SPIN2, a computationally intensive method for predicting protein sequence profiles, revealed insights on model performance. The Structured Transformer model improved perplexities over SPIN2 TAB1 on subsets of the test set, showing enhanced performance in designing protein sequences based on structural graphs. The model incorporates 3D structural encodings for efficient computation and achieves significantly improved perplexities on unseen folds compared to existing generative models. Our framework suggests efficiently designing protein sequences with deep generative models, emphasizing modeling sparse long-range dependencies in biological sequences."
}