{
    "title": "rJUBryZ0W",
    "content": "In representational lifelong learning, an agent aims to continually learn to solve novel tasks while updating its representation based on previous tasks. A framework for lifelong learning in deep neural networks is developed using generalization bounds within the PAC-Bayes framework. Learning involves constructing a distribution over networks from tasks seen so far and using it to learn new tasks, incorporating prior knowledge through a history-dependent prior. A gradient-based algorithm is developed to minimize an objective function motivated by generalization bounds. Learning from examples involves inferring general rules from a finite set of examples, requiring prior assumptions. In Machine Learning, the concept of inductive bias is crucial. Recent work in deep neural networks has utilized prior knowledge through structural constraints like convolutions and weight sharing. However, determining relevant prior information for a task can be challenging, highlighting the need to build prior knowledge through learning from previous interactions with the world. In lifelong learning, knowledge is extracted from observed tasks for future learning on new tasks, evaluated on performance. The agent learns through interaction with the world, transferring acquired knowledge to new tasks encountered. In lifelong learning, knowledge is extracted from observed tasks to improve performance on new tasks. Baxter's work introduced the concept of modeling lifelong learning as tasks sampled from an unknown distribution, allowing for the use of prior knowledge to enhance performance. This framework has led to various extensions and developments in the field. The BID25 framework provides generalization error bounds within the PAC-Bayes framework, leading to the development of a practical learning algorithm applied to neural networks. The main contributions include an improved theoretical bound, a learning algorithm for transfer of knowledge between tasks, and empirical performance enhancement compared to other methods. Since the BID25 framework provided generalization error bounds within the PAC-Bayes framework for practical learning algorithms applied to neural networks, recent developments in lifelong learning have focused on theoretical approaches but have not directly led to practical algorithms for deep neural networks. The DISPLAYFORM0 framework involves selecting a function h \u2208 H that minimizes the expected loss function E (h, z) based on a sample S drawn from an unknown probability distribution D. In classification, H is a space of classifiers mapping the input space to classes, and an inductive bias is needed for effective learning. The PAC-Bayes framework considers constructing a complete probability distribution instead of selecting a single classifier based on empirical error minimization. The PAC-Bayes framework, first formulated by BID22, involves constructing a complete probability distribution over H and selecting a hypothesis h \u2208 H based on this distribution. It is not necessarily Bayesian and offers flexibility in learning with some of the best generalization bounds available. Recently extended to lifelong learning by BID25, it will be applied to neural networks in this contribution. In this contribution, the generalization error and empirical error are defined in the standard learning setting. The PAC-Bayesian framework involves outputting a distribution over the hypothesis space H, aiming to provide a posterior distribution Q \u2208 M. The generalization error and empirical error are then averaged over the posterior distribution, describing a Gibbs prediction procedure. However, the generalization error er (Q, D) cannot be directly computed due to the unknown distribution D. In this section, a PAC-Bayesian bound is introduced for the single-task setting, which will also be applicable for lifelong-learning. The bound is based on a prior distribution P \u2208 M, independent of observed data S. The posterior distribution Q over hypotheses is output from the learning process. The bound states that the expected error er (Q, D) is upper bounded by the empirical error plus a complexity term with high probability. The PAC-Bayesian bound introduces a trade-off between fitting the data and a complexity term, encouraging the selection of a simple hypothesis. The contribution of the regularization term is more significant for smaller data sets, converging to zero for large sample sizes. The choice of prior affects the bound's tightness and should reflect prior knowledge about the problem, aiming for a prior close to posteriors achieving low training error. In the context of PAC-Bayesian bound, a focus on deriving an algorithm for lifelong-learning is emphasized rather than calculating the bound itself. The goal is to minimize the bound, even if it is vacuous, as it captures the generalization error behavior. Experimental use of positive and unbounded loss functions is explored, with the theoretical claim of bounding a variation of the loss clipped to [0, 1]. In the lifelong-learning setting, a lifelong-learner observes multiple training tasks from the same environment to extract common knowledge for learning new tasks. The problem is often referred to as learning-to-learn or meta-learning. A generalization bound is provided, extending previous work, and practical application in non-linear deep models is demonstrated using stochastic learning. The tasks share sample space, hypothesis space, and loss function, but differ in sample distribution. The lifelong-learning agent observes training sets from different tasks with unknown sample distributions. The goal is to extract knowledge from observed tasks to use as prior knowledge for learning new tasks. This prior knowledge is in the form of a distribution over hypotheses. When learning a new task, the learner uses the observed task's data and prior knowledge to output a posterior distribution over hypotheses. The quality of a prior is measured by expected loss when used for learning new tasks. In lifelong-learning, a distribution Q over all prior distributions is inferred, called the hyper-posterior distribution. This serves as a prior for learning new tasks, where the learner draws a prior from Q. The performance of Q is measured by the expected generalization loss of learning new tasks using priors generated from Q, denoted as the transfer error. In lifelong learning, a hyper-posterior distribution Q is inferred over all prior distributions, serving as a prior for new tasks. This distribution is updated after observing data from all tasks, allowing for minimization of the empirical multi-task error. The hyper-posterior is evaluated on new tasks, not on observed tasks used for training. In lifelong learning, a hyper-posterior distribution Q is inferred over all prior distributions, serving as a prior for new tasks. A novel bound on transfer error in lifelong learning setup is presented, with the theorem proved in the appendix. The transfer error is bounded by the empirical multi-task error plus two complexity terms. The proof presented in the appendix introduces two main steps. The first step uses a single-task PAC-Bayes theorem to bound the generalization error in each task separately. The second step bounds the generalization error at the task-environment level by considering the specific number of samples in each observed task. Our proof technique utilizes different single-task bounds in each step, with the hyper-prior still affecting the bound in Theorem 2. The complexity terms are tighter compared to BID25, as demonstrated in sections 8.1 and 8.2. Empirical evaluation is conducted in the experiments section. In the experiments section, the transfer risk using the bounds as learning objectives is evaluated, showing significant improvement. A practical learning procedure for differentiable models, including deep neural networks, is derived by choosing a specific form for the Hyper-posterior distribution Q. The text discusses limiting the search to a specific family of hyper-posteriors by choosing a Gaussian distribution for the hyper-prior parameters. This choice serves as a regularization term for learning the prior and encourages robustness to perturbations during training. Sampling from the hyper-posterior involves adding Gaussian noise to the prior parameters, aiming to prevent overfitting and select more stable solutions. The text discusses choosing solutions less prone to overfitting and better generalization. The single-task learning procedure minimizes generalization error and uses prior knowledge for a better learning objective. Formulating learning as an optimization problem enables joint learning of shared prior and task posteriors. The text discusses formulating lifelong-learning as an optimization problem by defining posterior and prior distributions. The hypothesis class H is defined as a family of functions parameterized by a weight vector. The aim is to use neural network architectures, specifically Stochastic NNs, for the algorithm. The text discusses using Stochastic NNs for optimizing posterior distributions with factorized Gaussian distributions for weights in deep models. Stochastic gradient descent is used as the optimization method. Stochastic gradient descent (SGD) is used to optimize posterior distributions with factorized Gaussian weights in deep models. Lower variance facilitates convergence speed. The empirical error term in each single-task bound poses challenges due to large datasets and non-linear terms. Unbiased and low variance gradient estimates are obtained by using a subset of data for each estimation. To optimize posterior distributions with factorized Gaussian weights in deep models using stochastic gradient descent (SGD), a randomly sampled mini-batch is used for gradient estimation. The reparametrization trick, based on Gaussian distributions, is employed for efficient and low variance optimization. The choice of using \u03c1 = log \u03c3 2 as a parameter allows for unconstrained parameters. The reparametrization trick involves drawing \u03b5 from a Gaussian distribution and applying a deterministic function to compute w. This allows for switching the order of gradient and expectation, making it computable with backpropagation. The LAP algorithm consists of two phases: meta-training to learn a prior from observed tasks, and meta-testing to apply the learned prior to new tasks. The LAP algorithm involves two phases: meta-training to learn a prior from observed tasks and meta-testing to apply the learned prior to new tasks. The first phase can be used independently as a multi-task learning method. The setup includes a simple 2D estimation problem with a goal to estimate the mean of the data generating distribution using Euclidean distance as the loss function. The data for each task is generated from specific distributions, and Algorithm 1 is run with complexity terms according to Theorem 1. The learned prior and single-task posteriors can be visually understood. In the meta-training phase of the LAP algorithm, a prior is learned from observed tasks. The learned prior and single-task posteriors are visually understood, with the prior located between the posteriors. The prior has larger covariance in the first dimension, indicating tasks are likely to have values around 1 in dimension 2 and values around 3 in dimension 1. This allows for learning new similar tasks with fewer samples. The transfer method's performance is demonstrated in image classification tasks using deep neural networks. The experiment involves using a small CNN with 2 convolutional layers and a linear hidden layer to learn a prior from 5 tasks in a task environment. The meta-test task has fewer training samples (2,000). The prior is visually represented as a blue dot with larger covariance in the first dimension. The performance of the transfer method is demonstrated in image classification tasks using deep neural networks. The experiment involves using a small CNN to learn a prior from 5 tasks in a task environment. The performance of transfer methods in image classification tasks using deep neural networks is compared. The methods include Scratch-standard, Scratch-stochastic, Warm-start-transfer, and Oracle-transfer. The Oracle-transfer method freezes all layers except the output layer, a common practice in transfer learning in computer vision. In computer vision, the Oracle-transfer method involves manually inserting prior knowledge based on task environment familiarity. Various methods are compared for transferring knowledge from training tasks, including LAP-M, LAP-S, LAP-PL, and LAP-KLD, each with different objectives and bounds. LAP-KLD includes task-complexity and environment-complexity terms, aiming to maximize the Evidence-Lower-Bound (ELBO). The minimization problem is equivalent to maximizing the Evidence-Lower-Bound (ELBO) in variational methods for hierarchical generative models. The ELBO serves as an upper bound on generalization error, but is less tight than PAC-Bayesian methods. Averaged-prior and MAML algorithms are used for meta-learning tasks in different ways. The LAP algorithm with different variants and objectives is compared in terms of performance. The LAP-M and LAP-S variants show significant improvement over learning from scratch and naive warm-start transfer, approaching the \"oracle\" method. LAP-S variant performs better due to a tighter bound. Other LAP variants with different objectives perform much worse, highlighting the importance of the tight generalization bound in the LAP algorithm. In our work, results for LAP-KLD suggest that minimizing upper bounds on generalization error may be more successful than maximizing lower bounds on model evidence. The \"averaged-prior\" method performs similarly to learning from scratch due to high non-linearity. MAML results are comparable to LAP, showing effectiveness in learning from few-shot tasks. Our framework for representational lifelong learning is motivated by PAC-Bayes. Our framework for representational lifelong learning, inspired by PAC-Bayes, adjusts a learned prior based on encountered tasks. It combines deep neural networks with gradient-based methods for efficient lifelong learning, supported by theoretical motivation and empirical demonstrations. The approach emphasizes using rigorous performance bounds to derive learning algorithms, showing that tighter bounds lead to improved performance. Our framework for representational lifelong learning adjusts a learned prior based on encountered tasks. It emphasizes using performance bounds to derive learning algorithms for efficient lifelong learning. The current version learns tasks in parallel, but a sequential approach would be more beneficial. Training stochastic models poses challenges due to high-variance gradients, so developing more stable convergence methods is a priority. Incorporating our approach in reinforcement learning, especially in model-free learning with model-based components, would be a valuable challenge. The proof of Theorem 2 is based on McAllaster's PAC-Bayes bound, with two steps to bound error from finite samples in tasks and limited tasks in the environment. The classical PAC-Bayes bound is restated using general notations in Theorem 3. Theorem 3 provides a bound on generalization error for observed tasks using a prior distribution over hypotheses and samples. The bound holds for all posterior distributions, even sample-dependent ones. The algorithm Q outputs a distribution over hypotheses based on prior and samples, with substitutions made for observed tasks. The loss function uses the hypothesis element in a tuple, and the prior over hypotheses is defined as a distribution over M x H. Theorem 3 provides a bound on generalization error for observed tasks using a prior distribution over hypotheses and samples. The bound holds for all posterior distributions, even sample-dependent ones. The algorithm Q outputs a distribution over hypotheses based on prior and samples, with substitutions made for observed tasks. The loss function uses the hypothesis element in a tuple, and the prior over hypotheses is defined as a distribution over M x H. The bound of Theorem 4 converges faster than the classical bound of Theorem 1. The network architecture for the permuted-labels experiment includes a small CNN with 2 convolutional layers, a hidden linear layer, and a linear output layer. Dropout with p = 0.5 is applied before the output layer. The LAP algorithm ran for 200 epochs with batches of 128 samples in each task. ADAM optimizer with a learning rate of 10^-3 was used. Initializations for weights were done randomly. The hyper-prior and hyper-posterior parameters for the trained network were set at \u03ba P = 2000 and \u03ba Q = 0.001 respectively, with a confidence parameter of \u03b4 = 0.1. The evaluation used the maximum of the posterior for inference, focusing on the means of the weights. Recent works have proposed tighter PAC-Bayesian bounds, but these were not incorporated in this experiment. Classification was done using the majority vote of several runs, yielding similar results."
}