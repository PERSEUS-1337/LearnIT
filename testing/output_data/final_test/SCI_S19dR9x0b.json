{
    "title": "S19dR9x0b",
    "content": "Recurrent neural networks have shown great performance in various applications but face challenges with large models on portable devices and high latency on servers. To address this, we quantize the network into binary codes, optimizing the process and achieving significant memory savings and real-time inference acceleration. This method is tested on LSTM and GRU models for language processing. By 2-bit quantization, we achieve ~16x memory saving and ~6x real inference acceleration on CPUs, with a reasonable loss in accuracy. 3-bit quantization results in almost no loss in accuracy, surpassing the original model, with ~10.5x memory saving and ~3x real inference acceleration. This method outperforms existing quantization works significantly. The alternating quantization technique is extended to image classification tasks, showing excellent performance in both RNNs and feedforward neural networks. The models in language models, neural machine translation, automatic speech recognition, and image captions often have too many parameters for portable devices with limited resources. RNNs can only be executed sequentially, causing large latency during inference. Techniques like low rank approximation and sparsity can help alleviate these issues. In this work, the focus is on quantization based methods, specifically quantizing all parameters into multiple binary codes. Binarization of weights and activations can achieve good performance in visual classification tasks, reducing memory usage and accelerating operations. Real coefficients can be incorporated to compensate for binarization errors, as shown in the challenging ImageNet dataset. Some recent works have employed quantization with more bits to bridge the gap between binarization and full precision networks. While most quantization works focus on CNNs, there is limited attention on RNNs, which are also demanding. Binarized LSTM with preconditioned coefficients has shown promising performance in certain tasks. The recent works have focused on quantization with more bits to improve RNNs, which are also challenging. BID12 and BID28 tested multi-bit quantized RNNs for predicting the next word, but results still lag behind full precision. This study introduces an optimization approach for multi-bit quantization, using learned binary codes {\u22121, +1} instead of rule-based methods. The optimization process is simplified by removing discrete unknowns, making it easier to handle. We propose alternating minimization for quantization, separating binary codes and real coefficients for efficient solving. With proper initialization, only two alternating cycles are needed for high precision approximation, effective for online activation quantization. Systematic evaluation on language models like LSTM and GRU shows significant memory savings and real inference acceleration with 2-bit and 3-bit quantization, with minimal loss in accuracy. Our proposed alternating quantization technique achieves significant memory savings and real inference acceleration in language models like LSTM and GRU with 2-bit and 3-bit quantization. The method outperforms existing quantization works and is applicable to image classification tasks as well. Balanced quantization BID28 optimally separates values into quantized intervals, reducing the need for costly operations in quantizing vectors. It equalizes data distribution by constructing intervals with equal data percentages and mapping their centers to quantization codes. This method improves upon uniform quantization for non-uniform data like trained weights and activations in deep neural networks. The affine transform on centers may still be suboptimal, and evenly spaced partition may not always be better than non-evenly spaced partition for specific data distributions. Greedy approximation BID6 learns quantization efficiently by minimizing residue sequentially. Refined greedy approximation BID6 further reduces quantization error in iterations. In experiments, a refined method is used to decrease quantization error in CNN weight quantization. Ternary quantization is proposed as a special case of 2-bit quantization with additional constraints. Efficient algorithms are lacking, so empirical methods are used to set entries for quantization. Ternary quantization is a special case of 2-bit quantization with the constraint that \u03b1 1 = \u03b1 2. The optimal coefficient \u03b1 1 (or \u03b1 2) can be derived using a least squares solution. A Binary Search Tree (BST) algorithm is used to determine the optimal code for quantization. In parallel, vector quantization is applied to compress weights in neural networks, reducing the number of parameters significantly with limited accuracy loss. The text discusses a quantization method for compressing weights in neural networks. It introduces a method for multi-bit quantization and explains how quantization codes are determined based on the least distance to all codes. The process involves partitioning the number axis into intervals corresponding to quantization codes. The text introduces a method for multi-bit quantization of weights in neural networks. It explains how to efficiently determine optimal codes by partitioning codes hierarchically into ordered subsets. This process involves constructing a binary search tree to compare codes with boundaries, reducing the number of comparisons needed. The method for multi-bit quantization of weights in neural networks involves constructing a binary search tree to compare codes with boundaries, reducing the number of comparisons needed. The optimal codes can be derived by a closed form solution for k = 2. The refined greedy approximation BID6 is improved by alternating minimizing DISPLAYFORM6. The method for multi-bit quantization of weights in neural networks involves using greedy initialization to find high precision quantization with two alternating cycles. The implementation details include quantizing RNN, specifically a one layer LSTM for language model prediction. In the method for multi-bit quantization of weights in neural networks, the multiplication between weight matrices and vectors x t and h t is the main computation. Quantization is applied row by row on the weight matrices, while h t needs to be quantized online during inference. This process brings more freedom to approximate weights better. Quantization is applied row by row on weight matrices in neural networks. For the quantized product between k w -bit W and k h -bit h t, there are binary and non-binary operations involved. The acceleration ratio can be up to 32\u00d7 in theory due to binary multiplication. The overall theoretical acceleration is computed based on LSTM hidden states. For (k h, k w) = (2, 2), the acceleration ratio is roughly 7.5\u00d7, and for (k h, k w) = (3, 3), it is 3.5\u00d7. In real implementations, the size of the matrix greatly affects acceleration, with memory reduction leading to better cache utilization. Binary multiplication kernel in CPUs achieves 6\u00d7 for (k h , k w ) = (2, 2) and 3\u00d7 for (k h , k w ) = (3, 3) compared to Intel Math Kernel Library. Binary multiplication can be conducted sequentially or by concatenating binary codes for parallel computing. The training of quantized neural networks involves adding small gradients to quantized weights without changing them, maintaining full precision weights to accumulate gradients, and applying quantization in every mini-batch. This process can be formulated as a bi-level optimization problem, where quantized weights are derived from full precision weights during forward propagation and gradients are propagated back during backward propagation. In this paper, the authors discuss the use of the \"straight-through estimate\" for quantization in RNNs. They compare different quantization methods on pre-trained LSTM weights and conduct experiments on language models. The authors evaluate LSTM BID9 and GRU BID0 on language models using perplexity per word (PPW) metric. They initialize with a pre-trained model and use vanilla SGD with an initial learning rate of 20. Training is terminated when the learning rate is less than 0.001 or after 80 epochs. The network is unrolled for 30 time steps and regularized with standard dropout (dropout probability of 0.5). The authors experiment with different quantization methods on the PTB dataset using LSTM and GRU models with a 300-size hidden layer. Results show that the proposed Alternating quantization method achieves lower error rates compared to other methods across varying bit precisions. The proposed Alternating quantization method achieves lower error rates across varying bit precisions compared to other methods. Results also show that less errors result in lower testing PPW. The experiments were repeated on other datasets with similar results for LSTM and GRU models. The experiments included quantizing both weights and activations with a batch size of 20. The final results are shown in TAB3, comparing with existing works and a competitive baseline. Our Alternating quantization method outperforms existing works in terms of lower error rates and testing PPW. It achieves compatible performance with 1-bit less quantization on weights or activations compared to Refined. The 3/3 weights/activations quantized LSTM shows even better performance than the full precision model, possibly due to regularization introduced by quantization. The WikiText-2 dataset is a recent alternative to PTB, with a large vocabulary and token count. The Text8 corpus dataset has 15.3M training tokens, 848K validation tokens, and 855K test tokens with a vocabulary size of 42K words. LSTM and GRU models are trained with one hidden layer of size 1024. The Alternating quantization method is tested on this dataset to determine its effectiveness with a larger dataset. The LSTM and GRU models were trained with a vocabulary size of 42K words and one hidden layer of size 1024. The Alternating quantization method showed excellent performance, surpassing Refined with 2-bit quantization on weights and activations. However, there was still a gap with full precision when using 3-bit quantization for GRU. The unified hyper-parameter setting across all experiments may have contributed to this gap, but specifically tuned hyper-parameters could potentially bridge it. The alternating quantization technique is not limited to language models and has been successfully applied to image classification tasks as well. In this work, the authors address the limitations of RNNs by proposing an alternating quantization method to reduce memory and latency. By applying this method to LSTM and GRU models on language tasks, they achieve comparable accuracy with significant reductions in memory and acceleration. The results are detailed in the appendix due to space limitations. The authors propose an alternating quantization method to reduce memory and latency in RNNs, achieving significant improvements in performance. They implement a binary multiplication kernel in CPUs, utilizing XNOR and bit count operations for acceleration on Intel Xeon E5-2682 v4 @ 2.50 GHz CPU. The authors implemented a binary multiplication kernel in CPUs using XNOR and bit count operations for acceleration. They compared their method with the Intel Math Kernel Library on matrix vector multiplication tasks, showing that their alternating quantization step only contributes minimally to the total execution time. The authors conducted experiments on the sequential MNIST classification task using their alternating quantization method, achieving acceleration with 2-bit and 3-bit quantization. The method can be extended to GPU, ASIC, and FPGA for further testing. The authors conducted experiments on the sequential MNIST classification task using their alternating quantization method, achieving acceleration with 2-bit and 3-bit quantization. The method can be extended to GPU, ASIC, and FPGA for further testing. LSTM of size 128 with quantized inputs showed plausible performance in a classification task on MNIST. The alternating quantization method is suitable for RNNs and feed-forward neural networks, achieving low testing error rates. The authors achieved low testing error rates using their alternating quantization method on CIFAR-10 with 2-bit weight and 1-bit activation. They used ADAM with a decaying learning rate and Batch Normalization with a batch size of 100. Their method outperformed other multi-bit quantization methods, showing the lowest testing error. The experiments followed the same settings as previous work, using 45000 images for training, 5000 for validation, and 10000 for testing on a VGG-like architecture. The authors utilized a VGG-like architecture with specific layers and parameters, including batch normalization and ADAM optimization. They achieved the lowest testing error rates for 2-bit weight and 1-bit activation using their alternating quantization method on CIFAR-10."
}