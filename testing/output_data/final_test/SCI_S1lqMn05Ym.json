{
    "title": "S1lqMn05Ym",
    "content": "In this work, the authors explore leveraging repeated structure in tasks to enhance learning speed and regularization. They introduce a default policy component to the KL regularized expected reward objective, learning it from data while limiting the information it receives. This approach, inspired by information bottleneck methods and variational EM algorithm, accelerates learning and improves performance in certain tasks. The video demonstrates experts and default policies on continuous control tasks. Good policies show similar behaviors across different contexts, needing only slight modifications. This concept has been explored in various fields, emphasizing the cost of information processing and internal computational constraints. The behavior of an agent minimizes the need for information processing and trades off task reward with computational effort. This idea can be modeled using tools from information and probability theory, such as constraints on the channel capacity in a Markov decision process. The paper explores injecting subjective knowledge into the learning problem by using an informative default policy learned alongside the agent policy. The paper explores using an informative default policy alongside the agent policy to encourage sharing of behavior across different parts of the state space by introducing information asymmetry between the two policies. The paper proposes using an informative default policy to create information asymmetry with the agent policy, leading to faster learning on various tasks. Key concepts include using s t and a t to represent state and action at time step t, and discussing connections to information bottleneck and variational EM algorithm. The objective is to maximize reward while staying close to a reference behavior defined by a default policy, using KL regularization and a discount factor. The agent policy is parameterized by \u03b8 and learned through history-dependent policies. The goal is to speed up learning on tasks by creating information asymmetry between the agent policy and a default policy. The entropy regularized objective in reinforcement learning prevents policy collapse to a deterministic solution, encourages learning multiple solutions, and provides robustness to perturbations and model mismatch. It can be approximated by using history-dependent entropy as an additional loss in RL training. In this work, the authors explore the advantages of regularizing towards more sophisticated default policies in reinforcement learning. The objectives can be generalized beyond the typical Markov assumption in MDPs, introducing additional correlations among actions using latent variables. This can be useful in partially observed setups where policies depend on history. In this study, the focus is on learning the default policy itself and the subjective knowledge it brings to the learning system. The concept of default behavior, or \"goal-agnostic,\" is explored in scenarios like a locomotive body navigating based on body configuration and a 3D visual maze with forward motion as a typical action. The default policy, also known as \"goal-agnostic,\" is a function of a subset of the interaction history. By hiding information from the default policy, it learns the average behavior over histories with the same value of x D t, making it generally useful regardless of the current goal. The default policy, or \"goal-agnostic,\" is optimized through supervised learning on trajectories generated by a different policy. The optimal default policy can be derived in the nonparametric case, encouraging similar behavior in different parts of the state space. During optimization, the default policy acts as a shaping reward while the KL entropy discourages deterministic solutions. Information theoretic constraints in reinforcement learning objectives are motivated by agent's computational limitations. An information theoretic regularization term is added to the expected reward to control the flow of information between goal-directed history and actions. The KL regularized objective in reinforcement learning acts as a lower bound to control the flow of information between goal-directed history and actions. This implementation follows the information bottleneck principle and is similar to training variational autoencoders and learning latent variable models. The goal is to maximize the log marginal likelihood of observations. The goal is to maximize the log marginal likelihood log p \u03b8 pX q where p \u03b8 px, zqdz can be bounded from below by E q \u03c6 pz|xiq rlog p \u03b8 px i |zq\u00b4log q \u03c6 pz|xiq p \u03b8 pzq. This lower bound exhibits an information asymmetry between q and p, similar to the objective in reinforcement learning. In the multi-task case, maximizing the objective involves learning a generative model of behaviors that can explain solutions to different tasks. Optimizing policies \u03c0 0 and \u03c0 involves alternating gradient ascent, with different algorithms used depending on the environment. For continuous control domains, SVG(0) with experience replay and KL regularization is utilized, while batched actor-critic algorithms are used for discrete action spaces. The algorithm uses a batched actor-critic approach with a learned state-value function and K-step returns for updating the value function and computing policy gradients. It is implemented in a batched distributed manner with a single learner and multiple actors. The off-policy versions for continuous and discrete action spaces are detailed in the appendix. There are connections between reinforcement learning and probabilistic modeling literature. Maximum entropy reinforcement learning and stochastic optimal control are based on the observation that the reinforcement learning problem can be interpreted as variational inference in a probabilistic graphical model. These algorithms aim to maximize expected reward with entropy or KL regularization. Originally used in robotics and control, there is now growing interest in deep reinforcement learning. The family of expectation maximization policy search algorithms cast policy search as an alternating optimization problem similar to the EM algorithm for learning probabilistic models. The DISTRAL algorithm and the present paper take an intermediate position where the default policy is learned but remains regularized relative to the policy. The Divide and Conquer BID9 algorithm learns an ensemble of policies specialized to different contexts, regularized with a symmetric KL penalty. BID10 proposes an information bottleneck architecture for policies with latent variables, leading to a KL-regularized formulation. EM policy search and other algorithms include a KL constraint to control the rate of policy change. The algorithms for policy optimization often use trust region methods, where a KL regularized objective requires certain assumptions to hold. Ensuring critical points of the regularized objective align with the non-regularized objective can be challenging without a trust region around the parameter. Other works, like Deep Mutual Learning, also utilize KL-regularization in supervised learning. The curr_chunk discusses the use of a learned prior and EWC to address catastrophic forgetting in policy optimization. It also mentions the use of a KL-regularized objective to ensure policies explored in a curriculum stay close to each other. Additionally, it explores the concept of bounded rationality models and the effect of using a learned default policy to regularize agent behavior in experiments. The curr_chunk explores the impact of conditioning the default policy on different information sets in various environments. It includes experiments with different types of walkers and tasks, such as avoiding walls while running through a terrain. The task is specified to the agent either through additional feature vectors or visual input. The curr_chunk discusses different types of tasks in various environments, including sparse and dense reward tasks like walking, avoiding walls, and going to randomly sampled targets. The curr_chunk discusses tasks involving moving a box to targets and foraging in a maze with sparse-reward features. The experimental setup includes policies trained with entropy regularization and a distributed actor-learner architecture. Refer to appendix C for more details. The experimental setup involves policies trained with entropy regularization and a distributed actor-learner architecture. Different information sets are passed to the default policy, and results for sparse-reward tasks with complex walkers are presented. The default policy with limited task information significantly speeds up learning for sparse-reward tasks with complex walkers like quadrupeds and humanoids. Additional information only improves performance in a few cases, with proprioceptive default policy showing the most significant gains overall. The default policy speeds up learning for sparse-reward tasks with complex walkers like quadrupeds and humanoids. Additional information enhances performance in some cases, but not for dense-reward tasks or simple walkers. The absence of gain is attributed to the simplicity of regular policy learning compared to KL-regularized setup. In dense-reward tasks, the agent has a strong reward signal, while for simple walkers, the action space is too simple for sophisticated exploration. The default policy, when fully informed, would just mimic the agent policy without providing additional learning signal. This lack of generalization across contexts renders the default policy ineffective as a regularization signal. The agent behavior on the moving target task with a quadruped walker is analyzed. The trajectory of the agent for this task is illustrated, showing the locations of the targets and the performance of the transfer task. The default policy is discussed in terms of its effectiveness as a regularization signal. The study analyzes the agent behavior on a moving target task with a quadruped walker. The KL divergence is high near the target, indicating deviations from standard walking behavior. Reusing pretrained default policies is explored for regularization in learning new tasks. The study explores using pretrained default policies for regularization in learning new tasks, showing significant improvement in learning speed. Ablative analysis compares different forms of regularization in the standard RL objective. The study investigates using pretrained default policies for regularization in learning new tasks, demonstrating improved learning speed. Different forms of regularization in the standard RL objective are compared, with KL-based variants showing performance improvement over the baseline. Regularizing against the information-restricted default policy outperforms regularization against an older version of the policy. The benefit of the default policy depends on the reward structure, with dense shaping rewards showing improved performance. Our method and the baseline perform similarly in relation to the walker's distance to the target, consistent with dense-reward results. Assessing the benefit of KL-regularized objective with an idealized default policy, a significant difference is shown between baseline and regularization variants when using a pretrained default policy. This suggests that gains are possible when a good default policy is known beforehand. Regularizing to a pretrained expert shows a significant speed-up, primarily used as a method to regularize against an existing solution. The experiments showed that there was no significant difference between regularization schemes, suggesting that the idea of a learned default policy can be viewed from a student-teacher perspective. The default policy can play the role of a teacher and be used in a new task. The method was also evaluated on the DMLab-30 set of environments. Recent works on multitask training in rich, first-person environments have utilized batched-A2C with the V-trace algorithm to maximize an entropy regularized objective. The agent receives visual information and instructions in some tasks, without a task identifier. The architecture involves passing frames, actions, and rewards through a deep residual network and LSTM to predict a policy and value function. Experiments are tuned with population-based training in DMLab, which offers a large action space. In multitask training in rich, first-person environments, batched-A2C with the V-trace algorithm is used to maximize an entropy regularized objective. The agent operates in a large action space but typically uses a restricted subset of actions for training and testing. This subset includes a forward bias to aid in navigation tasks. However, using a pure uniform default policy on a larger combinatorial space of actions can lead to under-performance when human knowledge is not utilized to define the right subset of actions. Learning the default policy, even without state information, helps identify valuable actions and creates a useful action space without manual intervention. Experiments show results with a flat action space of 648 actions. Comparing a baseline with a uniform default policy to variants where the default policy is learned, different default policies are explored. The entropy and distribution over actions are analyzed for the learned default policy. The default policy, whether feed forward or LSTM-based, plays a crucial role in guiding the agent's actions. While the feed forward default policy is conditioned on the full trace of observed states, the LSTM default policy only considers the previous action. This limitation means the LSTM default policy can only predict the most likely actions based on recent behavior, such as moving forward if that has been the trend. This highlights the importance of learning the default policy to create a useful action space without manual intervention. The vector default policy, independent of actions and states, outperforms the baseline in navigating efficiently. The entropy of the default policy decreases over learning frames, indicating a peaky distribution different from the uniform baseline assumption. LSTM default policy slightly underperforms compared to others, emphasizing the importance of defining a meaningful action space for solving tasks in DMLab without human expert knowledge. The default policy in DMLab outperforms the baseline in navigation efficiency. The agent shows a tendency to move forward 70% of the time and has a bias towards turning right and looking right, aiding in exploration of new maps. This bias provides a meaningful exploration boost in each episode. In this study, the influence of learning the default policy in the KL-regularized RL objective was examined. Information asymmetry between the default policy and the main one was enforced, leading to a significant speed-up in learning for sparse-reward tasks with complex walkers in continuous control. However, no significant gains were observed in dense-reward tasks or with simple walkers. Significant improvements were also seen in discrete action spaces. In this study, significant gains were demonstrated in discrete action spaces due to information asymmetry between the agent and the default policy. Best results were achieved when the default policy had limited information, allowing for task-agnostic behavior. These default policies can be reused to speed up learning on new tasks. The appendix details the connection between KL-regularized RL and information bottleneck, focusing on maximizing reward while minimizing information between actions and state. Some notational inconsistencies will be fixed in a later draft. The study demonstrates gains in discrete action spaces due to information asymmetry with the default policy. The appendix discusses maximizing reward while minimizing information between actions and state, leading to a KL-regularized RL approach implementing an information bottleneck. Different forms of the default policy restrict information flow between components of the interaction history. The text discusses the flow of information between components of interaction history and different interpretations of the KL regularized objective in reinforcement learning. It also mentions a distributed off-policy setup with one learner and multiple actors. In a distributed off-policy setup, the main agent receives parameters from the learner and unrolls trajectories in the environment, saving them to a replay buffer. The learner samples batches of trajectories, calculates gradients, and updates parameters, which are then communicated to the actors. Comparing performance on a moving target task with 1 and 32 actors, it is observed that having only 1 actor leads to slower and weaker learning. Detailed descriptions of continuous control tasks, including a walking task, are provided. The walking task involves moving in a specified direction with a target speed, receiving rewards based on speed difference. The humanoid task requires running through a terrain and avoiding walls, with rewards based on speed deviation from target. Target speed for both tasks is 3. The humanoid task involves navigating to one of K single targets on an 8x8 floor within 45 seconds, with a reward of 60. The walker must reach the target specified by a command vector, and once achieved, the episode ends. Another task involves reaching one moving target within 20 seconds, with the target reappearing in a new random place after each successful reach. The walker must move a box to one of the specified targets on a 3x3 floor with walls. If successful, the walker receives a reward before the target reappears in a new random position. The episode length is 25 seconds, and the reward is denoted as r. Another task involves moving a box to one of the targets and then going to another target within 30 seconds. The control timestep varies for different scenarios. The walker must collect 8 apples in a maze, receiving a reward for each apple. The episode ends when all apples are collected or time runs out. The episode length is 90 seconds, with a control timestep of 0.025 for jumping ball and 0.05 for quadruped. Agents run in an off-policy regime, sampling trajectories from a replay buffer. Architecture details and baselines are provided. In discrete experiments, V-trace off-policy correction is used. Hyperparameters for DMLab are reused from a previous paper. The action space consists of rotations, moving, and \"fire\" actions, resulting in a dimension of 648. The agent network is divided into actor and critic networks without parameter sharing. The agent network is divided into actor and critic networks without parameter sharing. Task-specific information is encoded by one layer MLP for feature-based tasks and a 3-layer ResNet for vision tasks. The actor network encodes a Gaussian policy using a two-layer MLP, while the critic network is a two-layer MLP with a linear readout. The default policy network has the same structure as the actor network but receives a concatenation of proprioceptive information with only a subset of task-specific information. ELU activation is used throughout. The agent network consists of actor and critic networks without parameter sharing. ELU activation is used. The default policy network has the same structure as the actor network. LSTM was considered for the default policy network but did not show a difference. Separate optimizers and learning rates are used for each network. Target networks are updated at a slower rate than online networks. The Retrace operator is used to correct for being off-policy. Algorithm 2 is an off-policy version with retraced Q function of the initial algorithm 1. The baseline is the agent network without the default policy with an entropy bonus \u03bb. Hyperparameters such as batch size and unroll length are considered for each task. When using the default policy, a regularisation parameter \u03b1 is used instead of an entropy bonus. The value function V is learned using V-trace, and the policy is updated using an off-policy corrected policy. The default policy is used with a regularisation parameter \u03b1 instead of an entropy bonus in Algorithm 2. Results show little difference in performance compared to the baseline, possibly due to the agent being trained quickly with a strong reward signal. The default policy does not significantly impact the state-conditional action distribution in sparse reward tasks with jumping ball. In sparse reward tasks, the proprioceptive default policy outperforms others, especially as the number of targets increases. The default policy quickly learns walking behavior, aiding the agent in exploring and finding targets. The default policy is beneficial for exploring and searching for targets in tasks with multiple targets. Transfer experiment results show similar effects in tasks involving moving a box to different targets. In transfer experiments, default policies significantly improve performance in various tasks, such as going to a target and pushing a box. The default policy helps avoid being stuck in certain behaviors and speeds up learning compared to starting from scratch. In transfer experiments, default policies significantly improve performance in various tasks, such as going to a target and pushing a box. Ablations for the walls quadruped are shown in figure 14, specifying the task and additional information about the default policy. Results for different orders of the default policy in the KL-term for the go-to moving target task with quadruped walker are presented in FIG0. In transfer experiments, default policies significantly improve performance in various tasks, such as going to a target and pushing a box. Ablations for the walls quadruped are shown in figure 14, specifying the task and additional information about the default policy. Results for different orders of the default policy in the KL-term for the go-to moving target task with quadruped walker do not show significant differences."
}