{
    "title": "BJgAGp5qLH",
    "content": "In enterprise text analysis, there is a wealth of data for developing AI-powered experiences to enhance productivity. Privacy constraints limit broad sampling of personal data, but manual annotation on public datasets like enterprise email is possible. This paper explores transferring information between email datasets to predict user intent, addressing challenges and proposing methods to bridge the transfer gap. Further discussions on this topic are raised for consideration. Using publicly available text data to train predictive models for privacy-aware enterprise settings is a fruitful direction in document understanding. Domain adaptation techniques are necessary when the labeled training dataset differs from the unlabeled test dataset, as they likely follow different distributions. The effectiveness of current domain adaptation techniques for enterprise text data predictive settings has not been thoroughly explored. In enterprise text data, domain adaptation techniques are used to analyze communication intent prediction in emails from different companies. The focus is on Meeting Intent and Commitment Intent tasks as binary classification tasks. Measures are sought to quantify differences between source and target domains. The transfer gap in enterprise email communication is measured through intrinsic and extrinsic analyses to understand distributional differences and their impact on predictive models. Contributions include ways to measure the transfer gap over different representations and evidence of downstream measurable differences. The text discusses distributional differences in enterprise email communication datasets and evaluates transfer methods from the literature to enterprise settings. It analyzes differences in distribution across various representation choices, comparing frequent words and contexts in the positive class. The text discusses distributional differences in enterprise email communication datasets and evaluates transfer methods from the literature to enterprise settings. It analyzes the most frequent words in positive intent across different tasks and domains, highlighting significant differences in word usage between meeting and commitment intents. The text explores distributional differences in positive intent words across different tasks and domains, focusing on differences in word usage between meeting and commitment intents. It investigates whether the top-30 positive-intent associated words are used in different contexts by embedding each sentence using contextual word embeddings. The study compared distribution of contextual word embeddings in Avocado and Enron domains using Maximum Mean Discrepancy (MMD). Results showed differences in sentence-level structure representation across domains using a CNN. The study compared sentence-level structure representation across domains using a CNN encoder/classifier trained in each domain. Results showed a significant difference in the distribution of dense encodings between sentences within the same domain compared to across domains, indicating a transfer gap in predictive model performance. The transfer gap in predictive model performance can be observed in individual words, context distribution, and sentence encodings. An ideal transfer learning method would need to address these differences. The study analyzes how this difference affects the performance of text classification and domain adaptation methods. Two classes of methods are used: non-transfer text classification methods and methods that perform domain adaptation. The extrinsic transfer gap is measured by fixing the training set and training each model on it. The study analyzes the extrinsic transfer gap in predictive model performance for text classification and domain adaptation methods. AUC is used as the performance metric due to its class-skew invariance. Two domain adaptation methods are utilized, including an mSDA autoencoder. The study analyzes the extrinsic transfer gap in predictive model performance for text classification and domain adaptation methods. Two domain adaptation methods are utilized: an mSDA autoencoder and a domain adversarial deep learning method using a CNN architecture. Non-transfer baselines include L1-regularized logistic regression and a CNN with word embeddings as input. The study compares different domain adaptation methods for text classification, including an mSDA autoencoder and a CNN with word embeddings. Results show that Enron's language variability in commitments is smaller than that of Avocado. The study compares different domain adaptation methods for text classification, showing that Enron commitments have less variability than Avocado. Transfer methods generally improve over logistic regression baseline but not over their most similar no-transfer counterpart. The gains in out-of-domain performance are due to improved modeling, but the gap between in-domain and out-of-domain performance remains. Many transfer papers do not compute in-domain performance or compare on a skew-invariant metric."
}