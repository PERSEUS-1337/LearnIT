{
    "title": "BklKK1nEFH",
    "content": "Bijectors.jl is a software package in Julia for transforming distributions, allowing flexible and composable implementations without being tied to a specific computational framework. Bijectors.jl improves variational inference by encoding statistical dependencies into the variational posterior using normalizing flows, relaxing the mean-field assumption. Transforming probability distributions is common in Bayesian inference and probabilistic machine learning, such as in Hamiltonian Monte Carlo and constructing learnable densities known as normalizing flows. Bijectors.jl enhances variational inference by incorporating statistical dependencies through normalizing flows, moving away from the mean-field assumption. This involves transforming probability distributions using techniques like coupling flows, which have gained attention in learnable density models. Bijectors.jl is a framework in Julia for creating and using bijectors, such as coupling flows, which are used to transform probability distributions in variational inference. Coupling flows involve constructing parameters for a bijector from one part of an input vector and applying it to another part, making the transformation invertible. The parameter-map can be complex, allowing for seamless interaction between different types of bijectors. Bijectors.jl allows for seamless interaction between standard and learnable bijectors, simplifying implementations like automatic differentiation variational inference (ADVI). New bijectors only require manual implementation of b(x) and b \u22121 (y). Neural autoregressive flows (NAFs) extend inverse autoregressive flow (IAF) by replacing the affine coupling law with a deep neural network. The only difference between IAF and NAF in Bijectors.jl is the choice of Bijector as the coupling law. Comparisons with related work are summarized in Table 2. Bijectors.jl allows for seamless interaction between standard and learnable bijectors, simplifying implementations like automatic differentiation variational inference (ADVI). It extends inverse autoregressive flow (IAF) with neural autoregressive flows (NAFs) using deep neural networks as the coupling law. Comparisons with related work can be found in Table 2 and more detailed comparisons in Appendix A.1. Another transformer module based on PyTorch, pyro.distributions.transforms, has been released in Pyro. At a first glance, its features seem similar to tensorflow.probability. Bijectors.jl demonstrates using normalizing flows to relax the meanfield assumption in variational inference for a two-dimensional Gaussian with dependent components. The mean-field assumption is incorrect in this case, and a mean-field multivariate normal combined with coupling flows is proposed as a solution. The text discusses using coupling flows to encode the structure of a model into the variational posterior at a low computational cost. Coupling flows are applied to create directed mappings between random variables, using different coupling laws such as affine and rational-quadratic splines. The resulting density is optimized using ELBO to obtain variational posteriors. The text introduces Bijectors.jl, a framework for working with bijectors to transform distributions. It demonstrates the flexibility of Bijectors.jl in introducing correlation structure to the mean-field ADVI approach, capturing the correlation structure of the true posterior. This approach allows for variational inference without adding a large number of parameters. See Appendix B.1 for experiment details. The NF variational posterior constructed using Bijectors.jl requires only a constant number of extra parameters compared to mean-field normal VI. This approach scales linearly with the number of unique edges between random variables in a generative model, making it applicable in various settings. Additionally, it can be extended to models represented by undirected graphs by adding couplings in both directions, offering a promising solution to challenges faced in mean-field VI. The text discusses comparisons between different frameworks for transformations in probabilistic programming, highlighting limitations in integration between different types of transformations. TensorFlow Probability's bijectors framework is noted for its comprehensive approach. The text discusses the first work to separate determinism and stochasticity, leading to the implementation of standard distributions as a TransformedDistribution. The authors of Bijectors.jl were motivated to create a similar framework in Julia. Julia's language is well-suited for innovation in implementation, such as code-generation and meta-programming for program transformations. The Bijectors.jl framework in Julia allows for the construction of new bijectors using higher-order bijectors like Inverse, Compose, and Stacked. Constructors are intuitively called, with compositions like b \u2022 inv(b) resulting in Identity() at compile time. Bijectors.jl in Julia allows for the construction of new bijectors using higher-order bijectors like Stacked, which specifies parts of the input vector to be passed to different bijectors. The abstraction of Stacked incurs zero-cost overhead as the loop for iterating through ranges and applying bijectors is unrolled at compile-time. Bijectors.jl can perform program transformations, such as simplifying b \u2022 b \u22121 to the identity mapping. In TensorFlow Probability, this simplification is achieved through caching, reducing computational work. Bijectors.jl in Julia allows for constructing new bijectors using higher-order bijectors like Stacked, with zero-cost overhead. The approach in TensorFlow and PyTorch uses caching for efficiency, but in Bijectors.jl, manual implementation is required through the forward method for a TransformedDistribution. This method returns a 4-tuple for efficient computation. In Bijectors.jl, manual caching is required for efficiency in computation using the forward method for a TransformedDistribution, returning a 4-tuple. In an experimental setup, data is generated with specific parameters to obtain a posterior multivariate normal distribution. In an experimental setup, data is generated with specific parameters to obtain a posterior multivariate normal distribution. Variational inference was performed on the model resulting in Figure 5 (c), with densities shown in Figure 3. The model used a coupling flow with a rational-quadratic spline and neural network for optimization. The KL-divergence and ELBO expressions are maximized with respect to parameters of \u03b8. The optimization process used DecayedADAGrad with specific settings and 5,000 optimization steps. The true posterior is not accessible for direct minimization of KL-divergence with the variational posterior. In practice, minimizing the KL-divergence directly in the mean-field case did not provide much gain compared to maximizing the ELBO. The NFVI approach consistently achieved lower KL divergence and a higher ELBO, indicating the effectiveness of the variational inference method. The main focus is on the KL-divergence and ELBO in variational inference. The KL-divergence quantifies the difference between variational and true posterior, while the ELBO is a lower bound on evidence. Visualizing KL-divergence is preferred over ELBO, with numerical values provided in Table 3. After 1000 steps, the optima are reached in the optimization process. After 1000 steps, the optima are reached in the optimization process using the ELBO as the objective. An affine transformation was also tested as a coupling law, resulting in good approximations. The neural network setup consisted of two layers with specific weight matrices and activations. The results are shown in Table 4 and Figure 4, demonstrating the effectiveness of the affine coupling law. After reaching the optima in the optimization process using ELBO as the objective, the KL-divergence can be minimized by maximizing the evidence lower bound (ELBO). If the variational posterior q(z) is a transformed distribution with base density q0 and transformation b, the ELBO expression becomes useful. When q0 is a computationally cheap density to sample from and has an analytical entropy expression, terms are incorporated into the ELBO. A Monte-Carlo estimate of the ELBO is used in practice, where \u03b7k \u223c q0(\u03b7) for k = 1, . . . , m, allowing for a gradient estimate wrt. parameters."
}