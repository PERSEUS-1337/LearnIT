{
    "title": "H1gJ2RVFPH",
    "content": "Theoretical analysis of approximate Gaussian posterior distributions on the weights of ReLU networks shows that even a simple non-Bayesian Gaussian distribution can fix the issue of high confidence far from training data. Employing a Bayesian method improves confidence calibration, motivating a range of Laplace approximations. Theoretical analysis shows that Laplace approximations can improve confidence calibration in neural networks, especially for test cases far from training data. Various Bayesian and non-Bayesian methods have been proposed for this goal in AI safety. ReLU networks, a widely used neural architecture, consist of linear layers and ReLU activation functions. However, their predictions have been shown to be miscalibrated, leading to overconfidence in out-of-distribution examples. Bayesian methods have been known to improve predictive uncertainty calibration. Bayesian methods have been shown to improve predictive uncertainty calibration in neural networks. Equipping ReLU networks with Gaussian probability distributions can mitigate the overconfidence issue without changing the decision boundary. Equipping ReLU networks with Gaussian probability distributions can mitigate overconfidence without altering the decision boundary. Various Gaussian approximations at the last layer of the network calibrate uncertainty, particularly near the data, and can mitigate asymptotic overconfidence. Mitigating overconfidence in deep learning models can be achieved by using Gaussian distributions on the weight space, with different levels of fidelity towards full Bayesian inference. Empirical validation shows that even a simple Laplace approximation to the last layer of deep ReLU architectures can compete with non-Bayesian methods designed to address overconfidence issues. Methods are developed to address the overconfidence problem in ReLU networks, with a focus on multi-class classification. The function f is considered piecewise affine if it consists of linear regions. ReLU networks create piecewise affine classifier functions using various layers and activation functions. In ReLU networks, activation functions and pooling layers are used for convolution. The logistic and softmax functions are defined for probability distributions over weight vectors. Posterior distributions arise from Bayes' theorem. Predictive distributions are considered for binary and multi-class cases. Eigenvalues are used for matrices. ReLU networks exhibit high confidence far from training data by scaling input points. Piecewise affine representation of output is used, showing that for almost any input, there exists a class with high confidence. Standard deep training treats neural networks as probabilistic models, but ignores uncertainty on weights. The lack of uncertainty in the Maximum a posteriori (MAP) value of weights leads to overconfidence in predictions. Approximations for the logistic link function exist when weights follow a Gaussian distribution. The softened version of the MAP prediction incorporates the covariance of the Gaussian distribution. The approximate probabilistic Gaussian formalism introduces a second set of parameters in the form of \u03a3, which can fix overconfidence problems by setting it to any sensible value. This approximation preserves the decision boundary induced by the MAP estimate, even when using different methods for uncertainty quantification. This property is important as it guarantees the application of this approximation to the last layer of any MAP pre-trained neural networks. The marginalized prediction in the last layer of MAP pre-trained neural networks has the same accuracy as the MAP classification. A Gaussian approximation of the posterior can be obtained by setting parameters based on the inverse Hessian. The binary classification assumes a Bernoulli distribution for p(y|x, w) and a Gaussian distribution for p(w). The theoretical contribution shows that z(x) converges to a value dependent on the mean and covariance of the Gaussian weights distribution. Theoretical analysis demonstrates that controlling the Gaussian weights distribution can bring the prediction closer to one-half away from training points for linear classifiers and ReLU networks. The input space is defined as a union of affine functions, allowing for a concise representation of the model. The distribution over weights in a ReLU network can be controlled to ensure predictions are close to one-half away from training points. The input space is represented by a union of affine functions. The covariance matrix \u03a3 can be chosen to respect upper bounds on logit and confidence values, ensuring that predictions remain within desired bounds. In the Bayesian setting, the confidence of a binary linear classifier with ReLU features can be controlled by increasing the minimum eigenvalue of the posterior covariance. This can be achieved through a Laplace approximation, where increasing the prior variance leads to higher eigenvalues of the posterior covariance, bringing predictions closer to zero. In a Bayesian setting, the Laplace approximation can control the confidence of a binary linear classifier with ReLU features by increasing the minimum eigenvalue of the posterior covariance. The eigenvalues of the covariance matrix are related to the prior variance, affecting the predictions' proximity to zero. The Hessian of the negative log-likelihood influences the eigenvalues of the covariance matrix, leading to high uncertainty away from training data. The Laplace approximation in a Bayesian setting controls confidence of a binary linear classifier with ReLU features by adjusting the spectral properties of the Hessian. Confidence decreases faster in sparser data regions in the feature space. Multi-class classifiers pose challenges due to lack of closed-form approximations, but the analysis can be extended to this case. In experiments, multi-class classifiers are effective in addressing the overconfidence problem in deep neural networks. Various methods have been proposed to combat this issue, including enhanced training objectives based on robust optimization. Bayesian methods have also been considered to mitigate overconfidence in neural networks. Our theoretical work provides a justification for addressing the overconfidence problem in neural networks, specifically in the ReLU-logistic case. We propose a practical method called last-layer Laplace approximation (LLLA) to validate our results, which offers a simpler alternative to full Bayesian treatments. Our results on Laplace approximation for deep networks apply to more general Laplace methods like Kronecker-factored Laplace. Empirical results are presented on a 2D toy classification task and out-of-distribution data detection experiments. The optimal prior variance is determined via a heuristic method without seeing any of the OOD datasets. The dataset is constructed by sampling input points from k Gaussians, with corresponding targets indicating the source Gaussian. A 5-layer ReLU network is used for feature mapping, with results showing high confidence in MAP predictions except near decision boundaries. MC-dropout does not address this issue, while ACET is costly and does not preserve boundaries. LLLA provides better calibrated predictions with high confidence near training points and uncertainty elsewhere, maintaining the MAP's decision boundary. LLLA yields competitive performance compared to both CEDA and ACET, providing better calibrated predictions with high confidence near training points and uncertainty elsewhere, while maintaining the MAP's decision boundary. The covariance acts as a \"moderator\" for the MAP predictions, mitigating overconfidence in ReLU networks. Analytical results bound the confidence of Bayesian prediction of linear classifiers and ReLU networks far away from the training data, motivating a spectrum of approximations. In contrast to other approximations, a simple Laplace method can mitigate overconfidence in ReLU networks, providing competitive performance compared to more expensive non-Bayesian methods. The method approximates the last-layer's posterior distribution, offering a computationally lightweight solution. The text discusses properties of matrices and vectors in relation to binary linear classifiers. It also touches on the distribution over weights in the classifier. The results are derived using mathematical proofs and computations. The text discusses the asymptotic behavior of z(\u03b4x) with respect to \u03b4, proving the case when \u03b4x goes to the decision boundary. It also introduces linear regions associated with a ReLU network and the properties of a binary linear classifier defined by a ReLU network. The text discusses the asymptotic behavior of ReLU networks and their linear regions, proving results related to decision boundaries and upper confidence bounds. The text discusses the asymptotic behavior of ReLU networks and their linear regions, proving results related to decision boundaries and upper confidence bounds. A ReLU network defined by f \u2022 \u03c6(x) := w T \u03c6(x) and N (w|\u00b5, \u03a3) be the distribution over w where the mean \u00b5 is fixed and \u03a3 is any SPD matrix. For any > 0 there exists \u03a3 such that for any x \u2208 R n far away from the training data, |z \u2022 \u03c6(x)| \u2264 . For any 0.5 < p < 1 there exists \u03a3 such that for any x \u2208 R n far away from the training data, \u03c3(|z \u2022 \u03c6(x)|) \u2264 p. The text discusses the asymptotic behavior of ReLU networks and their linear regions, proving results related to decision boundaries and upper confidence bounds. The posterior over w is obtained via a Laplace approximation with a prior N(w|0, \u03c3^2_0 I). The Hessian w.r.t. w at \u00b5 of the negative log-likelihood of the model is denoted as H. The eigenvalues of \u03a3 are non-decreasing functions of \u03c3. The negative log-likelihood of the Bernoulli distribution is considered, and the posterior covariance \u03a3 is discussed. The text discusses the Laplace approximation for the posterior covariance \u03a3 of a Gaussian distribution over w. It presents theoretical results on using the Laplace method to address overconfidence in predictions on ReLU networks. In this section, a Laplace method is described for capturing properties of ReLU networks trained via MAP estimation. The method applies Laplace approximation to the linear last layer of the networks for binary classification, obtaining the posterior of the weight of the linear classifier. In the context of ReLU networks trained via MAP estimation, a Laplace method is used to approximate the posterior of the weight of the linear classifier at the last layer. This method can be extended to multi-class classification by obtaining the posterior over a random matrix W. The inversion of the Hessian matrix is rarely a problem, even for large models like DenseNet-201 and ResNet-152. In the multiclass case, the posterior is approximated as a matrix Gaussian distribution using the Kronecker-factored Laplace approximation. This method is applied to the last layer of the network by finding the Kronecker factorization of the Hessian. The distribution of the latent functions is Gaussian, and the posterior can be obtained as MN(W|W MAP, U, V). The integral can be approximated via MC-integration, assuming the bias trick is already used. In practice, bias b is assumed to be independent of weight w in pre-trained networks. Laplace approximation is used to obtain Gaussian distributions for w T \u03c6(x)+b = f and W\u03c6(x) + b = f. The method of last layer Laplace approximation (LLLA) is applied by using Laplace approximation on the last layer of a ReLU network. Pseudocodes for LLLA are presented in Algorithms 1 and 2. We train networks in Table 1 for 100 epochs with batch size of 128, using ADAM and SGD with 0.9 momentum. Initial learning rates are 0.001 for MNIST and 0.1 for CIFAR-10, divided by 10 at epoch 50, 75, and 95. Standard data augmentations are used for CIFAR-10. LLLA utilizes the Kronecker-factored Hessian for multi-class classification, comparing OOD detection performance to temperature scaling method. The optimal temperature for LLLA is found using the method of Guo et al. (2017) and implemented from https://github.com/JonathanWenger/pycalib."
}