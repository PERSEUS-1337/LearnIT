{
    "title": "rJePwB8prH",
    "content": "In this work, six unsupervised disentanglement approaches were evaluated on the mpi3d toy dataset for the NeurIPS 2019 Disentanglement Challenge. The methods included Beta-VAE, Factor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE, and Beta-TCVAE. Beta-TCVAE was found to outperform the other methods based on five disentanglement metrics. The study evaluates various disentanglement methods using established metrics on mpi3d toy dataset for NeurIPS 2019 Disentanglement Challenge. The correlation between reported metrics and disentanglement potential is inconsistent. In a preliminary study for the NeurIPS 2019 Disentanglement Challenge, an autoencoder model was pre-trained with the VAE objective on the mpi3d toy dataset to prevent model collapse. Various variational objective functions were implemented, including \u03b2-VAE, \u03b2-TCVAE, Factor-VAE, Info-VAE, DIP-I-VAE, and DIP-II-VAE. \u03b2-TCVAE maximizes mutual information between data and latent variables while minimizing mutual information between latent variables. The ELBO of this objective is simplified as shown. The ELBO of the VAE objective includes terms for reconstruction loss, distance between prior and posterior latent distributions, and total correlation between latent variables. Increasing information capacity during training can improve learning of factorized latents. The capacity term C is introduced to facilitate learning of factorized latents in the VAE model. Gradually increasing C improves reconstruction capacity. Hyper-parameters were independently searched for each learning algorithm. Input images were 64 \u00d7 64 pixels, latent space size was 20. C was increased from 0 to 25 over 2000 iterations. Learning rate was adjusted based on loss function performance. Batch size was 64 for optimization. The trained models were evaluated based on five metrics: DCI, FactorVAE, IRS, MIG, and SAP-Score. Results are presented in TAB0. Latent variables were traversed and visualized. Models were implemented using PyTorch and compared for disentanglement in latent encodings. The study evaluated variational learning algorithms including \u03b2-VAE, Factor-VAE, DIP-I-VAE, DIP-II-VAE, Info-VAE, and \u03b2-TCVAE. Results show that \u03b2-TCVAE performed marginally better but none of the models achieved true disentanglement. Despite underperforming quantitatively, DIP-VAE-II showed promise in latent traversal. The study found the evaluation metrics inadequate and highlighted limitations in hyper-parameter search. The NeurIPS 2019 Disentanglement Challenge imposed an 8-hour training time limit on models, which was found insufficient. The encoder neural network had 5 convolutional layers with increasing kernel sizes. The decoder network had 6 deconvolutional layers with decreasing kernel sizes. The repository will be publicly released after the competition. The architecture used ReLU activations and had a decreasing number of channels from 256 to match the image space."
}