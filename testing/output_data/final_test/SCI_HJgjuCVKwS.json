{
    "title": "HJgjuCVKwS",
    "content": "Object recognition in real-world requires handling long-tailed or open-ended data. The paper investigates generalized few-shot learning (GFSL) where a model learns to classify both \"head\" and \"tail\" categories with few shots. The proposed Classifier Synthesis Learning (CASTLE) framework synthesizes calibrated few-shot classifiers alongside multi-class classifiers for \"head\" classes, using a shared neural dictionary. CASTLE optimizes GFSL learning objective, outperforming existing algorithms on MiniImageNet and TieredImageNet datasets. It addresses the challenge of recognizing objects in the \"long tail\" with limited data, surpassing previous state-of-the-art methods in few-shot learning. Few-shot learning focuses on limited instances per concept, distinguishing between data-rich \"head\" and data-scarce \"tail\" categories. It leverages data from SEEN classes to acquire effective classifiers for UNSEEN classes. Generalized Few-Shot Learning (GFSL) aims to bridge the gap between many-shot and few-shot learning for recognizing all object categories simultaneously. Generalized Few-Shot Learning (GFSL) focuses on joint classification of data-rich and data-poor categories. The model trained on SEEN categories must incorporate limited UNSEEN class instances for predictions in both \"head\" and \"tail\" categories. Unlike prior works, GFSL requires inductive modeling of the \"tail\" without prior knowledge of UNSEEN categories. Classifier Synthesis Learning (CASTLE) is proposed for Generalized Few-Shot Learning (GFSL) to create few-shot classifiers based on a shared neural dictionary across classes. These classifiers are used alongside many-shot classifiers to optimize multi-class classification. The goal is to have few-shot classifiers perform well on new classes while being competitive with many-shot classifiers on existing classes. The proposed approach, CASTLE, aims to improve few-shot classification by enhancing discernibility in visual embeddings. It outperforms existing methods on few-shot learning and generalized few-shot learning, showcasing better calibration between many-shot SEEN and synthesized UNSEEN classifiers. The approach is validated on MiniImageNet and TieredImageNet datasets, demonstrating competitive concept recognition performances. Few-shot learning involves training a classification model on a support set and optimizing the loss function to minimize the discrepancy between predictions and true labels. The model is often instantiated as an embedding function and a linear classifier. Few-shot learning faces challenges in transferring knowledge across visual concepts and assumes two sets of SEEN and UNSEEN classes. During training, it learns an inductive bias from SEEN classes to improve classifier performance. Generalized Few-Shot Learning (GFSL) aims to predict over both SEEN and UNSEEN categories simultaneously. Meta-learning has been effective for Few-Shot Learning by optimizing a shared function across tasks from SEEN class sets. In Generalized Few-Shot Learning, a few-shot classifier is trained on SEEN classes and evaluated on UNSEEN classes. The classifier is based on an embedding function that maps input examples to a latent space, aiming to pull similar objects close and push dissimilar ones far away. The prediction is made using a soft nearest neighbor classifier. The main idea of CASTLE is to synthesize classifiers for few-shot tasks from UNSEEN classes using a classifier composition model. This model learns many-shot classifiers and few-shot classifiers simultaneously, distilling important visual features for few-shot classification. The learning algorithm contrasts many-shot classifiers with few-shot classifiers by constructing classification tasks over a common set of neural bases. The classifier composition model in CASTLE synthesizes classifiers for few-shot tasks from UNSEEN classes by competing few-shot classifiers against many-shot classifiers. It uses a neural dictionary with learnable bases to encode shared primitives for composing classifiers of S \u222a U. The classifier composition model in CASTLE synthesizes classifiers for few-shot tasks by computing class signatures and coefficients to assemble classifiers using key and value embeddings from a neural dictionary. The classifier is then normalized and used for few-shot classification. The generalized few-shot learning approach aims to transfer knowledge from seen to unseen classes by generating compact latent features using neural bases. The GFSL classifier should perform well with both few-shot and many-shot classifiers, minimizing expected error in predicting test instances from both tail and head classes. The GFSL classifier generalizes its joint prediction ability to S \u222a U given D U train and \u0398 S during inference by training on the SEEN class set S and synthesizing classifiers for few-shot classes C while using many-shot classifiers for the remaining classes in S. The learning objective optimizes joint classifiers for all classes in S, including few-shot classifiers W C trained on K instances. The model minimizes loss over multiple GFSL tasks to extend discerning ability to UNSEEN classes with low error. During inference, CASTLE synthesizes classifiers for UNSEEN classes using a neural dictionary and makes joint predictions over S \u222a U with the help of many-shot classifier \u0398 S. Stochastic gradient descent is used to minimize the learning objective by sampling GFSL tasks in mini-batches. In this section, an efficient implementation utilizing a large number of GFSL tasks to compute gradients is proposed. The experiments validate the effectiveness of CASTLE in GFSL, comparing it with existing methods and analyzing algorithms with alternative protocols. The significant speed-up in convergence is observed, and multi-classifier learning is always used unless explicitly mentioned. In this study, CASTLE is shown to improve calibration between SEEN and UNSEEN classifiers, benefiting standard FSL performances. Two benchmark datasets from ILSVRC-12 are considered, with evaluations following specific splits. Various strong choices for deriving classifiers for SEEN and UNSEEN classes are explored, including a Multiclass Classifier (MC) + kNN approach. In this study, different approaches are explored for deriving classifiers for SEEN and UNSEEN classes in few-shot learning tasks. These approaches include ProtoNet + ProtoNet, MC + ProtoNet, and a combination of MC classifier and feature embedding. The goal is to improve calibration between SEEN and UNSEEN classifiers, benefiting standard FSL performances. For evaluation, we compare CASTLE with existing methods like L2ML and DFSL. CASTLE uses multiclass classifiers for SEEN classes and synthesized classifiers for UNSEEN classes to classify instances. Mean accuracy over SEEN and sampled UNSEEN classes is the main evaluation metric. 10,000 tasks are sampled for reliability, with test instances from head and tail categories used. In the evaluation, head and tail categories are used, with mean accuracy and 95% confidence interval reported. \u2206-value measures the average accuracy drop between predicting specific classes and all categories jointly. CASTLE outperforms existing methods on miniImageNet in mean accuracy and is least affected by \u2206-value. Additional GFSL measures include harmonic mean accuracy and AUSUC. Refer to the Appendix for experimental setups and evaluation measures details. When evaluating GFSL algorithms, mean accuracy and \u2206-value may not be sufficient indicators of performance. A proposed measure is the harmonic mean of mean accuracy for SEEN and UNSEEN categories when classified jointly, which is a better performance measure. Directly computing mean accuracy over all classes can be biased due to unequal class distribution. For example, a many-shot classifier focusing only on SEEN classes may outperform one that considers all classes. When evaluating GFSL algorithms, mean accuracy and \u2206-value may not be sufficient indicators of performance. The harmonic mean of mean accuracy for SEEN and UNSEEN categories when classified jointly is proposed as a better performance measure. Results show that classifiers focusing only on SEEN classes perform better than those considering both SEEN and UNSEEN classes. The top-1 accuracy for SEEN and UNSEEN classes is computed, and their harmonic mean is used as the performance measure. CASTLE outperforms others in this evaluation. Evaluating GFSL beyond 5 UNSEEN categories is also important. In evaluating GFSL, it is important to consider a large number of UNSEEN classes. CASTLE outperforms other approaches in UNSEEN and ALL categories on MiniImageNet and TieredImageNet datasets. It shows balanced confidence scores for SEEN and UNSEEN predictions without explicit calibration. In GFSL, CASTLE consistently outperforms other approaches in UNSEEN categories without explicit calibration. Calibration factor is computed to address prediction bias between SEEN and UNSEEN classifiers, resulting in balanced confidence scores. After calibration, CASTLE shows consistent improvement in accuracy compared to other methods in GFSL. The use of AUSUC as a performance measure accounts for calibration effects by adjusting confidence scores. Varying the calibration factor affects joint prediction performances across SEEN and UNSEEN categories. CASTLE outperforms other algorithms in predicting UNSEEN categories, as shown in Figure 3. It also excels in GFSL performance, especially in 1-shot learning, as demonstrated in Figure 4. The algorithm's robust evaluation includes studying harmonic mean accuracy with incremental UNSEEN \"tail\" concepts. Overall, CASTLE consistently outperforms baselines in various scenarios. The proposed approach outperforms previous state-of-the-art methods in both 1-shot 5-way and 5-shot 5-way accuracy on miniImageNet and TieredImageNet datasets. The results are presented in Table 4 and Table 5, supporting the hypothesis that jointly learning with many-shot classification enhances few-shot classifiers' discriminative abilities. Refer to the Appendix for more details on task setups, performance measures, and visualizations. Few-shot learning (FSL) addresses the challenge of training visual systems with limited data instances for rare or new categories. It involves meta-learning an inductive bias from known classes to aid in learning unseen classes with few training examples during deployment. This approach aims to prevent overfitting and bias towards data-rich categories. Few-shot learning (FSL) utilizes discriminative feature embeddings and nearest neighbor classifiers to recognize novel classes with few exemplars. Another approach involves learning a common initialization and adapting rapidly using gradient descents over few-shot training data from unseen categories. Low-shot learning has also been studied to recognize the entire set of categories. In few-shot learning, the goal is to recognize novel classes with few exemplars. Unlike previous approaches, the proposed method requires inductive transfer of knowledge from seen to unseen classes during evaluation. Various methods have been explored, including exemplar-based classification and learning separate classifiers for seen and unseen categories. CASTLE differs from related works by using a learned neural dictionary to compose classifiers and optimize a unified objective function. It outperforms existing methods in generalized few-shot learning and improves classifier discernibility in standard few-shot learning. In this study, a residual network (ResNet) is used as the embedding backbone \u03c6 following recent methods. The backbone network is pre-trained and model selection is performed. Momentum SGD with an initial learning rate of 1e-4 is used for training. Two benchmark datasets, MiniImageNet and ILSVRC-12, are utilized with specific class splits for meta-training, validation, and meta-test. The TieredImageNet dataset is a more complex version of MiniImageNet, with 34 super-categories and a total of 351 classes for meta-training. It presents a more challenging few-shot classification problem due to the divergence of super-concepts. The data sets are constructed using images from ILSVRC-12, with an augmented meta-train set for each data set. The auxiliary meta-train set is used to measure few-shot learning classification performance on the SEEN class set. Additional non-overlapping augmented class instances are collected from ImageNet to measure classification ability on seen classes. Few-shot instances are sampled from unseen classes for the generalized few-shot classification task. The model for few-shot classification tasks uses a residual network as the embedding backbone. Input images are resized to 80x80x3 for MiniImageNet and 84x84x3 for TieredImageNet. The network architecture includes three residual blocks with channels 160/320/640, leading to a 640-dimensional embedding. The model for few-shot classification tasks utilizes a residual network with a 640-dimensional embedding. The architecture is visualized in Figure A15 and references for building blocks can be found in Pytorch documentation. Prior to meta-training, a linear layer is added to the backbone output for initialization. Optimization is done through a 64-way classification problem on MiniImageNet (351-way for TieredImageNet) using cross-entropy loss with SGD. Model selection is based on 16 classes in MiniImageNet (97 classes in TieredImageNet) and the nearest neighbor few-shot classification performance is measured after each epoch. The most suitable embedding function is then recorded. The most suitable embedding function is recorded and used to initialize the embedding part of the model for CASTLE and comparison methods. The pre-trained backbone is utilized for initialization, with the initial learning rate set at 1e-4 and optimized with Momentum SGD. The learning rate is halved after 2,000 mini-batches. All methods are optimized over 5-way few-shot tasks during meta-learning. For CASTLE, a 24-way task is randomly sampled from S in each mini-batch, with 64 5-way tasks resampled from it. Instances in the 24-way task are encoded by the ResNet backbone. Synthesized 5-way few-shot classifiers are embedded into the global many-shot classifier, resulting in 64 different configurations. Evaluation involves randomly sampling instances from S with a batch size of 128 and computing the GFSL objective. Training and evaluation setups for generalized few-shot learning are detailed, along with descriptions of comparison methods. During training, ResNet backbone network is used for populated SEEN classes. |S|-way classifiers are trained in a supervised manner. Inference involves evaluating test examples of S categories with |S|-way classifiers and U categories with support embeddings from D U train. Generalized few-shot classification task is evaluated using multi-class classifiers' confidence and ProtoNet confidence. Few-shot classifier is trained using Prototypical Network with the same ResNet backbone network. Class prototypes of SEEN classes are computed during inference. During inference, class prototypes of SEEN classes are computed using 100 training instances per category, while prototypes of UNSEEN classes are based on a few-shot training set. The model combines learning objectives for many-shot and few-shot classification, allowing it to be used as a multi-class linear classifier on \"head\" categories and as ProtoNet on \"tail\" categories. The confidence of test instances is determined by their distance to both SEEN and UNSEEN class prototypes. During inference, the model predicts instances from SEEN class S with the MC classifier and uses few-shot prototypes to discern UNSEEN class instances. A harmonic mean based GFSL evaluation is conducted by computing prediction results on the joint label space of SEEN and UNSEEN instances separately. The evaluation of the generalized few-shot classification task involves taking the union of multi-class classifiers' confidence and ProtoNet confidence as joint classification scores on S \u222a U. Wang et al. (2017) propose learning to model the \"tail\" (L2ML) by connecting a few-shot classifier with the corresponding setup. In the experiment, the method of learning to model the \"tail\" (L2ML) connects few-shot and many-shot classifiers to learn classifier dynamics from data-poor \"tail\" classes to data-rich \"head\" classes. The approach involves training a many-shot classifier on SEEN classes and adapting it to UNSEEN classes. Classifier mapping is implemented using a ResNet backbone and a few-shot classifier is transformed into a many-shot classifier during meta-learning. Dynamic Few-Shot Learning without forgetting (DFSL) adopts a generalized few-shot learning objective, decomposing the learning into two stages. The method involves sampling a S-way few-shot task in each mini-batch to produce a linear few-shot classifier based on fixed pre-trained embeddings. The objective of L2ML is to regress the few-shot classifier close to the many-shot classifier while minimizing classification loss over randomly sampled instances. This approach uses a pre-trained multi-class classifier for \"head\" categories and predicted few-shot classifiers for \"tail\" categories. Another attention-based network constructs a classifier for a specific class by combining elements in the bases linearly. During training and inference, the DFSL model follows the strategy in (Gidaris & Komodakis, 2018) and utilizes a pre-trained backbone and cosine classifier to create a dictionary with size |S|. In each mini-batch of meta-training, a few-shot task is sampled from the SEEN class set, with an attention model composing the classifier by weighting elements in the dictionary not corresponding to the task's classes. For evaluation, DFSL samples instances from both the task's classes and the remaining classes in the dictionary. During inference, the cosine classifier is used for \"head\" classes, while the composed few-shot classifier is used for \"tail\" classes. Additionally, the auxiliary meta-train set from benchmark datasets is utilized during GFSL evaluations. During GFSL evaluations, the model's performance is measured based on many-shot accuracy and few-shot accuracy. Many-shot accuracy involves predicting SEEN class instances, while few-shot accuracy samples K-shot N-way tasks from unseen classes during inference. The model utilizes an attention-based network and a pre-trained backbone for training and inference, with the auxiliary meta-train set from benchmark datasets being used for evaluations. During few-shot evaluations, N classes are sampled from U, with K labeled instances used to build the classifier and 15N instances for evaluation. The test includes K = 1 and K = 5, varying N from {5, 10, 15, ..., |U|}. Generalized few-shot accuracy considers instances from S \u222a U and predicted labels in S \u222a U. In generalized few-shot learning (GFSL), the task is more challenging compared to many-shot and few-shot scenarios. During testing, K-shot N-way tasks are sampled from S \u222a U, with labeled instances from U and test instances from S. The accuracy drop in a GFSL model is measured using the \u2206-Value, which calculates the average accuracy drop in classification ability. In GFSL, accuracy is measured by computing many-shot and few-shot accuracy for SEEN and UNSEEN instances in the joint label space. The \u2206-Value represents the average decrease in accuracy. The generalized few-shot harmonic mean is used to balance performance measurement, considering top-1 accuracy and Mean Average Precision. A calibration-agnostic criterion is proposed for generalized zero-shot learning to avoid model evaluation influenced by calibration factors between SEEN and UNSEEN classes. The proposed approach aims to determine the calibration factor range for all instances and plot the SEEN-UNSEEN accuracy curve based on different calibration values. The AUSUC value is computed for sampled GFSL tasks, with ablation studies conducted on the CASTLE approach. Effects on neural dictionary size are analyzed for generalized few-shot learning. The neural dictionary size of 128 works best among all sizes tested. Comparing synthesized classifiers to multi-class classifiers, the synthesized classifier outperforms ProtoNet by a large margin. The model trained with unified learning objective (ULO) outperforms ProtoNet by a large margin. Classifier synthesis is effective against using sole instance embeddings but still lags behind multi-class classifiers trained on the entire data set. Different strategies for classifier synthesis are explored, including averaging embeddings and synthesizing classifiers based on each instance. The model trained with unified learning objective (ULO) outperforms ProtoNet by a large margin. Classifier synthesis strategies are explored, with \"Pre-AVG\" and \"Post-AVG\" options for averaging synthesized classifiers. \"Post-AVG\" does not significantly improve FSL and GFSL performance. The choice of \"Pre-AVG\" is preferred due to memory efficiency during meta-training. Additional five-shot learning results for generalized few-shot learning are provided for incremental evaluation. In generalized few-shot learning, CASTLE consistently outperforms baseline approaches across different evaluation setups. It adopts a multi-classifier training strategy, showing the impact on FSL and GFSL performance. Multiple classifiers are considered in a single mini-batch to improve classification results. The multi-classifier training method in CASTLE improves FSL and GFSL performance, converging faster and generalizing better than the vanilla model. Results on MiniImageNet show effectiveness with different numbers of classifiers, enhancing top-1 accuracy in 1-Shot GFSL tasks. The GFSL task involves varying numbers of shots from 5 to 20 and calculates sample-wise mean average precision (MAP) before harmonic mean. The harmonic mean combines the prediction performance of SEEN and UNSEEN instances. CASTLE excels in discerning UNSEEN instances, leading to high harmonic mean performance. Evaluation criteria for generalized few-shot classification on MiniImageNet include 1-shot 10-Way UNSEEN class tasks. Confidence calibration between predictions for SEEN and UNSEEN instances is crucial for improved performance. The importance of calibration factors in few-shot classification tasks is highlighted, with validation on UNSEEN classes and evaluation on test UNSEEN classes. The calibration factor is applied to improve prediction performance, as demonstrated in the study. The strategy involves subtracting a bias on prediction logits of SEEN classes to enhance prediction accuracy. Calibration bias is subtracted from the confidence for SEEN classes to balance predictions with unseen parts. Bias range is determined through sampling tasks and selecting the best value. Calibrated methods show consistent improvement in accuracy compared to results without calibration factor. Results are listed in Table A12 to Table A15, with UNSEEN classes ranging from 5 to 20 in GFSL tasks. After applying calibration bias to balance predictions, CASTLE shows improved joint prediction ability for UNSEEN instances, enhancing the final harmonic mean measurement. It fits the generalized few-shot learning task well, eliminating the need for additional calibration steps. Visualization of UNSEEN class embeddings from MiniImageNet demonstrates CASTLE's superior instance relationship understanding compared to other baseline approaches."
}