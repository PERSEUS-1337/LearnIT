{
    "title": "rJhR_pxCZ",
    "content": "As deep learning classifiers gain popularity, understanding label selection is crucial. Single decision trees are interpretable but not ideal for complex data. Variational autoencoders learn low-dimensional data representations but struggle with high-likelihood data. The differentiable decision tree (DDT) is introduced as a modular component for compressing high-dimensional data for classification. Supervised VAE (SVAE) with a Gaussian mixture prior leverages label information for improved generative modeling. The Differentiable Decision Tree (DDT) is connected to a Variational Autoencoder (VAE) to create a classifier+VAE (C+VAE) model. This model is competitive in classification error and log-likelihood, combining interpretability of decision trees with the effectiveness of deep learning approaches. The DDT allows for easy separation of classes in the data, making it highly interpretable, while the VAE learns an embedding of the data for classification with low expected loss. The DDT's expected loss is differentiable, enabling gradient-based training methods to be applied. The supervised VAE (SVAE) utilizes label information to improve log-likelihood by separating classes in latent space. Combining SVAE with DDT results in a competitive Classifier+VAE (C+VAE) model, showing clear semantic meanings in its decision tree nodes. This demonstrates the power of labeled data in autoencoding with a simple class-based Gaussian mixture model as a VAE's prior. The supervised VAE (SVAE) uses class labels to improve loglikelihood bounds and image generation on MNIST data. A differentiable decision tree (DDT) allows for computing gradients of expected loss for weight updates. Combining SVAE with DDT results in Classifier+VAE (C+VAE), which learns a latent variable distribution suitable for both classification and generation simultaneously. The Classifier+VAE (C+VAE) model, trained on MNIST data, produced competitive results in both classification error and log-likelihood using simple encoders and decoders. An analysis of the interpretability of a differentiable decision tree (DDT) trained on MNIST revealed that the tree effectively summarizes the classification process by testing encoded dimensions. The dimensions used by the tree correspond to meaningful macro-features learned by the encoder. The experimental results of the Classifier+VAE model, trained on MNIST data, were presented in Section 4. The variational autoencoder (VAE) was introduced as a latent variable model for efficient maximum likelihood learning, optimizing the lower bound on the marginal log-likelihood. The VAE optimizes the lower bound by reparameterizing q(z | x) to minimize the K-L divergence from a chosen prior distribution. The simplicity of this prior distribution restricts the flexibility of latent variable assignment but allows the VAE to be easily used as a decoder-based generative model. The model described introduces a differentiable decision tree, a supervised VAE with a modified prior, and a combined C+VAE model. These modifications aim to learn a latent variable distribution for classifying and generating data. The VAE objective is modified by using a mixture of unit Gaussians to address the issue of mixing data of various classes, making them difficult to separate for classification. The prior is changed to be class-focused, incorporating the notion of class-distributed data into the VAE objective. The VAE objective is updated by using a Gaussian mixture as a prior distribution, enhancing training with class labels and avoiding over-regularization issues. This approach, referred to as the supervised VAE (SVAE), improves sample quality in generated images. The decision tree is a simple, interpretable model used for non-parametric classification, constructed by algorithms like CART or C4.5. Inference is performed by walking an input from the root to a leaf, assigning a class probability vector. Decision trees classify well with richly descriptive features and are very interpretable. Deep networks are used for non-linear dimensionality reduction to allow decision trees to classify low-dimensional embeddings of high-dimensional data. Utilizing a probabilistic generalization of decision trees, each leaf returns a distribution over all classes. Decision trees classify well with richly descriptive features and are very interpretable. Each leaf corresponds to a region in an axis-aligned partitioning covering the data space. The expected loss of a decision tree on an instance drawn from a probability distribution can be computed by considering the region covered by the leaf. The text discusses the optimization of distribution parameters for a decision tree using gradient-based methods. It involves minimizing expected loss by calculating integrals and cumulative distribution functions. The distribution used is based on the encoder's outputs on input data. The text discusses optimizing distribution parameters for a decision tree using a deep encoder in an EM-style manner. A new architecture called C+VAE (Classifier+VAE) is introduced, which uses a Gaussian distribution parameterized by a deep encoder network for classification and data reconstruction. The learned representation includes more than just class information for downstream use. The C+VAE model utilizes a deep encoder network to parameterize a Gaussian distribution for classification and data reconstruction. The training procedure involves initializing encoder/decoder parameters, decision tree, and posterior class means, followed by gradient updates and decision tree re-training until convergence. The optimization function combines the supervised VAE objective and the decision tree error. The C+VAE model uses a DDT gradient to separate class means and measures the supervised VAE K-L divergence w.r.t. movable class-based means. An L2 loss is imposed on the encoded \u00b5 posterior value to prevent class means from drifting and encourage learning common factors of variation. This additional drift loss term improves training stability and classification performance. The modified VAE objective of the C+VAE is designed to minimize a specific function. The C+VAE model utilizes a differentiable decision tree for interpretable classification and generative modeling. Experiments were conducted on the MNIST dataset using a modified VAE with a 50-dimensional latent variable. The decision tree was trained using the CART algorithm with regularization techniques. The C+VAE model uses a differentiable decision tree for interpretable classification and generative modeling. The model's performance is listed in TAB0, showing the effect of training without backpropagated classification loss. C+VAE sans reconstruction focuses on learning an embedding suitable for classification. The text discusses training a model that learns an embedding suitable for classification with the DDT. It evaluates the efficacy of leveraging label data in a supervised VAE for generation, showing improved log-likelihood compared to a VAE with an unmodified Gaussian prior. The flexibility of a Gaussian mixture and multi-modal data contribute to the improved results. The text discusses the use of label information in a supervised VAE for classification with the DDT. It evaluates the benefit of reconstruction in learning an embedding that can be classified well. In a test, the reconstruction error feedback was turned off in learning, resulting in a significant improvement in classification error over M1:SVAE+CART. The combined method C+VAE with \u03b3 = 1000 and \u03bb = 0.1 showed a large improvement in classification error, highlighting the importance of both types of feedback in training. Despite competitive classification performance, C+VAE's log-likelihood was comparable to other VAE implementations. The combined method C+VAE with \u03b3 = 1000 and \u03bb = 0.1 showed a significant improvement in classification error by leveraging the simplicity of a decision tree for feature selection over latent dimensions. Dimension 21 of the latent code was identified as the macro-feature used for classification. Dimension 21 of the latent code is the macro-feature used by the decision tree to discriminate between certain digits. Visualizations in FIG1 show how varying dimension 21 affects the appearance of digits. FIG2 further illustrates this by fixing other dimensions to the mean of all class means. The mean of all 50 dimensions is shown in the center image for contrast, while the left and right images demonstrate the effect of low or high values of dimension 21 on the 'average digit'. Figures 4, 5, and 6 vary dimensions 10, 26, and 45, respectively. The decision tree uses dimension 0 to separate certain digits, with low and high values creating distinct appearances. Dimension 26 separates '2' from '4' and '7', with a notable impact on the lower-left corner. Dimension 45 also affects the appearance of digits. The effect of varying dimension 45 on the appearance of digits is highlighted. Latent codes with high values of dimension 1 lack a central vertical line, distinguishing them from class '1'. Differentiable Trees, such as Deep Neural Decision Forests, use deep networks for representation learning with decision trees. The proposed inference method integrates a decision tree to make it differentiable. The proposed inference method integrates a decision tree to make it differentiable, allowing for a novel approach in integrating a probability distribution over the decision regions. Another tree-based method uses differentiable boundary trees for effective classification, similar to our technique. The classification accuracy marginally outperforms our combined model, but the C+VAE also acts as a generative model without the complexity of dynamically constructed computation graphs. Other work includes classifying latent codes produced by a VAE, with M1 semi-supervised model learning to classify from the latent embedding similarly to our combined classifier. The M1 model trains the discriminator separately from the VAE, lacking interpretability. The M2 model is similar to the supervised VAE but does not change the VAE prior. BID2 introduces a Gaussian Mixture Variational Autoencoder for class-focused latent representation. Our work focuses on a supervised environment, making the VAE modification simpler and more interpretable. Future work includes applying the approach to other datasets like CIFAR-10 and exploring semi-supervised learning and clustering. MNIST digits generated by C+VAE from one class prior, with the option to sample from a mixture. Landscape version of decision tree learned by C+VAE shown in Figure 8."
}