{
    "title": "Bk7wvW-C-",
    "content": "In this paper, an asymmetric encoder-decoder structure is explored for unsupervised context-based sentence representation learning. The model, consisting of an RNN encoder and a CNN decoder, shows improved efficiency and performance without requiring an autoregressive or RNN decoder. Trained on large unlabeled corpora, the model's transferability is evaluated on downstream language understanding tasks, demonstrating simplicity, speed, and effectiveness in producing rich sentence representations. Learning to build a distributed sentence encoder in an unsupervised manner involves two key components: learning distributed word representations and composing a sentence representation from these word representations. Context plays a crucial role in human language understanding, as shown in numerous studies. The Skipthought model is a unified framework for learning language representation from unlabeled data. It uses an encoder-decoder model for unsupervised sentence representation learning, focusing on semantic similarity within adjacent sentences. This approach outperforms previous unsupervised pretrained models. The Skipthought model utilizes an encoder-decoder framework for unsupervised sentence representation learning, surpassing previous pretrained models. It incorporates 2 independent decoders to infer adjacent sentences differently. The model consists of an RNN encoder and a CNN decoder, with the encoder computing vector representations for sentences and the decoder reconstructing target sequences. The dimensions of word vectors and sentence representations vary based on the RNN encoder size. The model aims to provide faster training speed and better transferability by using a parameter-free composition function for sentence representation learning. The composition function combines global mean pooling and global max pooling over time on hidden states. The decoder, a 3-layer CNN, reconstructs target sequences by expanding the representation. The architecture allows for fully connected and convolution layers for efficient training. The decoder in the model is a 3-layer CNN that expands the representation of target sequences. It uses fully connected and convolution layers for efficient training, with a softmax layer for producing a probability distribution over words at each position. The decoder is not an autoregressive model, leading to high training efficiency. The decoder in the model is a 3-layer CNN that predicts all words in the next sequence at once, with a softmax layer for probability distribution. The training objective is to minimize negative log-likelihood over all positions in the target sequence. The design focuses on high training efficiency and strong transferability for an encoder-decoder model. The model uses a 3-layer CNN decoder to predict all words in the next sequence simultaneously. The necessity of an autoregressive decoder in learning sentence representations was tested, with findings presented in Table 1. The encoder has a bi-directional GRU, and the representation is produced by a combination of global mean-pooling and global max-pooling. The experiment compared different sampling strategies for an autoregressive decoder, finding that inputting correct words is not necessary. Predict-all-words decoders perform similarly to autoregressive decoders, and mean+max pooling enhances transferability. The model includes a bi-directional GRU encoder and a CNN decoder, inspired by BID3. The experiment compared different decoding settings for an autoregressive decoder, showing that inputting correct words is not crucial. The results indicate that the model with an autoregressive decoder performs similarly to a predict-all-words decoder. This suggests that testing the necessity of an autoregressive model is warranted. The study compared the performance of predict-all-words CNN and RNN decoders with autoregressive decoders. The predict-all-words RNN decoder showed similar results to autoregressive RNN decoders, indicating that the type of decoder does not significantly impact performance. The study supported using a predict-all-words CNN decoder for higher training efficiency and transferability. Unlike other models, they used multiple ways to compute hidden states for sentence representation, showing improved performance on the SNLI dataset. In a proposed RNN-CNN model, mean+max pooling shows stronger transferability than max pooling alone. The concatenation of these pooling functions is parameter-free and computationally efficient. Sharing parameters in the word embedding layer and word prediction layer enhances learning efficiency. In a proposed RNN-CNN model, mean+max pooling shows stronger transferability than max pooling alone. The tying in CNN decoder helps learn a better language model and reduces parameters to prevent overfitting. Word embeddings are initialized with pretrained word vectors like word2vec and GloVe. Hyperparameters were studied based on downstream tasks, with small improvements seen by increasing encoder dimensionality. Increasing the dimensionality of the RNN encoder improved model performance, while adding more layers to the decoder slightly improved performance on downstream tasks. However, the trade-off between performance and training efficiency led to the decision not to sacrifice efficiency for minor gains. The study compared the training time efficiency of adding more layers and increasing the dimension of convolutional layers in the CNN decoder. Results from models using the BookCorpus dataset were reported, with optimization done using the ADAM BID18 algorithm. Word2vec was chosen over GloVe for word embedding initialization, and the vocabulary for unsupervised training consisted of the top 20k most frequent words in BookCorpus. The study by Kiros et al. proposed a word expansion method that utilizes pretrained word embeddings to improve generalization in downstream tasks such as semantic relatedness, paraphrase detection, question-type classification, sentiment analysis, and textual similarity. After unsupervised training on the BookCorpus dataset, the encoder parameters are fixed and applied as a sentence representation extractor on various tasks. The study by Kiros et al. proposed a word expansion method using pretrained word embeddings to enhance generalization in downstream tasks. The encoder parameters are then utilized as a sentence representation extractor on different tasks. Two models were trained on the Amazon Book Review dataset, with the largest subset containing 142 million sentences. The models were evaluated using SentEval in PyTorch, with results presented in Table 2 for 10 evaluation tasks. The \"small RNN-CNN\" model had a representation dimension of 1200, while the \"large RNN-CNN\" had 4800. Results for the SNLI task can be found in Table 3. The Skip-thought model applied unsupervised representation learning for sentences by encoding the current sentence and decoding the surrounding 2 sentences. The Skip-thought+LN model improved performance on downstream tasks. The FastSent model generalized CBOW to sentence-level learning by using source and target word embeddings. Later, a CNN-LSTM model was introduced for sentence encoding and prediction. The proposed hierarchical model leverages context information from both sentence-level and paragraph-level to encode and predict sentences. The model outperforms existing unsupervised models in terms of training speed and performance on evaluation tasks. The comparison table shows the model's strong transferability and effectiveness. Our model, an encoder-decoder model, focuses on decoding subsequent sequences efficiently. It outperforms other models by utilizing next-words context information effectively. The small RNN-CNN model runs 3 times faster than Skip-thought on the same GPU machine during training. The DiscSent BID17 and BID26 models focus on learning sentence representations by distinguishing relationships between sentences. While promising, these models still perform worse on downstream tasks compared to encoder-decoder models. In contrast, the BYTE m-LSTM model proposed by BID32 utilizes a multiplicative LSTM unit to learn a language model on Amazon Review data. In contrast to DiscSent and BID26 models, BYTE m-LSTM uses LSTM unit BID20 to learn a language model on Amazon Review data. The model performs well on downstream tasks by producing distributed representations for left-context information. Training on Amazon Book review data resulted in performance gains for single-sentence classification tasks. Matching corpus domain with downstream task domain raises questions about the effectiveness of learning sentence representations and the comprehensiveness of downstream tasks. Unordered sentences can also be used for learning sentence representations. The ParagraphVec model learns sentence representations by predicting words within sentences. SDAE BID13 uses a denoising auto-encoder to learn sentence representations. The proposed RNN-CNN model outperforms SDAE by training faster and utilizing sentence-level continuity as supervision. Unsupervised Transfer Learning Skip-thought achieves 81.5 on large RNN-CNN BookCorpus and Amazon datasets. The proposed RNN-CNN model, trained on Amazon Book Review data, shows promising results on various tasks, outperforming other methods and demonstrating impressive transferability. The model utilizes a bi-directional LSTM as the sentence encoder with multiple fully-connected layers, achieving similar results to Skip-thought but with less training time. Supervised transfer learning with large labeled data also proves to be effective. Our RNN-CNN model, trained on Amazon Book Review data, outperforms BiLSTM-Max in supervised classification tasks but lags behind in semantic relatedness tasks. Unsupervised learning can reduce the cost and time of labeling training data. We propose an asymmetric encoder-decoder model with techniques for context-based unsupervised sentence representation learning, using simple techniques like an RNN encoder and predict-all-words CNN decoder. Our RNN-CNN model utilizes CNN as the decoder, learns by inferring next words, uses mean+max pooling, and ties word vectors with word prediction. The model is fast and efficient for learning sentence representations from unlabeled data. It outperforms other models and shows stronger transferability with a larger encoder size. The CNN decoder reconstructs 30 words next to each input sentence. Future research will focus on maximizing context information utility and designing simple architectures for optimal use. The CNN decoder in the RNN-CNN model reconstructs 30 words next to each input sentence. Different CNN decoders are labeled as \"CNN(10)\" and \"CNN(50)\" based on output length. Symbols like \"\u2020\" and \"\u2021\" indicate specific learning tasks and results reported in previous studies. Performance measures for STS14 include Pearson's and Spearman's score, while for MSRP, they are accuracy and F1 score."
}