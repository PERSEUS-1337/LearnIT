{
    "title": "H1Y8hhg0b",
    "content": "We propose a method for $L_0$ norm regularization in neural networks by pruning weights to zero during training. This regularization speeds up training, improves generalization, and is related to AIC and BIC model selection criteria. To address the non-differentiability of the $L_0$ norm, we introduce non-negative stochastic gates to determine which weights to prune. The use of the \"hard concrete\" distribution for these gates allows for differentiability in the regularized objective. Our method introduces a \"hard concrete\" distribution for the gates, enabling efficient learning of model structures with stochastic gradient descent. Deep neural networks are powerful function approximators but have drawbacks, which our approach addresses through effective regularization. Model compression and sparsification techniques can address the drawbacks of deep neural networks, such as overparametrization and overfitting. By sparsifying the model using L0 norm regularization, unnecessary computation and resources are avoided, reducing complexity and alleviating overfitting. In this paper, a general framework for surrogate L0 regularized objectives is proposed to address the intractable optimization problem of large models. The framework involves smoothing the expected L0 regularized objective with continuous distributions to maintain exact zeros in parameters while allowing for efficient gradient-based optimization. By transforming continuous random variables with a hard-sigmoid nonlinearity, the proposed method achieves this goal. The hard concrete distribution is proposed as a novel method for sparsifying parametric models like deep neural networks. It involves stretching a binary concrete random variable and passing its samples through a hard-sigmoid. This simple procedure is shown to be effective in various experiments. The L 0 norm penalizes non-zero entries in the parameter vector, promoting sparsity in estimates. AIC and BIC are model selection criteria based on specific choices of \u03bb. Unlike L 1 regularization, L 0 norm does not shrink parameter values. Optimization under L 0 penalty is challenging due to its combinatorial nature. The L 0 norm penalizes non-zero entries in the parameter vector, promoting sparsity in estimates. By using a Bernoulli distribution over binary \"gates\", the minimization of Eq. 1 can be reformulated to penalize the number of parameters being used. This approach involves a variational bound over the parameters with spike and slab priors and approximate posteriors. The first term of the equation is difficult to minimize due to the discrete nature of z, making gradient-based optimization challenging. Using REINFORCE for gradient estimation results in high variance and requires auxiliary models. Alternatives like the straightthrough estimator or the concrete distribution have drawbacks, such as biased gradients or the inability to have exact zero parameters during optimization. The discrete nature of the gates in optimization poses challenges for gradient-based methods. A solution is to use a continuous random variable to smooth the objective, allowing for efficient optimization while still enabling exact zero parameters. This approach involves using a hard-sigmoid rectification of the continuous variable to determine the gates, ensuring the possibility of exact zero values. By employing continuous distributions, Bernoulli gates in optimization can be represented as a close surrogate to the original objective function. This allows for a differentiable transformation of parameters, enabling a Monte Carlo approximation for the intractable expectation over the noise distribution. The total cost is now differentiable, with error loss measuring model fit and complexity loss measuring model flexibility. The total cost in Eq. 9 is now differentiable w.r.t. \u03c6, enabling efficient stochastic gradient optimization. The gradient of the log-likelihood w.r.t. parameters \u03c6 of q(s) is sparse due to rectifications, but this should not be an issue. The hard-sigmoid gate z is smoothed to a soft version on average, allowing for successful gradient-based optimization. An example visualization can be seen in Figure 2b. The framework described gives the freedom to choose an appropriate approach. The framework allows for choosing a smoothing distribution q(s) for a binary concrete random variable. The distribution parameters are stretched to an interval and a hard-sigmoid is applied to induce a new distribution with specific properties. The binary concrete random variable is a smooth approximation to Bernoulli random variables, allowing for gradient-based optimization through the reparametrization trick. The temperature parameter controls the degree of approximation, with a higher value concentrating the probability mass near the endpoints. The binary concrete distribution is a smooth approximation to Bernoulli random variables, allowing for gradient-based optimization. It can be viewed as a rounded version of the original binary distribution, with parameters that control the distribution's shape. The hard concrete distribution is obtained by stretching the concrete distribution and applying a hard-sigmoid function, assigning mass to {0, 1} and the rest to (0, 1). The hard concrete gate is determined by the location log \u03b1 and is smoothed by noise, resulting in a sigmoid distribution. The L 0 complexity loss under the hard concrete r.v. is expressed conveniently. At test time, an estimator is used for the final parameters under a hard concrete gate, allowing for sparse estimates without shrinkage on \u03b8. It may be desirable to impose prior assumptions on \u03b8 with alternative norms like the L 2 norm for smoothness. The L 2 norm penalty can be expressed under the Bernoulli gating mechanism, where \u03c3 is determined by the gate z. This allows for group sparsity instead of individual parameter sparsity for computational efficiency. Group sparsity is preferred for computational efficiency over parameter sparsity, allowing for practical computation savings. In neural networks, speedups can be achieved by implementing neuron sparsity in fully connected layers or feature map sparsity in convolutional layers using dropout-like procedures. The L0 and L2 penalties can be rewritten based on the number of groups and parameters in each group. Neuron sparsity is employed by introducing a gate per input neuron for fully connected layers and per output feature map for convolutional layers, with the gate shared across all locations. Compression and sparsification of neural networks have gained traction in the deep learning community. Parameter and neuron pruning techniques are commonly used for computational savings during training. However, these methods require training the original dense network, unlike the approach of exact sparsity during training. During training, sparsification occurs, allowing for conditional computation to speed up training. Logistic distributions are used to represent exact zero values, but are sub-optimal for approximating binary random variables. Smoothing transformations can enable gradient-based optimization of discrete random variables by smoothing binary latent variables with continuous noise. The hard concrete distribution allows for reparametrization gradients, representing exact zero and one values. It is validated on tasks like MNIST classification with MLP and CIFAR classification with Wide Residual Networks. Parameters are set accordingly for the concrete distributions. Our approach, based on recommendations from BID19, sets \u03b2 = 2/3 for concrete distributions. We initialize locations log \u03b1 by sampling from a normal distribution with a mean that matches the original dropout rate. Using a single sample of gate z per minibatch, we demonstrate training speedups without compromising network performance. Regularization is limited to L0 norm, and optimization is done with Adam using default hyperparameters. Our method is competitive in neural network compression compared to other approaches. Our approach minimizes parameters in layers where gates affect a larger part of the cost, unlike methods with sparsity inducing priors. By specifying separate \u03bb for each layer, we can increase sparsification on specific layers. The study compares the results of BID18 and L0 minimization under L0 hc in terms of neurons left after pruning and test set error. Different architectures like MLP Sparse VD and LeNet-5-Caffe Sparse VD are evaluated for potential speedup in training using FLOPs and dropout BID31 networks. The study compares the results of BID18 and L0 minimization under L0 hc in terms of neurons left after pruning and test set error. For dropout BID31 networks, L 0 minimization can yield significant computational benefits with minimal loss in performance. There is a significant difference in flop count for the LeNet model between different settings of \u03bb. The \u03bb sep. setting with larger values for \u03bb in convolutional layers is preferable for speedup over network compression. WideResNets apply L 0 regularization on weights of hidden layers for potential speedup in training. Results on benchmark classification tasks of CIFAR 10 and CIFAR 100 are presented in Table 2, comparing different network architectures and regularization techniques. The study employed L2 regularization and adjusted weight decay coefficients for layers with hard concrete gates. Optimization was done with a minibatch of 128 datapoints split between two GPUs. The results show the performance of various networks, including original ResNet-110, pre-act-ResNet-110, and WRN-28-10-dropout from previous studies. The study compared different network architectures and regularization techniques on CIFAR 10 and CIFAR 100. By optimizing the L0 norm of parametric models, a speedup in training time was achieved without compromising accuracy. The method involved smoothing the combinatorial problem with continuous distributions and a hard-sigmoid, leading to a novel distribution called the hard. The study introduced a novel distribution called the hard concrete, utilizing a hard-sigmoid transformation to mimic binary nature in neural network sparsification. The proposed L0 minimization process showed competitive results in sparsity while improving upon dropout regularization on CIFAR experiments. Future work includes exploring conditional computation for training large neural networks efficiently with learned sparsity patterns. Future research directions include adopting a full Bayesian treatment over parameters for further speedup and compression. Exploring the behavior of hard concrete r.v.s in binary latent variable models is also of interest, as they can serve as a replacement maintaining both discrete nature and efficient optimization. The objective function described is a special case of a variational lower bound under a spike and slab prior, known for its sparsity in Bayesian inference. The text discusses using variational inference with a spike and slab approximate posterior over parameters \u03b8 and gate variables z. The variational free energy under this prior and approximate posterior is calculated, with terms representing KL-divergence from the prior to the approximate posterior. The parameter \u03b8 j contains information about the data D, measured by KL-divergence from the prior p(\u03b8 j |z j = 1). By assuming KL(q(\u03b8 j |z j = 1)||p(\u03b8 j |z j = 1)) = \u03bb, we can view \u03bb as the flexibility of the prior, with \u03bb = 0 resulting in no code cost and possible overfitting. This assumption allows us to consider \u03bb as the amount of information the q(\u03b8 j |z j = 1) can encode about the data. The variational free energy can be re-written as DISPLAYFORM2 where \u03b8 is optimized and the negative log-probability of the data equals the loss L(\u00b7). Optimizing Eq. 21 penalizes the entropy of q(z) and allows for prior information about gate behavior. The expected L0 minimization procedure is a close surrogate to a variational bound involving a spike and slab distribution over parameters with fixed coding cost when gates are active. The hard concrete is a modification of binary concrete with a probability density function for the parameters when gates are active."
}