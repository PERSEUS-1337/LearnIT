{
    "title": "H1xtU6VtwH",
    "content": "Chinese text classification has gained increasing attention, but the issue of Chinese text representation, particularly with polyphones and homophones in social media, remains a challenge. To address this, a new structure called the Extractor, utilizing attention mechanisms, has been proposed. The Extractor-attention network (EAN) combines a word encoder and a Pinyin character encoder to enhance Chinese text representation. EAN outperforms hybrid encoder methods by utilizing multi-input information efficiently, leading to state-of-the-art results on 5 large datasets for Chinese text classification. The state-of-the-art results for Chinese text classification on 5 large datasets have been achieved using deep learning. However, there are still insufficient studies on Chinese text representation methods, which are crucial for the performance of classification models. Issues with previous representation methods include the limitations of word embedding and character embedding using Pinyin characters. The limitations of word and character embeddings in Chinese text representation methods are highlighted, particularly in encoding intricate language phenomena like polyphones and homophones. Humans naturally associate words with pronunciation, which is challenging for computers. Using voice can address representation issues better, such as with polyphones and homophones. The polyphone and homophone are examples of intricate language phenomena in Chinese text representation. Voice can address representation issues better, such as with polyphones and homophones, inspired by recent multimedia domain methods. Pinyin can express pronunciation precisely with no more than 6 letters and solves representation issues of Chinese characters or words. In Chinese text representation, intricate language phenomena like polyphones and homophones are common. Pinyin accurately expresses pronunciation with a limited number of letters, addressing representation issues of Chinese characters or words. The Extractor-attention network (EAN) is a new hybrid encoder network proposed for Chinese text classification. It includes a word encoder and Pinyin character encoder, inspired by Transformer. The Extractor in EAN utilizes a multi-head self-attention mechanism with separable convolution layers to encode Pinyin information and combine word and Pinyin encoders efficiently. Compared to previous methods, EAN has simpler encoders and a more complex combination part with deep self-attention mechanism. The Extractor-attention network (EAN) utilizes simple encoders and a deep self-attention mechanism to assign weights between features more accurately. It includes pooling layers for downsampling and separable convolution layers to compress parameters, representing Chinese text well and improving classification accuracy at a lower computational cost. Experimental results show EAN outperforms baseline models on all datasets with fewer parameters. The novel hybrid encoding method, including Pinyin, addresses text representation issues in Chinese text classification, particularly with language phenomena like polyphones and homophones. To address text representation challenges in Chinese text classification, a novel hybrid encoding method, including Pinyin, is proposed. A new attention architecture called the Extractor is introduced to express Chinese text information effectively. A complex attention method is also developed to combine word and Pinyin encoders efficiently. This method surpasses previous approaches and achieves state-of-the-art results on public datasets, utilizing deep neural networks for text classification without relying on hand-crafted features. The text representation for text classification involves converting texts into low dimension vectors using embedding methods like pre-trained word embedding, character embedding, and word embedding without pre-training. Both CNN-based and RNN-based methods have shown significant success in text classification, with attention mechanisms often used to capture critical features. The text representation for text classification involves converting texts into low dimension vectors using embedding methods like pre-trained word embedding, character embedding, and word embedding without pre-training. Some methods use CNN or RNN with attention mechanisms, while others solely rely on attention mechanisms. Different classifiers like softmax or sigmoid are used, with the most significant difference in Chinese text classification being the text presentation approach, including radical embedding. Various methods have been proposed for text representation in text classification, including using Chinese radicals, character strokes, visual character embedding, and Chinese Pinyin. These methods have shown good performance across different languages such as Chinese, Japanese, and Korean. Additionally, encoder methods for Chinese, Japanese, Korean, and English have been developed, focusing on character, word, and romanization word encoding. The methods for text representation in text classification include encoding character, word, and romanization word. A multi-channel CNN for Chinese sentiment analysis combines word, character, and Pinyin channels for better performance. While some Chinese text classification methods have shown good results, there are still some disadvantages such as inefficiency due to a high number of parameters. The Extractor-based method consists of a word encoder, Pinyin encoder, combination part, and classifier, with attention mechanisms employed inside and outside the Extractor. The Extractor-attention network (EAN) method architecture includes Batch Normalization (BN) after convolutional layers, rectified linear unit (RELU) activation function, pre-trained word embedding, Gaussian noise, dropout, BN, separable CNN layer, and Layer Normalization (LN). The Pinyin encoder consists of character embedding and Extractor parts to represent audio information from Chinese texts data. The Extractor-attention network (EAN) method architecture utilizes Batch Normalization (BN), separable CNN, max pooling, and dropout for audio information extraction from Chinese texts. Gaussian distribution initializes embedding weights, followed by BN, separable CNN, and max pooling in the Pinyin character embedding part. Residual connections are employed in each block to alleviate gradient issues and speed up training. Layer Normalization (LN) is applied after the residual connection. The attention block in the Extractor-attention network (EAN) method utilizes nonlinear multi-head self-attention structure with separable CNN for feature extraction. This structure enhances model representation ability by capturing local and position-invariance features efficiently, making it suitable for Chinese text representation and classification. The Extractor-attention network (EAN) method utilizes a nonlinear multi-head self-attention structure with separable CNN for feature extraction. This structure enhances model representation ability by capturing local and position-invariance features efficiently, making it suitable for Chinese text representation and classification. In the self-attention block, Q, K, V are matrixes of queries, keys, and values, respectively, with different representation subspaces generalized for different attention functions. The Scaled Dot-Product Attention is employed for each head to capture internal relationships, followed by concatenation of all heads and processing by a separable CNN layer to obtain the output P. The extraction block in the CNN compresses feature maps and extracts features. Pinyin character embedding has no word boundary issue but requires longer length than word embedding. Feature maps of Pinyin encoder may be too large, so filtration is used to alleviate this. Downsampling and 2-layers separable CNN are employed to narrow feature maps. The key problem of the hybrid encoder method is combining the encoders efficiently. Traditional combination methods are often used for this purpose. The hybrid encoder method combines encoders efficiently by using relatively uncomplicated encoders and complex combination methods. The output of word and Pinyin encoder is concatenated, and an Extractor is used to extract long-term dependencies and global features. A Scaled Dot-Product Attention is then employed to weight the final Extractor output. Global max pooling and flatten layers are avoided to reduce redundancy and parameters. The final hybrid representation f is computed by self-attention scores A based on the output matrix X from the Extractor. The classifier, consisting of full-connected layers and a softmax layer, is the final part. Various Chinese text datasets are used for news topic and sentiment classification, with 10K documents from the training data selected for validation. The validation set for various Chinese text datasets used for news topic and sentiment classification consisted of 10K documents from the training data. EAN was compared with different methods, including EmbedNet, GlyphNet, and fastText. A comparison baseline named TAN was designed based on EAN, replacing the Extractor with a Transformer encoder structure. The text discusses the setup for word and Pinyin character embedding, including the use of Jieba for Chinese text processing and SGNS vectors for embedding initialization. It also mentions the dimension and dropout rate for the embeddings, as well as the convolutional layer setup. The setup for word and Pinyin character embedding included 256 input channels, 4 heads in the attention block, separable convolutional layers, max pooling with size 3 and stride 2, and full connected layers with dropout rates. Adam optimizer with a learning rate of 0.001 was used, and experiments were conducted using Keras on GPU 1080Ti. The testing error rate results for EAN with hybrid encoder or single word encoder outperformed state-of-the-art baseline methods on all datasets. EAN excels in accuracy for Chinese text classification and the hybrid encoder showed superiority over the single encoder. Two extra combination methods were designed to prove effectiveness, as shown in Figure 2. The results in Table 4 show that the Concatenate+Extractor (C+E) combination method outperforms other methods, including Extractor+Concatenate (E+C). This suggests that straightforward encoders with complicated features combination may work better. The results of TAN are similar to EAN, but most results of letter are better than those of former. The optimal results were achieved using the Concatenate+Extractor method with 3 Transformers and 4 heads. The parameters of EAN (C+E) are fewer than other models due to the use of separable CNN and downsampling operations, making it computationally cheaper than other combination methods. The Extractor-attention network (EAN) is a novel approach for Chinese text classification that utilizes a hybrid encoder combining words and Pinyin characters. This method reduces parameters and improves efficiency, achieving state-of-the-art results on 5 public datasets. The Extractor-attention network (EAN) is a novel approach for Chinese text classification that utilizes a hybrid encoder combining words and Pinyin characters. It achieves state-of-the-art results on 5 public datasets and analyzes the effects of different encoder structures."
}