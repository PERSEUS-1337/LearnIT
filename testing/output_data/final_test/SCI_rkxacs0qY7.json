{
    "title": "rkxacs0qY7",
    "content": "Variational Bayesian neural networks (BNN) use variational inference over weights, but specifying meaningful priors and approximating posteriors in high-dimensional weight space is challenging. Functional variational Bayesian neural networks (fBNNs) maximize an Evidence Lower BOund (ELBO) defined on stochastic processes, allowing for rich structure in priors like Gaussian processes. The fBNNs provide reliable uncertainty estimates, scale well to large datasets, and extrapolate effectively with structured priors. Bayesian neural networks (BNNs) combine scalability, flexibility, and predictive performance with principled Bayesian uncertainty modeling. However, specifying meaningful prior distributions and accurate posterior inference remain challenging. Stochastic variational inference offers appealing update rules similar to backpropagation but faces difficulties in fitting accurate posterior distributions due to complex dependencies. Increasing the width of a shallow BNN results in a Gaussian limiting distribution. In this paper, the focus is on the relationship between Bayesian neural networks (BNNs) and Gaussian processes (GPs). The study introduces functional variational BNNs (fBNNs) to address the gap in expressive power between BNNs and GPs. The goal is to perform variational inference directly on the distribution of functions. Functional variational BNNs (fBNNs) aim to produce a distribution of functions with small KL divergence to the true posterior over functions. The KL divergence between stochastic processes can be expressed as the supremum of marginal KL divergences at finite sets of points. A functional ELBO (fELBO) training objective is presented, along with a GAN-like minimax formulation and a sampling-based approximation for functional variational inference. The spectral Stein gradient estimator (SSGE) is used to approximate the marginal KL divergence gradients. Comparisons are made between fBNNs and Bayes-by-Backprop (BBB) for weight-space inference. Functional variational BNNs (fBNNs) allow for specifying stochastic process priors with richly structured dependencies between function values. They can model various structures like smoothness and periodicity, and efficiently yield explicit posterior samples of the function. This enables fBNNs to be used in settings that require explicit minimization of sampled functions, such as Thompson sampling. Functional variational BNNs (fBNNs) can model richly structured dependencies between function values, allowing for explicit minimization of sampled functions like Thompson sampling. Unlike ordinary BNNs, fBNNs behave well as their capacity increases, making them suitable for larger networks without causing degeneracy in posterior inference. This is due to the direct definition of the prior over the space of functions, enabling sensible extrapolations for both explicit periodic priors and implicit structures. Functional variational BNNs (fBNNs) generate sensible extrapolations for explicit periodic priors and implicit piecewise priors, outperforming other approaches on regression datasets. Reliable uncertainty estimates enable state-of-art performance on the contextual bandits benchmark of BID46. Bayesian neural networks are defined by a prior on weights and a likelihood function. Variational Bayesian methods aim to fit an approximate posterior to maximize the evidence lower bound (ELBO). The most common training method is Bayes By Backprop (BBB), using a fully factorized Gaussian approximation for the posterior. The reparameterization trick allows for computation of gradients towards the mean and standard deviation for updates. The choice of prior is crucial for computational efficiency. Weight-space priors, such as independent Gaussian or Gaussian mixture distributions, are commonly used for computational convenience in Bayesian neural networks. Other priors like log-uniform and horseshoe priors serve specific purposes like model compression and selection. Stochastic processes are defined as collections of random variables on a probability space, with sample functions mapping index space to mathematical space. Stochastic processes can be characterized by marginals over finite index sets, with joint distributions defining a stochastic process F. Bayesian methods are applied to modern probabilistic models, including those with neural networks like BNNs and deep generative models. The Spectral Stein Gradient Estimator (SSGE) is a method for estimating the log density derivative function of implicit distributions, requiring only samples from the distribution. It involves a continuous differentiable density q(x) and a positive definite kernel k(x, x) in the Stein class of q. The method utilizes eigenfunctions of the kernel given by Mercer's theorem and the Nystr\u00f6m method. The Nystr\u00f6m method is used to approximate eigenfunctions and their derivatives. Function space variational inference is introduced for distributions over functions, with a neural network architecture for the variational posterior. Functional variational inference maximizes the functional ELBO, which is similar to the weight space ELBO but with distributions over functions rather than weights. The KL divergence between two stochastic processes does not have a convenient form due to the absence of an infinite-dimensional Lebesgue measure. The KL divergence between stochastic processes is challenging to work with due to the lack of an infinite-dimensional Lebesgue measure. To simplify this, we focus on the KL divergence between marginal distributions of function values at finite measurement sets. This approach allows us to rewrite the functional ELBO and analyze the supremum of marginal KL divergences over all finite measurement sets. Maximizing the fELBO involves a two-player zero-sum game similar to a GAN, where one player selects the stochastic network and the other chooses the measurement set. The infimum may not be achievable due to unbounded measurement set sizes. The function space KL divergence can be infinite if the prior assigns zero measure to functions representable by a neural network. Limiting the discriminator's capacity, similar to GANs, provides a well-defined training objective. Choosing measurement sets for the fELBO involves a two-player game similar to GANs, where one player selects the posterior network and the other the measurement set. The approach of concurrent optimization did not generalize well, and the measurement set maximizing the KL term is likely close to the training data. The fELBO involves choosing measurement sets that maximize the KL term to match the prior structure. A sampling-based approach is adopted to include random training inputs and points from the prediction domain. This helps the network exploit the structured prior for extrapolation. The fELBO involves maximizing the expected L X (q) with a sampling distribution c. When the measurement set includes all training inputs, L(q) becomes a log-ML lower bound. Consistency is maintained with finite measurement sets, but the approximation is not a lower bound on the log marginal likelihood. The relationship between adversarial and sampling-based inference is explored, showing that minimizing the KL divergence from the true posterior justifies their use. Theorem supports using adversarial and sampling-based objectives with finite measurement points. Consistency is maintained with a Gaussian process true posterior and variational family of Gaussian processes. A proper lower bound can be obtained with a finite measurement set, even if not all training inputs are included. The KL divergence term in the fELBO remains intractable without an explicit formula for the variational posterior density. The Jacobian can be multiplied by a vector using backpropagation to estimate log-density derivatives. The entropy derivative is generally intractable, but tractable for certain priors. Implicit stochastic process priors also pose challenges. The SSGE is used to estimate score functions for both in-distribution and out-of-distribution samples in experiments. The algorithm for fBNNs involves sampling function values, computing log likelihood gradients, and updating parameters using an optimizer. Measurement points from training data and a distribution are forwarded through the network to maximize the objective corresponding to fELBO with a regularization hyperparameter \u03bb. The KL term provides a lower bound of the functional. The algorithm for fBNNs involves sampling function values, computing log likelihood gradients, and updating parameters using an optimizer. It uses a restricted number of measurement points and a regularization hyperparameter \u03bb to control overfitting. Gaussian noise is injected on function outputs for stability. Variational inference has been applied to neural networks with different methods proposed for Gaussian posteriors and correlation modeling between weights. Some non-Gaussian variational posteriors like multiplicative normalizing flows and implicit distributions have been proposed for correlations between weights. Neural networks with dropout can be interpreted as Bayesian neural networks. Various methods like local reparameterization trick and Flipout aim to reduce variances during training by decorrelating gradients within a mini-batch. Different priors, including spherical Gaussian, log-uniform, and horseshoe priors, are used for model compression and selection. Recent attempts have been made to train Bayesian neural networks with functional priors. Recent attempts to train Bayesian neural networks with functional priors include BID8, which trained a BNN to mimic a GP prior but still required variational inference in weight space. Noise Constrastive Priors BID17 use a random noise prior in function space, but it is not rich enough to encourage extrapolation and pattern discovery. Neural Processes (NP) BID12 model conditional distributions given data points, with the prior specified implicitly by prior samples. In high dimensional spaces, conditional distributions become increasingly challenging. Gaussian processes are difficult to apply to large datasets due to computational complexity. Various methods have been proposed to reduce complexity, such as sparse GP methods. Variational Implicit Processes (VIP) use BNN priors and GPs to approximate the posterior, but cannot exploit structured GP priors. fBNNs make similar predictions to exact GPs, while standard BBB struggles to fit training data. Our experiments aimed to test the extrapolation ability of fBNNs with various structural motifs and compare their performance with other BNNs on tasks like regression. The variational posterior was represented as a stochastic neural network with independent Gaussian distributions over the weights. Measurement points were uniformly sampled from a rectangle containing the training inputs. The experiments tested the extrapolation ability of fBNNs with structured priors, including Gaussian processes modeling periodic structure. The fBNNs were able to make sensible predictions outside the observed data range by exploiting the underlying structure. The study compared fBNNs with structured priors to Bayes By Backprop and Gaussian Processes in fitting noisy observations of a periodic function. While BBB failed to capture the periodic pattern, fBNNs showed similar predictions to GPs with corresponding kernels, albeit with slightly smaller uncertainty. The extrapolation results were attributed to the functional prior rather than the network architecture. In this section, approximate posterior samples and marginals for two implicit priors are examined: a distribution over piecewise constant functions and a distribution over piecewise linear functions. Prior samples are displayed in FIG2. Each experiment run involved sampling a random function from the prior and selecting 40 data points from specific intervals. The tanh activation function was used to make the task more challenging for the fBNN. The tanh activation function was used in the experiment, showing that fBNNs made predictions with piecewise constant or piecewise linear structure. However, the posterior samples did not capture the full diversity of possible explanations. The network learned to generate functions with sharp transitions despite the tanh activation function encouraging smoothness. The experiment also involved using a BNN with five hidden layers and 500 units each, normalized inputs and targets, and fitting GP hyperparameters on datasets with less than 2000 data points from the UCI collection BID1. The inputs and targets were normalized. Observation noise variance was set to the true value. Trained GP used as prior for fBNNs. Measurement points included training examples and 40 randomly sampled points. Training budget of 80,000 iterations with linear annealing of KL term weighting factor. Standard deviation of observation noise was 0.02. Fully connected network with 2 hidden layers of 100 units and tanh activations used. Network trained for 20,000 iterations. Datasets split into 90% training and 10% test sets, repeated 10 times. In this study, fBNNs were compared with BBB and NNG using RMSE and test log-likelihood. Results showed that fBNNs outperformed both methods on most datasets. fBNNs are scalable to large datasets due to accessing data through expected log-likelihood. Experimental verification was done with large scale UCI datasets, with random splitting into training, validation, and test sets. Hyperparameters were selected using the validation set, and early stopping was performed after training for 80,000 iterations. In this study, fBNNs were compared with BBB and NNG using RMSE and test log-likelihood. Results showed that fBNNs outperformed both methods on most datasets. fBNNs are scalable to large datasets and were trained for 80,000 iterations with 1 hidden layer and 100 hidden units. GP hyperparameters were fit using mini-batches of size 1000 with 10000 iterations. Each experiment was run 5 times, and the mean and standard deviation were reported. Large scale regression results with bigger networks can be found in the appendices. fBNNs were evaluated on a contextual bandits benchmark BID46 for uncertainty modeling applications. In this study, fBNNs were evaluated on a contextual bandits benchmark BID46. The agent selects actions with the highest reward based on input context. Networks with one hidden layer of 50 units were used, and GP hyperparameters were fit with a budget of 10,000 iterations. FBNNs were trained for 2,000 epochs with 20 training examples and 5 randomly sampled points in each iteration. Learning rate was tuned between [0.001, 0.01], and validation performance determined the testing epoch. Control overfitting was managed during training. To prevent overfitting, the testing epoch was selected based on validation performance. Gamma(6., 6.) prior was used for observation precision modeling. fBNNs were compared with algorithms from BID46, using default settings. Experiments were run 10 times, with results reported in TAB2. fBNNs outperformed other methods significantly, maintaining consistent performance. Our fBNNs outperformed other methods significantly, maintaining consistent performance even with deeper and wider networks. Bayesian optimization experiments showed that fBNNs achieved comparable or better performance than RBF Random Feature in Max-value Entropy Search. In this paper, variational inference between stochastic processes was investigated, showing that the KL divergence between stochastic processes equals the supremum of KL divergence for marginal distributions over all finite measurement sets. Two practical functional variational inference approaches were presented: adversarial and sampling-based. Functional variational Bayesian neural networks were obtained by adopting BNNs as the variational posterior. Empirical results demonstrated that fBNNs extrapolate well, estimate reliable uncertainties, and scale to large datasets. The text discusses the concept of pushforward measure, canonical projection map, and cylindrical \u03c3-algebra in the context of variational inference between stochastic processes. The Kolmogorov Extension Theorem is a foundational result for constructing stochastic processes like Gaussian processes. It defines a measure on a cylindrical measurable space using canonical projection measures on finite sets of points. The theorem states that for an arbitrary index set T, there exists a unique probability measure \u00b5 on \u2126 T satisfying certain compatibility relationships with probability measures on finite subsets of T. The Kolmogorov Extension Theorem establishes a unique probability measure \u00b5 on \u2126 T for an index set T, with compatibility relationships with probability measures on finite subsets of T. This measure is a Gaussian measure on a separable Banach space in the context of Gaussian processes. The pushforward measures P Fn , M Fn are defined using a canonical projection mapping, inducing a partition on \u2126 Tc. The theorem also involves the collection D(T c ) of all finite subsets of T c. The Kolmogorov Extension Theorem establishes a unique probability measure on \u2126 T for an index set T, with compatibility relationships with probability measures on finite subsets of T. Denoting D(T c ) as the collection of all finite subsets of T c, the pushforward measure P T d corresponds to the finite marginals of P at \u2126 T d. The theorem involves building a finite measurable partition Q for any finite indices set T d, leading to the proof that each T d is contained in some D(T c). The KL divergence between two conditional stochastic processes is computed using datasets D1 and D2. It equals the marginal KL divergence on observed locations when D2 is empty. This justifies the use of M measurement points in adversarial and sampling-based functional VI. The proof of Theorem 2 involves X M as measurement points not in the training data, randomly sampled from c(x). In additional experiments, a classic time-series prediction problem on CO2 concentration at Mauna Loa Observatory is considered. Training data from 1958 to 2003 is used to model predictions for 2004-2048. Prediction results by BBB, fBNN, and GP are compared in FIG4. For the CO2 concentration prediction problem at Mauna Loa Observatory, BBB, fBNN, and GP models were compared. Both BBB and fBNN used a ReLU network with 2 hidden layers and 100 units each, trained for 30k iterations with Adam optimizer. fBNN's prior included RBF, RBF\u00d7PER, and RQ kernels. Performance of fBNN closely matched GP's exact prediction, successfully modeling long-term trends, local variations, and periodic structures. In this section, Bayesian Optimization is used to explore coherent posteriors with Max Value Entropy Search (MES) BID59. The extrapolation results successfully model long-term trends, local variations, and periodic structures. Weight-space prior and inference (BBB) do not capture the right periodic structure or provide meaningful uncertainty estimates. In Bayesian Optimization, the MES acquisition function requires samples of function minima, which can be challenging when using a Gaussian process model. Parametric representations of the function posterior allow for easier gradient descent and minima search. Different kernel functions like RBF, Order-1 ArcCosine, and Matern12 are experimented with in Bayesian optimization using 3-dim functions. Parametric approaches like fBNN, BBB, and Random Feature BID43 are compared, with fBNN using true kernel as functional priors. In Bayesian Optimization, different kernel functions like RBF, Order-1 ArcCosine, and Matern12 are used. Parametric approaches fBNN, BBB, and Random Feature BID43 are compared. For minima search, gradient descent is performed along the sampled parametric function posterior with 30 starting points. Networks with different sizes are used for fBNN and BBB. Training involves rescaling inputs and normalizing outputs. Training iterations are set to 20000 with an annealed coefficient of the log likelihood term. The results of training fBNN and BBB for 20000 iterations are compared, with fBNN and Random Feature outperforming BBB on all three functions. fBNN performs slightly worse than Random Feature with RBF priors, but outperforms it with ArcCosine and Matern12 functions due to better modeling of function structures. This experiment highlights the advantage of fBNN in learning parametric function posteriors for various priors. In large scale regression datasets, experiments were conducted with deeper networks using BBB, fBNNs, and SVGP. Results showed that FBNNs performed slightly worse than VFE, but the performance gap decreased with larger networks. BBB failed with large networks, and the performance gap between FBNNs and VFE decreased with fewer inducing points. Gaussian processes (SVGP) BID18 enables GP to scale up to large datasets with mini-batch training using 1000 inducing points. SVGP outperforms BBB and fBNNs on small datasets but performs worse on larger datasets. Learning rate and annealing were tuned for both SVGP and BNNs, with results averaged over 5 runs. SVGP performs worse than BBB and fBNNs due to limited capacity of 1000 inducing points. BNNs can use larger networks without high computational cost. Gaussian noise is injected to stabilize gradient computation in GP kernel matrix. Injecting noise on the GP prior is equivalent to having a kernel matrix K + \u03b3 2 I. Injecting noise on the parametric variational posterior does not affect the reparameterization trick. The method is applicable to implicit priors and experiments with piecewise constant and linear priors are conducted by randomly generating functions."
}