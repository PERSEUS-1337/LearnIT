{
    "title": "BJxRVnC5Fm",
    "content": "Pruning units in a deep network can speed up inference and training, reduce model size, and improve performance. Bias propagation is a pruning technique that outperforms simply removing units. Adapting scoring functions helps select the best units to prune, which remain consistent during training. Pruning effectively reduces the size of neural networks and accelerates inference by removing parts that minimally impact performance. In an interesting recent work, BID3 argue for the \"winning ticket\" hypothesis, where pruning a large network after training to produce a smaller network with one fifth of the weights can achieve close performance with reduced computational cost. This opens up new possibilities for pruning methods to detect useless units early in training and accelerate inference. The study also introduces mean replacement, a unit pruning method that extends bias propagation to non-constrained training settings. The study introduces mean replacement, a unit pruning method that extends bias propagation to non-constrained training settings. Regardless of the scoring function used, bias propagation reduces the pruning penalty for networks without batch normalization. Fine-tuning the pruned network with additional training iterations reduces the bias propagation advantage but not very quickly. Absolute valued approximation of the pruning penalty provides superior performance over the normal first order approximation. Selected units by the best performing scoring function come from a small subset, confirming previous observations on the lottery ticket and dead units. One common technique for network pruning is selecting small magnitude parameters, while second-order saliency measures have also been proposed. Various works aim to reduce network redundancy, with methods like magnitude-based parameter pruning, weight quantization, and Huffman coding achieving significant compression rates. Pruning during training has also been effective in achieving similar compression rates. Pruning during training has been effective in achieving compression rates, with methods focusing on Bayesian neural networks and removing parameters around specific units. Pruning units also improves storage and speed, especially when using dense representations. Some approaches involve pruning entire channels or using saliency functions to identify zero activations. Iterative pruning with single unit removals has been explored, with the absolute-valued Taylor approximation showing promising results. The investigation focuses on minimizing the impact of unit removal during pruning. While some works retrain the network post-pruning to reduce accuracy loss, it is computationally expensive. Another approach involves preemptively mitigating effects by penalizing activation variance and replacing units with low variance. This idea is extended to various pruning methods, unlike ablating units with mean activation which yields inferior performance. The authors propose a simple pruning method called mean replacement to reduce the loss incurred by unit ablation. This method aims to minimize the damage caused by pruning convolutional layers in networks like VGG-16 and ResNet. Pruning a network involves setting parameters to 0 to find the optimal mask that minimizes loss. The mask must follow constraints such as pruning units and limiting the number of elements set to 0. The optimal pruning is determined by finding the mask that satisfies these constraints. The optimal pruning is determined by finding the mask that satisfies constraints. The complexity of solving Eq. 1 increases exponentially with the number of units to prune. Units are ranked using a scoring function, and those with the lowest score are pruned. A good scoring function should have low inter-unit correlation. Pruning a fraction of units in a layer can significantly impact the network, leading to a pruning penalty that can be reduced by adjusting biases in the following layer. This approach aims to remove units in a way that maintains network performance, often by replacing them. Pruning units in a layer can impact network performance, often done by replacing them with a constant. Mean replacement involves using the mean of unit outputs before pruning. In fully connected networks, each unit has a single activation, while in convolutional layers, each unit is associated with multiple outputs. These outputs are replaced with the same constant. Mean replacement involves replacing pruned units with a constant, implemented by folding the constant into the bias parameter of downstream units. This is done by computing the mean unit output of a subset of training examples before pruning. Mean replacement involves replacing pruned units with a constant, implemented by folding the constant into the bias parameter of downstream units. In the context of a quadratic loss, mean replacement is shown to be the optimal strategy for linear regression settings. Pruning an input dimension and updating the bias of the next layer after pruning involves using the mean values of the pruned dimension. The optimal update for bias in the next layer after pruning involves using the mean values of the pruned dimension. This approach is similar to mean replacement, where pruned units are replaced with a constant folded into the bias parameter of downstream units. The optimal update for bias in the next layer is the Mean Replacement, as shown in linear regression settings. Most practical pruning methods use scoring functions to assign scores to units, aiming for minimal overhead. Scoring functions designed for single unit removal are often used to prune multiple units at once, but changes in scoring distribution can invalidate previous unit ordering. The complexity of scoring functions is linear with layer size or cardinality of D s. Six scoring functions are compared in experiments, with details summarized in TAB0. In experiments, 6 scoring functions are compared, with different types including random, norm, abs taylor, and taylor. The norm of parameters tends to increase during training, leading to smaller norms for units not contributing much. Taylor approximations are used with subsets sampled from training sets of different sizes. In experiments, 6 scoring functions are compared, including random, norm, abs taylor, and taylor. The norm of parameters tends to increase during training, leading to smaller norms for less contributing units. The curr_chunk discusses the performance of scoring functions and pruning penalties in network training. Various pruning strategies are compared by measuring the pruning penalty before and after pruning a predefined fraction of units. The curr_chunk discusses training convolutional networks on Cifar-10 and Imagenet-2012 with different sizes and depths, using various pruning fractions. The effectiveness of bias propagation is assessed by pruning different combinations of layers during training. Pruning penalties are calculated every 250 steps, and the performance of various pruning methods is evaluated on a five-layer convolutional network. The effectiveness of bias propagation in reducing pruning penalties for various methods on a five-layer convolutional network is demonstrated. Bias propagation decreases the penalty in almost all cases, showing that training the pruned network can compensate for damage caused by zeroing units without bias propagation. The study demonstrates that bias propagation can reduce pruning penalties in a five-layer convolutional network. By fine-tuning the network, the same loss as mean replacement pruning can be achieved. The experiment focuses on post-pruning loss after at least 25,000 training iterations on the Cifar-10 dataset, showing near-zero losses for most networks by that time. Scatter plots for different fine-tuning iterations reveal that the effect of mean replacement diminishes but still shows a difference after 500 fine-tuning steps. This supports the claim that immediate improvement in pruning penalty aids future optimization. The study shows that immediate improvement in pruning penalty benefits future optimization. Appendix 6.5 presents plots from networks pruned before training step 25k, while Appendix 6.6 explores iterative pruning experiments. Performance profiles are used to compare different scoring functions, including measurements from all Cifar-10 experiments for various pruning methods. Performance profiles of scoring functions for various pruning methods on Cifar-10 experiments are calculated by setting thresholds based on pruning penalties. Changing the interpolation constant \u03c4 helps understand how each method performs compared to the best scoring one. The y-axis shows the probability of a scoring function having a smaller pruning penalty than the threshold, while the x-axis represents the interpolation constant \u03c4 determining the threshold used for pruning measurements. The constant \u03c4 determines the threshold for pruning measurements. Bias propagation enhances scoring function performance. Mean Replacement consistently improves performance. ABS MRS and ABS RS have similar performance, with ABS MRS potentially slightly better. Direct first order approximations of pruning penalty perform worse than random selection, causing damage to networks. In FIG4, histograms show pruning penalties for experiments on the second convolutional layer using a pruning fraction of 0.1. The distribution of squared activations suggests high error terms for the approximation. In FIG5, units chosen by the scoring function decrease later in training, providing better approximations with small error terms. The experimental setup in FIG4 is used to track the accumulated set of pruned units at different time steps. The curves in FIG5 show the fraction of units pruned over time, indicating that a stable set of units is selected early in training. This suggests that the \"winning ticket\" can be identified early on. The study compares unit pruning strategies and introduces the mean replacement approach. The mean replacement approach substantially reduces the impact of unit removal on the loss function. Fine-tuning pruned networks does not quickly diminish the mean replacement advantage. Direct first-order approximation of pruning penalty is a poor predictor, while absolute value versions perform best. Best pruning methods identify a stable set of prunable units early in training, prompting future research on combining pruning and training to reduce computational costs. The Mean Replacement Saliency (MRS) scoring function is defined as the first order Taylor approximation of the pruning penalty after mean replacement. Pruning can be done at any part of the training process, with experiments conducted every 250 or 10000 steps for Cifar-10 and Imagenet-2012 respectively. The approximations on absolute change penalize both directions, emphasizing changes in the neural network rather than the loss function. During training, pruning is conducted every 250 or 10000 steps for Cifar-10 and Imagenet-2012, with different settings summarized in TAB2. Pruning involves selecting layers to prune, either individually or all at once, using a fraction or count to determine the number of units pruned at each step. Single unit removals are also performed for comparison with BID11. Confidence intervals are generated through 8 experiments for each setting, pausing training every 250 iterations for pruning measurements. During training, pruning is conducted every 250 or 10000 steps for Cifar-10 and Imagenet-2012. Pruning involves selecting layers to prune, using different settings. Pruning measurements include scoring functions calculation, unit selection, bias propagation, and pruning penalty evaluation. Validation subsets are sampled independently for each experiment. Mean replacement helps optimize by reducing loss gap before and after pruning. In the second half of training, the gap between loss before and after pruning is examined. Complimentary data is shared for the first half of training in FIG7, showing more points in the negative regime. The effectiveness of the pruning method decreases with more fine-tuning steps. Results in Section 4.2 and Appendix 6.5 focus on minimizing immediate damage to the network. In this section, the study extends to investigate whether different starting points in network optimization lead to different end points. Iterative pruning experiments are conducted with an increased number of fine-tuning steps. The pruning strategy involves pruning 1% of a layer at a time and performing 100 fine-tuning steps in between until the target pruning fraction is reached. All layers in the MEDIUM CONV are pruned together starting from iteration 60000, with a total of 93750 iterations performed. Results from experiments with iterative pruning in the MEDIUM CONV layer starting from iteration 60000 show that various pruning methods perform slightly better than random. The regularization effect of pruning is evident as test loss increases slower than training loss with increased sparsity. Further experiments with the VGG 11 network and 10 fine-tuning steps between pruning iterations confirm these findings. In Figure 11, pruning penalties are compared using data from Figure 4. Different pruning methods are grouped and compared, showing how the comparisons evolve with increasing fine-tuning steps. The blue cloud of points align almost perfectly diagonal, while the orange cloud (comparison against random scoring function) moves slower, highlighting the difference between random and other methods. Fine tuning steps=500bp_abs_mrs VS abs_rs rand VS bp_abs_mrs. In Figure 11, pruning penalties from different pruning methods are compared, showing how the differences diminish with increased fine-tuning steps. Future work will investigate discrepancies with experiments by BID11 and BID10, focusing on the impact of pruning strategy and hyperparameters on performance."
}