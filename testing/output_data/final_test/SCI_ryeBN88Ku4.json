{
    "title": "ryeBN88Ku4",
    "content": "We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation, enhancing autoregressive priors to generate high-quality synthetic samples. Our model utilizes simple feed-forward networks for fast encoding and decoding, enabling quicker sampling in the compressed latent space. By incorporating powerful priors over latent codes in a multi-scale hierarchical VQ-VAE setup, we achieve sample quality comparable to state-of-the-art Generative Adversarial Networks on diverse datasets like ImageNet, without GAN's drawbacks such as mode collapse. Deep generative models, including VAEs, flow-based, autoregressive models, and GANs, have improved significantly in recent years due to architectural and computational advancements. These models generate samples that closely resemble real data and have various applications such as super resolution, domain editing, artistic manipulation, and text-to-speech. Each type of generative model offers trade-offs in sample quality, diversity, and speed. Generative Adversarial Networks (GANs) optimize a minimax objective with a generator neural network creating images from random noise, while a discriminator classifies samples as real or fake. Larger GAN models can generate high-quality images, but struggle to capture the full diversity of the true distribution. Evaluating GANs is challenging, with no established measure for generalization. Likelihood-based methods optimize negative log-likelihood for model comparison. In this paper, the focus is on maximizing the negative log-likelihood (NLL) of the training data to measure generalization and model comparison. Likelihood-based models cover all modes of the data and avoid issues like mode collapse and lack of diversity in GANs. However, directly maximizing likelihood in the pixel space can be challenging due to limitations in sample quality assessment and comparison between different model classes. Introducing inductive biases like multi-scale or modeling dominant bit planes in an image can help alleviate some of these issues. In this paper, techniques from lossy compression are used to compress images into a discrete latent space for generative modeling. The compressed representations are significantly smaller than the original image but still allow for high-quality reconstruction. The prior over these representations is modeled using a state-of-the-art PixelSnail model, ensuring high-quality and coherent image generation. The VQ-VAE model BID28 is a fast and efficient communication system for encoding and decoding large images using a shared codebook. It offers 30x faster training and sampling over the discrete latent space compared to directly applying to pixels, making it ideal for high-resolution image training. The encoder maps observations to discrete latent variables, which are then quantized based on distance to prototype vectors in the codebook for reconstruction by the decoder. The VQ-VAE model uses prototype vectors in a codebook to quantize observations for reconstruction by the decoder. The model incorporates codebook loss and commitment loss to align the codebook with the encoder output and encourage the encoder to stay close to the chosen codebook vector. The VQ-VAE model uses prototype vectors in a codebook to quantize observations for reconstruction by the decoder. It incorporates codebook loss and commitment loss to align the codebook with the encoder output and encourage stability in the encoder's output. The codebook loss is updated using exponential moving average updates with a decay parameter \u03b3. The proposed method involves training a hierarchical VQ-VAE to encode images into a discrete latent space and fitting a PixelCNN prior over this space. The model uses a two-stage approach with quantized latent maps of different sizes for encoding and decoding images. This hierarchical approach aims to separate local texture information from global shape and geometry, allowing for tailored prior models at each level to capture specific correlations. The hierarchical VQ-VAE model uses tailored prior models at different levels to capture specific correlations. The top prior focuses on global information with a larger receptive field, while the bottom prior emphasizes local information with a larger conditioning stack. This hierarchical factorization allows for training larger models by separately training each prior. The hierarchical VQ-VAE model uses tailored prior models at different levels to capture specific correlations. Factorization allows training larger models by leveraging all available compute and memory on hardware accelerators for each prior. The multi-scale hierarchical encoder structure encourages encoding complementary information in each latent map to reduce reconstruction error. For 256\u00d7256 images, a two-level latent hierarchy is used, with the encoder network transforming and downsampling the image to a 64\u00d764 representation quantized to the bottom level latent map. The hierarchical VQ-VAE model uses tailored prior models at different levels to capture specific correlations. A stack of residual blocks scales down the representations to a top-level 32 \u00d7 32 latent map after quantization. Fitting prior distributions using neural networks from training data can significantly improve the performance of latent variable models. Sampling from the learned prior at test time results in more coherent outputs. The process of fitting a prior to the learned posterior can be considered as lossless compression of the latent space. The learned posterior in the VQ-VAE framework is a better approximation of the true distribution, leading to more realistic image samples. An auxiliary prior is modeled with a powerful neural network like PixelCNN, using self-attention layers and masked convolution blocks. The top-level has an unconditional network, while downstream layers use a conditional stack for spatial conditioning representations. The model uses PixelCNN to generate image samples conditioned on class labels and latent codes. The network includes residual gated convolution layers and causal multi-headed attention for regularization. Additionally, deep residual networks with 1x1 convolutions are added to improve likelihood without increasing memory usage. The bottom-level conditional prior operates on 64x64 latents. The model utilizes PixelCNN for generating image samples based on class labels and latent codes, incorporating residual gated convolution layers and causal multi-headed attention for regularization. The bottom-level conditional prior operates on latents with a 64x64 spatial dimension, requiring less powerful networks with no attention layers and deep residual conditioning stacks for efficiency. The framework is based on VQ-VAE and Gated PixelCNN augmented with self-attention, inspired by BigGAN's architectural advances. Incorporating architectural advances like self-attention, stabilization methods, and scaling up on TPUs improved sample quality in BigGAN. Our study explores how self-attention and compute scale enhance sample quality in VQ-VAE models. Recent models like Subscale Pixel Networks and hierarchical latent variables have also aimed to generate high-resolution images with likelihood-based approaches. In contrast to other models like BigGAN, our work focuses on using simple, feed-forward decoders and optimizing mean squared error in pixels to avoid hierarchy collapse issues. We use autoregressive models exclusively as priors in the compressed latent space, unlike other methods that use autoregressive decoders in the pixel space. In this section, qualitative results of our model trained on ImageNet 256\u00d7256 are presented. The samples show sharp and diverse images across various classes. A comparison with BigGAN-deep shows that our model provides samples with comparable fidelity but higher diversity. Likelihood based models allow assessing overfitting by comparing NLL values between training and validation sets. Our method uses VQ-VAE to generate diverse high resolution images with improved quality. Architectural advances in PixelCNN style priors, including self-attention, help capture object structure accurately in the latent space. The quality of samples is correlated with these improvements. The quality of samples generated by our model is linked to improvements in the negative log-likelihood in the latent space. Our class conditional samples rival those of Generative Adversarial Networks in quality, with greater diversity in various classes. Maximum likelihood in the latent space is shown to be a successful objective for learning generative models without the drawbacks of adversarial training. Additional samples from our ImageNet-trained model are provided without bias."
}