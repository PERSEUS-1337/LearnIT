{
    "title": "H1srNebAZ",
    "content": "Neural networks trained through stochastic gradient descent (SGD) have been around for over 30 years, but their inner workings remain elusive. This paper takes an experimental approach, focusing on the behavior of single neurons in deep neural networks. The experiments reveal that hidden neurons function as binary classifiers during training and testing, separating inputs into distinct categories. This sheds light on the internal mechanics of deep neural networks and how they encode information for prediction. Deep neural networks with binary weights and activations can perform image classification with surprising efficiency, offering object detectors and universal representations for new tasks. The characteristics of SGD trained neural networks allow for these behaviors to emerge. The limitations of deep neural networks trained with SGD include difficulties in continuous learning, robustness, and unsupervised learning. Understanding these limitations requires a better grasp of the intricate nature of neural networks, which can be achieved through experiments to uncover key mechanisms supporting their success. This insight can pave the way for future theoretical and practical developments by analyzing how neural networks work to improve their performance. The workings of hidden neurons in deep neural networks are still a mystery, with many studies focusing on their interpretability. It is believed that hidden neurons represent concepts with increasing abstraction at deeper layers. Some research shows that intermediate feature maps can detect higher-level objects, but it is unclear if this interpretation captures all relevant information. This paper explores the unknown aspects of how neurons encode information and the dynamics of training. The main finding is that a neuron's behavior can be approximated by a binary classifier during training, leading to the partitioning of inputs into two categories. During testing, experiments show that the binary partition observed in point 1 embeds core information used by the network for predictions. This behavior has been observed across different layers, networks, and problem scales, raising intriguing questions for future investigations. Recent works have explored how the activation of a single neuron is linked to the input image, using methods like deconvolution networks and occlusion analysis. Inverse problem formulations have also been used to reconstruct images from network representations. Recent advancements include quantifying the interpretability of signal extracted from visualization methods. Recent works have explored how individual neurons can capture visually consistent structures, with object detection emerging in units with highest activation in a CNN trained to recognize scenes. However, it is unclear if these observations reflect all relevant information in the feature map. Our paper focuses on validating the encoding of information in neurons, moving beyond interpretability to provide experiments for a complete description of neural encoding. The paper focuses on validating the encoding of information in neurons, moving beyond interpretability to provide experiments for a complete description of neural encoding. It challenges the binary nature of individual neurons, revealing a bimodal activation pattern that naturally emerges from conventional training procedures. The study challenges the binary nature of individual neurons and shows that a binary encoding emerges even in deep linear networks. The research focuses on understanding the behavior of neurons in a neural network by analyzing gradients with respect to activations on single samples. This perspective is crucial for interpreting the representation learned by a neuron. The behavior of neurons in a neural network is described by associating neurons with activation functions. The pre-activation and activation values are defined, and the spatial structure of convolutional layers is considered when studying statistical distributions. Three different architectures are experimented with, including a 2-layer MLP with dropout trained on MNIST, a 12-layer CNN with batchnorm trained on CIFAR-10, and a 50-layer ResNet trained on ImageNet. Additionally, ReLU activation is also used. The ResNet50 network is analyzed along with other models like MLP and CNN with different activation functions. Specific layers of these networks are referred to by their stage and position. The study focuses on analyzing neurons in ResNet models using Keras applications. The researchers examine gradients flowing through neurons to gain insights into how the representation of a single sample is constructed. The experiments were conducted using Keras and Tensorflow libraries, with a particular emphasis on understanding the training dynamics of neural networks. The study analyzes neurons in ResNet models using Keras applications, focusing on understanding how the representation of a single sample is constructed. The researchers record gradients of the loss with respect to activations during training, computing the average sign of partial derivatives to determine if increased activation benefits or penalizes sample classification. The study focuses on analyzing neurons in ResNet models using Keras applications. It examines how the representation of a single sample is constructed by recording gradients of the loss with respect to activations during training. The average sign of partial derivatives is computed to determine the impact of increased activation on sample classification. The results show that the derivative sign remains consistent throughout training, indicating the neuron's attempt to partition the input distribution into two distinct categories. The study analyzes neurons in ResNet models using Keras applications, focusing on the regularity of activation gradients during training. The activation of a sample in a neuron should consistently be pushed in the same direction to improve prediction. The behavior is more apparent in early layers compared to layers far from the output, raising questions about the presence of regular dynamics in early layers hidden by noise. The study examines the regularity of activation gradients in early layers of ResNet models, which are often obscured by noise. The presence of noise in gradients, particularly in ReLU-networks, raises questions about its impact on learning dynamics. The observed noise may be a result of architecture and training procedures, or it could be a crucial aspect of the learning process. The gradients suggest that neurons attempt to separate input categories, but it remains unclear if this separation occurs effectively during training. The study explores the activation gradients in early layers of ResNet models, focusing on the evolution of pre-activations for 'low' and 'high' categories. The visualization reveals a struggle to separate the categories during training, raising questions about the mechanism regulating sample partitioning in neurons. The dynamics show that both categories are distinguished but not completely separated before training stops. The final highest pre-activations are highlighted in yellow in the visualizations, showing histograms of average derivative signs of sample activations in different layers. Neurons act as binary classifiers, receiving consistent information on how to affect sample activations. Layers close to the output exhibit distinct categories where some activations always increase and others always decrease. The average sign of the loss function derivative determines category, fixed by network initialization. Derivative signs in output layer depend on class label, not input. Categories in dense2-relu are mostly present or absent, occasionally split. Category definition is a random subset selection. The pre-activation distributions of neurons in different layers evolve during training, with high and low categories being separated based on the average partial derivative sign. The final highest pre-activations of the high category show a complex transformation, not a simple translation. Additional images and videos can be found for further exploration. Neurons operate like binary classifiers during training, but does this reflect how they encode information during testing? This section tests if all information transmitted by a neuron is encoded in the binary partition observed earlier. The performance of neural networks is studied by modifying activations through quantization and binarization strategies. The focus is on highlighting the binary aspect of encodings and revealing structural components, such as the fuzziness of the binary rule and locating thresholds. ResNet50 is also studied in this section. The Section studies ResNet50 and tests if neural networks are robust to quantization of pre-activations by using two distinct values per neuron. Percentiles are computed to determine thresholds for quantization, with eleven thresholds tested between 0 and 100. Neural networks are remarkably robust to quantization of pre-activations, with performance being consistent across different layers. Only the conv1 layer from ResNet50 shows a significant decrease in accuracy when pre-activations are quantized, possibly due to poor gradient quality in early layers. The curr_chunk discusses a sliding window binarization experiment to understand how pre-activations are encoded in neural networks. This experiment aims to determine clear thresholds for categorizing pre-activations as low or high, using two thresholds to map activations to 1 or 0. The experiment involves using a sliding window with a width of 10 percentile ranks to map pre-activations of a neuron to 1 or 0. The center of the window determines which activations are mapped to 1, providing insights into how information is encoded in neural networks. Quantization is performed on pre-activations to observe binary partition encoding. Test accuracy is monitored after reinitialization and retraining of subsequent layers. Linear classifier probes are used for analysis instead of retraining all layers. Neurons transmit information through binary partition, improving network performance. Quantization is done on a single layer at a time using percentile ranks as thresholds. The networks show robustness to quantization, indicating neurons provide a binary signal. Results across layers and networks demonstrate a clear signal in measuring coding scheme categories indirectly. Results in Figure 4 reveal a clear signal across all layers and networks: the further from rank 50, the better the network performs. There is symmetry around percentile rank 50, indicating a fuzzy partition of two categories with a threshold at rank 50. Neurons encode information by partitioning inputs into two overlapping categories of varying sizes. The binary behavior of neurons in neural networks is observed to be symmetrically arranged around the 50th percentile rank, regardless of the position of activation thresholds. This binary behavior is also seen in linear networks without thresholding effects in hidden neurons, challenging previous studies on activation binarization. The study aims to validate a hypothesis on neuron behavior during training and testing, suggesting a simple binary nature. Our hypothesis suggests that neurons in neural networks behave like binary classifiers, separating inputs into two categories provided by backpropagated gradients. Experiments on networks of varying depths and widths consistently validate this behavior, indicating implications for neuron interpretability. While previous studies focused on high activations, we show that neurons tend to learn concepts that distinguish half of the observed samples, challenging traditional views on neuron behavior. The analysis challenges traditional views on neuron behavior by showing that neurons tend to learn concepts that distinguish half of the observed samples. Further investigations are needed to understand the regularity of gradients in deep networks and the role of activation functions in training dynamics. The role of activation functions in neural networks is to increase expressivity by adding non-linearities. However, it is unclear why one non-linearity is better than another. Research suggests that activation functions promote the emergence of binary encoding in neurons through well-positioned binarization thresholds. This new perspective may help address the generalization gap observed in deep learning models. Neurons prioritize samples with common patterns during training, observed indirectly in previous studies. A sliding window binarization experiment reveals a fuzzy, binary partition of inputs into two categories. This encoding pattern is consistent across all layers and networks, providing important information for classification. Neurons prioritize samples with common patterns during training, observed indirectly in previous studies. A sliding window binarization experiment reveals a fuzzy, binary partition of inputs into two categories. The layers are part of a network trained on MNIST, CIFAR-10, and ImageNet with different activation functions. The dynamics behind this prioritization between samples of the same category provide insights about generalization. The regularity of gradients and the prioritization effect suggest that the slope leading to local minima also matters for good generalization abilities. The observation in the current chunk suggests that neurons aim to partition samples based on the sign of the loss function derivative. Two experiments challenge this behavior but find that binarizing neuron pre-activations preserves task information. The findings raise important questions about network learning capabilities. The current chunk discusses questions regarding the convergence of first layer neurons in the presence of noisy/unstable partial derivatives, activation function design, and the generalization puzzle. Training information includes learning rate, batch size, and number of epochs for a neural network architecture. The network architecture includes convolution, activation function, and BatchNormalization layers. Data augmentation is used, and the network is based on the ResNet50 model from Keras applications. Training information details are provided. In Section 4, gradients and pre-activations are recorded for different numbers of samples across various layers of ResNet50. Percentile rank of ReLU threshold is computed based on pre-activation distributions of neurons, showing convergence in the last layers. Training samples used for Figure 4 are limited to 100,000 from the ImageNet dataset, while test error is calculated on the complete ImageNet validation set. The ReLU threshold does not seem to cause binary behavior of neurons. Histograms show average derivative signs of loss with respect to sample activations, revealing consistent information received by neurons on affecting sample activation. The neuron-wise histograms in FIG1 show that input samples have negative or positive derivatives, allowing neurons to act as binary classifiers. Layers from the first two rows are trained on MNIST with ReLU and sigmoid activation functions, while the third and fourth rows are trained on CIFAR-10 with ReLU and no activation function. Pre-activation distributions evolve during training, separating into high and low categories based on average partial derivative signs. The pre-activation distributions evolve during training, separating into high and low categories based on average partial derivative signs. Neurons act as binary classifiers, with input samples having negative or positive derivatives. The final highest pre-activations of the high category are highlighted to show it is not a simple translation. These illustrations can be viewed in video format on https://www.youtube.com/channel/UC5VC20umb8r55sOkbNExB4A. Histogram showing consistency between sample class and belonging to low category of neuron in dense2-relu. Most elements of a class are in the same category, with peaks at 0 and 100%. Video illustrations available at https://www.youtube.com/channel/UC5VC20umb8r55sOkbNExB4A."
}