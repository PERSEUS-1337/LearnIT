{
    "title": "Hyljn1SFwr",
    "content": "There is a debate about measuring information flow in Deep Neural Networks using information theory techniques. It is claimed that these networks have good generalization capabilities by compressing input information. The Information Bottleneck method is suggested as a comparator for network performance. The method of mutual information measuring is under scrutiny. In this paper, the development of the Information Plain (IP) is explained, comparing mutual information for compression judgment. The effect of clustering on network loss and network pruning is also discussed using the Information Plane. Deep Neural Networks (DNNs) have shown promising results in various fields, but the learning process and design principles of DNN architecture are still under investigation. The learning process and design principles of configuring DNN architecture are under-investigated. Recent attempts have focused on the learning dynamics of DNN from an information theoretic viewpoint, showing a compression behavior similar to the IB-method. This compression behavior has led to promising results in various applications. The IB-bound can be used to evaluate network architecture, with controversy surrounding its effectiveness. Some studies support its use, while others argue against it. Implementing the IB-constraint in DNN training has shown promising results, although criticisms exist regarding its optimization process. Recent studies question the necessity of compression for good generalization in neural networks, citing success with fully invertible models. Goldfeld et al. (2018) suggest that measuring mutual information tracks the clustering of internal layer representations. This work aims to explain the trajectories in information plane using binning, shedding light on DNN learning dynamics. The smaller bin size in the binning estimator causes layers to drift towards a fixed point in the information plane. Higher layers strongly influence the shape of lower layers. Clustering as a design parameter is explored, but no clear connection is found with the loss function. Network pruning using the information plane shows slightly positive indications. The experimental setup is outlined for replicable and comparable experiments. The experimental setup for replicable and comparable experiments involves using networks with specific designs, tracking layer activations, binning the activations, and judging clustering using bin histograms. Different activation functions like TanH and ReLU are compared, with combinations of the two also being tested. The experiments compare TanH and ReLU activation functions in mixed networks without regularization, using SGD. Results show that ReLU-layers influence deeper TanH layers, altering their pathway. The experiments compare TanH and ReLU activation functions in mixed networks without regularization, using SGD. Results show that ReLU-layers influence deeper TanH layers, altering their pathway. The compression effect weakens deeper in the network, with TanH-layers impacting ReLU-layers, even though TanH-layers show no compression. Previous layers influence compression in lower layers, with mutual information being constant for discrete input features in TanH-networks. The mutual information remains constant in TanH-networks with discrete input features, as shown in Table 1. This indicates a compression effect towards extreme points on the information plane as bin size decreases. In TanH-networks with discrete input features, mutual information remains constant as bin size decreases. This leads to a compression effect towards extreme points on the information plane. The more bins used by the estimator, the more layers drift towards the top right, indicating constant mutual information. The relationship between input X and layer T is defined by mutual information I(X; T) = H(X) - H(X|T). If there is no direct mapping between X and T, there will be an increase in H(X|T) leading to a reduction in mutual information. This also applies to the output Y. Figure 3 illustrates how mutual information can be lost through binning. When binning is done incorrectly, mutual information can be lost. Placing samples from different classes in the same bin makes it impossible to differentiate between classes based on the bin number. This results in a decrease in mutual information between the input and output layers. Similarly, having two samples of the same class in a bin only decreases mutual information. When binning is done incorrectly, mutual information can be lost, leading to a decrease in mutual information between the input and output layers. The Softmax output-layer drives neurons to output either 1 or 0 for each sample during training, causing the bins to empty out as the layer learns to correctly classify. The output layer drives neurons to output 1 or 0 for each sample during training, leading to a reduction in mutual information. As the layer learns to classify correctly, activations get binned into extreme bins, causing information loss. This information is propagated to the next layer, resulting in compression in some layers. During training, the network gradually improves its ability to differentiate between classes, leading to more accurate binning of samples. The output layer eventually produces an array with mostly zeros and one 1, allowing for unique class identification. During training, the network improves its ability to differentiate between classes, leading to accurate sample allocation. ReLU-layers take a different path than TanH-layers due to the inbuilt bin in ReLUs, resulting in lower entropy and less estimated mutual information initially. As training progresses, more activations leave the 0-bin, increasing entropy and filling previously empty bins, leading to the ReLU-layers path towards the top right. As training progresses, the ReLU-layers show a wider spread of activations in bins, leading to an increase in entropy and estimated mutual information. Larger bin sizes result in fewer total bins, causing the layers to drift away from the \"real\" mutual information. The information plane analysis suggests that clustering plays a significant role in the neural network's behavior. Increasing bin sizes can lead to multiple activations landing in the same bin, causing information loss. Inserting a bottleneck layer can highlight this clustering effect, as seen in a network with a 12-3-2-12-2-2 layer-neuron composition. The bottleneck layer shows higher compression than expected, indicating that the information plane may be tracking clustering patterns. The information plane analysis suggests that clustering is a key factor in neural network behavior. Investigating the advantages of clustering is important, as shown in the following section. Early stopping is explored as a method for achieving good generalization, but it does not guarantee success, as seen in the results for TanH and ReLU networks. Early stopping does not generalize well to the MNIST dataset. In deep neural networks, clustering plays a significant role in network behavior. The analysis indicates that layers at the end of the network can be pushed into a clustered state by the output layer when they are less necessary for mapping input to output. Smaller networks have shown comparable generalization performance to larger ones, suggesting that clustering could help identify less effective layers at the end. The study explores the potential use of clustering to identify and remove less effective layers in deep neural networks. By doubling the hidden layers and adding 3 neuron layers at the end, prune-able layers are investigated. The process involves removing layers with similar trajectories until all added layers are eliminated, with no decrease in scores observed. This clustering-based pruning approach shows promise for further exploration. The study explores clustering-based pruning in deep neural networks to identify and remove less effective layers. It has been found that the information plane tracks clustering of activations, especially when mixing activation functions like ReLU and TanH. The initial clustering of TanH activations in the 0-bin due to ReLU layers with 0 activations leads to reduced mutual information with the input and output. As training progresses, the diversification of ReLU activations results in a diversification of TanH activations. The composition of the network influences clustering, with a connection between clustering in the output layer and Early Stopping. However, this connection does not apply to global minima, as shown in results on the MNIST dataset. This suggests that claims about activations and clustering may not generalize across different datasets. The paper explores the information plane as a tool for neural network analysis, focusing on the influence of different bin sizes for binning estimation. It provides new evidence that the information plane primarily tracks clustering, as suggested by previous research. The study also investigates the use of clustering for NN design through pruning experiments, indicating potential for further investigation. The discussion delves into the impact of ReLU activation functions on neural networks, highlighting potential issues such as loss of mutual information and compression. It contrasts ReLU with TanH, noting ReLU's better generalization capabilities but also mentioning the problem of \"dying ReLUs\" as a form of excessive compression. The discussion explores the impact of ReLU activation functions on neural networks, emphasizing issues like loss of mutual information and compression. LeakyReLUs are favored over standard ReLUs for better generalization. Information theory, based on Claude Shannon's work, is crucial for understanding the transmission and processing of information in deep learning. In studying neural networks, understanding information theory is essential. Concepts like \"entropy\" and \"mutual information\" play a crucial role in learning and generalizing mappings in neural networks. Entropy, originating from thermodynamics, measures disorder in a system and the uncertainty of events. It quantifies the information needed to describe a random variable. Conditional entropy is the entropy of a random variable given certain conditions. Conditional entropy is the entropy of a random variable conditioned on another random variable's knowledge. It is calculated between two distributions X and Y. An important property is that the conditional entropy of X given Y is 0 only if X is a deterministic function of Y. Relative entropy, also known as Kullback-Leibler divergence, measures the difference between two distributions. It quantifies how inefficient it is to assume one probability distribution when the real one is different. Mutual Information (MI) measures how much information is shared between two variables. Mutual Information (MI) is a metric that measures the dependency between two variables, capturing dependencies that do not show in covariance. It is invariant to reparametizations and can also be used as a measure of independence between variables. MI is calculated using Shannons information theory and is 0 only if the variables are strictly independent. The Data Processing Inequality (DPI) principle states that information cannot be increased through manipulation. Compressing information allows for a direct mapping back to the original message. Mutual Information (MI) measures dependency between variables and can be used to determine how compressed a message is. The Information Bottleneck (IB) method, introduced by Tishby et al., aims to compress a message while retaining maximum information about an output Y. This results in X becoming a minimal sufficient statistic of X with respect to Y, capturing the mutual information I(X; Y). The IB method provides an exact solution to this optimization problem known as the Information Bound in the IP. Deep neural networks can be seen as Markov-Chains, with each layer depending on the previous layer's output. The Information Bottleneck method serves as a natural comparator of DNN architecture performance, with the closer to the IB Bound indicating a more optimal architecture. This compression of relevant information is similar to sending a JPEG image instead of a raw file. The trajectory of information planes is influenced by weight initialization. Mixture networks are less affected by activation functions in higher layers. ReLU shows no noticeable influence, while TanH layers exhibit significant influence in early periods. Early Stopping does not show distinct behavior on this dataset. The smaller the bin size used to estimate mutual information, the closer the layers tend to be to the maximum point in the top right. Models in the dataset are shown to be overfitted, with high accuracy early on but a decrease in confidence later. The model's confidence decreases as it reaches 8000 iterations compared to 1000, indicating a decrease in accuracy. A binning estimator is used to estimate the probability distribution of activations in different layers, separating the range into discrete bins. Schwartz-Ziv & Tishby (2017) utilize this method with TanH activation functions, defining bounds based on known limits or through maximum and minimum occurrences in samples. The activations recorded for each epoch and layer are allocated to bins to estimate the probability distribution. Schwartz-Ziv & Tishby (2017) use a binary array approach to calculate probabilities based on feature combinations. This method allows for the calculation of conditional probabilities P (T i , X) and P (T i , Y ). The conditional probabilities P (T i , X) and P (T i , Y ) are calculated using the inverse of the X and Y arrays and their respective already calculated probabilities. Activation data is taken where the inverse of X equals the manifestation of X, converted into continuous binary arrays, and the probabilities are calculated as P (T, X). Entropies and mutual information can then be calculated using the estimated probabilities."
}