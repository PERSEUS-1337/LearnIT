{
    "title": "S1g9N2A5FX",
    "content": "We present a framework for interpretable continual learning (ICL) that uses explanations of past tasks to improve performance on future tasks. The ICL idea can be applied to various continual learning approaches, with a focus on the variational continual learning framework. Saliency maps are used to provide task explanations, and a new metric is proposed to assess their quality. Experiments show that ICL achieves state-of-the-art results in continual learning performance and explanation quality. Continual learning involves accumulating knowledge from past tasks for future use in an online fashion. It requires adapting to domain shifts without revisiting previous data. Balancing stability and adapting to new tasks is crucial to avoid catastrophic forgetting. Various approaches, including regularization, have been introduced to address this issue. Various approaches have been proposed to address catastrophic forgetting in continual learning. These include regularization to maintain stability by restricting parameter changes, dividing the network architecture into reusable and task-specific parts, using reinforcement learning strategies, moment matching, and employing RL agents with a synaptic model. Additionally, frameworks have been developed where multiple agents learn to achieve multiple goals simultaneously and experimental evaluations of continual learning with variational Bayesian loss have been conducted. Our framework is the first to pursue a comprehensive interpretability approach in continual learning, based on imitating aspects of how humans learn continually. This approach aims to understand and explain tasks already accomplished to improve performance on similar tasks in the future. The development of cognitive abilities in children shows an understanding of concepts like gravity and geometric characteristics through consolidating interpretable information from past experiences. Interpretable methods can improve performance in continual learning by helping learners face consecutive tasks over time. In a continual learning framework, tasks are followed by an explanation stage to provide insights for subsequent tasks. For image classification, saliency maps highlight relevant areas for classification predictions. A summary of these maps per class represents the classifier's decisions on test data, aiding in understanding individual decisions and relevant input areas. This approach improves performance in consecutive tasks over time. In a continual learning framework, tasks are followed by an explanation stage using saliency maps to assess catastrophic forgetting and provide interpretable attention information for subsequent tasks. In a continual learning framework, tasks are followed by an explanation stage using saliency maps to assess catastrophic forgetting and provide interpretable attention information for subsequent tasks. A new metric is proposed to evaluate the quality of saliency maps resulting from classification decisions on test data. Saliency maps aim to explain the classifier's decision by highlighting the most influential features. A metric is introduced to assess the quality of explanations from saliency maps. In a continual learning framework, tasks are followed by an explanation stage using saliency maps to assess catastrophic forgetting and provide interpretable attention information for subsequent tasks. An attention mechanism is proposed to focus on important input parts based on feature relevance values learned in the latest task. The second goal involves exchanging interpretable information among different tasks, distinct from the first goal of addressing explanations related to the same task at different time steps. The assessment involved in the first goal is still needed regardless of the level of perfection of the second goal. The framework highlights the interpretable continual learning (ICL) framework where explanations of finished tasks are used to enhance the attention of the learner during future tasks. The proposed methodology is flexible and can be deployed with other continual learning frameworks. Our proposed methodology, ICL, focuses on interpretability in continual learning. It introduces a new metric for assessing saliency maps and utilizes an attention mechanism based on explanations from the latest task. The framework, based on VCL, shows state-of-the-art results in experiments on three datasets. In a variational Bayesian framework, the posterior of model parameters is continually updated from a sequence of datasets. A discriminative classifier with input x and output y has parameters \u03b8. The intractable posterior p(\u03b8|D 1:t ) is approximated by a tractable variational distribution q t. The framework allows for continuous updating of the approximate posterior q t (\u03b8) in an online fashion. The approximation is performed by minimizing the KL divergence over a family Q of tractable distributions. Saliency maps are used to detect the relevance of image parts for a specific class label in continual learning. VCL and coreset VCL are state-of-the-art methods that reduce catastrophic forgetting and outperform EWC and SI on various benchmarks. VCL is chosen as the continual learning framework in the paper. The explanation in the curr_chunk focuses on assigning relevance values to input features to create saliency maps for classification results. The method is based on prediction difference analysis (PDA) and quantifies the relevance of each feature by considering how the prediction would change without that feature. This approach has been shown to provide high-quality saliency maps with modest run-time overhead for various image datasets. The PDA algorithm computes the odds of a prediction by marginalizing features and approximating pixel values in image data. This method assigns relevance values to input features for creating saliency maps in classification results. The PDA algorithm computes the odds of a prediction by marginalizing features and approximating pixel values in image data. It assigns relevance values to input features using Laplace correction and weight of evidence (WE) to determine the significance of each feature in the prediction process. The PDA algorithm assigns relevance values to input features to determine their significance in predictions. These explanations can improve performance on subsequent tasks by building an attention mechanism that focuses on important parts of the input. The attention strategy involves using saliency maps to represent averaged relevance values for test images. The attention mechanism developed utilizes saliency maps to represent relevance values of input features for tasks. Averaged weight of evidence matrix is computed for upcoming task images, and an attention mask is inferred based on these values. The mask is calculated by averaging relevance values of a square around each pixel in the image. The attention mechanism uses saliency maps to focus on important locations at the border. It assesses catastrophic forgetting by comparing explanations of a task before and after learning other tasks. The main concern is evaluating saliency maps, with a method by BID2 using the Smallest Sufficient Region concept for confident classification. The saliency map should produce a small, confident region for classification. A rectangle representing the relevant region is chosen as the smallest rectangle containing all salient pixels. The area of this rectangle is denoted by a, with a threshold of \u00e3 = max(a, 0.05) to prevent instabilities. Prediction probability p is returned by the classifier for the specified label. The SSR saliency metric is defined by the prediction probability returned by the classifier for the specified label. It aims to identify the salient area bounded by a, with a focus on a small area and confident prediction. However, challenges arise in determining the salient region, threshold selection, and the risk of including non-salient areas. To address these issues, a new saliency map metric with more flexibility is proposed. The proposed saliency metric aims to evaluate saliency maps based on three aspects: number of salient pixels, average distance among salient pixels, and impact on classification prediction. Lower values for number and distance indicate better quality, while a higher impact on prediction is desired. This metric allows for more freedom in indicating salient pixels regardless of their location or vicinity to other salient pixels. The proposed saliency metric, called the flexible saliency metric (FSM), evaluates the impact of salient pixels on prediction by marginalizing over them. It considers the average spatial distance among salient pixels, the number of salient pixels, and the classification probability of the specified label. Experimental evaluations are conducted on three datasets: MNIST, notMNIST, and Fashion-MNIST. The experiments focus on evaluating the performance of the ICL framework in terms of classification accuracy and the quality of explanations through saliency maps. The study also examines how well ICL and VCL mitigate catastrophic forgetting by comparing classification accuracy and saliency maps. Saliency examples demonstrate that explanations from ICL do not suffer from catastrophic forgetting. The process involves extracting explanations after each task, using them for the attention mechanism in the next task, and applying VCL for task learning. Classification results from ICL are compared to different versions of the VCL algorithm. The study evaluates the ICL framework's performance in terms of classification accuracy and explanation quality through saliency maps. Results are compared to different versions of the VCL and EWC algorithms. Statistical significance of accuracy and FSM results are reported in Appendix A, along with a comparison to a fixed attention map as a baseline. The introduced ICL framework is evaluated for classification accuracy and explanation quality using saliency maps. It is compared to VCL and EWC algorithms in four continual learning experiments with MNIST, notMNIST, and Fashion-MNIST datasets. The attention mechanism based on saliency maps is assessed on top of the VCL algorithm. The size of pixels and surrounding squares used in the experiments are specified. ICL is compared to EWC and various versions of VCL in four experiments, achieving state-of-the-art classification accuracy in three out of four cases. It also performs equally well as VCL with a random coreset in the Permuted MNIST experiment. In a continual learning benchmark, datasets are formed by random permutations on labeled MNIST images for each task. Two values of hyperparameter \u03bb for EWC are tested, with \u03bb = 100 producing the best results. Fully connected single-head networks with two hidden layers are used, achieving high classification accuracy after 10 tasks. ICL and VCL with a random coreset perform best in Split MNIST experiment with five binary classification tasks. The architecture used for tasks processing includes fully connected multi-head networks with two hidden layers. ICL achieves the highest classification accuracy in the experiment. Split notMNIST dataset is larger and more challenging, with 400,000 training images and 10 character classes. ICL also outperforms competitors in classification accuracy. ICL achieves 90.3% average accuracy after 5 tasks on Split Fashion-MNIST dataset. The saliency metric FSM evaluates the quality of explanations from ICL, showing better results compared to VCL and EWC. The experiments on notMNIST and Split Fashion-MNIST show that ICL with VCL leads to better results in terms of FSM metric value, indicating more interpretable learning procedures and less catastrophic forgetting. The experiments demonstrate that ICL with VCL reduces catastrophic forgetting, as shown in five examples with test images and explanations before and after learning other tasks. The experiments show that a continual learning framework incorporating interpretability, using saliency maps, reduces catastrophic forgetting. The framework enhances the attention of the learner during future tasks by providing explanations of previously learned tasks. The proposed framework enhances interpretability and performance in continual learning by introducing a new metric for saliency maps and suggesting the use of a Bayesian attention mechanism for future work. The framework also outperforms competing frameworks in terms of average accuracy values. The study introduces a fixed attention map with concentric circles, showing higher classification accuracy with ICL compared to competitors in most cases. The use of fixed attention maps demonstrates the importance of the learning procedure. The study shows that the classification accuracy of an algorithm, particularly ICL, is significantly higher than its competitors. Results are statistically significant with a p-value of 0.05. ICL outperforms competitors in 6 out of 8 cases, with lower FSM values indicating better interpretability. Using ICL with VCL results in significantly better FSM values in all 6 cases."
}