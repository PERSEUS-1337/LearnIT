{
    "title": "HJl8_eHYvS",
    "content": "Deep reinforcement learning has succeeded in complex games like Atari and Go, but real-world decision making often requires reasoning with partial information from visual observations. Discriminative Particle Filter Reinforcement Learning (DPFRL) is a new framework that uses a differentiable particle filter with learned models in a neural network for reasoning with partial observations over time. DPFRL's discriminative parameterization improves performance, especially for tasks with complex visual observations, by directly training for decision making. Deep Reinforcement Learning (DRL) has shown success in various applications, including game playing and visual navigation. However, challenges arise in natural environments due to partial observability and complex visual inputs. Discriminative Particle Filter Reinforcement Learning (DPFRL) outperforms existing models in Flickering Atari Games and introduces a more challenging benchmark. DPFRL also performs well in visual navigation with real-world data. In natural environments, challenges arise due to partial observability and complex visual inputs. Decision making in such scenarios can be formulated as a partially observable Markov decision process (POMDP). Most POMDP RL methods track the belief using a recurrent neural network (RNN), which may require large amounts of data to learn complex beliefs. Model-based DRL methods aim to reduce sample complexity by learning an environment model simultaneously with the policy. Model-based DRL methods aim to reduce sample complexity by learning an environment model simultaneously with the policy. DVRL incorporates a generative observation model into the policy through a Bayes filter to deal with partial observability. Learning this model can be challenging due to the strong generative assumption requiring modeling the entire observation space. To navigate in a new environment, learning a complex sequence of characters is necessary. The curr_chunk contains a complex sequence of characters that need to be learned to navigate in a new environment. The curr_chunk consists of a complex sequence of characters for navigating in a new environment. The curr_chunk contains a complex sequence of characters used for navigation in a new environment. The curr_chunk consists of a complex sequence of characters for navigation in a new environment. The curr_chunk contains a complex sequence of characters for navigation in a new environment. The curr_chunk consists of a complex sequence of characters used for navigation in a new environment. The curr_chunk discusses the Discriminative Particle Filter Reinforcement Learning (DPFRL) method for navigation in new environments, focusing on tracking a latent belief and making decisions based on features of the belief. DPFRL approximates belief using weighted latent particles and tracks it with a non-parametric Bayes filter algorithm. Transition and observation models are neural networks optimized for the policy, with a discriminatively parameterized observation model to estimate log-likelihood directly. DPFRL uses a differentiable particle filter with a learning signal from the RL objective. It introduces learnable features based on Moment-Generating Functions for efficient and permutation invariant higher-order moment information. The method is evaluated on various POMDP RL domains including continuous control tasks, Flickering Atari Games, Natural Flickering Atari Games, and the Habitat visual navigation domain. DPFRL outperforms state-of-the-art POMDP RL methods by using a differentiable particle filter with discriminatively parameterized observation model for handling partial and complex observations. It introduces effective MGF features for empirical distributions and a new RL benchmark, Natural Flickering Atari Games, that includes partial observability and complex visual observations. The code will be open-sourced for future work on real-world decision-making problems formulated as POMDPs. Various neural models have been proposed for belief tracking in POMDPs, with DVRL using the Variational Sequential Monte-Carlo method. This method is maintained by a differentiable discriminative particle filter algorithm. The differentiable discriminative particle filter algorithm, used for belief tracking in POMDPs, includes transition and observation functions, as well as soft-resampling. The policy and value function are conditioned on the belief, represented by mean particle and moment generating function features. Generative modeling is not robust in complex observation spaces with high-dimensional irrelevant features, suggesting the need for more powerful models like DRAW. However, evaluating such models for each particle would increase computational cost and optimization difficulty. Learning a robust latent representation is crucial to avoid reconstructing. The latent representation is directly optimized for the policy in RL, skipping observation modeling. Discriminative RNNs are used for belief approximation in partially observable domains. Hausknecht & Stone and Zhu et al. extend DQN with an LSTM memory for Flickering Atari Games. Using a particle filter to track beliefs can improve RL performance. Embedding algorithms into neural networks for end-to-end discriminative training has gained attention. Training methods for belief tracking have been developed using different filters such as the differentiable histogram filter, Kalman filter, and particle filter. These methods require a predefined state representation and are limited to small state spaces. Integrating particle filters with RNNs has shown advantages for RL with complex partial observations. A discriminative reinforcement framework has been proposed for improved decision making from particle beliefs. DPFRL is a framework for reinforcement learning with complex observations. It consists of a discriminative particle filter tracking a belief state and an actor network learning a policy. The belief state is maintained using a set of weighted latent particles representing hypotheses. The particles are updated using a particle filter algorithm. The latent particles are updated using a particle filter algorithm in the belief update process. The transition update step involves sampling the next state based on the agent's action and a parameterized function for learning environment dynamics. Gradients for training the observation function are obtained through backpropagation of the standard RL loss. The discriminatively parameterized observation function is expected to be effective. The discriminatively parameterized observation function, f obs, is expected to be more effective than a standard generative model for reinforcement learning. Unlike a generative model that considers the entire observation space equally, f obs has a simpler form that only learns features relevant to RL, making it more flexible and efficient for particle filtering. The unnormalized likelihood value is computed based on different h i t values, simplifying the function for belief tracking and decision making. Weight updates are done in log-space for numerical stability. A fully connected layer is used for f obs to calculate observation log-likelihood. Differentiable particle resampling is employed to prevent particle degeneracy. The text discusses the use of Moment Generating Functions (MGFs) for summarizing empirical distributions in particle belief tracking. This method helps in optimizing f obs with global belief information and preventing particle distribution collapse. The text discusses the use of Moment Generating Functions (MGFs) for summarizing belief distributions in particle tracking. MGF features provide additional information from the empirical distribution and are computationally efficient. In the actor-critic setup, a policy network \u03c0(b t ) computes the policy p(a | b t ) with the assistance of a value network V (b t ). The experiment evaluates the A2C algorithm on various POMDP RL domains, including benchmark domains like Mountain Hike and Flickering Atari Games. A new, more challenging domain called Natural Flickering Atari Games is introduced, along with a visual navigation domain with RGB-D observations. DPFRL outperforms GRU in most cases due to its explicit belief tracking structure. DPFRL outperforms DVRL and GRU due to its discriminative observation model and explicit belief tracking structure. MGF features are found to be more effective for summarizing latent particle belief. The agent navigates from start to goal on a map with partial observations introduced by Gaussian noise. Results are averaged over 3 random seeds and smoothed over time for comparison with baselines. DPFRL and DVRL differ in the particle belief update structure, but they use the same latent particle size and number of particles as in the DVRL paper. All models are trained for the same number of iterations using the RMSProp optimizer. Learning rates and gradient clipping values are chosen based on a search in the BeamRider Atari game independently for each model. Further details are in the Appendix. No additional searches for network architecture and hyper-parameters have been performed. Mountain Hike is a continuous control problem demonstrating belief tracking for POMDP RL. Observations are disturbed with Gaussian noise, and complexity is increased by adding random noise vectors. The state and action space are defined with rewards given for each step. Training models for different scenarios is conducted. DPFRL outperforms DVRL and GRU in tracking a latent belief without explicitly modeling complex observations in Mountain Hike, a continuous control problem with disturbed observations. Results for different noise vector lengths show DPFRL learns faster and is unaffected by increasing noise, while DVRL and GRU degrade. Atari games, particularly Flickering Atari Games, are commonly used for benchmarking POMDP RL methods due to their partially observable nature. The text introduces Natural Flickering Atari Games, a new RL domain that combines challenges of partial observability and complex observations. Background videos from the ILSVRC dataset are used to simulate observation complexity without affecting decision making complexity, making it suitable for evaluating RL methods with complex observations. DPFRL outperforms GRU in various games, showcasing the importance of explicit belief tracking and its ability to learn a useful latent belief representation. It also surpasses DVRL, achieving state-of-the-art performance in Flickering Atari Games and Natural Flickering Atari Games. Results are summarized in Table 1, with significant differences highlighted in bold (p = 0.05). DPFRL significantly outperforms DVRL in Flickering Atari Games, achieving state-of-the-art results in 5 out of 10 games and performing comparably in 3 others. It also outperforms DVRL in Natural Flickering Atari Games, excelling in 7 out of 10 games. The policy-oriented discriminative observation model of DPFRL proves more effective for handling complex observations compared to DVRL. Visual navigation is a challenge for deep reinforcement learning, especially in environments with partial and complex observations. DPFRL is evaluated in the Habitat Environment using the Gibson dataset, where a robot must navigate to goals with limited visual information. The addition of irrelevant features can significantly impact performance, as seen in ChooperCommand. In the Habitat Environment using the Gibson dataset, DPFRL outperforms DVRL and GRU in visual navigation tasks. The models are trained with a similar architecture as Atari games but with an observation encoder tailored for RGB-D observations. Results show DPFRL's superiority in unseen environments, with higher SPL, Success Rate, and average rewards compared to DVRL and GRU. DPFRL outperforms the PPO baseline in visually rich domains and shows potential for further improvement by adding task-specific structures or training with PPO. Ablation study on Natural Flickering Atari Games reveals discriminative parameterization's effectiveness over generative parameterization. DPFRL-generative replaces the discriminative observation function with a generative one, modeling grayscale image observations with Gaussian distributions. Performance degrades compared to DPFRL, highlighting the importance of discriminative parameterization. More particles lead to better performance, with DPFRL-P1 showing poor results due to insufficient representation of complex tasks. More particles improve performance in tasks that require complex belief tracking. DPFRL-mean is not as effective as DPFRL with MGF features, which provide richer belief statistics. Comparing to DPFRL-GRUmerge, MGF features generally perform better, as RNN parameters are harder to optimize. Optimizing RNN parameters is challenging due to lack of permutation invariance to particles and long backpropagation chains. DPFRL combines Bayesian filtering and end-to-end RL for explicit belief tracking with learnable particle filters. It outperformed alternative methods in POMDP RL benchmarks and visual navigation tasks. MGF feature extraction improves performance, but DPFRL may struggle in certain cases. Particle filter is an approximate Bayes filter algorithm for belief tracking, approximating the posterior distribution with weighted particles and updating them in a Bayesian manner. It can approximate arbitrary distributions and is less susceptible to observation noise, but lacks additional learning signals for improved sample efficiency. Future work may combine generative and discriminative modeling with the DPFRL framework. The particle filter algorithm addresses particle degeneracy by resampling particles with repetition proportional to their weight, improving representation capacity in relevant state space regions. The moment-generating function is an alternative specification of the probability distribution of a random variable. The moment-generating function (MGF) is used to generate moments of a random variable, providing information about its probability distribution. In DPFRL, MGFs are used as additional features to extract moment information for decision making. Observation encoders are structured similarly to DVRL for fair comparison. In DPFRL, observation encoders are structured similarly to DVRL for fair comparison. For Mountain Hike, two fully connected layers with batch normalization and ReLU activation are used as the encoder. The rest of the domains down-sample images to 84\u00d784 and process them with 3 2D-convolution layers. Observation decoders for DVRL and PFGRU-generative have a reversed structure compared to the encoder. The decoder includes a fully connected layer that outputs the required dimension. In DVRL and PF-GRU, observation functions are implemented using fully connected layers with different encoding dimensions. The normal distribution parameters are learned through additional fully connected layers, with Softplus used for variance activation. Probabilities are computed in log space for numerical stability. The soft-resampling hyperparameter \u03b1 is set to 0.9 for Mountain Hike and 0.5 for other domains in DPFRL. DVRL averages particle weights to 1/K after each resampling step. The GRU in DVRL and DPFRL-GRUmerge is a single layer with input dimension equal to the latent vector dimension. MGF features use fully connected layers with feature dimensions as the number of MGF features. The activation function is the exponential function, but ReLU could be explored. Model Learning: A2C algorithm is used for RL with 16 parallel environments for Mountain Hike and Atari games, and 6 parallel environments for Habitat Navigation due to GPU memory constraints. Hyperparameters such as learning rate, gradient clipping value, and soft-resampling \u03b1 are tuned based on BeamRider results and applied universally. Best performing learning rates were 1e-4 for DPFRL and GRU, and 2e-4 for DVRL. In experiments, learning rates were optimized for DPFRL, GRU, and DVRL models. Gradient clipping value was set at 0.5. Soft-resampling \u03b1 values were adjusted for Mountain Hike and Atari games. Background videos were sampled from the ILSVRC dataset, ensuring a length of over 500 frames for variability. PyTorch with CUDA 9.2 was used for model implementation. In experiments, learning rates were optimized for DPFRL, GRU, and DVRL models with gradient clipping set at 0.5. Soft-resampling \u03b1 values were adjusted for different games. PyTorch with CUDA 9.2 was used for model implementation. The models were implemented using PyTorch with CUDA 9.2 and CuDNN 7.1.2. Flickering Atari environments were modified based on OpenAI Gym, and Habitat APIs were used for visual navigation. Servers with NVidia GPUs were used for running experiments. DPFRL with gated transition and observation functions outperformed GRU-based policy and DVRL. DPFRL outperforms GRU-based policy and DVRL in a visual navigation task. DVRL struggles with training the observation model initially, while GRU-based policy learns fast but struggles to achieve higher rewards. The reward curve is provided, and latent particles are visualized using PCA.Particles initially spread out, then form two clusters representing different hypotheses of the robot's state. After forming two clusters representing different hypotheses of the robot's state, they eventually converge into a single cluster (t = 81). Particle depletion and posterior collapse were not observed in the experiment, suggesting the need to consider adding entropy loss to the learned variance of f trans for future study."
}