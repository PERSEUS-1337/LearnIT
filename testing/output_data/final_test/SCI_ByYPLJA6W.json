{
    "title": "ByYPLJA6W",
    "content": "Our Distribution Regression Network (DRN) performs regression from input probability distributions to output probability distributions with fewer model parameters. It outperforms existing methods on synthetic and real-world datasets, generalizing the conventional multilayer perceptron by encoding probability distributions at each node. This approach expands regression analysis beyond real valued inputs and outputs. Regression on distributions, not as widely studied as functional regression, has many applications in predicting outcomes driven by stochastic processes like the Ornstein-Uhlenbeck process in commodity markets and quantitative biology. Regression on distributions is used in various fields like market forces and quantitative biology. Different methods have been proposed, such as the Triple-Basis Estimator (3BE) and Extrapolating the Distribution Dynamics (EDD), to predict outcomes based on input-output distributions efficiently. Our proposed Distribution Regression Network (DRN) is a novel approach based on spin models in statistical physics and artificial neural networks. Unlike traditional multilayer perceptrons, DRN encodes probability distributions in each node, allowing for richer propagation behavior and better regression performance with fewer parameters. The Distribution Regression Network (DRN) is a novel approach that encodes probability distributions in each node, allowing for richer propagation behavior and better regression performance with fewer parameters. The regression task is to learn the function f which maps the input distributions to the output distribution, without making further assumptions on the form of the distribution. The method can be generalized to regress to multiple output distributions, but for simplicity, single output regressions are discussed. The Distribution Regression Network (DRN) encodes probability distributions in each node, allowing for richer propagation behavior and better regression performance with fewer parameters. It is not a Bayesian network but regresses probability distributions using a feedforward network similar to MLP. Each node computes the probability distribution from incoming nodes in the previous layer and network parameters. The Distribution Regression Network (DRN) encodes probability distributions in each node by computing the probability density function of the node based on incoming nodes and network parameters. The unnormalized form of the probability distribution is computed by marginalizing over conditional probabilities, similar to the Boltzmann distribution in statistical mechanics. This approach reduces computational complexity through factorization. The energy function in DRN is formulated based on spin models in statistical physics, where spin alignment and critical phenomena are studied. Parameters are learned to ensure observed configurations have lower energies than unobserved ones. The energy function is part of the forward propagation process and is not directly optimized. The support length of the distribution is denoted by \u2206, and all terms in the energy function equation are normalized by the support length for invariance. The energy function in DRN is formulated based on spin models in statistical physics, where parameters are learned to ensure observed configurations have lower energies than unobserved ones. The energy function is normalized by the support length for invariance. Forward propagation in DRN is performed layerwise from the input layer using univariate integrals. The probability distribution is normalized, and the propagation of distributions within a connection unit forms the basis for forward propagation. In DRN, the weight in the node determines the output distribution shape. Positive weights attract the output to the input peak, while negative weights repel it. A large positive weight leads to identity mapping. Weight magnitude indicates the strength of attraction or repulsion. In DRN, weight in the node shapes the output distribution. Positive weights attract output to input peak, while negative weights repel it. Large positive weight leads to identity mapping. Weight and bias values play a role similar to inverse temperature in statistical physics. Cost function measured by Jensen-Shannon divergence between label and predicted output. The Jensen-Shannon divergence is used as a cost function in the network, calculated over all training data. Integrals are numerically computed in experiments by discretizing continuous probability density functions into discrete probability mass functions. The network cost is differentiable, with cost gradients derived using chain rule similar to backpropagation in neural networks. The derivative of the final layer node distribution is evaluated with respect to the current node distribution. DRN achieves high accuracy with fewer model parameters compared to 3BE and MLP. Unlike 3BE, DRN and MLP directly use distribution pdfs in their methods. The first experiment involves a synthetic dataset with a non-linear transformation function. The regression task complexity is shown with peak splitting and spreading behaviors. 1000 training and testing data were used for evaluation. 3BE generates 10,000 samples from each data distribution, providing a continuous output distribution. DRN and MLP output discrete pmf, requiring conversion to continuous pdf for regression performance evaluation. The regression performance on the test set is measured by the L2 loss between the continuous predicted distribution and the true distribution. The regression accuracy varies with the number of model parameters, which are varied for DRN and MLP using different depths and widths of the networks, and for 3BE by varying the number of Random Kitchen Sink features. DRN's test performance is comparable to other methods with fewer model parameters, showing little overfitting. 3BE starts to exhibit overfitting when the number of model parameters approaches 10,000. The OU process, described by a time-varying gaussian pdf, combines random walk with a drift towards a long-term mean. It has wide-ranging applications, such as in the commodity market where prices exhibit mean-reverting patterns. Modelling prices with the OU process helps in forming valuation strategies. The regression task involves mapping from an initial gaussian distribution at time t. The regression task involves mapping from an initial gaussian distribution at time t to the resulting distribution after a time step \u2206t. Different values for y and t init are sampled to create pairs of distributions for analysis. Comparisons are made between DRN, MLP, and 3BE in terms of model parameters required for achieving a small L2 test loss with varying training data sizes. The regression task involves mapping from an initial gaussian distribution at time t to the resulting distribution after a time step \u2206t. DRN requires 64 projection coefficients for input and output distributions, 17 Random Kitchen Sink features, and 272 model parameters. DRN demonstrates the OU process with positive weight parameter, negative bias position, and 5 DRN parameters after training. The network mimics the diffusion property and reflects random walk and mean-reverting properties of the OU process. DRN is used for the stock dataset with 7 model parameters. It outperforms 3BE and MLP in prediction accuracy. Co-movement of stock indices like BID6 and BID2 is significant. Previous day stock returns of Nikkei and Dow Jones are good predictors of FTSE return. Predicting future distribution of returns over constituent companies provides more information than just a weighted average. The regression task involves predicting the distribution of returns for constituent companies in FTSE k days later using the current day's returns data. Stock data from January 2007 to December 2015 is used with a sliding-window training scheme for adaptation to market conditions. Exponential window averaging is applied to reduce noise in the data. The study involved predicting the distribution of returns for constituent companies in FTSE using stock data from January 2007 to December 2015. Exponential window averaging was used to reduce noise in the data, and kernel density estimation was applied to estimate the pdf for DRN and MLP. The authors extended their method for multiple input functions and concatenated basis coefficients obtained from three input distributions. Return samples were scaled to [0, 1] before applying cosine basis projection for regression performance evaluation. The study involved predicting the distribution of returns for constituent companies in FTSE using stock data. Evaluations were done for predicting next-day distributions using regression performance measured by log-likelihood. DRN showed the best regression performance with a single-layer network using only 7 parameters. Visualizations compared predicted and ground truth distributions, showing DRN's results closest to the diagonal line. The study demonstrated that DRN had the best regression performance in predicting FTSE returns distribution several days ahead. Performance deteriorated with more days, but DRN consistently outperformed others in terms of moment plots and correlation values. Experiments on a real-world cell dataset also showed DRN's superior performance. The study analyzed cell data frames at 5-minute intervals, predicting short-axis length distribution based on long-axis length. DRN outperformed MLP and 3BE in log-likelihood on test data, showcasing its advantage in learning distribution regressions with fewer model parameters. The Distribution Regression Network (DRN) proposed in this paper generalizes the MLP framework by encoding a probability distribution in each node. Compared to MLP and 3BE, DRN can learn regression mappings with fewer model parameters. DRN treats each distribution as a single object, enabling it to represent regression mappings more efficiently. The Distribution Regression Network (DRN) can achieve similar or better regression performance with fewer parameters than 3BE. DRN's runtime is competitive with other methods. Future work includes extending DRN for different distribution regression tasks and regressing multivariate distributions. The DRN network architecture for the synthetic dataset results is shown with one input node, one output node, and hidden layers. Mean prediction times per data for DRN and baseline methods were compared on the CPU. The test loss for varying parameter sizes was shown for the synthetic dataset. MLP had the fastest prediction time, followed by DRN and then 3BE, all conducted on the CPU. For a fair comparison of runtime, model sizes were chosen to give a test L2 loss of about 0.37."
}