{
    "title": "rJg4YGWRb",
    "content": "Graph neural networks have recently gained popularity for achieving state-of-the-art accuracy in graph-based semi-supervised learning. These networks alternate between propagation layers and fully-connected layers. Surprisingly, a linear model without fully-connected layers can still perform comparably well, reducing the number of parameters. This allows for more innovative propagation layers, leading to a novel graph neural network with attention mechanisms instead of fully-connected layers. The text discusses a novel graph neural network that replaces propagation layers with attention mechanisms to improve predictions on citation networks. The approach outperforms other methods in experiments and provides insights on how neighbors influence each other. The use of semi-supervised learning with additional unlabeled data in graph form is explored as a solution to limited labeled data for accurate predictions. The graph provides pairwise relations among data points, labeled and unlabeled. Applications include citation networks where edges represent citation links determined by authors. External graph data are used in various applications like classifying users in social networks or items and customers based on purchase history. In this paper, the focus is on graph-based semi-supervised learning problems where the graph represents additional information not present in feature vectors. The goal is to classify nodes in a graph using a small subset of labeled nodes and all node features. Recent advancements in graph neural networks have shown significant improvements over traditional approaches on standard benchmark datasets. The study focuses on graph neural networks for semi-supervised learning on graphs. A linear classifier achieves comparable accuracy to the best graph neural network by only using linear propagation function from neighbors. This highlights the importance of aggregating information from neighbors in the graph, leading to the proposal of an attention-based graph neural network with reduced model complexity. The study proposes an attention-based graph neural network that reduces model complexity, dynamically identifies relevant nodes for classification, and improves accuracy on benchmark datasets. The learned attention strengths offer interpretability by explaining predictions and identifying relevant neighbors in decision-making on a graph with labeled and unlabeled data. The use of labeled and unlabeled data in graph-based algorithms has been addressed through Graph Laplacian and Expectation Maximization approaches. Graph information is utilized for explicit regularization, with Label Propagation being a popular non-parametric method that enforces label agreement in labeled instances. Recent approaches in semi-supervised learning include ManiReg BID4, which uses a weighted graph Laplacian with a closed form solution, and ICA BID30, which allows for more general local updates. SemiEmb BID42 was the first to use a deep neural network to model f(x), while Planetoid BID46 improves graph regularization by using skip-grams. Additionally, BID10 shows that bootstrapping models sequentially can further improve accuracy. Unsupervised node embedding for semi-supervised learning involves embedding nodes in a latent space using graph connectivity, followed by supervised learning on the embedded features. Various approaches like DeepWalk, node2vec, LINE, and Graph2Gauss use different methods such as skip-grams and random walks to achieve this. Graph2Gauss represents nodes as Gaussian distributions and minimizes the divergence between connected pairs. Post-processing schemes can further improve any node embedding. Graph neural networks (GNN) are extensions of neural networks to structured data encoded as a graph. GNNs apply recurrent layers to each node with additional local averaging layer. As the weights are shared across all nodes, GNNs can also be interpreted as extensions of convolutional neural networks on a 2D grid to general graphs. Graph neural networks (GNN) extend neural networks to structured data represented as a graph. GNNs apply recurrent layers to each node with a local averaging layer. The model parameters are trained on (semi-)supervised examples with labels. GNNs have been successfully applied in various applications such as molecular activation prediction, community detection, matrix completion, combinatorial optimization, and detecting similar binary codes. In particular, Graph Convolutional Network (GCN) has been proposed as a powerful architecture achieving state-of-the-art accuracy. In this section, a novel Graph Neural Network (GNN) model called Attention-based Graph Neural Network (AGNN) is proposed. The model aims to predict node labels using features X and graph A. The forward pass in a typical GNN involves propagation and a single layer perceptron. The model's performance is compared to state-of-the-art models on benchmark citation networks. The current states H(t) are represented by a propagation layer using matrix P. The neighborhood of each node is denoted by N(i), and a simple local averaging is performed. Graph Neural Networks encode graph structure via propagation layers and apply a single layer perceptron with shared weights on each node. Graph Convolutional Network (GCN) is a powerful architecture that reduces the number of parameters by weight sharing and encodes the invariance property of graph data. It has proven effective in various graph-related problems and achieved state-of-the-art performance in benchmark citation networks. GCN is a special case of GNN with specific propagation and perceptron layers, using a choice of matrix P = D^-1/2AD^-1/2, where A is the adjacency matrix and D is the degree matrix. The Graph Linear Network (GLN) is a simplified version of the Graph Convolutional Network (GCN) that removes nonlinear activation units. GLN uses linear propagation layers with weights trained on cross-entropy loss and a simple linear classifier at the output layer. This separation allows for a better understanding of the gain in the linear propagation layer versus the non-linear perceptron layer. GLN achieves comparable accuracy to the best GNNs, indicating the strength of GNN architectures lies in the propagation layer. Propagation layers are crucial for performance, as shown in Table 2. Significant accuracy gaps exist for approaches not utilizing the graph. A proposal to replace GLN's propagation layer with an attention mechanism is tested on benchmark datasets. In contrast to static and non-adaptive propagation used by some models, there is a need for dynamic and adaptive propagation layers in graph neural networks to capture the relevance of different edges. However, training complex models in a semi-supervised setting is challenging due to limited samples per class. More complex models do not necessarily improve performance, as seen in experiments with BID41 and BID31. The experiments with GLN suggest focusing on improving propagation layers by introducing a novel Attention-based Graph Neural Network (AGNN) with a single scalar parameter \u03b2 (t) at each layer. AGNN captures relevance through an attention mechanism that learns which neighbors are more relevant, similar to successful attention mechanisms in summarizing long sentences or images. This simple model with one parameter is crucial for training in a semi-supervised setting with few labels. In a semi-supervised learning setting, a word-embedding layer maps a bag-of-words representation into an averaged word embedding. Attention-guided propagation layers with parameter \u03b2 (t) and propagation matrix P (t) are used to adjust output row-vectors of nodes. The dynamic and adaptive propagation weights more relevant neighbors higher, with the addition of self-loop for node features and hidden states. The output layer includes weights trained on cross entropy loss. The softmax function ensures row-sums to one in the propagation layer. Attention from node j to node i captures relevance based on hidden states. Attentions select neighbors with the same class as more relevant. The AGNN architecture achieves the best performance on citation networks. Attention is computed over a node's neighborhood in AGNN, while other works use attention over all entities to create a \"soft neighborhood\". The AGNN model is tested on semi-supervised learning tasks on benchmark datasets of citation networks. The AGNN model achieves superior performance on citation networks by computing attention over a node's neighborhood. Three datasets are considered, each with different types of feature vectors. The GLN and AGNN models are trained and tested, with validation set labels used for optimizing hyperparameters. In training, hyperparameters like dropout rate, learning rate, and L2-regularization factor are optimized. AGNN uses 16 units in hidden layers and 4 propagation layers for CiteSeer and Pubmed, and 3 propagation layers for Cora. GLN uses 2 propagation layers. Input feature vectors are row-normalized. Average accuracy with standard error over 100 training instances is shown. The model is implemented on TensorFlow BID1. Experiments are detailed in Appendix B with fixed data splits. 20 labeled nodes for each class are used in all experiments. The linear classifier GLN achieves comparable or better performance than GCN on a fixed split of labeled nodes. A novel attention-based model AGNN outperforms all baseline methods on all datasets. Utilizing both graph structure and node features is crucial in semi-supervised learning on graphs. Planetoid BID46 achieved a breakthrough result by significantly improving upon existing methods like DeepWalk and node2vec. BID27 introduced graph neural networks to citation datasets, leading to state-of-the-art performance with GCN. Other variations like MoNet, Graph-CNN, and DynamicFilter followed suit. Bootstrap utilized a Laplacian regularized approach from BID47, achieving high-margin predictions with bootstrapping. In a random dataset split experiment, AGNN consistently outperforms other methods, except for DeepWalk. BID27 conducted a similar experiment with GCN, evenly spreading labeled nodes across topics for accurate results. The study conducted experiments with larger sets of labeled nodes on the Cora dataset using k-fold cross validation. AGNN consistently outperformed other architectures, showing improved performance with larger training sets. Incorporating attention into a model provides interpretation capability. The attention from one node to another gives insights on relevance for classification. Statistics on attention are shown for Cora, CiteSeer, and PubMed datasets. Average attention from one topic to another is defined as an edge-wise relevance score. If no attention is used, typical propagation occurs. The attention in the model provides interpretation capability, measuring the variation of attention from uniform propagation. The relevance score for CiteSeer and Cora datasets shows that attention is learning to prioritize nodes in the same class. The average attention in CiteSeer dataset at the first propagation layer shows that nodes in topic c1 pay more attention to neighbors in topic c2. The most influential relations are HCI\u2192Agents, Agents\u2192ML, Agents\u2192HCI, and ML\u2192Agents. The least influential relations are AI\u2192IR and DB\u2192ML. For Cora dataset, the relevance score at the second propagation layer indicates that CB\u2192PM and PM\u2192CB are the most influential relations. The least influential relations are GA\u2192PM and PM\u2192RL. These relations are estimated based on available datasets from the late 90s to early 2000s for CiteSeer and the 90s for Cora. In analyzing the edges with high and low relevance scores, self-loops are removed and edges are sorted based on relevance score. The top and bottom 100 edges are examined, showing a focus on connecting nodes of the same class. Results on benchmark datasets indicate that the architecture prioritizes attention between nodes of the same class. Additionally, nodes misclassified by GCN but correctly classified by AGNN are further analyzed in the test sets. The attention mechanism in AGNN correctly classifies nodes that were mistaken by GCN, highlighting the importance of local neighborhood contributions. Examples of the network's attention are shown, depicting the true classes of nodes in the target's neighborhood. The size of nodes reflects their attention to the target node, with a focus on self-attention. The attention mechanism in AGNN correctly classifies nodes that were mistaken by GCN, emphasizing the significance of local neighborhood contributions. Examples of the network's attention are illustrated, showing the true classes of nodes in the target's neighborhood. The size of nodes indicates their attention to the target node, with a focus on self-attention. AGNN correctly classifies nodes based on their connections and neighborhood characteristics. The AGNN attention mechanism correctly classifies isolated nodes that GCN fails to classify due to its static propagation. AGNN adapts better to graph topology, giving more weight to the target node and outperforming other methods in semi-supervised classification on citation network datasets. The learned attention provides insights on neighbor influences. AGNN outperforms GCN in classifying isolated nodes by adapting better to graph topology and providing insights on neighbor influences. The attention mechanism in AGNN shows how nodes in the 2-hop neighborhood influence target nodes, allowing the model to find correct labels despite increased complexity. In a semi-supervised setting with limited labeled examples, reducing model complexity is crucial for stable training and higher accuracy. Deeper GCN models are known to be unstable compared to shallower ones. The PubMed dataset used has 3 classes, and hyper-parameters were tuned to maximize validation performance using Adam optimization algorithm with weight decay. The AGNN models used weight decay and dropout in the first and last layers. Hyper-parameters were tuned for different settings and datasets, with a fixed value for the first propagation layer. Trained model parameters were chosen based on validation accuracy, with k-fold cross validation used for selecting the epoch with maximum accuracy. The same hyper-parameters as GCN were used for all experimental settings. In this section, experimental results justify the choice of propagation layers for each dataset. Different datasets require varying numbers of propagation layers for optimal performance. Tables show average testing accuracy for different choices of propagation layers. Performance of GCN on random splits and larger training set dataset splits is provided in the appendix. The experiments were conducted with the same hyper-parameters as chosen by BID27 for the fixed split. Average testing accuracy and standard error over multiple runs were provided in TAB13. Method 3-fold Split and 10-fold split were compared for classification accuracy with larger sets of labelled nodes in Graph-CNN Table 9."
}