{
    "title": "r1lQeoCVu4",
    "content": "Our approach involves teaching an RNN to approximate list-processing algorithms with a small number of training examples by formulating a semantic property that distinguishes common algorithms. This leads to a data augmentation scheme that encourages algorithmic behavior in RNNs for various list-processing tasks. The focus is on capturing symbolic computation and human cognition with neural models. In training recurrent neural networks (RNNs) to approximate symbolic algorithms in various domains, the challenge of sample efficiency remains open. Our goal is to teach RNNs to approximate list-processing algorithms by learning a family of element-wise changes from the training set. This approach aims to capture symbolic computation and human cognition with neural models. Our experiments demonstrate that augmenting the training set for RNNs can improve algorithm approximation from small training sets, sometimes needing only one example per input list length. The failure mode of unaugmented training is highlighted, showing the model behaving like an interpolating lookup table. This approach aims to move away from this behavior towards semantic learning. In this paper, the authors propose a method to distinguish algorithms from lookup tables using semantic properties and data augmentation to guide an RNN towards algorithmic behavior. They present a confusion matrix showing the performance of an RNN trained with ten examples for a copy algorithm task. The concept of parametricity is discussed, where an algorithm is defined as a function that behaves the same way for any input, in contrast to nonalgorithmic functions like lookup tables. The text discusses the concept of parametricity in algorithms, highlighting that algorithms behave consistently for any input, unlike nonalgorithmic functions like lookup tables. It mentions that any polymorphic function commutes with element-wise application of any function, illustrated by an example of doubling elements in a list and reversing the result. This is supported by results in type theory and programming language theory known as \"theorems for free.\" The text discusses the concept of parametricity in algorithms, highlighting that algorithms behave consistently for any input. It mentions that any polymorphic function commutes with element-wise transformations of its inputs. The function drop evens, which removes even elements from a list, is a legitimate algorithm that fails to obey a certain equation. This motivates the hypothesis that an algorithm should have systematicity in its definition. The text discusses the systematicity in defining algorithms, drawing an analogy with math and physics symmetries. It introduces a technique for learning G from training data of function f, using swaps to augment the training set. The approach involves parameterizing each swap and training a classifier to predict their commutativity with f. The text introduces a technique for learning G from training data of function f using swaps to augment the training set. It involves determining if pairs of length-matched lists are related by swaps that commute with f, resulting in positive or negative training examples. This setup allows for O(n^2) examples to learn G, supporting small-sample learning. The technique involves learning G from training data of function f using swaps to augment the training set. Each swap independently contributes to the commutativity of the swap set. The classifier C acts on a swap set by classifying individual swaps and combining embeddings of s and t. Per-sequence classification assumes swaps can be classified independently of the input list. The technique involves using swaps to augment the training set for learning a classifier C. Per-sequence classification predicts the classifier score by combining sequence elements with swap elements. Candidate augmentation examples are generated by pairing swap sets with training pairs. The technique involves using swaps to augment the training set for learning a classifier C. Candidate augmentation examples are generated by pairing swap sets with training pairs, and the augmentation training set consists of the top-scoring candidates for each list length. Four models are compared: per-task and per-sequence augmentation models, random augmentation model, and no-augmentation baseline, evaluated on list-processing functions categorized as polymorphic, token-dependent, and order-dependent. The study evaluated the sample efficiency of LSTM models trained on even-length lists of integers. Different list-processing functions were categorized as polymorphic, token-dependent, and order-dependent. Training was done on lists of lengths 2, 4, 6, and 8, with testing on lengths 2-8. The models were evaluated on learning problems with varying numbers of input-output examples per input list length. Results showed that LSTM models with two hidden layers of 128 units outperformed the non-augmented baseline on polymorphic target functions. However, random augmentation was not sufficient for good performance on non-polymorphic token-dependent functions. Per-sequence augmentation was suggested to outperform per-task augmentation for order-dependent functions. The two learned models achieve equal accuracy, with per-sequence augmentation possibly needing more data to train correctly. Learned augmentation models outperform the non-augmented baseline on certain tasks, but the advantage is less pronounced on other function types. Future directions include pairing augmentation schemes with different models and extending techniques to domains beyond list processing."
}