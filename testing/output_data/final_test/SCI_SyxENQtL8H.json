{
    "title": "SyxENQtL8H",
    "content": "In this contribution, a whole brain encoding model of auditory perception was estimated using fMRI data from 16 subjects watching a movie. Feature vectors aligned with audio timing were extracted from a Deep Neural Network, and encoding models were successfully trained to predict brain activity in specific brain regions. The contribution extends previous attempts by using a generic DNN to model brain activity in auditory and language processing regions. Incorporating machine learning in neuroscientific discovery aims at establishing predictive models for generalization to unseen data. In this study, a pretrained network was tested to estimate encoding models for naturalistic auditory perception using fMRI data from the ds001110 dataset. Functional and anatomical MRI data were collected from 36 subjects watching a 20-minute movie in an fMRI scanner. In this study, fMRI data was collected from 16 subjects watching the TV show \"Sherlock\". The audio track was extracted and feature vectors were obtained from SoundNet layers to align with fMRI temporal resolution. The study collected fMRI data from 16 subjects watching \"Sherlock\" and extracted feature vectors from SoundNet layers. Hierarchical clustering was used to parcellate each brain into 500 regions of interests. Encoding models were estimated for each subject and layer of SoundNet using neural networks with varying hidden layer neurons. ReLu activation was used for the hidden layer and linear activation for the output layer. Cross-validation was performed with four folds without shuffling the data. For each training fold, 10% of the data was kept for validation without shuffling. Adam optimizer with batch size 50 was used, MSE as loss function, and early stopping criterion based on validation MSE. R2 score on test set evaluated prediction quality of fMRI data. Control analysis involved 100 null encoding models from untrained SoundNet. This estimated chance level of dataset and gain from pretrained network. The null models and first four layers of SoundNet did not yield significant training. The focus was on layers conv5, conv6, and conv7, with 1000 neurons in the hidden layer enabling successful training. Results showed higher R2 scores with more neurons. The study focused on layers conv5, conv6, and conv7 of SoundNet with 1000 neurons in the hidden layer. The second fold consistently yielded low R2 scores for conv5 and conv6, while conv7 showed better generalization. Brain activity in ROIs could be predicted with R2 > 0.25 for all sixteen subjects, with conv7 being the most predictive layer for 14 subjects. The study found that brain activity in regions associated with auditory processing was linked to the last layers of SoundNet, particularly conv7. Additionally, activity in the left dorsolateral prefrontal cortex, related to verbal encoding, was predicted by conv5 and conv7. Less than five subjects showed activity in medial regions of the Default Mode Network. The study trained encoding models on individual subjects using SoundNet's deepest layers to predict brain activity in language-related networks. However, limitations include focusing on auditory stimuli and not capturing other brain functions like visual perception and cognitive processes. This resulted in an R2 value of 0.5 at best. The study suggests using more general feature extractors for a richer stimuli representation in brain activity modeling. Brain parcellations were estimated on single subject data with limited MRI time, potentially affecting ROI reliability. The relationship between encoding models and SoundNet layers was unclear, suggesting the need for fine-tuning or retraining for optimization. Temporal dynamics of feature vectors and fMRI data were ignored in the approach. In future studies, recurrent neural networks and graph representation learning will be considered to address temporal dynamics and dependencies between ROIs in brain connectivity."
}