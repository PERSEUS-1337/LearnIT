{
    "title": "S1x63TEYvr",
    "content": "Multi-hop text-based question-answering is a challenge in machine comprehension, requiring integration of facts from multiple passages. The Latent Question Reformulation Network (LQR-net) is a novel architecture designed for reasoning tasks, consisting of reading and reformulation modules. The reading module creates a question-aware document representation, while the reformulation module updates the question for each hop. Evaluation was done on the HotpotQA dataset to assess multi-hop reasoning capabilities. Our model outperforms the best published models in Exact Match (EM) and $F_1$ score on the public leaderboard. The ability to extract relevant information from large text corpora is a major challenge. Question-answering tasks evaluate neural architectures' reading capabilities, focusing on single text pieces. Current datasets emphasize detecting patterns and named entities but lack actual reasoning capabilities. Machine comprehension models need to gather and compose evidence for reasoning. Our proposed neural architecture for machine reading aggregates information from multiple paragraphs by sequentially and in parallel collecting and reformulating key details. The model utilizes token-level attention modules to answer questions and features an input-length invariant question representation. The curr_chunk introduces a dynamic max-pooling layer for question representation and an extractive reading-based attention mechanism. It showcases the model's advantages on the HOTPOTQA dataset and outlines the paper's structure. The prev_chunk discusses a neural architecture for machine reading that aggregates information from multiple paragraphs using token-level attention modules. The curr_chunk discusses the task of extractive machine reading, focusing on explainable multi-hop reasoning using the HOTPOTQA dataset. It involves answering questions by extracting information from two paragraphs out of a set of ten, with the rest being distractors. The input document consists of paragraphs from different Wikipedia articles. The curr_chunk discusses the HotpotQA dataset, which includes extractive questions with answers extracted from paragraphs and yes/no questions. It involves categorizing answers, predicting answer spans, and identifying supporting sentences. The approach is evaluated on the fullwiki configuration without supporting documents. The HotpotQA dataset involves answering questions from the entire Wikipedia corpus without supporting documents. Two major reasoning capabilities required are sequential reasoning and parallel reasoning, as illustrated in examples from HOTPOTQA. These reasoning patterns should influence the design of neural architectures. The Latent Question Reformulation Network (LQR-net) is a multi-hop model designed with four modules: encoding, reading, question reformulation, and answering. It utilizes K parallel heads and T sequential reading modules to update the question representation for multi-hop reasoning. The Latent Question Reformulation Network (LQR-net) is a multi-hop model with four modules: encoding, reading, question reformulation, and answering. The reading module computes a question-aware representation of the document, while the reformulation module updates the question representation in a latent space. Multiple heads, similar to the Transformer architecture, perform iterative mechanisms in parallel to compute independent reformulations. The final document representations are aggregated before being fed to the answering module for predicting the answer and supporting facts. The Latent Question Reformulation Network (LQR-net) consists of four modules: encoding, reading, question reformulation, and answering. The model utilizes K independent reading heads to process the document and question simultaneously. Each paragraph in the document is encoded using the pre-trained BERT model, and the token representations are concatenated to create a representation for the document. The Latent Question Reformulation Network (LQR-net) utilizes a Bidirectional Gated Recurrent Unit (BiGRU) to produce final representations of the document and question. An interpolation layer is used to compute the first representation of the question. The model consists of T hops of reading to extract relevant information from the document for the current question reformulation. The Latent Question Reformulation Network (LQR-net) utilizes a Bidirectional Gated Recurrent Unit (BiGRU) to produce final representations of the document and question. At step t, the module computes a question-aware representation of the document through document-question attention and document self-attention mechanisms. This attention mechanism involves constructing an interaction matrix between the document and the reformulated question, followed by computing document-to-question and question-to-document attentions to generate the question-aware representation of the document. The attention mechanism in the Latent Question Reformulation Network involves token-level attention for a finer-grained document representation. Self-contextualization is applied between the question-aware document representation to handle long-range dependencies, resulting in useful outcomes in experiments. The Latent Question Reformulation Network utilizes a reformulation module to update the question based on previous attention modules and document encoding. Reading-based attention is computed to generate attention vectors over the document, aiding in predicting answer spans. The Latent Question Reformulation Network uses attention values to re-weight document tokens and applies Dynamic Max-Pooling to capture important elements of the input sequence. This layer partitions the sequence into parts and applies max-pooling to create a fixed dimension matrix, preserving the global structure of the document. The Latent Question Reformulation Network utilizes Dynamic Max-Pooling to create a fixed dimension matrix based on the input sequence length. The answering module consists of BiGRUs followed by fully connected layers to supervise supporting facts, answer probabilities, and answer classification. Supporting facts are predicted using a sentence-based representation of the document. The model utilizes a multi-head configuration with independent parallel heads to process document representations. It optimizes on three subtasks (supporting facts, span position, classifier) by minimizing a combination of losses. The supporting facts loss is defined based on the number of sentences in the document. The task of selecting answers in multi-hop reading datasets is weakly supervised. The model tags the start and end positions of all occurrences of the answer in the supporting facts as valid solutions. The training loss is defined based on the number of occurrences of the answer in the context and the question type label index. The training loss is defined based on the number of occurrences of the answer in the context and the question type label index. The model is composed of 3 parallel heads with two reading modules and one reformulation module each. The hidden dimension of all GRUs is set to 80, and M = 100 is used to allocate resources. Additional \"easier\" examples are created by combining gold paragraphs with randomly selected paragraphs from the dataset. The model uses a hidden dimension of 80 for all GRUs and allocates space for questions and reformulations. Pre-trained BERT-base-cased model is utilized for embedding representations. The network is optimized using the Adam optimizer with defined parameters. Performance of the LQR-net on the HOTPOTQA dataset is presented in Table 1, showing strong results in answer prediction task, outperforming the current best model. The LQR-net model achieves competitive performance on the evidence extraction task, improving the best published approaches by 2.9 points on EM and 3.9 points on F1 score. An ablation analysis shows that the sequentiality and multiple parallel heads in the model are critical components, with a significant decrease in F1 score when the model lacks the capability to reformulate questions. The impact of parallel heads is also noted to be limited. The impact of parallel heads in the model is significant, even though it is less than sequentiality. Weak supervision of the answer involves labeling all answer occurrences in supporting facts as positive. Comparing this to labeling only the first occurrence as positive decreases the joint F1 score. The self-attention layer is found to be essential in the reading process. The self-attention layer is crucial in the reading process, as omitting it leads to an 8.3 point decrease in the F1 score. This layer allows for long-range information propagation between paragraphs. Unlike previous methods, it does not rely on handcrafted word relationships. Reducing the question representation to a single vector results in a 13.3 point drop in the F1 score, emphasizing the importance of maintaining the question representation as a matrix. The model is integrated into an open-domain question answering pipeline without associated supporting documents. The text discusses a two-stage process for answering complex questions using a paragraph retriever and an LQR model based on Wikipedia data. The approach is evaluated on the HotpotQA dataset, focusing on retrieving relevant paragraphs and their linked documents from Wikipedia. The study evaluates a two-stage process for answering complex questions using a paragraph retriever and an LQR model based on Wikipedia data. Results show strong performance on the HotpotQA dataset compared to other models. The model reformulates questions in a latent space, making it challenging to visualize, but the effectiveness is assessed by analyzing the evolution of probabilities across two hops of the model. The model displays reading-based attention for answering bridge questions, focusing on natural reasoning paths. Before the first reformulation module, attention is on the initial step of reasoning, such as focusing on the writer's name before the award description. Similarly, attention shifts to different aspects of the question in subsequent steps. The National Archives and Library of Ethiopia's performance is compared with other models on the development set of HOTPOTQA in the fullwiki setting. The model's Exact Match and F1 scores are evaluated against published models, showing distribution of probabilities for each word in the predicted span. The National Archives and Library of Ethiopia's performance is compared with other models on the development set of HOTPOTQA in the fullwiki setting. The model's Exact Match and F1 scores are evaluated against published models, showing distribution of probabilities for each word in the predicted span. In the answering module, the model focuses on Addis Ababa and its population, displaying visualizations of answer probabilities. Limitations include model failures in required reasoning, comparing properties, and meeting all question requirements. The analysis of errors in question-answering tasks revealed that only 3% of cases had answers selected from distractor paragraphs instead of gold ones. The model successfully identifies relevant paragraphs even among similar documents. It does not confuse binary yes/no answers with text spans. Multi-hop Machine Comprehension has gained popularity for assessing reading comprehension abilities. Large scale datasets like CNN/Daily Mail, SQuAD, and MSMARCO have led to the development of machine reading models with multiple attention layers. While most models focus on answering questions from a single paragraph, there are attempts at multi-documents question-answering tasks. QAngaroo dataset evaluates multi-hop reading architectures, but state-of-the-art models tend to use the dataset structure for input. Different approaches have emerged recently. Different approaches have been developed for HOTPOTQA, focusing on challenges of the dataset. Nishida et al. (2019) highlight the similarity between evidence extraction and extractive summarization tasks. Other models, like Qiu et al. (2019), integrate graph reasoning with BERT NER models. Question reformulation for multi-hop open-domain question answering has also been explored in related papers. Das et al. (2019) and Feldman & El-Yaniv (2019) propose frameworks for document retrieval and question reformulation using reinforcement learning. Memory networks, as designed by Weston et al. (2015b), Sukhbaatar et al. (2015), and Miller et al. (2016), utilize attention mechanisms to gather information iteratively from memory cells. These models have been applied to reading from sentences, paragraphs, and knowledge bases. The transformer architecture, introduced by Vaswani et al. (2017) for machine translation, includes multi-head attention layers in both encoder and decoder modules. This allows concurrent access to information, unlike traditional models with a single vector controller and simple dot-product attention mechanism. Our multi-hop reading model is designed for question-answering tasks, utilizing a sequence of token-level attention mechanisms to extract relevant information from paragraphs and update a latent representation of the question. The model achieves competitive results on the HOTPOTQA reasoning task, outperforming the current best approach. The multi-hop reading model outperforms the current best approach in the reasoning task, showing improved performance in Exact Match and F1 score. Sequential attentions analysis can offer human-interpretable reasoning chains. Examples from the HOTPOTQA development set illustrate the evolution of word probabilities in predicted spans. The model stops at the first hop of required reasoning, failing to compare two properties."
}