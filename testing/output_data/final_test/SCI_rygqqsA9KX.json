{
    "title": "rygqqsA9KX",
    "content": "Learning multimodal representations is a complex research problem due to multiple sources of information. Two key challenges include learning intra-modal and cross-modal interactions and being robust to missing or noisy modalities. The proposed model optimizes a joint generative-discriminative objective across multimodal data and labels, factorizing representations into multimodal discriminative and modality-specific generative factors. Discriminative factors are shared across modalities for tasks like sentiment prediction, while generative factors are unique to each modality. Multimodal machine learning involves learning from data across multiple modalities for tasks like sentiment prediction. The model presented in the study learns meaningful multimodal representations and achieves state-of-the-art performance on six datasets. It demonstrates flexible generative capabilities and can reconstruct missing modalities without affecting performance. The factorized representations help understand interactions influencing multimodal learning in various real-world applications. In this study, the focus is on learning rich representations from multiple modalities for analyzing multimedia content. The challenges include learning complex intra-modal and cross-modal interactions for prediction and ensuring robustness to missing or noisy modalities during testing. The proposed approach optimizes a joint generative-discriminative objective across multimodal data and labels to enhance the model's predictive capabilities. The Multimodal Factorization Model (MFM) factorizes multimodal representations into discriminative and generative factors. Discriminative factors are shared across modalities for tasks, while generative factors are unique to each modality for inference and handling noisy data. This approach aims to focus on learning from subsets of joint information, contrasting with jointly learning a single factor for generative and discriminative tasks. The Multimodal Factorization Model (MFM) defines a joint distribution over multimodal data, considering both generative and discriminative aspects. Through experiments, MFM shows improved multimodal representations with state-of-the-art performance on time series datasets and flexible generation capabilities. Reconstruction of missing modalities is also possible. The Multimodal Factorization Model (MFM) allows for reconstruction of missing modalities without affecting discriminative performance. It uses conditional independence assumptions and proposes a factorization over joint distribution of multimodal data. An approximate inference algorithm based on minimizing Wasserstein distance is used for inference. The Multimodal Factorization Model (MFM) derives its objective by approximating the joint-distribution Wasserstein distance through a mean-field assumption. It factorizes multimodal representations into discriminative and generative factors using a Bayesian network structure. Factors are generated from independent latent variables, contributing to the generation of labels and multimodal data. The Multimodal Factorization Model (MFM) approximates joint distribution Wasserstein distance through mean-field assumption. It consists of encoder and decoder modules for approximate inference and generative modeling using autoencoding structures like Variational Autoencoders (VAEs) and Wasserstein Autoencoders (WAEs). The Multimodal Factorization Model (MFM) utilizes Wasserstein Autoencoders (WAEs) for better latent factor disentanglement and sample generation quality. A variant is proposed to handle factorized joint distributions over multimodal data, incorporating nonlinear mappings in the encoder and decoder. The Multimodal Factorization Model (MFM) uses Wasserstein Autoencoders (WAEs) for improved latent factor disentanglement and sample generation quality. The decoder defines the generation process from latent variables using deterministic functions parametrized by neural networks. The cost function allows minimizing the 2-Wasserstein distance on static and time series data like text, audio, and videos. The model incorporates nonlinear mappings in the encoder and decoder for factorized joint distributions over multimodal data. The Multimodal Factorization Model (MFM) utilizes Wasserstein Autoencoders (WAEs) for enhanced latent factor disentanglement and sample generation quality. To relax the constraint on Q Z = P Z in Proposition 1, a generalized mean field assumption is made on Q based on conditional independence. This assumption allows for the inference of Z y to depend on all modalities X 1\u2236M, while the inference of Z ai depends only on the specific modality X i. A penalty term is added to the objective to find Q(Z \u22c5) \u2208 Q closest to prior P Z. The Multimodal Factorization Model (MFM) uses Wasserstein Autoencoders (WAEs) for improved latent factor disentanglement and sample generation quality. A penalty term is included in the objective to find Q(Z \u22c5) closest to prior P Z, enforcing the constraint Q Z = P Z. The objective involves a hybrid generative-discriminative optimization over multimodal data, with the first loss term focusing on data reconstruction and the second term on discrimination. The neural architecture of MFM is depicted in a figure. The Multimodal Factorization Model (MFM) utilizes Wasserstein Autoencoders for improved latent factor disentanglement and sample generation quality. A key challenge in multimodal learning is handling missing modalities, which can be addressed by inferring the missing modality conditioned on observed modalities. The inference process of MFM can be adapted using a surrogate inference network to reconstruct the missing modality. The implementation choices for the MFM neural architecture are discussed, focusing on inferring latent codes rather than the entire modality in the presence of missing modalities. The MFM neural architecture implementation choices involve using CNNs and FCNNs for multimodal fusion, with MFN for time series datasets. LSTM networks are used for various functions, and experiments are designed to show MFM's discriminative, generative, and interpretable capabilities. Details can be found in the appendix and code is available online. In experiments, MFM is tested on synthetic image datasets to assess discriminative and generative capabilities. Real-world video datasets are then used to evaluate MFM's performance against baselines, analyze design components, test robustness to missing modalities, and interpret learned representations. In this section, MFM is studied on a synthetic image dataset combining SVHN and MNIST modalities. 100,000 image pairs with the same labels are randomly paired, creating the SVHN+MNIST dataset. MFM shows improved multimodal representations in both classification and generation tasks on SVHN+MNIST. MFM outperforms MM in classification and generation tasks on SVHN+MNIST dataset. It shows flexible generation of images based on labels and styles, suggesting improved factorized representations. The transition to more challenging multimodal time series datasets is discussed next. The datasets consist of monologue videos with features extracted from language. The videos are segmented and annotated for 6 emotions. Results are reported using metrics like multiclass accuracy, F1 score, and Pearson's correlation. MFM outperforms existing multimodal prediction methods across all six datasets, achieving state-of-the-art or competitive results. The multimodal discriminative factor F y in MFM has successfully learned more meaningful representations, highlighting the benefit of factorized multimodal representations for discriminative tasks. MFM is model-agnostic and shows consistent improvements in discriminative performance across different encoders. Ablation studies using models M {A,B,C,D,E} analyze the effects of using multimodal discriminative factors and hybrid generative-discriminative objectives. The text discusses different variants of a model (M A, M B, M C, M D, M E, MFM) that incorporate generative-discriminative factors and modality-specific factors for multimodal tasks like modality reconstruction and label prediction. The models vary in their approach, with some focusing on discriminative factors, others on generative-discriminative objectives, and some using separate generative and discriminative factors. The study highlights the benefits of factorized multimodal representations for discriminative tasks. The text discusses the benefits of factorizing multimodal representations into discriminative and generative factors for tasks like sentiment prediction and modality reconstruction. It also evaluates the performance of the model in the presence of missing modalities using a surrogate inference model. The study compares a generative model and a discriminative model with a multimodal factorization model (MFM) that outperforms both in modality reconstruction and sentiment prediction. MFM with missing modalities performs similarly to MFM with all modalities observed, indicating robust representation learning. Discriminative performance is most affected by missing language modalities. The study shows that discriminative performance is most impacted by missing language modalities, while sentiment prediction is more resilient to missing acoustic and visual features. Reconstructing low-level acoustic and visual features is easier than high-dimensional language features with semantic meaning. Two interpretation methods are used to analyze the influence of individual factors in multimodal prediction and generation. The study uses interpretation methods to analyze the contribution of each modality in multimodal representations. Language modality is most informative for sentiment prediction, followed by the acoustic modality. A gradient-based interpretation method is used to analyze the contribution of each modality for every time step in multimodal time series data. The study uses interpretation methods to analyze the contribution of each modality in multimodal representations, measuring the gradient of generated modalities with respect to target factors. The gradient measures the influence of changes in factors on the generation of sequences, as observed in multimodal communicative behaviors indicative of speaker sentiment. Generative representation learning models interactions between modalities by capturing the joint distribution P(X1, \u22ef, XM) using various methods like undirected graphical models, directed graphical models, or neural networks. Some approaches compress multimodal data into lower-dimensional feature vectors for efficient use. Approaches compress multimodal data into lower-dimensional feature vectors for discriminative tasks by factorizing representations into generative and discriminative components. This factorized representation learning resembles disentangled data representations, improving performance on various tasks. Several methods involve specifying latent attributes to control data variations and learning latent variables through supervised training. The Multimodal Factorization Model (MFM) is proposed for learning multimodal representations by factorizing them into multimodal discriminative factors and modality-specific generative factors. This approach differs from previous methods by focusing on factorizing latent variables in multimodal data to maximize mutual information. The Multimodal Factorization Model (MFM) focuses on factorizing multimodal representations into discriminative and generative factors. It achieves competitive results on various datasets and allows for data generation based on factorized variables. Future work includes exploring extensions for video generation and unsupervised learning. The model sheds light on the benefits of learning factorized multimodal representations. The proof involves defining Wasserstein distance between distributions and introducing Lemmas to support the inference. The relationship between P(X,Y) and P(X,\u0176) is established through deterministic functions, leading to the conclusion that P(X,Y,Z) = P(X,Y,X,\u0176). This relationship is crucial for rewriting the equation with proposed inference distribution Q(Z|X). The proof involves defining Wasserstein distance between distributions and introducing Lemmas to support the inference. The relationship between P(X,Y) and P(X,\u0176) is established through deterministic functions, leading to the conclusion that P(X,Y,Z) = P(X,Y,X,\u0176). This relationship is crucial for rewriting the equation with proposed inference distribution Q(Z|X). The proof for Eq. equation 8 involves showing Dirac distributions for P(X 1\u2236M ,\u0176 Z = z) for all z \u2208 Z, followed by applying the tower rule of expectation and conditional independence properties. Various baseline models are referenced for detailed descriptions. The EF-SLSTM model uses a single LSTM on concatenated multimodal inputs, along with stacked and bidirectional versions. Variants of EF-HCRF include EF-LDHCRF and EF-HSSHCRF, which use latent variables to learn hidden dynamics. Variants of MV-HCRF, such as MV-LDHCRF, extend the model for multi-view data. MV-LDHCRF is a variation of the MV-HCRF model that uses LDHCRF instead of HCRF. MV-HSSHCRF extends EF-HSSHCRF by performing Multiview hierarchical sequence summary representation. Results for multimodal speaker traits recognition, sentiment analysis, and emotion recognition on various datasets show that MFM consistently achieves state-of-the-art or competitive results. The design of MFM successfully learns more meaningful representations for discriminative tasks. The text discusses the extraction of multimodal features for language, visual, and acoustic data to create time-aligned representations for discriminative tasks. Language features are converted using pre-trained word embeddings, visual features include emotions and facial action units, and acoustic features consist of various parameters. The granularity of input is at the word level to ensure alignment across modalities. The granularity of input is at the word level for alignment with audio using P2FA BID47. Expected feature values are used for visual and acoustic features. Updates were made to datasets like ICT-MMMO, YouTube, and MOUD due to sampling rate discrepancies. Experiments were conducted using the latest dataset versions available on GitHub. Baseline models were retrained for fair comparison. The text discusses the use of the normalized Hilbert-Schmidt Independence Criterion as an approximation for mutual information measures in analyzing time series data with varying time steps. Data augmentation or alternative kernel choices, such as the Global Alignment Kernel, are considered for accurate analysis. Averaging over time series data is performed before calculating the kernel score with the RBF kernel. The text discusses averaging over time series data with the RBF kernel bandwidth set at 1.0. It explores the language, visual, and acoustic modalities for personality traits prediction, finding language to be the most informative. Non-verbal behaviors are noted to be informative of personality traits. MFM reconstructs x i using convolutional+fully-connected layers for encoder and deconvolutional+fully-connected layers for decoder. Different convolutional layers are applied on SVHN and MNIST images to learn generative factors. Multimodal-discriminative factor is learned by concatenating features from two convolutional layers. This factor is used to predict labels and generate digits. The Memory Fusion Network (MFN) is used as the encoder in the multimodal time series data model. LSTM networks are used to parametrize functions for encoding and decoding, while FCNNs are used for other functions. The model addresses missing modalities through surrogate inference. The surrogate inference model infers latent codes from present modalities to reconstruct missing modalities or predict labels. Compared to MFM, our model has differences in prior matching, inference network, discriminative objective, and multimodal fusion. Fine-tuning with a classifier is done for fair comparison. Experimental results on various datasets show that MFM outperforms \u03b2-VAE in terms of performance across different metrics."
}