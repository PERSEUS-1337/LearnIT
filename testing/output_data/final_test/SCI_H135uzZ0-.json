{
    "title": "H135uzZ0-",
    "content": "The state-of-the-art for mixed precision training involves low precision floating point operations, particularly FP16 accumulating into FP32. Research in low and mixed-precision Integer training has focused on non-SOTA networks or small datasets. This work trains visual understanding neural networks on ImageNet-1K using Integer operations on General Purpose hardware, specifically Integer Fused-Multiply-and-Accumulate operations. A shared exponent representation of tensors and a Dynamic Fixed Point scheme suitable for neural network operations are proposed. The text discusses the development of efficient integer convolution kernels for neural networks, achieving high accuracy with INT16 training on ImageNet-1K dataset using SOTA CNNs. It highlights the interest in half-precision arithmetic for deep learning training, with a 1.8X improvement in training throughput compared to FP32 counterparts. Half-precision training offers potential speedup of training compared to FP32, with options for FP16 or INT16. INT16 provides higher precision but lower dynamic range than FP16, leading to different residual errors. There are algorithmic and semantic differences between the two data types. When discussing half-precision training, various algorithmic and semantic differences exist between FP16 and INT16 data types. The selection of tensor representation, multiply-and-accumulate operation semantics, down-conversion schemes, scaling techniques, and overflow management methods all play a crucial role in achieving state-of-the-art accuracy. Failure in selecting the right combination of these factors can lead to unsuccessful half precision training. This work describes a mixed-precision training setup using INT16 tensors with shared tensor-wide exponent and the potential for sub-tensor wide exponents. The text discusses the use of INT16 tensors with shared tensor-wide exponent and potential for sub-tensor wide exponents in mixed-precision training. It includes instructions for multiplying INT16 numbers, down-convert schemes, overflow management, and specialized low-precision instructions for operations like fused-multiply-and-accumulate (FMA) to enhance performance in neural network training dominated by GEMM-like, convolution, or dot-product operations. The proposed integer-16 based mixed-precision training strategy achieves performance speedups by migrating compute intensive operations to half precision, maintaining precision critical operations in single precision. This approach allows for training visual understanding CNNs with Top-1 accuracies on the ImageNet-1K dataset that match or exceed single precision results, without changing hyper-parameters. The proposed integer-16 based mixed-precision training strategy achieves high accuracy on ImageNet-1K dataset without changing hyper-parameters. Results show 75.77% Top-1 accuracy for ResNet-50, surpassing previous half-precision training results. Additionally, state-of-the-art accuracy is achieved with int16 training on GoogLeNet-v1, VGG-16, and AlexNet networks. The paper discusses literature on half-precision training, dynamic fixed point format, kernels, training operations, and presents experimental results. Reduced precision data representations, including 16-bit floating point storage for activations, weights, and gradients, have been used in deep learning training applications with minimal loss compared to baseline results. FP16/FP32 mixed precision requires loss scaling to achieve near-state-of-the-art accuracy. Custom fixed point representations offer increased precision and dynamic range, making them more robust and accurate for deep learning training. BID19 demonstrated up to 4\u00d7 improvement over floating point implementations on general purpose CPU hardware using dynamically scaled fixed point representation. BID5 conducted a study on low precision fixed point computation for deep learning, successfully training smaller networks using 16-bit fixed point on specialized hardware. Custom fixed point schemes with <16-bit precision have been used in various publications, such as BID0's dynamical fixed point format (DFXP) with low precision multiplications. BID1 proposed training with only binary weights, while BID7 extended this to include binary activations. BID8 suggested training with activations and weights. In contrast to previous studies on low precision fixed point computation for deep learning, which focused on smaller networks, this study proposes a dynamic fixed point representation for training large networks on general purpose hardware. The proposed method aims to achieve numerical parity with FP32 and outperform FP16 across a wide range of applications. The study introduces Dynamic Fixed Point (DFP) tensors for training large networks on general purpose hardware. DFP-16 data type offers a balance between float and half-float in terms of precision and dynamic range, with higher compute density and effective precision compared to half-floats. The DFP format offers a higher dynamic range and precision compared to half-floats, using fine-grained quantization with Blocked-DFP representation. It utilizes a single shared exponent for each tensor and performs arithmetic operations on DFP tensors using standard integer hardware. Software manages exponent handling and precision management for DFP tensors. The DFP format uses shared exponents for arithmetic operations on DFP tensors and data conversions between DFP and float. Converting floating point tensors to DFP involves deriving the shared exponent from the exponent of the absolute maximum value of the tensor. Multiplying two DFP-16 tensors results in a 32-bit tensor with a new shared exponent, while adding two DFP-16 tensors also produces a 32-bit tensor. Neural network training involves forward propagation, back-propagation, weight gradient computation, and solver operations on mini-batches of data points in a CNN. Down-Conversion scales DFP-32 output to DFP-16 for the next layer by right-shifting the 32-bit tensor to fit into a 16-bit tensor. The process uses a shared exponent for arithmetic operations and data conversions in the DFP format. In this work, a method using INT16 operations for convolutions and GEMM in neural network training is proposed. The approach involves dynamic fixed-point to floating-point conversions and optimized kernels for convolutions. The mixed precision training scheme includes FP, BP, and WU convolution kernels. The core compute kernels in this scheme are the FP, BP, and WU convolution functions which convert input DFP-16 tensors to FP32 tensors. Quantization steps follow the FP and BP operations, converting FP32 tensors to DFP-16 tensors for the next layer. The WU step is followed by Stochastic Gradient Descent (SGD) using FP32 tensors for weight-gradients and weights, producing updated weight tensors. Batch-norm layers involve loading DFP-16 tensors into registers before processing. The efficient implementation of core compute kernels using Integer FMA instruction sequence, specifically AVX512_4VNNI instruction, is discussed. The FPROP convolution kernel is written using AVX512_4VNNI instruction, capturing the 2-way horizontal accumulation operation in the data layout of weights. The dimensions of activations and weights are specified as N, C/16, H, W, 16 and N, respectively. The dimensions of activations and weights are specified as N, C/16, H, W, 16 and N, respectively. Multiplication of two INT16 numbers can result in a 30-bit outcome, causing an overflow of the INT32 accumulator. One way to prevent overflows is to convert an INT32 intermediate output into FP32 before accumulation. To prevent overflows in accumulation, INT32 results are converted to FP32 using specific instructions. The scale used is 2 (Einp+Ewt), and partial accumulations into INT32 are done for short chains. This strategy helps in managing overflows and improving performance. In the loop, there are AVX512_4VNNI instructions and VCVTINTFP32 instructions. The instruction overhead from overflow management varies between <1% to at most 3%. The accumulate chain length is optimized for instruction overheads and cache/instruction reuse, aiming for more than 200. Inputs are shifted by 1-bit for all convolutions to prevent overflow, resulting in a DFP15 representation of tensors. In experiments with mixed precision DFP16, several CNN models achieved state-of-the-art accuracy for ImageNet-1K classification. The models were trained from scratch using synchronous SGD on multiple nodes, with the first convolution layer and fully connected layers in FP32. Results showed top-1 accuracy of 75.77% and top-5 accuracy of 92.84% for ResNet-50. DFP16 achieved the highest accuracy on the ImageNet-1K classification task compared to other forms of reduced precision training. The validation/test loss closely tracks the training loss with DFP16, resulting in better generalization and accuracies. The conversion of convolution kernels from FP32 to DFP16 led to a 1.5\u00d7 speedup in overall performance. DFP16 achieved the highest accuracy on ImageNet-1K classification task. Conversion of convolution kernels to DFP16 led to 1.5\u00d7 speedup in overall performance. Memory prefetch optimization improves DFP kernels by 20%. Batchnorm computation is 2\u00d7 faster with DFP16 due to smaller memory footprint. Fusion of ReLU and EltWise layers with batchnorm avoids additional memory passes. Memory bandwidth optimizations are crucial with specialized compute accelerators. With optimizations, training throughput reaches 317 images/sec, 1.8X speedup over FP32 for ResNet-50. Mixed Precision DFP16 shows 1.6X speedup compared to FP32, maintaining high accuracy and potential 2\u00d7 savings in computation, communication, and storage. Dynamic fixed point representation scheme proposed for reduced precision INT-based training on large networks/data-sets. The DFP solution enables training of CNNs like ResNet-50, GoogLeNet-v1, VGG-16, and AlexNet with mixed precision DFP16 for ImageNet-1K classification. Future plans include extending this method to other network types like RNNs, LSTMs, GANs, and various applications."
}