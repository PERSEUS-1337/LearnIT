{
    "title": "HJeIrlSFDH",
    "content": "Lexical ambiguity is a challenge for machine translation systems, despite the use of neural networks and attention mechanisms. A study combines BERT language model and Transformer translation model to improve English-Japanese translation. The proposed models outperform vanilla Transformer and Google Translate, with Transformer_BERT-WE being particularly effective. The Transformer_BERT-WE model outperforms the vanilla Transformer in English-Japanese translation, showing that using contextualized word embeddings from BERT can improve translation quality by solving lexical ambiguity. Lexical ambiguity is a challenge for machine translation systems, even with the use of RNN and attention mechanisms. To address this, a combination of BERT language model and Transformer translation model has shown to be more effective in translating ambiguous sentences than traditional models like Google Translate. The Transformer_BERT-WE model improves translation quality by solving lexical ambiguity in English-Japanese translation. The Transformer BERT\u2212WE model achieves higher BLEU scores in general translation, showing that contextualized word embeddings from BERT can improve translation quality. Machine translation has evolved from RNN-based models to attention mechanisms, with seq2seq models playing a crucial role in neural machine translation. The Transformer architecture, based on attention mechanism, is the standard model for machine translation tasks. It has paved the way for advanced language models like BERT and GPT-2. Seq2seq models have evolved from RNN-based to transformer models, addressing lexical ambiguity challenges in machine translation. The curr_chunk discusses how the number of senses for each word impacts the performance of seq2seq models in translation tasks. It also introduces the BERT language model as a solution to lexical ambiguity issues. Additionally, it proposes context-aware machine translation architectures that integrate pretrained BERT and Transformer models. Evaluation results of the models are presented in section 6.3. Neural machine translation uses a seq2seq model with an encoder and decoder, typically built with RNN variants like LSTM or GRU. The encoder converts the input sequence into a fixed-sized vector, while the decoder generates the target sequence. The model aims to maximize conditional probability for translation tasks. The attention mechanism proposed by Bahdanau et al. (2014) is a significant improvement to seq2seq models in neural machine translation. It allows the decoder to selectively focus on all positions in the encoder, boosting the model's capability to learn long-term dependencies. The mechanism involves mapping a query and key-value pairs to an output vector, with the output vector being a weighted sum of the value vectors computed by an alignment function. The Transformer model, introduced by Vaswani et al. (2017), relies on self-attention and eliminates recurrence. This architecture allows each position to attend to all others in the sequence, outperforming RNN-based models in performance and training efficiency. The Transformer model, introduced by Vaswani et al. (2017), relies on self-attention and eliminates recurrence. It outperforms RNN-based models in performance and training efficiency. The Transformer consists of an encoder and a decoder, each with N = 6 identical layers. The encoder has a multihead attention mechanism and a fully connected feed-forward network in each layer. The decoder has a masked multihead attention, a multihead attention mechanism, and a fully connected feed-forward network in each layer. The Transformer model utilizes a multihead attention mechanism with outputs of dimension 512. Each attention head produces output vectors of dimension d v, which are concatenated and linearly projected to produce final output vectors of dimension 512. The model also includes a fully connected feed-forward network with hidden layer dimensions of 2048. The Transformer model utilizes multihead attention with outputs of dimension 512 and a fully connected feed-forward network with hidden layer dimensions of 2048. To address the limitation of the self-attention mechanism in capturing sequence order, positional encodings based on sine and cosine functions are added to the embedding layers. Lexical ambiguity, known as semantic ambiguity, refers to words with multiple meanings, such as homographs, which pose challenges in translation tasks. Homographs, words with multiple meanings, present challenges in translation tasks due to their semantic ambiguity. Despite advancements in neural machine translation, current systems struggle to accurately interpret sentences with homographs, leading to errors in translation. Google Translate struggles with translating sentences containing homographs due to semantic ambiguity. Machine translation models may misinterpret homographs and choose the dominant meaning if context is unclear. Training data may not provide enough information for accurate translation. Semantic ambiguity in machine translation can be addressed by representing different senses of homographs with different embedding vectors. This requires a large training set for the model to understand context properly. BERT, a pretrained language representation model, supports transfer learning and finetuning on various tasks by utilizing bidirectional Transformer encoders. BERT is a language model that learns bidirectional word representations by conditioning on both left and right contexts, outperforming unidirectional models like ELMo and OpenAI-GPT. It is pretrained on masked language modeling and next sentence prediction tasks, allowing it to understand the relationship between sentences. The pretrained BERT model can understand relationships between sentences and is finetuned for tasks like question answering and natural language inference. It uses WordPiece embeddings with a vocabulary of 30,000 tokens and generates vector representations for words based on context. This allows BERT to produce different embeddings for different senses of the same word. The pretrained BERT model generates vector representations for words based on context, allowing it to differentiate between different senses of the same word. Using t-SNE algorithm, word embeddings of dimension 768 are visualized, showing that words like \"bank\" and \"banks\" are clustered into separate groups based on their meanings. The words \"bank\" and \"banks\" are clustered based on their meanings. The original Transformer architecture uses one embedding vector for each word in the vocabulary list, which limits its ability to translate semantically ambiguous sentences. The BERT BASE model was integrated into a Transformer translation model to address this issue. Incorporating BERT into a Transformer translation model helps tackle lexical ambiguity. Two architectures are proposed: Transformer BERT\u2212WE and Transformer BERT\u2212Encoder. The models are compared with a baseline Transformer using the same hyperparameters. Key parameters include the number of layers (N), embedding layer dimension (d model), inner layer dimension (d f f), and number of attention heads (h). The Transformer model incorporating BERT adjusts key parameters such as N=3, d model=768, d f f=3072, and h=768 to match the pretrained BERT model. Dropout rate of 0.1 is applied to sublayer outputs. Training data from JESC includes various language styles. Rakuten MA tokenizes Japanese sentences, while English sentences are tokenized similarly to the baseline Transformer model. For the BERT Transformer models, BERT's internal word tokenizer and vocabulary are used, with [UNK], [CLS], and [SEP] tokens added for out-of-vocabulary words. 6,000 sentences with homographs from the IWSLT 2017 English-Japanese dataset are used to evaluate model performance using BLEU score. Training settings for the models include a batch size of 32 and AdamOptimizer with decayed learning rate. The Transformer BERT\u2212WE model outperforms other models in translation evaluations, achieving a BLEU score of 20.31 on the JESC test set. Training takes 1.5 to 2 days on a GTX 1080Ti, with a decayed learning rate and evaluation based on general and homograph translation criteria. The Transformer BERT\u2212Encoder model outperforms other models in translation evaluations, achieving a BLEU score of 8.67 on the IWSLT 2017 data set. It excels in translating homographs, such as correctly translating the word \"bank\" into \"\u571f\u624b\" or \"\u5cb8\" in Japanese. Another approach to solving lexical ambiguity was proposed by Liu et al. (2018), showing that translation models' performance degrades with an increase in the number of word senses. The study integrated word sense disambiguation (WSD) embedding vectors with a translation model's word embeddings using gating functions to create contextualized word embeddings. These were then used in an RNN-based seq2seq model for translation across multiple language pairs. Pretrained word embeddings, particularly on the source language side, were found to significantly enhance machine translation performance. In this work, the use of contextualized word embeddings, specifically with BERT language model, is shown to effectively address lexical ambiguity in machine translation systems. Two machine translation architectures integrating BERT and Transformer models are proposed, with the Transformer BERT\u2212WE model outperforming the vanilla Transformer model. This approach not only resolves lexical ambiguity but also improves translation performance. Our approach effectively resolves lexical ambiguity and enhances translation quality."
}