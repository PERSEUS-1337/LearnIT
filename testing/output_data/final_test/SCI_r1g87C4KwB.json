{
    "title": "r1g87C4KwB",
    "content": "Understanding the optimization trajectory in training deep neural networks is crucial. The hyperparameters of stochastic gradient descent impact the covariance of gradients (K) and the Hessian of the training loss (H) along this trajectory. Using a high learning rate or small batch size early in training leads SGD to regions with reduced spectral norm of K and improved conditioning of K and H. The break-even point, where these effects occur, is reached early in training. Empirical evidence supports these findings across various deep neural networks and tasks. Additionally, applying this analysis to networks with batch normalization layers reveals the need for a high learning rate to achieve loss smoothing effects previously attributed to BN alone. In this work, the impact of learning rate and batch size on the optimization trajectory of deep neural networks is investigated. Using a high learning rate early in training can lead to loss smoothing effects similar to batch normalization alone. Visualizations show how different learning rates affect the training trajectory on CIFAR-10. The optimization trajectory of deep neural networks is influenced by the choice of hyperparameters such as learning rate and batch size. The covariance of gradients (K) and the Hessian of the training loss (H) quantify different properties of the trajectory. The matrix K quantifies noise induced by gradient estimates, while H describes the curvature of the loss surface. Better conditioning of H is linked to the efficacy of batch normalization. The study predicts and demonstrates effects in the early training phase influenced by hyperparameter choices. The early phase of training is influenced by hyperparameter choices in stochastic gradient descent, leading to reduced spectral norms of K and H and improved conditioning. This effect is seen after a break-even point on the optimization trajectory. The study applies a theoretical model to predict these effects and validates them in networks with batch normalization layers. Using a large learning rate is crucial for achieving better conditioning. Using a large learning rate is crucial for achieving better conditioning in neural networks during the early phase of training. This is supported by recent studies on learning dynamics and optimization trajectories, showing improved conditioning with batch normalization layers. During the early phase of training, the initial phase determines the rest of the trajectory. Studies show that shuffling examples in the first epochs affects training, and adding regularization at the beginning impacts final generalization significantly. The covariance of gradients, denoted as K, reflects the variation in gradients across samples. The matrix K reflects gradient variation across samples. The Hessian quantifies loss surface shape, with largest eigenvalues stabilizing based on learning rate and batch size. Hessian can be decomposed into terms, with the dominant one being the uncentered covariance of gradients G. The study focuses on the implicit regularization effects of optimization methods in deep learning, particularly on the gradient descent trajectory. Previous research has explored generalization error bounds based on properties of the final minimum, such as parameter vector norm or Hessian. The work contrasts with prior studies by emphasizing the importance of understanding the optimization trajectory for generalization in deep networks. In this section, the focus is on implicit regularization effects attributed to gradient descent dynamics at the start of training and the covariance of gradients. Two conjectures are made about the optimization trajectory induced by stochastic gradient descent based on a theoretical model of learning dynamics in the early training stage. Key objects studied include the Hessian of the training loss and the covariance of gradients. The smallest nonzero eigenvalues of H and K impact optimization steps. Maximum values of Tr(K) and Tr(H) decrease with larger learning rates or smaller batch sizes. Conjectures are valid within stable training ranges. Focus is on the whole training trajectory, not just the final minimum curvature. Desirable effects on optimization discussed. In this section, the focus is on analyzing learning dynamics in the early phase of training and empirically validating conjectures related to reducing variance of mini-batch gradients and improving conditioning. The study extends to neural networks with batch normalization layers and reports results on CIFAR-10 and IMDB datasets using various architectures like SimpleCNN, ResNet-32, LSTM, and DenseNet on ImageNet dataset. In this study, the focus is on analyzing learning dynamics in the early phase of training and validating conjectures related to reducing variance of mini-batch gradients and improving conditioning. The study includes experiments with DenseNet on ImageNet, BERT on MNLI, and a multi-layer perceptron on FashionMNIST datasets. The top eigenvalues and eigenvectors of H and K are estimated using the Lanczos algorithm on a subset of the training set on CIFAR-10. The smallest non-zero eigenvalue is used to counteract bias in the approximation due to normally distributed noise from sampling data. The impact of using mini-batching in computing the eigenspectrum of K is also examined. In this section, the learning dynamics in the early phase of training are analyzed to verify assumptions made previously. The evolution of \u03bb 1 H is examined, showing that training becomes unstable when \u03bb 1 K reaches its maximum value. The increase of \u03bb 1 K and \u03bb 1 H leads to a decrease in stability, which is measured through a proxy of loss reduction on the training set. Visualizing the break-even point phenomenon in training dynamics, the trajectory with a lower learning rate reaches regions of the loss surface with the same accuracy as a higher learning rate but with larger \u03bb 1 K. The spectrum of K and H at the iteration when \u03bb K and \u03bb H reach the highest value shows more outliers for the lower learning rate. The dynamics of early training phase align with theoretical model assumptions. \u03bb 1 K and \u03bb 1 H increase proportionally, leading to decreased stability. Empirical validation of Con. 1 and Con. 2 is done in three settings using suitable learning rates and batch sizes. Results for SimpleCNN, ResNet-32, and LSTM are summarized in Fig. 4 and Fig. 5. Training curves are smoothed with a moving average. The choice of learning rate and batch size does not significantly influence the optimization trajectory for various architectures and datasets. Experimental results show that the effect on the spectrum of the Hessian matrix is weaker compared to the effect on the spectrum of the Jacobian matrix. In this section, we have demonstrated the variance reduction and preconditioning effect of SGD, which occur early in training. These effects also apply to other settings such as BERT on MNLI and DenseNet on ImageNet. The loss surface of deep networks is known to be ill-conditioned, motivating the use of second-order optimization methods. Recent studies suggest that batch normalization improves conditioning of the loss surface. Our conjecture is that using a high learning rate or a low batch size also leads to improved conditioning. We investigate how these phenomena are related, particularly in batch-normalized networks. The study investigates the validity of conjectures in batch-normalized networks by running experiments on a SimpleCNN model with batch normalization layers. Results show that a high learning rate is required for the evolution of \u03bb BN. The question of whether learning can be ill-conditioned with a low learning rate even when batch normalization is used is explored by tracking mini-batch gradients. The study compares SimpleCNN-BN optimized with \u03b7 = 0.001 and SimpleCNN optimized with \u03b7 = 0.01. Observations include the values of g / g 5, \u03bb 1 K, and \u03bb * K /\u03bb 1 K. Using a high learning rate is necessary to observe the effect of loss smoothing previously attributed to batch normalization alone. The study shows that a high learning rate is crucial for good generalization when using batch normalization. It is necessary to use a high learning rate in a batch-normalized network to improve conditioning of the loss surface. Using a high learning rate or a small batch size in SGD has effects on variance reduction and pre-conditioning along the optimization trajectory. The existence of a break-even point on the trajectory induced by SGD has significant implications. The break-even point has implications for training deep networks, with a high learning rate being necessary for loss smoothing effects. The break-even typically occurs early in training, possibly related to the critical learning period. A theorem is stated regarding the stability of training with different learning rates and batch sizes in SGD. The break-even point in training deep networks requires a high learning rate for loss smoothing effects. A theorem discusses the stability of training with different learning rates and batch sizes in SGD, showing that training becomes unstable at a certain iteration. The stability of training with different learning rates and batch sizes in SGD is discussed in a theorem. It shows that training becomes unstable at a certain iteration, and the distance to the minimizer decreases with increasing \u03bb 1 H. The proof shows that as the value of \u03bb1H decreases, the function \u03c8(t) increases, leading to a contradiction. This is important in understanding the behavior of training with different learning rates and batch sizes in SGD. The use of a small subset of the training set can approximate the largest eigenvalues of the true Hessian on the CIFAR-10 dataset, which is connected to the class structure in the data. In deep learning, mini-batch gradients are sampled and a Gram matrix is computed following Papyan (2019). The matrix has the same eigenspectrum as the original one. The experiments use a fixed batch size of 128, equivalent to 5% of the training set on CIFAR-10. The trace of the matrix is computed, approximating the top eigenvalues due to computational costs. The question of whether the batch size of 128 approximates the underlying matrix is investigated by comparing eigenvalues. The experiments describe details for training ResNet-32 and SimpleCNN on CIFAR-10. ResNet-32 is trained for 200 epochs with a batch size of 128, using standard data augmentation and preprocessing. Weight decay of 0.0001 is applied. SimpleCNN consists of two convolutional layers with 32 filters, followed by two layers with 64 filters and max-pooling. A densely connected layer with 128 units precedes the classification layer. The experiments involve training different models on various datasets. BERT-base is used on MNLI with a batch size of 32 for 20 epochs. An MLP with two hidden layers is trained on FMNIST for 200 epochs with a batch size of 64. An LSTM with 100 hidden units is used on IMDB with a vocabulary size of 20000 words. The model is trained for 100 epochs with a batch size of 128 using DenseNet-121 on ILSVRC 2012 dataset. The network is trained for 10 epochs with a batch size of 32 on ResNet-32 for CIFAR-10. No dropout or weight decay is used. Experiments are repeated in various settings, including additional data. Accuracy on training and validation sets is reported for ResNet-32 and CIFAR-10, as well as for SimpleCNN on CIFAR-10 and LSTM on IMDB. Fig. 8 and Fig. 9 show accuracy on the training and validation sets for LSTM on IMDB. Fig. 12 and Fig. 13 display results for LSTM model and IMDB dataset. BERT on MNLI is shown in Fig. 14 and Fig. 15. MLP on FMNIST results are in Fig. 16 and Fig. 17. DenseNet-121 on ImageNet results are in Fig. 18 and Fig. 19."
}