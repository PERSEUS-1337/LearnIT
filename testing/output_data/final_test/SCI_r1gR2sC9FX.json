{
    "title": "r1gR2sC9FX",
    "content": "Neural networks are highly expressive functions that can fit random input-output mappings with 100% accuracy. Deep ReLU networks are biased towards low frequency functions, finding simple patterns that generalize across data samples. Learning high frequencies becomes easier with increasing manifold complexity. The frequency components are robust to parameter perturbation, requiring fine tuning of parameters. Recent research has highlighted the importance of finely tuning parameters for high frequency functions in neural networks, despite their overparameterization. This challenges traditional views on model complexity and the ability to fit random data perfectly. Investigations are ongoing to understand implicit regularization mechanisms that may bias towards simpler solutions. In this work, the focus is on exposing a bias in neural networks towards low frequency functions, termed the spectral bias. ReLU networks favor smooth functions due to their piecewise linear structure. The bias affects both the learning process and the model parameterization, with lower frequencies being more robust to random perturbations. Additionally, there is an intricate relationship between the spectral bias and the geometry of the data manifold, making high frequencies easier to learn. The text discusses the bias in neural networks towards low frequency functions, known as the spectral bias. It highlights how high frequencies become easier to learn when data is on a lower dimensional manifold. The piecewise-linear structure of ReLU networks is utilized to evaluate and bound its Fourier spectrum, demonstrating the peculiar behavior of neural networks. The geometry of the data manifold attenuates the spectral bias in a non-trivial way, adding a layer of subtlety to the learning process. The paper presents a theoretical analysis of the spectral bias in deep ReLU networks, exploring the role of data manifold geometry in learning higher frequencies. It investigates the robustness of lower frequencies to network parameter perturbations and discusses the Fourier spectrum of ReLU networks. The study includes minimal experiments demonstrating the spectral bias and empirically illustrates robustness results. ReLU networks are CPWL functions represented by convex polytopes. Every CPWL function can be represented by a ReLU network, giving them universal approximation properties. Each linear region corresponds to an activation pattern of hidden neurons, with a matrix W that is modified based on neuron activity. ReLU networks are CPWL functions represented by convex polytopes, with each linear region corresponding to an activation pattern of hidden neurons. The structure of ReLU networks in the Fourier domain is studied, with the Fourier transform decomposing as a fairly intricate mathematical object. Lemmas provide the explicit form of the Fourier components, showing the Fourier transform of a polytope. The Fourier spectrum of a full dimensional polytope in R^d decays as the spectrum decays, with the linear regions of a ReLU network forming a cell decomposition of R^d. The Fourier components of the ReLU network exhibit highly anisotropic spectral decay in large dimensions, with the Lipschitz constant scaling exponentially in depth and polynomial in width. The number of linear regions can be tightly bounded, forming a cell decomposition of R^d. Montufar et al. (2014) and Raghu et al. (2016) derived tight bounds on the scaling behavior of neural networks, incorporating controls on capacity such as width, depth, and parameter norm. The spectral norm of each layer evolves during training, showing the network's ability to express larger frequencies. Architecture-dependent controls measure the network spectrum's amplitude at different frequencies. The model prioritizes learning lower frequencies first, even when higher frequencies have larger amplitudes. The spectral norm of weights increases as the model fits higher frequencies. The training of ReLU networks may be biased towards lower frequencies, as weight norm increases with training iterations. This fact is empirically investigated in the following sections. In the experiments presented, a 6-layer deep 256-unit wide ReLU network is trained to regress a mapping function with a focus on fitting lower frequencies first. The spectral bias phenomenon is discussed in relation to the results of previous sections. In the experiments, a 6-layer deep 256-unit wide ReLU network is trained to regress a mapping function, focusing on fitting lower frequencies first. The amplitude of frequencies is monitored during training, showing that lower frequencies are regressed first regardless of their amplitudes. Theoretical aspects suggest that higher frequencies are learned late in the optimization process due to the gradual increase in parameter norm by gradient descent. The MSE loss is used in experiments to avoid bias towards specific frequency components. The gradient of the MSE loss w.r.t. parameters shows a bias towards lower frequencies due to spectral decay rate. This extends findings from previous studies on sigmoid networks. The shape of the data-manifold impacts the learnability of high frequencies in a nontrivial way, as low frequencies functions in the input space may have high frequency components when restricted to lower dimensional manifolds of complex shapes. The spectral bias is influenced by the manifold shape, demonstrated in an illustrative minimal setting. The spectral bias is impacted by the shape of the data-manifold, influencing the learnability of high frequencies. A mathematical relationship between the Fourier spectrum of the network, the target function spectrum on the manifold, and the manifold geometry is presented. Experiments on MNIST and CIFAR-10 are included in appendices. The data is assumed to lie on a lower dimensional manifold in input space, with regression on the target function equivalent to regression on a function in latent space. Regression on the target function is equivalent to finding a function in the latent space, with neural networks biased towards low-frequency solutions. Methods like adversarial training and Mixup restrict the solution space. The experimental setup controls the data manifold shape and target function. The text discusses controlling the data manifold shape and target function using neural networks. Experiment 2 involves defining a signal on the latent space with specific frequencies and amplitudes. Experiment 3 adapts Experiment 2 for binary classification by thresholding the function. Visualization is simplified by using specific techniques. Loss curves Figure 3 show the evolution of the network spectrum during training for target functions defined on manifolds \u03b3L for various L. Increasing L results in better classification performance for the same target signal. This behavior is consistent with previous experiments. The experiments in Experiment 2 (Fig 3a-d) showed a rich interaction between the shape of the manifold and the learning task difficulty. The relationship between the network's frequency spectrum f and the fit f \u2022 \u03b3 L is mediated by the embedding map \u03b3 L. A signal on a manifold is easier to fit when the manifold's coordinate functions have high frequency components. This allows the same signal in a flower with more petals to be captured with lower frequencies of the network. The Fourier transform of the composite function investigates the kernel P \u03b3, encoding the correspondence between frequencies in input and latent space. The kernel behavior is studied in the regime where the stationary phase approximation applies. Non-zero values of the kernel correspond to pairs with a solution, utilizing Fourier series representation of \u03b3's components. The Fourier series representation of coordinate functions on an interval [0, 1] m yields conditions on frequencies, showing that large frequencies can be obtained with large coordinate function values. Pytorch's BCEWithLogitsLoss uses a sigmoid function before evaluating cross-entropy. Experiments demonstrate that coordinate functions cannot be arbitrarily scaled. Increasing frequency magnitudes in the latent space can be expressed with the same frequency in the data-domain. The neural network function can express large frequencies on a manifold with smaller frequencies in its input domain. Lower frequency components of trained networks are more robust to random perturbations in parameter space. This property is independent of the training process and depends on the parametrization of the model. The study presents empirical evidence and a theoretical explanation of the phenomenon observed in Experiment 4. Training is conducted for various frequencies and amplitudes, with results showing that higher frequencies are less robust than lower ones due to their smaller parameter space volume. The study provides empirical evidence and theoretical explanation for the observation in Experiment 4, showing that higher frequencies are less robust due to occupying a smaller parameter space volume. The volume ratio, DISPLAYFORM1, inherits the spectral decay rate of |f \u03b8 (k)|, indicating a bias towards learning functions with dominant lower frequency components in deep ReLU networks. Previous work has demonstrated the universal approximation capabilities of neural networks with sufficient width. The study provides empirical evidence and theoretical explanation for the observation in Experiment 4, showing that higher frequencies are less robust due to occupying a smaller parameter space volume. Previous work has demonstrated the universal approximation capabilities of neural networks with sufficient width. Montufar et al. (2014) and Raghu et al. (2016) showed that deep ReLU networks have polynomial growth in linear regions with width and exponential growth with depth. Analysis on the expressive power of deep neural networks explains why over-parameterized networks can learn random input-output mappings perfectly. Fourier analysis of deep ReLU networks reveals their spectral bias towards learning simple functions. In the study, deep networks prioritize learning simple functions during training, striking a balance between function smoothness and expressivity compared to kernel machines and K-nearest neighbor classifiers. Deep ReLU networks favor low frequencies and can approximate arbitrary functions efficiently. Fourier analysis shows their spectral bias towards learning simple functions. Neural networks exhibit a bias towards smooth functions, favoring low frequency ones, known as spectral bias. The geometry of data manifold impacts expressivity, allowing high frequency functions on complex manifolds to be expressed by lower frequency network functions. Parameters for lower frequencies are more robust to random perturbations. Future work exploring neural networks in Fourier domain is promising for quantifying model sensitivity. In this study, a 6 layer ReLU network with 256 units per layer was fitted to a target function \u03bb, which is a superposition of sine waves with increasing frequencies. Two settings were tested: one with equal amplitudes for all frequencies and another with larger amplitudes for higher frequencies. The network was trained for 80000 steps using full-batch gradient descent with Adam optimization. Stochastic gradient descent was avoided to eliminate parameter update stochasticity. The network was evaluated on a 200 point grid every 100 training steps. The study evaluated a 6-layer ReLU network with 256 units per layer trained on a target function composed of sine waves with increasing frequencies. The network's performance was assessed on a 200-point grid every 100 training steps, measuring the magnitude of its discrete Fourier transform. Spectral norms of the weights at each layer were also recorded during training. Additionally, the network's predictions, loss curves, and target function were visualized in figures. The study involved training a 6-layer ReLU network with 256 units per layer on a dataset obtained by mapping uniformly spaced points between 0 and 1 to the input domain. The network was trained using 50000 full-batch gradient descent steps of Adam. The experiment focused on evaluating the relationship between the network's Fourier spectrum and its depth, width, and max-norm of parameters through a qualitative ablation study. In one experiment, various networks were fitted to the \u03b4-function at x = 0.5 to test their ability to fit large frequencies. Increasing depth significantly improves the network's ability to fit higher frequencies, while increasing width has a weaker effect. Increasing weight clip also helps the network fit higher frequencies, consistent with Theorem 1. Lower frequencies are learned first, showing spectral bias. In an experiment on MNIST, it was shown that learning random labels is harder when input samples lie on the same manifold. Using a 64-dimensional feature-space E of a denoising autoencoder as a proxy for the data-manifold, the study compared learning a signal on a realistic data-manifold like MNIST with a flat manifold of the same dimension. The decoder functions as an embedding of E in the input space X = R 784, training a network on reconstructions of the autoencoder. An injective embedding of a 64-dimensional hyperplane in X is used for comparison. Two networks are trained under identical conditions, showing loss curves in FIG2. This complements previous findings that fitting random labels to random inputs is easier when defined on the full dimensional input space. Deep neural networks are biased towards learning low frequency functions, resulting in rare isolated bubbles of constant prediction. This implies that there should be a path connecting any two points in the input space with the same network prediction. An experiment using AutoNEB shows paths between Cifar-10 images classified as the same target class exist. This result holds when using variational autoencoders. Variational autoencoders are used to construct linear paths between points in the input space of a neural network. Paths are created between training images, from a training image to an adversarial image, and between adversarial images. Only pairs of images belonging to the same class or originating from another class but classified as the specified class are considered. 50 random training images and 50 random images from other classes are selected for each class. The study used variational autoencoders to create paths between images in the input space of a neural network. Paths were generated between training images and adversarial images without crossing decision boundaries. The AutoNEB parameters were set for four iterations with 10 steps of SGD. The results showed that paths between all pairs of images for all CIFAR10 labels were within the same connected component of the DNN output. The study used variational autoencoders to create paths between images in the input space of a neural network. Paths were generated between training images and adversarial images without crossing decision boundaries. The paths are visually simple and quantitatively slightly longer than linear connections. Fourier transform is used to represent functions as a sum of oscillating functions. Each row in FIG2 represents a path from an adversarial sample to a true training image in image space. The experiment demonstrates the ability to find paths from adversarial examples classified as one class to actual training samples of that class using a ResNet-20. The function f(x) is expressed as a weighted sum of plane waves using the Fourier inversion theorem. The Fourier transform of a function f is represented by f(k) in n-D space, where the magnitude k is inversely proportional to the wavelength. The behavior of f for k \u2192 \u221e indicates the smoothness of the function. In asymptotic notation, a function with a spectrum of O(k^-2) is smoother than one with O(k^-1). A smooth function must decay faster than any rational function of k, assuming it is integrable. This decay is a consequence of higher-frequency oscillations vanishing faster in smoother functions. The Fourier transform of a function in n-D space is represented by f(k), where the magnitude k is inversely proportional to the wavelength. A smoother function must decay faster, as per the Riemann-Lebesgue lemma. The Fourier transform diagonalizes the differential operator and can be generalized to tempered distributions. The Fourier transform of a polytope can be computed recursively. The Fourier transform of an m-dimensional polytope is expressed as a sum over its m-1 dimensional boundaries, with a weight term that terminates if Proj F (k) = 0. A book-keeping device called the face poset is used to structure this computation, creating a tree diagram with polytopes of various dimensions as nodes. Starting from the full dimensional polytope, edges are drawn to codimension-one boundary faces with weighted terms, yielding tree paths where each face has a weight term of O(k - 1). The Fourier transform of an m-dimensional polytope is computed by summing over its boundaries, terminating when Proj F (k) = 0. Using a face poset, a tree diagram is created with polytopes as nodes, leading to tree paths with decreasing dimensions. The final transform is obtained by multiplying weights along each path and summing over all paths. The spectrum is O(k - d) unless a path terminates prematurely at a lower-dimensional face, resulting in a dominant O(k - r) term. The Fourier spectrum of a full dimensional polytope in Rn decays as O(k - d) for almost all k's, with exceptions lying on lower dimensional subspaces. The Lipschitz constant of a ReLU network is bounded by a certain expression. The spectral norm of a matrix W is defined as W h = i |w i \u00b7 h|, where w i is the i-th row of W. The behavior of P \u03b3 (l, k) is investigated for large frequencies l and smaller frequencies k in the input domain. In the regime where the stationary phase approximation applies to P \u03b3, the integrand oscillates fast enough for the only constructive contribution to come from where the phase term does not change with z. This leads to the condition \u2207 z u(z) = 0. Periodic boundary conditions are imposed on the components of \u03b3, with the manifold contained in a box in R d. The \u00b5-th component \u03b3 \u00b5 can be expressed as a Fourier series. The text discusses conditions on \u03b3 for the RHS to be large in magnitude, even with fixed k. The volume of parameter space contributing to frequency components decays with increasing k in a given neural network. The text also mentions ReLU networks and deep networks. Deep networks, unlike K-nearest neighbor models, are parameter efficient and can approximate any target function. They separate input space into linear regions that grow polynomially in width and exponentially in depth. DNNs are biased towards lower frequency functions, striking a balance between smoothness and expressibility. K-nearest neighbor (KNN) is historically important for classification due to its simplicity and consistency as an approximator. However, it is slow for large datasets. A comparison of smoothness between KNN and DNN is conducted by training classifiers on a frequency signal and evaluating the 2D FFT. The resulting probability is plotted for various values of K. The study compared the smoothness of K-nearest neighbor (KNN) and deep neural networks (DNN) by training them on a dataset and analyzing the frequency spectrum. The results showed that DNNs are smoother than KNNs at all values of K considered, indicating that DNNs exploit the geometry of the manifold better than KNNs."
}