{
    "title": "HJxwAo09KQ",
    "content": "Deep learning has shown that learned functions can outperform hand-designed functions on perceptual tasks, suggesting that learned update functions may also outperform hand-designed optimizers. However, learned optimizers are difficult to train and have not shown speedups over hand-designed optimizers. A new training scheme is proposed to overcome these challenges, allowing neural networks to optimize faster than traditional methods. Neural networks can optimize faster than traditional methods by training the optimizer against validation loss. This approach leads to better generalization compared to first-order methods. The learned optimizer significantly reduces training time for convolutional networks. Learning optimization algorithms such as BID5, BID30, BID13, BID0, BID36, BID19, and BID2 can lead to significant performance improvements for specific tasks. However, training these learned optimizers is challenging. Existing approaches fall into two categories: black-box methods like evolution, random search, reinforcement learning, or Bayesian optimization, which do not scale well with the number of parameters, and first-order methods that compute gradients of optimizer effectiveness. Training optimization algorithms like BID4 and BID21 involves costly iterative application of update rules and backpropagation, known as \"unrolled optimization.\" Truncated backpropagation through time (TBPTT) is used to address backpropagation through many optimization steps, but it can lead to increased bias and exploding gradients. Previous techniques for training recurrent neural networks via TBPTT have not been effective for training optimizers. This paper explores the impact of bias and exploding gradients on training analytically and experimentally. In this study, the authors investigate the detrimental effects of bias and exploding gradients on training optimizers. They propose a solution by optimizing a distribution over optimizer parameters using variational optimization. Two unbiased gradient estimators are defined for this purpose, allowing for stable and efficient training of learned optimizers. The approach is demonstrated by training a learned optimizer for optimizing convolutional networks on image classification, showing improved validation loss and faster wall-clock time compared to hand-designed optimizers like SGD+Momentum. The study investigates bias and exploding gradients in training optimizers, proposing a solution by optimizing a distribution over optimizer parameters using variational optimization. A learned optimizer demonstrates faster wall-clock time compared to hand-designed optimizers like SGD+Momentum, RMSProp, and ADAM. It also shows promising generalization ability on out-of-distribution tasks. The study proposes optimizing a distribution over optimizer parameters using variational optimization to address bias and exploding gradients. The learned optimizer achieves faster wall-clock time compared to hand-designed optimizers like SGD+Momentum, RMSProp, and ADAM, with promising generalization ability on out-of-distribution tasks. Positive performance differences are observed on most tasks compared to the baseline optimizers. Learning an optimizer involves a bi-level optimization problem with inner and outer levels. The inner minimization optimizes the weights of a target problem using an update rule, while the outer loop updates optimizer parameters to minimize optimizer performance. The study proposes optimizing a distribution over optimizer parameters using variational optimization to achieve faster wall-clock time and promising generalization ability on out-of-distribution tasks. The text discusses training an optimizer by computing derivatives of the outer-objective with respect to the optimizer parameters through unrolling the optimization process. This involves forming an unrolled computational graph by iteratively applying an optimizer to optimize the weights of a target problem. Gradients for the optimizer parameters are computed by backpropagating the outer objective. The text discusses the challenges of computing gradients for optimizer parameters through truncated backpropagation in unrolled optimization. It highlights issues of bias and exploding norm, illustrated with an example of learning a learning rate. The text discusses learning a scalar learning rate (\u03b8) to minimize a target problem. Low momentum values converge to the first local minimum, while higher values settle at the global minimum. Larger values oscillate before settling. The final loss surface is smooth for small training steps but becomes discontinuous with larger steps. The inner problem is a MLP, and different numbers of inner-loop steps are explored. The text discusses the outer-gradient in optimizing a loss surface with two local minima. The outer-gradient grows exponentially with the number of gradient descent steps. This leads to a problem with long unrolled gradients, as it becomes a matrix polynomial of degree T. The task involves optimizing a loss surface with two local minima using a momentum-based optimizer. Different momentum values lead to convergence in different minima or oscillations between them. Increasing unrolling steps make the loss surface less smooth. In neural network problems, the outer-loss surface becomes more complex with more unrolling steps. Increasing unrolling steps make the loss surface more complex. Low truncation amounts lead to suboptimal learning rates. Outer-gradients shift from highly negative to slightly above zero during inner-training. Cumulative outer-gradient value reaches equilibrium when crossing zero, affecting the learning rate and outer-loss. When using short truncation windows in unrolled optimization, bias increases, causing outer-loss to rise. Results show that despite initializing close to the optimal learning rate, outer-training with truncated backprop leads to a decrease in the learning rate. This shift is due to the anti-correlation between the sum of truncated outer-gradients and the true value. The outer-gradients in truncated unrolled optimization are anti-correlated to the true outer-gradient, leading to a decrease in the learning rate. To address this, a smoothed outer-loss surface is optimized instead of the direct loss function. The variance (set to 0.01) determines smoothing in gradient estimators for L(\u03b8). Two unbiased estimators, g rp and g es, are combined using inverse variance weighting. Variances of g es and g rp can differ significantly, so the merged estimator has the lowest variance. Antithetic sampling is used to reduce variance in estimating g es. The cost of computing g es and g rp is reduced using antithetic sampling. Data parallelism is leveraged to compute multiple samples. The same samples are used for evaluating variances \u03c3 2 rp and \u03c3 2 es. An increasing curriculum over steps per truncation is used during outer-training. The optimizer architecture consists of a simple fully connected neural network with one hidden layer. The optimizer architecture includes a neural network with one hidden layer of 32 ReLU units, applied independently to each target problem weight. The network outputs an update direction and log learning rate for weight updates, taking as input gradient, parameter value, RMS gradient terms, moving averages of gradients, and current iteration representation. Ablation studies are conducted on these inputs, with further architectural details in Appendix C. The optimizer is trained against a three-layer CNN inner-trained for ten thousand iterations on image classification tasks. The Imagenet dataset is divided into classes, with outer-training focusing on optimizer performance on a three-layer convolutional network trained on 10-way classification problems. The learned optimizers show some ability to generalize to new architectures and datasets, as seen in the comparison against hyperparameter tuned Adam, RMSProp, and SGD+Momentum on a six-layer convolutional neural network trained on MNIST. The optimizer trained to target validation loss generalizes better than the one trained to target train loss. Additional experiments test generalization to other tasks. The optimizer is trained with increasing unrolled steps and jittered by a small percentage. Asynchronous batched SGD is used with 128 CPU workers. Despite stability improvements, there is still variability in optimizer performance. Training loss is used to select the best model for further use. The optimizer is trained with increasing unrolled steps and jittered by a small percentage using asynchronous batched SGD with 128 CPU workers. Training loss is used to select the best model for further use. The performance of the learned optimizer is compared against other first-order methods on a sampled validation task, showing faster convergence on training loss but poor performance on validation loss when outer-trained against the training outer-objective. However, when outer-trained against the validation outer-objective, faster optimization and lower validation loss are achieved. The learned optimizer outperforms the best baseline optimizer 98% of the time, even though generalization was not the focus. It shows improved performance on various datasets and network configurations. Performance on a six-layer CNN trained on MNIST demonstrates the optimizer's ability to reduce loss. The model described in \u00a74.1 outperforms baseline optimizers, showing improved performance on different datasets and network configurations. The distribution of outer-loss performance is compared with different features removed, revealing the combined gradient estimator's stability and superiority. Poor convergence is found with fewer unrolling steps and high variance with more truncation steps. Learned optimizers perform well without certain features but require momentum-based features for convergence. The learned optimizer's performance was evaluated by removing key components such as the gradient estimator, unrolling curriculum, and specific features fed to the optimizer. Different configurations were tested, showing that the gradient estimator and an increasing schedule of unroll steps are crucial for optimal performance. In this work, the importance of gradient estimator and increasing unroll steps for optimal performance of learned optimizers is highlighted. Challenges such as \"exploding\" gradients and bias from truncated backpropagation are addressed by constructing a variational bound of the outer-objective. By using a combined estimator and curriculum over truncation steps, learned optimizers achieve over five times speedup compared to existing ones. Future research will explore the generalization of learned optimizers across tasks. The curr_chunk discusses the application of learned optimizers in meta-learning and the potential for developing new algorithms. It also mentions using gradient estimators for training recurrent problems and improving outer-training stability. The final loss after T steps of gradient descent is highlighted. The final loss after T steps of gradient descent can be updated by computing the derivative with respect to the parameters of the optimizer. The exploding outer-gradient comes from the recursive definition involving the Hessian at every iteration. Parameters are initialized for outer-training, with inner loop parameters randomly initialized for each truncation. Positive and negative sequences are computed iteratively based on the current outer-training index. The final loss after T steps of gradient descent can be updated by computing the derivative with respect to the parameters of the optimizer. Positive and negative sequences are computed iteratively based on the current outer-training index. The architecture details include using diagonal preconditioning optimizers and existing learned optimizers. The outer-training algorithm involves computing empirical variance and mean of each weight for each estimator, updating meta-parameters with SGD, and using a combined gradient estimator. Our architecture operates on each parameter independently using a single hidden layer feed forward MLP with 32 hidden units. It takes momentum and rms terms as input, inspired by Adam, with different decay values. Despite being critical, features like momentum parameters have minimal impact. Weight values are also used as features for the optimizer to learn arbitrary norm weight decay. The optimizer learns arbitrary norm weight decay by passing weight values and using tanh squashing functions at different timescales. Activations are normalized and passed into a hidden layer MLP with relu activations. The network produces two outputs combined in an exponential manner using temperature parameters. The optimizer targets a 3 layer convolutional neural network with 3x3 kernels, and 32 units per layer, using relu activations and glorot initializations. The outer-training set consists of a family of 10 way classification problems on subsets of 32x32 Imagenet. The optimizer utilizes two temperature parameters to scale the initialization for more efficient training. The model is trained using a linear schedule on the number of unrolling steps from 50-10k over 5k outer-training iterations. The outer-parameters are optimized using Adam with a batch size of 128 and learning rates of 0.003 for training and 0.0003 for validation. The validation outer-objective is harder, requiring a lower learning rate for stable training. In this work, the focus is on learning optimizers for specific task distributions. The models show promising generalization when transferred to different architectures, depths, and number of parameters. This indicates a reasonable inductive bias in the learned optimizer. When training models, the computation in the learned optimizer is linear in terms of model parameters and smaller than computing gradients. The bulk of the computation involves batched matrix multiplies. Performance of the optimizer is tested on CPU and GPU with models like Adam and SGD. In this paper, the performance of the learned optimizer is measured on CPU and GPU using Adam and SGD in TensorFlow. The learned optimizer is 16% slower than Adam and SGD on CPU, and 57% slower than Adam and 102% slower than SGD on GPU. Overhead is higher on GPU due to increased ops, but a fused kernel could reduce this. Despite the slowdown, performance gains are observed. The learned optimizer is slower than traditional optimizers on CPU and GPU, with a higher overhead on GPU. However, performance gains of over 400% are still achieved, making the optimizer considerably faster in wallclock time."
}