{
    "title": "rkMnHjC5YQ",
    "content": "We propose a new algorithm for learning a one-hidden-layer convolutional neural network with learnable convolutional and output weights. The algorithm is inspired by isotonic regression and landscape analysis of non-convex matrix factorization problems. This theoretical focus aims to provide provably efficient algorithms for learning neural networks, particularly convolutional architectures with practical applications. Recently, BID2 proved that learning a single non-overlapping convolutional filter is NP-hard. The question now is whether efficient algorithms can be designed to learn convolutional neural networks. They propose an algorithm to learn a convolutional neural network with two unknown layers efficiently. The main result is that a convolutional neural network with two unknown layers and overlapping patches can be learned efficiently. The algorithm combines isotonic regression and landscape analysis to reduce learning a CNN with piecewise linear activation to a linear activation problem. Learning a linear convolutional filter is also shown to be reducible to a non-convex matrix factorization problem. The algorithm efficiently learns a convolutional neural network with two unknown layers and overlapping patches by reducing it to a non-convex matrix factorization problem. The analysis connects to the spectral properties of Toeplitz matrices and may inspire further development in designing provable learning algorithms for complex models. Recent work has relaxed assumptions on Gaussian input and non-overlapping patches for learning convolutional filters. BID5 showed that stochastic gradient descent can recover the true filter with close patches. BID13 proposed an iterative algorithm inspired by isotonic regression for learning filters with overlapping patches. These analyses focus on learning one unknown convolutional filter, while other works have shown positive results for learning fully connected neural networks using kernel methods and tensor techniques. The algorithm proposed does not assume knowledge of the input distribution, unlike kernel methods and tensor decomposition techniques. It addresses the non-convex nature of learning neural networks and utilizes noise-injected gradient descent to find global minima efficiently. Recent studies have explored the landscape properties of neural networks, with a focus on reducing certain drawbacks in existing methods. The algorithm reduces the convolutional neural network learning problem to matrix factorization, utilizing geometric properties. It assumes n data points with labels generated by a two-layer CNN. The prediction function is defined, and proper scaling is achieved by setting the norms of certain parameters equal. The algorithm reduces CNN learning to matrix factorization, using geometric properties. It assumes n data points with labels from a two-layer CNN. Proper scaling is achieved by setting parameter norms equal. The main result is described with assumptions on input distribution and convergence guarantees of the algorithm. The symmetry assumption in BID13 and learning theory papers like BID0 is based on identity covariance, which holds true when data is whitened. Input layers in architectures often assume these properties due to batch normalization. Boundedness is a standard assumption to exclude abnormal input distributions, weaker than the Gaussian input distribution assumption. Assumption 3.1 states conditions for input distribution symmetry, identity covariance, and boundedness. The second assumption pertains to the patch structure with a larger stride. Our second assumption is on the patch structure, assuming a stride larger than half of the filter size, common in convolutional neural networks. Activation functions like ReLU are assumed to be piecewise linear. The algorithm described has three stages: learning outer layer weights, recovering filter weights, and selecting the best weight combination. Our first observation focuses on learning non-overlapping parts of patches jointly in the second layer. By defining a selection matrix for the non-overlapping part of each patch, we aim to learn the filter weights and the non-overlapping segment simultaneously. The algorithm proceeds by defining a prediction function using the non-overlapping filter weights. The algorithm for learning a linear CNN involves iterative updates inspired by isotonic regression to estimate the gradient. Isotropic noise is added to escape saddle points in the non-convex objective function. After sufficient iterations, a pair of weights and parameters close to the truth is obtained. After obtaining a pair of weights and parameters close to the truth or negative truth, Stage 2 can be skipped if there is no overlap between patches. In Stage 1, a good approximation to the second layer is learned, reducing the problem to learning a convolutional filter using Convotron. By drawing many samples and choosing the solution with lower squared error, the correct solution can be obtained. Theorem 3.1 guarantees that Algorithm 1 can learn the target convolutional neural network in polynomial time, making it the first proper learning algorithm for CNNs with overlapping patches. Key ideas for designing the algorithm and proving its correctness are discussed in this section, with a focus on analyzing the process stage-wise. The population L2 loss is shown to be the standard loss, assuming non-overlapping patches and an identity covariance of x. Recent advances in non-convex optimization have led to a regularized loss function that ensures all local minima are global, making it easier to find a global minimum. By using a stochastic gradient oracle, we can implement noise-injected stochastic gradient descent for solving the population risk equation efficiently. Theorem 6 in BID7 states that after polynomial iterations, the iterative procedure returns an optimal solution with high probability for the objective function. Using a piece-wise linear activation function in a CNN, a stochastic gradient oracle can still be obtained. Lemma 4.1 provides properties of the stochastic gradient for a linear CNN under certain assumptions. If the norms of certain parameters are bounded, then the differences are also bounded. After recovering the outer layer weights in Stage 1, Convotron is used to obtain the filter weights. The analysis of Convotron handles average pooling as the outer layer and can also handle any fixed outer layer weights and noise. The theorem states that Convotron returns optimal weights with high probability after a polynomial number of iterations. The analysis in the current chunk extends the theorem and proof for covariance being identity and no noise in the label to handle non-identity covariance with good condition number and bounded probabilistic concept noise. It focuses on the convergence rate depending on the least eigenvalue of a matrix and how to pick the correct hypothesis. The analysis in the current chunk focuses on bounding the loss of each instance based on the closeness of parameters, with the goal of achieving prediction error. Theorem 4.3 provides a guarantee under certain assumptions, and Lemma 4.2 further bounds the loss in terms of parameter closeness. The analysis focuses on bounding loss based on parameter closeness to achieve prediction error. Theorem 4.3 and Lemma A.1 are combined to obtain the desired result. Simulations verify the proposed method's effectiveness with fixed input dimension and filter size, varying stride size. Performance is measured using the angle between parameters. Stage 1 and Stage 2 convergence are shown in FIG2 with specific parameters. The paper proposes an efficient algorithm for learning a one-hidden-layer convolutional neural network with overlapping patches. The algorithm combines ideas from isotonic regression, landscape analysis of non-convex problems, and spectral analysis of Toeplitz matrices. The results show low test error for different input distributions and stride sizes. Future work includes extending the algorithm to learn complex models with multiple filters. The paper introduces an algorithm for learning a one-hidden-layer convolutional neural network with overlapping patches, combining ideas from isotonic regression, landscape analysis of non-convex problems, and spectral analysis of Toeplitz matrices. The algorithm aims to learn complex models with multiple filters. Lemmas and theorems are presented to aid in the analysis, with a focus on bounding terms and extending results to overall loss. The paper presents Lemmas and Theorems to aid in analyzing a one-hidden-layer convolutional neural network algorithm for learning complex models with multiple filters. The focus is on bounding terms and extending results to overall loss, including bounding eigenvalues of matrices using the Gershgorin Circle Theorem. The text discusses maximizing a bound over all variables, including a tridiagonal symmetric Toeplitz matrix with eigenvalues of a specific form. The gradient bound is derived using Convotron analysis and a modified gradient update is presented."
}