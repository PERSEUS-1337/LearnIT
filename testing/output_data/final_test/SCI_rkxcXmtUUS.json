{
    "title": "rkxcXmtUUS",
    "content": "Recent work has shown a hierarchical correspondence between layers of deep neural networks trained on object recognition and brain areas in the primate visual system. In a study on the mouse visual system, features extracted from a CNN trained on ImageNet outperformed classical models in predicting neuron responses. However, there was no evidence of a hierarchical correspondence between brain areas and CNN layers. Visual object recognition is a fundamental task performed by the primate brain via a hierarchy of visual areas. Recent work on modeling neural responses in sensory brain areas builds upon hierarchical convolutional neural networks trained to solve object recognition tasks. However, in the mouse visual system, object recognition does not provide more discriminative features compared to a random network. Training on ethologically relevant visually guided behaviors may be necessary to understand the functional organization of the mouse visual cortex. Recent research has shown that hierarchical convolutional neural networks trained for object recognition tasks have provided insights into the organization of sensory brain areas in macaques and humans. These models have revealed a correspondence between the layers of the CNNs and areas of the ventral stream, showing a hierarchical organization in the human auditory cortex. The question remains about the hierarchical organization of visual object processing in the mouse visual cortex and the effectiveness of deep neural networks trained on object classification for modeling the mouse visual system. Recent research has shown that hierarchical convolutional neural networks trained for object recognition tasks have provided insights into the organization of sensory brain areas in macaques and humans. These models have revealed a correspondence between the layers of the CNNs and areas of the ventral stream, showing a hierarchical organization in the human auditory cortex. The question remains about the hierarchical organization of visual object processing in the mouse visual cortex and the effectiveness of deep neural networks trained on object classification for modeling the mouse visual system. The study aims to characterize the mouse visual system using goal-driven deep neural networks, extracting features from a standard CNN (VGG16) to predict responses of neurons in four mouse visual areas to static natural images. The study used VGG16 to predict neural activity in mouse visual areas, outperforming a Gabor filter model. However, VGG16 did not significantly outperform a network with random weights. No evidence was found for a hierarchical correspondence between deep network layers and visual areas. The study used a core network, spatial transformer readout, shifter network, and modulator network to analyze neural responses. The modulator network in the study uses running speed and pupil dilation to predict spike rates. It consists of four main components: a core for nonlinear features, a readout for mapping features to neuron responses, a shifter for predicting receptive field shifts, and a modulator for providing gain factors based on mouse behavior. VGG16 is used for the core due to its simplicity and performance in characterizing rodent visual areas. The study utilizes a spatial transformer readout on VGG16 output feature maps to predict receptive field locations for neurons. Feature vectors are extracted at these locations and combined linearly. Feature weights are regularized with an L1 penalty for sparsity. A shifter predicts global receptive field shifts based on pupil position, while a modulator uses running speed and pupil diameter to adjust neuron gains. The model predicts neuron responses by combining feature vectors extracted from VGG16 output feature maps using a spatial transformer readout. A soft-thresholding nonlinearity is applied to generate non-negative spike rate predictions. Excitatory neurons in areas V1, LM, AL, and RL were recorded using a two-photon mesoscope, yielding a large number of neurons for analysis. Pupil position was also monitored during the experiments. Neurons from areas V1, LM, AL, and RL were monitored along with pupil position, dilation, and running speed. Visual stimuli included 5100 grayscale images from ImageNet presented on a screen. Each image was shown for 500 ms with a blank screen interval. Activity was extracted between 50 ms and 550 ms after stimulus onset for analysis using a model fitted for each scan, brain area, VGG16 layer, random initialization, and input resolution. The study optimized input image resolutions for VGG16 layers to match brain representations. No hierarchical correspondence found between brain areas and convolutional layers in terms of performance. The study compared VGG16 performance with classical models and evaluated the model's ability to capture stimulus-driven variability. VGG16 features explained a significant portion of the oracle correlation for visual input information. Additionally, a Gabor filter bank model was considered as a baseline for predictive performance. The study compared VGG16 performance with classical models and evaluated its ability to capture stimulus-driven variability. VGG16 outperformed a Gabor filter bank model, suggesting that training on static object recognition may not be necessary for achieving state-of-the-art results. The study compared VGG16 performance with classical models and found that training on static object recognition may not be necessary for achieving state-of-the-art results. A large collection of random features followed by rectification can provide a powerful feature space, with the number of LN layers being critical. The fourth and fifth rectified convolutional layers of the random core were identified as the best predictive layers for the visual areas studied. The study compared VGG16 performance with classical models and found that training on static object recognition may not be necessary for achieving state-of-the-art results. The nonlinear degree, rather than the training goal, dictates how close representations are to neural activity. There was no match between the hierarchy of mouse visual cortical areas and CNN layers. VGG16's performance was matched by random weights, implying the power of random features in machine learning. The best predictive VGG layer for any brain area depended on image resolution. The study suggests that training VGG16 on static object recognition alone may not accurately describe representations in the mouse cortex. It is important to consider the image resolution when feeding data into VGG16. Training with more ethologically relevant visually guided tasks for mice could provide a better understanding of the mouse visual system. Training VGG16 on static object recognition may not accurately describe representations in the mouse cortex. Using dynamic stimuli like prey capture tasks could reveal the functional organization of the mouse visual system."
}