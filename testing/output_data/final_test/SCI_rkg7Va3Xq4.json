{
    "title": "rkg7Va3Xq4",
    "content": "In this paper, the author discusses different types of explanations in intelligent agents, including process accounts and preference accounts. They also explore how explanations can arise in multi-step decision making, such as conceptual inference, plan generation, and plan execution. Additionally, alternative methods of questioning agents and receiving answers are considered. Intelligent systems are increasingly used for critical tasks like driving cars and controlling military robots, raising concerns about the interpretability of their behavior. Research on explanation in systems learning from experience has focused on tasks like object recognition, but there is a need for research on explanation for complex tasks involving multi-step decision making. Challenges arise in communicating solutions that combine high-level representations and reasons for their selection. In this paper, the focus is on explaining the reasons for behavior, specifically self explanations. The problem of explainable agency is discussed, which involves the agent carrying out a search to find solutions based on domain knowledge and evaluation criteria. This is considered less challenging than postulating the reasons for another agent's behavior. The curr_chunk discusses the need for explainable planning in intelligent agents, highlighting the importance of justifying decisions in comprehensible terms. It mentions research on self-explanation and proposes hypotheses on when different types of accounts are useful. Additionally, it explores generating explanations over different content types and alternative ways to pose questions and present answers. The curr_chunk emphasizes the need for substantial research on explainable agency, focusing on two primary forms of self-explanation related to decision-making processes in intelligent systems. It discusses the origins of heuristic search in artificial intelligence and the importance of establishing which aspects of decision-making to elucidate before developing computational methods for self-explanation. The curr_chunk discusses the task of explaining the processing that produced solutions in a problem space. It highlights the importance of providing explanations for why a solution is preferable, similar to think-aloud protocols in human problem solving and early AI systems. The curr_chunk discusses the process of explaining problem-solving strategies in AI systems, focusing on the importance of generating and storing decision-making content. This task is similar to think-aloud protocols used in human problem solving and early AI systems. The curr_chunk discusses the importance of storing decision-making content in AI systems to explain problem-solving strategies. It involves generating, storing, retrieving, and communicating information to find solutions to complex problems. The AI literature includes research on analogical planning but lacks focus on self-explanation. The curr_chunk discusses different forms of self-explanation in AI systems, including recording reasoning, answering questions, and explaining final solutions produced by heuristic search. It also mentions computational models of argument and various problem-solving techniques. The curr_chunk discusses various approaches to constraint satisfaction and heuristic search in AI systems, highlighting the importance of different heuristics guiding search towards solutions. It emphasizes the task of providing explanations for preferred solutions, distinct from generating think-aloud protocols. The curr_chunk focuses on the distinction between process and preference explanations in heuristic search. It compares the task to recommender systems that rank candidates for users to consider, emphasizing the importance of quality over means in decision-making. The curr_chunk discusses the importance of preference explanations in heuristic search, where the final set of candidates is retained and their ordering is explained based on criteria. It highlights the need for an explainable agent to justify why certain candidates are ranked lower or not included in the solution set. The text also mentions the ability to question the selection of subplans or proof trees in hierarchical solutions, emphasizing the importance of explaining decision-making processes. The curr_chunk discusses the abilities needed for an intelligent agent to provide preference explanations in heuristic search. This includes generating and ranking solutions based on explicit criteria, comparing ranked solutions, and communicating solution differences. The details of these abilities depend on the scoring and ranking process used. The curr_chunk discusses different schemes for ordering candidates in heuristic search, such as using weighted scores or a lexicographic function. It also mentions using user profiles as heuristics to guide search and rank solutions, with examples like personalized route planning and scheduling. The curr_chunk discusses two forms of self-explanation in heuristic search: best-first search through partial routes and repair-space search through complete schedules. It raises the question of which form is more useful for humans interacting with intelligent agents. In this paper, the author discusses two forms of self-explanation in heuristic search: best-first search through partial routes and repair-space search through complete schedules. The most appropriate form of explanation depends on the user's aims, with researchers interested in heuristic search favoring process explanations. This group includes cognitive psychologists and AI researchers concerned with system operation and search mechanisms. Preference explanations will be favored by system users interested in the results of heuristic search, including end users of autonomous agents and AI researchers. This group includes those who use recommender systems but have little knowledge of how they operate, yet still want to understand why one item is ranked higher than another. Tasks involving explanations of complex multi-step reasoning typically occur in planning domains. A planning system can support both forms of explanation discussed above by storing choices, scores, and selected alternatives during the search process. It can also find multiple solutions, rank them, and provide preference explanations. Plan execution involves monitoring the environment for anomalies and deciding whether to continue or revise the plan. The role of preference accounts in executing fully grounded plans is less obvious, but frameworks like BID2 allow multiple choices that can be ranked by value. Conceptual inference supports agency by drawing conclusions using deductive or abductive reasoning. Conceptual inference supports preference explanations by evaluating alternative derivations based on criteria like reasoning chain length or default assumptions. Explainable agents must be able to answer questions about their decision making in a way understandable to humans, with natural language being a possible modality for input and output. In the planning context, process-oriented queries can ask about actions considered, scoring, expected results, and the selected action. In the planning context, questions can be asked about actions considered, scoring, expected results, and the selected action. Context about the situation is necessary to identify and retrieve relevant decisions. Questions can be asked in natural language or through a graphical interface displaying the search tree for process explanations. The user can specify both the plan and situation by clicking on a node in the tree. In the planning context, questions can be asked about actions considered, scoring, expected results, and the selected action. A drop-down menu would let the user indicate preferences for knowing about choices, evaluation scores, or the selected option. A graphical interface simplifies preference explanations by displaying alternative solutions and their scores on each criterion. The user can compare solutions, propose candidates, or drill down into hierarchical solutions to inspect rankings for subtasks. The agent must respond in terms the user will understand, using natural language answers with relevant domain terms. In the planning context, questions can be asked about actions considered, scoring, expected results, and the selected action. A drop-down menu would let the user indicate preferences for knowing about choices, evaluation scores, or the selected option. Graphical interfaces offer another way to answer process-related questions, say by highlighting selected choices in the search tree, or preference-oriented ones, say by graphing the scores and weights of solution criteria. Systems that combine natural language and graphical interactions may be desirable for different user preferences. In this paper, the author discusses explainable agents, focusing on the reasoning behind complex decision making. Two types of explanations are distinguished: process accounts and preference accounts. The author proposes that researchers interested in heuristic search details will prefer process accounts, while end users will lean towards preference accounts. Plan generation, plan execution, and conceptual inference support both types of explanation, with execution posing challenges for preference accounts. Different modalities for presenting and answering explanation-related questions are also explored. Research on explainable agency is deemed a high priority in planning circles. Research on explainable agency is a high priority in planning circles and the broader AI community. Intelligent agents must be able to explain their reasoning in a way that is understandable to humans and relevant to their goals. Different users will have varying priorities and problem types, so frameworks supporting a range of preferences are needed. The first steps should involve designing, implementing, and demonstrating examples of explainable agents with identified abilities. This experience will uncover additional challenges for developing truly trustworthy intelligent systems."
}