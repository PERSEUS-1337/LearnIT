{
    "title": "H1gjM1SFDr",
    "content": "High-dimensional data often lie in or close to low-dimensional subspaces. Sparse subspace clustering methods with sparsity induced by L0-norm, such as L0-Sparse Subspace Clustering (L0-SSC), are more effective than its L1 counterpart. Noisy L0-SSC is proposed to handle noisy data and improve robustness. The optimal solution achieves subspace detection property (SDP) for separating data from different subspaces. Theoretical guarantees are provided for the correctness of noisy L0-SSC on noisy data. Noisy-DR-L0-SSC is also proposed for recovering subspaces on dimensionality. Noisy-DR-L0-SSC is proposed to recover subspaces on dimensionality reduced data by first projecting the data onto a lower dimensional space and then performing noisy L0-SSC to improve efficiency. This method is effective for handling noisy data that may lie close to subspaces, unlike traditional methods that are restricted to clean data exactly in subspaces. Noisy-0-SSC is proposed to handle noisy data and improve robustness by achieving subspace detection property (SDP). Noisy-DR-0-SSC recovers subspaces on dimensionality reduced data by first projecting data onto a lower dimensional space and then performing noisy 0-SSC for efficiency. Experimental results show the effectiveness of both methods in clustering scientific data in various fields. Subspace clustering algorithms, like Sparse Subspace Clustering (SSC) and 0-Sparse Subspace Clustering (0-SSC), are effective in separating high-dimensional data into clusters based on underlying subspace structures. These methods aim to partition data so that data belonging to the same subspace are identified as one cluster. Sparse subspace clustering methods, such as SSC and 0-SSC, effectively separate data based on subspaces. The Subspace Detection Property ensures that similarity between data from different subspaces vanishes in the sparse similarity matrix, leading to strong clustering performance. When subspaces are independent or disjoint, solving the canonical sparse linear representation problem using data as the dictionary satisfies the SDP under certain conditions. SSC has been successfully applied to deep neural networks. Sparse Subspace Clustering (SSC) is a method used to separate data based on subspaces. It has been applied to deep neural networks and can handle noisy data close to disjoint or overlapping subspaces. The method achieves subspace detection property with high probability, as shown in theoretical results by Soltanolkotabi & Cands (2012). Noisy 0 -SSC enhances SSC by guaranteeing clustering correctness on noisy data, addressing the limitation of SSC in handling noisy data close to disjoint or overlapping subspaces. The theoretical analysis establishes the correctness of noisy 0 -SSC in terms of the subspace detection property under both deterministic and randomized models. Noisy-DR-0 -SSC is a model that projects data onto a lower dimensional space before performing noisy 0 -SSC, proving to recover the underlying subspace structure effectively. The approach is robust to noise and demonstrates advantages over its 1 counterpart. In this paper, notations for subspace clustering on noisy data are introduced. The uncorrupted data matrix Y is defined as [y1, ..., yn] \u2208 Rd\u00d7n, where d is the dimension. Various mathematical symbols and operations are explained, such as the Frobenius norm, singular values, and projection operators. The uncorrupted data matrix Y = [y1, ..., yn] \u2208 Rd\u00d7n, where d is the dimensionality and n is the data size. The data X are normalized with unit 2-norm in deterministic noise model. The analysis can be extended to a random noise model with max ni \u2264 \u03b4. Random noise model assumes columns of Z are sampled i.i.d. with max ni \u2264 \u03b4. This model does not require spherical symmetric noise as in 0-SSC Yang et al. (2016). The subspace detection property for noisy and noiseless 0-SSC is defined in Definition 1, where Z* represents the optimal solution. It states that Z i is a nonzero vector, with nonzero elements corresponding to columns of X from the same subspace as y i. This property holds for both noisy and noiseless scenarios. The subspace detection property for noisy and noiseless 0-SSC is analyzed through deterministic, semi-random, and fully-random models. In the deterministic model, subspaces and data are fixed, while in the semi-random model, subspaces are fixed but data is independently distributed. The fully-random model assumes both subspaces and data are independently and identically distributed. Theoretical results on subspace detection property for noisy 0-SSC are presented under deterministic and randomized models. Data is normalized to unit norm and lies on a sphere. Two subspaces, S1 and S2, are in a three-dimensional space. The external subspace is the span of yi in S1 and yj in S2. The intersection of this external subspace and S1 is a dashed line yi OA. The assumption of general position is introduced before analyzing noisy 0-SSC. The distance between a point x and a subspace S is defined as d(x, S) = inf y\u2208S x\u2212 y 2. External subspaces are defined as the spanned by a set of linear independent points {y ij } L j=1 \u2286 Y. Data points in Y (k) are considered away from external subspaces if each of them is away from its associated external spaces. The subspace detection property for noisy 0-SSC holds if data points in Y(k) are away from external subspaces. Definitions of minimum restricted eigenvalue and subspace separation margin are needed. Perturbation bound for distance between data point and subspaces is crucial for establishing conditions when subspace detection property holds for noisy 0-SSC. Lemma 1 states that if Y \u03b2 has full column rank and certain conditions are met, then the optimization problem of noisy 0-SSC is separable. Lemma 2 shows that the optimal solution to the noisy 0-SSC problem is also the optimal solution to a sparse approximation problem with uncorrupted data as the dictionary. If data points are away from external subspaces, the subspace detection property holds for noisy 0-SSC. The subspace detection property holds for x i with the optimal solution to the noisy 0-SSC problem, where nonzero elements of \u03b2 * correspond to the columns of X from the same subspace as y i. This result is based on Lemmas 1 and 2, showing the separability of the optimization problem and the connection to a sparse approximation problem with uncorrupted data. Lemma 2 and Remark 2 discuss the conditions for noisy 0-SSC, highlighting the similarities with noiseless 0-SSC when \u03b4 = 0. Theorem 1 establishes geometric conditions for subspace detection in noisy 0-SSC, often linked with the optimal solution \u03b2*. The correctness of noisy 0-SSC is guaranteed based on \u03bb and geometric conditions, independent of the optimal solution. Minimum distance M i > 0 between y i and its external subspaces is crucial for this guarantee. Theorem 2 discusses the subspace detection property for noisy 0-SSC under deterministic model, with conditions related to \u03bb. It defines quantities \u00b5 r and \u03c3 X,r for clean and noisy data, with specific requirements for the optimal solution \u03b2* and the point x i. The conditions for \u03bb 1 and \u03bb 2 can be chosen accordingly, ensuring the subspace detection property holds. Noisy 0-SSC encourages sparse solutions with a large \u03bb for subspace detection. The analysis focuses on random distribution of clean data in each subspace, assuming isotropic samples on a sphere. Condition (a) ensures bounded projections of data points onto unit vectors. The lemma provides a geometric concentration inequality for the distance between a point y and its external subspaces, ensuring the subspace detection property under randomized models. It gives a lower bound for the minimum distance between a point y and its external subspaces. The lemma ensures the subspace detection property under randomized models by providing a geometric concentration inequality for the distance between a point and its external subspaces. It does not require uniform distribution of subspaces, unlike previous analyses. Noisy Dimensionality Reduced 0-SSC (Noisy-DR-0-SSC) is proposed to address the high computational cost of noisy 0-SSC on high-dimensional data. It performs subspace clustering on dimensionality reduced data by first obtaining the compressed data X = PX with a linear transformation P, and then applying noisy 0-SSC on the compressed data. This approach improves efficiency by operating on the lower-dimensional data structure. The proposed method involves using a random projection induced by randomized low-rank approximation of the data to perform Noisy-DR-0-SSC, which aims to preserve subspace information while removing uninformative dimensions efficiently. Randomized algorithms are known for their efficiency and have been extensively studied in computer science and numerical linear algebra. Randomized algorithms are widely studied in computer science and numerical linear algebra for accelerating matrix computations. A random matrix is generated and QR decomposition is performed to obtain the basis of its column space. This leads to a randomized low-rank decomposition of the matrix X, preserving subspace information efficiently. The randomized low-rank decomposition of data X using random projection is crucial for preserving subspace information efficiently. The subspace detection property on the dimensionality-reduced dataX is maintained by ensuring certain conditions hold after linear transformation. The optimal solution \u03b2 * to the noisy 0 -SSC problem is denoted, with additional quantities defined for analysis. The subspace detection property holds for Noisy-DR-0 -SSC under a deterministic model, with specific conditions on the optimal solution and dimensions. The text discusses the optimization of the objective function for noisy 0-SSC and Noisy-DR-0-SSC using Proximal Gradient Descent (PGD). It mentions conditions for subspace detection property, updates to the variable \u03b2 in each iteration, and convergence to a critical point. Theorems from Yang & Yu (2019) are referenced to show boundedness of the optimal solution \u03b2*. The text discusses the optimization of the objective function for noisy 0-SSC and Noisy-DR-0-SSC using Proximal Gradient Descent (PGD). Theorems from Yang & Yu (2019) show that \u03b2 is bounded and conditions for global optimality. Performance comparison with other clustering methods is also demonstrated. Measures like Accuracy (AC) and Normalized Mutual Information (NMI) are used to evaluate clustering methods. Noisy-DR-0-SSC and noisy 0-SSC consistently outperform other methods. The best clustering accuracy is achieved when \u03bb is between 0.5 and 0.95. Experimental results on CMU Multi-PIE data are presented in Table 2. Cluster accuracy for SSC-OMP on extended Yale-B dataset is reported according to You et al. The cluster accuracy of SSC-OMP on the extended Yale-B dataset is reported by You et al. (2016). Noisy-DR-0-SSC is significantly faster than noisy 0-SSC, with provable recovery of subspaces from noisy data. Experimental results demonstrate the superior performance of noisy 0-SSC and Noisy-DR-0-SSC in subspace detection. The effectiveness of both noisy 0-SSC and Noisy-DR-0-SSC in recovering subspaces from noisy data is demonstrated through experimental results. The subspace detection property holds for all cases, as proven through analysis. The perturbation of distance to subspaces is discussed in Lemma B, showing the difference in distance of a point to the column space of two matrices. The proof involves the projection of the point onto the subspace and the perturbation bound on the orthogonal projection. The proof of Lemma 1 involves showing that X beta has full column rank and satisfies certain inequalities. The optimal solution to the sparse approximation problem is discussed in the proof of Lemma 2, where the columns of U QA are orthonormal in the singular value decomposition. The columns of U QA are orthonormal in the singular value decomposition, leading to \u03c3min(QA) = \u03c3min(A). After projection, the noise in the projected data is bounded by \u03b4. Additionally, for \u03b2 \u2208 R n with \u03b2 0 = r \u2264 r0, |\u03c3min(X \u03b2 ) \u2212 \u03c3min(X \u03b2 )| \u2264 Cp,p 0. After projection, the noise in the projected data is bounded by \u03b4. Additionally, for \u03b2 \u2208 R n with \u03b2 0 = r \u2264 r0, |\u03c3min(X \u03b2 ) \u2212 \u03c3min(X \u03b2 )| \u2264 Cp,p 0."
}