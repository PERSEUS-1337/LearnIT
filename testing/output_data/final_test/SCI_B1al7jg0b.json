{
    "title": "B1al7jg0b",
    "content": "Catastrophic interference in continual learning is addressed by the \"Conceptor-Aided Backprop\" (CAB) algorithm, which shields gradients using conceptors to prevent degradation of previously learned tasks. CAB outperforms other methods on disjoint and permuted MNIST tasks. Continual learning aims for machines to retain previous skills while learning new ones, but neural networks often forget previous tasks when trained on a sequence of tasks. Neural networks face the challenge of catastrophic interference, where they forget previous tasks when learning new ones. Various methods, such as elastic weight consolidation (EWC) and incremental moment matching (IMM), have been proposed to address this issue in continual learning. EWC uses the posterior distribution of old task parameters as a prior for new tasks, while IMM introduces mean-IMM and mode-IMM methods. These approaches have shown significant improvements in continual learning in neural networks. In the field of Reservoir Computing, an effective solution to continual learning using conceptors was proposed to train a recurrent neural network. Conceptors are a neuro-computational mechanism used in various neural information processing tasks. In this paper, a conceptor-aided backpropagation (CAB) algorithm is proposed to train feed-forward networks. CAB computes a conceptor for each layer to preserve linear transformations during gradient descent. Experimental results show competitive performance. The paper discusses conceptors, incremental learning, stochastic gradient descent, and compares CAB's performance. The paper introduces the CAB algorithm for training feed-forward networks using conceptors to preserve linear transformations during gradient descent. It compares the algorithm's performance on different tasks and provides a brief overview of conceptor theory and its application in training linear readouts of recurrent neural networks. The optimization problem involves minimizing a loss function with a control parameter \u03b1. The closed-form solution is obtained using the correlation matrix R and the identity matrix I. The conceptor C acts as a soft projection matrix on the linear subspace of x samples, preserving vectors in the subspace and denoising when noise is added. The quota Q(C) of a conceptor is defined as the mean singular value. The quota Q(C) of a conceptor is the mean singular values, measuring the fraction of total dimensions claimed by C. Logic operations on matrix conceptors can be defined, including \u00acC softly projecting onto the orthogonal complement of C, C i \u2228 C j representing the union of two sets of sample points, and C i \u2227 C j reflecting de Morgan's law. Geometry of Boolean operations on 2-dimensional conceptors is illustrated in Figure 2. The OR and AND operations on 2-dimensional conceptors create ellipsoids that enclose or are contained in the argument conceptor's ellipsoids. Conceptors are used for continual learning in a linear model trained on multiple tasks sequentially. The conceptor Cj is a soft projection matrix onto the linear subspace spanned by input patterns from the j-th task. The conceptor Cj is a soft projection matrix onto the linear subspace spanned by input patterns from the j-th task. The method involves storing input and output vectors, computing a conceptor for each task, and utilizing a helper conceptor to characterize the available memory space for the task. The conceptor Cj is a soft projection matrix onto the linear subspace spanned by input patterns from the j-th task. The weight increment W j inc does not interfere much with the previously learned weights W j\u22121 due to regularization, allowing the algorithm to exploit only the components of input vectors in the still unused space for new tasks. The algorithm described in the previous section computes W j inc using ridge regression or stochastic gradient descent. The regularization term in the cost function helps nullify input components not in the linear subspace characterized by F j, allowing W j inc A j\u22121 x j to converge to 0. The algorithm computes W j inc using ridge regression or stochastic gradient descent, with the regularization term helping nullify input components not in the linear subspace characterized by F j. The update rule for W j involves using a mini-batch of size n B and a conceptor-projected input vector. Regularization is applied on the weight increment W j inc rather than the final weight W j. The conceptor-aided algorithm guides gradients in a multilayer feed-forward network using a matrix conceptor from previous tasks during error back-propagation. It applies the conceptor-aided SGD technique in every layer of the network. The incremental training method with CAB involves updating network parameters using a matrix conceptor from previous tasks during error back-propagation. This method guides gradients in each layer of the network, different from classical backpropagation. The incremental training method with CAB involves updating network parameters using a matrix conceptor from previous tasks during error back-propagation. This method guides gradients in each layer of the network, different from classical backpropagation. Conceptors remain the same until convergence for each task, and an optimal aperture can be found through cross-validation search. The conceptor-aided backprop involves passing input vectors through a feed-forward network to compute the cost function and update the conceptor for already used space in every layer. The cost function is computed by adding the loss for each task with a regularizer to obtain the total cost. The backward procedure of conceptor-aided backprop involves computing the gradient of the loss function on the activations to update the network parameters. The conceptor-aided backprop algorithm involves projecting the gradient of weights by a conceptor matrix to indicate free memory space, computing gradients on pre-nonlinearity activations, adding regularization terms, and propagating gradients to lower-level hidden layers. Performance evaluation was done on the permuted MNIST experiment. In a proof-of-concept demonstration, a feed-forward network with [784-100-10] neurons was trained to classify 10 permuted MNIST datasets. The network used logistic sigmoid neurons and mean squared error as the cost function, optimized with Vanilla SGD. EWC was also tested on the same task with the same network architecture for comparison. The EWC algorithm was implemented with a learning rate of 0.01 and a weight of 15 for the Fisher penalty term. The average testing accuracy of 95.2% was achieved after learning all 10 tasks sequentially. Another experiment with EWC on a network with higher capacity resulted in an average accuracy of around 93% after learning 10 tasks sequentially. During incremental learning of 10 permuted MNIST tasks, the network's ability to learn new tasks decreases as the input space becomes more crowded. The quota of memory space in the input layer increases less with each new task, indicating less free space available. Despite this, the network can still learn new tasks based on their input components in non-overlapping space. In an experiment involving incremental learning of 10 permuted MNIST tasks, the network's ability to categorize disjoint MNIST datasets into 10 classes was tested. The original MNIST dataset was split into two disjoint datasets, and the network had to learn and classify them sequentially. The state-of-the-art accuracy achieved was 94.12% by BID13 using IMM, while EWC achieved an average accuracy of 52.72%. A feed-forward network with [784-800-10] neurons and logistic sigmoid nonlinearities in hidden and output layers was trained for this task. The network used for testing IMM and EWC had [784-800-800-10] rectified linear units (ReLU), while CAB achieved better performance with fewer layers and neurons. The time complexity of computing a conceptor by ridge regression is O(nN^2 + N^3), where n is the number of samples and N the number of features. The wall time taken to compute a conceptor from the entire MNIST training set is also mentioned. The training set consists of 55000 images with 784 pixels, taking 0.42 seconds of CPU time. The conceptor-based incremental ridge regression algorithm was reviewed, and a conceptor-aided backprop algorithm was designed to minimize interference between tasks in neural networks. In Jaeger (2014), different scenarios for continual learning in reservoir computing are investigated using conceptors to analyze the relatedness of learning tasks. The permuted MNIST task represents unrelated tasks, while the disjoint MNIST task represents tasks from the same parametric family. Ongoing research focuses on enabling conditions for continual learning in geometrical terms. The ongoing research focuses on enabling conditions for continual learning in geometrical terms, with future analysis funded by the European H2020 collaborative project NeuRAM3."
}