{
    "title": "r1ez_K1wPH",
    "content": "In this paper, the large-sample behaviors of Q-value estimates are investigated with closed-form asymptotic variances. Confidence regions for Q-value and optimal value functions are efficiently constructed, along with policies to minimize estimation errors. A policy exploration strategy based on estimating relative discrepancies among Q estimates is proposed, showing superior performance in numerical experiments compared to benchmark approaches. The study focuses on the classical reinforcement learning problem where the agent interacts with a random environment in a Markov decision process, aiming to maximize accumulated discounted rewards over time while uncertain about the true dynamics. The paper investigates statistically efficient methodologies to quantify errors and uncertainties in Markov decision processes. It focuses on large-sample behaviors of estimated Q-value and optimal value function, showing asymptotic convergence to a computable distribution as data sizes increase. This allows for the construction of accurate policies to obtain better decision-making outcomes. The paper focuses on statistically efficient methodologies to quantify errors and uncertainties in Markov decision processes. It aims to construct accurate confidence regions for quantities related to the optimal policy and design good exploration policies using tight error estimates. This is motivated by applications such as autonomous driving, where an agent collects experience before deploying the optimal policy to gain rewards. The paper proposes an efficient strategy for exploration in Markov decision processes by optimizing the worst-case estimated relative discrepancy among Q-values. This approach, termed Q-OCBA, utilizes Q-value estimates and randomized policies to achieve the optimal policy. Our technique, Q-OCBA, utilizes Q-value estimates and randomized policies to achieve optimal allocation, consistently outperforming other exploration policies. It resolves technical challenges in previous work by Mannor et al., allowing for generalization of variance results to Q-values and optimal value functions. Our results utilize an implicit function theorem applied to the Bellman equation to obtain gradients for Q-values and translate to the optimal value function. This extends to constrained MDP and approximate value iterations, distinguishing our work from dynamic treatment regimes literature. Our infinite-horizon results on optimal value and Q-value are unique, especially in the non-unique policy case. In this paper, the authors describe their MDP setup and notations, present results on large-sample behaviors, explore exploration strategies, and provide experimental results. They also generalize theoretical results to constrained MDPs and problems using approximate value iteration, include numerical experiments, and provide proofs. The focus is on an infinite horizon discounted reward MDP with state space S, action space A, reward function R, transition probabilities P, discount factor \u03b3, and initial state distribution \u03c1. The MDP setup includes state space S, action space A, random reward R(s, a), transition probability P(s|s, a), discount factor \u03b3, and initial state distribution \u03c1. The reward and transition probability distributions are unknown to the agent. A policy \u03c0 maps states to action probabilities. The value function associated with a policy \u03c0 is defined as V * = V \u03c0 * and Q-value is denoted by Q(s, a). The Bellman equation for Q takes the form for any (s, a) \u2208 S \u00d7 A. Under Assumption 2, the optimal policy \u03c0 * is unique and deterministic with \u03c0 * (a|s) = 1 (a = a * (s)). The text discusses the statistical quantities arising from data observations, including sample mean and variance of rewards, empirical transition matrix, and the construction of an estimate of Q called Qn. It also introduces V*n(s) and \u03c7*n as empirical errors due to noises in the data. The text focuses on empirical errors in collected data, assuming MDP or Q-value evaluation can be done offline. Results on the asymptotic behaviors of Qn and V*n are presented, with an assumption on the exploration policy \u03c0. Assumption 3 states that the Markov chain with transition probability P\u03c0 is positive recurrent, with a unique stationary distribution denoted as w. The text discusses the asymptotic normality of Qn under exploration policy \u03c0, with assumptions on the collected data and positive recurrent Markov chain. It introduces notations for multivariate Gaussian distribution and indexing rules for vectors and matrices. Theorem 1 states that under certain assumptions, Qn is a consistent estimator of Q. The asymptotic variance \u03a3 is derived from the delta method and involves sensitivities of Q with respect to parameters and variances of parameter estimates. The asymptotic normality of V * n and \u03c7 * n can be established using Theorem 1. Corollary 1 establishes the asymptotic normality of V * n and \u03c7 * n under certain assumptions. The results are derived using an implicit function theorem on the Bellman equation, allowing for generalization to optimal value functions and distributional statements. Another potential approach involves perturbation analysis on the linear program representation of the MDP. The perturbation analysis on the linear program representation of the MDP provides gradient information of V* and Q. Theorem 1 and Corollary 1 allow for statistical inference, constructing confidence regions for subsets of Q-values. A key quantity of interest is Q(s, a1) - Q(s, a2), aiding in exploration policy design. If the optimal policy is not unique, estimated Q and V* may fluctuate between optimal actions. In this scenario, when there is no unique optimal policy, the estimated Q and V* values may vary between optimal actions. This leads to a complex large-sample behavior, where the limit distribution can become non-Gaussian. The sensitivity to certain parameters can differ based on the perturbation direction, due to solution non-uniqueness in the MDP representation. This non-degeneracy can result in a bias behavior of 1/ \u221a n-order, similar to the concept of \"non-regularity\" in decision-theoretic regression. The policy is highly sensitive to estimation noises and incurs a 1/ \u221a n-order bias behavior. When there is no unique optimal policy, a non-unique optimal policy captures this behavior, leading to a bias arising when K > 1. Generalizations of large-sample results are developed for constrained MDP and approximate value iteration. Results are utilized to design exploration policies focused on minimizing the probability of selecting a suboptimal policy for the accumulated reward. A strategy is proposed to maximize the worst-case relative discrepancy among all Q-value estimates. Our procedure aims to maximize the minimum relative discrepancy among Q-value estimates, focusing on the difficulty in obtaining the optimal policy. The criterion aims to make the problem \"easiest\" by minimizing the probability of suboptimal selection between close Q-values for different actions. The Q-OCBA procedure aims to minimize suboptimal selection between close Q-values for different actions. It involves a multi-stage optimization and parameter update scheme as data is sequentially estimated. The Q-OCBA procedure aims to minimize suboptimal selection between close Q-values for different actions through a multi-stage optimization and parameter update scheme. Admissible policies are derived by running a Markov chain on exploration actions, optimizing over the set of stationary distributions for tractability. The algorithm Q-OCBA is described for practical implementation. The Q-OCBA procedure aims to minimize suboptimal selection between close Q-values for different actions through a multi-stage optimization and parameter update scheme. Algorithm 1 describes Q-OCBA, using two stages with a specified exploration policy. The algorithm involves sequential updating rules for exploration, with a focus on estimating coefficients using plug-in estimators based on collected data. In the Q-OCBA procedure, plug-in estimators are used to estimate Q-values based on earlier data. Numerical experiments support large-sample results and compare Q-OCBA performance against benchmark methods using the RiverSwim problem. Rewards are given at boundary states, and a policy with 0.8 probability of swimming right is used. The coverage rates of constructed 95% CIs are shown for small and large m values, with accurate CI coverages observed for large sample sizes. The efficiency of the exploration policy is then investigated by comparing Q-OCBA with K = 2 to four benchmark policies. The study compares different exploration policies, including random exploration, UCRL2, and PSRL with varying parameters. A two-stage implementation is used for all policies, with an initial warm start for better performance. The probabilities of obtaining the optimal policy are compared, showing the importance of the warm start for UCRL2 and PSRL. In numerical experiments, Q-OCBA outperforms other methods in selecting the correct policy. The benchmark policies perform worse with larger values of r L due to a misalignment between Q-values and exploration needs. Q-OCBA focuses on efficient exploration to minimize incorrect policy selection, unlike UCRL2 and PSRL which aim to balance exploration-exploitation trade-off. Our goal is to minimize incorrect policy selection by utilizing variance information from the first stage. Additional results on large-sample behaviors for constrained MDPs and estimations based on approximation value iteration are presented. The constrained MDP setting for budgeted decision-making and safety-critical applications is considered. We aim to maximize long-run accumulated discounted reward while constraining the long-run accumulated discounted cost. Data is assumed to come in as before, with observations on the incurred. In this section, we focus on estimating the error of the optimal value in constrained MDPs. An optimal policy is characterized as a \"split\" policy, allowing randomization between two actions at a particular state. The randomization probability is referred to as the mixing parameter \u03b1*. The randomization probability in constrained MDPs is known as the mixing parameter \u03b1*. Theorem 3 states conditions for a unique optimal policy, including cases where randomization occurs between two actions at one state with \u03b1*. The mixing parameter \u03b1* in constrained MDPs determines the optimal policy. When the constraint is non-binding, it reduces to the unconstrained scenario. If the constraint is binding, \u03b1* must be chosen to ensure equality, leading to additional noise in the estimation of V*. Updating a large state space using T \u00b5 R ,P (.) may be computationally infeasible, so approximate value iteration with a dimension-reducing mapping M is used. In constrained MDPs, the mixing parameter \u03b1* determines the optimal policy. When the constraint is non-binding, it reduces to the unconstrained scenario. Updating a large state space using T \u00b5 R ,P (.) may be computationally infeasible, so approximate value iteration with a dimension-reducing mapping M is used. In this setup, Q M is a fixed point of the operator M \u2022 T \u00b5 R ,P (\u00b7), and large-sample error estimates are derived. Assumptions are made on the generalization map M g to guarantee the existence of Q M. In constrained MDPs, the mixing parameter \u03b1* determines the optimal policy. Updating a large state space using T \u00b5 R ,P (.) may be computationally infeasible, so approximate value iteration with a dimension-reducing mapping M is used. Q M is a fixed point of the operator M \u2022 T \u00b5 R ,P (\u00b7), with large-sample error estimates. Assumptions on the generalization map M g ensure the existence of Q M. Markov Chain with transition probabilityP \u03c0, set of states {(s, a) : s \u2208 S 0 , a \u2208 A} in same communication class, positive recurrent. Theorem 4 states that under certain assumptions, local approximation methods like linear interpolation, k-nearest neighbors, and local weighted average satisfy Assumption 4. Additional numerical experiments and results on estimation quality of Q-values, V * and \u03c7 * are presented in Sections B.1 and B.2. In Section B.2, additional numerical results are provided on the estimation quality of Q-values, V*, and \u03c7*. The coverage rates converge to 95% as the number of observations increases, with sample sizes of 5 \u00d7 10^4 or lower being sufficient to elicit asymptotic results. The coverage rates for approximate update converge slower compared to exact update, with a sample size of 5 \u00d7 10^4 not being sufficient. Larger sample sizes, on the order of 10^7, are needed for approximate update to reach the nominal coverage of 95%. The rates of convergence to the nominal coverage vary for different values of \u03c0(1|s). When \u03c0(1|s) = 0.85, convergence is fastest, reaching 95% coverage at n = 10^5. However, for \u03c0(1|s) = 0.8 and 0.9, larger sample sizes are needed for convergence. Discrepancies in estimates of Q, V*, and \u03c7* can occur when coverage is far from the nominal rate. When coverage is close to 95%, all quantities seem to attain accuracy simultaneously. In Q-OCBA, convergence behaviors predicted by Theorem 1, Corollary 1, and Theorem 4 are observed to hold. The second-stage exploration policy maximizes the worst-case relative discrepancy among all Q-value estimates. To obtain the best estimate of \u03c7*, it is beneficial to consider solving min to derive the optimal second-stage exploration policy \u03c0w. Comparisons of 95% CI lengths and coverages for different exploration policies are shown in Table 9. In Q-OCBA, the strategy aims to shorten CI lengths of estimates. A total observation budget of n = 10^4 is used, with 30% devoted to the initial stage. Comparisons with pure RE and \u03b5-greedy are made, showing that Q-OCBA achieves nominal 95% coverages for Q-values and \u03c7* estimates. Q-OCBA generally leads to much shorter CI lengths. Q-OCBA leads to significantly shorter confidence interval lengths compared to other methods, with reductions of up to 80%. It also shows more stable performance than RE and \u03b5-greedy, especially for larger values of r L. \u03b5-greedy with a small value of \u03b5 can perform similarly to Q-OCBA when r L = 1, but struggles to explore all (s, a) pairs for larger values of r L. Q-OCBA outperforms pure RE and \u03b5-greedy, with at least 40% shorter CI lengths for the estimates. The performance of \u03b5-greedy depends on the exploration of (s, a) pairs with larger Q-values, which can vary with changes in r L. This misalignment leads to different performances for \u03b5-greedy. In this section, the proofs of the main results are presented. By treating P as an N-dimensional vector, the partial derivatives of F are denoted. P is interpreted as a transition matrix of a Markov Chain, and the implicit function theorem is applied to the equation F(Q, \u00b5 R, P) = 0. The implicit function theorem is applied to the equation F(Q, \u00b5 R, P) = 0, resulting in a unique continuously differentiable function \u03c6. The partial derivatives of \u03c6 satisfy certain conditions, and the asymptotic normality of \u03c7 \u03c0 * n follows from the continuous mapping theorem. Additionally, the delta method is used to obtain further results. The asymptotic normality of the estimated value function under a given policy\u03c0 is established by finding the fixed point of the empirical Bellman equation. Corollary 2 states that under certain assumptions, the transition matrix P\u03c0 and diagonal matrix W have specific properties. The proof involves defining a mapping F\u03c0 and applying the implicit function theorem to find a unique continuously differentiable function \u03c6\u03c0. The proof involves defining a mapping F\u03c0 and applying the implicit function theorem to find a unique continuously differentiable function \u03c6\u03c0, leading to the conclusion. The MDP problem is written in its LP representation, with the dual problem's decision variables representing occupancy measures. Non-unique optimal policies in the MDP result in non-unique optimal solutions in the dual problem, indicating degeneration of the primal problem. Multiple choices for the set of basic variables exist in this case. In this case, there are multiple choices for the set of basic variables at the optimal solution. When coefficients perturb along a direction, the objective value changes by fixing the basic variables. The set of directions can be partitioned into subsets, where perturbation affects the LP optimal value. The argument focuses on the LP with objective value s \u03c1(s)V (s), and the same can be repeated for each V (s) by setting \u03c1(s) = e s. The directional Jacobian of V with respect to P, \u00b5 R is defined. The proof of Theorem 3 uses the LP representation of the constrained MDP. The occupancy measure x s,a satisfies the LP, and once solved, it can be translated. The LP representation of the constrained MDP involves solving for an optimal solution x * s,a. The number of structural constraints is m s + 1, with a unique and non-degenerate optimal solution. Perturbing the parameters does not lead to negativity for non-basic variables. Two cases are considered based on the binding of the first constraint, leading to either a deterministic optimal policy or a perturbed policy with retained constraints. In this case, the analysis reduces back to Corollary 2. x s,a > 0 for only one a, except for one state s r where x sr,a * 1 > 0 and x sr,a * 2 > 0 for two distinct actions. Perturbing the parameters retains basic and non-basic variables, with the optimal policy \u03c0 * split at the same state. The mixing parameter \u03b1 * is defined as \u03c0 * (a * 1|s r ), and \u03c0 * (a * 2|s r ) = 1 \u2212 \u03b1 * . By applying the implicit function theorem, there exists a function \u03c6 L such that L \u03c0 * (s) = \u03c6 L (\u00b5 C , P, \u03b1 * ). V * can be viewed as a function of \u00b5 R , P. The Jacobian of V* with respect to \u00b5R, \u00b5C, and P is denoted as \u2207\u00b5R,\u00b5C,P V* (\u00b5R, \u00b5C, P). It can also be denoted as \u2207\u00b5R,\u00b5C,P,\u03b1V* (\u00b5R, P, \u03b1*). The notation \u2207x f represents the multi-dimensional Jacobian matrix, while \u2202x f is used for scalar Jacobians. The equation \u2207\u00b5R,\u00b5C,P V* (\u00b5R, \u00b5C, P) = \u2207\u00b5R,\u00b5C,P,\u03b1* V* (\u00b5R, P, \u03b1*)[I, \u2207\u00b5R,\u00b5C,P \u03b1*(\u00b5C, P)T]T = \u2207\u00b5R,\u00b5C,P V* (\u00b5R, P, \u03b1*) + \u2202\u03b1* V* is used for derivations. The derivation of \u2207\u00b5R,\u00b5C,P V* (\u00b5R, P, \u03b1*) and \u2207\u00b5R,\u00b5C,P L \u03c0* (\u00b5C, P, \u03b1*) follows the same analysis as G\u03c0 and H\u03c0 V in the proof of Corollary 2. Proof of Theorem 4 involves assigning auxiliary random variables to \u03bcR,n and Pn for all i /\u2208 S0. Changing the distribution of [\u03bcR,n, Pn] S\\S0 will not affect QMn. The stationary distribution w of transition matrix P \u03c0w is obtained by letting i / \u2208 S 0 for all i \u2208 S."
}