{
    "title": "rylxrsR9Fm",
    "content": "In this paper, a neural network framework called neuron hierarchical network (NHN) is proposed, focusing on the hierarchy of neurons rather than layers. The model prunes low-sensitivity neurons and adds new ones, emphasizing individual neuron relations over layers. Random architecture search is used to find the best base model, while evolutionary search determines the optimal locations and connections for added neurons. Experimental results demonstrate that NHN outperforms handcrafted and randomly searched architectures on Cifar-10 with higher test accuracy, fewer parameters, and less searching time. Neural networks can be designed by human experts or search algorithms, with models having more parameters generally performing better. Redundancy of parameters exists in handcrafted and discovered architectures. Pruning unimportant neurons highlights hierarchical relations, but accuracy may decrease after parameter reduction. A heuristic procedure is proposed to construct neuron-based network architectures. The proposed heuristic procedure constructs neuron-based network architectures by pruning redundant connections and neurons, adding new neurons to strengthen hierarchy, achieving competitive performance. NHN outperforms DenseNet, SMASH BID4, and hierarchical representation with fewer parameters. Plain networks like VGG nets have one input and output path per hidden layer, while branching networks split and merge computation flow. Many advanced network architectures with branching structures and skip connections are being developed to improve deep neural network performance. These architectures include stacked-between operations and multi-branching computation graphs to address gradient vanishing issues during training. Various techniques aim to weaken the hierarchy between layers by introducing complex paths in the data flow, as highlighted by architecture search algorithms. Neural architecture search (NAS) is a computationally intensive process that designs network architectures for language modeling and image classification. Efforts are being made to reduce the computational costs of NAS by designing reusable cells or sharing trained models. Recent advancements in Neural Architecture Search (NAS) aim to reduce computational costs by designing individual reusable cells or sharing trained weights between models. Techniques such as describing candidate models as subgraphs of a single directed acyclic graph (DAG) and applying genetic algorithms have shown promising results. Regularized evolution (RE) has also been proposed to improve model selection processes. Additionally, convolutional neural fabrics (CNF) and random search methods have been explored in the search for optimal CNN architectures. The search methods BID2, BID4, and BID10 have been investigated in neural architecture search. The best model found in ENAS BID33 is a 12-layer CNN with over 21 million parameters achieving 4.23% test accuracy on Cifar-10. Removing 75% - 90% parameters in convolutions does not significantly affect test accuracy. Architectures in search methods are represented as directed graphs with nodes as operation layers or blocks, not individual neurons. Random and evolutionary search tend to find complex architectures with branching paths, but pruned versions perform nearly as well. The hierarchy of neurons should work as well as the hierarchy of layers in neural architecture. A three-step course is proposed to discover the optimal neuron hierarchy for image classification: (1) discover the optimal layer hierarchy with ENAS, (2) prune unimportant neurons, and (3) randomly add new neurons to enrich the expressive capacity of neuron hierarchy networks. The three-step procedure for optimizing neuron hierarchy networks mimics the natural development of human brains. It involves creating and searching with ENAS, pruning unimportant neurons, and adding new neurons. This process improves the learning capability of neural networks by emphasizing the relation between individual neurons based on traditional layer-based models. The existence of neurogenesis in mature brains is debated, but software simulation shows its benefits. The study employs a macro search method to discover a 12-layer CNN architecture for image classification. Dropout is not used during searching and re-training to distinguish important neurons. A cosine schedule with warm restarts is used to adjust the learning rate, and Cutout is employed to enhance sample capacity and model generalization. The architecture discovered by ENAS in 2017 enriches sample capacity and improves model generalization. Network pruning methods like DNP are used to remove unimportant parameters in CNNs, reducing memory and bandwidth consumption. Grouped convolution is employed to replace global average pooling during the search for optimal layer-hierarchy models. After training the optimal model discovered by ENAS, a large portion of trained kernels are flat, prompting the removal of 85% based on standard deviation thresholds. This leads to an accuracy drop, highlighting the need to improve model representation by adding add-on neurons between existing ones. After training the optimal model discovered by ENAS, a large portion of trained kernels are flat, prompting the removal of 85% based on standard deviation thresholds. This leads to an accuracy drop, highlighting the need to improve model representation by adding add-on neurons between existing ones. Add-on neurons can be inserted between arbitrary neurons in different layers, following specific rules for data dimensions and operation types. The 12-layer architecture discovered by ENAS is divided into segments with uniform data dimensions, restricting the insertion of add-on neurons across segments. Two types of operations, 3 \u00d7 3 convolution and 5 \u00d7 5 convolution, are allowed in add-on neurons. The Location-Direction Search (LDS) algorithm is used to discover and incorporate add-on neurons in the network. The Location-Direction Search (LDS) algorithm is utilized to search for specific positions and connections for add-on neurons in the network. An evolutionary approach is used to define an initial population for location search, with each individual representing a possible add-on location combination. The search process involves training models in the population and crossing over the best models to improve the search speed for add-on neurons. The Location-Direction Search (LDS) algorithm uses an evolutionary approach to search for add-on neuron positions and connections in the network. The best models are crossed over to speed up the search for add-on neurons, with a focus on sparse selection of add-on directions to prevent new layer formation. The Location-Direction Search (LDS) algorithm employs an evolutionary approach to find optimal add-on neuron positions in the network. No mutation or crossover is used during direction search. Add-on neurons are inserted between specific layers, with a constraint on the number of add-on locations per layer. The overall location search complexity is 1.9 \u00d7 10^8. The Location-Direction Search (LDS) algorithm uses weight sharing to increase search speed. Shared weights are defined beforehand for all possible connections at all locations and directions, denoted by W = {w l,d |l \u2208 L, d \u2208 D}. This approach helps determine optimal add-on directions efficiently. The proposed approach for neuron hierarchical networks involves generating subgraphs for location or direction search using shared weights. The weights are selected from W k,t = {w l,d |l \u2208 L k,t , d \u2208 D k,t } in an asynchronous training fashion. The algorithm for the hierarchical network includes initializing the layer hierarchy network G and training it from scratch. The Cifar-10 test errors of NHN and other methods are compared, showing architectures discovered by handcrafted, random search, and evolutionary methods. The performance of Evolving DNN on Cifar-10 is reported, with various models and their corresponding parameters, error rates, and scores listed. Different techniques like Dropout, Shake-Shake regulation, Cutout, and Drop-path are denoted by specific abbreviations. Smaller score indicates better overall performance. The Cifar-10 dataset is used to train and evaluate a neuron hierarchy network. It consists of 50k training images and 10k test images. Standard pre-processing and data augmentation techniques are applied. Experiments are conducted on a single NVIDIA GTX 1080Ti video card. The experiments are conducted on a single NVIDIA GTX 1080Ti video card. ENAS finds a layer-hierarchy model with 96.55% test accuracy on Cifar-10 with Cutout in 16 hours. Without Cutout, the accuracy is 95.08%. Retraining the best model takes 9 hours. The DNP method prunes low sensitivity connections in the model, removing 85% of input connections on average. Over 70% of parameters are pruned in total. The DNP method prunes over 70% of parameters, impacting performance. Neuron hierarchy search involves gene codes for add-on locations in layers. The DNP method prunes over 70% of parameters, impacting performance. Neuron hierarchy search involves gene codes for add-on locations in layers. A negative gene value (-1) means no add-on neuron connected. Gene value of -1 generated with 0.8 probability to avoid excessive add-on neurons. Models trained for 25 iterations with batch size of 100. Submodels evaluated with batch size of 500. 5 new models generated instead of mutating best models. Cosine schedule for learning rate used, decreasing from 0.05 to 0.001. Location search takes around 10 hours to complete. Best 3 sets of discovered locations illustrated in FIG2. After discovering the best locations for add-on neurons in layers, a direction search is performed which takes about 9 hours to finish. Only weights in add-on neurons are trained during this process. Once the best locations and directions are found, the entire network is re-trained from scratch using Cutout BID9 and Dropout BID39 for better generalization. Nesterov BID31 is used as the gradient descent method with a momentum term of 0.9. The Nesterov BID31 gradient descent method with a momentum term of 0.9 and l2 regularization of 2 \u00d7 10 \u22124 is used for training. Gradient clipping BID32 with a threshold of 5.0 is applied to prevent gradient explosion. The final training takes 6 hours (260 epochs) with a test accuracy of 96.52% for NHN, which has approximately 7M parameters. NHN outperforms most methods in Table 1, but some achieve lower test errors. The NHN outperforms most methods in Table 1 with a test accuracy of 96.52% and approximately 7M parameters. It achieves lower test errors compared to other methods like DenseNet with Shake-Shake and AmoebaNet with Cutout, despite having a competitive performance score. The searching and training of the neuron hierarchy network takes around 46 hours, with 10 hours dedicated to the search time of NHN. The NHN outperforms most methods in Table 1 with a test accuracy of 96.52% and approximately 7M parameters. It achieves lower test errors compared to other methods like DenseNet with Shake-Shake and AmoebaNet with Cutout. The search time of NHN is 10 hours, which is longer than ENAS due to additional operations. NHN achieves higher accuracy with fewer parameters compared to SMASH BID4 and hierarchical representation. It also shows a significant speedup in search time compared to Progressive NAS and EAS by NT BID5. The network architecture search method allows layer transformations during architecture search by enforcing Actor Networks for widening or deepening. BID0 proposed learning connectivity in ResNeXt using a uniform grid approach. However, our work represents each node as a neuron without strict constraints on connections. The best architecture discovered by connectivity learning is impressive but limited to ResNeXt. Randomly searched models were chosen over fixed or handcrafted architectures for experimentation. The NHN model is not based on ENAS micro search results, despite having higher test accuracy with fewer parameters than macro search. This is due to the extensive use of depthwise separable convolution BID7, which cannot be pruned directly. Replacing these convolutions with normal convolutions only slightly improves accuracy by 0.3%, but increases parameters to 67.8M, over 4 times more than the macro model. It also requires over 28GB of memory space. The NHN model outperforms ENAS micro search results with higher test accuracy and fewer parameters. Add-on neurons at lower layers improve performance by enriching low-level features. Perturbation on these neurons leads to performance degradation, confirming optimal search results. The paper does not focus on neuron fields BID12 or training methods on randomly generated neuron graphs. The paper proposes a method to construct neuron-based architectures efficiently, highlighting the significance of individual neurons. This approach competes well with state-of-the-art methods in image classification, addressing redundancy in layer-based architectures. The NHN model outperforms ENAS micro search results, showing higher accuracy with fewer parameters. The proposed method constructs neuron-based architectures efficiently, emphasizing individual neurons and reducing redundancy in weights. The NHN model, discovered through ENAS and LDS, outperforms ENAS and other models in Cifar-10 classification with fewer parameters. NHN also has a faster search time compared to other network architecture search methods."
}