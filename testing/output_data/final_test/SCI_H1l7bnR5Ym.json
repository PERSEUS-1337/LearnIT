{
    "title": "H1l7bnR5Ym",
    "content": "Probabilistic modelling is used to combat mode collapse in Generative Adversarial Networks (GAN). A novel framework called ProbGAN is proposed, which learns a distribution over generators using stochastic gradient Hamiltonian Monte Carlo. The method is shown to yield an equilibrium where generator distributions match the data distribution. Empirical evidence demonstrates the superiority of ProbGAN over other GAN frameworks on various datasets. Multiple generators are used in GANs to address training difficulties and mode collapse. Objective design improvements include f-divergence and \u03c72-divergence. Learning multiple generators instead of a single one helps capture data distribution granularity. The Bayesian GAN framework, proposed by Saatci & Wilson in 2017, utilizes probabilistic modeling to aggregate multiple generators in order to better model multi-modal distributions. This approach helps alleviate mode collapse and enhances the interpretability of learned generators. The framework is built upon Bayesian models for generator and discriminator, showing effectiveness in semi-supervised image classification tasks. However, a critical theoretical question regarding its convergence remains unanswered. The current Bayesian GAN framework lacks a convergence guarantee to the real data distribution. A new probabilistic framework is proposed with improved convergence and empirical performance. The main contribution is the establishment of a probabilistic treatment where any generator distribution faithful to the data distribution is an equilibrium. The proposed probabilistic framework for Generative Adversarial Networks establishes equilibrium between generator and discriminator distributions. Special Monte Carlo inference algorithms efficiently approximate non-differentiable criteria, showing superiority over state-of-the-art GAN methods in empirical studies on various datasets. The function involves DISPLAYFORM0 Eqn. 1 with p data as real data distribution and p gen as generated data distribution. Objective functions \u03c6 1 , \u03c6 2 , \u03c6 3 are chosen for the generator to generate the target data distribution. Various GAN objectives are summarized in TAB0, including min-max version, non-saturating version, LSGAN, and WGAN. Training GAN with multiple generators is explored to address mode collapse. Wang et al. (2016b) propose progressively training new generators using a subset of training data. The text discusses different approaches to training new generators in GANs, including using a subset of training data, reweighting samples, and involving third-party classifiers. The Bayesian GAN model by Saatci & Wilson adopts a unique method of modeling generator and discriminator distributions. The authors argue that learning the generator distribution in GANs offers better ability to fit multi-modal data. GAN frameworks are categorized into optimization-based and probabilistic methods. Probabilistic methods involve generators and discriminators evolving as particles of underlying distributions. ProbGAN is introduced as a probabilistic modeling approach with a Bayesian interpretation. Inference algorithms are developed for ProbGAN. ProbGAN introduces a probabilistic modeling approach with a Bayesian interpretation, focusing on learning the generator distribution in GANs for fitting multi-modal data. Inference algorithms are developed for ProbGAN, with discussions on constituent formulations and comparison with Bayesian GAN. ProbGAN aims to find a generator distribution that matches the target data distribution. It learns distributions of the generator and discriminator, treating the target data distribution as a fixed environment. The generator and discriminator observe each other and adjust their parameters accordingly. ProbGAN is presented in a Bayesian formulation, where each distribution update is viewed as a posterior inference process. ProbGAN updates generator/discriminator distributions based on previous time step and target data distribution. Likelihood terms in Eqn. 2 reflect preference for generators and discriminators. Likelihoods encode information reflecting objectives of generators and discriminators. ProbGAN uses different priors for the generator and discriminator, unlike Bayesian GAN. The generator's prior is based on its distribution in the previous time step. This approach aims to reduce the discriminator's ability to distinguish between real and generated data, leading to equal likelihoods for all generators. The approach in ProbGAN involves using different priors for the generator and discriminator, with the generator's prior based on its distribution in the previous time step. This strategy aims to maintain equal likelihoods for all generators, enhancing the adaptability of the model. The use of an evolving prior for the generator is crucial for improved convergence compared to Bayesian GAN, which suffers from fixed and weakly informative priors. The gradients for the GAN objective and the prior in ProbGAN are computed using two methods: Gaussian Mixture Approximation (GMA) and Partial Summation Approximation (PSA). GMA approximates the prior distribution as a Mixture of Gaussian, while PSA recursively unfolds the prior gradient as a summation over historical GAN. These methods address the challenge of computing the prior's gradient when no exact analytic form is available. In ProbGAN, the prior gradient is computed using Gaussian Mixture Approximation (GMA) and Partial Summation Approximation (PSA). The prior gradient can be recursively unfolded as a summation over historical GAN objective gradients. To reduce computational costs, a subset of discriminators is maintained by subsampling the whole sequence. This approach ensures good convergence properties for ProbGAN. ProbGAN demonstrates good convergence properties by analyzing the distribution evolution of Bayesian GAN (BGAN) and comparing it with ProbGAN. Theorem 1 shows that an ideal generator distribution leads to equilibrium in the dynamics, where the discriminator cannot distinguish between synthetic and real data. This results in the generator distribution remaining unchanged as the discriminator has no preference over generators. The discriminator in Bayesian GAN plays a crucial role in likelihood, while the prior must be carefully designed for the model to reach an equilibrium where the generator is optimal. The BGAN algorithm faces convergence issues due to its prior and likelihood design, with a fixed Gaussian prior being used in practice. The algorithm performs distribution dynamics, as shown in Corollary 1. The Bayesian GAN algorithm adjusts likelihood order for distribution dynamics, matching target data distribution with mixed data from generators. A weak prior in BGAN hinders convergence even with faithful data distribution generation. Our solution addresses the issue of generator degeneration by using the previous generator distribution as the prior. We demonstrate the superior convergence of our model on a categorical distribution through an analytical case study, showing proper convergence compared to other models. Theoretical analysis validates that BGAN's formulation is the only one yielding proper convergence. However, its choice of likelihood and prior leads to compatibility issues, making it unsuitable for minimax-style GAN objectives. This limitation may restrict BGAN's usage since many widely used GAN objectives follow a min-max fashion. The GAN objective must be decomposable, but no valid objective meets this requirement. Incompatible conditionals in Eqn. 9 lead to unpredictable behavior during sampling. Lemma 1 states that conditional distributions can only be represented if X and Y are independent and L(x, y) is decomposable. The model is evaluated with two inference algorithms, ProbGAN-GMA and ProbGAN-PSA, compared to three baselines: GAN, MGAN, and BGAN. In the experiment, different GAN objectives (GAN-MM, GAN-NS, WGAN, LSGAN) are tested with multiple generators and the same discriminator architecture. The dataset consists of a uniform mixture of n modes in a high-dimensional space. Each mode is on a d-dimensional sub-space. The experiment parameters are set to n=10, D=100, and d=2. In the experiment, n, D, and d are set to 10, 100, and 2 respectively. Hyper-parameters for A and b are \u03c3 A = \u03c3 b = 5. Models train ten generators and evaluate using hit ratio, hit distance, and cover error metrics based on generated data belonging to real data modes. The experiment involved setting n, D, and d to 10, 100, and 2 respectively, with hyper-parameters for A and b as \u03c3 A = \u03c3 b = 5. Ten generators were trained and evaluated using hit ratio, hit distance, and cover error metrics to assess the generated data's proximity to real data modes. The hit ratio and hit distance results showed that probabilistic methods consistently achieved a hit ratio of 1, indicating close proximity to the target distribution modes. The optimization-based methods, such as GAN and MGAN, have a larger hit error and may generate data samples that do not belong to any mode. These methods fit the target uniform distribution worse than probabilistic methods, as shown by the cover error. Data generated by GAN or MGAN tends to be under-dispersed and does not cover the entire true mode region, while probabilistic methods align better with the ground truth distribution due to stronger exploration power. The incompatibility issue of BGAN with minimax-style GAN objectives is empirically verified in experiments. BGAN with the WGAN objective achieves much poorer coverage than with other GAN objectives. Our model is more robust to the choice of GAN objectives, consistently showing lower cover errors. Visual comparisons show that data distribution generated by BGAN trained with WGAN objective tends to shrink. The datasets CIFAR-10, STL-10, and ImageNet are used for evaluation. CIFAR-10 has 50k training and 10k test images from 10 classes. STL-10 is more diverse with 100k images, while ImageNet has over 1.2 million images from 1,000 classes. Evaluation metrics include Inception Score and Fr\u00e9chet Inception Distance. Results are compared with baselines using the same settings as MGAN. More details are provided in the appendix. Our proposed ProbGAN model outperforms baselines on CIFAR-10, STL-10, and ImageNet datasets. The 'IS\u22120.1FID' principle is used to select the best epoch for evaluation. Probabilistic methods show better performance than optimization-based methods in GAN training. Injecting stochasticity into GAN training helps generate more multi-modal images, with performance gaps increasing as datasets become more diverse. Our ProbGAN model improves FID by 4.00 on CIFAR-10 compared to MGAN, and by 4.82 and 18.06 on STL-10 and ImageNet, respectively. Bayesian GAN shows a significant performance drop with min-max style GAN objectives, while ProbGAN consistently performs well with any GAN objectives. Qualitative results show that baselines often suffer from mode collapse, unlike ProbGAN. Our experiment shows that mode collapse is a common issue in GAN training, with one or two generators degenerating during training. However, ProbGAN is robust to mode collapse, as demonstrated in visual results on CIFAR-10. The proposed ProbGAN model is able to generate visually appealing images with complex details while improving robustness against mode collapse. ProbGAN is a novel probabilistic modelling framework for GAN that introduces a new likelihood function and prior, along with scalable inference algorithms. The framework aims to extend to non-parametric Bayesian modelling and explore theoretical properties of GANs in a probabilistic context. The importance of understanding the theoretical behavior of Bayesian models is emphasized, with the hope of inspiring further exploration into Bayesian deep learning. Theorem 1 states that for GANs to have symmetry, the GAN objective functions must satisfy a specific condition. This condition is weak but holds for common GAN objectives and neural network parameterizations. The generator distribution remains unchanged based on certain dynamics. Theorem 1 states that for GANs to have symmetry, the GAN objective functions must satisfy a specific condition. This condition is weak but holds for common GAN objectives and neural network parameterizations. The generator distribution remains unchanged based on certain dynamics. Algorithm 1 from the original paper implies Eqn. 14 where \u03b8 is defined as p(\u03b8 | z). The total summation is a Monte Carlo approximation of the expectation in Eqn. 15. The same derivation can be done for q. Corollary 1 is obtained. Inception score and FID are discussed, noting sensitivity to model used and data splits. FID computed by a PyTorch Inception model is lower than Tensorflow model. In experiments, Inception score and FID are computed using the Tensorflow Inception model for fair comparison with prior work. Model architecture details are provided. Model architecture details are provided for GANs, including the number of generators and discriminators used in training. The neural network structures follow a specific pattern with deconvolution and convolution layers, along with batch normalization and activation functions. The model architecture for GANs includes four (or five) convolution layers with batch normalization and leaky-ReLU activations. Training hyperparameters include Adam optimization with a learning rate of 2 \u00d7 10 4. Generators and discriminators have batch sizes of 120 and 64, with dimensions specified for each layer. In a toy example, the adapted SGHMC inference algorithm setup involves input, hidden, and output layers with dimensions of 100, 1000, and 1 for the discriminator. The target data distribution is a categorical distribution Cat(\u03bb 1:N ) where \u03bb i = p data (x i ). Generator G i generates data following the categorical distribution p data (x; \u03b8). The probability distributions of generator and discriminator are categorical distributions q g (\u03b8 g ) = Cat(\u03b2 1:Ng ). In experiments with N = 10, N g = 20, N d = 100, different models are compared using various likelihood and prior pairs. Model A represents Bayesian GAN design, while model D is a proposed model. Models B and C are for ablation study. Evaluation is done using l1 distance on categorical distributions. In experiments with different models using various likelihood and prior pairs, the generator and discriminator distributions are updated iteratively to monitor the distance between target data distribution and generated data distribution. Model A and model B quickly get trapped in bad local minima due to non-informative prior, while model C shows different results with different GAN objectives. ProbGAN converges successfully. Probabilistic models in Bayesian GAN outperform optimization-based methods in fitting true distribution modes. MGAN still faces mode collapsing issue despite using an additional classifier. ProbGAN-GMA + GAN-NS and ProbGAN-PSA + GAN-NS successfully fit each mode of the true distribution with one generator. Bayesian GAN probabilistic models excel in fitting true distribution modes, while optimization-based methods struggle and produce under-dispersed data. BGAN's success is expected due to its objective not being in a min-max style. Images generated by models trained on CIFAR-10 under different GAN objectives are showcased."
}