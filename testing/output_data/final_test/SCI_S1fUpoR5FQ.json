{
    "title": "S1fUpoR5FQ",
    "content": "The quasi-hyperbolic momentum algorithm (QHM) is a simple alteration of momentum SGD, combining plain SGD with a momentum step. It has connections to other algorithms and can recover a set of two-state optimization algorithms. A variant of Adam called QHAdam is proposed, showing improved training results in various settings, including a state-of-the-art result on WMT16 EN-DE. The simplicity of QHM and QHAdam is expected to attract interest from practitioners and researchers. The quasi-hyperbolic momentum (QHM) algorithm is a modification of momentum SGD, combining plain SGD with momentum. It aims to reduce variance in parameter updates, providing faster convergence in deep learning. QHM's update rule is a weighted average of momentum's and plain SGD's update rule. QHM's update rule combines momentum and plain SGD, leading to faster convergence in deep learning. QHAdam, a variant of Adam, is also proposed. QHM is connected with various algorithms in Section 4, showing reciprocal benefits. The set of optimization algorithms that QHM recovers is characterized. Empirical demonstrations in Section 6 show superior optimization by QHM and QHAdam in deep learning settings. In deep learning settings, QHM and QHAdam show superior optimization. QHM combines momentum and plain SGD for faster convergence, while QHAdam is a variant of Adam with efficiency and simplicity. Practical tips are provided for using QHM and QHAdam, along with evidence that momentum often yields negligible improvement over plain SGD. The paper discusses optimization algorithms like stochastic gradient descent (SGD) and momentum, with notation and primitives defined for model parameters and loss functions. It emphasizes the practical promise of these algorithms for practitioners. The paper discusses optimization algorithms like stochastic gradient descent (SGD) and momentum, with update rules described for each algorithm. Momentum algorithm uses a momentum buffer controlled by an exponential discount factor \u03b2, with a common rule of thumb for \u03b2 being 0.9. The momentum is normalized to control variance, in contrast to traditional formulations. In the proposed quasi-hyperbolic momentum (QHM) algorithm, the momentum buffer is normalized by (1 \u2212 \u03b2) to remove dependence on \u03b2 and allow interpretation as a weighted average of past gradients. QHM introduces an immediate discount factor \u03bd, combining plain SGD (\u03bd = 0) and momentum (\u03bd = 1). Comparing QHM to momentum, QHM is seen as a \u03bd-weighted average of momentum and plain SGD update steps. The quasi-hyperbolic momentum (QHM) algorithm decouples the momentum buffer's discount factor from the current gradient's contribution, unlike traditional momentum. QHM introduces a discount factor \u03bd, allowing for a weighted average of past gradients. The algorithm aims to reduce variance by adjusting the momentum buffer's variance with the discount factor \u03b2. The quasi-hyperbolic momentum (QHM) algorithm introduces a discount factor \u03bd to adjust the momentum buffer's variance, allowing for a weighted average of past gradients. QHM is more efficient than traditional momentum and recovers other optimization algorithms like Nesterov's accelerated gradient (NAG) with \u03bd = \u03b2. The quasi-hyperbolic momentum (QHM) algorithm, with a discount factor \u03bd, recovers Nesterov's accelerated gradient (NAG) with \u03bd = \u03b2. This sheds light on NAG's update rule as a \u03b2-weighted average between momentum and plain SGD. Efficiency-wise, QHM's compute/memory cost is equivalent to that of NAG. Recht (2018) establishes a strong connection between gradient-based optimization and PID control, with QHM being related to PID as a superfamily. Viewing \u03b2 as a constant imposes a restriction on the ratio between PID coefficients, while viewing \u03b2 as a free variable allows QHM to recover nearly all PID coefficients. The transformation of variables reduces memory and compute costs, but is still costlier than QHM. Another PID setting is discussed, which is degenerate and linearly dependent. The \"synthesized Nesterov variant\" algorithm, SNV, is used to improve optimizer robustness under noise. SNV is related to QHM and they recover each other. AccSGD algorithm, proposed by Jain et al. (2017) and Kidambi et al. (2018), aims to improve convergence in stochastic least squares optimization compared to momentum and NAG. It is parameterized by \u03b4, \u03ba, \u03be, and \u03b5, and utilizes a specific update rule. QHM is related to AccSGD, with QHM being able to recover AccSGD. AccSGD is related to QHM, with QHM being able to recover AccSGD. QHM recovers NAG, while AccSGD does not recover QHM. Efficiency-wise, AccSGD requires 1 auxiliary buffer and is computationally costlier than QHM. Various convergence results follow simply via these connections, with QHM having the fastest known global convergence rate for strongly convex, smooth loss functions in the deterministic case. In the stochastic (minibatch) case, QHM's recovery of AccSGD gives the same convergence results as in Kidambi et al. (2018) for -approximation of the minimal loss. QHM unifies two-state optimization algorithms efficiently, making them more accessible to practitioners. It better recovers the Polyak (1964) subfamily and the NAG (Nesterov, 1983) subfamily, while restricting the PID (Recht, 2018) parent. Additionally, QHM handles multiplicative noise better than the SNV (Lessard et al., 2016) subfamily with convergence guarantees. In Appendix D, the set of two-state optimization algorithms recoverable by QHM is characterized. This aims to facilitate future work by providing a routine conversion to QHM for accessibility and efficiency benefits. Many-state optimization algorithms can be achieved by combining multiple momentum buffers with different discount factors. In preliminary experiments, it was found that using multiple momentum buffers in optimization algorithms yields negligible value compared to using a single slow-decaying momentum buffer with appropriate settings. The Aggregated Momentum (AggMo) algorithm performs a linear combination of multiple momentum buffers, with an extended variant allowing for different combinations. Appendix H discusses and compares QHM and AggMo, supporting the findings of the preliminary experiments. The proposed algorithm QHAdam replaces Adam's moment estimators with quasi-hyperbolic terms, parameterized by \u03b1, \u03b2 1 , \u03b2 2 , \u03bd 1 , and \u03bd 2. It recovers Adam, RMSProp, and NAdam under different settings. QHAdam is inspired by Adam and has variants like AMSGrad and AdamW. Experimental settings are summarized in TAB1 and detailed in Appendix I. The experimental settings for the QH algorithms are detailed in TAB1 and Appendix I. Parameter sweeps are conducted with small models, training for 90 epochs with size-64 minibatches. For QHM, \u03b1 is initialized at 1 and decayed 10-fold every 30 epochs. The sweep grid includes various parameterizations of plain SGD, momentum, and NAG. QHAdam fixes \u03b1 = 10^-3, = 10^-8, \u03bd 2 = 1, and \u03b2 2 = 0.999, sweeping over \u03bd 1 and \u03b2 1. A default \u03bd = 0.7 and \u03b2 = 0.999 are selected based on preliminary experimentation on the MNIST dataset. Results from parameter sweep experiments on the MNIST dataset show that QH algorithms outperform NAG and Adam, even with default parameters. QH algorithms improve training and validation metrics, with faster convergence and reduced training time compared to plain SGD. In experiments comparing plain SGD, NAG, and momentum, little difference was found in performance. The benefit of momentum and NAG is attributed to the increase in effective step size. QHM provides significant benefits without changing the step size. Case studies in image recognition, language modeling, reinforcement learning, and neural machine translation show the effectiveness of QH algorithms. In image recognition, a ResNet152 model trained on the ILSVRC2012 dataset using QHM converges faster and achieves slightly better validation error compared to NAG. In language modeling, a FConv language model trained on the WikiText-103 dataset with QHM also shows improved performance over NAG. In reinforcement learning, the QH algorithm is applied to the TD3 algorithm in various MuJoCo environments, outperforming the Adam baseline on most tested environments. In neural machine translation, the QHAdam algorithm yields improvements in average reward on multiple environments compared to Adam. The QHAdam algorithm improves performance and robustness in neural machine translation by controlling the maximum per-step update. It outperforms the Adam baseline, converging for all seeds and achieving a new state-of-the-art result of 29.45 BLEU. QHAdam achieves a new state-of-the-art result of 29.45 BLEU, improving stability and performance with a simple optimizer swap. Plain SGD performs nearly as well as NAG, outperforming momentum. Higher values of \u03b21 did not yield significant improvements. Consider using QHM or QHAdam instead of momentum or NAG. When using QHM or QHAdam with \u03b2 = 0.9, set \u03bd = 0.7 and \u03b2 = 0.999 as a rule of thumb. Convert learning rates from momentum/NAG to QHM by multiplying by (1 \u2212 \u03b2) \u22121. Future work could explore the performance gains of QHM and QHAdam on various tasks and architectures. Future work could focus on creating an effective \u03bd, \u03b2 adapter for optimization algorithms like QHM and QHAdam, possibly based on techniques such as YellowFin or continuous-time optimal control analysis. Additionally, exploring convergence results for QHAdam and QHM in stochastic settings would be beneficial. Momentum in distributed, asynchronous settings has been studied, with some noting delays in asynchronous SGD. In asynchronous SGD, delays are akin to adding momentum. The optimal momentum constant shrinks with more workers. QHM and QHAdam are computationally cheap and can outpace vanilla counterparts. They enable the use of high exponential discount factors through immediate discounting. Parameter sweep experiments show their efficiency. The paper's appendices present various analyses and derivations related to optimization algorithms, including momentum, QHM, PID control, and the implications on training stability. The appendices provide additional analyses and derivations related to optimization algorithms, including momentum, QHM, PID control, and their impact on training stability. Appendix G contains miscellaneous derivations, Appendix H compares QHM and AggMo, Appendix I describes parameter sweep setup, and Appendix J presents parameter sweep results. The momentum buffer is interpreted as a discounted sum estimator in the QHM algorithm for variance reduction. The exponential discount function \u03b4 EXP,\u03b2 and the exponentially weighted moving average EWMA \u03b2 are key components in optimization algorithms like momentum and QHM. The EWMA serves as an estimator of the expectation of a random variable x, with a time-consistent discount function. The covariance of the EWMA when x 0...t are independent random vectors is given by a specific formula for variance reduction. The covariance matrix \u03a3 is used in variance reduction of the EWMA by adjusting \u03b2. Momentum reduces variance but increases bias with higher \u03b2. Time-inconsistency is introduced for recent gradients to contribute significantly. Hyperbolic discounting is a classical approach proposed by Chung & Hernstein in 1961. Hyperbolic discounting, proposed by Chung & Hernstein in 1961, is a time-inconsistent discount function used in consumer choice and stochastic optimization. It defines the hyperbolic discount function and hyperbolic weighted moving average. Unlike EWMA, HWMA is not a discounted sum average and requires decay of c for computational efficiency. In preliminary stochastic optimization experiments, HWMA showed benefits over EWMA by limiting the number of past gradients used. Quasi-hyperbolic discounting approximates time-inconsistency of hyperbolic discounting by upweighting the current step, making it more practical for optimization. The quasi-hyperbolic discount function and quasi-hyperbolic weighted moving average (QHWMA) are more suited for practical optimization. QHWMA is a discounted sum average and can be viewed as an estimator under certain assumptions. When \u03bd = 1, QHWMA is equivalent to EWMA. The quasi-hyperbolic discount function is time-inconsistent when \u03bd = 1. QHWMA is a \u03bd-weighted average of EWMA and x0, making it easy to compute online. The QHWMA can be computed online by tracking the EWMA, requiring no additional memory. The variance of QHWMA decreases with increasing \u03b2 and \u03bd, leading to the motivation for QHM. QHM replaces the EWMA momentum buffer with QHWMA for variance reduction and less biased gradient estimation. The update rules for the momentum and QHM algorithms are explicitly written, with a comparison between QHM and momentum highlighting their differences in discount factors. The QHM algorithm aims to reduce variance and bias in gradient estimation by replacing the EWMA momentum buffer with QHWMA. Section 4 discusses connections to other optimization algorithms and their convergence properties. The convergence properties of QHM from a variance reduction standpoint are not formally analyzed. Recent efforts have focused on reducing the variance of stochastic gradients in optimization algorithms, particularly in the finite-sum setting. Momentum is also explored as a method for variance reduction. The connection between PID control and gradient-based optimization is described. A PID controller uses control signals to minimize the error between target and current states. The PID control function, consisting of P, I, and D terms, is discretized in discrete time. The D term is often filtered to reduce noise. Recht (2018) links optimization to PID control, where the goal is to minimize the error between target and current states by adjusting control signals. The PID control function, consisting of P, I, and D terms, is discretized in discrete time. Recht (2018) shows that PID in this setting encapsulates gradient descent, momentum, and NAG. The D term is a weighted sum of an EWMA of gradients and the current gradient. The output of a PID control optimizer is a weighted sum of the momentum buffer, the current gradient, and the sum of all past gradients. The text discusses the theoretical treatment of optimization algorithms, viewing optimizers as linear operators with internal state buffers. It introduces a square matrix T for updating the optimizer state and parameters, imposing restrictions for analytical purposes. The optimizers are considered to act coordinate-wise, leading to a Kronecker product representation of T. The text discusses the theoretical treatment of optimization algorithms, viewing optimizers as linear operators with internal state buffers. It introduces a square matrix T for updating the optimizer state and parameters, imposing restrictions for analytical purposes. The optimizers act coordinate-wise, leading to a Kronecker product representation of T. The internal state of QHM includes momentum and model parameter buffers, with transition matrices mapping between them. The optimizer state can be expressed in terms of initial state and past gradients. Recht (2018) establishes a connection between gradient-based optimization and PID control. The text discusses PID control optimization, using update rules for parameters kP, kI, kD. The internal state includes buffers for P, I, D terms, and initial parameters. The transition matrix T maps the state to the next iteration. The optimizer state can be expressed in terms of past errors and gradients. It establishes a connection between PID control and QHM algorithms. QHM is a PID superfamily algorithm that can recover most PID coefficients, except when certain conditions are met. It imposes a restriction on the ratio between kP and kD when \u03b2 is viewed as a constant, but can recover nearly all PID coefficients when \u03b2 is a free variable. SNV, a synthesized Nesterov variant algorithm, is used for analysis. The \"SNV\" algorithm is a variant used to improve optimizer robustness under relative deterministic noise. It utilizes a coordinate-wise decomposition with internal buffers \u03be t and \u03be t\u22121. The transition matrix T SNV maps to the optimizer state \u03b8 t. QHM and SNV are related, with QHM recovering the Robust Momentum method. QHM and SNV recover each other, with QHM also recovering the Robust Momentum method. The AccSGD algorithm proposed by Jain et al. and Kidambi et al. aims to address failures of momentum and NAG in stochastic least squares optimization, offering faster convergence in certain regression settings. The update rule of AccSGD is parameterized by \u03b4, \u03ba, \u03be, and \u03b5, utilizing coordinate-wise decomposition with internal buffers. AccSGD cannot recover NAG as claimed by Kidambi et al. QHM is shown to recover AccSGD. The appendix describes a two-state optimizer (\"TSO\") with transition matrices and update rules for QHM. The unrolled update rule for QHM involves solving equations to establish dependencies and recover relationships. This setting differs from typical PID control by controlling the derivative of the controller's output. Exponential decay with a discount factor is added to the I term to avoid parameter blowup. The proposed PID algorithm by An et al. incorporates exponential decay to the I term, making it the momentum buffer. However, the D term is a weighted sum of the P and I terms, resulting in a less expressive algorithm compared to Recht (2018). This PID control optimizer is costlier than QHM, requiring 2 auxiliary memory buffers and specific computational operations per update step. The stability properties of Adam and QHAdam are discussed in relation to a step size upper bound to prevent training process \"explosion\" in deep learning models. The training process for deep learning models can face instability due to large parameter updates in Adam optimizer. To address this, one can decrease the learning rate, increase hyperparameters (though this can be challenging), or clip gradients. Each approach has its own considerations and trade-offs. The Adam optimizer provides an upper bound on per-step updates, ensuring stability by restricting the magnitude of updates regardless of gradient distribution. QHAdam offers further control by adjusting hyperparameters to lower the maximum update size and improve stability. QHAdam allows for lowering the maximum update size by adjusting hyperparameters, improving stability. This is achieved by setting \u03bd 2 below 1 and making simplifications such as fixing = 0.24 and removing bias correction of moment estimators. The update rule for QHAdam is bounded by a constant dependent on \u03b2 1 , \u03b2 2 , \u03bd 1 , and \u03bd 2. The values of x i that maximize g 2 t+1 st+1 are characterized by first-order conditions. In the limit case of t \u2192 \u221e, the bound simplifies for vanilla Adam. The correct bounds contradict the claim in Kingma & Ba (2015) regarding Adam's per-coordinate step size. The recommended setting of \u03b2 2 = 0.999 makes the bound large. In Kingma & Ba (2015), a high value of \u03b22 = 0.999 in Adam leads to large bounds. Lowering \u03b22 can slow down training, prompting the use of QHAdam with decreased \u03bd2. Experiments show that increasing \u03b22 beyond 0.98 can cause training instability, especially in scenarios with rare inputs or labels like machine translation. Reddi et al. (2018) propose the AdamNC algorithm, suggesting adjustments to \u03b22. The AdamNC algorithm proposed by Reddi et al. (2018) recommends a high value for \u03b22 to capture a long history of past gradients. This appendix includes derivations that do not cleanly fit elsewhere, such as the covariance matrix of the QHWMA and a comparison of QHM and Aggregated Momentum (AggMo) for an autoencoder task. The AggMo algorithm, parameterized by discount factors \u03b2 \u2208 R K and learning rate \u03b3 > 0, uses the update rule to maintain momentum buffers. In an autoencoder task, the EMNIST dataset is used instead of MNIST, with 10% of the training dataset held out for validation. The AggMo algorithm uses discount factors \u03b2 and learning rate \u03b3 for momentum buffers. The best parameterization for AggMo is found to be \u03b2 = [0, 0.9, 0.99, 0.999] and \u03b3 = 0.1, named \"AggMo-Best\". This parameterization is converted into QHM-Converted with \u03b1 = 28, \u03bd = 0.97, and \u03b2 = 0.999. No optimization or parameter sweeps were performed for QHM-Converted. Results show that QHM-Converted outperforms AggMo-Best on the autoencoder task, indicating that using multiple momentum buffers with an arbitrary weighting scheme provides negligible benefit over using a single slow-decaying momentum buffer with an appropriate weight. Lucas et al. (2018) interpret AggMo as passive damping for physical systems, where fast-decaying momentum buffers \"dampen\" the system. In the context of passive damping for physical systems, Lucas et al. (2018) propose an extension of AggMo with alternate weighting schemes using separate per-buffer learning rates. This extension, with K = 2 and discount factors [0, \u03b2], recovers QHM and shows marginal value in providing additional damping for slow-decaying momentum buffers. Weighting schemes of multiple momentum buffers (extended AggMo with K > 2) did not significantly improve over a single momentum buffer. Lucas et al. (2018) did not explore these alternate schemes, suggesting future research to determine effective weighting methods. QHM may be preferable due to the added costs and complexity of AggMo. Experiments were conducted using Python 3.7 and PyTorch 0.4.1. Experiments in the study were conducted using Python 3.7 and PyTorch 0.4.1. Training occurred over 90 epochs with a minibatch size of 64. The learning rate had a linear warmup in the first epoch and used a step decay schedule. Different parameterizations were run 3 times with different seeds. The study explored a two-dimensional grid for \u03bd and \u03b2 values. The study conducted experiments using Python 3.7 and PyTorch 0.4.1, training over 90 epochs with a minibatch size of 64. Different parameterizations were run 3 times with different seeds on a two-dimensional grid for \u03bd and \u03b2 values. The model used in the study is a multinomial logistic regression for digit recognition over the EMNIST dataset, optimized with QHM and QHAdam. The model used in the study is a multilayer perceptron with pixel vector input and hidden layers of sizes 200, 100, and 50 units. The final layer is followed by softmax. The task is the same as in Logistic-EMNIST-QHM, and the optimizer used is QHM. The model for image recognition on the CIFAR-10 dataset is an 18-layer convolutional residual network with preactivations. The objective is cross-entropy loss with L2 regularization. The non-baseline optimizer is QHM with specific parameters. The learning rate is increased as per Section 7.1. Evaluation includes running 3 seeds for each optimizer. The model used is GCNN-14 variant for language modeling on the WikiText-103 dataset. Refer to RN50-ImageNet-QHM for implementation details."
}