{
    "title": "HyxnZh0ct7",
    "content": "In this paper, the focus is on adapting deep networks to new concepts with few examples using efficient non-deep learning methods. The proposal is to teach a deep network to use standard machine learning tools like ridge regression for quick adaptation. This involves back-propagating errors through solver steps, utilizing the Woodbury identity to make the process efficient with a small number of examples. The proposal includes closed-form and iterative solvers based on ridge regression and logistic regression components. Our methods offer a new approach to few-shot learning, outperforming state-of-the-art benchmarks. While humans excel at fast mapping, supervised learning algorithms, especially neural networks, require extensive data for good generalization. Acquiring labeled data can be costly, limiting researchers in fields like drug discovery. Data scarcity also poses challenges, such as classifying rare animal species. Few-shot learning, such as classifying rare animal species with limited training examples, has gained interest in the machine learning community. Most methods for few-shot learning use metalearning to improve learning with more training episodes by transferring knowledge. Meta-learning in few-shot classification often involves nested training loops where the base learner operates at the level of individual episodes with a small set of labeled training images. The meta learner improves base learner performance in few-shot learning by training on multiple episodes with limited labeled training images. Each episode involves learning new classes with a few sample images per class, creating a 3-way, 2-shot classification problem. The network's weights are shared across episodes, along with the hyper-parameters of the differentiable ridge regression layer. The paper proposes using simple learning algorithms as base learners in a meta-learning algorithm, where the network's weights are shared across episodes to improve learning capabilities. Different methods for base learners are discussed, ranging from nearest-neighbors to optimizing iterative learning algorithms. The paper suggests using simple learning algorithms like ridge regression as base learners in a meta-learning framework for few-shot classification. These algorithms allow for efficient solving of learning problems and can leverage Woodbury's identity for computational speed gains. Extensive experiments on Omniglot, CIFAR-100, and miniImageNet show competitive performance with fast and simple implementation. Meta-learning is a fast and simple approach that can achieve competitive or superior performance in accuracy compared to state-of-the-art methods. The concept gained importance in the machine learning community decades ago, with key works discussing dynamically adjusting learning algorithms' biases and interpreting neural network weight updates as learnable functions. Lifelong learning scenarios, where algorithms encounter ordered learning problems and reuse accumulated knowledge, have also been explored. Meta-learning involves algorithms improving their performance on tasks with experience and the number of tasks. It consists of two nested learning levels, with the base level solving single learning problems and the meta-level accumulating knowledge across multiple problems. A simple approach to meta-learning is training a similarity function on various matching problems, which has proven to be effective. The general strategy of few-shot classification algorithms involves learning as information propagation from support to query images. Parameter updates occur within the outer training loop, limiting adaptation at test time. To address this, a neural network can predict its own parameters, as demonstrated by recent studies. Meta-learning approaches such as adapting neural networks on the fly and interpreting gradient updates as learnable functions have shown promise. Recurrent neural networks, particularly LSTMs, are identified as suitable for meta-learning due to their ability to handle long-term dependencies. Recent research has explored using the MAML algorithm for few-shot learning, backpropagating through the update rule instead of designing a separate meta-learner module. Several recent methods use memory-augmented models to address catastrophic forgetting in deep neural networks. These models can retain and access important information from previous episodes. Attention-based approaches, like soft attention with temporal convolutions, help to specify relevant knowledge within an episode. In this paper, the focus is on using simple and fast base learners like ridge regression instead of complex methods like nearest-neighbour. These base learners offer more flexibility by producing different parameters for each episode. They also show a fast convergence rate, especially when a closed form solution is available. Previous work has explored using analytic formulations for obtaining meta-gradients and optimizing hyper-parameters. Recent advancements have derived backpropagation forms for SVD and Correlation Filter, enabling the application of SGD to deep neural networks solving eigenvalue problems or linear equations. Meta-learning involves improving learning skills by modifying inductive bias through experienced episodes. It consists of a base learner working on individual tasks and a meta-learner improving base learner performance across episodes. The goal is to enable efficient adaptation to new episodes. The goal of meta-learning is to enable a base learning algorithm to adapt efficiently to new episodes by generalizing from a set of training episodes. Few-shot classification involves using small sets of images from unseen classes. A generic feature extractor can be trained to map input embeddings to outputs, with a predictor parameterized by specific parameters for each episode. Training and assessing the predictor involves access to training samples and test data. The text discusses the standard train-test protocol in machine learning, where a predictor is trained on training samples and evaluated on test samples. To improve this process, meta-learning introduces meta-parameters \u03c9 and \u03c1 to aid in transferring knowledge between episodes efficiently. The text discusses the use of meta-parameters \u03c9 and \u03c1 to improve the training process in machine learning. These meta-parameters affect the generalization properties of learned predictors and can be optimized using back-propagation and stochastic gradient descent. The choice of a simple learning algorithm \u039b is crucial for optimization. The text discusses the importance of choosing a simple learning algorithm \u039b for optimization in machine learning. Various meta-learning algorithms differ in their choice of \u039b, with examples including using least-squares based solutions for ridge regression and logistic regression as base learners. In the outer loop of meta-learning, SGD learns parameters for feature representation and hyper-parameters. A linear predictor is adapted over a single episode, with remaining layers trained from scratch to generalize between episodes. Inputs are pre-processed by a CNN, and the final linear predictor is considered, with parameters reorganized into a matrix. Least-squares regressors offer closed-form solutions, although prone to overfitting. Ridge regression with L2 regularization can be easily integrated into meta-learning using automatic differentiation packages. Matrix inversion may need special treatment when close to singular, but in our experiments, this was not necessary. In meta-learning, using the Woodbury formula can reduce the computational cost of matrix inversion, making it more efficient for few-shot learning scenarios. This method, originally designed for regression, also works well for classification tasks with one-hot vector outputs. In meta-learning, the Woodbury formula can reduce matrix inversion computational cost, making it efficient for few-shot learning. Calibration of output for cross-entropy loss is crucial for evaluating test samples. Hyper-parameters like \u03bb, \u03b1, and \u03b2 can be learned by the meta-learner. Integration of other learning algorithms like Newton's method is possible within the meta-learning framework for linear models with convex loss functions. Logistic regression in meta-learning can be used for classification without calibration, using Newton's method and Iteratively Reweighted Least Squares (IRLS) algorithm. The algorithm updates parameters based on inputs X and binary outputs y, applying a sigmoid function to predictions. This approach is similar to ridge regression and can be used for meta-learning. In meta-learning, logistic regression can be used for binary classification in one/few-shot learning. The approach involves a small number of steps to obtain final parameters, with a cost linear in embedding size. The Woodbury formula is employed to achieve this, making the inner inverse cost negligible. This strategy can also be applied to other learning algorithms like L1 minimization and LASSO. The training procedure is organized into episodes, each corresponding to a few-shot classification problem. In meta-learning, logistic regression is used for few-shot learning. The training procedure involves episodes with their own training and test sets, each containing a few samples per image. Each episode has two sets of labels: Y for training the base learner and Y for computing the error to enable back-propagation. Each episode corresponds to a mini-batch of size S = N (K + Q), where N is the number of classes, K is the number of samples per class, and Q is the number of query images per class. This section provides practical details for the novel methods R2-D2 and LR-D2 introduced in previous sections. In meta-learning, logistic regression is used for few-shot learning with episodes containing training and test sets. The novel methods R2-D2 and LR-D2 are introduced for multi-class and binary classification problems using few-shot learning benchmarks like Omniglot, miniImageNet, and CIFAR-FS. The few-shot setup requires distinct image and class sets for meta-training and meta-testing. Omniglot is a dataset of handwritten characters with a high number of classes. The code for the methods and CIFAR-FS splits can be found at a specified URL. The \"MNIST transpose\" dataset has 20 examples of 1623 characters in 50 different alphabets, resized to 28\u00d728 with four rotated versions per instance. miniImageNet has 60,000 RGB images from 100 classes, downsampled to 84\u00d784, with 64 classes for meta-training and 16 for meta-validation. CIFAR-FS (CIFAR100 few-shots) is proposed as a new dataset for few-shot learning, randomly sampled from CIFAR-100. It offers a challenge with high inter-class similarity and a resolution of 32\u00d732, allowing for fast prototyping. Shallow networks are used to produce features for base learners. The methods involve using a shallow network with four convolutional blocks, each consisting of a 3\u00d73 convolution, batch-normalization, 2\u00d72 max-pooling, and leaky-ReLU. The network has four convolutional layers with different filter sizes. Dropout is applied to the last two blocks for experiments on miniImageNet and CIFAR-FS. No fully connected layer is used, instead, the output of the third and fourth convolutional blocks are flattened and concatenated to feed to the base learner, resulting in high-dimensional features. The Woodbury formula is utilized to handle high-dimensional features efficiently in few-shot learning scenarios. In few-shot learning scenarios, the use of high-dimensional features is optimized through the Woodbury formula, resulting in significant computation gains. Training with a higher number of classes is crucial, even though the test time problem may be 5 or 20-way. Models are trained with a random number of shots to simplify training across different configurations. In few-shot learning scenarios, models are trained with a random number of shots to simplify training. The batch size is kept constant throughout episodes, with specific values set for different datasets. Training is done using Adam with a decreasing learning rate, and stopped when the error on the meta-validation set does not decrease significantly. SGD is used to learn parameters of the CNN and calibration layer of R2-D2. The performance of our closed-form base learner R2-D2 in multi-class classification is compared against the current state of the art for shallow architectures. Results show average classification accuracies obtained from sampling 10,000 episodes. Various models like PROTO NET, RELATION NET, SNAIL, and GNN are evaluated on different datasets with confidence intervals provided. Best performances are highlighted, and results from prototypical networks are reported based on provided code. The authors compare their closed-form base learner R2-D2 in multi-class classification against current state-of-the-art methods for few-shot classification. Different models use varying numbers of filters per layer and additional modules like relation modules and graph neural networks. The architectures of representative methods are adjusted for a fair comparison, and dropout is applied on the last two layers for all experiments. The proposed method R2-D2 achieves superior accuracy compared to state-of-the-art with shallow architectures on miniImageNet and CIFAR-FS datasets. R2-D2 outperforms GNN by an average of 4.3% on TAB0 problems and shows competitive results on Omniglot TAB2. Using a \"lighter\" embedding still maintains performance in line with the state of the art, while increasing capacity of other methods is not consistently beneficial. Our R2-D2 model outperforms SNAIL on CIFAR-FS and matches its results on the 5-shot case. LR-D2 achieves high performance on multi-class classification by combining N binary classifiers. The study compares the performance of different models in multi-class prediction tasks, with MAML, PROTO NET, SNAIL, GNN, and RELATION NET achieving high accuracy rates. The LR-D2 model shows competitive results in binary classification tasks but is less efficient than R2-D2 due to the need to solve N binary problems per iteration. In TAB4, the performance of ridge regression and logistic regression base learners is compared with other methods in binary classification. LR-D2 and prototypical network are evaluated without oversampling ways, using annealed learning rates and Group Normalization BID59. The default setup from MAML is used, confirming the validity of both approaches for binary classification. LR-D2, utilizing Newton's method, outperforms MAML in efficiency for a higher number of base learner steps. Comparing time required on a single GPU for 10,000 episodes, LR-D2 shows superior performance. Incorporating fast solvers with closed-form solutions like R2-D2 allows for efficient adaptation to unseen learning problems, achieving comparable efficiency to prototypical networks and outperforming MAML. The use of the Woodbury identity provides significant computational gains in scenarios with few samples and high dimensionality, such as one-shot or few-shot learning. Our base learner R2-D2, a differentiable ridge regression model, performs almost as fast as prototypical networks in few-shot learning. It strikes a balance between not adapting for new episodes like metric-learning approaches and avoiding costly iterative methods like MAML or LSTM-based meta-learners. The results show excellent performance on few-shot learning benchmarks, generalizing well to episodes with new classes. Future work may explore Newton's methods with more complex second-order structures. In this work, the authors evaluated their proposed methods R2-D2 and LR-D2 in the few-shot learning scenario, which involves discriminating between images with very few examples. The training procedure involves two nested loops, with the inner loop solving the few-shot classification problem and the outer loop guiding the former by modifying the inductive bias of the base learner. Few-shot benchmarks enforce disjoint classes between dataset splits. In few-shot learning, methods vary in adaptability for solving classification problems with unseen classes. Examples include MAML for efficient parameter fine-tuning and metric learning methods like prototypical and relation networks for fast but non-adaptive solutions. Our work introduces a novel technique called R2-D2 for few-shot learning, which allows per-episode adaptation and achieves strong performance using ridge regression in the inner loop. This approach is efficient and can be extended to other solvers, offering design flexibility in solving classification problems with unseen classes. The R2-D2 technique for few-shot learning involves per-episode adaptation using ridge regression, which can be extended to other solvers. It differs from multi-task learning by having splits with disjoint sets of classes in datasets. Meta-learning methods for few-shot learning consider adaptation during training to mimic the test-time setting, learning how to learn from limited data. This approach differs from basic transfer learning where a neural network is pre-trained on one dataset/task and then adapted to another dataset/task. In a baseline experiment, a 4-layers CNN architecture was pre-trained for a standard classification problem using the same training datasets. The network's convolutional part was used as a feature extractor, and the results showed a significant drop in performance compared to the proposed R2-D2 method on miniImagenet and CIFAR datasets. This confirms that basic transfer learning techniques with shared feature representation are not effective for competitive results in few-shot learning. To achieve competitive results in few-shot learning, it is essential to enforce the generality of underlying features during training by back-propagating through the adaptation procedure. This involves using regularization techniques like Tikhonov regularization, which can be interpreted as a prior gaussian distribution of parameters. Experimenting with different variants of regularization, such as diag(\u03bb), can potentially exploit the fact that parameters have different scales. This approach aims to improve the performance of models compared to basic transfer learning techniques with shared feature representation. After applying the Woodbury identity, the final expression for W is obtained. The effect of using SGD to learn the hyper-parameters of the base learner \u039b is illustrated in FIG3. Learning the scalar \u03b1 for calibrating the output of R2-D2 is crucial, while learning \u03bb is optional. Updating \u03b1 with SGD allows recovery from poor initial values with minimal performance loss. The performance of R2-D2 with diag(\u03bb) formulation does not improve results compared to using a simple scalar \u03bb, and deteriorates for \u03bb > 0.01."
}