{
    "title": "SJgob6NKvH",
    "content": "In this work, a grounded policy learning problem called Read to Fight Monsters (RTFM) is proposed, where agents must read to understand new environment dynamics instead of memorizing information. The model txt2\u03c0 captures interactions between the goal, document, and observations, allowing for generalization to new environments in reinforcement learning. On RTFM, txt2\u03c0 outperforms baselines like FiLM and language-conditioned CNNs. Our model txt2\u03c0 excels on complex RTFM tasks through curriculum learning, outperforming baselines like FiLM and language-conditioned CNNs. Language-conditioned policy learning leverages machine reading to generalize to new environments, addressing limitations of RL in real-world problems. The curr_chunk discusses the Read to Fight Monsters (RTFM) problem where an agent must reason over language goals, environment dynamics, and observations to achieve a goal. The agent needs to generalize to new environments by selectively reading relevant information. The curr_chunk discusses the generation of environment dynamics and language descriptions to train and evaluate RTFM. It introduces txt2\u03c0 for joint reasoning in RTFM, showing superior performance compared to previous models. The curr_chunk discusses the sample efficiency and final win-rate on RTFM, highlighting the need for improved language-grounded policy learning to tackle the complexity of scaling to longer documents and natural language variations. The curr_chunk discusses policy learning for imperative instructions and the combination of imperative instructions with descriptions of dynamics. It also touches on language grounding in non-linguistic contexts like images and games. The curr_chunk discusses language grounding in interactive games, focusing on generalization to new goal descriptions and environment dynamics. The agent must reason over a language goal, environment dynamics in a text document, and observations to solve the goal effectively. The curr_chunk discusses the use of a large number of unique environment dynamics and language descriptions to study generalization in interactive games. This approach aims to help agents generalize to new environments with unseen dynamics by providing a diverse set of environment dynamics and goals to learn from. The agent is provided with environment dynamics, observations, and a goal instruction. The game includes monsters, teams, element types, item modifiers, and items. The player picks up items or engages in combat with monsters. The player can possess one item at a time. The figure illustrates key snapshots from a trained policy on a randomly sampled environment. The agent engages in combat with monsters using weapons, evades distractors, and defeats the target to win the episode. The game world is visualized as a matrix of text with entities occupying each cell. Human-written templates define monster teams, effective modifiers, and the goal to defeat specific teams. The agent in the game must cross-reference information in the document and observations to achieve the goal of defeating monsters. Each episode involves sampling groups, monsters, modifiers, and elements. The agent is presented with a document containing randomly ordered statements corresponding to group assignments. Target and distractor monsters, along with items to defeat them, are sampled for each episode. The agent in the game must identify the target team, monsters, elements, and modifiers to defeat monsters and win the game. Deviating from the correct trajectory results in losing the game. Rewards are given for winning (+1) or losing (-1). The agent in the game receives a reward of +1 for winning and -1 for losing. RTFM presents challenges requiring a large number of grounding steps to solve tasks. The agent must reason over language goals, document dynamics, and environment observations. Environments are split into train and eval sets with randomised target and distractor positions. Over 2 million environments are available for training and evaluation without shared assignments to test generalisation. The txt2\u03c0 model proposes a method to build representations capturing interactions between goals, environment descriptions, and observations. It utilizes the Bidirectional Feature-wise Linear Modulation (FiLM 2) layer to create codependent representations of text and visual inputs. This approach allows the agent in RTFM to filter concepts in both visual and text domains using language and visual observations. FiLM 2 builds codependent representations of text and visual inputs by incorporating conditional representations of the text given visual observations. The output consists of modulated visual features and text features, with element-wise addition and multiplication operations. The FiLM 2 model incorporates conditional representations of text given visual observations to build codependent representations of text and visual inputs. It computes summaries using self-attention and conditional summaries using attention, concatenating them into text features along with visual features for processing through FiLM 2 layers. The final output is processed by MLPs to compute a policy distribution over actions and a baseline for advantage estimation. The FiLM 2 model integrates text and visual inputs by computing summaries using self-attention and conditional summaries using attention. These summaries are concatenated into text features along with visual features for processing through FiLM 2 layers. The final output is processed by MLPs to compute a policy distribution over actions and a baseline for advantage estimation. The FiLM 2 model integrates text and visual inputs by computing summaries using self-attention and conditional summaries. The visual input to the FiLM 2 layer includes positional features, while the text input consists of various summaries and attention mechanisms. The features are concatenated and processed through FiLM 2 layers to compute a policy distribution over actions and a baseline for advantage estimation. The FiLM 2 model integrates text and visual inputs by computing summaries using self-attention and conditional summaries. The visual input includes positional features, processed through FiLM 2 layers to compute a policy distribution over actions and a baseline for advantage estimation. The document is encoded differently for attention with visual features and with the goal, using bag-of-words embeddings and positional features. Training is done using TorchBeast, an implementation of IMPALA. Variants of RTFM include varying grid-world sizes and allowing dynamic group assignments for disambiguation. The curr_chunk discusses the comparison of txt2\u03c0 to other models, training on one set of dynamics and evaluating on a held-out set. Three variants of txt2\u03c0 are studied, including ones without document attention and visual attention. The comparison of txt2\u03c0 to other models is discussed, focusing on training on one set of dynamics and evaluating on a held-out set. Three variants of txt2\u03c0 are studied, including ones without document attention and visual attention. The document is represented through self-attention, and curriculum training results are shown in Table 2. Five randomly initialized models are kept throughout the curriculum, with transfer results displayed for each stage. Training environments are evaluated based on win rate after training for a specified number of frames. In appendix C, model details and baselines are provided for txt2\u03c0 compared to other models. A simplified variant of RTFM is used for comparison, showing txt2\u03c0's higher performance and sample efficiency. Ablated variants fail to solve tasks, highlighting the importance of combined features in txt2\u03c0's success. The txt2\u03c0 model, along with its ablated variants and baselines, demonstrates strong generalization to new evaluation environments in tasks involving deducing cyclic dependencies. It outperforms FiLM and CNN models in sample efficiency and performance on both training and new environment dynamics. txt2\u03c0 is more sample efficient than other models when transferring to new environments. Experiments show its performance on the full RTFM problem, starting with simpler variants and gradually increasing complexity to 10x10 worlds with moving monsters and natural language templated descriptions. Curriculum learning is crucial for making progress on the RTFM problem. Training on smaller worlds allows models to generalize to bigger worlds. However, the final model's performance still trails behind that of human players, highlighting the difficulties of the RTFM problem. The difficulties of the RTFM problem are highlighted by players who can consistently solve it, indicating room for improvement in developing better language grounded policy learners. Attention maps in txt2\u03c0 help identify relevant information in the document, with goal-conditioned attention locating the target clause and intermediate layer attentions focusing on key regions. Analysis of trajectories shows well-performing policies exhibit consistent behaviors, while poorly-performing policies struggle on the full RTFM. The well-performing policies in RTFM exhibit consistent behaviors such as picking the correct item to fight the target monster, while poorly-performing policies struggle by occasionally picking the wrong item and getting stuck in evading monsters. The proposed model, txt2\u03c0, addresses the grounded policy learning problem by requiring the agent to reason over language goals, environment dynamics, and observations. The model aims to generalize through reading a large number of procedurally generated environment dynamics. txt2\u03c0 is a model that captures interactions between goals, documents, and observations to generalize to new environments. It outperforms baselines like FiLM and language-conditioned CNNs, excelling in complex RTFM tasks. Despite curriculum learning, it still trails human performance, indicating room for improvement in grounded policy learning. Language understanding through reading shows promise in learning policies for new environments. The text discusses using external documentation and language goals to reason about plans and induce hierarchical policies. It includes a playthrough example of a trained policy in different environments and details about the hyperparameters used in the experiments. The LSTM shares weight with the Goal LSTM, with hidden dimensions of 10 for Inventory and Goal LSTMs and 100 for Vis-doc LSTM. Word embedding dimension is 30. Text representations include self-attention over bidirectional LSTM-encoded goal, document, and inventory. Attention outputs are replicated over the grid dimensions and concatenated with observation embeddings. The FiLM baseline encodes text similarly to the CNN model but uses FiLM layers instead of convolutional layers. Training is done using IMPALA implementation. During training, IMPALA is used with 20 actors and a batch size of 24. RMSProp is optimized with a learning rate of 0.005, annealed linearly for 100 million frames. A small negative reward of -0.02 is applied per time step with a discount factor of 0.99. Entropy cost is included for exploration. Policy gradient, entropy loss, and baseline loss are added with specific weights. Grid search is performed for tuning models using training environments. During training, models are tuned using a grid search on training environments to select hyperparameters. Performance on the Rock-paper-scissors task is evaluated across models, with results shown for both familiar and unfamiliar environments. The Rock-paper-scissors task involves interpreting environment dynamics from a document to solve the task. In a Rock-paper-scissors-like game, characters are sampled and a dependency graph is set up between them. Monsters with randomly assigned types and corresponding items are spawned in the world. The player must obtain the correct item to defeat the monster. The winning strategy involves identifying the monster type, finding the item that defeats it, picking up the item, and engaging the monster in combat. This game can be generalized to new environments by permuting 3-character dependency graphs from an alphabet. The study involved splitting environment dynamics by permuting 3-character dependency graphs from an alphabet into training and held-out sets. Models were trained on 10 \u00d7 10 worlds from the training set and evaluated on both seen and unseen environments. Results showed that models generalized well to environments not seen during training, regardless of whether the world configuration or environment dynamics were unfamiliar. The study split environment dynamics by permuting graphs into training and held-out sets. Models were evaluated on new environments, showing transfer behavior and sample efficiency. txt2\u03c0 outperformed FiLM and CNN models consistently. txt2\u03c0 outperforms FiLM and CNN models consistently in training on held-out environment dynamics. It is more sample efficient and achieves higher win-rate. There are 24360 unique grid configurations and 4060 unique dependency graphs in the training set. The model finishes an episode in approximately 10 frames. Below is a list of entities and modifiers contained in RTFM. The document contains human-written natural language templates for the goal and dynamics of the game. It includes goal statements for defeating teams and statements about monsters, team assignments, modifiers, and element types. There are 12 templates for goal statements and 10 templates for each type of environment dynamics statement. The document is composed of shuffled statements, with each statement filled with specific details for monsters, teams, modifiers, and elements."
}