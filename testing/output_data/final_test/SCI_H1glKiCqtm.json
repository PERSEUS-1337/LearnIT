{
    "title": "H1glKiCqtm",
    "content": "Word embeddings are widely used in machine learning for natural language processing. Recent interest has focused on applying these techniques to programming languages, but none have used pre-trained embeddings on code tokens. However, using pre-trained embeddings on code tokens can provide benefits such as faster training, improved performance, and resistance to overfitting. The choice of language for the embeddings does not need to match the task, as even embeddings pre-trained on human languages can benefit programming languages. One of the initial steps in a machine learning natural language processing (NLP) pipeline is converting one-hot encoded tokens into dense representations using an embedding layer. Pretrained parameters, like GloVe embeddings, can be used for transfer learning, similar to using pretrained parameters in computer vision tasks. These parameters are fine-tuned during training on the downstream task. The embedding layer in NLP pipelines is fine-tuned during training on downstream tasks, using pre-trained embeddings for faster training, improved performance, and reduced over-fitting. There is growing interest in applying NLP techniques to programming languages, but current methods do not utilize pre-trained code embeddings. In this paper, experiments using pre-trained code embeddings are detailed for predicting a method name from a method body, known as extreme summarization. The semantic knowledge of variable names aids in reasoning about method bodies, chosen for human understanding. The focus is on whether pre-trained code embeddings reduce the need for fine-tuning in downstream tasks. The study focuses on research questions regarding the effectiveness of pre-trained code embeddings in reducing training time, improving performance, increasing stability of training, and reducing over-fitting. The choice of corpora used for pre-trained code embeddings is also explored. The study involves training embeddings for C, Java, and Python code separately, comparing them with natural language embeddings, and testing them on an extreme summarization task in Java. The study utilizes a language model for training the embeddings. The study utilizes a language model for training embeddings, specifically focusing on long-term dependencies and hierarchical relations within method bodies. A language model is a probability distribution over sequences of tokens, with each token represented by a one-hot vector. The study utilizes a language model for training embeddings, specifically focusing on long-term dependencies and hierarchical relations within method bodies. They use a recurrent neural network model called AWD-LSTM-LM to predict the next token in a sequence of code tokens. The model generates method names by taking code tokens as input and using a Gated Recurrent Unit (GRU) and convolutional filters to produce attention features. This model was chosen for its state-of-the-art performance on extreme summarization datasets and has an open-source implementation. The dataset used for pre-trained code embeddings was gathered from GitHub, focusing on projects with over 10,000 stars. Projects were tokenized and split into subtokens for C, Java, and Python languages. Each language had approximately 100 million tokens across 20 million lines of code. Different vocabularies were used for each language. Natural language embeddings were trained on the WikiText-103 dataset. The AWD-LSTM-LM model was trained on the WikiText-103 dataset with modified embedding and hidden dimensions. Tokens not in the top 150,000 common or appearing less than 3 times were converted to <unk>. The extreme summarization task dataset consists of 10 Java projects with full Java methods extracted. The Copy Convolutional Attention Model was trained on 10 Java projects with their own vocabulary. It achieved a 4% relative improvement in F1 scores for each embedding, with validation losses shown in Figure 1. The Copy Convolutional Attention Model trained on 10 Java projects showed improved performance with pre-trained embeddings. Results in Table 3 demonstrate faster training, lower losses, and less overfitting compared to random embeddings. Speedup and improvement in test loss were also observed for each project-embedding combination. Results in Table 3 show the relative benefits of using pre-trained embeddings for each project-embedding combination. Some projects, like elasticsearch, did not see any advantages and even experienced a slowdown with certain embeddings. Speedup and improvement in test loss have a medium to strong positive correlation with the amount of pre-trained embeddings used. Results in Table 3 demonstrate the advantages of using pre-trained embeddings for different project-embedding combinations. Despite expectations, Java embeddings did not outperform Python or C embeddings in terms of speedup and improvement. Surprisingly, English embeddings showed comparable performance, even though they were trained on human languages. The task of extreme summarization may not heavily rely on syntactic information from programming languages. The language of the dataset used for pre-training code embeddings may not be as important as the quality of the dataset in terms of method and variable names. English embeddings, trained on human languages, performed similarly to programming languages. Over-fitting was observed with random embeddings compared to pre-trained code. The study compares over-fitting in random embeddings versus pre-trained code embeddings. The over-fit factor is calculated based on validation loss, with random embeddings showing the worst performance. Interestingly, there is no correlation between overlap and over-fitting. Pre-trained embeddings perform similarly across projects, indicating dataset language may not be crucial. The study compares over-fitting in random embeddings versus pre-trained code embeddings, with pre-trained embeddings showing similar performance across projects. Language models have been successful in transfer learning for natural language processing applications. Recent work has focused on analyzing language models and their role in transfer learning, as well as using probabilistic models for source code. Predicting variable and method names has become a common task in machine learning, with a shift towards representing programs as graphs using abstract syntax trees. The study evaluates the impact of pre-trained code embeddings on training time, performance, and stability. Results show a 1.93x speedup in training time and a 5% relative validation loss improvement. Pre-trained embeddings also increase stability, as seen in the variance of validation loss curves. The study compares random embeddings to pre-trained embeddings, showing that pre-trained embeddings reduce over-fitting. The choice of corpora for pre-trained embeddings does not necessarily align with the downstream task language. Semantic information in variable names is crucial for performance, explaining why English embeddings perform well."
}