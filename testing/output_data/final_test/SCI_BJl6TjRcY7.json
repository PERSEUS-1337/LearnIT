{
    "title": "BJl6TjRcY7",
    "content": "The text discusses the development of a motor architecture for controlling high-dimensional physically simulated humanoids. The proposed model can compress expert policies and learn a motor primitive embedding space offline. It enables one-shot imitation of whole-body humanoid behaviors and can be used to train controllers for various tasks. The system also includes an experience-efficient method for offline policy cloning. The text introduces linear feedback policy cloning as an efficient method for controlling simulated humanoid bodies in machine learning for control and robotics. Despite advancements in deep reinforcement learning, behaviors still exhibit idiosyncrasies, making humanoids a valuable context for research. One advantage of working with humanoids in this context is the availability of motion capture data to design controllers for humanlike movement. The challenge is to sequence and generalize individual movements or \"skills\" in a task-directed manner, requiring an architecture that supports representation and composition of a large number of skills. This paper presents a step towards achieving this goal. In a setting with numerous experts excelling in single skills, the goal is to transfer these skills into a shared policy that can perform like each expert and generalize to unseen behaviors. A system is designed for one-shot imitation and skill reuse, scaling to a large number of skills with minimal manual intervention. The primary contribution is a neural network architecture called neural probabilistic motor primitives, capable of representing and generating various motor behaviors efficiently. The module can sequence and compose individual motor behaviors, generating novel movements consistent with training data. Training controllers to reuse this module for new tasks produces human-like movement. Policy transfer or cloning avoids closed-loop RL training, with one approach involving behavioral cloning from expert data. The approach of linear feedback policy cloning (LFPC) efficiently transfers expert knowledge to a student policy by matching noise-feedback properties along expert reference trajectories. This method is competitive with behavioral cloning from many rollouts, offering a cost-effective alternative in settings where obtaining many rollouts is challenging. Learning from motion capture (mocap) can provide strong constraints for acquiring specific behavioral skills in robotics and animation. Recent approaches have shown the possibility of acquiring single skills/behaviors through knowledge transfer from expert systems to student systems, aiming for compression or combination of functions. Imitation learning involves creating a student system by compressing or combining expert qualities. Behavioral cloning is a basic algorithm where the policy is trained from expert examples. One-shot imitation allows the system to imitate behavior at test time. This concept is similar to studying full-body humanoid movements. In this work, an architecture encouraging imitation of motor details is employed, scaling the approach to expert demonstrations. Unlike previous work on large-scale humanoid tracking, this approach focuses on motor primitives learned from related behaviors for skill reuse. The concept of motor primitives is supported by neuroscience, showing evidence of lower dimensional control signals coordinating behaviors produced by spinal circuits and the cortex organizing primitive motor behaviors. The architecture considered is inspired by a probabilistic latent bottleneck on sensory-motor mapping. Research in robotics explores various parameterizations of motion trajectories, including \"movement primitives\" like probabilistic movement primitives. These approaches focus on learning movement trajectories from demonstrations rather than actuation/stabilization elements. The curr_chunk discusses the use of linear-feedback policies for control, obtained through optimization methods like differential dynamic programming. These policies can be used to train neural networks and transfer optimal behavior. The curr_chunk describes the expert policies used in the work, the Neural Probabilistic Motor Primitive architecture, and two approaches for training the module offline to transfer and consolidate experts for generating expert data. The curr_chunk discusses the training procedure for time-indexed neural network policies that are robust to action noise using the CMU Mocap database. It includes various clips of generic whole-body movements from over 100 subjects, with a focus on locomotion behaviors like walking, running, jumping, and turning. The training procedure involves creating expert policies for a diverse set of skills from the CMU Mocap database, including locomotion and hand movements. The goal is to develop a motor primitive module that can deploy these skills without manual alignment, relying on a representation that encodes behaviors effectively and allows for easy recall. This is achieved through an autoregressive latent variable model for compressing expert skills. The autoregressive latent variable model is trained on short look-ahead snippets of the reference trajectory to produce actions based on the current state and target. The model's architecture allows for selective execution of behavioral modes and one-shot imitation. It uses a latent variable at each time step to model the state conditional action distribution. The encoder and decoder distributions are designed to reflect short-term motor behavior. The model uses an autoregressive process with a weak prior to ensure similar representations for nearby trajectory snippets in the latent space. The training objective includes an evidence lower bound with a parameter to tune the weight of the prior, allowing for optimization using supervised learning. The architecture implements a conditional information bottleneck between future trajectory and action, without conditioning on actions. The model uses an auto-correlated prior to encourage nearby latent states to be close in the latent space. The information bottleneck limits dependence on future trajectory, forming a compressed representation for action choice. The model implements an information bottleneck between future trajectory and action without conditioning on actions. It uses an auto-correlated prior to encourage nearby latent states to be close in the latent space, forming a compressed representation for action choice. When transferring knowledge from an expert policy to a student, the goal is for the student to replicate the expert's behavior in a set of states visited by the expert. This involves ensuring that expert actions and student actions are close under a plausible expert state distribution. Behavioral cloning involves optimizing the objective by replacing \u03c1 E with an empirical distribution of state-action pairs S. Anticipating and generating an appropriate set of states for training the student can be costly. To efficiently compress the behavior of multiple experts, two schemes are investigated to record experts' state-action mappings and train the student via supervised learning. These schemes are easy to implement and do not require querying multiple experts simultaneously, making them scalable for dealing with numerous experts. Behavioral cloning involves gathering noisy trajectories from the expert and logging their optimal actions. This data is used to train the student via supervised learning. The approach is scalable and does not require querying multiple experts simultaneously. Linear-feedback policy cloning (LFPC) is an approach that logs the action-state Jacobian and expert action along a single nominal trajectory to construct a linear feedback controller. The goal is to perform as well as behavioral cloning while using fewer expert rollouts. Experts trained to reproduce single clips can be seen as nonlinear feedback controllers around this nominal trajectory. Experts trained to reproduce single clips can be seen as nonlinear feedback controllers around a nominal trajectory. A linear feedback policy about the nominal trajectory of the expert can approximate the expert's behavior. The linear feedback policy can approximate the expert behavior well for clips examined. The expert is presented as a feedback controller around a nominal trajectory with states, actions, and Jacobians. The LFPC objective replaces expert actions with a Jacobian-based linear feedback policy, available offline. The choice of perturbation distribution is important for the objective. Estimating the stationary transition noise distribution induced by noisy actions is a cheaper alternative to the state-dependent distribution. Objective 8 shows robustness to variations in \u2206 and is similar to denoising autoencoders. The augmented objective 8 can produce effective results with a fixed marginal distribution for all clips. The augmented objective 8 demonstrates robustness even with limited states, comparing trajectory rollouts for reference behaviors at varying noise levels. LFPC serves as a data augmentation method and aims to match mean action and Jacobian at relevant behavioral states sampled along the nominal trajectory. The Neural Probabilistic Motor Primitive architecture is trained using LFPC by adapting the objective in Eqn. 3. Perturbations are drawn from a suitable distribution, and results are grounded in transferring a single-skill policy between networks. Performance of time-indexed policies for experts is compared in Figure 4. Additional validation can be found in appendix D. The trained neural probabilistic motor primitive models are validated by comparing model variations on training and testing data. Different levels of regularization and a smaller latent space dimension are explored. The models are assessed based on expert imitation rather than action-reconstruction loss, encoding expert trajectories into latent variables for policy execution in the environment. The trained neural probabilistic motor primitive system can execute behaviors conditioned on an open-loop noisy latent variable trajectory, implying that the decoder has learned to stabilize the body during latent-conditioned behavior. This approach is validated by comparing the performance of the trained system against experts on training and held-out clips using tracking rewards. The comparison results show that cloning with medium regularization based on 100 trajectories works best, while LFPC with similar parameters performs less effectively. Regularization and a large latent space are crucial for good results. Setting the autoregressive parameter to 0 hurts performance, validating the choice of prior. When one-shot imitation fails, it raises questions about the decoder's ability to express desired actions or the encoder's failure to encode the trajectory effectively. The decoder's ability to express desired actions may fail when one-shot imitation does not work. Optimization can improve the performance by directly optimizing the decoder mean. This shift is visualized in a three-dimensional space. In a three-dimensional space, the original and optimized latent trajectories are visualized, showing significant performance improvement. Optimization can enhance the decoder's ability to represent behaviors where one-shot imitation fails. The decoder may still be slightly undertrained on rare behavior categories. Optimization improved median relative expert performance from 43% to 78% for clips with less than 50% expert performance. Seamless transitioning between behaviors is possible by concatenating latent-variable trajectories. In a three-dimensional space, optimized latent trajectories show significant performance improvement, enhancing the decoder's ability to represent behaviors. Seamless transitioning between behaviors is achieved by concatenating latent-variable trajectories. Motor primitive modules with more regularization trained faster and achieved higher final performance. The model accurately tracks target speed with a reward function centered at the target speed. The motor primitive module is challenged by a task requiring abrupt, redirected movement with sharp turns and speed changes. The higher-level controller is given a constant target that randomly moves when the humanoid is near it. This task demands a wide range of quick locomotion behaviors, allowing the HL-controller to learn to control the body through the learned primitive space. The study found that more regularized motor primitive modules led to stable initial behavior and faster training, achieving higher performance. The model demonstrated robustness to unseen perturbations and successfully solved tasks involving abrupt speed changes and varied locomotion behaviors. In a study on locomotion behavior, different movements like walking, jogging, and running were analyzed. A high-level controller using visual inputs successfully completed an obstacle course task. The study highlighted the effectiveness of using pretrained neural motor primitives for training new controllers on sparse reward tasks. The resulting movements were humanlike, indicating a well-structured embedding space. The module also enabled smooth physics-based control. The paper discussed transfer and compression of control policies. The paper describes approaches for transferring and compressing control policies using a motor primitive module. The module learns to represent and execute motor behaviors for controlling a simulated humanoid body. By using behavioral cloning or linear feedback policy cloning, the neural probabilistic motor primitive system can perform robust one-shot imitation with limited data. The LFPC method shows promise for transferring expert behavior with just a single rollout, making it useful for settings where rollouts are costly. Further improvements can be made by tuning certain parameters. The neural probabilistic motor primitive module can be improved by tuning parameters, such as the marginal noise distribution \u2206. It is interpretable and reusable, serving as a basis for continual learning of motor skills. Future work could expand the range of behaviors or demonstrate continual learning and reuse of new skills. Data for the project was obtained from mocap.cs.cmu.edu, and the model was trained using the reparametrization trick BID15 BID31 and stochastic gradient descent with ADAM BID14. The emphasis of the proposal is to match the responsivity of the expert policy in a neighborhood around each state, rather than focusing on activation or KL matching. Robust knowledge transfer discussed here is different from other settings, such as matching exact activations in the presence of perturbations. In the context of control policies, the student system aims to replicate the expert's actions despite input perturbations. This approach may not be suitable for control tasks where small input changes are crucial. The student policy behaves in an open-loop fashion, neglecting local feedback. In a locomotion experiment, the student policy is tested without the need for time indexing. The student policy, tested in a locomotion experiment, replicates expert actions despite input perturbations. It does not require time indexing and shows generalization in the presence of noise. The cloned-policy trajectories return to the limit cycle despite deviations from the reference trajectory."
}