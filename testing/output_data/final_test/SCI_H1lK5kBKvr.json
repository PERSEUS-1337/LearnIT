{
    "title": "H1lK5kBKvr",
    "content": "Recovering 3D geometry, albedo, and lighting from a single image is a challenging task. Prior knowledge like linear 3D morphable models (3DMM) is often used to aid in reconstruction. However, linear models struggle with facial images in diverse conditions. Recent methods use convolutional neural networks (CNN) to directly regress face shape and texture, but these models have limitations. This paper proposes training a model with adversarial loss in a semi-supervised manner on hybrid batches of unlabeled and labeled face data. Our proposed model utilizes adversarial loss in a semi-supervised manner on hybrid batches of unlabeled and labeled face images to leverage large amounts of unlabeled face images. A novel center loss ensures that facial images of the same person have consistent identity shape and albedo. The model disentangles identity, expression, pose, and lighting representations, improving reconstruction performance and enabling facial editing applications like expression transfer. Extensive experiments show high-quality reconstruction compared to state-of-the-art methods, even under various expression, pose, and lighting conditions. 3D face reconstruction from 2D images for applications like face recognition, face puppetry, and virtual make-up is achieved using a 3D morphable model (3DMM) as a strong prior assumption. The conventional approach involves searching for 3DMM parameters through analysis-by-synthesis optimization to match the 2D image by optimizing shape and texture. Regressing 3DMM parameters using CNN has shown success in face reconstruction, but is limited to a linear low-dimensional subspace. Linear statistical models have constraints in capturing the nonlinear facial variations in ethnicity, age, expressions, and skin tones, requiring a large dataset for training. The most popular 3D face model, 3DMM, was built by merging BFM with 200 subjects and FaceWarehouse with 150 subjects in different expressions. LSFM was constructed from 10,000 facial identities but only in neutral expressions. Various models have been proposed to improve 3D face reconstruction, such as encoder-decoder networks by Tran & Liu and Tran et al. Tran et al. (2019) introduced encoder-decoder networks for direct regression of face shape and texture, offering higher representation power than linear models. However, these nonlinear models were trained on the 300W-LP dataset, limiting their ability to reconstruct facial texture accurately. Additionally, the models decoded face albedo and shape parameters separately without considering facial identity, which should be consistent across different face images. In this paper, a novel encoder-decoder architecture using inverse rendering is proposed to disentangle face albedo and shape parameters, ensuring alignment with facial identity for applications like face recognition and animation. The vision system decomposes 2D face images into sematic representations, addressing the entanglement of identity and expression in existing methods. The system decomposes 2D face images into disentangled representations like identity, expression, pose, and lighting codes. It uses computer vision and graphics techniques for self-supervised learning with unlabelled data. The network is trained semi-supervised on hybrid batches of unlabelled and labelled face images generated from a 3DMM model. The reconstructed 3D face shape is ensured to be realistic through the use of a discriminator network and semi-supervised adversarial training. Graph convolutional network (GCN) is utilized for reconstructing the 3D face shape efficiently on 3D meshes. Modeling graph convolutions on 3D meshes is memory efficient and allows for processing high resolution 3D structures. GCN-based methods outperform other state-of-the-art methods for reconstructing 3D face shapes. A GCN network is used to learn an illumination-independent face albedo, followed by a CNN-based decoder network with skip connections and a patchGAN to enhance facial texture details. Face recognition and center loss are applied to extract identity representation from multiple face images, ensuring compactness and separability for different individuals. The proposed method achieves state-of-the-art performance in 3D face reconstruction, utilizing a semi-supervised and adversarial training process to exploit unlabelled face data efficiently. It also introduces a novel framework for extracting nonlinear disentangled representations from face images, enhancing applications like face recognition and facial expression transfer. The curr_chunk discusses a framework for extracting nonlinear disentangled representations from face images using face recognition losses and shape pairwise loss. It also includes background information on linear 3DMM and face rendering processes. The model achieves state-of-the-art performance in face reconstruction. The curr_chunk explains the modeling of 3D face texture using PCA and the rendering process using weak perspective projection. It assumes the face is a Lambertian surface and approximates global illumination with spherical harmonics. The curr_chunk discusses using spherical harmonics for illumination modeling and spectral graph convolution for reconstructing 3D face shapes represented as a triangular mesh. The rendering process depends on a parameter set including illumination coefficients. Spectral graph convolution operates in the graph Fourier transform domain using Laplacian matrix eigenvectors. The curr_chunk discusses spectral graph convolution using Chebyshev polynomials for mesh filtering and an encoder-decoder architecture for semi-supervised adversarial training to extract disentangled semantic representations from a single image. It also mentions utilizing an inverse rendering technique with a parameterized illumination model and differentiable renderer for face image reconstruction under varying conditions. The model is trained on unlabeled and labeled face images to reconstruct face albedo, shape, and render face images under varying conditions. The encoder network extracts identity, expression, pose, and lighting representations using a ResNet-50 backbone. Multiple losses are applied to the network for training. The ResNet-50 network is used as the backbone of the encoder network, followed by four branches of fully connected layers extracting identity, expression, pose, and lighting codes. The shape decoder network, based on the COMA architecture, utilizes a graph convolutional network to generate 3D face shapes from a 192-D vector input. The albedo decoder network, like the shape decoder, is a graph convolutional network. It takes only the identity code as input to ensure consistency in facial albedo across different photos. Facial occlusions like hair, glasses, and microphones are excluded from the albedo. Face segmentation is applied to remove these occlusions. Aging, injury, and other factors affecting facial albedo are not considered. After learning the lighting representation, the albedo decoder network is changed to a CNN network with skip connections to the encoder. The albedo decoder network is transformed into a CNN network with skip connections to the encoder, resembling U-Net architecture. PatchGAN is used to enhance facial texture realism. The network is trained with a multi-task loss for 3D face shape and albedo regression, incorporating face recognition, reconstruction, shape, adversary, and regularization losses. Face recognition loss focuses on extracting identity code for facial identity representation. The deeply learned identity code is enhanced with \u03bb center for balancing loss functions. Face recognition loss is crucial for learning facial identity independently of other factors. Face reconstruction loss involves rendering and eliminating facial occlusions. Image gradient difference loss (GDL) is used for detailed reconstruction. Sparse landmark loss aids in learning face pose for better reconstruction. The sparse landmark loss aids in learning face pose for better reconstruction by defining the projected face shape, ground truth sparse 2D landmarks, and applying the idea of GDL on sparse landmarks. Shape loss prevents unrealistic 3D face shapes and linear 3DMM constraints through semi-supervised training on hybrid batches of unlabeled and labeled face data. In a semi-supervised manner, our network is trained on hybrid batches of unlabeled and labeled face images using the 300W-LP dataset with fitted 3DMM shapes. The BFM template with 53,215 vertices is utilized, and a new face shape template with 37,202 vertices is created by removing the neck and ears. Pairwise shape loss is employed to disentangle identity and expression codes during training on the 300W-LP dataset. To disentangle the identity and expression codes, the network is trained on the 300W-LP dataset using pairwise shape loss. This involves interchanging expression parameters between input images to create 3D face shapes. Shape smooth loss is applied through Laplacian regularization to refine the shapes and remove noise. The Laplacian equation is used to calculate the difference of each vertex with its first order neighbors to match a shape template. Albedo symmetry loss helps in learning face albedo by enforcing facial symmetry. Adversarial loss is used for generating realistic 3D face shapes in unsupervised learning, following the concept of generative adversarial networks (GAN). The adversarial loss is utilized to train an encoder-decoder network and discriminator network based on WGAN-div for generating realistic 3D face shapes. The discriminator network distinguishes between fake shapes reconstructed from the network and real shapes sampled from the linear 3DMM distribution. A min-max optimization problem is formulated with an adversarial loss term. Ablation tests are conducted to demonstrate the effectiveness of the framework design. The framework design is evaluated through ablation tests, reconstruction error comparisons with 3D face scans, and expression transfer applications. The model is trained on hybrid batches of unlabeled face images from CelebA dataset and labeled face images from 300W-LP dataset. Evaluation is done using MICC Florence and AFLW2000-3D datasets. The BFM model's face region is used as a 3D face mesh template. Optimization is performed using Adam and RMSprop optimizers. The effects of shape smooth loss and adversarial loss on shape reconstruction quality are studied. Conventional smoothing loss causes abnormal effects on face shapes, such as vertices on the mouth's inner edge distancing from neighbors and flat nostrils. Without adversarial loss, face meshes may shrink and eyebrows extrude out. The adversarial loss ensures generated face shapes are not distorted. The adversarial loss ensures that generated face shapes are not distorted, maintaining proximity to human face shape distribution. Albedo symmetric loss with facial mask is crucial for lighting representation learning to avoid confounding shade and lighting with facial occlusions. Without proper learning of lighting, even with a high representation power albedo decoder, the model may fail to learn lighting details. The input image in Figure 3 shows that reconstructing high fidelity texture alone may not align with the face shape, leading to odd results in different poses. To disentangle lighting from albedo, a facial mask with albedo symmetric loss is used. A CNN-based albedo decoder with skip connections improves facial albedo detail. Texture ablation test in Figure 3 demonstrates failures in lighting when removing facial mask and albedo symmetric loss. The model is evaluated on the MICC Florence dataset for quantitative analysis. The study evaluates the 3D face shape quantitatively on the MICC Florence dataset, using videos of subjects in neutral expressions. The evaluation metric involves calculating point-to-plane L2 errors with the predicted face shape. Additionally, the model is qualitatively evaluated on the AFLW2000-3D datasets. The study evaluates 3D face shape quantitatively on the MICC Florence dataset using videos of subjects in neutral expressions. The model is also qualitatively evaluated on the AFLW2000-3D datasets. A fine-scale corrective model adds more details to the linear model, but the reconstructed face shape may fail if the foundation face shape is not good enough. The proposed method reconstructs facial texture in more detail compared to previous work by Tewari et al. (2018). The CNN-based decoders have higher representation power than a linear 3DMM, but they were trained on the 300W-LP dataset. The nonlinear model struggles to fit this dataset, resulting in poor performance on face images with diverse expressions. In contrast, the proposed model achieves better performance across various conditions and disentangled representations improve face reconstruction and editing applications. The paper introduces an encoder-decoder architecture for reconstructing 3D faces from single images, with disentangled representations for identity, expression, pose, and lighting. It also demonstrates expression transfer between different face images, maintaining identity while changing pose, lighting, and expression. The model shows high robustness and utilizes a semi-supervised training scheme to leverage unlabeled face images effectively. The paper presents a model for reconstructing 3D faces from single images, with disentangled features for identity, expression, pose, and lighting. It uses an adversarial loss to prevent unrealistic face generation and outperforms existing methods in both quantitative and qualitative evaluations."
}