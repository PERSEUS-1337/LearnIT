{
    "title": "rJlnOhVYPS",
    "content": "Person re-identification (re-ID) involves identifying individuals across different cameras, but adapting models between datasets is challenging due to domain diversities. Unsupervised domain adaptation methods transfer knowledge using pseudo labels from clustering algorithms, but label noise hinders performance. To address this, a Mutual Mean-Teaching (MMT) framework is proposed to refine pseudo labels in the target domain for better feature learning. The Mutual Mean-Teaching (MMT) framework refines pseudo labels in the target domain for better feature learning, introducing a soft softmax-triplet loss to support learning with soft pseudo triplet labels. This approach achieves significant improvements in unsupervised domain adaptation tasks, with mAP increases of 14.4% to 18.2%. The proposed Mutual Mean-Teaching (MMT) framework aims to refine noisy pseudo labels in Unsupervised Domain Adaptation (UDA) methods for person re-ID. It utilizes on-line refined soft pseudo labels to improve feature learning by optimizing neural networks under joint supervisions of off-line refined hard pseudo labels. This approach is inspired by teacher-student approaches and achieves better UDA performance. The text discusses training two networks simultaneously to refine pseudo labels for better feature learning in Unsupervised Domain Adaptation methods for person re-ID. A collaborative training strategy using temporally average models and soft pseudo labels is proposed to iteratively improve feature representations and achieve state-of-the-art performance. The text introduces a novel soft softmax-triplet loss to enable using triplet loss with soft pseudo labels in the Mutual Mean-Teaching (MMT) framework for person re-ID. The collaborative training strategy improves feature representations iteratively, tackling the label noise problem crucial for superior performance in clustering-based UDA methods. The Mutual Mean-Teaching (MMT) framework introduces soft labels for more reliable training, enabling the use of soft triplet labels to learn discriminative person features. It shows strong performance in unsupervised domain adaptation (UDA) tasks for person re-ID, with significant improvements in mAP compared to state-of-the-art methods. Innovative methods for unsupervised domain adaptation in person re-identification have been proposed. Fan et al. (2018) suggested assigning labels to unlabeled samples and optimizing the network with generated targets. Another approach involved bottom-up clustering with a repelled loss and assigning hard pseudo labels for global and local features. Existing methods have struggled with noise from hard pseudo labels, while SPGAN (Deng et al., 2018) and PTGAN focused on style transfer for domain-invariant features. HHL (Zhong et al., 2018) learned camera-invariant features using style-transferred images. Generic domain adaptation methods for close-set recognition aim to learn features that minimize the distribution differences between the data sources. HHL (Zhong et al., 2018) focused on camera-invariant features through style-transferred images, while ENC (Zhong et al., 2019) utilized soft labels with an exemplar memory module. MAR employed multiple soft-label learning by comparing with reference persons, but the accuracy of labels from reference images and features may limit performance advancements. Methods for domain adaptation aim to minimize distribution differences between data sources. Adversarial learning and Maximum Mean Discrepancy (MMD) loss are commonly used techniques. However, these methods assume shared classes between domains, which is not suitable for unsupervised person re-ID. Teacher-student models, popular in semi-supervised learning, aim to provide consistent training supervisions. Existing methods for domain adaptation focus on minimizing distribution differences between data sources using adversarial learning and Maximum Mean Discrepancy (MMD) loss. However, these methods require shared classes between domains, which is not ideal for unsupervised person re-ID tasks. Teacher-student models offer consistent training supervisions by utilizing different models' predictions. Temporal ensembling, mean-teacher model, and deep mutual learning are some examples of teacher-student mechanisms used in semi-supervised learning. These methods are mostly designed for close-set recognition problems and may not be directly applicable to unsupervised domain adaptation tasks for person re-ID. Existing methods for domain adaptation focus on minimizing distribution differences between data sources using adversarial learning and Maximum Mean Discrepancy (MMD) loss. However, for unsupervised person re-ID tasks, handling noisy labels is crucial. Methods for noisy label correction can be categorized into loss correction, label correction, and noise-robust methods. Loss correction methods model noise transition matrix, while label correction methods directly correct noisy labels. Noise-robust methods design robust loss functions against label noises but may not address triplet loss with noisy labels effectively. Our proposed Mutual Mean-Teaching (MMT) framework addresses noisy pseudo labels in clustering-based Unsupervised Domain Adaptation (UDA) methods for person re-ID tasks. Existing methods focus on minimizing distribution differences between data sources, while our method specifically targets noisy label correction for improved performance. Our proposed Mutual Mean-Teaching (MMT) framework aims to refine pseudo labels in clustering-based Unsupervised Domain Adaptation (UDA) methods for person re-ID tasks. It introduces off-line refined hard pseudo labels and on-line refined soft pseudo labels for optimal domain adaptation performance. Additionally, a novel soft softmax-triplet loss is introduced to better utilize the softly refined pseudo labels, working jointly with the soft classification loss. The Mutual Mean-Teaching (MMT) framework refines pseudo labels in clustering-based Unsupervised Domain Adaptation (UDA) for person re-ID tasks. It introduces off-line refined hard pseudo labels and on-line refined soft pseudo labels to optimize domain adaptation performance. Additionally, a soft softmax-triplet loss is introduced to better utilize the refined pseudo labels, working jointly with the soft classification loss. The network parameters and a learnable target-domain classifier are optimized with respect to an identity classification loss and a triplet loss until training converges. However, errors in pseudo label generation hinder feature learning. The Mutual Mean-Teaching (MMT) framework, along with a soft softmax-triplet loss, addresses errors in clustering algorithms that hinder feature learning. In supervised pre-training for source domain UDA tasks, a deep neural network is pre-trained on the source domain to model a feature transformation function and separate features using classification and triplet losses. The Mutual Mean-Teaching (MMT) framework utilizes soft identity classification and soft softmax-triplet losses to separate features of different identities. The framework is based on clustering-based UDA methods with off-line refined hard pseudo labels, aiming to improve feature learning by addressing errors in clustering algorithms. The MMT framework incorporates on-line refined soft pseudo labels in addition to off-line refined hard pseudo labels to mitigate pseudo label noise. Soft pseudo labels are generated by training two collaborative networks with different initializations, allowing them to capture training data distribution for improved feature learning. The training data distribution and class predictions serve as soft labels for training, but are not perfect due to errors and noisy pseudo labels. To prevent bias, past average models are used to generate soft pseudo labels for collaborative networks. Both hard and soft labels are used to train the networks, with the best performing past model used for inference. The networks are denoted as F(\u00b7|\u03b81) and F(\u00b7|\u03b82) with pseudo label classifiers C t 1 and C t 2. The same image batch is fed to both networks for training. To train the collaborative networks, the same image batch is fed to both networks with random erasing, cropping, and flipping. Pseudo label confidences are predicted for each target-domain image. To prevent error amplification, the temporally average model of each network is used to generate reliable soft pseudo labels for supervising the other network. The collaborative networks use temporally average models to generate soft pseudo labels for supervising each other, preventing error amplification. Soft classification loss is optimized using these pseudo labels, improving dis-related predictions between the networks. The proposed method involves using soft triplet labels generated by collaborative networks' past temporal average models to optimize softmax-triplet loss, addressing the challenge of optimizing triplet loss with soft pseudo labels. Our MMT framework utilizes soft triplet labels for training, overcoming the limitations of hard supervisions. It combines off-line refined hard pseudo labels with on-line refined soft pseudo labels to optimize the overall loss function and improve domain adaptation performance. The proposed method is evaluated on three person re-ID datasets. The Market-1501 dataset has 32,668 annotated images of 1,501 identities, DukeMTMC-reID contains 16,522 person images of 702 identities, and MSMT17 has 126,441 bounding boxes of 4,101 identities. Four domain adaptation tasks are set up for evaluation. The training data organization for domain adaptation tasks involves mini-batches with 64 person images of 16 identities, with hard pseudo labels updated after each epoch. Images are resized to 256 \u00d7 128 before being input into networks. Hyper-parameters for the MMT framework are chosen based on a validation set of the Duke-to-Market task and applied to other domain adaptation tasks. The training process for domain adaptation tasks involves a two-stage training scheme with ResNet-50 or IBN-ResNet-50 as backbone networks. The networks are optimized using the ADAM optimizer with a weight decay of 0.0005. Randomly erasing is only used in target-domain fine-tuning. In Stage 1, source-domain pre-training is conducted with ImageNet pre-trained weights, updating network parameters independently. In Stage 2, end-to-end training with MMT is performed based on pre-trained weights. In the end-to-end training with MMT, two networks are updated collaboratively using pre-trained weights \u03b81 and \u03b82. The loss weights \u03bbtid and \u03bbttri are set to 0.5 and 0.8 respectively, with a fixed learning rate of 0.00035 for 40 epochs. K-means clustering is utilized with different numbers of pseudo classes for different datasets. The proposed MMT method outperforms state-of-the-art methods on Market-1501, DukeMTMC-reID, and MSMT17 datasets. The MMT framework outperforms existing methods on domain adaptation tasks without manual annotations. It achieves near fully-supervised learning performance and surpasses clustering-based SSG by significant margins on Market-to-Duke and Duke-to-Market tasks. The MMT framework outperforms existing methods on domain adaptation tasks without manual annotations, achieving state-of-the-art performances on all tested target datasets with different cluster numbers. The proposed pseudo label refinery shows significant improvements on Market-to-Duke and Duke-to-Market tasks, even with a specified cluster number of 500. The MMT method demonstrates significant improvements compared to other methods. When comparing with Co-teaching (Han et al., 2018) on unsupervised person re-ID task with 500 pseudo identities, there is a noticeable drop in mAP on Market-to-Duke and Duke-to-Market tasks. Co-teaching is designed for close-set recognition problems with manually generated label noise, which limits its effectiveness in this scenario. Our proposed framework evaluates components through ablation studies on Duke-to-Market and Market-to-Duke tasks with different backbones. Results in Table 2 show the effectiveness of the soft pseudo label refinery in handling noisy pseudo labels. Baseline models using only off-line refined hard pseudo labels exhibit significant drops in mAP. The proposed MMT framework demonstrates the effectiveness of soft pseudo label refinement and the soft softmax-triplet loss. Experiments show considerable drops in mAP when removing the soft softmax-triplet loss, highlighting its importance in improving performance on different datasets and backbones. The proposed Mutual Mean-Teaching (MMT) framework utilizes soft pseudo label refinement and soft softmax-triplet loss to improve network structures. Soft labels are generated online for one network using the past average model of the other network, leading to a 5.3% mAP improvement. This framework effectively avoids bias amplification and can be simplified by using only one network for training with its past temporal average model. The proposed Mutual Mean-Teaching (MMT) framework utilizes soft pseudo label refinement and soft softmax-triplet loss to improve network structures. Experiments show significant mAP drops when not using the mutual mean-teaching scheme, validating its necessity for providing robust soft pseudo labels. Despite performance declines without certain components, MMT outperforms the baseline model significantly. Our proposed MMT outperforms the baseline model significantly by utilizing soft pseudo label refinement and hard pseudo labels. While soft pseudo labels bring improvements, hard labels are essential for learning discriminative representations. Hard triplet loss is not necessary in our framework. In our framework, the hard triplet loss is not essential as experiments show similar performance without it. We introduce an unsupervised Mutual Mean-Teaching (MMT) framework to address noisy pseudo labels in clustering-based unsupervised domain adaptation for person re-ID. The key is refining pseudo labels to model inter-sample relations better. Additionally, a soft softmax-triplet loss is proposed to support learning with softly refined triplet labels. The softmax-triplet loss is proposed in the MMT framework to improve person re-ID performance on domain adaptation tasks. Two temporal average models are introduced to provide complementary soft labels and prevent training error amplification. The effectiveness of this design is verified by comparing predictions with and without the average models. In the MMT framework, two temporal average models prevent networks from converging too quickly during collaborative training. Weighting factors \u03bb t tri and \u03bb t id are tuned for experiments, showing robustness except when \u03bb t id = 1.0. The impact of different parameters is analyzed, with the framework being insensitive except for the elimination of hard classification loss. In Figure 4 (a-b), the effect of weighting factor \u03bb t tri in equation 9 is investigated. The weight for soft softmax-triplet loss is \u03bb t tri and for hard triplet loss is (1 \u2212 \u03bb t tri). MMT-500 is tested with ResNet-50 and IBN-ResNet-50 backbones with varying \u03bb t tri values. Soft softmax-triplet loss is removed when \u03bb t tri is 0.0, and hard triplet loss is eliminated when \u03bb t tri is 1.0."
}