{
    "title": "rkxSEQtLUS",
    "content": "In the visual system, neurons respond to their classical receptive field (RF) and can be modulated by stimuli in the surround through lateral connections. A new model, CNNEx, uses supervised learning for feedforward connections and unsupervised learning for lateral connections within a convolutional neural network. These connections help units integrate information from their surround, improving network robustness and performance on noisy MNIST and CIFAR-10 datasets. The same unsupervised learning rule generalized to both datasets, showing potential for application to networks trained on other tasks. Our framework incorporates lateral connections learned using a modified Hebbian rule into convolutional neural networks, enhancing robustness to adversarial attacks. The brain's use of recurrent connections, including lateral and feedback connections, provides immunity to attacks and enables unsupervised learning for rich internal representations. This model can be applied to networks trained on various tasks. Our model incorporates lateral connections into convolutional neural networks, improving performance and robustness when exposed to noise perturbations in MNIST and CIFAR-10 datasets. The integration of lateral connections is highlighted as a crucial area for future research, with a focus on orientation and distance dependence. The curr_chunk discusses the construction of filters using estimates of spatial receptive field sizes in mouse V1, showing positive and negative lateral connection weights. Predicted connections match experimental data, with normative models based on sparse coding predicting anti-Hebbian lateral connections. Our model, inspired by the MGSM model, aims to optimize coding of images by inferring contextual interactions between receptive fields and their surroundings. Unlike other normalization schemes in deep neural networks, we propose a specific computational role for each pyramidal neuron in integrating lateral input information optimally. Our proposal aims to optimize coding of images by incorporating optimal lateral connections into feedforward neural networks. The synaptic weights are learned unsupervisedly using a specific rule, different from Hebbian learning. Experimental data comparison is also discussed. The study compared predicted lateral connections in mouse cortex to experimental data using natural images and RF features. Noise was added to images for testing, and network architecture details were provided. The model architectures used for training the CNNEx and CNN-PM models are described in Table 1. Different hyperparameters were used for training MNIST and CIFAR models, with 10 epochs for MNIST and 50 epochs for CIFAR. Pytorch (v. 0.3.1) on a NVIDIA GTX 1080 Ti GPU was used for all experiments. Lateral connections were trained using supervised learning before freezing. After training feedforward weights in the network using supervised learning, lateral connections were introduced between units in the first two convolutional layers. The lateral connection weights are learned once at the end of supervised training without updating the feedforward weights through backpropagation. Future work will explore methods for semi-supervised learning combining supervised learning of feedforward weights with unsupervised learning of lateral connections. Network regularization included a weight decay value of 0.005 and a dropout fraction of 0.5 applied after each convolutional layer. The model utilized dropout after each convolutional layer and the first fully connected layer, along with lateral connections in the first two convolutional layers. Hyperparameters were selected through grid search and model performance was evaluated with and without regularization. Lateral connections improved model accuracy on the MNIST dataset. The use of lateral connections in the model improved robustness to noise in CIFAR-10, with a slight decrease in accuracy on original images. Fine-tuning after incorporating lateral connections may help recover some of this loss. Lateral connections, combined with regularization, resulted in better performance on both MNIST and CIFAR-10 datasets by reducing redundancy and noise in feature activations. The CNNEx model uses contextual information to modulate unit responses, achieving higher accuracies under noisy conditions. Lateral connections in the model capture structure in the world's statistics via unsupervised learning, allowing for integration of information across space and features. This approach predicts relevant world structure for a given task, focusing on necessary features rather than arbitrary structures. Deep neural networks incorporate recurrent connections for visual attention mechanisms. Ladder networks combine supervised and unsupervised learning using noise injection, while our model utilizes a modified Hebbian learning rule for optimal lateral connections within each layer. This approach focuses on learning connections based on unit activations, accounting for inherent neuronal noise variability. Neurons in cortical circuits compute responses to stimuli, with optimal lateral connections providing robustness by integrating information from various sources. A simple network architecture was used as a proof-of-concept, not achieving state-of-the-art performance on image datasets. Improvements could be made by fine-tuning models or using deeper architectures. Future experiments will test scalability of learning optimal lateral connections. Future experiments will need to test the scalability of learning optimal lateral connections on more complex network architectures and larger image datasets like ImageNet, to determine if these connections offer any benefits against noise or adversarial images."
}