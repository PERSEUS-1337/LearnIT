{
    "title": "SkgODpVFDr",
    "content": "Convolutional Neural Networks (CNNs) utilize multiple convolution layers for vision tasks, with regular convolution based on Receptive Field (RF). Neurons in lower layers with smaller RF contribute to creating neurons in higher layers with larger RF, allowing for capturing global context. To address the limitations in lower layers of the biological brain, a spatially shuffled convolution (ss convolution) is proposed as an extension of regular convolution, enabling the use of information outside the RF through spatial shuffling. The experiments on CIFAR-10 and ImageNet-1k datasets demonstrate that ss convolution enhances classification performance in various CNNs. CNNs, inspired by the cat visual cortex, excel in image recognition, natural language processing, and speech recognition. The Receptive Field (RF) in convolution layers plays a crucial role in capturing global context through hierarchical architectures. The units in high layers of CNNs capture global context, influenced by feedback from higher-order areas and horizontal connections. Neurons in the primary visual cortex adjust based on information outside their receptive field, allowing for the use of global context. Feedback connections act as attention, while horizontal connections enable communication between distant neurons in the layer. In this work, the focus is on introducing horizontal connections for regular convolution in CNNs through spatially shuffled convolution (ss convolution). This operation incorporates information outside the receptive field without adding learnable parameters, inspired by the function of horizontal connections in visual processing. The design of ss convolution in CNNs is inspired by horizontal connections to incorporate information outside the receptive field. Experiments on CIFAR-10 and ImageNet datasets show improved classification performance with ss convolution. Spatial shuffling allows regular convolution to utilize information outside its RF, enhancing CNNs' performance. The atrous convolution and deformable convolution broaden the receptive field in CNNs. Atrous convolution has a fixed stride, while deformable convolution has a flexible stride. Deformable convolution requires extra computations but allows for more flexibility in processing distant pixel information for local information at a layer. The size of the convolution kernel grows with the number of layers, requiring a larger kernel size for processing local information. The Squeeze and Excitation module modulates activation values using global context obtained through Global Average Pooling, allowing CNNs to utilize information outside their receptive field. A sparse kernel caused by a large stride is not suitable for processing local information. The ss convolution gives marginal improvements on SEResNet50. The ss convolution improves the performance of SEResNet50 by making the RF location-dependent, unlike the location-independent RF of ResNet with SE module. This difference may be why ss convolution enhances classification performance. Attention Branch Networks (ABN) use an attention mechanism for top-down visual explanation, modulating activation values based on larger RF outputs from the side branch. In experiments, ss convolution enhances performance on ABN by modulating activation values based on external information. Unlike ShuffleNet's channel shuffling, ss convolution uses spatial shuffling to incorporate information outside the regular convolution's RF. Spatially shuffled convolution (ss convolution) is introduced as a mechanism to incorporate information outside of the regular convolution's receptive field (RF). It consists of spatial shuffling and regular convolution, with a fixed permutation matrix used for shuffling the input before applying regular convolution. The hyper-parameter \u03b1 controls the amount of channel shuffling. The hyper-parameter \u03b1 controls channel shuffling in ss convolution. A fixed permutation matrix is used for shuffling. Results on CIFAR-10 show best performance at \u03b1 around 0.06. Group convolution variants are not suitable for shuffling. ResNeXt utilizes heavily group convolutions. Test error on CIFAR-10 is marginal with shuffling. Spatial shuffling is proposed for group convolution. The spatial shuffling proposed for group convolution involves interleaving shuffled parts according to Eqn. 3, improving classification performance of ResNeXt on CIFAR-10 and ImageNet-1k datasets. Data augmentation techniques are applied to the images before training the models. Data augmentation techniques such as resizing, flipping, and normalization are applied to images before training models using ResNet50, DenseNet121, SEResNet50, and ResNet50 with ABN for ImageNet-1k experiments. Momentum SGD optimizer with specific parameters is used for training models on CIFAR-10 and ImageNet datasets. Regular convolutions are replaced with ss convolutions in experiments, maintaining a consistent alpha value across all layers. In experiments, regular convolutions are replaced with ss convolutions using a single alpha value across all layers. A grid search is conducted for alpha values of 0.02, 0.04, and 0.06, with the selection based on validation dataset performance. Results show that ss convolution generally improves classification performance in various CNNs, except for SEResNet50 which shows marginal improvements. The small alpha values lead to minimal input shuffling, resulting in improved performance without additional learnable parameters. The inference speed of ss convolution is slightly slower but shows a 0.5% improvement in the ImageNet-1k dataset. Efficient implementation may reduce the speed gap between regular convolution and ss convolution. Two analyses are conducted to understand the performance improvement: receptive field analysis and layer ablation experiment. The study investigates how spatial shuffling (ss) convolution impacts model predictions in the primary visual cortex. Using an ImageNet-1k pre-trained ResNet50 model, the analysis focuses on calculating the receptive field (RF) to determine if ss convolution utilizes information beyond the regular convolution's RF. The RF is optimized using input x, RF R, sigmoid function \u03c3, and model output \u03c6 l. The local perceptual loss is defined as the first term in the equation, similar to perceptual loss but with a dot product of M. The study explores the impact of spatial shuffling convolution on model predictions in the primary visual cortex. It involves optimizing the receptive field (RF) using input x, RF R, sigmoid function \u03c3, and model output \u03c6 l. The local perceptual loss minimizes feature distance in a specific region between \u03c3(R) \u00b7 x and x, with additional penalties to avoid trivial cases. Adam optimizer is used with specific hyperparameters, and optimization stops after 10000 iterations. Data augmentation and preprocessing follow the same procedure as in Sec. 4.1. The study explores the impact of spatial shuffling convolution on model predictions in the primary visual cortex. Data augmentation and preprocessing are applied following the same procedure as in Sec. 4.1. The receptive field (RF) is optimized using input x, RF R, sigmoid function \u03c3, and model output \u03c6 l. RFs for different models are calculated and visualized in Fig. 3, showing changes in features inside the blue box. The size of RF increases with deeper layers, as seen in the RFs of ResNet50 with regular convolution. The study investigates the impact of spatial shuffling convolution on model predictions in the primary visual cortex. Results show that the RFs of ResNet50 with ss convolution and SEResNet50 cover the entire image, indicating successful utilization of information outside the regular convolution's RF. The size of RFs remains consistent across layers in both models, suggesting that the SE module and ss convolution contribute to utilizing information beyond the regular convolution's RF. The study explores the impact of spatial shuffling convolution on model predictions in the primary visual cortex. Results suggest that ss convolution improves classification performance on various CNNs by utilizing information outside the regular convolution's RF. Further analysis in channelwise RF analysis reveals location-independent information in SEResNet50 and location-dependent information in ResNet50 with ss convolution. This difference is attributed to the global average pooling in the SE module, which does not conserve spatial information. The study investigates the impact of spatial shuffling convolution on model predictions, showing that it improves classification performance by utilizing information outside the regular convolution's receptive field. Ablation experiments reveal that the biggest drop in performance occurs when conv4 4 is removed, indicating the importance of utilizing information between middle and high layers. Even ablation of the first bottleneck (conv2 1) leads to degraded performance, highlighting the usefulness of information outside the regular convolution's receptive field even at lower layers. The study proposes spatially shuffled convolution (ss convolution) to incorporate horizontal connections in regular convolution, capturing information outside the receptive field even in lower layers. Results suggest that utilizing distant information improves classification performance across various CNNs. The receptive field of different layers in ImageNet-1k pre-trained ResNet50 and SEResNet50 is shown in Figures 7 and 8. The red color indicates changing features, while white represents invariant features when the pixel value changes. The layers are described in Table 5."
}