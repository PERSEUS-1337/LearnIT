{
    "title": "BJQRKzbA-",
    "content": "Efficient neural architecture search using evolutionary algorithm with hierarchical genetic representation scheme. Outperforms manually designed models for image classification with top-1 error of 3.6% on CIFAR-10 and 20.3% on ImageNet. Random search achieves slightly lower accuracy but reduces search time significantly. Neural network architectures for image classification have evolved from deep, chain-structured layouts to more complex, graph-structured topologies. Automated algorithms are now being used to find optimal architectures in a given search space to maximize validation accuracy. These algorithms include random search, Monte Carlo Tree Search, evolution, and reinforcement learning. Neural network architectures have evolved to complex graph-structured topologies. Automated algorithms like Tree Search, evolution, and reinforcement learning are used to find optimal architectures. Architecture search is computationally intensive, so constraints are used to reduce complexity. In this work, a hierarchical network structure is imposed with flexible topologies at each level. Hierarchical neural network architectures are formed by stacking motifs at different levels, allowing for immediate propagation of changes across the network. This approach, similar to handcrafted designs like VGGNet and ResNet, is discovered through evolutionary or random search methods. The evolution of neural architectures is a sub-task of neuroevolution, where both the network's topology and weights are simultaneously evolved. Our work demonstrates that random or evolutionary methods, when combined with a powerful architecture representation, can achieve competitive performance on image classification tasks with significantly less computational resources. This approach, similar to handcrafted designs like VGGNet and ResNet, allows for immediate propagation of changes across the network. Our main contributions include introducing hierarchical representations for neural network architectures, demonstrating the effectiveness of simplistic random search for competitive image classification architectures, and presenting a scalable evolutionary search variant that achieves the best published results. The text discusses flat and hierarchical representations of neural architectures, with a focus on primitive operations and building blocks for forming larger motifs. The text discusses neural network architectures represented by a computation graph with feature maps and primitive operations like convolution and pooling. An architecture is defined by an adjacency matrix specifying the operations' placement between nodes, which are assembled into motifs for sequential computation in the neural network. The hierarchical architecture representation involves level-2 motifs in a neural network where feature maps are computed sequentially following a topological ordering. The key idea is to have motifs at different hierarchy levels, with lower-level motifs serving as building blocks for higher-level motifs. A hierarchical architecture representation is defined by a hierarchy of levels containing motifs, with the lowest level being primitive operations. The assembly process involves network structures of motifs at all levels and a set of bottom-level primitives. The bottom level includes six primitives such as 1x1 convolution with fixed channel number C, followed by batch normalization and ReLU activation. Evolutionary search over neural network architectures can be performed by treating the representations of hierarchical genotypes as genotypes. An action space for mutating hierarchical genotypes is introduced, along with a diversification-based scheme to obtain the initial population. Tournament selection and random search methods are described for the evolutionary search process. The text describes the process of mutating hierarchical genotypes in evolutionary search. It involves sampling target levels and motifs, replacing operations, and altering existing edges. The population of genotypes is initialized by creating a \"trivial\" genotype and diversifying it. The text discusses diversifying genotypes in evolutionary search by applying random mutations to create non-trivial architectures. The evolutionary search algorithm uses tournament selection to refine the population over time. The evolution algorithm refines the population by training models from scratch for a fixed number of iterations. A tournament selection process is used to select genotypes with the highest fitness, with selection pressure controlled by tournament size. The population grows over time to maintain architecture diversity. The evolutionary method BID16 was explored as an alternative to random search. Random search involves generating a population of genotypes randomly, computing fitness for each genotype, and selecting the one with the highest fitness. This method can run in parallel, reducing search time. The distributed implementation includes a controller for evolution and workers for evaluation, sharing a memory for genotypes and fitness data queue. The controller performs tournament selection of genotypes from M, mutates them, and inserts them into Q for evaluation. Workers pick up genotypes from Q, train architectures, and record validation accuracy in M. No synchronization is needed during architecture evolution, and all workers are fully occupied. The search framework is used to learn the architecture of a convolutional cell quickly. The architecture search is conducted on the CIFAR-10 training set, split into 40K training and 10K validation images. Candidate models are trained on the training subset and evaluated on the validation subset to determine fitness. The selected cell is then integrated into a larger model trained on the combined subsets for final accuracy evaluation on the CIFAR-10 test set. The test set is only used for model evaluation, not selection. Additionally, cells learned on CIFAR-10 are also evaluated in a large-scale setting. The final model evaluation includes using a model with 3x3 convolutions and learned convolutional cells on CIFAR-10 and ImageNet challenge dataset. The model consists of separable convolutions to control channel numbers and reduce spatial resolution, followed by global average pooling and a linear softmax layer. The fitness computation is done using a smaller model trained with specific parameters and optimization techniques. The evaluation of the learned cell architecture on CIFAR-10 involves using a larger model with specific parameters and optimization techniques. The model is trained for 80K steps, starting with a learning rate of 0.1, which is reduced by 10x at various intervals. Variance due to optimization is considered important for fair comparison and model assessment. For evaluation on the ILSVRC ImageNet challenge dataset BID18, a similar architecture to CIFAR is used with specific changes. The model is trained for 200K steps with SGD momentum, starting with a learning rate of 0.1 and reduced by 10x at different intervals. The batch size is 1024, weight decay is 10^-4, and no auxiliary losses or label smoothing are used. The training approach did not utilize certain techniques proven effective in BID31, such as auxiliary losses, weight averaging, label smoothing, or path dropout. Training augmentation included random crops, horizontal flips, and brightness/contrast changes. The study compared fitness and parameters in flat and hierarchical representations over 7000 evolution steps using 200 GPU workers. The study compared fitness and parameters in flat and hierarchical representations over 7000 evolution steps using 200 GPU workers. The evolution process is visualized in Fig. 3, showing the fitness growth and best fitness observed at each step. The study compared fitness and parameters in flat and hierarchical representations over 7000 evolution steps using 200 GPU workers. The number of parameters in the small network was shown in Fig. 3, with flat genotypes achieving higher fitness but at the cost of larger parameter count. A parameter-constrained variant of the flat genotype was also considered, where only genotypes with parameters under a fixed threshold were permitted. Hierarchical and flat genotypes achieved similar fitness in this setting. The improvement in fitness of the hierarchical architecture was correlated with the accuracy improvement of the corresponding large model trained till convergence, as shown in Fig. 4. Accuracy improvement during evolution is measured with respect to the initial random genotype. The small model is used for fitness computation, while the large model deploys the evolved cell architecture for training and evaluation. Architecture search time is significantly reduced, taking 1 hour for random search over 200 architectures and 1.5 days for evolutionary search with 7000 steps. This is faster than previous reports using fewer GPUs. Classification results on CIFAR-10 and ILSVRC sets are presented for architectures found using various representations and search methods. The evaluation of architectures found using random and evolutionary search on CIFAR-10 and ImageNet shows that randomly sampled architectures perform well due to the representation power of architecture spaces. Random search achieves competitive results on both datasets in just 1 hour. Evolution over hierarchical representations yields the best results with a classification error of 3.75% on CIFAR-10, further improved to 3.63% with more channels. On the ImageNet validation set, the model achieves 20.3% top-1 classification error and 5.2% top-5 error. It outperforms other methods on CIFAR-10 and ImageNet, with a model size comparable to Inception-ResNet-v2 but larger than NASNet-A. The best classification error achieved is 3.63% with more channels. Evolutionary search with hierarchical representation achieved a classification error of 20.3% on the ImageNet validation set. The best-performing architecture found using the proposed framework outperformed other methods on CIFAR-10 and ImageNet. The hierarchical cell architecture search discovered skip connections between input and output nodes, utilizing a novel evolutionary method with smaller operations as building blocks. Strong results were achieved using simplistic search algorithms, leading to state-of-the-art performance on A ARCHITECTURE. The best architecture for hierarchical cell visualization uses motifs 1, 3, 4, and 5, with motifs 3 and 5 being dominant in the construction of the cell."
}