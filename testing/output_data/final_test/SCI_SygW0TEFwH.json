{
    "title": "SygW0TEFwH",
    "content": "The novel black-box adversarial attack algorithm presented achieves state-of-the-art model evasion rates with high query efficiency under $\\ell_\\infty$ and $\\ell_2$ metrics. It utilizes a sign-based gradient estimation approach, leading to smaller memory usage and eliminating the need for hyperparameter tuning or dimensionality reduction. The algorithm's theoretical performance is guaranteed and outperforms all submitted attacks on two public black-box challenges and a model trained against transfer attacks. The algorithm achieves high evasion rates against adversarial attacks, outperforming all submitted attacks. It is $3.8\\times$ less failure-prone and requires $2.5\\times$ fewer queries compared to the best combination of state-of-the-art algorithms. It can evade a standard MNIST model with just 12 queries on average and an IMAGENET model with 579 queries. Adversarial examples exploit vulnerabilities in Deep Neural Networks, with research initially focusing on white-box attacks. In this paper, the focus is on studying attack algorithms under a black-box threat model, which assumes access to predictions instead of gradients. The complexity of estimating gradients is being reduced by focusing on estimating just the signs of the gradients. The goal is to recover the sign of the gradient with high query efficiency for generating effective adversarial examples. Related work is organized into Adversarial Example Generation and Sign-Based Optimization themes, focusing on white-box and black-box settings. The Fast Gradient Sign Method (FGSM) is a key method in producing adversarial examples. Several methods, including FGSM, have been used to produce adversarial examples for various learning tasks. In the blackbox setting, Narodytska and Kasiviswanathan use a naive policy of perturbing random image segments. Bhagoji reduces feature space dimensions before estimating gradients, while Chen introduces a principled approach using gradient-based optimization. Our work contrasts with previous methods by focusing on estimating the sign of the gradient to efficiently generate adversarial examples. Unlike other approaches that use random sampling, our method employs adaptive construction of queries to the model. Our method focuses on estimating the sign of the gradient to efficiently generate adversarial examples, using adaptive construction of queries to the model. Other approaches involve learning adversarial examples on a different model, which is expensive and not directly comparable to our setup. In the context of general-purpose continuous optimization methods, sign-based stochastic gradient descent was studied in both zeroth- and first-order setups. SignSGD was shown to have faster empirical convergence than SGD and reduced communication costs across multiple workers. ZO-SignSGD algorithm outperformed NES on MNIST using the sign of the gradient for better convergence. This approach intersects adversarial machine learning and black-box optimization by estimating and utilizing the sign of the gradient. SignHunter is a novel algorithm that exploits the separability property of the directional derivative of the loss function to estimate gradient sign bits. It offers a worst-case theoretical guarantee on the number of queries required to outperform FGSM in black-box attacks. The approach is evaluated on a wide range of datasets and threat models, demonstrating superior performance compared to previous works in the field. SignHunter's adaptive search for the gradient sign allows it to craft adversarial examples efficiently, outperforming FGSM and other black-box attacks. A software framework is released to benchmark these attacks on various models. SignHunter characterizes adversarial cones and highlights the gradient masking effect in a black-box setup. The black-box adversarial attacks involve generating a perturbation vector to modify the input x to deceive the network's prediction while staying close to the original input. Gradient-based techniques are commonly used to craft adversarial examples by estimating the gradient through querying an oracle for the network's loss value. The basic approach for black-box adversarial attacks involves querying an oracle to estimate the network's loss value and approximate the gradient using the finite difference method. Recent works aim to reduce query complexity by utilizing data-dependent priors, but fail to adaptively construct queries based on past responses for more efficient gradient recovery. The goal is to efficiently estimate the gradient sign bits of a loss function through adaptive queries, maximizing directional derivatives from limited function value queries. This approach aims to improve gradient recovery efficiency in black-box adversarial attacks. The goal is to efficiently estimate gradient sign bits of a loss function through adaptive queries, maximizing directional derivatives in black-box adversarial attacks. The sign bit vector is encoded in {-1, +1} rather than {0, 1}, following a non-standard definition for DNN gradients. The maximization is based on DqL(x, y) = q T g * , where q = sign(g * ). The directional derivative of the loss function at an input/label pair (x, y) in the direction of a binary code q is separable. SignHunter algorithm maximizes a black-box function over the binary hypercube. SignHunter algorithm reformulates the gradient sign estimation problem into n 1-dimensional binary black-box optimization problems, reducing the search space of sign bits. By using n + 2 queries, one can recover the gradient sign bits efficiently. However, this method still faces the O(n) complexity of full gradient estimation methods. The SignHunter algorithm reformulates the gradient sign estimation problem into n 1-dimensional binary black-box optimization problems, reducing the search space of sign bits. By using n + 2 queries, one can efficiently recover the gradient sign bits. The query can recover at most one sign bit, but by flipping all bits at once, it is possible to recover more sign bits per query. With three queries, max(r, n \u2212 r) sign bits can be recovered using a flip/revert procedure based on a majority voting mechanism. The SignHunter algorithm reduces the search space of sign bits by formulating the gradient sign estimation problem into n 1-dimensional binary black-box optimization problems. By using n + 2 queries, one can efficiently recover the gradient sign bits. The query can recover at most one sign bit, but flipping all bits at once allows for more sign bits to be recovered per query. With three queries, max(r, n \u2212 r) sign bits can be recovered using a flip/revert procedure based on a majority voting mechanism. Additionally, iterative procedures on subsets of coordinates in the guess vector q can recover sign bits based on the length and correct signs of the chunk. The SignHunter algorithm aims to recover as many sign bits as possible with as few queries as possible, focusing on coordinates with large gradient magnitudes. It uses a divide-and-conquer search approach, starting with an initial guess of the sign vector and flipping the sign of coordinates to optimize the search process. This technique efficiently recovers gradient sign bits with a limited number of queries. SignHunter is an algorithm that aims to recover sign bits efficiently by flipping signs in coordinates with large gradient magnitudes. It uses a divide-and-conquer search approach, requiring 2 log(n)+1 \u2212 1 sign flips in an n-dimensional search space. If the query budget is not exhausted, the algorithm can update the sign vector and restart the procedure. Starting with a sign vector close to the optimal one reduces the number of queries needed. SignHunter is guaranteed to perform at least as well as FGSM with O(n) oracle queries. Theorem 1 shows the optimality of SignHunter with 2 log(n)+1 queries, crafting adversarial examples effectively. The proof is in Appendix B, providing an upper bound on query requirements. In practice, SignHunter requires fewer queries for convergence to an adversarially helpful sign vector. SignHunter guarantees fast convergence to an adversarially helpful sign vector. It uses the best sign estimation obtained so far, similar to FGSM, but with a more efficient gradient sign estimation routine. SignHunter is suitable for parallel hardware and has a smaller memory footprint, making it efficient for batch attacks. The algorithm for crafting black-box adversarial attacks with SignHunter is outlined in Algorithm 2. SignHunter is compared with other algorithms for crafting untargeted black-box adversarial examples. A variant named Rand is introduced for adaptive query construction. Different threat models are considered on various datasets. Experiments setup details and resources can be found at https://bit.ly/3acIHoQ. The experiment setup for SignHunter involves giving each attacker a budget of 10,000 oracle queries per attack attempt and evaluating on 1000 images from MNIST, CIFAR10, and IMAGENET. Perturbation bounds were not standardized, and results were based on standard models without adversarial hardening. The observed bound from Cohen et al. (2019) was used for CIFAR10. SignHunter does not have hyperparameters, but hyperparameters of other algorithms were tuned starting from default values. SignHunter's finite difference probe \u03b4 is set to the perturbation bound for computing finite difference and crafting adversarial examples. Results show trade-off between evasion rate and queries needed for adversarial examples on MNIST, CIFAR10, and IMAGENET classifiers under perturbation constraints. Table 1 summarizes the comparison between SignHunter and Bandits TD in terms of query efficiency for achieving a desired success rate. SignHunter outperforms previous approaches in most settings, except for the IMAGENET 2 setup where Bandits TD shows better efficiency. Despite searching over more dimensions, SignHunter is remarkably efficient in achieving 100% evasion with just 12 queries per image in the \u221e setup against the MNIST classifier. In the 2 setup, SignHunter's performance degrades but still outperforms other algorithms. In the 2 setup, SignHunter perturbs all coordinates with the same magnitude, giving it fewer degrees of freedom compared to other algorithms. Despite this limitation, SignHunter maintains its effectiveness, outperforming other algorithms. SignHunter maintains its effectiveness in the 2 setup, outperforming other algorithms like FGSM across all datasets. For example, SignHunter achieves a lower failure rate compared to FGSM on CIFAR10 2 and IMAGENET with a significantly lower query budget. The iterative framework of perturbing data points supports the observation that SignHunter is stronger than FGSM. SignHunter converges faster and optimizes both Hamming and cosine similarity metrics compared to Bandits TD. SignHunter's gradient sign estimation is most obvious in the IMAGENET 2 setup. Once an attack is successful, the estimated gradient sign is used for the rest of the plot. In the \u221e settings, SignHunter's plot does not improve compared to its 2 counterpart, as attacks are successful early on. The gradient direction may be very local and not capture the global loss landscape. SignHunter's adaptive query construction is highlighted as effective compared to Rand's poor performance. SignHunter's adaptive query construction is effective, outperforming Rand in most scenarios. It is 3.8\u00d7 less failure-prone than other approaches and requires 2.5\u00d7 fewer queries overall. SignHunter was evaluated against adversarial training, showing promising results in public challenges for MNIST and CIFAR10, and using ensemble adversarial training for IMAGENET. SignHunter's attacks in the Public MNIST Black-Box Attack Challenge achieved the lowest model accuracy of 91.47%, surpassing all other submitted attacks. The average number of queries per successful attack was 233. The SignHunter's attacks in the Public MNIST Black-Box Attack Challenge achieved the lowest model accuracy of 91.47%, with an average of 233 queries per successful attack. The Gradient-Aligned Adversarial Subspace (GAAS) method provides an approximation of the adversarial cone dimensionality by finding orthogonal perturbations that are adversarial to the model. Linearizing the model's loss function reduces this to finding orthogonal vectors aligned with its gradient. Aligning vectors with SignHunter's estimation (SAAS) instead of the gradient (GAAS) gives a better approximation of the adversarial cone. SignHunter's query-efficient finite-difference sign estimation captures larger-scale variations in the loss landscape, providing a better approximation of the adversarial cone compared to aligning with the gradient. GAAS finds fewer adversarial directions for the v3 adv-ens4 model than the naturally trained v3 model, while SAAS reports similar probabilities for both. SAAS and GAAS exhibit contrasting results in generating adversarial examples for neural nets. SAAS finds more orthogonal adversarial vectors than GAAS, providing a better understanding of the space of adversarial examples without white-box access to the models. SignHunter is a binary black-box optimization algorithm that maximizes directional derivatives to craft adversarial examples efficiently. It outperforms FGSM with a guarantee after O(n) queries and is robust to gradient masking. The algorithm constructs adaptive queries and achieves the highest evasion rate on two datasets. SignHunter is a binary black-box optimization algorithm that maximizes directional derivatives to craft adversarial examples efficiently. It achieves the highest evasion rate on two public black-box attack challenges and breaks a model that argues robustness against substitute-model attacks. The performance of the noisy FGSM on standard models on MNIST, CIFAR10, and IMAGENET datasets is shown, considering the \u221e threat perturbation constraint and the top-k setup. The performance of SignHunter, a binary black-box optimization algorithm, is validated on MNIST, CIFAR10, and IMAGENET datasets with perturbation constraints. The proof of Theorem 1 regarding the optimality of SignHunter is presented, assuming the finite difference approximates the directional derivative well. SignHunter is proven to be as effective as FGSM in crafting adversarial examples by recovering the gradient sign vector through binary codes and queries. This is based on the separability property of the directional derivative, showing SignHunter's effectiveness after 2 log(n)+1 queries. The experiments setup involved tuning hyperparameters for fair comparison among algorithms using a synthetic concave loss function. Results showed ZO-SignSGD converges faster than NES, and BanditsTD outperformed others. Adversarial examples generation experiments revealed failure rate and query efficiency aligned with expectations. In experiments on adversarial examples generation, SignHunter showed a tuning-free setup, offering an edge over other black-box attacks that require parameter tuning. Tables and figures in the paper detail the experimental setup, model sources, and algorithm hyperparameters. SignHunter's superior performance in generating adversarial examples without parameter tuning is attributed to the well-behaved synthetic loss function and the structure of the perturbation region. The experiments were conducted on a CUDA-enabled NVIDIA Tesla V100 16GB, with hyperparameters outlined in Tables 4, 5, 6, and 7. Figure 6 illustrates the algorithms' performance on the concave loss function after hyperparameter tuning. SignHunter's performance in crafting adversarial black-box examples outperforms both NES and ZO-SignSGD, with fast convergence similar to ZO-SignSGD. Results of experiments are shown in tables and figures, including attacks effectiveness on MNIST and IMAGENET under perturbation constraints. Table 10 summarizes the effectiveness of attacks on IMAGENET under different constraints, including failure rate, average number of queries, Hamming similarity, cosine similarity, success rate, and average number of queries per successful image. Table 10 summarizes the effectiveness of attacks on IMAGENET under different constraints, including failure rate, average number of queries, Hamming similarity, cosine similarity, success rate, and average number of queries per successful image. The more effective the attack, the closer its curve is to the bottom right of the plot. Hamming Similarity and Avg. Cosine Similarity rows show the similarity metrics between estimated and true gradients. Success Rate row reports cumulative distribution functions for the number of queries required for a successful attack. Average number of queries used per successful image is also reported. Performance curves of attacks on IMAGENET are shown for different scenarios. Performance curves of attacks on IMAGENET for different perturbation constraints are shown in Figure 9. The plots display average loss, Hamming similarity, cosine similarity, success rate, and average number of queries per successful image. The effectiveness of each attack is indicated by how close its curve is to the bottom right of the plot. The experiments show results of crafting adversarial black-box examples on adversarially trained models, with model accuracies ranging from 91.47% to 96.81%. Different attack methods were used, including SignHunter and PGD, with varying success rates. SignHunter network achieved 47.16% PGD on cross-entropy loss for adversarially trained public network, 64.38% FGSM on CW loss for the same network, and 67.25% FGSM on CW loss for naturally trained public network. Top 1 Error Percentage is 85.23%. Performance curves of attacks on black-box challenges for MNIST, CIFAR10, and IMAGENET are also discussed. The experiment analyzed the effectiveness of attacks on different datasets by measuring the success rate, average number of queries, and cosine similarity. The distribution of gradient magnitudes was also examined in the study. The experiment analyzed the effectiveness of attacks on different datasets by measuring the success rate, average number of queries, and cosine similarity. SignHunter's adaptive flips show a clear advantage over other schemes despite having a worse upper-bound on the query complexity. The authors discuss recent work related to their proposition, including Parsimonious Black-Box Adversarial Attacks. The proposed algorithm SignHunter outperforms previous attacks with a 52.84% success rate and 569 average queries. It also achieves a 98% success rate with 578.56 average queries, surpassing the performance of SIMBA. SignHunter uses binary flips for a group of coordinates, showing better performance compared to other methods. In experiments by Guo et al. (2019), SIMBA and its variant SIMBA-DCT show comparable performance to SignHunter in attacking the IMAGENET v3 model. However, SIMBA's performance drops significantly in the \u221e setup. Harmonica and SignHunter optimize a black-box function over the binary hypercube {\u00b11} n with different assumptions on the objective function. SignHunter assumes separable objective function, optimizing with O(n) queries. If assumption not met, O(mn) complexity with restarts. Empirical comparison shows SignHunter outperforms Harmonica with 8\u00d7 fewer queries and computational advantage. SignHunter performs similarly in different perturbation setups, while other algorithms like NES and BanditsTD show decreased performance in certain setups. To address this, SignHunter could potentially benefit from ternary sign flips in future work. Figure 15 shows the performance of black-box attacks in different perturbation constraints, indicating the average number of queries used per successful image for each attack. Note that this setup is similar to the one examined in Section 4."
}