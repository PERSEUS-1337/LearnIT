{
    "title": "rkecJ6VFvr",
    "content": "The 2-simplicial Transformer extends the Transformer model with higher-dimensional attention for logical reasoning in deep reinforcement learning. The Transformer model introduces a vector space V for value vectors and an inner product space H for query and key vectors, supporting message passing between entities. This relational inductive bias perceives relations based on the dot product of query and key vectors. The real world has structure beyond direct relationships between entities. The 2-simplicial Transformer generalizes the Transformer architecture by incorporating 2-and 3-way interactions between entities using algebras, specifically the Clifford algebra Cl(H) of the space H of queries and keys. This allows for the representation of higher-order relationships essential for extracting predictive power from data across various domains. The 2-simplicial Transformer utilizes the Clifford algebra Cl(H) to represent entities and their interactions in a higher-order structure, enabling the network to learn how to transform entities into queries and keys. This allows for the extraction of predictive power from data across different domains, as demonstrated in a reinforcement learning example in the BoxWorld environment. In bridge BoxWorld, the agent must use two keys simultaneously to obtain the Gem, creating 3-way relationships between entities. The deep reinforcement learning agent architecture follows a 2-simplicial Transformer block instead of a standard Transformer block. The experiments show that the simplicial agent has an advantage over the relational agent in reasoning tasks. The use of tensor products of value vectors is inspired by linear logic in vector spaces and has been utilized in various natural language processing models. The main novelty in the model is the introduction of 2-simplicial attention into the Transformer architecture, allowing logical reasoning to be incorporated. The Transformer architecture includes a word embedding layer, encoder, and decoder, while the Transformer block is a repeated sub-model of the encoder. The idea of propagating information between nodes using weights from statistical mechanics via the Hopfield network is fundamental. The 2-simplicial Transformer blocks operate on entity representations in vector spaces V and H. Additional algebraic structures on H, such as the \"multiplication\" tensor and Clifford algebra, are used in the 2-simplicial attention mechanism. The standard Transformer block generates vectors from entities via a learned linear process. The standard Transformer block generates query, key, and value vectors for each entity through a learned linear transformation. These vectors are used to compute a refined value vector for each entity, resulting in a new entity representation. Multiple-head attention allows for the propagation of information along different channels. In multiple-head attention with K heads, information is propagated between entities along K channels of dimension dim(H)/K. The attention output is computed by summing the value vectors along these channels. This form of attention is referred to as 1-simplicial attention, where entities are updated based on pairs of entities using tensor products of value vectors. The scalar triple product is used in multiple-head attention to update entities based on pairs of entities using tensor products of value vectors. It has a geometric interpretation in terms of the volume of a tetrahedron in three dimensions. The scalar triple product, represented by vectors a, b, c, can be understood in terms of vector norms and configurations on the 2-sphere. The cross product is used to calculate volumes of tetrahedrons in three dimensions. In high-dimensional spaces, the Clifford algebra of H is the natural framework for the triple scalar product. The minimum value of the scalar triple product is zero when vectors a, b, c are pairwise orthogonal. The scalar triple product of vectors a, b, c can be zero when they are pairwise orthogonal. Using 2-simplicial attention, entities update based on the degree of attention. Multiple-head 2-simplicial attention is computed similarly to 1-simplicial attention. Conjunction is modeled by multiplication in an algebraic structure. In 2-simplicial attention, separate inner product spaces are used for each simplicial dimension. The update rule for each head in each dimension is modified based on the number of heads. Training stability is achieved with an additional layer normalization on the output of 2-simplicial attention. The time complexity of 1-simplicial attention is O(N^2) while for 2-simplicial attention it is O(N^3). The 2-simplicial attention introduces virtual entities to serve as a \"scratch pad\" for iterated ordinary attention, with a time complexity of O(N^3) due to calculating attention for every triple of entities. The update rule for virtual entities is based on the range N < j, k \u2264 N + M, ensuring only value vectors from virtual entities are propagated. The 2-simplicial attention introduces virtual entities for iterated ordinary attention, with a complexity of O(N^3). The update rule for virtual entities is similar to memory nodes in other architectures. The method for updating virtual entities has complexity O(N M^2), with M being of order \u221aN for desired complexity O(N^2). The environment in reinforcement learning is a variant of the BoxWorld environment. The BoxWorld environment is a rectangular grid with the player, locked boxes, and keys. The player can collect keys, open locked boxes with matching key and lock colors, and must attain a white key to win. The BoxWorld environment involves the player collecting keys to open locked boxes and ultimately attaining a white key known as the Gem. The player must navigate through the episode by choosing the correct keys to avoid distractors and reach the Gem for a reward. In the BoxWorld environment, two keys are now required to obtain the Gem, with two loose keys on the board. The player must step on lock tiles with both keys to end the episode with a +10 reward. Graphically, Gems with multiple locks are denoted with two vertical white tiles on the left and two lock tiles on the right. Multiple locks generate paths leading to each lock with no overlapping colors, and distractor boxes are replaced by a new type called a bridge. In Bridge BoxWorld, a new type of distractor called a bridge is introduced, which is a locked box with a lock color from one solution branch and a key color from the other branch. Opening the bridge makes the puzzle unsolvable, and an episode ends when the player either obtains the Gem (+10 reward) or opens the bridge (-1 reward). Standard BoxWorld is solved using relational reasoning, while Bridge BoxWorld emphasizes reasoning about 3-way relationships. The Bridge BoxWorld introduces a new type of distractor called a bridge, which is a locked box with a lock color from one branch and a key color from the other. The solution graphs in this scenario involve 2-simplex motifs, where identifying the bridge is crucial. The baseline relational agent is similar to previous work but with improved layer normalizations. The network architecture and code for both agents are available online. The network architecture of the relational and simplicial agent is described, with differences noted between the two models. The input is an RGB image passed through convolutional layers with ReLU activation functions. The output is a tensor with positional encoding, reshaped into Transformer entities. The network architecture of the relational and simplicial agent involves passing an RGB image through convolutional layers with ReLU activation functions. The output is a tensor of Transformer entities, including entity representations and virtual entities, which are then processed through two iterations of the Transformer block with feedforward layers and ReLU activation. The Transformer block in the network architecture involves passing the input tensor through 2-simplicial attention and concatenating the result with the output of 1-simplicial attention before going through feedforward layers. The pseudo-code for the ordinary and 2-simplicial Transformer blocks is provided. Our implementation of the standard Transformer block in the network architecture involves passing the input tensor through 2-simplicial attention, concatenating the result with the output of 1-simplicial attention, and then going through feedforward layers. The spaces of entity representations have dimensions 64, with two heads of 1-simplicial attention in both the relational and simplicial agent. In the simplicial agent, there is a single head of 2-simplicial attention with dimension 48 and two virtual entities. The output of the Transformer blocks is a tensor of shape [N, 64], and max-pooling is applied over the entity dimension to compute a vector v \u2208 R 64. The input tensor is processed through 2-simplicial attention, concatenated with 1-simplicial attention output, and passed through feedforward layers. Entity representations have dimensions of 64, with two heads of 1-simplicial attention. In the Transformer block, a single head of 2-simplicial attention with dimension 48 is used. The final fully-connected layer produces logits for actions and a value function. Layer normalization is applied before the linear transformation for better results. The implementation of the relational model in our experiments uses the distributed off-policy actor-critic architecture IMPALA with RMSProp optimization algorithm. Hyperparameters for IMPALA and RMSProp are provided in Table 1 of Appendix E. The implementation solves the BoxWorld environment with specific parameters. After training for 2.35 \u00d7 10 9 timesteps, the implementation solved over 93% of puzzles on a 9 \u00d7 9 grid. The relational and simplicial agent was then trained on bridge BoxWorld with specific conditions. The bridge in the BoxWorld environment was uniformly sampled with no restrictions, and the lock color was always on the top solution path. Four independent trials of both agents were trained to 5.5 \u00d7 10 9 timesteps or convergence. The simplicial agent showed a clear advantage over the relational agent in performance comparisons. Training runs and analysis of the agents are provided in the appendices. The experiments reported in the study compared the performance of a 2-simplicial Transformer with a standard Transformer. Results showed that the 2-simplicial model outperformed the standard model, even when adding more depth. The analysis revealed that the 2-simplicial agent learned to use 2-simplicial attention effectively. The Transformer blocks process entities on a game board of size R \u00d7 C, with an extra column for inventory. There is a strong relation between game board tiles and Transformer entities, with positional encoding used to pass grid information. Attention distributions for a simplicial agent are displayed in Figure 7, showing ordinary attention of virtual entities in the first iteration of the simplicial Transformer block. In the first iteration of the simplicial Transformer block, virtual entities exhibit strong attention towards specific locks and keys. The 2-simplicial attention in the second iteration shows non-generic focus on certain entities like top and bottom locks on the Gem, the player, and specific inventory-associated entities. The 2-simplicial Transformer block shows strong attention towards specific entities like top and bottom locks on the Gem, the player, and inventory-associated entities. This suggests that the agent has learned to use 2-simplices in a meaningful way, indicating a bias towards learning representations of higher-order relations between entities. The 2-simplicial Transformer is an inductive bias for 3-way interactions between entities, showing better performance in the BoxWorld environment. This bias involves predicting 3-way interactions and can be useful for problems with higher-order relations. Inspired by the history of logic and algebra, the design incorporates Boole's idea of modeling relationships with multiplication in an algebra using Clifford algebras in deep learning. The Transformer model and its variants, like the Universal Transformer, are seen as versatile units for computing with learned representations, akin to the Neural Turing Machine and Differentiable Neural Computer. The Transformer can be viewed as a block of parallel RNNs updating their hidden states by attending to each other's states. This connection is further explained in the context of the 2-simplicial Transformer and its relation to earlier work in NLP literature. The Neural Turing Machine (NTM) consists of an RNN controller with weight matrices W, U, b, input symbol x, hidden states h, memory slots M, and query q. The NTM differs from the Transformer by having a separate recurrent controller and asymmetric roles between the controller's hidden state and memory slots. The Transformer, on the other hand, removes this asymmetry by sharing transition functions between layers. The Transformer removes the asymmetry in the NTM by updating every memory slot with a feedforward network in each timestep. The entity representations in the Transformer can be viewed as hidden states of parallel RNNs. The key and value vectors replace the double role of M in the NTM, with cosine similarity being replaced by the dot product. The connection between the Transformer and RNNs is made, noting similarities with second-order RNNs and multiplicative RNNs in the update rule. The Transformer block refines the Hopfield network by replacing entity vectors with query, key, and value vectors, and the nonlinearity with a feedforward network. The Transformer architecture refines the Hopfield network by using query, key, and value vectors instead of entity vectors, and replacing the nonlinearity with a feedforward network. The dynamics are stabilized by layer normalization, and the initial representations incorporate information about the underlying lattice through positional embeddings. The architecture inherits from both the logical tradition of denotational semantics and the statistical mechanics tradition via Hopfield networks. The volume of k-simplices for k < dim(H) is used to define coefficients for simplicial attention. The Clifford algebra Cl(H) is generated by vectors in an inner product space H, with a Z-grading defined by an orthonormal basis. The Clifford algebra Cl(H) has a Z-grading determined by the degree of elements. Reversion is an operation in the algebra, allowing the definition of magnitude for any element. The dimension of H is denoted by n, and coefficients for simplicial attention are defined using the volume of k-simplices. The Clifford algebra Cl(H) has a Z-grading determined by the degree of elements. Reversion is an operation in the algebra, allowing the definition of magnitude for any element. The dimension of H is denoted by n, and coefficients for simplicial attention are defined using the volume of k-simplices. The wedge product of k-vectors in H can be thought of as an oriented k-simplex, and the magnitude of this wedge product in the Clifford algebra computes the volume. The volume of a k-simplex in H with vertices 0, v1, ..., vk is computed using the wedge product in the Clifford algebra. For k = 2, the unsigned scalar product is the absolute value of the dot product. For k = 3, the formula involves the angles between vectors a, b, and c. In the three-dimensional case, the volume is the absolute value of the determinant. If the vi are all pairwise orthogonal, the geometric content of the lemma is clear. The experiments in the original BoxWorld paper (Zambaldi et al., 2019) had a timestep cap of 120 per episode, but our experiments were run without a cap. This choice impacted our reported sample complexities, aiming to reduce generalization error in deep reinforcement learning architectures for similar environments. The experiments in the original BoxWorld paper had a timestep cap of 120 per episode, but our experiments were run without a cap to reduce generalization error in deep reinforcement learning architectures for similar environments. The learning curriculum undermines the goal of generalization, as it makes expectations conditional on a suitable curriculum, which may not always be clear in advance. The absence of an episode horizon allows for better generalization to similar puzzles. The relational agent performs well without an episode horizon in the original BoxWorld. The simplicial agent has slower gradient descent steps compared to the relational agent. The training throughput of the relational agent is higher than that of the simplicial agent. The primary performance difference lies in the time taken to compute gradients. Increasing GPU memory and IMPALA workers reduces the performance gap. Time-adjusted performance of the simplicial agent is shown in Figure 8. The simplicial agent's graph is scaled differently from the relational agent's. Running 2-simplicial attention in parallel with ordinary attention could reduce performance gap. Detecting bridges without memory is unreliable due to key collection. Experiments involve few virtual entities and Transformer block iterations. The simplicial Transformer block was tested with minimal configuration, without multiple heads of 2-simplicial attention. The training runs for relational and simplicial agents are provided, with details on the attention of the trained simplicial agent. The roles of virtual entities and heads vary across trained simplicial agents. In the context of the best simplicial agent, standard entities are indexed from 0 to 39, and virtual entities are indexed as 40 and 41. The first 1-simplicial head focuses on inventory information, with attention shifting to specific entities based on key acquisitions. The second 1-simplicial head exhibits color-based attention patterns. The video of a typical example showing 1-simplicial attention in the first Transformer block is available online. The analysis focuses on 2-simplicial attention in the second iteration of the Transformer block, organizing episodes of bridge BoxWorld by their puzzle type. In the analysis of bridge BoxWorld episodes, attention is visualized in 2-simplicial form in the second Transformer block. Entities and their associations are highlighted in relation to the gem, inventory, and player. The attention patterns are shown in Figures 7, 12, and 13. In the second Transformer block, attention distributions for virtual entities are visualized in 2-simplicial form. Initial representations of virtual entities are denoted as f1 = e40 and f2 = e41. The representations are updated in the first iteration using attention coefficients. The vector propagated in the second iteration includes terms related to 2-simplicial attention. In the second Transformer block, attention distributions for virtual entities are visualized in 2-simplicial form. The output of the 2-simplicial head with target i = 25 is approximately a conjunction of entity 1 and entity 0, followed by a disjunction. An additional layer normalization is applied before passing through the system. The 2-simplicial head output for entity 25 is crucial for the performance differences between simplicial and relational agents. Training curves show a common plateau at a win rate of 0.85, with both agents excelling at solving (1, 1, 2) puzzles. The per-puzzle win rates for simplicial and relational agents are compared using the RMSProp optimization algorithm. RMSProp is a mini-batch version of Rprop, updating weights using only the sign of the gradient. The RMSProp algorithm is a refinement of Rprop, where weights are updated by the same absolute amount \u03ba in each step, with only the sign of the update varying. The algorithm introduces a decay rate p, with Rprop being the p \u2192 0 limit of RMSprop. There is a trend in the literature towards using RMSprop with large values of the hyperparameter \u03b5, with examples of \u03b5 = 0.1 being used. This \"large \u03b5 RMSProp\" originated in previous studies. The RMSProp algorithm is a refinement of Rprop, with weights updated by a fixed amount \u03ba. RMSprop introduces a decay rate p, with Rprop being the p \u2192 0 limit. There is a trend towards using RMSprop with large \u03b5 values, leading to \"large \u03b5 RMSProp\" in previous studies. In large \u03b5 RMSprop, the gradient size is scaled by a sigmoid function, affecting updates based on gradient magnitude. Logical reasoning is complex to define and identify in agents like animals or deep reinforcement learning agents. Aristotle viewed logic as studying general patterns to distinguish valid and invalid philosophical arguments, aiming to produce strategies for winning argumentation games. Logic involves two players asserting and refuting propositions, with an observer learning predictive patterns for game outcomes. In a series of games, a player follows an explicit strategy distilled from general patterns, leading to consistent wins. Recognizing logical reasoning in behavior is challenging due to implicit strategies and difficulty in identifying domain-specific rules. Argumentation games theory in mathematical logic explores mathematical proof as strategy in game semantics. In a reinforcement learning problem, one player asserts a proposition G and the other player interrogates this assertion. The goal is to synthesize a proof of G from a set of hypotheses through a series of actions, with positive or negative rewards based on success. A deep reinforcement learning agent with a policy network is used for this task. The goal of Aristotle's observer in a reinforcement learning problem is to predict the actions that lead to success or failure in constructing a proof. The observer's training objective is to learn from data to identify a strategy for proving propositions, with the agent, environment, and optimization process playing key roles. The deep reinforcement learning agent's logical reasoning is questioned, with a focus on identifying if its behavior is governed by logical reasoning. The use of \"logic probes\" is proposed to automate this process, similar to recent developments in neural network probes. The BoxWorld environment emphasizes planning and reasoning components in an agent's policy. The BoxWorld environment stresses planning and reasoning in an agent's policy. The logical structure of BoxWorld is crucial, with each episode containing basic facts about obtainability represented by axioms. Linear logic is used to capture the relationships between keys and boxes in the environment. The logical structure of BoxWorld episodes is represented by axioms in linear logic, where strategies for obtaining the Gem are equivalent to proofs in intuitionistic linear logic. In bridge BoxWorld, the conjunction of linear logic captures the semantics of requiring two keys to obtain the Gem. The axioms in bridge BoxWorld episodes contain formulas representing the colors of the keys needed for the Gem, with strategies being equivalent to proofs of these axioms. The logical structure of BoxWorld episodes is represented by axioms in linear logic, where strategies for obtaining the Gem are equivalent to proofs in intuitionistic linear logic. In bridge BoxWorld, the conjunction of linear logic captures the semantics of requiring two keys to obtain the Gem. The correspondence between agent behavior in bridge BoxWorld and proofs in linear logic is explained, with representations of relations taking the form of query and key vectors. The Transformer is an inductive bias for learning structural representations in the form of graphs, with entities as vertices and relations as edges. These representations may organize word meanings in a syntax tree, resembling a two-dimensional space in language. Mathematical concepts like simplicial sets are used to encode the incidence relations between entities. The simplicial set encodes incidence relations between simplices of different dimensions. Recent works in neuroscience emphasize spatial structure and prompt the question of whether a simplicial inductive bias can aid in abstract reasoning."
}