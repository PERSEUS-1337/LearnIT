{
    "title": "Byt3oJ-0W",
    "content": "Permutations and matchings are fundamental in latent variable models for aligning and sorting data. This paper introduces new methods for learning in these models by approximating discrete maximum-weight matching using the Sinkhorn operator. The Gumbel-Sinkhorn method, an extension of the Gumbel-Softmax method, is defined for distributions over latent matchings. The effectiveness of this method is demonstrated by outperforming competitive baselines on various tasks such as sorting numbers, solving puzzles, and identifying neural signals in worms. In recent research efforts, models have been given the ability to manipulate latent combinatorial objects like stacks, memory slots, mathematical expressions, program traces, and first-order logic. These discrete operations can be approximated using differentiable operations on continuous relaxations, allowing them to be included as modules in neural network models trained end-to-end by gradient descent. Matchings and permutations play a crucial role in aligning, canonicalizing, and sorting data in various applications. Learning algorithms for supervised learning have been developed for various applications involving matchings. However, there is a need to learn models with latent matchings, where the matching is not provided as supervision. This is a common setting, as shown in a neuroscience problem involving the identification of neurons from C. elegans. Maximizing the marginal likelihood for problems with latent matchings is challenging due to the need to compute an intractable partition function for a structured distribution. Our contributions include presenting a theoretical result on approximating non-differentiable permutations with a differentiable relaxation using the Sinkhorn operator. We introduce Sinkhorn networks in Section 3, which extend previous work on predicting rankings. Additionally, we introduce the Gumbel-Sinkhorn in Section 4, enabling optimization of marginal likelihood through the reparametrization trick. In Section 5, the study demonstrates that their methods outperform neural network baselines in tasks like sorting numbers, solving jigsaw puzzles, and identifying neural signals from C. elegans worms. They use a temperature-dependent softmax function to approximate discrete categories with continuous values, extending it to permutations through the Sinkhorn operator. The Sinkhorn operator normalizes rows and columns of a matrix iteratively, belonging to the Birkhoff polytope. Choosing a category can be seen as a maximization problem, with the choice of a permutation being parameterized. The Sinkhorn operator normalizes rows and columns of a matrix iteratively, belonging to the Birkhoff polytope. Parameterizing the choice of a permutation can be achieved through a square matrix X, with M(X) obtained as the limit of S(X/\u03c4). Theoretical findings show that M(X) \u2248 S(X/\u03c4) with a small \u03c4, as summarized in Theorem 1. The Sinkhorn operator normalizes rows and columns of a matrix iteratively, belonging to the Birkhoff polytope. Theoretical findings show that the approximation in Theorem 1 cannot be realized in practice due to a limit on Sinkhorn iterations. Instead, an incomplete version of the Sinkhorn operator is considered, where iterations are truncated. This approximation is applied in the context of artificial neural networks to encode permutations and train networks with such layers. The supervised task involves learning a mapping from scrambled objects to non-scrambled ones using permutation-valued regression. A neural network is used to process the input data and minimize reconstruction error. The final network output is parameterized as the solution of the assignments problem, using a differentiable function to replace the non-differentiable one. The choice of a suitable value for \u03c4 is crucial to avoid vanishing gradients or nonsensical reconstructions. Adding noise to the output layer serves as a regularization technique to ensure uniqueness of the output. The regularization device g(X, \u03b8) ensures uniqueness of the output by using permutation equivariant networks to process each piece of X, creating rows of the matrix g(X, \u03b8) with N outputs. The Sinkhorn operator is then applied to these rows for permutation equivariance. The Sinkhorn operator mixes local likelihood vectors to produce consistent assignments in permutation equivariant networks. Sinkhorn networks are supervised methods for reconstructing scrambled objects using non-linear transformations. Different architectures can be used depending on the data, as illustrated in three examples in Section 5. The Sinkhorn operator in permutation equivariant networks reconstructs scrambled objects by learning permutations or matchings between objects. Using a combination of convolutional networks, it generates assignment probabilities for pieces of the object to be in specific positions. Soft-permutation resolves inconsistencies in the assignment probabilities to recover the original object during training. The Gumbel-Softmax or Concrete distributions were defined for computational graphs with stochastic nodes to enable learning of a probabilistic latent representation of permutations. This choice is guided by the re-parameterization trick and the need for differentiable sampling methods. The Gumbel trick is used in Perturb and MAP methods for sampling in discrete graphical models by maximizing random perturbations of potential functions. This re-parameterizes categorical distributions by injecting noise, but for large sets, rank-one perturbations are proposed as a more feasible alternative. The Gumbel-Matching distribution with parameter X is a heuristic approach that provides bounds in the partition function and unbiased samples from the true density. The Gumbel-Sinkhorn distribution with parameter X and temperature \u03c4 is a relaxation for doubly stochastic matrices, converging almost surely to samples of the Gumbel-Matching distribution. The Gumbel-Matching and Gumbel-Sinkhorn distributions do not have tractable densities, but likelihood-free methods have been developed for learning in such distributions. A latent variable model with observed data Y and latent variables Z = {P, W} is considered for approximating the distributions. The text discusses approximating the posterior probability using variational inference by maximizing the ELBO. It assumes prior and variational posteriors decompose as products and focuses on the discrete part of the problem. The variational prior and posteriors on P are parameterized using Gumbel-Matching distributions, replaced by Gumbel-Sinkhorn distributions for differentiability. Results show that our approach outperforms BID52 in sorting tasks with higher accuracy. Generalization to different intervals on the real line is evaluated using isotropic G.S.(X = 0, \u03c4 prior ) distribution for the uniform prior over permutations. The intractable FORMULA12 is simplified by re-parameterizing the prior and posterior with matrices \u03b5 of Gumbel i.i.d variables. The KL term is simplified to KL((X + \u03b5)/\u03c4 \u03b5/\u03c4 prior ) for tractable computation. The KL term simplifies to KL((X + \u03b5)/\u03c4 \u03b5/\u03c4 prior ) and can be computed explicitly. However, applying non-invertible transformations may lead to a less tight lower bound. Experimental results on MNIST show minimal loss of tightness, indicating the effectiveness of the approach on permutations. The key aspect is that both the prior and posterior are the same function of a simpler distribution. In this section, experiments comparing different Sinkhorn network architectures are performed, with a focus on probabilistic constructions and marginal inferences over permutations. The capabilities of Sinkhorn Networks are illustrated by sorting numbers using artificial neural networks. Additional experimental details can be found in the appendix. The network is trained with pairs of the same X in sorted order, learning to sort up to 120 numbers. Evaluation shows the proportion of sequences with errors. Results indicate the network can sort numbers even outside the U(0,1) interval, suggesting no overfitting. The network can sort up to 120 numbers without overfitting, as shown by evaluation results. Comparisons with a more complex network in BID52 reveal better performance with our network. Additionally, a CNN is used in the first layer for mapping puzzle pieces in image reconstruction tasks. Various evaluation measures are reported, including error proportions and loss functions. In image reconstruction tasks, the network can sort up to 120 numbers without overfitting. Results show better performance compared to a more complex network. Using l1 and l2 losses and Kendall tau, the network achieves low errors on MNIST puzzles up to 6x6 pieces. However, errors increase as the number of black pieces grows. In Celeba, puzzles up to 5x5 pieces are solved with only 21% of face pieces incorrectly ordered. Performance decreases with small or large temperature settings, but slightly with fewer Sinkhorn iterations. In image reconstruction tasks, the network can sort up to 120 numbers without overfitting. Results show better performance compared to a more complex network. Using l1 and l2 losses and Kendall tau, the network achieves low errors on MNIST puzzles up to 6x6 pieces. However, errors increase as the number of black pieces grows. Performance decreases with small or large temperature settings, but slightly with fewer Sinkhorn iterations. Temperature plays a relevant role in parameter convergence, with different temperatures resulting in convergence in different phases or regions. Learning in the Imagenet dataset is more challenging due to the lack of sequential structure that generalizes among images. The network achieves a .72 Kendall tau score, tying with DeepPermNet on the stacking of up to the sixth fully connected layer. The DeepPermNet network, based on AlexNet BID27, has a simpler design with only two layers and fewer parameters compared to the more complex network. The best-performing model had around 1,050,000 parameters, while DeepPermNet's layer connecting fc6 with fc7 had approximately 19,000,000 parameters. The network can be trained to solve Jigsaw Puzzles and transform MNIST digits into another with hard and soft reconstructions shown. The DeepPermNet network, with 19,000,000 parameters in the fc6 to fc7 layer, explores the phenomenon of convolutions becoming fully connected as pieces increase. The Jigsaw Puzzle task becomes ill-posed with too many pieces, leading to non-unique reconstructions in the binarized MNIST dataset. Reconstructions become a multimodal distribution over permutations in this scenario. The network was modified to stack several second layers linking an intermediate representation to the output, training it to reconstruct arbitrary digits given scrambled pieces. Results show reconstructions can be identified visually and by a neural network assessment, achieving 99.2% accuracy on the test set. The CNN achieved an 85.1% accuracy on the test set with arbitrary transformations of digits. Reconstructions of \"digits\" from a mixed dataset were shown in Figure 4. The possibility of alternative generative modeling using random assembly of noise pieces was suggested, but training the network without supervision was not explored in this work. The C. elegans neural inference problem involves using the G.S. distribution as a continuous relaxation for stochastic nodes in a computational graph. C. elegans is a nematode with a stereotypical neural configuration, making it possible to match observed neural dynamics to neuron identities in its brain. The problem of inferring neural dynamics in C. elegans involves a Bayesian hierarchical model with latent variables W and P. W encodes dynamics with a sparse connectome prior, while P represents neuron matching with a flat prior over permutations. The goal is to find the posterior distribution p({P, W }|Y ) to address the simultaneous problem with multiple worms sharing the same dynamical system. Variational inference is used to find the posterior distribution p({P, W }|Y ) in a Bayesian hierarchical model for inferring neural dynamics in C. elegans. Results in Table 3 show that the proposed method outperforms baselines like MCMC due to poor mixing, confirming the stochastic nature of the problem. The method presented in this study confirms the stochastic nature of the problem and outperforms previous approaches. It focuses on marginal inference in a model with a latent matching, which poses challenges for standard learning techniques. The study introduces a technique for marginal inference in models with latent permutations, extending the Gumbel-trick to Gumbel Sinkhorn distributions. This method outperforms previous approaches and addresses challenges in standard learning techniques. The study introduces a technique for marginal inference in models with latent permutations, extending the Gumbel-trick to Gumbel Sinkhorn distributions. Their framework produces tractable densities, favoring the Gumbel Sinkhorn distribution as a tighter relaxation. The approach links to recent developments in optimal transport, drawing on entropy-regularization techniques. The OT technique developed in Cuturi FORMULA0 introduces the 'Sinkhorn distance' for entropy-regularized transportation problems. This approach connects to recent work on permutations and generative modeling, emphasizing the use of neural networks to learn the OT cost function. The work extends previous research on neural networks for learning permutation-like structures. It introduces a temperature parameter and considers a limit argument critical to the case. The Sinkhorn iteration is interpreted as mean-field inference in a Gibbs distribution over matchings, enabling end-to-end learning in an unrolled inference algorithm. Future work may explore unrolling alternative algorithms for marginal inference over matchings. Sinkhorn networks were recently introduced in a different study, focusing on representational aspects of CNN's. In contrast, this work is more concerned with fundamental properties and introduces a temperature parameter \u03c4. Despite using a simpler network with fewer parameters and layers, equivalent performance was achieved on the Jigsaw puzzle task. The authors acknowledge the need for more complex architectures for certain tasks and hope their general theory, including Theorem 1 and the notion of equivariance, will contribute to further developments in that direction. The Sinkhorn networks have shown the ability to learn permutations in basic cases, such as sorted numbers and facial pieces. However, challenges arise with more complex scenarios like Imagenet. To address this, a sequential stage with memory buffer for solutions and exploration of more complex parameterizations for permutations are proposed. Alternatively, reinforcement learning could be used. The Sinkhorn networks have shown the ability to learn permutations in basic cases, such as sorted numbers and facial pieces. Challenges arise with more complex scenarios like Imagenet. To address this, a sequential stage with memory buffer for solutions and exploration of more complex parameterizations for permutations are proposed. Alternatively, reinforcement learning techniques could be used. The current work presents a significant step towards solving the \"Order Matters\" problem by introducing Tensorflow code for Gumbel-Sinkhorn networks and providing a rigorous proof of Theorem 1 related to the Sinkhorn theorem in matrix theory. The Sinkhorn operator is defined for matrices with positive diagonals, resulting in a doubly stochastic matrix. It can be obtained by iteratively normalizing the rows and columns of a matrix. The operator is represented by a formula involving exponential functions and is proven to yield a doubly stochastic matrix. Key properties of doubly stochastic matrices are also discussed. The Birkhoff polytope is the set of doubly stochastic matrices of dimension N, with permutation matrices being extremal points. The matching operator aims to maximize a linear functional in the space of permutation matrices or the Birkhoff polytope. The Birkhoff polytope is the set of doubly stochastic matrices, with the matching operator maximizing a linear functional in the space of permutation matrices or the Birkhoff polytope. The main theorem states that for a doubly stochastic matrix P, its entropy is defined as h(P) = -\u2211 P(i,j) log(P(i,j)). The convergence of S(X/\u03c4) is proven under a stochastic regime, with Lemmas 1 and 2 providing key relations and uniqueness of solutions. Lemma 1 shows the existence and uniqueness of the solution P\u03c4 under a strict concavity condition. The Lagrangian of the constrained problem is stated, leading to the conclusion that S(X/\u03c4) = P\u03c4. Lemma 2 discusses the scenario where entries of X are independently drawn from a distribution. Lemma 2 states that if entries of X are independently drawn from a distribution, then almost surely, the optimal solution lies on a face of dimension 0. The optimal solution lies on a vertex, which is unique. The solution P \u03c4 converges to P 0 as \u03c4 approaches 0. The function f \u03c4 converges to f 0, and the values converge to imply the convergence of P \u03c4 to P 0. The function f \u03c4 converges to f 0, and the values converge to imply the convergence of P \u03c4 to P 0. Lemma 1 states that f 0 (P \u03c4) \u2212 f 0 (P) > \u03b5 when P \u2212 P 0 > \u03b4, P \u2208 B N. Convergence (equation 11) is a direct consequence of Lemma 3, where P \u03c4 = S(X/\u03c4) and P 0 = M(X). This can be seen as a generalization of the approximation result arg max i x i = lim \u03c4 \u21920 + softmax(x/\u03c4). The entropy h(\u00b7) is defined using Tensorflow on a cluster with multiple GPUs for efficient exploration of hyperparameters. Experiments were conducted with a 10x10 batch size using Gumbel perturbations for reconstructions. The Hungarian Algorithm was used for evaluation, and experiments in section 5.4 followed model specifications from Linderman et al. The network architecture involves sorting numbers with a specific number of parameters connecting hidden layers. The first layer in the network is a convolutional layer with n filters and ReLU + max-pooling operations. The second layer connects the output of the convolution to stacked images and p^2 units. The total number of parameters in the network is determined by the puzzle size and network specifications. For a 3x3 puzzle on Imagenet, the network had 1,053,440 parameters. The optimal network had 1,053,440 parameters with specific configurations. Additional fully connected second layers increase the total number of parameters. The computation of KL((X + \u03b5)/\u03c4 \u03b5/\u03c4 prior) is shown, involving the density of the variable h = (a + g)/b with a Gumbel distribution. The log density ratio LR(z) between components of h 1 and h 2 is calculated, followed by taking expectations with respect to the distribution of h 1 and \u03b5. E(\u03b5) = \u03b3 \u2248 0.5772 is determined using the law of the unconscious statistician. In a study involving Jigsaw puzzle tasks on Celeba, performance measures were provided for extreme hyper-parameter values. Results showed that a single Sinkhorn iteration already provided reasonable performance, with temperature selection being crucial. Additionally, a 2-layer CNN was evaluated in detecting transformed digits, with the most challenging transformation being to the digit one. In figure 4, transformations are shown where digits can be reconstructed from pieces of 'strokes' or 'dust'. Results in Table 7 display accuracy in the C.elegans neural identification problem for varying candidate neurons and worms. In Table 7, additional results for the C.elegans experiment are presented, showing a connection between the distribution in (6) and the Sinkhorn operator. The optimization problem involves maximizing the entropy term in the parameter \u03b8. The Sinkhorn operator provides approximations for the partition function and marginals by maximizing the entropy term in the parameter \u03b8. It complements classical approximations like Bethe and Kituchani's, leading to approximate inference algorithms. The Gumbel-max trick and Gumbel-Sinkhorn reparameterizations are also discussed. The Gumbel-Matching and Gumbel-Sinkhorn methods involve using Gumbel noise for approximate inference."
}