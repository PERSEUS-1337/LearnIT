{
    "title": "SyYe6k-CW",
    "content": "Recent advances in deep reinforcement learning have improved performance on applications like Go and Atari games. Balancing exploration and exploitation in complex domains remains a challenge. Thompson Sampling offers an elegant approach to exploration by utilizing posterior samples of the model. Approximate Bayesian methods have made posterior approximation for neural network models practical. Using approximate Bayesian neural networks in a Thompson Sampling framework shows promise. Benchmarking various methods for approximate posterior sampling combined with Thompson Sampling on contextual bandit problems reveals successful approaches. Recent advances in reinforcement learning with deep neural networks have sparked interest in sequential decision-making. Neural networks are powerful function approximators, but quantifying model uncertainty on new data remains a challenge. Understanding what is not yet known is crucial for effective exploration in decision-making tasks. Sequential decision-making involves adapting slowly converging uncertainty estimates to the online setting. Thompson Sampling and Posterior Sampling offer an efficient solution to the exploration-exploitation dilemma in sequential decision making by maintaining a posterior over models and choosing actions based on their probability of being optimal. Maintaining this posterior is challenging but crucial for effective deep exploration throughout the decision-making process. Approximate Bayesian methods for deep neural networks have been developed due to the intractability of maintaining an exact posterior. These methods, such as variational methods and stochastic minibatch Markov Chain Monte Carlo, are challenging to evaluate and are rarely compared on benchmarks measuring the quality of uncertainty estimates for downstream tasks. To address this, a benchmark for exploration methods using deep neural networks has been developed, comparing various Bayesian approximations under Thompson Sampling for contextual bandits in sequential decision making. All code and implementations for reproducing the experiments will be available open-source. In the context of reinforcement learning, exploration techniques like epsilon-greedy and posterior sampling methods are actively researched. This paper focuses on how different posterior approximations impact Thompson Sampling performance in contextual bandits. Various techniques, including adding random noise to parameters or bootstrap sampling, are compared empirically. No single algorithm outperformed the others in this study. In contextual bandit decision making scenarios, various algorithms were compared, with no single algorithm emerging as the best in every problem. Dropout, random noise injection, and bootstrapping showed performance improvement in some tasks but struggled with challenging exploration tasks. Complex algorithms like Variational Inference and Black Box \u03b1-divergence had issues with partial optimization in online scenarios. Bayesian linear regression on deep network representations proved to be a robust and easily adjustable approach. Thompson Sampling is discussed in Section 2. Thompson Sampling is discussed in Section 2, focusing on the contextual bandit problem where an algorithm selects actions based on incoming contexts and generates rewards. The cumulative regret is defined as the difference between the algorithm's total reward and the optimal reward. The paper addresses how approximated model posteriors impact decision making through Thompson Sampling in contextual bandits. Various algorithmic approaches are studied to approximate a posterior distribution over problem instances. Thompson Sampling is a classic algorithm that requires minimal information to make decisions. Thompson Sampling is a classic algorithm that draws samples from a posterior distribution and takes greedy actions under the optimal policy. It has been proven effective for bandit problems in both practice and theory. The algorithm is especially useful for deep neural networks where access to the full posterior is limited. Various algorithmic design principles are considered for simulations, including linear methods, variational inference, dropout, Monte Carlo methods, and Gaussian. In the appendix, nonlinear algorithms' posteriors are visualized on a synthetic one-dimensional problem. Bayesian linear regression is applied for exact posterior inference in linear models, with closed-form updates provided. The joint distribution of \u03b2 and \u03c32 for each action is modeled, allowing for adaptive noise level estimation and improved exploration. The text discusses the posterior distribution for action i in Bayesian linear regression, with parameters for \u03c32 and \u03b2. Approximations are considered for efficient computation, especially in the case of Bayesian neural networks. The focus is on adaptive noise level estimation and exploration in nonlinear algorithms. In Bayesian linear regression, two linear approximations with diagonal \u03a3 are studied to understand their impact on performance. The PrecisionDiag approximation minimizes KL divergence, while the Diag approximation aims to minimize a different objective. These approximations set expectations for more complex approaches like mean-field variational inference or Stochastic Gradient Langevin Dynamics. In Bayesian linear regression, linear approximations with diagonal \u03a3 are studied for performance impact. Linear algorithms lack representational power but complement with accurate uncertainty estimates. A Bayesian linear regression on top of a neural network's last layer is proposed for better results. The text discusses using a deep net to learn a representation z and then using Bayesian linear regression to regress values v on z, obtaining uncertainty estimates on the \u03b2's for decision-making via Thompson Sampling. The network is used solely for finding representations z, and the linear regression is updated at different time-scales compared to the network. The text discusses training a neural network to obtain z, which is then used in Bayesian regression. The algorithm Neural Linear updates z periodically with online updates. Another algorithm, Neural Greedy, trains a neural network and acts greedily. Variational Inference approximates the posterior by finding a distribution that minimizes KL divergence. Variational inference approximates the posterior by minimizing KL divergence within a tractable family, using optimization methods instead of sampling like MCMC. The posterior is often approximated by a mean-field or factorized distribution, with criticism for underestimating uncertainty. Expectation-propagation algorithms are based on message passing framework for iterative updates. The algorithm BID30 is based on the message passing framework and iteratively approximates the posterior by updating a single approximation factor at a time. It minimizes local KL divergences, often using likelihoods from the exponential family for computational reasons. Methods like Power EP and black-box \u03b1-divergence minimization are used to optimize the global EP objective via stochastic gradient descent. The optimal \u03b1 value depends on the problem at hand. Dropout is a training technique where the output of each neuron is independently zeroed out with probability p at each forward pass. Once the network has been trained, dropout can still be used to obtain a distribution of predictions for a specific input, resembling Thompson sampling. Monte Carlo sampling is a reliable tool in the Bayesian toolbox, estimating the posterior through drawing samples, which is useful for highly parameterized deep neural networks. Among Monte Carlo methods, Hamiltonian Monte Carlo (HMC) is considered a gold standard algorithm for neural networks due to its use of gradient information and momentum. However, it is not feasible for larger datasets as it involves a Metropolis accept-reject step that requires computing the log likelihood over the entire dataset. Stochastic Gradient Langevin Dynamics (SGLD) methods have been developed to approximate HMC using mini-batch stochastic gradients by adding Gaussian noise to model gradients during updates, resulting in an approximate sample from the posterior. Different strategies have been developed for augmenting gradients and noise in stochastic gradient descent. BID25 found that a preconditioner based on the RMSprop algorithm works well for deep neural networks. BID1 introduced Stochastic Gradient Fisher Scoring to precondition according to the Fisher information, removing the need for annealing the learning rate to zero. BID26 developed methods for sampling from the posterior using a constant learning rate in stochastic gradient descent and evaluated diagonal-SGFS and constant-SGD algorithms. In this work, constant-SGD algorithms are used with a constant learning rate for stochastic gradient descent. The learning rate is determined by the batch size, number of data points, and an online average of the diagonal empirical Fisher information matrix. Stochastic Gradient Fisher Scoring involves updating model parameters with a stochastic gradient. The Bootstrap method is used to approximate the sampling distribution of estimators by training multiple models on different datasets. In order to emulate Thompson Sampling, a parameter p is set to append new data points to each dataset independently at random. Testing was done with cases q = 5, 10 and p = 0.8, 1.0 using neural network models. Even with identical datasets, random initialization and SGD lead to different predictions. Parameter-Noise is a recent approach for exploration in deep RL, perturbing network weights with isotropic Gaussian noise during action selection. The network uses layer normalization to ensure all weights are updated. The network uses layer normalization to ensure all weights are on the same scale. Gaussian noise is added to perturb model parameters, aiming for more sensible actions than epsilon-greedy. Bayesian Non-parametric Gaussian processes are a standard method for modeling distributions over continuous functions, but their computational scalability limits their use on small datasets. Gaussian processes are commonly used for modeling continuous functions, but their scalability is limited on small datasets. Various methods like pseudo-observations and variational inference are used to approximate Gaussian processes. A multi-task Gaussian process model with optimized hyperparameters is implemented to learn correlations between outputs. The covariance function of the GP is defined by DISPLAYFORM2, with task kernels between tasks t and l given by DISPLAYFORM3 2 ). Length-scales, amplitude parameters, and inducing points are optimized for a Sparse Variational GP BID18. Uncertainty estimates drive sequential decision-making in static and dynamic scenarios, illustrating the complexities that arise. In dynamic settings, the goal is to return an approximate posterior distribution using observed data. The data-points are no longer independent, leading to two distributions: the posterior given the observed data and a new estimate. The distance between these distributions is not always a definitive measure of performance, as a poorly-approximated decision boundary can lead to sub-optimal actions being repeatedly selected. In dynamic settings, the goal is to return an approximate posterior distribution using observed data. The data-points are no longer independent, leading to two distributions: the posterior given the observed data and a new estimate. The distance between these distributions is not always a definitive measure of performance, as a poorly-approximated decision boundary can lead to sub-optimal actions being repeatedly selected. An algorithm based on Thompson Sampling's assumption aims for \u03c0 t to be close to \u03c0 * t for all t to achieve strong performance, but formalizing this concept is challenging due to the difficulty in comparing posterior distributions when different decisions are made. In a dynamic setting, an approximate posterior distribution is returned using observed data. The posterior distribution is computed using Bayesian linear regression formulas. The posterior distribution for each arm after 500 pulls is shown in FIG2. Thompson Sampling with approximate posterior distributions is also considered. The approximate posterior with diagonal covariance matrix, Diag in red, is compared to the actual posterior after n = 500 decisions in FIG2. The impact of this mismatch on regret is illustrated, showing how inaccurate posteriors can lead to different behaviors. The PrecisionDiag approximation outperforms the actual posterior, while the diagonal covariance approximation suffers poor regret with increasing dimension. Simulations and outcomes of bandit problems with different algorithms are presented in this section. Neural network architectures for contextual bandit experiments involve a fully-connected feedforward network with two hidden layers of 100 units each and ReLu activations. Key parameters for each algorithm can be found in Table 2 in the appendix. Updating Models: Linear algorithms are updated after each time-step using specific methods. For neural networks, training occurs for a set number of mini-batches every few timesteps. The size of each mini-batch is 512, and varying the training frequency proved crucial for certain algorithms. Two metrics, cumulative regret, and simple regret, are reported in the study. Hyper-Parameter Tuning: Deep learning methods are sensitive to hyperparameter selection, which is dataset dependent. The experiment reshuffles contexts and reruns 50 times to obtain cumulative regret distribution statistics. In the bandits scenario, hyperparameter tuning is crucial for algorithms, but often we do not have access to each problem beforehand. Algorithms are tested with hyperparameters guessed a-priori and then optimized on different datasets using Bayesian Optimization. For example, Dropout has versions named Dropout, Dropout-MR, and Dropout-SL optimized on Mushroom and Statlog datasets. In bandit algorithms, actions 1, 2, 3, 4, and 5 correspond to red, green, black, and yellow regions. A data buffer was not used due to observed catastrophic forgetting. Algorithms were evaluated on various real-world datasets like Mushroom, Statlog, Covertype, Financial, Jester, Adult, Census, and Song datasets. The datasets used in the bandit algorithms exhibit a range of properties such as size, signal strength, and reward type. Results for Mushroom, Statlog, Covertype, Financial, and Jester datasets are summarized in a table. Linear models are used as a baseline comparison for algorithms. Box plots of algorithm ranks across bandit problems are shown in the appendix. The experiments compare approximate methods with exact posterior computation. Hyper-parameter configurations are in TAB2 in the appendix. Real-data problems may not need much exploration. An artificial problem, the wheel bandit, is designed with exploration parameter \u03b4. Contexts are sampled uniformly in a unit circle in R2. There are 5 possible actions, with the first action always offering a reward independently of the context. The experiments compare approximate methods with exact posterior computation. An artificial problem, the wheel bandit, is designed with exploration parameter \u03b4. Contexts are sampled uniformly in a unit circle in R2. There are 5 possible actions, with the first action always offering a reward independently of the context. For X \u2264 \u03b4, actions are sub-optimal, while for X > \u03b4, only one action is optimal depending on the context components. The problem can be generalized for d > 2. The problem can be easily generalized for d > 2. Results are shown in Table 9. Overall, there is significant room for improvement in uncertainty estimation for neural networks in sequential decision-making. Sequential decision-making requires frequent model updates as data accumulates, leading to challenges with slow convergence. Decoupling representation learning and uncertainty estimation may improve performance, as seen in the NeuralLinear algorithm. The decoupling of representation learning and uncertainty estimation in neural networks can improve performance. Algorithms are sensitive to hyperparameters, and Stochastic Gradient Descent provides sufficient exploration. Simple methods like Stochastic Gradient Descent may be more justified than complicated variations in some scenarios. All algorithms in the evaluation use linear models. Linear methods offer a strong baseline with the ability to compute informative uncertainty measures. They perform well in several datasets, reacting quickly to unexpected rewards. However, some datasets require more complex non-linear representations. Linear methods offer computational advantages but may struggle with complex non-linear datasets. Diagonalizing the precision matrix in mean-field Variational Inference outperforms diagonalizing the covariance matrix. The NeuralLinear algorithm shows promise in improving neural networks by simplifying data representation. Its exploration mechanisms add value and it can learn tasks simultaneously. NeuralLinear algorithm simplifies data representation and accurately quantifies uncertainty over linear models. It outperforms RMS algorithms and greedy linear approaches in the Wheel problem, showing promising results in solving complex tasks. NeuralLinear algorithm is effective in solving problems that require non-linear representations where linear approaches fail. It is easy to tune and robust in hyper-parameter configurations. Deployment to large scale systems may have technical difficulties, but standard solutions could mitigate these issues. In experiments, NeuralLinear is computationally fast compared to other algorithms. Bayes By Backprop performed poorly in experiments, ranking in the bottom half of algorithms. The performance of Black Box \u03b1-divergence algorithms, specifically Bayes By Backprop, was poor in experiments, underperforming compared to exact mean field solutions. The difference in performance due to training steps outweighed the difference between mean field solutions and exact posteriors, indicating the need for full optimization in scenarios where uncertainty estimates directly impact data collection. The poor performance of Black Box \u03b1-divergence algorithms, similar to BBB, is attributed to partial convergence and sensitivity to training steps. They perform terribly in Mushroom bandit and slightly worse in other datasets compared to variational inference. Investigating ways to improve convergence of uncertainty estimates is a promising future direction. Constant-SGD emerges as the top performer on Covertype, showcasing its effectiveness in scenarios requiring non-linearity and exploration. The method SGFS outperforms Constant-SGD in the Wheel Bandit problem due to injected noise promoting exploration. Bootstrap shows significant improvements over RMS in various datasets, especially in scenarios where randomness from SGD is not sufficient for exploration. However, the heavy computational overhead of Bootstrap may not always justify its benefits. Optimized versions of BootstrappedNN use only q = 2 and q = 3 networks. The optimized versions of BootstrappedNN use only q = 2 and q = 3 networks, which did not significantly improve performance compared to manually setting q = 10. Bootstrapped NNs struggled with the Wheel problem and had similar performance to RMS. Parameter-Noise provided a performance boost across datasets, with an average rank of 20.9 compared to RMS at 28.7. The algorithm Parameter-Noise-SL outperforms RMS with an average rank of 20.9 compared to 28.7. However, it is difficult to tune and sensitive to noise levels. On the Wheel problem, both Parameter-Noise and RMS struggle with underexploration, except for ParamNoise-MR which performs well. Developing intuition for the heuristic is challenging due to lack of transparency. Dropout with p = 0.8 consistently delivers better results than p = 0.5. Optimized versions of the algorithm show improvements over base RMS, especially Dropout-MR. In the Wheel problem, Dropout is outperformed by RMS, but Dropout-MR offers gains. The algorithm Dropout-MR shows gains compared to RMS but is not competitive with the best algorithms. It heavily depends on hyper-parameters and the contribution of better training versus exploration needs further investigation. Gaussian processes perform well on small data problems but struggle on larger ones. Sparse GP did not perform as well as simpler methods. The impact of approximate model posteriors on decision-making performance was empirically studied. In a study on decision-making in contextual bandits, the impact of approximate model posteriors was empirically examined. Robust methods that accurately measured uncertainty on complex representations were found to be effective. However, more complex approaches that learned representation and uncertainty together required heavier training and exhibited stronger hyper-parameter dependence. Further exploration and development of these methods are recommended for future work. In a study on decision-making in contextual bandits, approximate model posteriors were examined. Various methods like BayesByBackprop, Bootstrapped NN, and Dropout were tested with different parameters for training. In a study on decision-making in contextual bandits, approximate model posteriors were examined using methods like BayesByBackprop, Bootstrapped NN, and Dropout with different training parameters. Dropout (RMS3) and Dropout (RMS2) with probabilities of 0.8 were used, along with Greedy NN approach with fixed learning rate. Different variations of RMS2 and SGFS algorithms were also tested, with results published in a conference paper at ICLR 2018. Table 5 shows the simple regret incurred by linear models using various algorithms on bandits. Values are the mean over 50 trials with standard error. Different algorithms like LinGreedy and LinPost were tested with their respective regret values. Table 6 shows the cumulative regret incurred by models using algorithms on bandits, normalized with respect to the performance of Uniform. Published as a conference paper at ICLR 2018 Table 7 shows the simple regret incurred by different algorithms. Table 7 presents the simple regret incurred by models using algorithms on bandits, while Table 8 displays the elapsed time for these algorithms. Table 9 shows the cumulative regret on the Wheel Bandit problem with varying values of \u03b4. Values are normalized with respect to the performance of Uniform. The Mushroom Dataset (Schlimmer, 1981) contains 22 attributes per mushroom, with two classes: poisonous and safe. In a bandit problem, the agent decides whether to eat a mushroom, with rewards based on its safety. The Shuttle Statlog Dataset BID2 is also used in the study. The Shuttle Statlog Dataset BID2 involves predicting the state of the radiator subsystem during a space shuttle flight with 7 possible states. The Covertype Dataset BID2 classifies northern Colorado forest areas into 7 cover types based on 54 features. The Financial Dataset tracks stock prices of 21 publicly traded companies. The Financial Dataset consists of stock prices from 21 companies in NYSE and Nasdaq over 14 years. The dataset was used to create 8 potential portfolios. The Jester Dataset involves ratings for 100 jokes from users, with 32 ratings as user context and 8 arms for recommendations. The Adult Dataset BID24 BID2 contains personal information from the US Census Bureau database, with the goal of predicting if a person makes over $50K a year. The dataset includes 14 different occupations as feasible actions based on 94 covariates. The agent receives a reward of 1 for correct predictions and 0 otherwise. The US Census (1990) Dataset BID2 involves predicting the occupation of individuals among 9 classes using 389 covariates. The agent also receives a reward of 1 for correct predictions and 0 otherwise. The YearPredictionMSD Dataset is a subset of the Million Song Dataset BID4, with the goal of predicting the year a song was released based on 90 audio features. The agent receives rewards for correct predictions on 250,000 randomly selected data points. Gaussian rewards are provided based on the distance between the predicted year interval and the actual release year. Other datasets like Statlog, Covertype, Adult, and Census were also tested in BID11."
}