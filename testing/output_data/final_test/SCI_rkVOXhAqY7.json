{
    "title": "rkVOXhAqY7",
    "content": "The Conditional Entropy Bottleneck (CEB) objective functions are based on the Minimum Necessary Information (MNI) criterion and are applied to classification tasks. CEB provides well-calibrated predictions, detects challenging out-of-distribution and whitebox adversarial examples, and shows robustness against adversaries. It fails to learn from information-free datasets, addressing generalization issues in Machine Learning. Many proposed defenses against adversarial attacks lack robustness against powerful adversaries and fail in the presence of concerted attackers. Classifiers struggle with out-of-distribution detection, giving poor signals for data substantially different from their training data. Various approaches have been suggested to address this issue but often perform poorly on obviously different data. Classifiers tend to be overconfident in their predictions, leading to miscalibrated results. They also have a tendency to overfit to the training data, memorizing random labelings and failing to generalize. This highlights the importance of testing generalization by providing information-free datasets. The field of lossless compression and machine learning focus on optimal representation and prediction, respectively. Lossless compression aims to maintain all information in X and Y, while machine learning aims to make optimal predictions. The field of machine learning focuses on making optimal predictions on unseen data by learning representations that capture shared information between X and Y, measured by mutual information. This mutual information measures the amount of information necessary to define the relationship between X and Y, ensuring that any information less than I(X; Y) is insufficient for prediction tasks. The Minimum Necessary Information (MNI) criterion for a learned representation focuses on capturing semantically meaningful information to optimize prediction tasks in machine learning. It emphasizes the importance of retaining only relevant information in high-dimensional datasets to improve system performance. The Minimum Necessary Information (MNI) criterion emphasizes the importance of capturing semantically meaningful information in machine learning. It focuses on the uniqueness of entropy as a measure of information and the necessity for the information captured in representations to be necessary to solve the task. Additionally, it stresses the minimality of information retained in representations to avoid incorporating non-semantic information like noise or spurious correlation. The MNI criterion emphasizes capturing semantically meaningful information in machine learning. It focuses on the uniqueness of entropy and the necessity for information in representations to solve tasks. One way to satisfy this criterion is by learning a representation Z X of X only, maintaining conditional independence between Y and Z X given X. The MNI criterion emphasizes capturing semantically meaningful information in machine learning by learning a representation Z X of X only, maintaining conditional independence between Y and Z X given X. However, maximizing I(X; Z X ) while satisfying the MNI criterion is challenging, leading to the need for a different approach. The MNI criterion aims to capture meaningful information in machine learning by learning a representation Z X of X while maintaining conditional independence between Y and Z X given X. To achieve the MNI, an alternative approach is needed. The Markov chain and mutual information's chain rule ensure non-negativity of conditional information, allowing for optimization towards minimizing a term to reach the optimal value of 0. This leads to the Conditional Entropy Bottleneck objective. The Conditional Entropy Bottleneck objective involves balancing two terms, encoder e(z X |x) and backward encoder b(z X |y), to minimize a variational objective function. The classifier c(y|z x) approximates p(y|z X) and supports amortized inference on large-scale problems. The Variational Conditional Entropy Bottleneck (VCEB) involves learning distributions with parameters for encoder, backward encoder, and classifier. The Residual Information term represents excess information in the representation beyond shared information. The Information Bottleneck (IB) learns a representation subject to a soft information constraint controlled by \u03b2. Various variations on this objective are described in the appendix. The optimal surfaces for CEB and IB are shown, with the MNI point labeled on both. Adjusting \u03b2 determines a unique point in these information planes relative to I(X; Y). IB is a tabular method not usable for amortized inference, but recent works have extended it for this purpose. The tabular optimization procedure for IB applies to CEB by setting \u03b2 = 1/2. The Deterministic Information Bottleneck learns hard clusterings, unlike earlier IB approaches. CEB rectifies IB's parallelogram by subtracting I(Y; Z) at every point. The Variational Information Bottleneck (VIB) objective is presented as a variation on Dropout, with a marginal posterior and a hyperparameter \u03b2. The optimal value for \u03b2 is shown to be 1/2 when adhering to the MNI criterion. Comparing VIB with variational CEB, their difference at \u03b2 = 1/2 is highlighted, both having a dependence on log p(y) that needs to be tracked. The text discusses the difficulty in finding the optimal point for m(z X) in the context of VIB and CEB models. It mentions that VIB may converge to a looser approximation of mutual information compared to CEB. The tightness of b(z X|y) to the optimal p(z X|y) depends on the tightness of c(y|z X) to the optimal p(y|z X). The optimality of CEB representations in learning is also mentioned. The CEB representations aim to learn the optimal amount of information about the observed data, guided by the Minimum Noiseless Information criterion. The bidirectional CEB objective is crucial for satisfying the MNI criterion, ensuring that the right amount of information is included for predicting Y. Only models that successfully learn this amount of information can be MNI-optimal. The primary experiments focus on comparing models with different objective functions rather than achieving state-of-the-art results. Results are presented for the classification of Fashion MNIST BID46 using five models: a deterministic model (Determ), three VIB models with varying beta values (VIB 0.5, VIB 0.1, VIB 0.01), and a CEB model. These models are also used in calibration, out-of-distribution, and adversarial experiments. The five models compared in the experiments share the same inference architecture mapping X to Y. Fashion MNIST was used without a validation set to test training algorithms. The models were trained with specific hyperparameters, including a learning rate of 0.001 for 40 epochs before lowering it. The CEB model had a different training algorithm, while the other models tracked their training loss instead of Re X/Y. During training, all five models showed smooth convergence to their final performance without any non-monotonic test accuracy. The dynamic learning rate schedule did not affect performance negatively. Underconfidence and overconfidence were defined based on points relative to the diagonal. In a simple classification problem with a uniform class distribution, I(X; Y) can be computed as log C. A comparison of variational models' rates and accuracies is shown in TAB0, with all models except VIB 0.5 achieving the same accuracy. Stochastic models approached the ideal rate of 2.3 nats through different paths. In training, models converge smoothly without non-monotonic test accuracy. Calibration plots show models transitioning from under- to overconfidence. Overconfidence is linked to exceeding the MNI during training. Geometric explanation in Appendix A. The five models are tested for out-of-distribution detection using three different datasets. Three metrics for thresholding are used, including classifier entropy and rate. A specific metric for CEB is also employed, calculating predicted residual information. The VIB 0.5 model performs poorly at out-of-distribution tasks compared to other VIB models and CEB. This is attributed to VIB 0.5 not learning all the necessary information. In contrast, the other VIB objectives perform well by capturing more information from the training set. Adversarial examples and attacks like Fast Gradient Method are also discussed. The Carlini-Wagner (CW) attack, introduced as a practical method using a blackbox optimizer, is relevant in the context of adversarial analysis. This work trains a VIB model with a learned marginal for robustness against attacks like CW. CW with a confidence penalty and a custom CW attack targeting detection mechanisms are also discussed. The Carlini-Wagner (CW) attack is considered the current gold standard attack for evaluating model robustness. It searches for adversarial perturbations by optimizing directly on the model, providing more insights into model robustness compared to other attack methods like FGM or DeepFool. The Carlini-Wagner (CW) attack is a strong option for testing robustness, focusing on attack efficiency by searching over perturbation magnitudes. Three variants of the CW L2 targeted attack are explored, with different implementations and adjustments. Increasing the default CW learning rate was necessary for the attacks to succeed on CEB, with one variant incorporating a detection tensor into the loss function. The Carlini-Wagner (CW) attack is used to test robustness by targeting the trouser class in Fashion MNIST. Different variants of the attack are run on the entire test set, with varying levels of efficiency and cost. CW (C = 1) Det. is particularly expensive and is only run on the CEB model. The Carlini-Wagner (CW) attack is used to test robustness by targeting the trouser class in Fashion MNIST. CW (C = 1) Det. is only run on the CEB model. The metric for robustness involves counting adversarial examples that change correct predictions to incorrect ones, measuring perturbations using different norms, and evaluating adversarial detection. Results are shown in TAB2, with VIB 0.01 and VIB 0.1 performing well in detection, while VIB 0.5 is more robust. The VIB 0.5 model shows robustness but struggles with detection, indicating a trade-off between learning necessary information for detection and minimum information for robustness. In contrast, the CEB model maintains both necessary information for detection and minimum information for robustness, making it more robust to attacks like the CW (C = 1) Det. attack. The model becomes more robust to attacks with lower success rates and larger perturbation magnitudes. Using Fashion MNIST images with random labels, CEB and VIB models show different learning behaviors, with VIB models only learning with \u03b2 \u2264 0.001. The resistance of CEB and VIB models to memorizing random labels demonstrates the effectiveness of the Minimum Necessary Information criterion. The Conditional Entropy Bottleneck (CEB) is introduced as a way to improve OoD detection, adversarial example detection, robustness, calibration, and generalization without additional regularization or hyperparameters. Objective hyperparameters can lead to suboptimal behavior, such as memorizing random labels or reducing robustness. Future work will explore generalizing CEB beyond two observed variables. The MNI criterion and CEB address issues like miscalibration, failure at OoD tasks, vulnerability to adversarial examples, and dataset memorization by reducing the retention of training data in the learned representation. The geometry of optimal surfaces for CEB and IB models is illustrated in Figure 4, showing the exchange rate between bits of information. Achieving the MNI point requires 2 bits of information exchange. The MNI point is achieved by exchanging 2 bits of information. The value of \u03b2 that hits the MNI point is \u03b2 = 2, determining the model's pareto-optimal frontier. The equivalence between CEB and IB is shown in Figure 4, illustrating the exchange rate of information. The geometry of the pareto-optimal frontier is determined by the function (\u03b2) for a model and dataset. If an IB model and a CEB model have the same value of > 0 at equivalent \u03b2, the CEB model will yield a value of I(Y; Z) closer to I(X; Y) due to lower tangent line slopes. This explains why V IB 0.5 fails to capture as much information as the CEB model. The CEB model outperforms V IB 0.5 in capturing necessary information, especially at the pareto-optimal frontier. The difference in performance is likely due to various factors, such as the ease of training continuous conditional distributions. The analysis supports targeting the MNI point and treating CEB as an objective without hyperparameters. The MNI point, a point at (I(Y; Z) = I(X; Y), I(X; Z) = H(X)), and a point at (I(Y; Z) = 0, I(X; Z) = H(X|Y), are identified by the dataset. Selecting a point between these requires justification. Getting closer to the MNI point is only possible by setting CEB's \u03b2 = 1 due to convexity. Improvements can be made by adjusting the model, architecture, or dataset. IB and CEB models suggest inspecting training examples with high residual information for label noise detection. CEB's residual information can be used to measure quality. The feasible region for CEB collapses to a line segment I(X; Z|Y) = 0 with 0 \u2264 I(Y; Z) \u2264 I(X; Y). Similarly, the IB feasible region is the diagonal line I(X; Z) = I(Y; Z). Label-conditional generative models are easy to train in this case. It is never possible to learn a representation that exceeds the MNI, I(X; Z) \u2264 H(X) = I(X; Y). CEB is independent of the optimization methods used, focusing on variational objectives for simplicity. Any approach to optimize mutual information terms can work as long as they respect the bounds required by the objective. Some approaches in the literature aim to optimize mutual information terms, such as BID26, BID10, BID21, BID20, and BID34. However, these approaches do not fully meet the MNI criterion. Training algorithms can be designed without relying on validation set performance by minimizing Re X/Y on the training set. One simple training approach involves starting with a high initial learning rate and gradually decreasing it. The algorithm involves lowering the learning rate if the mean Re X/Y of the previous epoch is not less than the lowest Re * X/Y seen so far. This approach does not require a validation set and aims to improve the learned representation's performance. The algorithm involves lowering the learning rate if the mean Re X/Y of the previous epoch is not less than the lowest Re * X/Y seen so far. For the experiments, models have a core architecture with a Wide Resnet BID47 encoder and a two-layer MLP classifier. Stochastic models parameterize a multivariate Normal distribution for the latent representation. The models evaluated do not differ from Determ. None of the models use regularization. VIB models have an additional learned marginal, while CEB model has a backward encoder. Both CEB and VIB have a trained marginal for comparison. Any distributional family can be used for the encoder. Reparameterizable distributions are convenient. In this work, the choice of distributional family for the encoder is crucial and should match that of the decoder or be a mixture. Normal distributions were chosen for ease of use, but other distributions can be used depending on the dataset. No additional regularization was used on the deterministic model, as the 4-dimensional bottleneck acted as a strong regularizer. In the experiments, standard forms of regularization did not prevent the CW attack from being successful, and deterministic networks did not avoid memorizing the training set. The conditions for infinite mutual information in previous work do not apply to CEB or VIB. Numerical instability was almost non-existent in experiments using continuous representations. The CEB objective is symmetric and can be applied to tasks like conditional generation of X given Y = y by introducing a new variable Z Y. Variational bounds can be derived for H(Z Y |X) and H(X|Z Y), with p(z Y |x) bounded by e(z Y |x) and p(x|z Y) by a decoder distribution d(x|z Y). The CEB objective introduces a new variable Z Y for conditional generation of X given Y = y. The decoder distribution d(x|z Y) limits the information used to decode X, ensuring it cannot memorize X. The model can be turned into an unconditional generative model by sampling Y from the training data. The bidirectional CEB model aims to learn a unified Z that can handle both conditional generative models of Y and X. By ensuring consistency between Z X and Z Y through variational approximations, the model can generate either output given either input. The trained model focuses on learning a hierarchical model with CEB, aiming to maximize mutual information between variables in the hierarchy. This allows for training deep models in a principled manner. The Hierarchical CEB objective aims to minimize I(Z i\u22121 ; Z i |Y) to ensure consistency between network layers and data. This objective is stable and can reduce the number of parameters in the model if Z i have the same dimensionality. There is no requirement for Z i to have the same latent dimensionality. The authors define Predictive Information as the mutual information between past and future under an assumption of temporal invariance. They show that predictive information is a subextensive quantity, indicating that past observations contain minimal information about the future as the time window increases. The application of CEB to extract predictive information is straightforward. The application of CEB to extract predictive information involves learning a representation Z t that optimally covers the information in past and future sequences. This representation does not rely on past information when predicting future events and can be extended to a bidirectional model with two representations, Z <t and Z \u2265t, for observations before and after timestep t. Using the same encoder for both parts of the bidirectional CEB objective ties the two representations together. The bidirectional CEB objective involves using the same encoder for both parts to tie two representations together. Modeling choices for capturing remaining entropy in data include using a probabilistic RNN with powerful decoders per time-step or decoding the entire sequence from Z t in a single feed-forward network. This approach may be suitable for short prediction horizons or small predicted data in stochastic environments. Multi-scale sequence learning involves considering sequence learning at multiple temporal scales, combining architectures like time-dilated WaveNet with CEB. Unsupervised CEB learning is challenging due to the ill-posed nature of the problem without a defined task. The authors of BID6 preferred barely compressed representations, suggesting unsupervised learning may devolve into lossless compression. Unsupervised learning may devolve into lossless compression, making it challenging to determine what information should be kept in the learned representation. Defining a task is crucial to constrain the amount of information and identify important bits for downstream tasks. Unsupervised learning requires defining a task to constrain information in the representation. Selecting a function to transform the dataset into a new random variable can help in modeling useful bits for downstream tasks. Choosing the identity function results in maximal mutual information between variables, but may not be optimal for normal tasks. The selection of the transformation function is crucial for effective representation learning. The use of noise functions simplifies the selection of a transformation function for representation learning. By introducing a random noise variable U and a function X = f(X, U) to eliminate irrelevant information in the dataset X, denoising autoencoders can be used to reconstruct corrupted inputs effectively. The authors propose a formalization of the idea of reconstructing corrupted inputs in representation learning using the MNI criterion and deriving CEB as the optimal objective. They emphasize the importance of learning a representation consistent with uncorrupted inputs by using a bidirectional model with two encoders and decoders. This approach allows for a clean and consistent learned representation for downstream tasks. The authors propose a formalization of reconstructing corrupted inputs in representation learning using the MNI criterion and deriving CEB as the optimal objective. They emphasize the importance of learning a representation consistent with uncorrupted inputs by using a bidirectional model with two encoders and decoders. This approach allows for a clean and consistent learned representation for downstream tasks. The objective simplifies to a Noising CEB Autoencoder, predicting noisy X from X. The corresponding Semi-Supervised CEB is presented directly. The authors present a formalization of reconstructing corrupted inputs in representation learning using the MNI criterion and deriving CEB as the optimal objective. They show visualizations of the Fashion MNIST tasks, including a trained 2D CEB latent representation achieving \u223c92% on the test set. The CEB model organizes concepts closely related together, such as \"shirt\" classes near the center and \"shoe\" classes toward the lower right. The CEB model organizes adversaries into the \"trousers\" class, while the VIB models have adversaries mixed throughout. All models are better than the deterministic model, which cannot distinguish between adversaries and true examples."
}