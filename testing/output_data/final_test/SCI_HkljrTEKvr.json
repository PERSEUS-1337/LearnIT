{
    "title": "HkljrTEKvr",
    "content": "Unpaired image-to-image translation has seen success in handling multimodal challenges and limitations in existing approaches. The proposed Hierarchical Image-to-image Translation (HIT) method addresses these issues by formulating the problem in a semantic hierarchy structure to control multimodal uncertainty. Domain-specific variations are viewed as multi-granularity properties, allowing for control over the granularity of multimodal translation. The proposed method, Hierarchical Image-to-image Translation (HIT), aims to improve multimodal translation by dividing domains into subdomains to capture variations. By leveraging the inclusion relation among domains, distributions of parent and children are constrained to be nested. Experimental results show promising performance in image-to-image translation tasks. In recent years, significant progress has been made in attribute and category transfer tasks in image processing, thanks to advancements in generative adversarial nets (GANs). Various methods like pix2pix, UNIT, CycleGAN, DiscoGAN, DualGAN, and DTN have been developed for this purpose. The focus has shifted towards addressing challenges like involving translation among multiple domains in one model, which is a practical need for users. StarGAN and BicycleGAN are two models that address challenges in image translation tasks. StarGAN uses one generator to transform images across different domains efficiently, while BicycleGAN tackles the multimodal nature of translation tasks by providing multiple possible outputs for a single input. Recent works such as BicycleGAN, MUNIT, and DRIT have successfully modeled continuous and multivariant distributions for domain-specific information in translation tasks. They aim to combine multi-domain and multimodal translation abilities into one model for diverse and high-quality results. The curr_chunk discusses the natural hierarchical relationships between categories in multi-domain and multimodal translation models. It explains how categories like cat, dog, and bird share common attributes, and how finer-grained categories can be created based on visual variations. The text also mentions the horizontal and vertical views of multi-domain and multimodal issues in image translation tasks. The curr_chunk discusses a Hierarchical Image-to-image Translation (HIT) method for translating object images among multiple category domains and their children domains by modeling variations in Gaussian distributions in a common space. This approach differs from previous methods by considering multiple continuous and multivariant Gaussian distributions in a common space. The curr_chunk discusses improving the traditional conditional GAN framework to possess hierarchical discriminability for translating object images among multiple category domains and their children domains. The approach involves modeling variations in Gaussian distributions in a common space, with distributions nested between parent and children domains. Experiments validate the competitive performance of the method against state-of-the-arts. The curr_chunk discusses the advancements in Generative Adversarial Networks (GANs) with improved stability through better network architectures, distribution metrics, and normalization schemes. These enhancements have enabled GANs to be applied to various conditional tasks such as image generation, super resolution, text-to-image, 3D reconstruction, and image-to-image translation. Pix2pix is highlighted as the first unified framework for image-to-image translation. The curr_chunk discusses different methods for image-to-image translation using conditional GANs, including UNIT, DiscoGAN, DualGAN, and CycleGAN. These methods aim to address the need for pairwise supervision information between domains. Recent works in this area focus on multi-domain and multimodal translation, with StarGAN being a notable advancement in handling translation among several domains in one generator. To address multi-domain and multimodal translation, recent advancements include StarGAN for handling translation among multiple domains in one generator, BicycleGAN for modeling continuous distributions, MUNIT and DRIT for diverse translation results from unpaired data, and Chen et al.'s interpolation of latent codes for generating diverse images. In contrast, the focus is on achieving multi-domain and multimodal translation in one model using natural hierarchical relationships among domains defined by category or attribute. Hierarchical learning is a natural way for humans to understand objects, moving from abstract to detailed. Generative models are used to disentangle factors from low to high-level representations. In natural language processing, a probabilistic word embedding method captures semantics described by the WordNet hierarchy. The goal is to estimate conditional probabilities for image translation among multiple domains and modalities. Our goal is to estimate conditional probabilities for image translation among multiple domains by disentangling content and style parts using an image-to-image translation model. The model assumes that the content part is shared across domains, while the style part is domain-specific. This allows for translating images to different domains by sampling from a continuous distribution and using a deterministic decoder. The proposed method aims to preserve the complex spatial structure of objects in low-dimensional vectors while focusing on discriminative domain-specific information. It involves modeling Gaussians of styles for all domains in a common space to generate target images. The framework consists of five modules: an encoder, domain distributions modeling module, decoder, discriminator, and hierarchical classifier. The encoder extracts domain-irrelevant and domain-specific features, which are then used by the decoder to reconstruct images. The decoder reconstructs inputs using domain distributions modeled in a common space based on a semantic hierarchy structure. It combines domain-irrelevant features and styles to translate to the target domain, guided by adversarial and hierarchical classification losses. Multi-domain translation occurs horizontally among categories, while multimodal translation is vertical within categories. The nested relationship allows for translation across conditional distributions. In this paper, the domain is modeled as nested Gaussian distributions in a common space for Hierarchical Image-to-image Translation (HIT) between any two domains. The framework includes adversarial loss, hierarchical cross-entropy loss, nested loss, and bidirectional reconstruction losses to ensure accurate image translation. The relationship between parent and child nodes in the hierarchy is defined as a partial order relation. The concept of nested (encapsulation) in taxonomy is discussed, where one category is nested within another. The loss in maintaining this nested relation between probability densities is quantitatively measured using a nonnegative threshold. Computing this measurement is challenging for most distributions, including Gaussians. The loss in maintaining the nested relation between probability densities is quantitatively measured using a nonnegative threshold, specifically using the KL divergence for Gaussians. The penalty between positive and negative pairs of distributions is minimized to learn nested distributions for domains in a hierarchy. Our HIT model includes an adversarial loss, hierarchical classification loss, and general reconstruction losses to improve image quality and classify generated images. The adversarial loss helps match generated images to real data, while the hierarchical classification loss distinguishes domains in a hierarchy. The hierarchical classification loss aims to distinguish domains in a hierarchy by summing up losses of all levels above a certain level. Additionally, bidirectional reconstruction loss encourages meaningful information encoding and inverse reconstruction between G and E through image and feature reconstruction. The model also includes a distribution modeling module for each domain to learn a pair of mean vector and diagonal covariance matrix of Gaussian. The model includes a distribution modeling module for each domain to learn mean vector and covariance matrix of Gaussian. Experiments conducted on datasets like CelebA, ImageNet, and ShapeNet show improved style transfer between real images. CelebA dataset provides over 200K face images with 40 attribute annotations, organized in a hierarchy based on gender, age, and hair color. Images from super domains like house cats, dogs, and big cats are collected from ImageNet. ShapeNet consists of 51,300 3D models covering 55 common and 205 finer-grained categories, with each model having 12 2D images. A three-level hierarchy of furniture, including tables and sofas, is defined. The evaluation metrics used in this paper include Inception Score (IS), Fr\u00e9chet Inception Distance (FID), and Learned Perceptual Image Patch Similarity (LPIPS). Baselines compared are the multi-domain method StarGAN and the multimodal method MUNIT. The paper compares the multi-domain method StarGAN and the multimodal method MUNIT on various domain pairs. StarGAN excels in fine-grained translations among attribute domains, while MUNIT shows competitive results in quantitative comparisons with baselines. The paper compares the multi-domain method StarGAN and the multimodal method MUNIT. MUNIT's image quality on CelebA is unsatisfactory, possibly due to its reliance on adversarial learning for fine-grained attribute differences. MUNIT excels in diversity performance by involving two domains in one model with a triplet of backbone components. The method in question considers both multimodal and multi-domain translation within one triplet, striking a balance between image quality and diversity. Our HIT achieves promising qualitative results in translation among categories with large variations, showing good disentanglement of content and style in generated images. The fixed styles from specific category distributions result in images with similar styles but dissimilar content appearances. As we move towards leaf-level, translated images have fewer variations with more specified conditions by higher-level categories. Results in Fig.6 validate the nested relationship of styles in different levels, with distributions in low levels being local modes of ancestor domains in high levels. An experimental analysis in the Appendix explores parameter-sensitiveness of m and \u03b1 in constraining nested relationships among distributions. Smooth translation via interpolations between styles from different attribute domains is possible, and additional style adversarial loss enhances controlled translation using referenced real image styles. Fig.7(b) showcases example results demonstrating the semantics of gender, age, and hair color. The proposed Hierarchical Image-to-image Translation (HIT) method successfully transfers gender, age, and hair color semantics to input images. The method achieves granularity controlled translation objectives by incorporating multi-domain and multimodal translation into one model. However, a limitation is the assumption of a single Gaussian for each category domain, which may not be suitable for datasets with large variations within domains. The parent distributions should be a mixture of Gaussians. In this paper, the ShapeNet model is discussed, focusing on the challenge of sparse sampling around parent distribution centers. The idea of using a mixture of Gaussians to capture nested relationships between parent and children distributions is explored. However, computing the KL divergence between two mixture of Gaussians is difficult, and the re-parameterization trick for distribution sampling during optimization is not easily applicable. Future research directions include finding better assumptions for nested relationships. The Adam optimizer with specific parameters is used for training HIT on all datasets. In this section, the training process involves datasets for 300K iterations with a learning rate decay every 100K iterations. Batch size is set to 16, and specific loss weights and parameters are empirically determined. Random mirroring is applied during training. Figures 8, 9, and 10 illustrate nested relationships among categories in different hierarchy levels, using CelebA as an example. The root category \"face\" branches into gender attributes, then age attributes, and finally hair color categories. Samples within each category show intraclass variations. The impacts of hyper-parameters in nested distribution learning on word embedding tasks were studied in a previous work. In this section, an analysis is made on their effects in the current image generation task. The distribution margin parameter has a larger impact than the nested threshold parameter. A trade-off value of 200 is set for the distribution margin parameter, while a relative small or large value performs well for the nested threshold parameter in terms of image quality. In the image generation task, setting \u03b1 to a value in the left half axis is recommended to avoid too much relaxation of the nested constraint. A value of 50 for \u03b1 with a ratio of 1:4 between \u03b1 and m is found to be consistent with previous observations."
}