{
    "title": "BJg_FgBtPH",
    "content": "Open-domain dialogue generation has gained attention in Natural Language Processing. Holistic dialogue evaluation is essential, with human ratings as the gold standard. Automated evaluation methods are desired due to inefficiency and cost. This paper proposes evaluation metrics capturing quality and diversity using GPT-2 context coherence, fluency, and $n$-gram diversity. The metrics show strong correlation with human judgments, aiding in developing artificial general intelligence. Dialogue generation is crucial for developing artificial general intelligence. Recent research has focused on open-domain dialogue systems, with applications in industry like Microsoft's Xiaoice and Baidu's Dumi. Evaluating dialogue models efficiently is a challenge, with human-based rating being the prevalent method but deemed impractical due to variations in models and hyperparameters. Previous automatic evaluation metrics have mainly focused on dialogue quality and diversity. The evaluation approach proposed in this work introduces holistic metrics that consider context coherence, language fluency, and response diversity in generated dialogues. GPT-2 is suggested for measuring quality, while n-gram based entropy is used to assess response diversity. The proposed evaluation metrics measure response diversity under augmented queries with controlled diversity. Human evaluations validate the metrics, which align well with heuristics used in language generation tasks like machine translation and summarization. These metrics, such as BLUE for machine translation and ROUGE for summarization, are suitable for tasks with low conditional entropy. The current chunk discusses the limitations of traditional metrics like BLEU, METEOR, and ROUGE in evaluating dialogue generation due to their inability to capture semantic similarity. Researchers have found a disconnect between these metrics and human judgments, highlighting the need for more advanced evaluation methods. The current chunk introduces a new approach called RUBER for evaluating open-domain dialogue systems. It combines a referenced metric that measures similarity between model-generated and reference responses using word embeddings, and an unreferenced metric that captures relevance between query and response. This blended metric aims to address the limitations of traditional metrics like BLEU, METEOR, and ROUGE in capturing semantic similarity. The RUBER approach combines referenced and unreferenced metrics to evaluate open-domain dialogue systems. It uses a neural network classifier to determine response appropriateness, with positive examples as references and negative examples randomly chosen from the dataset. Ghazarian et al. (2019) improved RUBER by incorporating contextualized embeddings from BERT, showing reduced correlation with human judgments, reflecting the diversity of acceptable responses in open-domain dialogue. The current work proposes metrics to evaluate the quality and diversity of open-domain dialogue generation, focusing on coherence to the query. This approach differs from prior work that measures coherence based on a binary classifier, instead exploring language modeling for evaluation. In contrast to using a binary classifier, this work explores language modeling to evaluate the coherence and fluency of responses to a query. The response coherence metric is based on the conditional probability of the response given the query, while response fluency is measured by negative perplexity. Additionally, diversity in text generation is assessed using n-gram based metrics like unigram entropy. In contrast to binary classifiers, this study focuses on language modeling to assess response coherence and fluency to a query. Diversity in text generation is evaluated using n-gram based metrics like unigram entropy. Mou et al. (2016) found that measuring diversity solely based on generated utterances may not be accurate. Instead, they propose evaluating diversity with controlled queries to ensure varied responses. Controlled queries involve minimizing diversity in meaning and word use, preventing dialogue models from producing repetitive responses. Ideal models exhibit diverse word choices and sentence structures. In this work, a metric is proposed to evaluate open-dialogue models based on both quality and diversity of generated dialogues. The metric considers context coherence, response fluency, and diversity measures using WordNet substitution and Conditional Text Generator. Empirical results show that language model-based metrics outperform previous methods in capturing quality and diversity in generated dialogues. Two effective approaches are proposed for generating diverse utterances: word substitution and text generator with k-best decoder. The curr_chunk discusses the correlation between diversity metric and human judgments on response diversity, proposing datasets to improve human evaluation. It also mentions GPT-2 as a language model capturing coherence in dialogue. The datasets, human ratings, and metric implementation are released as open-source contributions for further research. GPT-2 can be fine-tuned on dialogue datasets to capture query-response dependence. Context coherence is defined as loglikelihood of response given query, normalized by response length. Normalized score ranges from 0 to 1, with lower bound defined as 5th percentile to avoid extreme values. The response fluency score is normalized to a range of 0 to 1 using a pretrained language model, GPT-2. Response diversity is measured by augmenting queries with controlled diversity through WordNet Substitution (WS) and Conditional Text Generator (CTG) methods. These approaches aim to provide sentences close in meaning but slightly different in word use to avoid feeding identical inputs to dialogue models. WordNet Substitution (WS) is a word-level manipulation method that generates four augmented inputs by substituting verbs, nouns, adjectives, and adverbs with synonyms. In contrast, Conditional Text Generator (CTG) uses a sequence-to-sequence or transformer model to produce augments conditioned on the context of prior utterances in multi-turn datasets. The CTG model evaluates the top-5 beams with the concatenated utterance history to provide responses with controlled diversity. The study utilizes a DailyDialog dataset for empirical analysis, containing high-quality multi-turn dialogue pairs. A seq2seq model with attention was trained using OpenNMT, consisting of a 2-layer LSTM with 500 hidden units on both the encoder. The study used a seq2seq model with a 2-layer LSTM for training. Responses were generated using top-k sampling with different values. GPT-2 models of varying sizes were tested, with fine-tuning on training data. WordNet substitution and conditional text generator were used for query diversity. The perplexity of the fine-tuned language model on the test dataset was 16.5. The study utilized a seq2seq model with a 2-layer LSTM for training and tested GPT-2 models of different sizes. WordNet substitution and conditional text generator were employed for query diversity. A Transformer model was trained on training and validation data for query augmentation. Human ratings were collected via Amazon Turk to assess the quality of generated query-response pairs. 200 datapoints were selected for evaluation on Context Coherence and Fluency metrics. After collecting human ratings via Amazon Turk, 500 responses were evaluated for Context Coherence, Fluency, and Diversity metrics. Pearson Correlation was computed to validate the metrics and datasets. Results showed disagreement between coherence metric, human evaluation, and RUBER for a specific response. The study compared a new context coherence metric with RUBER and found higher correlation with human judgments. Fine-tuning a language model on the dialogue dataset improved results, showing better dependency capture. Even without fine-tuning, the language model-based metric outperformed RUBER in correlating with human ratings. The study found that fine-tuning correlated with human ratings better than RUBER. Inter-rater reliability results also supported the context coherence metric. A case study showed disagreement between the coherence metric and RUBER in evaluating response coherence. The study compared the fluency metric with human judgments, showing high correlation. Fine-tuning GPT-2 improved Pearson correlation from 0.43 to 0.82. Outliers were corrected, and human ratings showed high consistency. The study evaluated ratings with and without fine-tuning of GPT-2, showing higher correlations with human ratings. Datasets were assessed using WS and CTG, with higher correlations in n-gram entropy compared to baseline. WS and CTG datasets displayed clustered datapoints and slopes closer to 1, consistent with reported correlations. Inter-rater reliability was also examined. The study evaluated inter-rater correlations and variance in human ratings for different datasets. WS and CTG datasets showed high correlations, while the Baseline Dataset had poor inter-rater correlations due to diverse model outputs. The evaluation metric on the datasets can reveal diversity of a dialog system consistent with humans. This paper introduces a comprehensive evaluation method for open-domain dialogue models, focusing on both response quality and diversity. GPT-2 is used to assess fluency and coherence, while controlled diversity is measured through WordNet Substitution and Conditional Text Generator. The proposed metrics align well with human judgments, and resources are provided to support further research in this area. The goal is to enhance comparability among open-domain dialogue systems."
}