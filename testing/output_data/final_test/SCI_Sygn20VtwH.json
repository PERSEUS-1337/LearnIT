{
    "title": "Sygn20VtwH",
    "content": "Metagross is a new neural sequence modeling unit with recursive parameterization of gating functions. The unit controls its gating mechanisms through instances of itself in a recursive manner, providing benefits for learning hierarchically-structured sequence data. Extensive experiments show its utility in various tasks, achieving state-of-the-art performance. The proposed approach aims to achieve state-of-the-art performance on various tasks by designing effective sequential inductive biases. Models with gating functions, such as recurrent models, have shown significant impact in applications like natural language processing and speech. These gating functions provide fine-grain control over temporal compositionality and are typically static and trained through alternate transformations over the original input. The proposed method, Meta Gated Recursive Controller Units (METAGROSS), introduces a new sequence model with recursive parameterization for deep reasoning about inputs. It utilizes a soft dynamic recursion mechanism to learn the depth of parameterization per token, enabling meta-gating for temporal compositionality control at different abstraction levels. This approach combines the benefits of recursive reasoning with recurrent models, particularly useful for modeling hierarchical data. The work introduces a new sequence model, METAGROSS, with recursive parameterization for deep reasoning about inputs. It proposes a soft dynamic recursion mechanism for controlling compositional flow, combining recursive reasoning with recurrent models for modeling hierarchical data. Key contributions include a distinct recursive parameterization of recurrent gates. METAGROSS is a recurrent model with a soft dynamic recursion mechanism that learns the recursive depth at a token-level. It achieves state-of-the-art performance on various sequence modeling tasks. The main unit of the model is defined as h t = Metagross n (x t , h t\u22121 ) with a nonlinear activation function \u03c3 r like tanh. The Metagross unit recursively calls itself until a max depth L is reached, with f t and o t parameterized by forget and output gates at the maximum depth. A residual connection h n t = h t + x t aids gradient flow. Learning the depth of recursion is data-driven using \u03b1 t , \u03b2 t , with a linear transformation layer F * (xt) = W x t + b. \u03b1, \u03b2 control recursion extent for a soft depth hierarchy. Static variation computes \u03b1, \u03b2 based on global information. METAGROSS can be used as a non-autoregressive parallel model with trainable scalar parameters \u03b1, \u03b2. It offers speed benefits compared to recurrent models like Transformer. The non-autoregressive variant eliminates the need for previous hidden states and can replace position-wise feed-forward layers. METAGROSS is a non-autoregressive parallel model with trainable scalar parameters \u03b1, \u03b2, offering speed benefits over recurrent models like Transformer. It eliminates the need for previous hidden states and can replace position-wise feed-forward layers. The model is evaluated on tasks like sorting sequences and tree traversal. METAGROSS is a non-autoregressive parallel model with trainable scalar parameters \u03b1, \u03b2, offering speed benefits over recurrent models like Transformer. It eliminates the need for previous hidden states and can replace position-wise feed-forward layers. The model is evaluated on tasks like sorting sequences and tree traversal. The constructed trees have random sparsity with varying depths, requiring hierarchical structure and long-term reasoning. In the logical inference task, semantic equivalence of statements with logic operators is determined using a dataset proposed in (Bowman et al., 2014). The model METAGROSS is evaluated on tasks like sorting sequences and tree traversal, as well as logical inference with logic operators. Results show that while all models solve tasks with n = 3, challenges increase with longer sequences, remaining difficult for neural models. METAGROSS outperforms baselines, especially in perplexity. S-BiLSTMs are better than LSTMs, while ON-LSTMs excel in sorting tasks. METAGROSS achieves state-of-the-art performance in logical inference, surpassing other models. Our model METAGROSS achieves state-of-the-art performance on the CIFAR-10 dataset, outperforming the recent Trellis Network. It also outperforms a wide range of other published works on the MNIST dataset while having significantly fewer parameters than the Trellis Network. This demonstrates that METAGROSS is a competitive long-range sequence encoder. Our experiments show that METAGROSS, a competitive long-range sequence encoder, outperforms all competitor approaches in Semantic Parsing and Code Generation tasks. Specifically, TranX + METAGROSS achieves state-of-the-art performance, demonstrating ablative benefits. Additionally, experiments on IWSLT datasets confirm the superior performance of METAGROSS compared to published results. In experiments, METAGROSS outperforms competitors in Semantic Parsing and Code Generation tasks. The Transformer networks were adapted with a non-autoregressive version of METAGROSS. Model BLEU scores show competitive results compared to other models. Experimental results on Neural Machine Translation on IWSLT 2015 En-Vi show that METAGROSS performs well in polyphonic music modeling. It outperforms other models in tasks evaluated on Negative Log-likelihood (NLL) using datasets like Nottingham, JSB Chorales, and Piano Midi. METAGROSS achieves lower NLL scores compared to GRU, LSTM, G2-LSTM, B-LSTM, and TCN models. METAGROSS achieves state-of-the-art performance on polyphonic music modeling tasks, outperforming competitive models like Gumbel Gate LSTMs. Ablation studies show that the base unit and maximum depth used by METAGROSS vary depending on the task, with different units performing better on different datasets. The optimal base unit and maximum depth of METAGROSS vary depending on the task, as shown in Table 9. Figure 6 illustrates the dynamic recursion depth on CIFAR and MNIST datasets, reflecting how the parameter tree is constructed during training. METAGROSS builds different trees for each dataset, with diverse construction of recursive parameters observed across all datasets. The recursive gates fluctuate more on CIFAR compared to Music modeling. Our model adjusts dynamically on different datasets, with recursive gates remaining constant on MNIST. The network structure changes rhythmically on CIFAR and Music datasets, adapting to the complexity of the information processed. Effective inductive biases for sequential representation learning have been a fruitful research area, leading to advancements in gated recurrent models and convolution techniques. Recent advancements in sequential representation learning have led to the development of various models, from gated recurrent models to self-attention based models. Researchers have explored recursive networks and proposed methods like Tree-LSTMs for syntax-guided composition in language processing. Structured gating mechanisms, such as Ordered Neuron LSTMs, have been introduced to imbue recurrent units with tree-structured inductive bias. Efforts have also been made to learn composition processes without guidance or syntax-based supervision. Recent advancements in sequential representation learning have led to the development of various models, from gated recurrent models to self-attention based models. Researchers have explored recursive networks and proposed methods like Tree-LSTMs for syntax-guided composition in language processing. Efforts have also been made to imbue recurrent units with tree-structured inductive bias, showing the importance of recurrence in modeling hierarchical structure. Various approaches, such as learning hierachical representations across multiple time-scales and exploring the abstraction and controller over a base recurrent unit, have demonstrated reasonable success. Additionally, recent works have focused on speeding up the recurrent unit and exploring the marriage of recursive and recurrent models. Recent advancements in sequential representation learning have led to the development of various models, from gated recurrent models to self-attention based models. Researchers have explored recursive networks and proposed methods like Tree-LSTMs for syntax-guided composition in language processing. Efforts have also been made to imbue recurrent units with tree-structured inductive bias, showing the importance of recurrence in modeling hierarchical structure. Various approaches, such as learning hierarchical representations across multiple time-scales and exploring the abstraction and controller over a base recurrent unit, have demonstrated reasonable success. Additionally, recent works have focused on speeding up the recurrent unit and exploring the marriage of recursive and recurrent models. Notion of recursive and recurrent architectures is notable, with differences in proposed methods like Recursive Recurrent Networks for machine translation and RRNet for learning hierarchical structures dynamically. AlvarezMelis & Jaakkola proposed doubly recurrent decoders for tree-structured decoding, similar to our model which focuses on learning gating controllers. Our work introduces Meta Gated Recursive Controller Units (METAGROSS), a sequence model with recursive parameterization of gating functions. This method achieves competitive results across various benchmarks in language, logic, and music. We also propose a non-autoregressive variation of METAGROSS, enhancing state-of-the-art Transformers. The network learns a dynamic recursive parameterization, showcasing its expressiveness. The network demonstrates a dynamic recursive parameterization, highlighting its flexibility to learn dynamic parameter structures based on the data."
}