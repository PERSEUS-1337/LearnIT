{
    "title": "SyJ7ClWCb",
    "content": "This paper explores defense strategies against adversarial attacks on image-classification systems by transforming inputs before feeding them to the system. Image transformations like bit-depth reduction, JPEG compression, total variance minimization, and image quilting are studied. Total variance minimization and image quilting are effective defenses, especially when the network is trained on transformed images. These defenses are non-differentiable and random, making it challenging for adversaries to bypass them. The best defense eliminates a significant percentage of strong gray-box and black-box attacks. Intelligence in security-sensitive applications requires robustness to adversarial attacks, which current machine learning models are vulnerable to. Adversarial examples have been used to attack image classification, speech recognition, and robot vision systems. Defenses are being developed to increase robustness against these attacks, considering model properties and problem characteristics. Defenses against adversarial attacks fall into two main categories: model-specific strategies that enforce model properties and model-agnostic defenses that try to remove adversarial perturbations from input. Model-agnostic defenses like JPEG compression or image re-scaling have not been very effective, while model-specific defenses make strong assumptions about the adversary. In this paper, the focus is on enhancing model-agnostic defense strategies by developing approaches to remove adversarial perturbations from input images while maintaining classification accuracy. Various transformations such as image cropping, rescaling, JPEG compression, total variance minimization, and image quilting are explored for their effectiveness against existing attacks. The study focuses on enhancing defense strategies against adversarial attacks in image recognition systems. Image transformations like total variation minimization and image quilting are effective in countering attacks, even in gray-box settings. These non-differentiable and random defenses eliminate a significant percentage of attacks, perturbing pixel values by 8% on average. An adversarial attack in image recognition involves generating perturbed images to deceive a classifier. The success rate of the attack is measured by the proportion of altered predictions, using normalized L2-dissimilarity. Strong attacks have a high success rate with significant perturbations. In image recognition, adversarial attacks aim to deceive classifiers by generating perturbed images. The success rate of a strong attack is high with low normalized L2-dissimilarity. Adversaries may use black-box or gray-box attacks, with the latter having access to model architecture and parameters but unaware of the defense strategy. Defense strategies aim to maintain predictions on adversarial examples equal to clean examples. Image transformation defenses involve complex, nondifferentiable, and potentially stochastic functions. Adversarial attacks in image recognition involve complex, nondifferentiable functions like the fast gradient sign method (FGSM) and iterative FGSM. Other attacks aim to minimize the Euclidean distance between input and adversarial examples, such as DeepFool for binary classifiers. The DeepFool attack projects x onto the decision boundary defined by h(\u00b7) for M iterations, particularly suited for ReLU-networks. Carlini-Wagner's L2 attack combines a differentiable surrogate for classification accuracy with an L2-penalty term. CW-L2 finds a solution to the optimization problem with a margin parameter \u03ba and perturbation norm trade-off parameter \u03bbf. The text discusses adversarial attacks on images, where perturbations are made to alter model predictions. Various attacks are performed using different techniques such as image cropping, bit-depth reduction, JPEG compression, total variance minimization, and image quilting. The goal is to investigate if these image transformations can undo the effects of the adversarial attacks. The text explores adversarial attacks on images using techniques like image cropping, bit-depth reduction, JPEG compression, total variance minimization, and image quilting to investigate their effects on model predictions. The text discusses various methods to alter adversarial perturbations in images, including cropping, rescaling, bit-depth reduction, JPEG compression, and compressed sensing with pixel dropout. These techniques aim to remove small variations in pixel values to improve model predictions. The reconstructed image removes adversarial perturbations by selecting pixels using Bernoulli random variables and applying total variation minimization to construct a simple image with minimal fine-scale variation. In our implementation, we set p = 2 and use a special solver based on the split Bregman method for total variance minimization. TV minimization effectively removes adversarial perturbations in images. Image quilting is a technique that synthesizes images by piecing together small patches from a database. Image quilting is a technique used to remove adversarial perturbations in images by constructing a patch database of \"clean\" images and selecting patches from the database to create a synthesized image. This defense ensures that the resulting image only consists of pixels that were not modified by the adversary. The effect of image quilting on adversarial images is illustrated in Figure 2. The absolute differences between quilted original and quilted adversarial images appear smaller in non-homogeneous regions, suggesting different defenses from TV minimization. Five experiments were conducted to test the efficacy of the defenses, including gray-box and black-box attack scenarios. The experiment in Section 5.4 combines defenses with ensembling and model transfer, while Section 5.5 investigates attacking networks trained on image-transformations in a gray-box setting. Section 5.6 compares defenses with prior work on the ImageNet dataset using adversarial attacks on a ResNet-50 model. Code to reproduce results is available at https://github.com/facebookresearch/adversarial_image_defenses. Our defense strategies against adversarial attacks are measured in terms of normalized L2-dissimilarity. Classification accuracies are reported based on this metric. Different attacks have varying levels of normalized L2-dissimilarity, which can be adjusted by changing parameters such as step size or perturbation. Hyperparameters for our defenses are fixed across all experiments. The study used a quilting patch size of 5\u00d75 and a database of 1,000,000 patches randomly selected from the ImageNet training set. Different experiments used nearest neighbor patch selection with K=1 or K=10. In the cropping defense, 30 crops of size 90\u00d790 were sampled from the 224\u00d7224 input image, rescaled, and averaged for model predictions. Results of ResNet-50 accuracy on transformed adversarial images for four attacks were shown in FIG2, with different transformations applied at test time. The ResNet-50 model is successfully attacked by four adversaries in most cases, with some image transformations helping to reduce the impact of the attacks. Ensembling 30 predictions over random image crops improves accuracy to 40-60%, showing susceptibility of adversarial examples to changes in location and scale of perturbations. Image transformations like total variation minimization and image quilting can successfully defend against adversarial examples from various attacks, improving classification accuracy by 30-40%. While image quilting maintains its defense effectiveness regardless of the adversary's strength, it does impact the model's accuracy on non-adversarial images. The high performance of image cropping-rescaling may be due to the network being trained on such images, highlighting the importance of transformations in mitigating adversarial attacks. The study investigates the impact of image transformations on defense effectiveness against adversarial attacks. ResNet-50 models were trained on transformed ImageNet training images using various techniques like bit-depth reduction, JPEG compression, TV minimization, and image quilting. The classification accuracy of the networks was measured on adversarial images, assuming a black-box setting. Results show that the network is more robust to image cropping-rescaling than other transformations, affecting the effectiveness of defenses even after removing adversarial perturbations. In experiments testing defense effectiveness against black-box attacks, ResNet-50, ResNet-101, DenseNet-169, and Inception-v4 networks were used. Image-quilting defense proved highly effective, successfully defending against 80-90% of attacks even at high dissimilarity levels. Ensembling different defenses and transferring attacks to different networks were also evaluated. Ensembling different defenses and transferring attacks to various network architectures in a black-box setting showed a 1-2% increase in classification accuracy. The experiment involved using ResNet-50, ResNet-101, DenseNet-169 BID17, and Inception-v4 BID33 networks with image quilting and TVM defenses. In experiments, gains of 1-2% in classification accuracy were achieved by ensembling different defenses, while transferring attacks to various network architectures led to a 2-3% improvement. Inception-v4 performed the best, but this could be due to its higher accuracy in non-adversarial settings. The best black-box defense reached 71% accuracy against all four defenses, with attacks reducing accuracy by up to 6%. Image transformations were effective against adversarial images, especially when networks were re-trained to be robust to these transformations. In a gray-box setting, the adversary has access to networks but not input transformations at test time. Four attack methods were used to create novel adversarial images against transformation-robust networks. Results show bit-depth reduction and JPEG compression are weak defenses. Image cropping, rescaling, total variation minimization, and image quilting are robust defenses in the white-box setting, classifying up to 50% of adversarial images correctly. Defenses were compared with ensemble adversarial training, showing gains in classification accuracy and effectiveness against attacks. Ensemble adversarial training fits convolutional network parameters on diverse adversarial examples to enhance robustness. Experiments compared a model trained on FGSM-generated adversarial examples with ResNet-50 models using various defenses. Differences in assumptions between ensemble adversarial training and defense strategies were noted. The study compares defense strategies against adversarial attacks, showing that ensemble adversarial training performs better on FGSM attacks but is outperformed by transformation-based defenses on other attacks. Input transformations significantly outperform ensemble adversarial training against iterative attacks, with defenses being 18-24 times more robust. Combining cropping, TVM, and quilting increases defense accuracy against DeepFool gray-box attacks. The study found that combining cropping, TVM, and quilting significantly improves defense accuracy against DeepFool gray-box attacks, reaching 51.51% compared to 1.84% with ensemble adversarial training. Image transformations can remove adversarial perturbations while preserving visual content by training the network on transformed images. Different transformations have varying effectiveness, with the best defense against each attack highlighted in bold. The study suggests using non-differentiable and randomized input transformations as effective defenses against attacks. Total variation minimization and image quilting are highlighted as strong defenses due to their non-differentiable nature and randomization. These defenses make it difficult for adversaries to incorporate transformations in their attacks. Randomized defenses like total variation minimization and image quilting make it challenging for attackers to find perturbations that alter predictions across a distribution of images. These defenses are considered stronger than deterministic denoising methods like bit-depth reduction or JPEG compression. Defenses like total variation minimization and image quilting are more effective against attacks compared to adversarial-training approaches. Transformation-based defenses are model-agnostic and generalize well across different attack methods. Similar defenses could be useful in domains like semantic segmentation and speech recognition. Total variance minimization can be applied in speech recognition to remove perturbations from waveforms. In future work, the study aims to explore new attack methods designed to bypass input-transformation defenses and investigate combining input-transformation defenses with ensemble adversarial training."
}