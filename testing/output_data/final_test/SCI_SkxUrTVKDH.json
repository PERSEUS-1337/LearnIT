{
    "title": "SkxUrTVKDH",
    "content": "In this paper, a new approach based on differential inclusions of inverse scale spaces is proposed to generate a family of models from simple to complex ones by coupling gradient descent and mirror descent. The Split Linearized Bregman Iteration (SplitLBI) is introduced, showing global convergence in deep learning. Experimental evidence suggests that SplitLBI may achieve state-of-the-art performance. Experimental evidence suggests that SplitLBI can achieve top performance in large scale training on ImageNet-2012 dataset. It reveals effective subnet architecture with comparable test accuracies to dense models after retraining instead of pruning well-trained ones. Deep neural networks derive their expressive power from millions of optimized parameters using Stochastic Gradient Descent and variants like Adam. Over-parameterization aids in optimization and generalization, simplifying the landscape of empirical risks for efficient gradient descent towards global optima. Contrary to common belief, over-parameterization does not necessarily lead to bad generalization or overfitting. Compressive networks are desired in real-world applications like robotics and self-driving cars. Regularization techniques like L1 and L2 are used in deep learning to enforce sparsity on weights for compact networks. However, L1 regularization sacrifices prediction performance due to highly correlated weights violating conditions for sparse model selection. L2 regularization is often used for correlated weights as low-pass filtering. Regularization techniques like L1 and L2 are used in deep learning to enforce sparsity on weights for compact networks. Group sparsity regularization has also been applied to neural networks for optimal neuron grouping and structured sparsity. Researchers typically start with training a big model and then prune or distill it to a smaller size without sacrificing performance. Recent studies have focused on compressive deep learning by identifying subnetworks within over-parameterized models that achieve comparable accuracy with the original network. The question of whether it is necessary to fully train a dense model before finding important structural sparsity has been raised. This paper introduces a dynamic approach to deep learning with structural sparsity, establishing a family of neural networks ranging from simple to complex. The paper introduces a dynamic approach to deep learning with structural sparsity, establishing a family of neural networks from simple to complex. It designs dynamics that exploit over-parameterized models and structural sparsity by lifting original network parameters to a coupled pair. The dynamics, known as Split Linearized Bregman Iteration (SplitLBI), ensures global convergence guarantee. SplitLBI is a SGD extension with structural sparsity exploration, transitioning to sparse mirror descent with strong coupling. It allows rapid learning of important subnet architectures through the structural sparsity parameter \u0393, revealing key sparse subnet architectures early in training. This parameter helps find \"winning tickets\" in early epochs for the \"lottery\". The Linearized Bregman Iteration (LBI) was initially used in applied mathematics for image reconstruction and compressed sensing, later applied to logistic regression. Convergence analysis was done for convex problems, but remains open for non-convex problems in deep learning. Statistical model selection consistency was established for high dimensional linear regression under similar conditions as Lasso. SplitLBI was introduced to relax these conditions and learn structural sparsity. SplitLBI was proposed to learn structural sparsity in linear models under weaker conditions than generalized Lasso. It is now being used to train highly non-convex neural networks with structural sparsity, with global convergence analysis based on the Kurdyka-\u0141ojasiewicz framework. Global convergence of SplitLBI in nonconvex optimization is established, showing that the iterative sequence converges to a critical point of the empirical loss function from any initializations. SplitLBI demonstrates comparable or better performance than other training algorithms on large scale datasets like ImageNet-2012, with additional structural sparsity for interpretability. The structural sparsity parameters provide important information about subnetwork architecture with comparable accuracies to dense models. SplitLBI with early stopping can yield fast \"winning tickets\" without fully training over-parameterized models. The SplitLBI method shows superior performance on large datasets like ImageNet-2012, maintaining structural sparsity for interpretability. The learned filters are visualized at different epochs, demonstrating sparse selection without compromising accuracy. The supervised learning task involves mapping input space to output space by minimizing loss functions on training samples. The dynamics consider a sub-gradient for sparsity-enforced regularization. The SplitISS method introduces a regularization term for structural sparsity in neural networks, utilizing a variable splitting strategy. It involves a differential inclusion system for gradient descent of parameters and mirror descent for sparsity enforcement. The SplitISS method introduces a regularization term for structural sparsity in neural networks, utilizing a variable splitting strategy. In mirror descent flow, gradient descent operates in the dual space of sub-gradients, driving the flow in the sparse primal space. The solution path exhibits a separation of scales, with important parameters learning fast and unimportant parameters learning slowly. The inverse scale space property in neural network training shows statistical model selection consistency and reduced bias as \u03ba increases. Empirically, this property holds for highly nonconvex neural network training, where important sparse filters are selected early on. This contrasts with popular SGD which returns dense filters. SplitISS avoids parameter correlation issues in over-parameterized models, achieving model selection consistency under weaker conditions than generalized Lasso. It uses 2-regularization near the sparse path of \u0393 for weight parameter W instead of 1-sparsity. The Split Linearized Bregman Iteration (SplitLBI) with 2-regularization near the sparse path of \u0393 explores highly correlated parameters in over-parameterized models and sparsity regularization. It provides a simple discrete approximation using Euler forward discretization of dynamics. The SplitLBI method generates sparse models with a global convergence condition, using a group Lasso penalty for sparsity enforcement in convolutional neural networks. It provides a closed form solution for each convolutional filter, treating convolutional and fully connected layers differently. The SplitLBI method ensures global convergence by using a group Lasso penalty for sparsity in convolutional neural networks. It treats convolutional and fully connected layers differently, providing a closed form solution for each convolutional filter. The method guarantees convergence to a critical point of the loss function. The SplitLBI algorithm for single hidden layer networks with Gaussian inputs is rewritten as a Linearized Bregman Iteration. Global convergence is established under certain assumptions, including Lipschitz continuity and bounded level sets of the loss function. The Lyapunov function is also considered for convergence analysis. The Lyapunov function is a Kurdyka-\u0141ojasiewicz function on any bounded set, and Assumption 1 is regular in the analysis of nonconvex algorithms. The main theorem states the global convergence of SplitLBI under certain conditions. Typical examples for neural networks are also provided. Corollary 1 states that a sequence generated by SLBI for neural network training converges to a stationary point of L n (W) under certain conditions. Stochastic variants of SplitLBI and implementations are discussed, along with experiments showcasing the utilities of weight parameter W t and structural sparsity parameter \u0393 t in prediction and interpretability. The text discusses the use of SplitLBI with momentum and weight decay for neural network training with large datasets. The algorithm is applied to boost networks by sequentially growing them from simple to complex structures. Various algorithms are evaluated on different backbones like LeNet and AlexNet. Various algorithms are evaluated on different backbones like LeNet, AlexNet, VGG, and ResNet. Default hyper-parameters for Split LBI are specified for MNIST, Cifar-10, and ImageNet-2012 datasets. Batch sizes and data augmentation techniques are also mentioned for the different datasets. Sparsity is defined as the percentage of non-zero parameters in the models. The curr_chunk discusses the SplitLBI method for exploring over-parameterized models and compares different variants of SGD and Adam in experiments. SGD variants include Naive SGD, SGD with l1 penalty, and SGD with momentum. Learning rates are set at 0.1 for SGD and 0.001 for Adam, gradually decreasing by 1/10 every 30 epochs. SplitLBI achieves state-of-the-art performance on ImageNet-2012, Cifar-10, and MNIST compared to classical networks like LeNet, AlexNet, and ResNet. The structural sparsity parameter in SplitLBI explores important sub-network architectures for loss reduction and improved interpretability. SplitLBI achieves state-of-the-art performance on ImageNet-2012, Cifar-10, and MNIST by exploring sparse models for improved interpretability. Visualizations show high order global correlations in sparse filters compared to dense filters. The first convolutional layer of ResNet-18 is analyzed, and the filters learned by SplitLBI are compared with SGD. The gradient images of the first convolutional layer filters are visualized and compared based on their magnitudes. The visualization shows the evolution of the model parameters at different epochs, highlighting the preference of ImageNet filters for non-semantic features. The first convolutional layer of ResNet-18 trained on ImageNet learned non-semantic textures rather than shape for image classification tasks. Enhancing semantic shape invariance learning is crucial for improving the robustness of convolutional neural networks. Ablation studies were conducted on Cifar-10 dataset with VGG-16 and ResNet-56 to evaluate this. Ablation studies were conducted on Cifar-10 dataset with VGG-16 and ResNet-56 to evaluate global convergence and structural sparsity. SplitLBI with momentum and weight decay was chosen for the experiments, with variations in hyper-parameters \u03ba and \u03bd to analyze sparsity and validation accuracies of sparse models. SplitLBI with various hyper-parameters \u03ba and \u03bd was used to validate results on the Cifar-10 dataset. The experiments showed that larger \u03ba leads to slower convergence, and sparse subnetworks perform comparably to dense models without retraining. The sparsity of the model increases with higher values of \u03ba and \u03bd. In practice, a moderate choice of \u03ba and \u03bd is preferred to prevent over-sparsification and maintain model accuracies. Sparse models can achieve comparable predictive power to dense models without fine-tuning or retraining. The structural sparsity parameter \u0393 t can capture important weight parameter W t, leading to effective subnetworks in early epochs. This approach outperforms existing pruning strategies by SGD. SplitLBI introduces a pruning strategy that utilizes structural sparsity at different training epochs to define subnet architecture. Experiments on Cifar-10 dataset compare VGG-16, ResNet-50, and ResNet-56 networks. Momentum and weight decay are used with specific hyperparameters. SplitLBI introduces a pruning strategy using Lasso penalty to sparsify convolutional filters in VGG-16 (Lasso) and ResNet-50 (Lasso) models. Experiments show that SplitLBI achieves comparable or better accuracy than competitors at similar sparsity levels, even with sparse architecture learned. The study demonstrates that SplitLBI can achieve high accuracy with sparse architecture, revealing important subnetwork structures early on. This eliminates the need to fully train dense models, as successful sparse architectures can be identified through early stopping. The proposed method is based on differential inclusions of inverse scale spaces, implemented through a variable splitting scheme. The study shows that SplitLBI can achieve high accuracy with sparse architecture, revealing important subnetwork structures early on. It is based on differential inclusions of inverse scale spaces and can be used to train deep networks with proven global convergence. The global convergence of SplitLBI (Eq. 16a-16c) is established based on the Kurdyka-\u0141ojasiewicz framework. The sequence {(W k , \u0393 k )} converges to a stationary point of L, and {W k } converges to a stationary point of L n (W). The Kurdyka-\u0141ojasiewicz property is defined using notions from variational analysis, with the subdifferential playing a central role in the definitions. The subdifferential of a function h, denoted \u2202h(x), is a set of vectors satisfying certain conditions. The KL property is crucial for analyzing nonconvex algorithms and is defined based on the Kurdyka-\u0141ojasiewicz framework. A function h is called a KL function if it satisfies the KL property at each point of its domain. KL functions include real analytic functions, semialgebraic functions, tame functions in o-minimal structures, continuous subanalytic functions, and locally strongly convex functions. Examples that satisfy the Kurdyka-\u0141ojasiewicz property include multivariable real functions and semialgebraic sets in R^p. The class of semialgebraic sets is stable under various operations like finite union, intersection, Cartesian product, and complementation. Examples include polynomial functions, indicator functions of semialgebraic sets, and the Euclidean norm. Moving on to the deep neural network training problem, we consider a feedforward neural network with hidden layers. The weight matrices between the layers are denoted by W_i \u2208 R^di\u00d7di\u22121. The weight matrices between the layers in a feedforward neural network are denoted by W_i \u2208 R^di\u00d7di\u22121. To verify the Lyapunov function F satisfies the Kurdyka-\u0141ojasiewicz property, an o-minimal structure is needed, which is a sequence of definable subsets of R^n satisfying certain conditions. The o-minimal structure in R^n contains algebraic subsets, finite unions of intervals and points, and semialgebraic sets. It also includes sets with real-analytic functions and the graph of the exponential function. This structure is stable under sum and composition operations. The o-minimal structure in R^n includes algebraic subsets, finite unions of intervals, and semialgebraic sets. It is stable under various operations of analysis. The Kurdyka-\u0141ojasiewicz property for smooth and non-smooth definable functions has been established. The proof of Corollary 1 involves verifying the Lyapunov function satisfies the Kurdyka-\u0141ojasiewicz inequality. The functions involved are definable, semi-algebraic, and compositions of definable functions. The composition of 2 and 1 norms, and the conjugate of group Lasso penalty is the maximum of group 2-norm. The group Lasso and its conjugate are definable as compositions of definable functions, satisfying the Kurdyka-\u0141ojasiewicz inequality. The analysis is motivated by previous works and involves verifying properties such as descent, relative error, continuity, and Kurdyka-\u0141ojasiewicz property. The sufficient descent property of the generated sequence is established using the Lyapunov function. The continuity property is guaranteed by the continuity of L(W, \u0393) and the relation lim k\u2192\u221e B g k \u2126 (\u0393 k+1 , \u0393 k ) = 0. Global convergence of SLBI is achieved under the Kurdyka-\u0141ojasiewicz assumption of F. At a critical point of F, it is shown that \u1e21 \u2208 \u2202\u2126(\u0393) and \u03b1\u03bd \u22121 (\u0393 \u2212W ) = 0. This concludes the proof of the theorem. The sufficient descent property of the sequence is established using the Lyapunov function. The continuity property is guaranteed by the continuity of L(W) and \u2207 L(W). If the assumptions hold, both \u03b1{L(P k )} and {F(Q k )} converge. In this subsection, the bound of subgradient is provided, showing convergence of \u03b1{L(P k )} and {F(Q k )} to the same finite value, with monotonically decreasing behavior. The boundedness of W k and \u0393 k is established, leading to the conclusion that the subgradient is bounded. In this subsection, a bound of subgradient is provided, showing convergence of \u03b1{L(P k )} and {F(Q k )} to the same finite value. The model accuracy for SLBI is robust to hyperparameters in terms of convergence rate and generalization ability, as shown in validation curves for dense models W t with different \u03ba and \u03bd on the test set of Cifar10. The experiments conducted on Cifar10 with VGG16 and ResNet56 models focused on ablation studies on \u03ba and \u03bd hyperparameters. Performance of models W t with different hyperparameter combinations was recorded for each epoch. Results showed that larger kappa values led to better validation accuracies. The experiments focused on tuning hyperparameters \u03ba and \u03bd for VGG16 and ResNet56 models on Cifar10. Larger \u03ba values led to slightly lower validation accuracies, with \u03ba = 1 achieving the best test accuracy. Tuning \u03bd showed less effect on generalization performance but could result in a sparse model with better performance than SGD. Computational cost comparisons were made between optimizers SGD (Mom), SplitLBI (Mom), and Adam (Naive) on one GPU. The experiments were conducted on one GTX2080 GPU, focusing on GPU memory usage and batch processing time. The study explored sparsity in different layers of Lenet-3, with lower sparsity observed in layers with more parameters. Inspired by previous work, the experiment involved training a subnet obtained after 100 epochs and using the \"rewind\" trick for fine-tuning at different epochs. The models were further optimized using SplitLBI (Mom-Wd) technique. The study involved training models with different sparsity levels using the \"rewind\" trick for fine-tuning at various epochs. Three networks were studied - LeNet-3, Conv-2, and Conv-4, with varying levels of sparsity. LeNet-3 is less over-parameterized compared to the other two networks. The sparsity for LeNet-3 is 0.055, Conv-2 is 0.0185, and Conv-4 is 0.1378. The study compared the sparsity levels of different layers in the model, showing that fc-layers are sparser than conv-layers. Fine-tuned rewinding subnets for overparameterized networks like Conv-2 and Conv-4 produced better results than full models. For less over-parameterized models like LeNet-3, fine-tuned subnets achieved comparable performance to dense models and outperformed retrained sparse subnets. This suggests that the subnet architecture revealed by structural sparsity parameter \u0393 T is valuable for fine-tuning sparse models. The experiments in Fig. 5 show the value of parameter \u0393 T for fine-tuning sparse models with performance comparable to dense models. Table 10 details hyperparameter settings, while Figure 8 illustrates sparsity calculations over epochs with early stopping."
}