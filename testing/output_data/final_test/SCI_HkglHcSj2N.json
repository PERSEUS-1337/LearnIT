{
    "title": "HkglHcSj2N",
    "content": "Designing rewards for Reinforcement Learning (RL) is challenging, especially in robotics where detecting goal achievement requires supervision. Methods like Hindsight Experience Replay (HER) show promise in reaching various goals without explicit rewards. Incorporating demonstrations can speed up policy convergence significantly. Our method can speed up policy convergence to reach any goal, surpassing other Imitation Learning algorithms. It can be used with trajectories without expert actions, leveraging kinesthetic or third person demonstration. Reinforcement Learning has shown impressive results in simulated tasks, but struggles in real-world robotics due to difficulties in setting up the same learning environment, such as access to a reward function. Self-supervised methods can overcome this limitation. Self-supervised methods in robotics involve training a goal-conditioned policy to reach any previously observed state, using off-policy RL algorithms to achieve the indicator reward by relabeling collected trajectories. This method, known as Hindsight Experience Replay (HER), allows for learning in continuous spaces without the need for additional instrumentation. In robotics, self-supervised methods like Hindsight Experience Replay (HER) aim to train a goal-conditioned policy to reach any observed state without direct reward supervision. However, challenges arise when there are bottlenecks in the state space, making it difficult for random motion to traverse them easily. This problem could be addressed by engineering a reward to guide the agent towards the bottlenecks, but this goes against the goal of learning without direct reward supervision. In this work, the focus is on leveraging a few demonstrations to enhance the learning of goal-reaching policies in robotics, particularly in the goal-conditioned setting. The study explores how methods like Imitation Learning (IL) can outperform the demonstrator without the need for additional rewards, even when expert trajectories are suboptimal. The study focuses on improving goal-reaching policies in robotics by leveraging a few demonstrations. The developed method can utilize demonstrations without expert actions, which is beneficial in practical robotics scenarios. The framework aims to boost goal-conditioned policy learning with only state demonstrations in a Markov decision process. The study aims to improve goal-reaching policies in robotics by utilizing demonstrations without expert actions. The policy is conditioned on a goal, and the reward is based on reaching the goal. Access to expert trajectories is assumed, where actions are taken to reach the goal efficiently. In this section, different algorithms are compared to pure Hindsight Experience Replay. A novel expert relabeling technique is proposed, along with a goal-conditioned GAIL algorithm trained with state-only demonstrations. Expert trajectories are collected by reaching specific goals, which can also be used to reach other states within the demonstration. This relabeling method leverages data augmentation and the assumption of quasi-static tasks for effective training. In FIG2, two agents are compared in the Four Rooms environment: one trained with Behavioral Cloning and the other with expert relabeling. Behavioral Cloning can deviate from demonstrations and requires many demonstrations with increasing state dimensions. The goal-conditioned case is less affected by the first issue, but scaling to high-dimensional sensory inputs like images poses challenges. Other Imitation Learning algorithms can help mitigate these problems. In this section, the formulation of GAIL is extended to tackle goal-conditioned tasks and combined with HER to outperform the demonstrator and generalize to all goals. The algorithm, called goal-GAIL, conditions the discriminator on the goal and uses off-policy algorithm DDPG for relabeling techniques. The combination of rewards in the goal-conditioned case makes the interpretation of the fitted Q slightly tricky, but both rewards aim to improve performance. The goal-GAIL algorithm combines rewards from GAIL and Behavioral Cloning to push policy towards goals. The weight of the GAIL reward can be annealed to avoid conflicts. Both methods use expert state-action pairs, limiting their use to setups with the same agent. However, replacing the action in GAIL with the next state could provide more data. The text discusses using demonstrations to accelerate learning in goal-conditioned tasks without rewards. Two key questions are addressed: the effectiveness of using demonstrations for learning and the efficiency of Expert Relabeling for data augmentation. Experiments are conducted in simulated robotic tasks using 20 demonstrations and performance is evaluated based on the percentage of goals achieved by the agent in feasible goal space. The experiments focus on the percentage of goals reached in the feasible goal space by an agent. A point mass is placed in an environment with four rooms connected through small openings. The action space is continuous, specifying changes in state space corresponding to the goal space. In goal-conditioned tasks like Pick and Place, a fetch robot must pick and place a block in a desired point in space. The goal space is the block's position. The text explores how methods can use a few demonstrations to accelerate learning in goal-conditioned tasks. In goal-conditioned tasks, methods can leverage a few demonstrations to accelerate learning. Using GAIL with relabeling (GAIL+HER) outperforms running each in isolation. HER alone converges slowly but reaches the same final performance with enough time. GAIL learns fast initially but its final performance is limited due to lack of reward information beyond demonstrations. The Expert Relabeling technique introduced in Section 3.1 is beneficial in goal-conditioned imitation learning. It brings performance boosts for both Behavioral Cloning methods and goal-GAIL in different environments. Without expert relabeling, the agent fails to learn how to reach intermediate states visited during a demonstration. This is crucial in cases where only observation-only demonstrations are available. In goal-conditioned imitation learning, the goal-GAIL method outperforms BC combined with HER in the four rooms environment. GAIL learns a well-shaped reward by matching the state-distribution of the expert, encouraging the agent to move towards the goal. Practical applications may involve experts with erratic behavior. In goal-conditioned imitation learning, the goal-GAIL method outperforms BC combined with HER in the four rooms environment by matching the state-distribution of the expert. Practical applications may involve experts with erratic behavior. When studying how different methods perform with a sub-optimal expert, adding noise to optimal actions greatly affects approaches like Behavioral Cloning, while discriminator-based methods can leverage noisier experts effectively. This suggests that having some noise in the expert could actually improve the performance of adversarial approaches. In this work, a novel algorithm called goal-GAIL is introduced to improve convergence speed in goal-conditioned tasks using only a few demonstrations. Expert relabeling as data augmentation enhances the performance of goal-GAIL and goal-conditioned Behavioral Cloning without requiring expert actions. Imitation Learning, such as goal-GAIL, improves convergence speed in goal-conditioned tasks with few demonstrations. It does not require expert actions and is robust to sub-optimalities in expert behavior. Various methods like Behavioral Cloning and Inverse Reinforcement Learning can be used to train desired behaviors. Generative Adversarial Imitation Learning (GAIL) is a formulation that optimizes policy using off-policy algorithms. In goal-conditioned tasks, off-policy algorithms are efficient but struggle to outperform the expert without additional rewards. Relabeling methods like Hindsight Experience Replay can help in sparse reward cases, but challenges remain in adapting to unseen situations. This work focuses on reaching any state upon demand, a common goal in robotics multi-task learning. To address the inefficiency of learning complex policies, leveraging demonstrations and introducing new techniques like annealing of Behavioral Cloning loss and relabeling expert trajectories can be effective. Additionally, experimenting with Goal-conditioned GAIL and constructing a data-set of state-action-goal tuples can improve learning efficiency in goal-conditioned tasks. In goal-conditioned reinforcement learning, a supervised regression algorithm is used to compute loss and gradient without additional environment samples. Combining this loss with other policy updates can be done efficiently. However, when combining behavioral cloning and deterministic policy gradient updates, improvement guarantees with respect to task reward may be lost. This can be addressed by applying a Q-filter or annealing the loss, allowing the agent to eventually outperform the expert. In goal-conditioned reinforcement learning, a supervised regression algorithm is used to compute loss and gradient without additional environment samples. Combining this loss with other policy updates can be done efficiently. The experiments detailed in Algorithm 1 show different variants that allow the agent to outperform the expert. Parameters such as \u03b1, \u03b2, p, \u03b4 GAIL, and EX-PERT RELABEL are defined, along with task horizons and discount factors for different environments. The Q function, policy, and discriminator are implemented using fully connected neural networks. DDGP is used for policy optimization with a hindsight probability set at 0.8. The behavior cloning loss weight \u03b2 is annealed over rollouts. The discriminator reward weight \u03b4 GAIL is set to 0.1 and is not annealed. Different setups for training the discriminator are used, comparing various state-goal combinations. Parameters for sub-optimal expert experiments are specified for different environments."
}