{
    "title": "HkL7n1-0b",
    "content": "The Wasserstein Auto-Encoder (WAE) is a new algorithm for generative modeling that minimizes a penalized form of the Wasserstein distance between the model distribution and the target distribution, resulting in a different regularizer compared to the Variational Auto-Encoder (VAE). The Wasserstein Auto-Encoder (WAE) is compared with other techniques, showing it is a generalization of adversarial auto-encoders (AAE). WAE shares properties of VAEs while generating higher quality samples. Representation learning has evolved from supervised to unsupervised generative modeling, with VAEs being a well-established approach but generating blurry samples for natural images. Generative adversarial networks (GANs) have shown impressive visual quality in image samples but lack an encoder and are harder to train, suffering from mode collapse. Various configurations of GANs and combinations with VAEs have been explored, but a unified framework combining the strengths of both is yet to be discovered. This work builds on theoretical analysis from previous research, approaching generative modeling from an optimal transport perspective. The text discusses the importance of optimal transport (OT) in applications where data is supported on low dimensional manifolds. OT provides a weaker topology compared to other divergence measures like f-divergences. VAE and WAE minimize reconstruction cost and penalize discrepancies between distributions, with VAE ensuring the encoder matches the distribution induced by the input examples. The text introduces Wasserstein Auto-Encoders (WAE) as a new family of regularized auto-encoders that minimize optimal transport between data distribution P X and latent variable model P G. WAE promotes better reconstruction by allowing latent codes of different examples to stay far apart. Wasserstein Auto-Encoders (WAE) minimize optimal transport between data distribution P X and latent variable model P G. WAE consists of a c-reconstruction cost and a regularizer penalizing distribution discrepancies in Z. Empirical evaluation on MNIST and CelebA datasets shows WAE maintains VAE properties while generating higher quality samples. Two regularizers are proposed, one based on GANs and the other on maximum mean discrepancy. The paper introduces Wasserstein Auto-Encoders (WAE) which minimize optimal transport between data distribution P X and latent variable model P G. It proposes two regularizers, one based on GANs and the other on maximum mean discrepancy, leading to WAE-GAN and WAE-MMD algorithms. Theoretical considerations from previous work are used to derive the WAE objective, showing equivalence to a problem involving the optimization of a probabilistic encoder Q(Z|X). The new method introduced in the paper minimizes the optimal transport cost between data distribution P X and latent variable model P G. It involves an auto-encoder formulation where the decoder reconstructs encoded training examples while the encoder matches the encoded distribution to a prior and ensures informative latent codes for reconstruction. The class of f-divergences BID21 is defined by a rich class of divergences induced by the optimal transport problem. Kantorovich's formulation involves a measurable cost function and joint distributions of X and Y with marginals PX and PG. In the case of a metric space, the p-Wasserstein distance is defined as the p-th root of the p-Wasserstein. The p-Wasserstein distance is defined as Wp = dp(x, y) for p \u2265 1, where Wp is the p-th root of Wc. Kantorovich-Rubinstein duality holds when c(x, y) = d(x, y). Generative models like VAEs and GANs aim to minimize discrepancy measures between data distribution PX and model PG. Standard divergences are difficult to compute when PX is unknown and PG is parametrized by deep neural networks. Tricks and variational lower bound are used to address this issue. In this work, the focus is on latent variable models defined by sampling a code Z from a distribution P(Z) and mapping it to an image X with a transformation. Different methods like f-GANs, Wasserstein-GAN, and OT cost can be used to minimize divergence between data distribution PX and model PG. The OT cost in latent variable models with non-random decoders simplifies through the map G, leading to a conditional distribution identical to the prior distribution P(Z). This result is proven in Theorem 1 for deterministic P(G(X|Z)) and any function G. The WAE objective allows for optimization over random encoders Q(Z|X) instead of all couplings between X and Y. Constraints on Q Z are relaxed by adding a penalty to the objective, leading to a nonparametric set of probabilistic encoders. Deep neural networks are used to parametrize both encoders Q and decoders G, allowing for non-random encoders mapping inputs to latent codes. Two different penalties D Z (Q Z , P Z ) are proposed, with adversarial training used to estimate them. The WAE-GAN introduces an adversary in the latent space Z to separate \"true\" points from \"fake\" ones. This approach moves the adversary from the input space X to the latent space Z, making the task easier when P Z has a single mode. A penalty based on maximum mean discrepancy (MMD) is proposed for divergence measure. The WAE-MMD introduces a penalty based on maximum mean discrepancy (MMD) for divergence measure. It utilizes an unbiased U-statistic estimator with stochastic gradient descent methods to optimize the Wasserstein Auto-Encoder. The WAE-MMD algorithm utilizes a penalty based on maximum mean discrepancy for divergence measure. It involves optimizing the Wasserstein Auto-Encoder using stochastic gradient descent methods with an unbiased U-statistic estimator. The encoders in the algorithm can deterministically map input points to latent codes. Classical unregularized auto-encoders only minimize the reconstruction cost, leading to non-overlapping zones for different training points scattered across the latent space. The encoder in variational auto-encoders (VAEs) does not provide a useful representation due to non-overlapping zones in the latent space. VAEs minimize a variational bound on KL divergence, but do not guarantee a match between the encoded distribution and the prior like Wasserstein Auto-Encoders (WAE) do. VAEs require Gaussian encoders and random decoders, while WAE allows for non-Gaussian encoders. WAE minimizes optimal transport W c (P X , P G ) and allows both probabilistic and deterministic encoder-decoder pairs. The VAE regularizer can be equivalently written as a sum of D KL (Q Z , P Z ) and mutual information I Q (X, Z). WAEs drop the mutual information term in the VAE regularizer, making it equivalent to adversarial auto-encoders (AAE). Theory suggests that AAEs minimize the 2-Wasserstein distance between P X and P G, providing a theoretical justification for AAEs. WAE generalizes AAE by allowing any type of encoder-decoder pair. WAE generalizes AAE by allowing any cost function in input space X and any discrepancy measure in latent space Z. Zhao et al. (2017b) proposed InfoVAE, similar to BID3, with a reconstruction cost defined implicitly through negative log likelihood term. WAEs specify cost explicitly without constraints. Our approach is based on the primal form of Optimal Transport (OT), focusing on generative modeling. Unlike previous works, we introduce regularizers that are unique and discuss implications for generative modeling. The Wasserstein Generative Adversarial Network (WGAN) minimizes the 1-Wasserstein distance for generative modeling, while the Wasserstein Autoencoder (WAE) can handle any cost function and comes with an encoder. The WGAN approach is from the dual form, limiting its applicability to other cost functions, unlike WAE which operates from the primal form. Various approaches have been proposed in the literature to handle non-trivial constraints in Optimal Transport (OT). For example, BID11 suggested penalizing the objective with a term \u03bb \u00b7 E ( \u2207f (X) \u2212 1) 2, while BID5 proposed penalizing the objective with the KLdivergence \u03bb \u00b7 D KL (\u0393, P \u2297 Q). BID8 showed that entropic regularization drops constraints on functions in the dual formulation. In the context of unbalanced optimal transport, the objective is regularized with additional divergences. In generative modeling, only one extra divergence is found to be necessary. Some GAN variations lack an encoder, making it difficult to reconstruct latent codes. Approaches blending GANs with autoencoder architectures have been explored in the literature. The authors propose using the discrepancy between Q Z and E Z \u223cP Z [Q Z|G(Z ) ] as the objective for the max-min game between the encoder and decoder. They suggest adding a reconstruction term to address the lack of reciprocity in training. WAE differs from traditional GANs and has a solid theoretical foundation. Some methods use reproducing kernels to match distributions directly in input space, but they require larger mini-batches for training. The authors propose using WAE models to achieve accurate reconstructions of data points, reasonable geometry of the latent manifold, and high-quality random samples. They trained WAE-GAN and WAE-MMD on real-world datasets MNIST and CelebA to test generalization on both training and test data. The authors used WAE models with deterministic encoder-decoder pairs and convolutional deep neural network architectures to train on MNIST and CelebA datasets. They utilized Euclidean latent spaces, isotropic Gaussian prior distributions, and a squared cost function for data points.\u03bb = 10 was found to work well across all datasets considered. Choosing dz larger than the dataset's intrinsic dimensionality forces the encoded distribution to live on a manifold in Z. The authors used WAE models with deterministic encoder-decoder pairs and convolutional deep neural network architectures to train on MNIST and CelebA datasets. They utilized Euclidean latent spaces, isotropic Gaussian prior distributions, and a squared cost function for data points. Choosing dz larger than the dataset's intrinsic dimensionality forces the encoded distribution Q Z to live on a manifold in Z, making matching Q Z to P Z impossible if P Z is Gaussian and may lead to numerical instabilities. Random samples VAE WAE-MMD WAE-GAN Figure 3: VAE (left column), WAE-MMD (middle column), and WAE-GAN (right column) trained on CelebA dataset. In \"test reconstructions\" odd rows correspond to the real test points. Results of VAEs are also reported. The authors used WAE models with deterministic encoder-decoder pairs and convolutional deep neural network architectures to train on MNIST and CelebA datasets. They observed that using WAE-MMD with the RBF kernel failed to penalize outliers in the latent space. To address this issue, they used the inverse multiquadratics kernel with a data-dependent bandwidth selection, significantly improving the model's performance. The performance of WAE models with multivariate Gaussian vectors drawn from P Z was significantly improved compared to the RBF kernel. The quality of samples depends on the accuracy of matching between Q Z and P Z, with even slight differences affecting sample quality. WAE-GAN may generate better samples than WAE-MMD, but WAE-GAN is unstable due to adversarial training. The quality of generated images from Wasserstein auto-encoders (WAE) is better than Variational Autoencoders (VAE), with WAE-GAN slightly outperforming WAE-MMD. The Fr\u00e9chet Inception Distance is used for quantitative assessment, confirming the image quality. Test reconstructions and interpolations show the effectiveness of Wasserstein auto-encoders in generating images. Using optimal transport cost, Wasserstein auto-encoders are a new family of algorithms for generative models. Experimental results show that images sampled from trained WAE models are of higher quality compared to VAEs, maintaining stability in training and reconstruction quality. Future work includes exploring criteria for matching encoded distribution to prior distribution, adversarially training the cost function in input space, and theoretical analysis of dual formulations for WAE-GAN and WAE-MMD.GANs and VAEs, while different in conceptual frameworks and performance, share important features. Both GANs and VAEs can be trained by sampling from a model without knowing its density, and can be scaled up with SGD. This allows for the use of flexible implicit models defined by sampling a code from a distribution and mapping it to an image. Various variations of VAEs and GANs are available in the literature, with the original GAN approach minimizing a certain function with respect to a deterministic decoder. The GAN approach minimizes a lower bound on the JS-divergence by training a deterministic decoder G and discriminator T in alternating steps. The f-GAN modification allows for lower bounding any desired f-divergence. The 1-Wasserstein distance W1 is suggested as a better alternative for generative modeling. The Wasserstein GAN (WGAN) uses the 1-Wasserstein distance to provide stable gradients for generative modeling, addressing the \"vanishing gradient\" problem. Variational auto-encoders (VAE) utilize models that minimize a lower bound on the 1-Wasserstein distance. The conditional distribution P G (X|Z) in VAE is often parametrized by a deep net G using Gaussians. VAE minimizes an upper bound on the negative log-likelihood by restricting Q to a class of Gaussian distributions. The AVB method in VAE aims to reduce the gap between the true negative log-likelihood and the upper bound by enlarging the class Q using GANs. It allows for different conditional distributions for various transformations, replacing the intractable term with an adversarial approximation. The D KL term serves as a regularizer, and dropping it results in a chaotic encoding of training points into nonoverlapping zones. Adversarial auto-encoders (AAE) replace the D KL term in VAE with a regularizer to ensure no \"holes\" in the latent space Z, improving sample generation from P G (X|Z). Matching Q Z to P Z prevents chaotic encoding and allows for different conditional distributions Q(Z|X), including Gaussians. The performance of various conditional distributions Q(Z|X) is explored, such as Gaussians in VAEs, implicit models in AVB, and deterministic encoder mappings. Joint probability distributions of three random variables (X, Y, Z) are considered, with P G,Z (Y, Z) representing a joint distribution. The optimal transport problem involves couplings between X and Y, with \u0393(Y|X) as a non-deterministic mapping. Theorem 1 demonstrates how to factor in this scenario. Theorem 1 explains how to factor the non-deterministic mapping \u0393(Y |X) through Z, decomposing it into Q(Z|X) and P G (Y |Z). Different sets of joint distributions are denoted by P(X \u223c P X , Y \u223c P G ), P(X \u223c P X , Z \u223c P Z ), and P X,Y,Z. The upper bound is determined by P X,Y \u2286 P(P X , P G ) when P G (Y |Z) are Dirac measures. After 30 epochs, the encoder and decoder in WAE-GAN were adjusted with convolutional architectures using 4x4 filters. The adversary architecture included convolution and strided convolution layers with batch normalization and rectified linear units. Two heuristics were applied during training. The VAE model used cross-entropy loss with a Bernoulli decoder and the same architectures and hyperparameters as listed above. CelebA images were pre-processed by taking a 140x140 center crop and resizing to 64x64 resolution. Mini-batches of size 100 were used, and models were trained for various epochs (up to 250). The WAE models reported all utilized strides 2 and SAME padding, with two heuristics applied during training. The WAE models were trained for 55 epochs and VAE for 68 epochs. WAE-MMD used \u03bb = 100 and WAE-GAN used \u03bb = 1. Both models used \u03c3 2 z = 2. The learning rates were adjusted during training, with encoder and decoder architectures using fully convolutional designs. In experiments, log prior was added to the adversary output to aid learning the remaining density term. VAE model used cross-entropy reconstruction loss with \u03b1 = 10^-4 as initial learning rate. Architectures and hyperparameters remained consistent."
}