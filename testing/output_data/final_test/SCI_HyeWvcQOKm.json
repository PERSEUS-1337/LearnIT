{
    "title": "HyeWvcQOKm",
    "content": "We introduce the Transductive Consensus Network (TCN), a semi-supervised multi-modal classification framework that combines consensus and classification mechanisms. Through ablation studies, we demonstrate the importance of both mechanisms working together. TCNs outperform or match the performance of benchmark algorithms on datasets like Bank Marketing and DementiaBank with only 20 to 200 labeled data points. The CN framework is briefly reviewed for supervised, multi-view classification, involving interpreter networks and a discriminator to distinguish latent representations. In this paper, the Transductive Consensus Network (TCN) is introduced as a semi-supervised multi-modal classification framework. The training involves optimizing two targets iteratively, with a focus on distinguishing latent representations. The input data includes labeled and unlabeled data points, with a emphasis on multiple modalities. The classifier and discriminator are similar to the CN framework, but the classification loss is defined only on labeled data while the discriminator loss is defined across both labeled and unlabeled data points. The Transductive Consensus Network (TCN) functions through consensus and classifier mechanisms to compress data samples into meaningful interpretations. Ablation studies test three variants, including TCN-svm which results in trivial classifiers. TCN-AE has inferior performance compared to TCN, as reconstruction in an autoencoder style counteracts the consensus mechanisms. Experiments on two classification datasets show the impact of these mechanisms. In experiments on two classification datasets, consensus network models struggle with imbalanced data, so negative samples are randomly sampled to create a more balanced dataset. Categorical raw data is converted, and training stops when the classification loss is higher than log2\u2248 0.693. In experiments on two classification datasets, consensus network models struggle with imbalanced data. Training stops when the classification loss is higher than log2\u2248 0.693. If this happens, the model is re-initialized with a new random seed and training is restarted. The similarity for interpretations is calculated by averaging negative divergences. The \"similarity\" has a maximum value of 0 and no theoretical lower bound. In experiments on classification datasets, consensus network models struggle with imbalanced data. Training stops when losses converge, with interpretations forming distinct shapes at different steps. At step 30, interpretations of three modalities form 'drumstick' shapes with high similarity."
}