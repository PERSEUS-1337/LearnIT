{
    "title": "BkgGmh09FQ",
    "content": "A study on system resource efficiency for super-resolution (SR) in convolutional architectures reveals insights on compressing models to meet memory and computation targets, and techniques to compact SR models for resource-constrained devices. The study focuses on compressing super-resolution models to meet memory and computation targets for resource-constrained devices, achieving comparable performance with existing models. This research lays the groundwork for further exploration into resource efficiency in super-resolution tasks. Deep learning networks for super-resolution tasks are resource-intensive, with billions of operations needed due to the spatial dimensions of feature maps. Previous works have focused on improving efficiency in deep learning models for tasks other than super-resolution, achieving size and compute gains without performance loss. However, the application of these efficiency methods in super-resolution tasks has not been extensively studied. Efficiency methods for super-resolution (SR) tasks have not been extensively studied. Existing studies focus on discriminative tasks with different architectures, making it challenging to apply those results to SR. The up-sampling structure of SR models may lead to stronger side-effects on image distortion when using efficiency methods. A systematic study is detailed in this paper to bridge the gap between SR and resource-efficient deep models, exploring the impact of various efficiency techniques on image distortion quality. New insights are provided on the effectiveness of low rank tensor decomposition and convolution approximations in SR tasks. Efficiency methods for super-resolution (SR) tasks have not been extensively studied. Existing studies focus on discriminative tasks with different architectures, making it challenging to apply those results to SR. The up-sampling structure of SR models may lead to stronger side-effects on image distortion when using efficiency methods. A systematic study is detailed in this paper to bridge the gap between SR and resource-efficient deep models, exploring the impact of various efficiency techniques on image distortion quality. New insights are provided on the effectiveness of low rank tensor decomposition and convolution approximations in SR tasks. SR networks suffer from a worse trade-off between efficiency and performance as more layers are compressed. Successful quantization techniques used in image discriminative tasks are equally successful in SR. Neural networks for SR have shown superior performance against previous traditional approaches. In SR tasks, models are evaluated using image distortion metrics like PSNR, SSIM, and IFC, or perception metrics like NIQE and BRISQUE. Distortion models prioritize pixel-to-pixel comparisons and are trained on L1 or L2 loss, producing visually pleasing results on structural images. Different convolutional networks have been proposed for SR, with variations in feature extraction and up-sampling techniques. Some models use more layers, recursive layers, or memory blocks for improved performance. Perceptual super-resolution models focus on reconstructing unstructured details with high perceptual quality using popular image distortion models and various loss functions like perceptual, contextual, adversarial, and Gram loss. These models, such as EUSR and SRResNet, generate visually pleasing results but may not work well for image classification tasks. Efficient super-resolution models aim to run faster and perform better than existing models like SRCNN. They focus on building resource-efficient architectures and trade-offs between performance and efficiency. Related work includes investigating the impact of grouped convolutions. Future work will explore the variability of training and evaluating these models. This study is the first systematic exploration of efficiency methods for super-resolution, measuring performance using PSNR and SSIM metrics. New metrics are introduced to measure the trade-off between performance and efficiency, including the number of Giga Mult-Adds and parameters saved for every 0.01dB PSNR loss in test sets. The study introduces new efficiency metrics for super-resolution models, including B100 BID41 and Urban100 BID21, calculated by comparing compressed and uncompressed models. The baseline model used is RCAN Zhang et al. (2018), with training details similar to EDSR Lim et al. (2017). Training data includes 48\u00d748 RGB patches from the DIV2K dataset BID52, augmented with flips and rotations. The model is optimized using ADAM Kingma & Ba (2014) with specific hyper-parameters. The model is trained using the ADAM optimizer with specific hyper-parameters. Mini-batch size is 16, learning rate is halved at 200 epochs, and L1 loss is used for 300 epochs. Two models are trained from scratch and used as pre-trained models for faster convergence. Ternary quantization is applied for 40 epochs with quantization enabled in each forward pass. Evaluation includes exploring resource-efficient architectures and applying techniques to different parts of the model. Resource-efficient architectures utilize low rank tensor decomposition and convolutional approximation techniques to create fast and accurate image discriminative models. The trade-off solutions involve modifying 3x3 convolution layer blocks, exploring bottleneck design, separable/grouped convolutions, and channel shuffling. Batch normalization layers are removed to improve performance and reduce GPU memory usage. The experiments focus on replacing 3x3 convolutions in the baseline model. In experiments, 3x3 convolution layers in the residual groups of the baseline model are replaced to reduce the number of residual groups and channels. The network is made shallower to better understand architectural changes. The residual bottleneck design from ResNet is adopted with a reduction factor, using 1x1 convolutions for compression and recovery of information. The curr_chunk discusses the use of grouped convolutions to reduce computation cost and improve efficiency in convolutional neural networks. It mentions replacing 3x3 convolutions with grouped convolutions and adopting depthwise separable convolutions for further optimization. Additionally, channel shuffling is incorporated for improved performance. The curr_chunk introduces the concept of channel shuffling in convolutional neural networks to improve information flow among channels. It also discusses the use of grouped convolutions and channel splitting to enhance efficiency and reduce computation cost. Additionally, inverted residuals are mentioned for enabling skip connections directly on bottleneck layers. Our experiments show that using bottlenecks alone or grouped convolutions result in the best trade-offs between memory, compute, and performance. Reducing the number of features for inverted bottlenecks severely impacts performance. Results for upscaling show similar performance and efficiency trade-offs. Inverted residuals may perform better on models with larger feature sizes. In image discriminative tasks, using bottlenecks for resource-efficient SR architectures is recommended. For tight memory and efficiency budgets, depthwise separable convolutions are suggested. Low rank tensor decomposition provides the best trade-offs, followed by separable/grouped convolutions, and channel splitting/shuffling. BID3 and BID30 have shown the feasibility of these architecture changes. In image classification, channel splitting and shuffling techniques have been explored for efficiency. However, applying these techniques in super-resolution models led to a significant drop in performance. To address this, bottleneck reduction and depthwise separable convolutions were applied to different parts of the baseline model. Experimentation showed that using enhanced upscaling modules with skip connections improved performance at a slight increase in memory and compute costs. The enhanced recursive upscaling module (ERUM) is used to maintain memory cost while upscaling. The number of ERUMs corresponds to the scaling factor, with each recursing twice or thrice for different scales. Experimentation with ERUMs for up-sampling is denoted by a postfix -e. Trade-off metrics are calculated based on a baseline model with ERUM as its up-sampling layer. Recommendations include gradually implementing bottleneck reduction and group convolutions for model compression. Based on experimentation with the enhanced recursive upscaling module (ERUM) for up-sampling, recommendations include implementing bottleneck reduction and group convolutions for model compression. Different budgets were used to derive the best models, which were compared with existing literature. The techniques can be applied to any model for trade-offs between performance and efficiency, resulting in models that are better or comparable to others in the literature. Our best model (blrn-e[rb]) outperforms models with 1,500K parameters and below in size and evaluation metric. It has x6 less operations than MemNet BID51 and is comparable to the CARN model in terms of operations, with a 2.5x size reduction. It surpasses earlier models like VDSR BID27 and later models like SRMDNF BID61 for 3x and 4x scales. The second and third best models also outperform earlier models with significant savings in operations for 3x and 4x scales. These techniques designed for image discriminative tasks show effectiveness in SR. Visual comparisons for some models can be found in the appendix. Next experiments will explore quantization and extreme low-precision use. In a set of experiments, the viability of quantization and extreme low-precision (ternary/binary) for reducing system resources in SR is examined. By applying 8-bits integer quantization to the baseline model, a 4x reduction in memory is achieved, enabling support for low-power embedded devices. The model is trained in full precision and then quantized using TensorflowLite for integer-only arithmetic BID23, resulting in a slight evaluation loss in 2x scaling and a slight improvement in 4x scaling. The results indicate that deep neural networks are robust to noise. The success of using binarized and ternarized neural networks in image tasks motivates experimenting with these techniques in SR. The baseline SR architecture is modified by replacing convolution layers with sum-product convolution layers to approximate matrix multiplication. These layers represent a sum-product network and are robust to noise and perturbations caused by quantization, making quantization highly recommended, especially on hardware that can utilize its benefits. The study explores the use of ternary weights in convolution layers for super-resolution tasks. By tuning the parameter r, the performance of the model can be improved at the cost of additional computations. Group convolutions are also utilized to reduce the number of additions. The evaluation results show that increasing r beyond a certain point does not lead to further improvements in performance. The study suggests using a lower r value instead of grouped convolutions to improve efficiency in super-resolution tasks. Extensive experiments show that low rank tensor decomposition is the most effective technique for balancing performance and efficiency in reducing image distortion. For improved efficiency in super-resolution tasks, using low rank tensor decomposition is recommended over grouped convolutions. To balance performance and efficiency, separable/grouped convolutions are advised for further memory and compute compression. It is suggested to reduce the number of layers or utilize channel splitting and shuffling for efficiency on conventional hardware. A mixture of convolution and resource-efficient units is recommended unless additional compression is necessary. Avoid architecture changes on the first and last convolution layers, and utilize quantization if supported by the hardware."
}