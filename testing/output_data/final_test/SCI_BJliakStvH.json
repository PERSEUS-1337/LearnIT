{
    "title": "BJliakStvH",
    "content": "In Inverse Reinforcement Learning (IRL), the focus is on estimating a reward function that explains an expert agent's behavior on a task. However, behavior can often be represented by a simple reward and hard constraints. The agent aims to maximize rewards while adhering to these constraints. The problem is reformulated on Markov Decision Processes (MDPs) to estimate state, action, and feature constraints that drive the agent's behavior. The approach is based on Maximum Entropy IRL, allowing for reasoning about expert demonstrations in an MDP. This method helps infer which constraints can be added to the MDP to enhance performance. Inverse Reinforcement Learning (IRL) focuses on estimating a reward function to explain expert behavior. A new algorithm infers Maximum Likelihood Constraints to explain behavior, aiding in designing robot behavior in new domains. This method helps identify objectives driving behavior, reducing the burden of manual task specification for reinforcement-learning-enabled agents. Recent work in Inverse Reinforcement Learning (IRL) has shifted towards learning a rich class of task specifications using demonstrations, addressing limitations in reward function-based approaches. Other research has focused on learning constraints, such as forbidden behaviors, to better represent complex task domains. In this work, a novel method for inferring constraints in safety-critical systems is presented, drawing from the Maximum Entropy approach to Inverse Reinforcement Learning. The method aims to select constraints that maximize the likelihood of observing demonstrations and explain differences between expected and demonstrated behavior. Our method improves on prior work by simultaneously considering constraints on states, actions, and features in a Markov Decision Process (MDP) to rank options based on their effect on demonstration likelihood. The Inverse Reinforcement Learning (IRL) problem was first proposed by Kalman (1964) as the Inverse problem of Optimal Control (IOC), later brought into the domain of MDPs and Reinforcement Learning (RL) by Ng et al. (2000). One challenge in IRL is that a policy can be optimal with respect to an infinite set of reward functions. The Inverse Reinforcement Learning (IRL) problem addresses the challenge of policies being optimal with respect to various reward functions. Different approaches, such as Maximum Entropy IRL (MaxEnt), have been developed to tackle this ambiguity and produce a single stochastic policy matching feature counts without additional constraints. The distribution over trajectories is used to identify behavior-modifying constraints. Markovian rewards are concise but limited in capturing all task specifications. Non-Markovian Boolean specifications offer a way to describe complex objectives and constraints. Techniques inspired by the MaxEnt framework calculate the posterior probability of satisfying Boolean specifications using demonstrations. Constraints are states, actions, or features that must be avoided in the environment. Chou et al. (2018) infer trajectory feature constraints from a nominal model of the environment and demonstrated trajectories by sampling higher-reward trajectories. They identify constraints as the subset of the feature space containing these trajectories but not the demonstrated ones. Their approach lacks a mechanism for determining the best cost function to rank these constraints. Our approach ranks constraints based on likelihood, using the MaxEnt framework to select constraints that maximize the likelihood of demonstrated behaviors in a Markov Decision Process (MDP). We augment the feature space with state- and action-specific features to compare the impacts of different types of constraints on demonstration likelihood. In a Markov Decision Process (MDP), a trajectory is a sequence of states and actions chosen by an agent according to a policy. The MDP consists of states, actions, state transition probabilities, initial state distribution, features mapping, and a reward function. The agent navigates the MDP by selecting actions based on a policy that is a probability distribution over available actions. In a Markov Decision Process (MDP), a trajectory accumulates features based on states and actions. An augmented feature map expands the feature space with binary variables. Agents aim to maximize the total reward for a trajectory, influenced by the MDP's reward function. Conventional Inverse Reinforcement Learning (IRL) focuses on inferring a reward function from observed behavior. Constraints in an MDP are behaviors not explicitly disallowed by the structure but are infeasible for the system being modeled. For example, a generic MDP for cars may allow speeds up to 500km/h, but specific constraints for a car on a road may limit this. The specific car on a roadway may have constraints on speed and acceleration due to legal and physical limitations. These constraints define sets of state-action pairs that violate system specifications, including state, action, and feature constraints. The constraints in an MDP involve state-action pairs that violate system specifications, including feature constraints. By augmenting the set of features, state and action constraints become special cases of feature constraints. Compound constraints can be obtained by combining constraint sets. Adding a constraint to an MDP influences agent behavior by restricting state-action pairs in trajectories. Actions in each state must be restricted to prevent violating the constraint. In an MDP, constraints restrict state-action pairs to prevent violations. Substituting actions in states can lead to a modified MDP. Empty action sets in states are deemed invalid, as agents must have valid actions. Agents in deterministic MDPs avoid actions leading to empty states to prevent constraint violations. In MDPs, constraints are imposed to prevent violations. An additional constraint set is added for empty states, recursively until a fixed point is reached. The semantics of empty states in MDPs with stochastic transitions can have multiple interpretations. Nominal MDPs can be categorized as generic or baseline models. The car MDP in Section 3.2 is an example of a generic nominal model. The text discusses using a nominal model to infer constraints specialized to a specific car and task in MDPs. A baseline nominal MDP serves as a snapshot of a well-characterized system, allowing for the learning of a nominal reward function. This approach is demonstrated in a human obstacle avoidance example, where constraints are inferred from changes to the system with respect to the baseline. The text discusses inferring constraints in a constrained MDP based on demonstrations. The goal is to maximize the likelihood of constraints given the demonstrations. The approach involves solving demonstration likelihood maximization using a maximum entropy model. In a deterministic MDP, the probability of a trajectory being executed is exponentially proportional to the reward earned. The parameter \u03b2 describes how closely an agent optimizes the reward function. For finite horizon planning, the partition function is the sum of exponentially weighted rewards for feasible trajectories. Constraints modify the set of feasible trajectories. The set of feasible trajectories in a deterministic MDP is denoted by \u039e M. Constraints alter feasible trajectories, affecting the probability of observing a set of demonstrations. The goal is to maximize demonstration probability by choosing constraint set C to minimize Z(C) while not invalidating any demonstrated trajectory. The value of Z(C) is minimized by maximizing the sum of feasible trajectories. The optimization involves maximizing the sum of exponentiated rewards of infeasible trajectories by choosing a constraint set C to minimize Z(C). The constraint must not conflict with any demonstrations to be learned effectively. Future work aims to relax the requirement of isolating successful, constraint-respecting demonstrations in order to learn about constraints generally respected by a set of demonstrations. The solution to equation (8) for deterministic MDPs can also approximate optimal constraint selection for stochastic MDPs if stochastic outcomes have little effect on behavior. Reformulating the approach based on maximum causal entropy is needed to fully address the stochastic case, which is saved for future work. Careful selection of the constraint hypothesis space C is crucial for meaningful solutions to equation (8). In order to avoid the trivial solution of choosing the most restrictive constraint, one approach is to use domain knowledge to limit constraints to a library of plausible sets. Another method is to use minimal constraint sets for the hypothesis space, gradually expanding the estimated constraint set without overfitting to demonstrations. This involves selecting minimal constraints for states, actions, or features iteratively. The approach for iteratively growing the estimated constraint set involves finding the most likely minimal constraint set by calculating expected feature counts using an augmented indicator feature map. Ziebart et al. (2008) proposed an algorithm for this purpose, which computes the expectation of the total number of times a feature will be accrued per trajectory. The algorithm presented addresses the problem of tracking state visitations and feature accruals in trajectories. It produces a maximum entropy distribution over trajectories while avoiding counting additional accruals for trajectories that have already accrued a feature. The input includes the MDP, time horizon, and a time-varying policy, with the output being an array representing the expected proportion of trajectories. The algorithm presented deals with tracking state visitations and feature accruals in trajectories by producing a maximum entropy distribution. It aims to find the most likely constraint set that explains the behavior, even if combining minimal constraints is necessary. This problem is analogous to the maximum coverage problem and can be solved using a simple greedy algorithm with known suboptimality bounds. Algorithm 2 adapts a greedy heuristic to solve the constraint inference problem by growing the estimated constraint set at each iteration. The suboptimality bound for this approach is analogous to the greedy solution for the maximum coverage problem. The algorithm adapts a greedy heuristic to grow the constraint set based on KL divergence, ensuring alignment with empirical data. The grid world MDP environment involves moving through a 9x9 grid to reach a goal state. The algorithm grows the constraint set based on KL divergence in a grid world MDP environment, moving through a 9x9 grid to reach a goal state. State-action pairs produce distance features with negative rewards for short trajectories. Additional green and blue features are generated by certain actions. The true MDP is shown with constraints, while a nominal MDP is used for constraint inference. Estimated constraints align with true constraints, but not all are identified. The performance of the approach varies based on the number of available demonstrations and the selection for the threshold d DKL. Lower values of d DKL lead to greater false positive rates, while having more demonstrations reduces the rate of false positives. More demonstrations also allow the behavior predicted by constraints to better align with the system. With fewer than 10 demonstrations and a low d DKL, there is a trade-off between low KL divergence and a high false positive rate. Selecting d DKL = 0.1 strikes a balance, producing few false positives with sufficient examples and lower KL divergences. The results show obstacles in the human's environment and learned constraints. Trajectories from humans navigating around obstacles are mapped into a grid world for analysis. The MaxEnt IRL method successfully predicted the existence of a central obstacle based on human demonstrations without additional environmental features. While not all constrained states were estimated, those identified made obstacle states unlikely to be visited. Adjusting the d DKL threshold could help identify additional constraints. Our novel technique for learning constraints from demonstrations improves upon previous work in constraint-learning IRL by identifying the most likely constraint(s) in a principled framework. The numerical results in Section 4 highlight the usefulness of our approach, although it is based on a formulation that only holds for deterministic MDPs. We plan to explore a maximum causal entropy approach to handle stochastic MDPs. The methods presented here for learning constraints from demonstrations aim to handle stochastic MDPs and relax the requirement for demonstrations to be constraint-violation-free. Algorithm 1 tracks expected feature accruals over time, suggesting potential for reasoning about non-Markovian constraints. The formulation of maximum likelihood constraint inference for IRL shows promising results and opens avenues for further investigation. In our context, constraints describe how observed behaviors differ from possible behaviors in the MDP structure. Demonstrations are assumed to be consistent with system constraints, avoiding empty states. Stochastic transitions to empty states are omitted from demonstrations, impacting the MDP's modification for reasoning about demonstrated behavior. When modifying the MDP to reason about demonstrated behavior, updated transition probabilities are needed to eliminate transitioning to empty states, which are never observed in demonstrations. The modified probabilities are related to observed state-action pair frequencies, reflecting the behavior expected in selected trajectories for demonstrations. These modifications do not directly reflect the underlying system's reality but capture the apparent behavior observed in demonstrations. The modifications to deterministic MDPs eliminate transitioning to empty states not observed in demonstrations. The solution to finding constraints is akin to solving the maximum coverage problem, where trajectories are covered based on observed behavior. Algorithm 2 iteratively constructs the solution by taking the union of previous constraints, resembling a greedy approach to the maximum coverage problem. The solution to the maximum coverage problem is applied to find constraints in deterministic MDPs, eliminating transitioning to unobserved states. The suboptimality bound for the greedy solution is used to determine the eliminated probability mass."
}