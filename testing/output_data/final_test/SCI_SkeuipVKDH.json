{
    "title": "SkeuipVKDH",
    "content": "In unsupervised learning of disentangled representations, penalizing total correlation of latent variables is a promising method. However, this strategy can fail due to a discrepancy between sampled latent representation and its mean representation. Low total correlation of sample distribution does not guarantee low total correlation of the mean representation. A remedy called RTC-VAE is proposed to rectify this issue, showing a more reasonable distribution of the mean representation in experiments. Experiments show that our model has a more reasonable distribution of the mean representation compared with baseline models like \u03b2-TCVAE and FactorVAE. Recent works suggest that decomposing the ELBO could lead to distinguishing the factor of disentanglement, with a focus on total correlation (TC). Adding weights to this term in the objective function of a VAE model can help learn a disentangled representation. The total correlation of a sampled representation should describe the level of factorising, with a low value suggesting a less entangled joint distribution. However, a high total correlation of the mean representation can lead to undesirable entanglement. As regularization strength increases, the total correlation of sampled and mean representations are negatively correlated. The total correlation of mean representations and sampled representations are negatively correlated. Locatello et al. (2018) questioned the effectiveness of disentanglement methods, stating that unsupervised learning of disentangled representations is challenging without inductive biases. The issue of total correlation in penalizing strategies can be addressed by adding a penalty term on the variance of sampled representations. In response to the challenge of total correlation in penalizing strategies, a penalty term is proposed to address the issue by aligning sampled representations with mean representations. Various methods for estimating total correlation are studied and compared against the ground truth value on the multivariate Gaussian distribution. The curse of dimensionality is highlighted as a drawback of using (minibatch) estimators. The method of (minibatch) estimators suffers from the curse of dimensionality and drawbacks, leading to decreased estimation accuracy with higher latent space dimensions. Total correlation measures the difference between joint and marginal distributions of random variables, indicating entanglement. Disentanglement is sought through low total correlation of latent variables. The total correlation of sample representation and mean representation can differ significantly. Forcing the former to be small does not ensure the latter is small. Theorem 1 explains how a mean representation with arbitrarily large total correlation can have sample representations with bounded total correlation. The proof details are in Appendix A.3. Theorem 1 also clarifies how one can make T C(\u00b5) arbitrarily large with fixed parameters, providing insight into contradictions observed by Locatello et al. Theorem 1 explains the contradiction observed by Locatello et al. that a low total correlation of sample representation does not guarantee a low total correlation of mean representation. Neural networks can easily find a distribution with low TC(z) when only penalizing TC(\u00b5), leading to a disparity between TC(\u00b5) and TC(z). This highlights the need for a regularizer to penalize the difference between the distributions of \u00b5 and z. In the study of disentanglement, a modification of the VAE framework introduced an adjustable hyperparameter \u03b2 to balance latent channel capacity and independence constraints with reconstruction accuracy. FactorVAE and \u03b2-TCVAE were proposed to address the trade-off between reconstruction quality and disentanglement by penalizing total correlation between latent variables. These methods aim to improve the distribution of \u00b5 and z for better representation learning. The difference between FactorVAE and \u03b2-TCVAE lies in their strategies for estimating total correlation. Chen et al. (2018) used formulated estimators, while Kim & Mnih (2018) utilized the density-ratio trick. These approaches are discussed further in Section 5. Additionally, there are works on representative learning with inductive biases, such as Rolinek et al. (2019). The disentanglement metric will be covered in Section 6. The introduced objective function penalizes total correlation in the average evidence lower bound (ELBO). The drawback of the current approach is that while sample representations may appear disentangled, the mean representation can still be entangled. To address this issue, RTC-VAE penalizes off-diagonal terms in the covariance matrix to prevent vanishing mean representations. DIP-VAE incorporates an additional penalty term to ensure a distance between the mean representation and 1. The DIP-VAE method introduces a penalty term to create a distance between the mean representation and 1. However, this approach does not always outperform other VAEs due to conflicting penalty terms leading to model convergence issues. In contrast, our objective penalizes the standard deviation instead of the mean, forcing the distribution of z to be similar to the mean distribution. This helps prevent large total correlation in the mean representation and low total correlation in z. The naive Monte Carlo method underestimates total correlation, but Kim & Mnih (2018) proposed a discriminator network using the density-ratio trick to address this issue. Chen et al. (2018) introduced two estimators of total correlation, Minibatch Weighted Sampling (MWS) and Minibatch Stratified Sampling (MSS), with MSS described as sampling from a minibatch of samples. Density-ratio trick can also be used to estimate KL-divergence. The implementation of MSS in Chen et al.'s code is not clear, specifically the computation of the log importance weight matrix. In their experiment, they implemented MSS as MSS1, while Chen et al.'s implementation is denoted as MSS0. Proposition 1, a fundamental property in information theory, is used to approximate the total correlation of the mean representation in latent space. In the comparison of methods MWS, MSS 0, and MSS 1 for estimating total correlation in latent space, it was found that MWS tends to underestimate, while MSS 0 and MSS 1 are more accurate for dimensions \u2264 4. However, for higher dimensions, MSS 0 and MSS 1 tend to overestimate when total correlation is small. Overall, MSS 1 provides estimates closer to the ground truth than MSS 0. The text discusses a formalized way of analyzing data to convey ideas directly. It introduces a scenario with specific distributions and claims that when total correlation is low, only certain elements have bounded values while others are very small. It provides a proof for a 1-D case and extends it to D-dimensions. In D-dimension, the probability P(|z is not small when t is as small as \u03c3. Assigning weights to elements q(z) does not significantly change the analysis. \u03b2-TCVAE shows increasing total correlation with higher regularization strength. Strongly correlated z's can lead to arbitrarily large total correlation. As \u03b2 increases, VAE trained with certain estimators may encourage dimensions of very low variance, leading to strong correlations between dimensions. This can result in a sampled distribution with very low total correlation, which is not ideal as latent dimensions should not be fewer than ground truth. Models trained on datasets like dSprites can learn to represent complex geometry with multiple dimensions, resulting in more active dimensions than expected. In our implementation, we use the method of discriminators (density-ratio trick) to represent complex geometry with multiple dimensions. The experiment shows that this method provides a more stable estimation of total correlation when training VAE. We use datasets like dSprites, Shapes3D, and 3D faces for our experiments. Further tests on a larger scale are planned for the future. Each model is trained with 10 different initializations, and hyperparameter \u03b2 ranges from 2 to 10. Hyperparameter \u03b7 is fixed at 10 due to the variance term in the equation becoming small shortly after training begins. In experiments, a higher value of \u03b7 does not bring further improvement since z is close to \u00b5 already. Lower values may not guarantee z being close to \u00b5. RTC-VAE has lower T C mean with different regularization strength than FactorVAE. The T C mean behaves almost identically as T C sample in RTC-VAE. The problem of contradictory behaviors of T C mean and T C sample is remedied by RTC-VAE. Additionally, the ELBO of RTC-VAE converges faster than FactorVAE. FactorVAE tends to have some strongly distributed latent dimensions. FactorVAE tends to have strongly correlated latent dimensions, while RTC-VAE shows well factorized latent distributions. There is no widely accepted metric for measuring disentanglement, with various attempts being challenged by Locatello et al. Different metrics vary due to different initializations and datasets. Some proposed methods include using a classifier to measure each dimension of latent space and ground truth factors, as well as a metric that considers modularity, compactness, and explicitness. In analyzing disentanglement, Ridgeway & Mozer (2018) discussed the importance of compactness and modularity in latent representations. They argued that a modular representation should convey information of at most one ground truth factor per dimension. However, multiple latent dimensions can work together to represent multiple factors while remaining disentangled. Our RTC-VAE rectifies the total correlation penalty to improve the distribution of mean representations compared to baseline models. Theoretical proofs help diagnose symptoms of entanglement, contributing to explainability in unsupervised learning of disentangled representations. Theorem 1 restated: Let \u00b5 \u223c N(0, \u03a3) and \u03c3 j be the standard deviation of \u00b5 j, j = 1, \u00b7 \u00b7 \u00b7 , D, and max j \u03c3 j = c 0. For a fixed \u00b5, let z \u223c N(\u00b5, \u03a3(\u00b5)), where \u03a3(\u00b5) is diagonal and satisfies certain conditions. Then TC(z) \u2264 C for some C = C(R, c 0, \u00b7 \u00b7 \u00b7 , c 4, l) > 0. Proof: Since KL-divergence is non-negative, if TC(z) + is bounded, then TC(z) must be bounded. The KL-divergence between two distributions P and Q is defined as the density function for a multivariate Gaussian distribution N(\u00b5, \u03a3) is p(x) = 1/(2\u03c0)^(n/2) det(\u03a3)^(1/2) exp(\u22121/2 (x \u2212 \u00b5)^T \u03a3^\u22121 (x \u2212 \u00b5))."
}