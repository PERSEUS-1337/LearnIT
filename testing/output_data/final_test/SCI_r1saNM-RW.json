{
    "title": "r1saNM-RW",
    "content": "Support Vector Machines (SVMs) are popular for classification and regression analysis but can be computationally expensive to train at a large scale. A novel coreset construction algorithm is proposed in this paper to generate compact representations of massive data sets and speed up SVM training. The coreset is a weighted subset of data points that allows SVMs trained on it to be competitive with those trained on the original data set. The analysis provides bounds on the number of samples needed for accurate approximations and conditions for the existence of compact and representative coresets. The practical effectiveness of the algorithm is empirically evaluated. Coresets are small weighted subsets of input points that approximate the original data set, accelerating machine learning algorithms like kmeans clustering and logistic regression. They provide a compact representation of data structure with provable approximation guarantees for specific algorithms. Coresets are small weighted subsets of input points that approximate the original data set, providing a compact representation with provable approximation guarantees for machine learning algorithms. They greatly reduce manual labeling and computation time for training, supporting applications with streaming data. In addition to serving as efficient representations of data sets, coresets are used to automate tasks like generating video representations and detecting outliers. They are also valuable in training Support Vector Machines (SVMs) for classification and regression analysis. A new coreset construction algorithm is introduced in this paper to accelerate SVM training on large-scale data sets by approximating the importance of each point efficiently. The paper introduces a new coreset construction algorithm for accelerating SVM training on large-scale datasets by approximating the importance of each point efficiently. It includes lower bounds analysis on sample requirements, efficiency and theoretical guarantees analysis, and evaluations on synthetic and real-world datasets showcasing practical effectiveness. The CVM method provides a (1 + \u03b5) 2 approximation to the two-class L2-SVM in O(n/\u03b5 2 + 1/\u03b5 4 ) time, with noted inferior accuracy and convergence properties compared to existing SVM implementations. Various geometric approaches and algorithms have been explored to address the quadratic optimization problem inherent in SVM, including Frank-Wolfe and Gilbert's algorithm for approximations and polytope distance reduction. The algorithm trains an SVM in linear time using a primal-dual approach combined with Stochastic Gradient Descent. It generates an \u03b5-approximate solution with high probability by accessing single features of training vectors. The method is nondeterministic and returns the correct \u03b5-approximation with a constant probability. BID22 introduces Pegasos, a stochastic sub-gradient algorithm for solving SVM optimization problems in linear time. It offers probabilistic guarantees and performs well empirically, but cannot be easily extended to streaming settings due to changes in the gradient with new data points. BID13 presents an alternative approach to training SVMs in linear time using the Cutting-Plane algorithm for classification and ordinal regression. BID10 constructs coresets to approximate maximum margin separation for input data. The feature space is defined by points and queries as d + 1 dimensional. The hinge loss of a point with respect to a separating hyperplane is calculated. The SVM objective function is presented as the sum of per-point evaluations. Soft-margin SVM Problem is defined for a set of weighted points with a regularization parameter. The primal of the SVM problem is expressed as a quadratic program for a set of weighted points. Coresets provide a compact representation of the data set to approximate the SVM cost function uniformly. The goal is to reduce the runtime of standard SVM algorithms by compressing the input points to a sublinear size. An \u03b5-coreset is defined for a set of weighted points with a weight function. Our goal is to efficiently construct an \u03b5-coreset, (S, v), from a set of weighted points. The algorithm involves generating an importance sampling distribution with upper bounds on point importance, sampling points, and assigning weights inversely proportional to sample probability. The number of points needed for the \u03b5-coreset is determined by a function. The number of points needed to create an \u03b5-coreset with a high probability is determined by the desired accuracy \u03b5, failure probability \u03b4, and data set complexity. The algorithm uses importance sampling to generate a distribution based on input points, resulting in a weighted set that serves as an unbiased estimator for any query. Sampling points according to their sensitivity minimizes estimation variance. The algorithm for creating coresets uses importance sampling to generate a distribution based on input points, resulting in a weighted set that serves as an unbiased estimator for any query. Coresets provide efficient approximations to the optimal SVM solution, with the merge-and-reduce technique ensuring the algorithm is run against small partitions of the original data set. This approach leverages the composability of coresets, reducing the coreset construction problem for a large set of points into computing coresets for a smaller set. The algorithm for creating coresets uses importance sampling to generate a distribution based on input points, resulting in a weighted set that serves as an unbiased estimator for any query. Coresets provide efficient approximations to the optimal SVM solution, with the merge-and-reduce technique ensuring the algorithm is run against small partitions of the original data set. This approach leverages the composability of coresets, reducing the coreset construction problem for a large set of points into computing coresets for a smaller set. Obtaining polylogarithmic size coresets implies nearly linear time for coreset construction, with an extension to accelerate performance by using an efficient gradient-based method like Pegasos for computing an approximately optimal solution in scenarios with small C. The algorithm for creating coresets uses importance sampling to generate a distribution based on input points, resulting in a weighted set that serves as an unbiased estimator for any query. Coresets provide efficient approximations to the optimal SVM solution, with the merge-and-reduce technique ensuring the algorithm is run against small partitions of the original data set. This approach leverages the composability of coresets, reducing the coreset construction problem for a large set of points into computing coresets for a smaller set. The optimal solution can be computed using an Interior Point Method, and any SVM solver can be used as a replacement. The main result is Theorem 9, which establishes sufficient conditions for the sensitivity of a point in terms of the complexity of the data set. The algorithm CORESET(P, u, \u03b5, \u03b4) computes an \u03b5-coreset (S, v) with probability at least 1 \u2212 \u03b4. Theoretical results show the influence of the regularization parameter, C, on the coreset size. Sensitivity of points is defined based on a weight function. A hard point set with sensitivity sum \u2126(nC) suggests a large C requires more samples. Lemma 5 proves a set of n points with sensitivity bounded below by \u2126. Lemma 5 establishes a hard point set with sensitivity bounded below by \u2126, which can also prove a bound nearly exponential in dimension, d. Corollary 6 states the existence of a set of n points with total sensitivity bounded below by DISPLAYFORM0. The upper bounds on sensitivity of each data point shed light on data sets where coresets of small size exist, potentially speeding up SVM training. Sensitivity is defined for an arbitrary point p = (x i , y i ) \u2208 P, with P yi and P c yi denoting points of label y i and its complement, respectively. The points are normalized to have a Euclidean norm of at most one. The sensitivity of points in a data set P is bounded by a certain formula. The sum of sensitivities over all points in P is also bounded by another formula. Theorem 9 states that Algorithm 1 can generate an \u03b5-coreset with a certain size and probability. Theorem 9 implies that \u03b5-coresets of polylogarithmic size can be obtained for reasonable \u03b5 and \u03b4, with d = O(polylog(n)). Corollary 10 shows that training an SVM on an \u03b5-coreset yields a competitive solution with the optimal solution on the full dataset. The sensitivity of a point is determined by its relative weight, distance to label-specific mean point, and distance to the optimal hyperplane. The sum of sensitivities determines the number of samples needed. The sum of sensitivities increases monotonically with the sum of distances of points from their label-specific means. An extension of Lemma 7 is provided for cases where only an approximately optimal solution to the SVM problem is available. The performance of the coreset construction algorithm is evaluated on synthetic and real-world datasets, comparing it to uniform subsampling and the Pegasos algorithm. The study evaluated the performance of coreset construction algorithms on various datasets, comparing them to uniform subsampling and the Pegasos algorithm. Results for relative error and sampling variance were averaged across 100 trials for different subsample sizes. Computation time for each approach was also measured. The experiments were conducted in Python on a specific machine. The study evaluated coreset construction algorithms on various datasets, including Synthetic, Synthetic100K, HTRU 2, CreditCard 3, and Skin 4. Evaluation included computing relative error of sampling-based algorithms compared to optimal SVM solution cost. The study evaluated coreset construction algorithms on various datasets, comparing the error of the sampling-based approach to the optimal SVM solution cost. The coreset construction algorithm generated a compact solution with lower variance compared to uniform subsampling. The study evaluated coreset construction algorithms for generating compact representations of data points to speed up SVM training. The method is also applicable to streaming settings and can potentially be extended to other machine learning algorithms like deep learning. The estimator variance of query evaluations is lower with judicious sampling distribution based on points' sensitivities compared to uniform sampling. Points are defined with weights and sensitivities for bounding sensitivity. The sensitivity of points is crucial for bounding sensitivity in query evaluations, leading to lower estimator variance with judicious sampling distribution based on sensitivities. Points are defined with weights, and the sensitivity of any arbitrary point is analyzed for bounding purposes. The sensitivity of points is crucial for bounding sensitivity in query evaluations, leading to lower estimator variance with judicious sampling distribution based on sensitivities. Points are defined with weights, and the sensitivity of any arbitrary point is analyzed for bounding purposes. For any p i \u2208 P +, let p + \u2206i = p + \u2212 p i and p \u2212 \u2206i = p \u2212 \u2212 p i. The sensitivity can be decomposed into the sum over two disjoint sets, S(P) = S(P 1 ) + S(P \u2212 ). The coreset constructed by the algorithm is an \u03b5-coreset with probability at least 1 \u2212 \u03b4 if |S| \u2265 \u2126(t \u03b5 2 d log t + log(1/\u03b4)), with the computation time dominated by computing the optimal solution of the SVM problem using interior-point Method. The algorithm constructs an \u03b5-coreset (S, v) for (P, u) with a time complexity of O(n^3) where L is the bit length of the input data. The coreset ensures a bounded sensitivity in query evaluations, reducing estimator variance through judicious sampling distribution based on sensitivities."
}