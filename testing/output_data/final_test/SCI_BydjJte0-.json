{
    "title": "BydjJte0-",
    "content": "Many deployed learned models are black boxes, keeping internal information hidden for security reasons. However, this work demonstrates that neural network attributes can be exposed through a sequence of queries, making them vulnerable to attacks. This technique can also be used to protect private content from automatic recognition models. The distinction between white box and black box models is blurred, as black-box models take query inputs and return outputs without revealing their internal workings. Our work focuses on inferring information about black box models to turn them into white box models, which has legal implications for intellectual properties and privacy-sensitive training data. Revealing hidden details can make the model more vulnerable to attacks but can also be useful in scenarios like protecting private content from recognition models. Our work aims to understand the connection between white box and black box approaches in neural network models. We introduce the concept of \"model attributes\" to categorize information about the model's architecture, optimization process, and training data. This research is crucial for investigating the type and amount of information that can be obtained from a black-box access to a model. Our work investigates the connection between white-box and black-box neural network models by categorizing model attributes and exploring the information that can be extracted from black-box models through querying. We propose novel metamodel methods to predict model attributes using query input-output pairs, contributing to the understanding of internal information extraction from black-box models. The text discusses the extraction of information from black-box models through query inputs, optimizing query inputs for more information extraction, studying factors affecting model generalisability, and empirical verification of susceptibility to adversarial attacks. Model extraction attacks aim to reconstruct model parameters or build avatar models to mimic target models. Our approach complements the avatar method by training a metamodel to predict model hyperparameters. Membership inference attacks can infer training data, model architecture, and optimization process, enabling more effective attacks on black box models. Adversarial image perturbations (AIPs) are small perturbations that mislead the network. Research on this topic has grown recently, showing that minimal perturbation can completely mislead an image classifier. Effective AIPs require gradients of the target network. Different ways to attack black boxes include approximating gradients with numerical gradients and using the avatar approach to train a white box network. Our metamodel complements the avatar approach by predicting model hyperparameters. Our metamodel complements the avatar approach by determining network hyperparameters. Adversarial examples can transfer between networks, especially within the same architecture family. The metamodel exploits this transferability to generate targeted AIPs. The metamodel is a classifier of classifiers that submits query inputs to a black box model and returns predicted model attributes. It learns to infer model attributes and searches for query inputs to extract more information. The main methods are introduced in the context of MNIST digit classifiers, which are quick to train and were used for meta-training. The proposed approach involves training a diverse set of 11k MNIST classifiers for meta-training and evaluation of metamodels. The dataset of classifiers, MNIST-NETS, shares a common convnet skeleton architecture and covers 12 model attributes. The approach is generic and not limited to specific tasks or data types. The proposed approach involves training a diverse set of MNIST classifiers with different architectural hyperparameters, optimisation algorithms, and training data variations. The convnet structure includes linear mapping, non-linear activation, and optional dropout. The metamodel needs to be trained over a diverse set of models to learn generalisable features. The study involves training a diverse set of MNIST classifiers with various architectural hyperparameters, optimization algorithms, and training data variations. The models have been trained with learning rate 0.1 and momentum 0.5 for 100 epochs on a GPU machine, taking around 5 minutes to train each model. Training of 10,000 classifiers has taken 40 GPU days. The study trained 10k classifiers on a GPU machine, taking 40 GPU days. Low-performance classifiers were pruned, resulting in 8,582 classifiers. Ensembles were created by grouping identical classifiers, leading to 11,282 final models. Multiple splits of MNIST-NETS were introduced with varying generalization requirements. Each split had 5,000 training, 1,000 testing, and 5,282 attributes evenly covered. Including a black-box model in the meta-training set can make attribute prediction easier. The study introduced different splits for training and testing models, including Random (R) and harder Extrapolation (E) splits. The metamodel predicts attributes of black-box models by submitting query inputs and observing outputs. Three approaches for metamodels, collectively named kennen 2, are proposed. kennen-o selects a fixed set of queries from a dataset and learns a classifier to predict 12 attributes in f. The training objective involves meta-training models and cross-entropy loss. The classifier is modeled via multilayer perceptron with two hidden layers. In experiments, MLP outperformed linear classifiers. The optimization problem is solved accordingly. The optimization problem in equation 1 is solved via SGD by approximating the expectation over f \u223c F by an empirical sum over the training split classifiers for 200 epochs. For query inputs, a random subset of n images from the validation set is used. The method kennen-i crafts a single query inputx over the meta-training models to repurpose a digit classifier f into a model attribute classifier. The method kennen-i repurposes a digit classifier into a model attribute classifier by crafting inputs that leak internal information via digit prediction. These inputs are then used to predict attributes in a black-box model. The training objective involves the 10-dimensional output of the digit classifier f. The input must remain a valid image. The method kennen-i repurposes a digit classifier into a model attribute classifier by crafting inputs that leak internal information via digit prediction. The loss L, along with the attribute label y of f, guides the digit prediction f(x) to reveal the attribute a instead. The optimisation problem involves training the digit classifier with the attribute label as ground truth, averaging the loss over models instead of images, and optimising the input x instead of the model f. The learned query input x is used to predict the attribute for the black box model g. Gradient information from g is not used. The input x is initialised with a random sample from the MNIST validation set and SGD is run for 200 epochs, with x truncated back to [0, 1] to enforce the constraint. The method kennen-i can only predict a single attribute at a time. The method kennen-i can only predict one attribute at a time and has limitations in predicting attributes with more than 10 classes. To overcome these drawbacks, kennen-io combines kennen-i and kennen-o approaches by using an additional interpretation module to support the optimization of multiple query inputs. kennen-io supports optimizing multiple query inputs with MLP layers. The training objective includes predicting attributes with a black box model. To enhance stability, the model is initialized with kennen-o and updated alternately for 200 epochs. The method constructs datasets and metamodels to extract information from black-box classifiers. In this section, we evaluate the ability of kennen to extract information from black-box MNIST digit classifiers. Metamodels, kennen-o/i/io, show high prediction accuracy for various attributes. The neural network output contains rich information, with attributes like dropout and max-pooling predicted with high precision. Outputs of networks trained with dropout layers form clusters, explaining the good prediction performance. The metamodels kennen-o/i/io demonstrate high prediction accuracy for various attributes, including algorithm and batch size. kennen-i, despite lower performance, is efficient for predicting kernel size and pooling attributes linked to spatial structure. kennen-io outperforms kennen-o/i in all attributes. The study examines factors contributing to successful prediction of black box internal attributes. The study evaluates the prediction accuracy of metamodels kennen-o/i/io for black box internal attributes. Performance is measured by varying the number of meta-training models, queries, and query output quality. Training kennen-o with different numbers of meta-training classifiers shows a diminishing return but performance improvement with larger sets. The performance of kennen-o against the number of queries with probability output is also analyzed. The study evaluates the prediction accuracy of metamodels kennen-o/i/io for black box internal attributes. The average performance saturates after \u223c 500 queries, but with only \u223c 100 queries, ample information about the neural network can be retrieved. Different output types are compared, showing that bottom-1 outputs leak more information than top-1 outputs. The accuracies for 100 probability, top-10 ranking, bottom-1, and top-1 outputs are 73.4%, 69.7%, 54.4%, and 39.5% respectively, with bottom-1 outputs containing the most information. The study compares different output types in prediction accuracy for black box internal attributes. Outputs beyond top-1 contain more information, with diminishing returns for each additional label. Extrapolation split experiments show the impact of meta-training model distribution. The study compares prediction accuracy of shallower models (#layers \u2264 10) on the training split and deeper ones (#layers > 10) on the testing split. E-split accuracy is evaluated over non-splitting attributes A \\ \u00c3. Normalised accuracy (N.Acc) compares E-split performance to R-split performance on non-splitting attributes. The study evaluates the normalised accuracies of kennen-o and kennen-io on R and E splits, with E-split performances being lower than R-split ones. It is recommended to include all expected black-box attributes during meta-training for better performance. The study compares kennen-o and kennen-io performances on R and E splits, with E-split results lower. Including all black-box attributes during meta-training is recommended for better performance. Kennen-io outperforms kennen-o in generalisability, especially under severe extrapolation. Future work will explore how out-of-domain query inputs enhance metamodel generalisation. Metamodels can extract inner details precisely, but full explanations are beyond the paper's scope. Input and output analyses show discriminative features for model attributes in metamodel inputs. The input data points are visualized using t-SNE to show discriminative features for model attributes. Clusters of same-colored points indicate highly discriminative features, with some attributes forming clear clusters while others are too complicated for 2-dimensional representation. See Appendix \u00a7D for experimental details. The clusters in the neural network outputs are too complex for 2-dimensional representation. For kennen-io, improved clusters are observed for pool and ks attributes. Crafted query inputs induce better clustering, leading to successful predictions. Confusion matrices for kennen-o/io show more confusion between similar classes. Semantic attribute information is present in the neural network outputs, indicating that metamodels can generalize beyond artifacts. The metamodels in kennen-io exhibit greater concentration of masses on correct and similar attribute classes, indicating improved generalization. Black-box access to a neural network reveals internal information, with kennen-io generalizing better than kennen-o. Performance decreases when the black-box classifier differs from metatraining classifiers. The best metamodel, kennen-io, shows decreased performance compared to metatraining classifiers. However, prediction accuracy for black box internal information remains high. Additional ImageNet experiments are conducted to assess practical implications on realistic image classifiers. The study uses kennen-o to predict architecture family of black-box ImageNet classifiers and explores attacking black boxes with adversarial examples using extracted information from 19 pretrained ImageNet classifiers. The study explores predicting classifier families (S, V, B, R, D) using kennen-o method with MLP architecture. 100 random queries from ImageNet validation set show high kennen-o performance (90.4%) compared to random chance (20.0%). In attacking ImageNet classifiers with adversarial image perturbations (AIPs), knowledge about the black box architecture family enhances effectiveness. AIPs are crafted perturbations to mislead models, with GAMAN being a robust variant. Generating AIPs against black boxes involves numerical gradient, avatar network, or transferability methods. The metamodel strengthens transferability-based attacks. Our metamodel enhances transferability-based attacks by showing that AIPs transfer better within an architecture family than across different families. We predict the black box family, such as ResNet, and generate AIPs against multiple instances within that family. Through systematic testing, we confirm that AIPs generalize better within a family when generated against multiple instances from the same family. The study shows that AIPs transfer better within an architecture family than across different families. Generating AIPs against multiple instances within the same family leads to more effective attacks. The misclassification rate is reported on ImageNet validation images, with within-family performances outperforming across-family ones. Targeting a specific black box family results in more effective AIPs compared to targeting all networks. Reverse-engineering enables more effective attacks in various scenarios. The study demonstrates that AIPs are more effective when targeting a specific black box family compared to targeting all networks. Misclassification rates vary depending on the scenario, with reverse-engineering leading to more successful attacks. The metamodel kennen accurately predicts architecture families for ImageNet classifiers, enabling focused attacks on black-box systems. The study shows that reverse-engineering a black box can make it more vulnerable to adversarial attacks. The dataset MNIST-NETS is discussed, along with the performance of kennen-io in comparison to kennen-o. The sensitivity of kennen-o performance is measured with respect to the number of queries and training samples. The study discusses the sensitivity of kennen-o performance to the choice of queries and the possibility of optimizing the query set. Training kennen-o with different numbers of queries shows greater sensitivity for smaller query sets. A new approach, kennen-io, efficiently solves a continuous optimization problem to find query inputs from the entire input space. Comparing kennen-io to kennen-o with multiple query samples shows that kennen-io outperforms kennen-o in exploration effectiveness. The study compares kennen-o and kennen-io in terms of exploration effectiveness and detectability of attacks. The experiment involves collecting 1000 black-box models and passing 100 query images for each model. The data points are visualized using t-SNE to show the results for kennen-o and kennen-io. Developing a method to optimize or sample natural queries for strong attacks is suggested as future work. In figures 9 and 10, results for kennen-o and kennen-io are shown using t-SNE visualization. The perturbed images and examples of AIPs are displayed in figure 7, with perturbations nearly invisible to the human eye. A diverse set of AIPs based on different architecture families is shown in figure 8, resulting in varied patterns."
}