{
    "title": "BkeU5j0ctQ",
    "content": "Deep neuroevolution and deep reinforcement learning (deep RL) are popular policy search approaches. Neuroevolution is stable but not sample efficient, while deep RL is more efficient but can be unstable. Combining these methods is a new approach for optimal results. Previous combinations used evolutionary algorithms or goal exploration with DDPG. This paper proposes a different combination using cross-entropy. The paper proposes a new method called CEM-RL, combining Cross-Entropy Method (CEM) with Twin Delayed Deep Deterministic Policy Gradient (TD3) for deep reinforcement learning. It is evaluated on traditional deep RL benchmarks. CEM-RL offers advantages over competitors in policy search, maximizing unknown utility functions. Research on policy search methods has grown due to deep neural networks, enabling efficient deep RL techniques and evolutionary methods like deep neuroevolution with ESs. Evolutionary methods are less sample efficient but offer higher parallelization capability. Evolutionary methods are less sample efficient than deep RL methods, as they learn from complete episodes. Deep RL methods, on the other hand, use elementary steps as samples and exploit more information. Off-policy deep RL algorithms can use a replay buffer to improve sample efficiency. However, off-policy deep RL methods like DDPG are known to be unstable and sensitive to hyper-parameter settings. In Section 4, a new method combining the cross-entropy method (CEM) with DDPG or TD3 is proposed. Section 5 explores the experimental properties of this CEM-RL method, showing advantages over individual components and a competing approach. The conclusion highlights the potential in combining evolutionary and deep RL methods for policy search. The Cross-Entropy Method (CEM) is a strong baseline in policy search problems despite its simplicity compared to deep RL methods. Research has explored synergies between evolution and reinforcement learning, with some similarities to meta-learning. Unlike other methods, the outcome of the RL process is not incorporated back into the agent's genome in this literature. The combination of evolution and reinforcement learning updates the same parameters iteratively, unlike the RL process where the outcome is not incorporated back into the agent's genome. The GEP-PG approach, similar to BID3, focuses on diversity rather than performance, leading to better final solutions and less variance during learning on benchmarks like Continuous Mountain Car and HALF-CHEETAH-V2. However, the sequential nature of the combination limits the efficient gradient steps of the deep RL part. The authors in BID18 introduce optimization problems with a surrogate gradient, modifying the covariance matrix of an ES to create a hybrid algorithm capable of outperforming standard gradient descent and pure ES methods on simple benchmarks. They suggest its potential application in RL due to the presence of surrogate gradients in Q-learning and actor-critic methods, although practical validation is needed. Their approach, like ours, uses a gradient method to enhance an ES, but differs in directly changing the sample distribution with gradient information. The authors in BID18 introduce optimization problems with a surrogate gradient, modifying the covariance matrix of an ES to create a hybrid algorithm capable of outperforming standard gradient descent and pure ES methods on simple benchmarks. They suggest its potential application in RL due to the presence of surrogate gradients in Q-learning and actor-critic methods, although practical validation is needed. Their approach, like ours, uses a gradient method to enhance an ES, but differs in directly changing the sample distribution with gradient information. In contrast, the closest work to ours is BID14, where they introduce the ERL algorithm, a combination of DDPG and a population-based evolutionary algorithm. The algorithm consists of a population of actors that are mutated and selected based on fitness, while a single DDPG agent is trained from the samples generated by the evolutionary population. This agent is periodically inserted into the population, leading to performance improvements when the gradient-based policy improvement mechanism of DDPG is efficient. The method presented combines CEM and TD3 to benefit from the gradient-based policy improvement mechanism of TD3, the stability of ESs, and the sample efficiency of importance mixing. This setup enhances information transfer between RL and evolutionary algorithms, unlike previous works that did not utilize the search efficiency of ESs. Evolutionary algorithms in the paper manage a limited population and generate new individuals randomly near elite individuals. Evolution strategies involve retaining only one individual from each generation, which is the mean of the distribution for generating new individuals. Estimation of Distribution Algorithms (EDAs) represent the population as a distribution using a covariance matrix \u03a3. Evolutionary algorithms manage a limited population by generating new individuals near elite individuals. Estimation of Distribution Algorithms (EDAs) represents the population using a covariance matrix \u03a3, defining a multivariate Gaussian function. Various instances of EDAs, such as the Cross-Entropy Method (CEM) and Covariance Matrix Adaptation Evolution Strategy (CMA-ES), are covered. The CEM involves fixing the number of elite individuals to a certain value K e, using the fittest individuals to compute the new mean and variance of the population for sampling the next generation. The generation is sampled with added variance to prevent premature convergence. Individuals are sampled by adding Gaussian noise around the mean of the distribution \u00b5 using the current covariance matrix \u03a3. The fitness of new individuals is computed, and the top-performing individuals are used to update the distribution parameters. CEM and CMA-ES differ in how they update the covariance matrix. The algorithm used combines elements of CEM and CMA-ES, using the current mean \u00b5 for estimating the new covariance matrix \u03a3. Gaussian noise is added to prevent premature convergence, with the standard deviation updated exponentially. Sampling from \u03a3 can be computationally intensive due to the large number of actor parameters. The Deep Deterministic Policy Gradient (DDPG) and Twin Delayed Deep Deterministic policy gradient (TD3) algorithms are off-policy, actor-critic deep RL algorithms. DDPG suffers from instabilities due to overestimation bias in critic updates, while TD3 addresses this by using two critics and taking the lowest estimate of action values. Properly tuned code baselines incorporating advanced mechanisms can improve the sensitivity to hyper-parameter settings. The CEM-RL method combines CEM with either DDPG or TD3, resulting in CEM-DDPG and CEM-TD3 algorithms. The mean actor \u03c0 \u00b5 is initialized with a random actor network, and a unique critic network Q \u03c0 is also initialized. A population of actors is sampled by adding Gaussian noise around \u03c0 \u00b5, and half are evaluated for fitness based on cumulative rewards. The critic is updated using one half of the actors, while the other half updates the actor following the critic's gradient for a fixed number of steps. The CEM algorithm evaluates actors based on critic feedback for a fixed number of steps. The top-performing half of the population is used to update the actor's parameters. The critic is trained from the replay buffer with new samples introduced at each generation. This involves training the critic for 10,000 mini-batches, divided into 2000 mini-batches per learning actor. The CEM algorithm evaluates actors based on critic feedback for a fixed number of steps. A pseudo-code of CEM-RL is provided in Algorithm 1. When the gradient steps increase actor performance, CEM incorporates those actors in its computations. If the gradient steps decrease performance, CEM focuses on standard samples around \u03c0 \u00b5. Poor samples are ignored as they do not provide new insight. However, all evaluated actors contribute to the replay buffer for future learning actors and the critic, facilitating exploration and information flow between deep RL and evolutionary parts. The CEM algorithm evaluates actors based on critic feedback for a fixed number of steps. Good actors found through evolution fill the replay buffer for the RL algorithm to learn. CEM-RL applies gradient steps at each iteration, unlike ERL which injects deep RL actors sporadically. CEM-RL does not use deep RL actors, unlike ERL. In Section 5.2, properties between ERL and CEM-RL are compared based on empirical results. The CMA-ES is considered more sophisticated than CEM, but CMA-ES's evolutionary path mechanism results in inertia in \u03a3 updates, hindering the beneficial effect of RL gradient steps. The CEM-RL algorithm's performance is studied in comparison to CEM and TD3 separately, exploring the impact of removing the CEM mechanism. The actor and critic hyper-parameters of DDPG are initialized, including the random actor, critic, target critic, and replay buffer. The algorithm involves drawing a population, training the actor and critic, evaluating fitness, and updating policies based on collected experiences. In this section, the CEM-RL algorithm is compared to ERL in various continuous control tasks using the MUJOCO physics engine. The performance of CEM-RL is evaluated with factors such as importance mixing, action noise, and tanh non-linearity. The implementation was done using the PYTORCH library and built around DDPG and TD3 implementations. For the ERL implementation, the networks were trained with TD3 and DDPG algorithms using tanh non-linearities in the actor network. The Adam optimizer with a learning rate of 1e \u22123 was used for both the actor and critic. The discount rate \u03b3 was also specified. Architecture details can be found in Appendix A. CEM-TD3 was compared to TD3 and a multi-actor variant of TD3, as well as CEM-RL to ERL based on various benchmarks. Additional results were presented in a separate section, comparing CEM-TD3 to CEM, TD3, and a multi-actor variant of TD3. Scores were reported for each algorithm after specific steps in the environment. CEM-TD3 outperforms CEM and TD3 on various benchmarks like HALF-CHEETAH-V2, HOPPER-V2, and WALKER2D-V2, with less variance than TD3. Results also show CEM-TD3 performs well on ANT-V2 and surprisingly, CEM excels on SWIMMER-V2. The ensemble method of CEM-TD3 improves exploration and stabilizes performance. Removing the CEM mechanism in an ablative study resulted in a population of 5 actors following the TD3 gradient. The algorithm CEM-TD3 outperforms multi-actor TD3 on various benchmarks, indicating that the evolutionary part contributes to its performance. Comparing CEM-RL to ERL using DDPG, results show the effectiveness of the combination scheme in CEM-TD3. After 1 million steps, CEM-RL methods outperform ERL on HALF-CHEETAH-V2, HOPPER-V2, and WALKER2D-V2. CEM-TD3 also outperforms CEM-DDPG on WALKER2D-V2. The difference in results from the ERL paper is attributed to using 10 seeds instead of 5 and potential implementation variations. In CEM-RL, increasing learning steps to 6 million improved performance. CEM-TD3 outperforms CEM-DDPG on most benchmarks, especially in hard environments like WALKER2D-V2 and ANT-V2. SWIMMER-V2 is an exception. Overall, CEM-RL generally outperforms ERL. In Appendix B, the influence of importance mixing on CEM and CEM-RL performance is investigated. Results show limited impact on sample efficiency of CEM-TD3, possibly due to faster covariance matrix movement in CEM-RL. Appendix C analyzes the effect of adding Gaussian noise to CEM-TD3 actions, with no conclusive evidence of performance improvement. In ERL, additional noise is beneficial as it converges to a unique individual, while CEM maintains exploration. ERL collapses towards a single individual, unlike CEM, due to sampling methods. On the SWIMMER-V2 benchmark, deep RL methods face deceptive gradient information, impacting performance. ERL shows better resistance to detrimental gradients compared to CEM-RL. Using a tanh non-linearity in actor architecture often leads to stronger performance than using RELU, suggesting neural architecture search in RL. Combining evolutionary and deep RL methods, CEM-RL outperforms evolution strategies, sample efficient off-policy deep RL algorithms, and ERL. Despite being evolutionary, CEM-RL remains competitive in sample efficiency. Our study questions why the simple CEM algorithm performs well on the SWIMMER-V2 benchmark. Importance mixing and adding noise to actions did not show clear benefits in our empirical study. Further investigations are needed to understand the critical properties for policy search algorithms' performance and sample efficiency. Future work may involve designing an ERL algorithm based on CEM and exploring the impact of neural architecture. The study questions the performance of the CEM algorithm on the SWIMMER-V2 benchmark and suggests further investigations into critical properties for policy search algorithms. Importance mixing and adding noise to actions did not show clear benefits in the empirical study. Future work may involve designing an ERL algorithm based on CEM and exploring the impact of neural architecture. Evolution strategies involve reusing samples from previous generations to improve sample efficiency. Importance mixing can enhance efficiency by a factor of ten, with most savings coming from using samples from the previous generation. In CEM-RL, adaptation is needed for actors taking gradient steps. Importance mixing is implemented in CEM as described in BID22. Importance mixing is applied to only half of the population not receiving gradient steps from the RL critic. This introduces some instability without significantly increasing sample efficiency. Performance may even decrease in some environments. The gain in sample reuse is minimal in CEM-RL, varying across environments. Importance mixing is applied to only half of the population in CEM-TD3, showing limited impact on performance across different benchmarks. In some environments, importance mixing decreases final performance, while in others, it accelerates learning initially but results in equivalent final performances. This contrasts with previous findings and may be attributed to differences in search space dimensions. The experiments show that the dimensions of search spaces in CEM are larger than in previous studies, affecting covariance matrix estimation. CEM struggles to solve environments over one million steps, hindering exploration and sample reuse. RL gradient steps accelerate covariance matrix displacement, reducing sample reuse opportunities. In contrast to previous findings, importance mixing in CEM-TD3 has limited impact on performance across different benchmarks. The replay buffer of DDPG gets filled with noisy experiences, including action space noise and parameter space noise. In CEM-RL, only parameter space noise is used. Adding action space noise did not show significant performance improvement. CEM-TD3 explores enough of the action space on its own without the need for additional noise. CEM results in better exploration compared to adding Gaussian noise to actions. Evolutionary algorithms converge to a unique individual, while evolutionary strategies maintain exploration. The difference in policy parameter update dynamics in CEM-RL and ERL is highlighted, with a comparison shown in FIG6 for training on HALF-CHEETAH-V2. In ERL, the evolutionary population shows less diversity compared to CEM-RL, with redundancies in parameters leading to convergence to a single individual. CEM-RL introduces completely new samples in each generation, resulting in better parameter space exploration. Further analysis in ERL reveals a loss of intra-population diversity, with around 55% of populations sharing parameters. The results show that in ERL, around 55% of populations have a similarity above 80%. The parameters of the population in ERL concentrate around those of the DDPG actor, which quickly spreads into the population. The resulting DDPG actor is the elite of the population 80% of the time and is introduced into the population 98% of the time. The integration of the DDPG actor is passive, with little variation in exploration direction. The CEM-RL approach integrates gradient information differently by proactively exploring promising directions. Experiments on the SWIMMER-V2 benchmark show that CEM outperforms TD3 and ERL outperforms CEM-DDPG. However, in SWIMMER-V2, any deep RL method provides deceptive gradient information. In SWIMMER-V2, deep RL methods like CEM-RL and ERL are compared. CEM-RL is less efficient due to deceptive gradient effects, while ERL better resists this issue. DDPG actors are often rejected in favor of evolutionary populations. Khadka & Tumer found that DDPG actors were rejected 76% of the time on SWIMMER-V2. Comparing CEM and ERL shows ERL performs better. In SWIMMER-V2, DDPG actors were rejected 76% of the time. Comparing CEM and ERL, ERL performs better on benchmarks. The impact of using different non-linearities in CEM-TD3 actors is explored, with results showing a significant drop in performance when switching from tanh to RELU. Switching from tanh to RELU in network architectures can lead to a significant drop in performance, as seen in benchmarks like ANT-V2 and SWIMMER-V2. The CEM algorithm on SWIMMER-V2 experienced a 60% performance drop with RELU. Different non-linearities in CEM-TD3 actors were explored, showing a notable decrease in performance when using RELU. Additional results on ANT-V2 benchmark are discussed in FIG0."
}