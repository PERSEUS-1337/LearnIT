{
    "title": "HJlF3h4FvB",
    "content": "Distillation is a method used to transfer knowledge from one model to another, often resulting in higher accuracy. The paper focuses on the concept of \"early stopping\" as a key factor in distillation. It is argued that an overparameterized teacher network harvests dark knowledge through early stopping, prioritizing informative information over non-informative data. This concept, known as Anisotropic Information Retrieval (AIR), is characterized by the eigenspace of the Neural Tangent Kernel (NTK). AIR provides a new understanding of distillation, which is further utilized to refine noisy labels through a self-distillation algorithm. The paper introduces a self-distillation algorithm to refine noisy labels by sequentially distilling knowledge from the network in the previous training epoch. The algorithm benefits from more than just early stopping, achieving better testing accuracy and avoiding early stopping entirely. Theoretical proof shows convergence to ground truth labels for overparameterized neural networks, leading to better generalization. Deep learning achieves state-of-the-art results in computer vision and natural language processing. Distillation is proposed to guide the training of a student network using a teacher network's predictions. Some studies show that a student network with the same parameters as the teacher network can outperform the teacher network. In distillation, knowledge transfer from teacher to student network improves training. Soft labels from teacher network provide \"Dark Knowledge\" encoded by relative probabilities of incorrect outputs. In distillation, knowledge transfer from teacher to student network improves training by transferring \"Dark Knowledge\" encoded by relative probabilities of incorrect outputs. The authors aim to theoretically explain how neural networks learn this Dark Knowledge and understand the regularization effect of distillation. They assume the teacher network is overparameterized and can memorize all labels, leading to convergence where the network's output matches the ground truth hard labels. The authors explain that the core factor enabling an overparameterized network to learn \"Dark knowledge\" is early stopping. They introduce the concept of Anisotropic Information Retrieval (AIR) to refine noisy labels using this knowledge transfer process. The authors introduce a new self-distillation algorithm using Anisotropic Information Retrieval (AIR) to refine noisy labels. By dynamically adjusting supervision strength, the algorithm can recover correct labels and achieve state-of-the-art results on Fashion MNIST and CIFAR10. Their theoretical study shows that the algorithm can sustain long training without overfitting noise, making it more user-friendly. The proposed algorithm aims to understand distillation theoretically and its role in refining noisy labels using overparameterized neural networks. It justifies the regularization effect of distillation by explaining how early stopping is essential for extracting dark knowledge from hard labels. The algorithm proposed in the curr_chunk utilizes distillation to train a neural network under label corruption, guaranteeing recovery of correct labels with a 2-loss. It does not require early stopping or a validation set, leading to better generalization. The overparameterized networks encourage the use of large models for improved results. In an experiment, a big model teaches a smaller model using distillation, showing that early stopping can enhance the process. The teacher model trained with 80 epochs yields the best results, indicating the importance of early stopping for extracting informative information. The concept of Anisotropic Information Retrieval (AIR) is introduced to exploit convergence speed discrepancies. Anisotropic Information Retrieval (AIR) exploits convergence speed differences in training overparameterized neural networks. Informative information converges faster than noise. Previous studies in various fields have shown selective bias in iterative algorithms, such as fitting low frequency components first in linear equation solvers and recovering image features earlier in image processing methods. In kernel learning, early stopping of gradient descent is equivalent to ridge regression. In neural network training, bias from larger eigenvalues is reduced quicker by gradient descent. Neural networks find low frequency patterns easily, but noisy labels can slow down training. Memorization effects show networks first learn with clean labels then with wrong labels. AIR of overparametrized neural networks is characterized using Neural Tangent Kernel to track gradient descent trajectory in infinitely wide networks. The trajectory of gradient descent algorithm learning infinitely wide neural networks is characterized by training the neural network using the 2-loss on a dataset. The evolution of the error can be formulated in a quasi-linear form, with the eigenvectors being orthogonal. The projection evolution is considered in this context. The eigenvectors are orthogonal, and the evolution of the projection of the loss function in different eigenspaces shows that the component in the eigenspace with a larger eigenvalue converges faster. This phenomenon, known as AIR, describes how the gradient descent algorithm searches for information components at different rates. The paper considers informative information as the eigenspaces associated with the largest eigenvalues of NTK. The components of label noise in the largest eigenspaces of NTK are decreasing, with the projection of the supervision signal to the eigenspace with a larger eigenvalue considered as useful information. The self-distillation algorithm calculates the ratio of the norm of the label vector in the top-5 eigenspace, showing a decrease in informative information with increasing noise levels. This leads to the exploration of how \"Dark Knowledge\" can aid in label refinement due to challenges in acquiring clean labels for training deep neural networks with noisy labels. Training deep neural networks with noisy labels is challenging. Recent studies have shown that neural networks tend to fit clean labels before noisy ones during training. Using distillation for label refinement has gained attention, with a new self-distillation algorithm proposed for recovering clean labels. Various methods, such as regularizing intrinsic dimensionality and joint optimization frameworks, have been suggested to address label corruption in deep models. In the literature of distillation, various methods have been proposed to refine noisy labels in deep neural networks. Tanaka et al. (2018) introduced a joint optimization framework, while Bagherinezhad et al. (2018) utilized distillation for ImageNet label refinement. Li et al. (2019) used early stopping to address label noise, but it can be challenging to tune. Our proposed self-distillation algorithm aims to utilize AIR for label refinement during training. The self-distillation algorithm proposed in this section utilizes informative knowledge learned in early epochs to guide training in later epochs, enabling the model to generalize well. This algorithm distills knowledge sequentially without the need for early stopping, generating the final model in one round of training. The self-distillation algorithm proposed in this section utilizes early epoch knowledge to guide later training, generating the final model in one round of training with minimal computational cost. The algorithm involves using a label function h(\u00b7) and an interpolation coefficient \u03b1 t, chosen based on the algorithm's purpose. The introduction of h(\u00b7) boosts AIR and information from previous epochs, with a theoretical justification provided for its performance. The self-distillation algorithm utilizes early epoch knowledge to guide later training efficiently. It involves a label function h(\u00b7) and interpolation coefficient \u03b1 t. The algorithm is theoretically justified for performance in binary classification tasks with overparameterized neural networks. The dataset consists of clean and corrupted data points, randomly sampled from a distribution P with balanced clusters. In this work, the dataset is defined as an ( , \u03c1) dataset where clusters lie within a Euclidean ball. The dataset assumes that data in the same cluster has the same ground truth label. Two-layer neural networks are considered for input data x \u2208 R d, with the output f determined by a weight matrix W and activation function \u03c6. The output vector on the data matrix is denoted as T. The output vector on the data matrix is denoted as T. Considering the MSE loss, the small eigenvalue of the neural network covariance matrix is denoted as \u03bb(D). The matrix C is composed of cluster centers, and \u039b is defined as min(\u03bb(C), \u03bb(X)). The convergence of the self-distillation algorithm to ground truth labels is established under certain conditions in Theorem 1, with bounded functions and a specific learning rate. The self-distillation algorithm ensures convergence to ground truth labels without early stopping, with improvements over previous results on noisy labels. The algorithm guarantees correct class labels on the training set and convergence of outputs to the ground truth. The algorithm guarantees convergence to ground truth labels without early stopping, leading to better generalization. Experimental results on CIFAR10 with 40% noise injection show that self-distillation prevents overfitting. The algorithm is tested on corrupted Fashion MNIST and CIFAR10 datasets with varying noise levels. In self-distillation, the algorithm adjusts \u03b1 t by setting 1 \u2212 \u03b1 t = \u03bb * accuracy for convergence without early stopping. Training is done on the shake-shake network with 32 channels using momentum SGD. Data augmentation includes mean subtraction, random flip, and random crops. Testing is performed on the original 32 \u00d7 32 image. The algorithm adjusts \u03b1 t by setting 1 \u2212 \u03b1 t = \u03bb * accuracy for convergence without early stopping. The direct ratio \u03bb is the only tuning parameter, with specific values set for different noisy levels in CIFAR10 and Fashion MNIST datasets. Self-distillation enhances the AIR and extracts more dark knowledge from the data via early stopping. Self-distillation algorithm dynamically enhances AIR for better generalization by preserving knowledge gained in earlier epochs without requiring early stopping. Information gain is achieved by subtracting informative information from the label vector, showing that the supervision signal gains more information than using noisy labels directly. This paper explores distillation in overparameterized neural networks, revealing the property of Anisotropic Information Retrieval (AIR) where informative information is learned first. Distillation of Dark Knowledge is attributed to early stopping, leading to a proposed self-distillation algorithm for noisy label refinement. The algorithm enhances AIR for better generalization by preserving knowledge without early stopping. The analysis is based on the assumption that the teacher neural network is overparameterized. When not overparameterized, the network will be biased towards the label even without early stopping. The bias providing more information is still an interesting problem. For label refinery, the analysis is mostly in the symmetric noise setting, with interest in extending to the asymmetric setting. Preliminaries discuss properties of the neural network, including the Jacobian matrix and random Gaussian initialization. The text discusses properties of neural networks, including the Jacobian matrix and random Gaussian initialization. It also presents perturbation analysis of the Jacobian matrix for clusterable data matrices. The analysis assumes overparameterization of the teacher neural network and explores bias in label refinement. The proof of the theorem divides the learning process into two stages. In the first stage, the neural network aims to achieve the right classification with 0-1 loss converging to 0. The margin is shown to be larger than 1\u22122\u03c1 2. In the second stage, the network further enlarges the margin, leading to 0-2 loss convergence to zero. This dynamic is illustrated by the average Jacobian for parameters W 1 and W 2 and data matrix X. The proof of the theorem involves two stages in the learning process. The neural network first aims for correct classification with 0-1 loss converging to 0 and a margin larger than 1\u22122\u03c1 2. In the second stage, the margin is further enlarged for 0-2 loss convergence to zero, illustrated by the average Jacobian for parameters W 1 and W 2 and data matrix X. The current chunk discusses the residual equation and defines the support subspace based on cluster membership in a dataset with K clusters and unit Euclidean norm points. The current chunk discusses an -clusterable dataset with K clusters and unit Euclidean norm points. It focuses on reducing the dataset to its cluster centers and setting parameters for optimization. The learning rate, residual norm, and support subspace are defined for the optimization process. The current chunk discusses the optimization process for a dataset with K clusters and unit Euclidean norm points. It focuses on setting parameters for optimization, such as the learning rate and residual norm. The dynamic of gradient descent and projection of the residual on the support subspace are also explained. The chunk concludes with a bound on the norm of the residual and a lemma regarding projection matrices. The optimization process for a dataset with K clusters and unit Euclidean norm points is discussed. Parameters for optimization, such as learning rate and residual norm, are set. The dynamic of gradient descent and projection of the residual on the support subspace are explained. A bound on the norm of the residual and a lemma regarding projection matrices are provided. The chunk concludes with adapting the theorem to a -clusterable dataset through perturbation analysis. The chunk discusses the optimization process for a dataset with K clusters and unit Euclidean norm points. Parameters like learning rate and residual norm are set, and the dynamic of gradient descent is explained. The chunk concludes with adapting the theorem to a -clusterable dataset through perturbation analysis. The parameters are updated by gradient descent, ensuring certain inequalities hold. The proof of the main theorem is finalized by setting specific values for \u0398 and C3. The optimization process for a dataset with K clusters and unit Euclidean norm points is discussed, along with adapting the theorem to a -clusterable dataset through perturbation analysis. The proof of the main theorem involves updating parameters through gradient descent to maintain specific inequalities. The optimization process for datasets with K clusters and unit Euclidean norm points is also discussed, along with adapting the theorem for a -clusterable dataset using perturbation analysis. The proof of the main theorem involves updating parameters through gradient descent to maintain specific inequalities. By lemma 1 and lemma 2, as long as k \u2265 \u03b1 lower bounding the eigenvalue of the gram matrix, we need to verify another condition related to R on M. By Bernstein's inequality, with probability 1 \u2212 \u03b4/4, we can choose M = d + C 4 log 8 \u03b4 + 2. Experimenting with a modified CIFAR10 dataset for binary classification, using class 2 and 7 as positive and the rest as negative, training resnet56 with MSE loss and specific parameters. In section 2.3 experiment, data batch (size=128) from testset was fetched and labels were randomly corrupted with noise levels 0-0.5. The ratio of label vector norm in top-5 eigenvalues of NTK was plotted. Information gain was calculated by comparing norm ratios from self-distillation algorithm and NTK eigenvalues, plotted over 1500 iterations."
}