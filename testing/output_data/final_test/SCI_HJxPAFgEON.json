{
    "title": "HJxPAFgEON",
    "content": "We explore weakly supervised structured prediction with reinforcement learning, focusing on optimizing metrics like accuracy in question answering or BLEU score in machine translation. Unlike common RL settings, the deterministic environment dynamics in SP are not fully utilized by model-free RL methods. To address this, we propose using model-based RL methods, specifically planning-based SP with a Neural Program Planner (NPP). The Neural Program Planner (NPP) is used for weakly supervised program synthesis from natural language, achieving a new state-of-the-art accuracy of 47.2% on the WIKITABLEQUESTIONS benchmark. Structured Prediction (SP) can be framed in a reinforcement learning (RL) framework, optimizing task metrics like question answering accuracy and BLEU score, while avoiding exposure bias compared to maximum likelihood training. When compared to maximum likelihood training commonly used in SP, previous works on applying RL to SP problems often use model-free RL algorithms and fail to leverage SP characteristics. In SP problems, the environment dynamics are known, deterministic, reversible, and can be searched due to a perfect model of the environment. This allows for the application of model-based RL methods that utilize planning as a primary component. For example, semantic parsers trained by RL typically rely on beam search for inference. The policy used for beam search in semantic parsing may struggle to assign the highest probability to the correct program due to locally normalized probabilities. This limitation leads to mistakes in tasks like WEBQUESTIONSSP, where important information may be ignored, and spurious programs may receive high probabilities. The beam search policy in semantic parsing struggles to assign the highest probability to the correct program, leading to spurious programs receiving high probabilities. Resolving this issue involves using the full program and its execution output to evaluate quality based on context, akin to planning. By training a value function to compute the utility of each token in a program, errors related to context can be addressed. Training a value function to compute token utility in a program involves considering program and token probability, attention mask, beam and question context, and overlap features. Applying a planner using the learned value function to re-rank candidates in the beam significantly improves accuracy. On the WIKITABLEQUESTIONS benchmark, a 0.9% improvement was achieved, reaching an accuracy of 47.2%. The WIKITABLEQUESTIONS dataset contains tables from Wikipedia with question-answer pairs split into train/dev/test sets. The dataset contains 2,108 tables and 18,496 question-answer pairs split into train/dev/test sets. Each table can be converted into a directed graph for querying. Challenges include tables from various topics, new tables at test time, unnormalized table contents, and complex semantics requiring multiple-step reasoning. The dataset contains tables and question-answer pairs for training. The framework uses a neural \"programmer\" and a symbolic \"computer\" for program synthesis with weak supervision. Functions are added to the Lisp interpreter for WIKITABLEQUESTIONS. In program synthesis with weak supervision, decision making is improved by focusing on pretrained search policies. The problem is to generate a program using a mapping function, f \u03b8 pxq, where \u03b8 represents model parameters. The quality of the program is evaluated using a reward function Rp\u00e2; x, yq, comparing the output with the correct answer. The context includes natural language. The goal in program synthesis with weak supervision is to find optimal parameters \u03b8 that map context x to an answer, maximizing return on a test set. The policy f \u03b8 is decomposed into a search model s \u03c6 and a value model v \u03c9. The search model learns a conditional distribution \u03c0 \u03c6 that assigns probabilities to candidate programs given the context. The distribution \u03c0 \u03c6 assigns probabilities to candidate programs for a novel context. Exact or approximate inference methods like beam search can be used to find the most likely programs. \u03c0 \u03c6 is typically an autoregressive model, and policy gradient techniques can optimize the parameters of a stochastic policy. Training objective is to optimize expected return using context-answer pairs. Decision-time planning relies on a value network trained to predict outcomes. The text introduces a max-margin training objective for the value model v \u03c9, optimizing to rank rewarded programs higher. It describes the NPP architecture using a seq2seq model for candidate program answer pairs, program token representation, and a training procedure based on max-margin/ranking objective. The framework is broadly applicable to applying RL in semantic parsing. In the context of semantic parsing, the NPP framework utilizes a pre-trained search policy to generate candidate programs through beam search. Each program is scored based on token features and global statistics, using a seq2seq model with LSTM and convolutional layers to capture sequential inputs and inner function scoring. The NPP framework uses a pre-trained search policy for generating candidate programs through beam search. Each program is scored based on token features and global statistics using a seq2seq model with LSTM and convolutional layers. The final score of a candidate program is calculated by summing its token scores, making the score easily understandable. Context features are represented for each token to better score tokens based on the overall environment context. The NPP framework utilizes a pre-trained search policy to generate candidate programs through beam search. Each program is scored based on token features and global statistics using a seq2seq model with LSTM and convolutional layers. The final score of a candidate program is calculated by summing its token scores.softmax attention across question tokens helps discern the importance of the question when generating the current token. t prob and p prob represent decisions from the search model, while t agree is the number of candidate programs with token a t at position t. NPP training is formulated as a learning to rank problem, optimizing pairwise ranking among candidate programs. The NPP framework uses a pre-trained search policy to generate candidate programs through beam search. NPP training compares two setups: single MAPO and stack learning. In the single MAPO setup, NPP learns from a different distribution than the intended dev/test data due to biased candidate programs. However, NPP still improves the prediction of MAPO. To address this issue, NPP training data is generated with a stacked learning setup. The study focuses on training a planner using stacked learning with MAPO models on a semantic parsing benchmark dataset. The NPP framework avoids bias in training data generation by decoding each training query with a MAPO model that has not seen the query before. The search policy is pretrained using MAPO, and NPP is trained to rescore candidate programs. The WIKITABLEQUESTIONS BID11 dataset is used for the empirical study. The study focuses on training a planner using stacked learning with MAPO models on a semantic parsing benchmark dataset called WIKITABLEQUESTIONS BID11. The dataset contains tables extracted from Wikipedia, question-answer pairs, and uses string match to identify phrases in the tables. NPP is compared to the current state of the art model on the dataset, achieving 46.3% accuracy with an ensemble of size 10. The study focuses on training a planner using stacked learning with MAPO models on a semantic parsing benchmark dataset called WIKITABLEQUESTIONS BID11. NPP is compared to the current state of the art model on the dataset, achieving 46.3% accuracy with an ensemble of size 10. Training details include setting parameters for experiments, using Adam optimizer with a learning rate of 10\u00b43, and tuning hyperparameters on the development set. The ensemble of K MAPO models with NPP is formulated by summing normalized NPP scores under different MAPO models. The effectiveness of proposed programs token representations was evaluated through a feature ablation test. The program probability and program agreement features were found to be crucial, helping to identify valuable programs and their divergence. Question referencing features also played a significant role in the evaluation. The program's token representations were evaluated through a feature ablation test, highlighting the importance of features like q tok for query level context. NPP consistently improved MAPO precision under different settings, despite variances in MAPO training. Training NPP on MAPO data splits improved precision, even with exposure bias in the training data. The MAPO models were trained and evaluated on separate train/dev splits using a Leave-One-Out scheme. Stacked learning helped NPP improve precision significantly and reduce variances on both dev and test sets. Ensembling MAPO settings with 5 models produced state-of-the-art results. NPP improves precision by 1.1% with 5 MAPO models and 0.9% with 10 models. It aims to select the correct program by overcoming wrong choices made by MAPO, such as selecting spurious programs or executing incorrect table filtering. NPP demotes spurious programs by utilizing a larger time horizon and rewarding earlier program tokens based on later choices. NPP improves precision by selecting correct programs and demoting spurious ones. It scores semantically correct programs higher in the beam to overcome wrong choices made by MAPO. An example is shown where NPP reevaluates a program to find the last row in a table, scoring tokens within this function higher than in the incorrect program. NPP improves program selection precision by scoring tokens within functions higher than incorrect programs. It can better understand questions and grade programs closely related to the query. Reinforcement learning in structured prediction struggles with limited use of world models and considering past/future program context. Neural Program Planner (NPP) is proposed to improve program selection precision by scoring tokens within functions higher than incorrect programs. It enhances understanding of questions and closely grades programs related to the query. NPP addresses limitations in reinforcement learning by considering past and future program context, leading to a 0.9% improvement in state-of-the-art performance for a difficult SP task."
}