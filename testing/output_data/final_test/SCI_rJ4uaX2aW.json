{
    "title": "rJ4uaX2aW",
    "content": "A common way to speed up training of large convolutional networks is to add computational units and use data-parallel synchronous Stochastic Gradient Descent (SGD) with a mini-batch divided between units. Increasing the number of nodes leads to larger batch sizes, but training with a large batch can result in lower model accuracy. The current recipe for large batch training may not be general enough and could lead to optimization difficulties. To address this, a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS) is proposed. Using LARS, AlexNet and ResNet-50 were scaled to a batch size of 16K, reducing the time taken to train large Convolutional Neural Networks (CNN). Training large convolutional networks with data-parallel Stochastic Gradient Descent involves dividing the mini-batch among computational units. Increasing the number of workers increases the batch size, impacting model accuracy. To compensate for fewer weight update iterations with larger batches, adjusting the learning rate is suggested. However, increasing the learning rate can lead to optimization challenges, especially at the start of training. To address this, a \"learning rate warm-up\" technique was proposed to mitigate these issues. To address challenges in training large convolutional networks with data-parallel Stochastic Gradient Descent, a \"learning rate warm-up\" technique was proposed by BID4. This technique involves starting training with a small learning rate and gradually increasing it to the target base learning rate. By applying this method along with a linear scaling rule, BID4 successfully trained ResNet-50 with a batch size of 8K. However, when attempting to train AlexNet on ImageNet using the same approach, training diverged for batch sizes larger than 2K. To improve model convergence and accuracy for larger batch sizes, Local Response Normalization layers in AlexNet were replaced with Batch Normalization, resulting in improved accuracy and training stability. The Layer-wise Adaptive Rate Scaling (LARS) algorithm proposes using separate learning rates for each layer to improve training stability. By analyzing the ratio between the norm of layer weights and gradients update, it was observed that training becomes unstable if the ratio is too high. LARS differs from other adaptive algorithms by defining the update magnitude with respect to the weight norm for better control of training speed. The Layer-wise Adaptive Rate Scaling (LARS) algorithm defines update magnitude with respect to weight norm for better training speed control. Training CNN with large batches requires careful adjustment of hyper-parameters to maintain accuracy. When training with large batches, it is suggested to increase the batch size and learning rate proportionally while keeping other hyper-parameters unchanged. Linear LR scaling can help maintain accuracy, but scaling above a certain point may lead to training divergence. When training with large batches, linear scaling works better for networks with Batch Normalization. Different approaches like \"square root scaling\" of LR and LR warm-up have been proposed to overcome instability during training. These methods have been used to train models like Inception and ResNet with batch sizes up to 8K without loss in accuracy, constituting the current state-of-the-art for large batch training. Large batch training methods have been used to train models like Inception and ResNet with batch sizes up to 8K without loss in accuracy. The \"generalization gap\" issue with large batch training was addressed by BID8, who found that large-batch methods tend to converge to sharp minimizers of the training function, affecting generalization ability. Experimenting with BVLC AlexNet, training with a batch size of 4K using larger learning rates resulted in divergence with rates above 0.06. In experiments, changing the base LR from 0.01 to 0.08 led to training divergence with LR > 0.06 even with warm-up 2. The best accuracy for B=4K is 53.1% with LR=0.05, and for B=8K it is 44.8% with LR=0.03. Replacing Local Response Normalization with Batch Normalization in the AlexNet-BN model improved stability. Training used SGD with momentum=0.9, weight decay=0.0005 for 128 epochs and a polynomial decay LR policy with base LR=0.02. With BN, larger LR-s could be used without warm-up, achieving 58.9% accuracy for B=4K with LR=0.18 and 58% for B=8K with LR=0.3. BN widened the range of LRs with good accuracy but resulted in a 2.2% accuracy loss for B=8K. The accuracy loss for B=8K was not due to a generalization gap but rather slow training. The ratio of weights and gradients varies significantly between layers, affecting training sensitivity. The ratio of weights to gradients varies between layers, impacting training sensitivity. The LR \"warm-up\" aims to stabilize training by gradually increasing the learning rate. Another approach suggests keeping weight updates small compared to weight norms to ensure training stability. The value of stochastic gradient \u2207L(w l t) is close to the true gradient, with the learning rate \u03b7 depending on batch size. The optimal \u03b7 increases monotonically with batch size, helping to address vanishing and exploding gradient problems. Local LR \u03bb l is defined for each layer using a \"trust\" coefficient \u03b7 < 1/4. LARS algorithm is used for SGD training to address optimization difficulties without replacing standard regularization methods. The LARS algorithm allows for larger weight decay, automatically controlling the norm of layer weights. With batch sizes of 16K, accuracy is slightly lower than baseline, but can be recovered by training for more epochs. However, with batch sizes of 32K, even models trained with LARS and large learning rates do not reach baseline accuracy, requiring longer training to recover lost accuracy. Training longer can help recover lost accuracy, especially with larger batch sizes. For example, doubling the number of iterations for Alexnet and Alexnet-BN with a batch size of 16K improved accuracy by 2-3%. Similarly, training Resnet-50 for an additional 100 epochs brought the top-1 accuracy back to the baseline of 75.5%. Increasing training duration is necessary to maintain accuracy, as seen with Googlenet where using LARS optimization eliminated the need for auxiliary losses. The original model lacks Batch Normalization, utilizing data augmentation for regularization. Googlenet is challenging to train with large batches, requiring extended epochs and ramp-up for scaling the learning rate. Large-batch training often leads to divergence, prompting the development of the LARS optimization algorithm to adapt learning rates per layer based on weight and gradient norms. The LARS optimization algorithm helps with vanishing and exploding gradients by adjusting learning rates per layer based on weight and gradient norms. Despite using LARS and warm-up, increasing the learning rate for very large batches is still challenging. To maintain accuracy, increasing the number of epochs and implementing extensive data augmentation is necessary to prevent over-fitting."
}