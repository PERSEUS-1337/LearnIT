{
    "title": "SkeGvaEtPr",
    "content": "Neural Markov Logic Networks (NMLNs) are a statistical relational learning system that learns implicit representations of first-order logic rules as a neural network potential function. They can represent any Markov Logic Network (MLN) and work well even without embeddings of constants, making them suitable for predictive tasks beyond transductive settings. NMLNs show promise in knowledge-base completion tasks and generating molecular data. The generation of molecular data involves estimating parameters for a statistical relational model from examples of relational structures like social networks, protein-protein interaction networks, and the Web. Learning a probability distribution over relational structures from few examples is challenging, but can be achieved by identifying repeated regularities in the structure and using statistics about these regularities to construct a distribution. This approach, combined with the maximum-entropy principle, leads to distributions like Markov logic networks. Neural Markov Logic Networks (NMLN) are proposed in this paper as a powerful alternative to classical MLNs. Unlike traditional MLNs that require domain experts to design statistics or perform structure learning, NMLNs use neural networks to model the probability distribution without prior knowledge of the statistics. This eliminates the need for feature engineering and aligns with the trend of adopting deep learning techniques. The statistical relational learning community has yet to explore this direction extensively. The main contributions of this work include introducing a new statistical relational model, providing a theoretical justification based on Min-Max-entropy, offering a Tensorflow implementation, and demonstrating its effectiveness in knowledge-base completion and generative modeling of small molecules. The integration of logic reasoning and neural models has been a topic of increasing interest in recent years. The integration of logic reasoning and neural models has gained attention in recent years. Fuzzy logic-based methods are optimized for neural models implementing FOL relations, while ProbLog language is extended to predict probabilities of atoms using neural networks. Lifted relational neural networks unfold neural networks with shared weights for improved performance. The integration of logic reasoning and neural models has gained attention in recent years. In the context of Graph Neural Networks (GNN) models, neural networks are used to extract regularities in non-euclidean settings. The proposed Neural Theorem prover improves reasoning capabilities by exploiting the geometry of the embedding space. However, it is limited to transductive settings and lacks probabilistic modeling of relational structures. In GNNs, latent representations of nodes are obtained through an aggregation of neighboring nodes' representations. Previous attempts integrated MLNs with neural components to model distributions of structural properties of the graph. This paper follows Model A from Ku\u017eelka et al. (2018) in a function-free first-order logic language L. The text discusses a logic language L built from constants and predicates, defining possible worlds as pairs of constants and ground atoms. Fragments are defined as restrictions of possible worlds to specific constants, which can be anonymized. The collection of all anonymized fragments of a given world is denoted as \u0393k(\u03c9). In the context of a logic language L and possible worlds defined by constants and predicates, anonymized fragments of width k are collected in \u0393k(\u03c9). The goal is to compute statistics on fragments to construct a distribution on relational structures \u03c9, using a fragment potential function \u03c6(\u03b3) to search for a maximum-entropy distribution p(\u03c9). The goal is to compute statistics on fragments to construct a distribution on relational structures \u03c9, using a fragment potential function \u03c6(\u03b3). The resulting maximum-entropy distribution resembles Markov logic networks, providing a starting point for designing statistical relational models. This approach eliminates the need to provide statistics in advance and allows for learning them together with the probabilistic model in a differentiable manner. To find a good parametric function \u03c6(\u03b3; w) to describe data, a learning principle is needed. By solving the maximum-entropy optimization problem with a single informative constraint on potential \u03a6(\u03c9), the solution shifts from a uniform distribution to a more informative one with lower entropy. This approach combines maximizing entropy with selecting the maximum-entropy distribution and minimizing it by choosing the most informative statistics. The Min-Max-Entropy model, known as Neural Markov Logic Network, is defined for the target probability distribution P \u03c9. It involves solving the Max-Entropy problem with neural potentials and incorporating the minimization of entropy by optimizing parameters w i. The optimization problem focuses on minimizing entropy to select optimal values for w i. The Min-Max-Entropy model, Neural Markov Logic Network, involves optimizing parameters w i to minimize entropy and solve the Max-Entropy problem. The unconstrained optimization problem for maximizing log-likelihood is solved using a gradient-based scheme. At a stationary point, the expected value of gradients under the distribution must match those evaluated at data points. The model anonymizes a fragment to preserve structural behaviors, losing identity details. This differs from transductive models like NTP, which use specific constants to improve predictions. An embedding fragment neural potential is defined based on the anonymized fragment and the constants involved. In transductive settings, the same constant set is used for learning and inference. The embedding fragment neural potential \u03c6 e is a function of both \u03b3, encoding structural properties, and c( S, \u0398), providing a latent representation for constants in set S. The function is parameterized by w and inspired by NLP works. The embedding vectors' components are treated as weights of potential functions and updated using gradients. The potential functions are updated using gradients, leading to the development of similar embeddings for similar constants. Adding embeddings of constants improves prediction capability in transductive settings. To learn Neural Markov Logic Networks, optimization procedures rely on sampling methods like MCMC. In this paper, MCMC methods, specifically approximate Gibbs Sampling, are used to sample from Neural Markov Logic Networks. The method recovers a discrete version of the Contrastive Divergence algorithm when run for a limited number of steps. Gibbs sampling struggles with distributions high in determinism, which can be addressed in normal Markov logic networks using MC-SAT algorithm. However, Neural Markov Logic Networks lack explicit logical encoding for deterministic constraints. Neural Markov Logic Networks avoid learning deterministic distributions by adding noise during training, with a parameter \u03c0 n \u2208 [0, 1]. This noise acts as a regularizer and prevents overfitting to training data. The model is implemented in Tensorflow to utilize GPU parallel computation capabilities. Multiple Markov chains are run to maximize efficiency. To maximize GPU parallel computation capabilities, multiple Markov chains are run in parallel. Different global neural potentials can rely on fragments of varying sizes to focus on learning local or global statistics of the data. The choice of fragment size is crucial for correct modeling of data distribution. However, evaluating the neural potentials requires a summation over a large number of terms. In Knowledge Base Completion (KBC), the task is to complete an incomplete Knowledge Base (KB) in a transductive setting. Data is provided in a positive-only fashion, where unknown and false facts are treated the same way as false during training. Ku\u017eelka & Davis (2019) showed consistency of learning by maximum-likelihood under the missing-completely-at-random assumption. The \"Smokers\" dataset is a classic example in statistical relational learning literature, with two relations defined on people: the predicate Smokes for smokers and the predicate friendOf for friendships. The system learned the symmetric nature of friendship, that friends of at least two smokers are also smokers, and that two smokers who are friends of the same person are also friends. The dataset \"friendOf\" maps people to their friends and is used to demonstrate how a statistical relational learning algorithm can model correlations in smoking habits among friends. Weighted logical rules are used in MLNs to represent these relationships. A NMLN was trained on the Smokers dataset without prior knowledge of relevant rules, allowing the model to identify informative statistics through neural potential functions. The Smokers dataset is used for a Knowledge Base Completion task, showing the model's learned rules in Figure 2. The Nations dataset contains information about countries and their relations, with 14 constants, 56 relations, and 2565 true facts. It was used in a Knowledge Base Completion task where a neural model outperformed others. Unary predicates were removed for the ComplEx model. NMLNs were used to tackle the KBC task on the Nations dataset. In this section, NMLNs were used to tackle a KBC task on the Nations dataset. The neural potentials were implemented as 2 hidden-layer neural networks with specific activations. Hyperparameters were selected using a validation set, and the evaluation procedure involved predicting rankings of test facts and their corruptions to calculate MRR and HITS@m. Gibbs sampling was used to estimate marginal probabilities in the Neural Markov Logic Network during training. The study compared different models for a Knowledge Base Completion task on Nations dataset. The NMLN-Emb model outperformed competitors on the HITS@1 metric, showing a significant improvement. The plain NMLN also performed better than differentiable provers, despite not using embeddings for reasoning. Overall, NMLN-Emb performed equally or better than competitors in all metrics, showcasing its effectiveness in reasoning tasks. Our model learns statistics in a differentiable manner, leading to a more detailed probability distribution. This allows for the application of NMLNs to generative tasks in non-euclidean settings. In generation tasks, the model learns the probability distribution of relational structures induced by a graph, making it applicable to any graph domain. Sampling techniques like Gibbs Sampling can be used to generate new samples. In a molecule generation task, the Gibbs Sampling technique was used to extract new samples from the ChEMBL molecule database. The dataset was limited to molecules with 8 heavy atoms, represented using the RDKit framework. Only molecules with common atom types and two types of bonds were considered. Fragment neural potentials were implemented as neural networks with specific hyperparameters selected. The study utilized Gibbs Sampling to generate new molecules from the ChEMBL database with specific criteria. A comparison was made between training data and generated molecules, showing similarities in structural patterns and variety. Statistical analysis was also conducted on a sample of 1000 training and generated molecules. In Li et al. (2018), statistical analysis was conducted on a sample of 1000 training and generated molecules using the RDkit framework. Neural Markov Logic Networks were introduced as a model combining neural networks with uncertainty handling in a maximum-entropy framework. The system performed well on small domains, with the main challenge being scalability to larger domains. Neural Markov Logic Networks (NMLNs) face challenges in scaling to larger domains due to their inability to handle large knowledge bases. More research is needed to identify more manageable subclasses of NMLNs and leverage insights from lifted inference literature. Every Markov logic network can be transformed into a Neural Markov Logic Network using a carefully selected potential function. This translation process is detailed in the literature, showing the close relationship between the two frameworks. The text discusses the transformation of Markov logic networks into Neural Markov Logic Networks using a carefully selected potential function. This process shows the close relationship between the two frameworks. The text discusses the translation of Markov logic networks into Neural Markov Logic Networks by selecting appropriate weights and potential functions. It demonstrates that all distributions in Model B can be converted to Model A, with a focus on injectivity of groundings. This transformation maintains the essence of Markov logic networks while imposing the injectivity requirement on groundings. The text discusses the transformation of Markov logic networks into Neural Markov Logic Networks by selecting weights and potential functions. It shows how distributions in Model B can be encoded in Model A, emphasizing the injectivity of groundings. This process, although not very efficient, demonstrates the ability to represent molecules using this approach. In this work, molecules are described using structural symbolic representations with Atom-type unary predicates (C, N, O, S, Cl, F, P) and Bond-type binary predicates (SINGLE, DOUBLE)."
}