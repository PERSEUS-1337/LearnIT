{
    "title": "HylwpaNKPB",
    "content": "Graph Convolution Network (GCN) is a widely used graph model for semi-supervised learning, but it struggles with extracting higher-order neighborhood information. A new approach, called higher-order weighted GCN (HWGCN), leverages Lasso to automatically learn weight matrices for higher-order neighbors, improving node representations. This method has achieved state-of-the-art results. Our model, HWGCN, has outperformed others on node classification tasks using Cora, Citeseer, and Pubmed datasets. CNNs excel in image tasks due to their grid-like structure, but real-world data like social and biological networks do not fit this model. The increasing focus on studying non-Euclidean structured data has led to research efforts in graph learning, inspired by the success of CNNs in computer vision tasks. Graph convolutional neural networks encode the graph structure to operate on the neighborhood of each node, with approaches categorized into non-spectral and spectral methods. This work specifically focuses on extending the graph convolution spectral filter. The focus is on extending the graph convolution spectral filter. The existing model, GCN, only considers first-order neighbors, limiting its representational capacity. A new model, HWGCN, is proposed to mix neighborhood information at different orders for more expressive representations from the graph. In the proposed graph model HWGCN, researchers leverage node features along with graph structure for convolutional filter formulation, allowing better neighbor selection at different distances and improved node representations from first-order and higher-order neighbors. The contributions include analyzing GCN, emphasizing the importance of similarity between different order information, and refining the architecture for more expressive graph representations. In HWGCN, convolutional filters are built using first-order and higher-order neighbors to aggregate information effectively. Lasso and node features are leveraged to minimize feature loss between different order neighbors, achieving state-of-the-art classification accuracy in experimental studies on various datasets. The adjacency matrix A defines connections between nodes in a graph, with a diagonal degree matrix D representing node features. The Graph Convolution Network (GCN) by Kipf & Welling is used as a base model, with a graph convolutional layer defined by input and output activations, normalized adjacency matrix \u00c3, and learnable weight matrices. The GCN model with l layers is defined for semi-supervised node classification tasks. The softmax function normalizes the final output matrix Z for c l class labels. Cross-entropy loss is used for evaluation and weights are calculated with backpropagation using optimization algorithms like Adam. The HWGCN model leverages k th -order adjacency matrices and weight matrices for convolutional filter formulation, benefiting from neighborhood information. The GCN model simplifies the Graph Laplacian by restricting filters to first-order neighbors, limiting the capture of node representation differences. Deeper GCN models do not improve node classification performance and can even harm predictions. Stacking multiple layers with one-hop message propagation is not necessary for learning latent information from higher-order neighbors. MixHop was designed by Abu-El-Haija et al. (2019) to mix feature representations of higher-order neighbors in one graph convolution layer. The layer output may impose lower-order information on higher orders and increase feature correlations due to the nonnegative matrix properties. MixHop was designed to mix feature representations of higher-order neighbors in one graph convolution layer. Theorem 1 states that the adjacency matrix multiplied by itself p and q times results in 0 if there are two walks between nodes v i and v j of lengths p and q. The concept of k th -order adjacency matrix is introduced to distinguish lower-order neighborhood information mixed with higher orders. The k th -order adjacency matrix A (k) is defined based on the path distance between nodes v i and v j as d ij. A naive solution to formulate the filter is to add all k th -order adjacency matrices A (k) to A, but this could lead to a dense matrix with noisy information. Different neighbors contribute to node semantics differently, requiring a more sophisticated solution. The k th -order weight matrix is formulated by assigning different weights to higher-order neighbors based on their layer-wise and node-wise importances. The weight matrix is sparse and learnable, allowing for the specification of the importance of each higher-order neighbor. The proposed graph convolution model adds higher-order weight matrices to approximate layer-wise modeling capacity. Lasso is used for feature selection by shrinking coefficients and setting some to zero. The proposed method utilizes Lasso for higher-order neighbor selection in a graph convolution model. The first-order neighbors share commonalities with the central node, providing less noisy labels. The optimization problem involves a penalty parameter \u03bb that controls coefficient shrinkage. The proposed method uses Lasso for higher-order neighbor selection in a graph convolution model. The optimization problem involves setting \u03bb to a large value to control coefficient shrinkage and shifting the sum of coefficients to a constant value \u03b1. Scale transformation is performed for higher-order neighbors based on the scale of first-order neighbors. A loss function is proposed to control the scale of feature vectors for higher-order neighbors. The proposed method involves using a loss function to control the scale of feature vectors for higher-order neighbors in a graph convolution model. This method also includes assigning different weights to neighbors and selecting a certain proportion of neighbor nodes to improve performance and reduce training time. The performance of the proposed model HWGCN is evaluated on various datasets in this section. The HWGCN model is tested on citation network benchmark datasets like Cora, Citeseer, and Pubmed. Nodes represent documents, edges denote citation links, and node features correspond to bag-of-words representation. The model is compared against previous methods and state-of-the-art baselines. The HWGCN model, utilizing a two-layer GCN structure with 16 hidden units, is trained on citation network benchmark datasets like Cora, Citeseer, and Pubmed. It is compared against previous methods and state-of-the-art baselines, including manifold regularization, skip-gram based graph embeddings, iterative classification algorithm, multi-layer perceptron, graph attention networks, plain GCN, and MixHop. The model is trained through 200 maximum epochs using Adam with specific parameter settings. For datasets, random splits of Cora, Citeseer, and Pubmed are adjusted to align with different scenarios: 5, 10, or 20 instances per class for training, with 500 and 1000 instances for validation and test. Experiments involve adding weight matrices to determine effective neighborhood distance, using 20 instances for training and 1000 for testing. Results show the benefits of mixing neighborhood information at different distances in a graph. The study found that incorporating weight matrices at specific distances in a graph improved node classification performance. Optimal results were achieved with k values of 4, 5, or 6 for Cora, k = 6 for Citeseer, and k = 5 for Pubmed. The proposed HWGCN model outperformed previous methods and baselines in most cases on random dataset splits, as highlighted in Table 4. HWGCN outperforms MLP, GAT, GCN, and MixHop on Cora, Citeseer, and Pubmed datasets by improving accuracy without complicating the model architecture. The success lies in considering higher-order neighborhood information and using weight matrix formulation to enhance node representations. Based on experiments and comparisons with previous approaches, HWGCN achieves state-of-the-art performance on Cora, Citeseer, and Pubmed datasets. The model uses Lasso to select relevant higher-order neighbors, which helps alleviate overfitting. Additionally, HWGCN considers higher-order neighborhood information and weight matrix formulation to enhance node representations. Several approaches have been proposed for graph-based neural networks, including convolutional networks for learning molecular fingerprints, diffusion-convolution neural networks, and unified frameworks for generalizing CNN architectures to non-Euclidean domains. Some researchers have also explored pooling operations on graphs, such as SortPooling and novel graph pooling techniques. The curr_chunk discusses novel graph pooling and convolution approaches in graph-based neural networks. It includes spectral approaches, Chebyshev polynomial approximation for filters, and the introduction of graph convolutional network (GCN) by Kipf & Welling in 2017. These methods have shown impressive performance in node, link, and graph classification tasks. Our work is based on the graph convolutional network (GCN) model, which updates node states by aggregating feature information from directly neighboring nodes. However, GCN fails to learn higher-order neighborhood information through multiple layers, leading to performance drop-off beyond two layers. To address this limitation, we propose a more sophisticated convolutional filter that leverages node features and graph structure from higher-order neighbors while avoiding information overlaps. This approach differs from MixHop by focusing on higher-order neighborhood information. In this paper, a novel model HWGCN is proposed to improve graph convolutions by incorporating higher-order neighborhood information in a weighted and orthogonal manner. The method leverages node features and graph structure to enhance node representations and minimize feature loss through Lasso regularization. Experimental results show state-of-the-art performance on standard citation network datasets, outperforming other baselines. The proof of Theorem 1 involves showing the number of walks from node vi to vj in graph G of length k. This is done by induction, starting with walks of length 1 and using the number of walks of length k-1. By summing the walks with all vertices as their penultimate vertex, the total number of walks from vi to vj can be determined. The theory shows that different matrix powers may have non-zero elements in the same position, especially in graphs with circles. This leads to information overlap between lower and higher-order matrices. Accuracy curves for training sizes of 5 and 10 per class on three datasets are shown in Figures 4, 5, and 6 for random and fixed splits. The distance matrix is replaced with matrix powers, and accuracy curves for training sizes of 5, 10, and 20 per class are shown in Figures 7, 8, and 9. Lasso with l1-norm provides sparse solutions, as shown in Tables 6, 7, and 8 for weight absolute value statistics W(k). Large weight absolute values only occupy a small portion."
}