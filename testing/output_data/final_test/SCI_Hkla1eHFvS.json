{
    "title": "Hkla1eHFvS",
    "content": "Reinforcement learning agents need to explore unknown environments to solve tasks. Exploration methods are proposed as approximations, but it's unclear what objective they optimize or how to incorporate prior knowledge. A single exploration policy is learned to solve multiple tasks, amortizing the cost of learning. Exploration is recast as State Marginal Matching, matching state distribution to target distribution. Our algorithm optimizes exploration by reducing it to a two-player game between a state density model and a parametric policy. Theoretical analysis shows that prior exploration methods acquire a replay buffer that performs distribution matching, explaining their success in single-task settings. Our algorithm explores faster and adapts more quickly on simulated and real-world tasks compared to prior methods. The inability of current RL algorithms to explore limits their applicability to long-horizon control tasks. Prior work has focused on exploration strategies, with random actions being limited in state coverage. More advanced techniques like intrinsic motivation accelerate learning but lack a clear optimization objective for good exploration. The absence of a metric to quantify exploration makes it challenging to evaluate these methods effectively. The lack of a metric to quantify exploration hinders comparison of exploration methods and assessing progress. Current methods target single-task settings, making it difficult to repurpose them for multiple tasks. To address these limitations, a multi-task setting is considered, aiming to learn a task-agnostic exploration policy that can adapt to various reward functions, reducing the cost of learning to explore. This policy serves as a prior for solving downstream tasks, with training focused on acquiring this task-agnostic policy and testing on its application. During training, a task-agnostic exploration policy is acquired to quickly explore and maximize task reward. Learning a single exploration policy is challenging compared to exploration during learning a single task. Various exploration methods are used to efficiently explore high-reward states, but the final policy may only focus on high-reward states for the current task. The objective to optimize for a good exploration policy is questioned. The objective of obtaining a good exploration policy is questioned, and exploration is recast as a State Marginal Matching problem. This involves learning a mixture of policies to match a desired state distribution, encouraging the policy to visit as many states as possible. The distribution matching objective allows for the incorporation of prior knowledge about the task, such as safety constraints, state preferences, reward shaping, or the importance of each state dimension. An algorithm is proposed to optimize the State Marginal Matching objective by framing it as a two-player, zero-sum game between a policy player and a density player. The problem of exploration in reinforcement learning is reframed as a two-player, zero-sum game between a policy player and a density player. A Nash Equilibrium is found using fictitious play, where the algorithm fits a state density model and updates the policy to visit states with low density. This approach explores more effectively and adapts quickly to new tasks compared to existing methods. The algorithm sheds light on prior work on exploration, showing that while existing algorithms do not match distributions, the replay buffer does, potentially explaining the success of prior methods. The exploration methods in reinforcement learning involve using prediction errors or encouraging the agent to visit novel states. While most methods explore effectively during task-solving, the policy obtained may not be a good exploration policy. In contrast, a method discussed in the text converges to a highly-exploratory policy by maximizing state entropy in the training objective. Various exploration algorithms can explore in the space of actions, policy parameters, goals, or states, with common strategies like -greedy and Ornstein-Uhlenbeck noise. Recent work has shown that adding noise to policy parameters can improve exploration in reinforcement learning. Methods exploring in the space of states or goals have been proposed, with some focusing on state-entropy as a means of exploration. Hazan et al. (2018) introduced the State Marginal Matching objective, which is similar to the exploration method discussed. The main contributions of the current study are demonstrating the competitiveness of state-entropy-based exploration and highlighting how prediction error-based methods implicitly maximize state-entropy. The text discusses how exploration methods based on prediction error implicitly maximize state-entropy. It also touches on goal-conditioned RL as a special case of State Marginal Matching. The problems of exploration and meta-reinforcement learning are interconnected, with effective exploration being crucial for solving downstream tasks. The study aims to learn a policy as a prior for solving tasks, requiring only a single target state marginal distribution. Our method simplifies the problem assumptions and training procedure, making it easier to apply in real-world domains compared to standard maximum action entropy algorithms. These algorithms maximize entropy over actions, not states, and can be viewed as performing inference on a graphical model based on the likelihood of a trajectory. The exact relationship between distributions over trajectories and states is computationally challenging for most MDPs. The idea of distribution matching in imitation learning has been successful, with methods iterating between learning a policy and a reward function obtained via a density model. Unlike inverse RL algorithms, this approach assumes access to the density of the target state marginal distribution instead of expert trajectories, which can be more challenging to provide in realistic settings. Our work unifies prior exploration methods by performing approximate distribution matching, explaining how state distribution matching can be done properly. We propose the State Marginal Matching problem as an objective for learning to explore and provide an algorithm for optimizing it in a Markov Decision Process setting. In a Markov Decision Process (MDP) with fixed episode lengths, dynamics distribution, and initial state distribution, the policy and MDP form a generative model over states. The state marginal distribution represents the probability of visiting each state during a finite-length episode. The target distribution over states encodes beliefs about tasks at test-time, allowing for the matching of state distributions. In a Markov Decision Process (MDP), the state marginal distribution represents the probability of visiting each state during an episode. The goal is to find a policy closest to the target distribution, measured using KL divergence. State Marginal Matching maximizes a pseudo-reward function to encourage exploration in the space of states. State Marginal Matching maximizes a pseudo-reward function to encourage exploration in the space of states by introducing a parametric state density model to approximate the policy's state marginal distribution. This approach breaks the cyclic dependency between the reward function and the policy, making it more challenging than standard RL. Proposition 3.1 states that solving the max-min optimization problem is equivalent to finding the Nash equilibrium of a two-player, zero-sum game where a policy player chooses the policy and a density player chooses the density model. The Nash existence theorem guarantees a stationary point in such games. The common approach is to alternate updating between players to reach a solution. The algorithm for optimizing the State Marginal Matching objective iterates between fitting a density model and training the policy with a RL objective. Fictitious play converges to a Nash equilibrium in finite time by choosing the best strategy in response to the opponent's historical average. The algorithm for optimizing the State Marginal Matching objective involves fictitious play alternating between fitting a density model to historical policies and updating the policy with RL to minimize log-density. The exploration policy is the historical average policy, randomly sampling one of the policy iterates at the start of each episode. The resulting algorithm efficiently implements these steps. The algorithm for optimizing the State Marginal Matching objective involves fitting a density model to historical policies and updating the policy with RL to minimize log-density. Exploration methods based on prediction error do not converge to an exploratory policy, even in the absence of extrinsic reward. The exploration bonus in Pseudocounts and other prediction-error methods do not lead to convergence to a single exploratory policy in stochastic MDPs. Existing methods are all special cases of an abstract exploration algorithm alternating between intrinsic reward computation and RL. Prior methods excel at solving hard exploration tasks by visiting a wide range of states through historical averaging over policies. This leads to a diverse replay buffer, explaining their success. Using a mixture of policy iterates throughout training can help obtain exploration from these methods. The following section will compare SMM against prior exploration methods. The study compares SMM to prior exploration methods using simulated control tasks and historical averaging. It also evaluates the effect of historical averaging on exploration methods like SAC, GAIL, C, PC, and ICM. Code will be released upon publication. The study compares SMM to prior exploration methods using simulated control tasks and historical averaging. It evaluates the effect of historical averaging on exploration methods like SAC, GAIL, C, PC, and ICM. SMM uses prediction error as a bonus and implements a 2D Navigation environment to compare exploration in state space versus action space. Results are averaged over random seeds. In simulated experiments, SMM explores multiple hallways efficiently compared to SAC. In the Manipulation environment, SMM adapts quickly and achieves a higher success rate than other exploration methods. In simulated experiments, SMM explores multiple hallways efficiently compared to SAC, achieving a success rate 20% higher than the next best method in fewer episodes. Historical averaging is crucial for convergence in SMM, while the choice of prediction error or VAE is less significant. Further ablation studies of SMM are provided in Appendix B.2. In simulated experiments, State Marginal Matching (SMM) explores efficiently compared to other methods, achieving a higher success rate in fewer episodes. Historical averaging is important for SMM's convergence. In a final experiment, SMM's ability to reflect prior knowledge injected via the target distribution is tested. In a real-world robotic control task using the D'Claw robotic manipulator, State Marginal Matching (SMM) explores effectively, showing half the distance to the target distribution compared to the SAC baseline. Future methods could improve matching state marginals in exploration. In experiments on the D'Claw robotic hand, a target distribution with uniform mass over all object angles was used. The Sim2Real approach trained algorithms in simulation and measured the knob rotation on the hardware robot. SAC and SMM were trained on the real robot, with SMM showing more knob movement and state exploration than SAC. Results were averaged over multiple seeds. In the real robot experiments, SMM turned the knob more and to a wider range of angles compared to the baselines. Statistical tests showed significant differences in the number of rotations and maximum angles turned. Another experiment focused on learning exploration policies directly in the real world without using a simulator. In real-world experiments, SMM explored a wider range of angles compared to baselines, showing significant differences in rotations and maximum angles turned. The results suggest that exploration techniques may be useful in real-world tasks, encouraging further study in this area. The State Marginal Matching objective provides a clear solution for exploration algorithms. The State Marginal Matching objective ensures that the policy visits states in proportion to their density under a target distribution, promoting exploration and allowing users to bias exploration towards preferred states. The resulting policy can be used as a prior in multi-task settings, facilitating faster adaptation to new reward functions. Our algorithm differs from previous methods by including a crucial historical averaging step for convergence. The algorithm aims to explore efficiently in new tasks by using State Marginal Matching (SMM). It introduces an extension called SM4 that incorporates mixture modeling to address exploration challenges in large state spaces. The goal is to develop more effective and principled RL methods that reason about distributions rather than individual states. The algorithm introduces SM4, a mixture modeling approach to accelerate exploration in large state spaces by decomposing the target distribution into easier-to-learn distributions. This is achieved by learning a set of policies for distribution matching, minimizing KL divergence between the mixture distribution and the target distribution. The algorithm introduces SM4, a mixture modeling approach to accelerate exploration in large state spaces by decomposing the target distribution into easier-to-learn distributions. It emphasizes exploring states with high density under the target state distribution, unvisited states, and distinct from other agents. The algorithm fits a density model to approximate the state marginal distribution, incorporating a state entropy term for rewarding novel state visits. This approach aims to address the limitations of prior works in scaling to complex tasks. The algorithm introduces SM4, a mixture modeling approach to accelerate exploration in large state spaces by decomposing the target distribution into easier-to-learn distributions. It fits a density model to approximate the state marginal distribution, learns a discriminator to predict visited states, and uses RL to update policies to maximize expected return. The updates for each policy can be conducted in parallel in a distributed implementation. SM4 is a distributed implementation that explores different parts of the state space independently. It compares SM4 to baselines like SAC and shows heavy reliance on key differences. Training time performance on the Navigation task is plotted, highlighting the effectiveness of SM4. SM4 relies heavily on key differences from SAC, studying the effect of mixture modeling on test-time exploration in the Manipulation environment. Increasing the number of mixture components improves agent success, with historical averaging showing a smaller effect. Efficient exploration may require either historical averaging or mixture modeling, but not necessarily both. On the Navigation task, SM4 with three mixture components outperforms ablation baselines lacking conditional state entropy, latent conditional action entropy, or both. In SM4, both entropy terms contribute to exploration ability, with state entropy being critical. Increasing mixture components and using historical averaging accelerates exploration. The choice of target distribution depends on test-time tasks, aiming to minimize episodes to reach goal states. The text discusses reaching the goal state in a trajectory, emphasizing the probability of reaching the goal at any state. It also mentions the expected number of episodes needed to reach the goal state, with an upper bound on the hitting time. The text discusses the expected hitting time to reach the goal state, emphasizing the optimal target distribution and policy. It introduces a smoothed target density and presents a proof using Lagrangian optimization. Goal-Conditioned RL can be seen as a special case of State Marginal Matching when the goal-sampling distribution is learned with the policy. By using a mixture policy, we can learn goal-conditioned policies and optimize the objective with Mixtures of Policies. In goal-conditioned RL, the distribution over states visited by the policy when attempting to reach a goal is crucial. The State Marginal Matching objective involves sampling goals with probability equal to their density under the target distribution. Goal-conditioned RL is suitable for settings where sampling goals is easy, especially in cases with a small or low-dimensional goal space. Importance sampling can be used to generate goals for training the goal-conditioned policy when a large collection of goals is available. In goal-conditioned RL, the distribution over states visited by the policy when attempting to reach a goal is crucial. The State Marginal Matching objective involves sampling goals with probability equal to their density under the target distribution. Goal-conditioned RL is suitable for settings where sampling goals is easy, especially in cases with a small or low-dimensional goal space. Importance sampling can be used to generate goals for training the goal-conditioned policy when a large collection of goals is available. Navigation, Manipulation, and D'Claw environment parameters are summarized in Table 1. In goal-conditioned RL, the State Marginal Matching objective involves sampling goals with probability equal to their density under the target distribution. The experiments used a simulated Fetch Robotics arm and terminated each episode after 50 environment steps or if the block fell off the table. Two target state marginal distributions were considered, with fixed weights for rewards. In goal-conditioned RL, rewards are defined based on the block and robot positions, with a weighted sum of three reward terms. At test-time, a goal block location is sampled uniformly on the table surface. In Manipulation-Half, the target state density favors states where the block is on the left-side of the table. In Manipulation-Half, higher reward is given for states where the block is on the left-side of the table. The D'Claw robot controls three claws to rotate a valve object in a 9-dimensional action space and a 12-dimensional observation space. Each episode is fixed at 50 timesteps, about 5 seconds on the real robot. The target state distribution places uniform probability mass over all object angles. GAIL incorporates reward shaping terms to discourage high joint velocity and deviations from initial joint positions. In order to compare GAIL with exploration methods, synthetic states were sampled to train GAIL. Different sampling distributions were used for object and gripper positions in various environments. The discriminator input was restricted to specific state dimensions to address reachability issues. In our experiments, we used the best GAIL ablation model to compare against the exploration baselines. We estimated the density of data x in our SMM implementation and compared the wall-clock time of each exploration method. The computational cost of our method is comparable with prior work, and we studied the effect of restricting the GAIL discriminator input to fewer state dimensions. We trained the GAIL discriminator on different subsets of the state vector, including object and gripper positions only. Varying the sampling distribution for the gripper position showed that sampling closer to the object improved object position entropy but hurt gripper position entropy. Restricting the discriminator input to fewer state dimensions yielded interesting observations in both domains. Restricting the discriminator input to fewer state dimensions affects its ability to distinguish between expert and policy states. Training on the entire state vector leads to perfect classification, but some expert states may not be reachable from the initial state. Hyperparameter settings are summarized in Table 3, with different algorithms trained for varying numbers of steps on different domains. Loss hyperparameters were tuned for each exploration method, including SAC reward scale and count coeff. In the Manipulation experiments, different sampling strategies for historical averaging were tested, including Uniform, Exponential, and Last. Uniform sampling worked less effectively, possibly due to early iteration policies not being trained enough. There was little difference between Exponential and Last sampling methods, as well as between sampling 5 vs. 10 historical policies. Keeping checkpoints from every iteration was deemed unnecessary. Network hyperparameters included using a Gaussian policy with two. In the experiments, network hyperparameters were set for all algorithms using specific configurations for policy, value function, Q-function, discriminator, and density model. The replay buffer was pre-filled with random actions for stability. One discriminator update was performed per SAC update. In the experiments, network hyperparameters were set for all algorithms using specific configurations. GAIL training was found to be more unstable compared to exploration baselines. Early termination was used for GAIL instead of the final iterate. Visualization of different methods pushing the block in the Manipulation environment was done. SMM showed success in exploring a wide range of states at test-time. The environment reward in the Manipulation environment is a weighted sum of three terms: r goal (s), r robot (s), and r action, with weights -20, 1, 0.1 respectively. Three exploration methods (ICM, Count, SMM) optimize an auxiliary exploration loss to encourage the agent to move around the block. Compared to SAC, these methods may have worse returns for r goal (s) and r action (s) but quickly learn to maximize the sparse reward r robot (s)."
}