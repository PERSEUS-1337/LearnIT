{
    "title": "rygMWT4twS",
    "content": "Stochastic gradient descent (SGD) is a popular approach for stochastic optimization, especially in machine learning. Recent research has focused on optimizing convex loss functions and training nonconvex deep neural networks using SGD. Chen et al. (2018) proposed using a consistent gradient estimator as a cost-effective alternative in scenarios where unbiased estimators are expensive to compute. It has been shown that consistent estimators yield similar convergence behavior to unbiased ones for strongly convex, convex, and nonconvex objectives. Our analysis covers strongly convex, convex, and nonconvex objectives, verified with experiments on synthetic and real-world data. This work introduces new research directions for more efficient SGD updates and training algorithms for large-scale graphs. In supervised learning, the goal is to train a predictive model that minimizes the expected loss between predictions and ground truth labels. Stochastic gradient descent (SGD) is a popular optimization method for this task, updating model parameters using unbiased gradient estimators until convergence. Stochastic gradient descent (SGD) is commonly used in supervised learning to minimize expected loss. The unbiased estimator g k efficiently computes the gradient \u2207f (w k ) using only a few training examples. However, in scenarios like graph nodes, computing the unbiased gradient estimator \u2207 (w, \u03be i ) can be costly due to information aggregation across neighborhoods, resulting in O(n) complexity. The paper discusses the use of a consistent gradient estimator as an alternative to an unbiased one for training graph convolutional neural networks. It analyzes the convergence behavior of stochastic gradient descent when using this estimator, showing that it results in the same convergence behavior as unbiased estimators. The distinction between unbiasedness and consistency in estimators is highlighted, showing that they are not subsuming concepts. The concept of asymptotic unbiasedness is introduced to illustrate how bias may vanish in the limit. An example is provided where an estimator can be unbiased but inconsistent, showcasing the difference between the two properties. The estimator discussed is unbiased but inconsistent, with examples illustrating the distinction between the two properties. Consistency is demonstrated through the variance estimator approaching zero, despite a nonzero bias for finite sample sizes. Asymptotic unbiasedness is also mentioned, where bias may vanish in the limit. The text discusses the consistency of an estimator in a real-world graph learning scenario, highlighting the convergence behavior of SGD with consistent gradient estimators. The scalability bottleneck in training with large graphs is addressed, emphasizing the importance of efficient training methods. The theory presented in this work supports the efficient training of graph neural networks by addressing doubts on optimization solver convergence. Consistent estimators show similar convergence behavior to unbiased ones, offering convenience for further analysis. This work opens up opportunities for improving analysis by relaxing assumptions, refining convergence rates, and designing acceleration techniques. Unbiasedness and consistency are distinct concepts, with the intention to establish error bounds for consistent gradient estimators similar to standard SGD results. The graph convolutional network model, GCN, learns embedding representations of graph nodes using parameter matrices and nonlinear activation functions. The network transforms initial feature inputs to output embedding matrices through multiple layers. The final embedding can be used for prediction tasks. The loss for node v in a graph convolutional network is computed using the corresponding rows of H, with neighbors considered across multiple layers. To address high computational costs, a biased but consistent gradient estimator was proposed by Chen et al. (2018), involving sampling a constant number of nodes in each layer. This approach generalizes from finite to infinite graphs, transforming the matrix expression into an integral transform. The integral transform in a graph convolutional network generalizes the input and output vectors into feature and embedding functions, respectively. The matrix A is transformed into a bivariate kernel, and the loss is written as a function of the output. Sampling on all network layers is facilitated for defining a gradient estimator, with consistent results using a biased estimator involving sampling nodes in each layer. Define an estimator of G that is consistent for the optimization problem with a convex feasible region. The standard update rule for SGD involves the projection onto the feasible region, with the global minimum denoted as w*. The projection is omitted, assuming w* is an interior point of S. Convergence results focus on how fast w approaches w* or f(w) approaches f(w*). A convexity assumption on f may be used for analysis. Definition of l-strongly convexity is provided. Consistency of estimator gN of h is defined. The section discusses convergence guarantees for the gradient estimator gNk in the context of sample size Nk. The analysis is based on assumptions of unbiasedness and boundedness of the gradient, with a focus on how fast the estimator approaches the truth. One common assumption is that the probability decreases exponentially with the sample size. This assumption is similar to the one used by Homem-de-Mello (2008) in stochastic optimization. The exponential tail occurs when moment generating functions exist, as shown in Figure 3 for the motivating application GCN. The gradient is a random variable dependent on history, denoted as filtration. The gradient estimator g k is consistent and obeys a tail bound assumption to determine the required sample size for a desired probability of an event happening during T SGD updates. The relation between sample sizes, probability, and convergence results is established under the event E \u03b4 with a sufficient condition of (7). The purpose of the tail bound assumption is to establish this relation for convergence results. The convergence results in this work are based on tail bounds. The exponential tail for GCN is discussed in Section 5.4. Assumption 2 imposes a bounded-gradient condition. Results for consistent gradient estimators show O(1/T) convergence for the expected error. Theorem 2 introduces a relative gradient estimator error \u03b4 for bounding the error in the strongly convex function f. With T updates and diminishing step size \u03b3 k = [(l \u2212 \u03b4)k] \u22121, the bound is satisfied with high probability. The assumption on l is mild and needed for the induction proof. Theorem 3 states that under certain conditions and with a smooth function f, a result for the iterate w T can be obtained with high probability. Linear convergence is possible with a constant step size, as shown by Bottou et al. (2016) in a similar setting. Theorem 4 presents results on the iterate error with a constant step size in a different setting. It compares the squared iterate error bounds in Theorem 2, showing different convergence speeds. For convex functions, O(1/ \u221a T ) convergence is typically achieved with unbiased gradient estimators. Theorem 5 discusses consistent gradient estimators for convex functions in a bounded feasible region with finite diameter. It outlines conditions for convergence with diminishing step sizes and provides a convergence rate result. Theorem 6 extends the discussion on gradient estimators to the nonconvex case, showing convergence with constant step size for T updates. Theoretical results show convergence for SGD with consistent gradient estimators, with different rates for strongly convex, convex, and nonconvex cases. In practice, sample size and learning rate are hyperparameters tuned against a validation set. The relative gradient estimator error decreases inversely with sample size to maintain the same failure probability. Theorem 7 discusses linear convergence for strongly convex functions with a non-vanishing right-hand side, and the impact of sample size and gradient error on error bounds. Theorem 7 discusses linear convergence for strongly convex functions with a non-vanishing right-hand side, and the impact of sample size and gradient error on error bounds. It states that consistent estimators result in the same convergence behavior as unbiased ones, requiring sufficient sample size for probabilistic convergence of the gradient estimator. Experiments using consistent gradient estimators illustrate the convergence behavior of SGD in training the GCN model. The GCN model training process involves using three datasets: a synthetic dataset called \"Mixture\" with overlapping Gaussians, and two real-world benchmarks, Cora and Pubmed. The synthetic dataset aims to avoid regularity in sampling, making it challenging for classifiers trained with independent data points to predict component labels accurately. Graph-based methods are more successful in this scenario. The Cora and Pubmed datasets are citation networks used to predict topics. The code repository for the GCN model will be shared upon paper acceptance. The GCN model is hyperparameterized by the number of layers and uses an L2 regularization to make the loss function strongly convex. The predictive model involves softmax(AXW(0)), where X is the input feature matrix and P is the output probability matrix. The model differs from logistic regression by incorporating neighborhood aggregation AX. Training involves sampling a batch from the training set and evaluating the gradient of the loss function. The analyzed consistent gradient training involves sampling input layers with different index sets to evaluate the loss gradient. Figure 1 shows convergence curves of overall loss on all training examples. Comparisons are made between standard SGD, consistent gradient estimators with varying sample sizes, and the Adam training algorithm, showing that Adam converges faster than standard SGD. Increasing sample size results in the loss curve with consistent gradients approaching that of an unbiased one. In the analyzed training, consistent gradient estimators with varying sample sizes show convergence curves approaching that of standard SGD. Training loss serves as a surrogate measure of model performance, with early termination acting as regularization. Small sample sizes may be practically useful despite not meeting theoretical assumptions. Test accuracy comparisons show Adam and standard SGD achieving similar results, with consistent gradient SGD occasionally surpassing them. In experiments, consistent gradient SGD outperforms Adam and standard SGD in accuracy for sample sizes of 400. Results show faster convergence with Adam, while 2-layer GCN improves test accuracy significantly. Small sample sizes can achieve state-of-the-art results. The study demonstrates that consistent gradient SGD outperforms Adam and standard SGD in accuracy for sample sizes of 400. Results show faster convergence with Adam, while 2-layer GCN significantly improves test accuracy. Small sample sizes can achieve state-of-the-art results, with empirical findings supporting the exponential tail assumption in GCN. The study analyzes the convergence behavior of SGD with consistent gradient estimators, showing exponential convergence for 2-layer GCN. It is the first work to explore this with biased or noisy estimators, focusing on learning with large graphs. The findings suggest that the convergence behavior remains consistent with unbiased estimators, hinting at potential improvements in update formulas. The development of more efficient training algorithms for neural networks on large graphs is a key focus. Message passing neural networks, including GCN, face challenges with costly neighborhood aggregation. Consistent gradient estimators offer a promising alternative, with potential applications in learning to rank for information retrieval systems. The curr_chunk discusses a new training algorithm for learning to rank in information retrieval systems, utilizing consistent gradient estimators and SGD. The theory developed in the work offers guarantees of training convergence. The proof of Theorem 7 is also provided, showing the probability of gradient error inequality. The curr_chunk provides experiment details, including data set summaries, hyperparameters, parameter initialization methods, and run time information. It discusses a Gaussian mixture model with random connections between points and probabilities of connection within and across components. The text also mentions the use of Glorot uniform initializer for parameter initialization and highlights the computational efficiency of smaller sample sizes in training algorithms. The curr_chunk discusses the computational efficiency of smaller sample sizes in training algorithms, with SGD showing faster run times compared to standard SGD and Adam."
}