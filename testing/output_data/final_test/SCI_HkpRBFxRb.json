{
    "title": "HkpRBFxRb",
    "content": "Reinforcement Learning (RL) involves modeling behavior policies for decision-making tasks. Temporal Difference (TD) learning is a key aspect, where the value function is updated towards a bootstrapped target. A new approach called Confidence-based Autodidactic Returns (CAR) allows the RL agent to learn the weighting of n-step returns in an end-to-end manner, unlike the traditional lambda-returns. Confidence-based Autodidactic Returns (CAR) enables the agent to determine the weighting of n-step returns, outperforming traditional methods. Experiments on the A3C algorithm in Atari 2600 domain show the effectiveness of CAR and weighted mixtures of multi-step returns. Reinforcement Learning (RL) is utilized for goal-directed decision-making tasks, modeled as Markov Decision Process (MDP). Traditional tabular methods for MDPs become infeasible due to maintaining estimates for every state. Recent advances in representation learning, such as deep neural networks, have revolutionized Reinforcement Learning (RL) by enabling generalization over large state spaces. Traditional RL methods using hand-crafted features have been limited in applicability, but deep neural networks allow for hierarchical compositional representations that lead to remarkable results, such as solving Atari 2600 tasks from raw pixels and achieving super-human performance in complex tasks. Building accurate and powerful state and action value function estimators is crucial for successful Reinforcement Learning solutions, as many practical methods rely on Temporal Difference Learning. \u03bb-returns are highly effective in improving value function estimates and policy estimates, leading to faster learning. \u03bb-returns (LR) are effective for faster propagation of delayed rewards and provide a trade-off between using complete trajectories and bootstrapping from n-step returns. With the advent of deep RL, multi-step returns have gained popularity, but LR's exponentially decaying weighting for n-step returns is seen as an ad-hoc design choice. This paper benchmarks \u03bb-returns and proposes a generalization called Confidence-based Autodidactic Returns (CAR). The paper benchmarks \u03bb-returns and introduces Confidence-based Autodidactic Returns (CAR) as a generalization to alleviate the need for ad-hoc weight choices in deep reinforcement learning. CAR dynamically adjusts weights for n-step returns based on different states, showing significant improvement over traditional methods. In this section, basic concepts for understanding the work are presented. An MDP is defined as a tuple S, A, r, P, \u03b3, where S is the set of states, A is the set of actions, r is the reward function, P is the transition probability function, and \u03b3 is the discount factor. The agent interacts with an environment over discrete time steps, receiving a state and selecting an action at each step. The agent in a Markov Decision Process (MDP) receives a state and selects an action from the available set. The policy \u03c0 maps states to actions, aiming to maximize the cumulative discounted future reward. Actor Critic algorithms parameterize the policy and value function, updating policy parameters using Policy Gradient Theorem based objective functions. The value function estimates serve as a baseline to reduce variance in policy gradient estimates. The A3C algorithm introduces actor-critic algorithms that work on high-dimensional visual input space by executing multiple actor learners on different threads to explore different parts of the state space simultaneously. It uses n-step returns as an estimate for the target value, with a hyper-parameter controlling the level of rolling out policies. The A3C algorithm uses n-step returns to estimate target values, with a hyper-parameter controlling policy rollout levels. Weighted average of n-step return estimates can be used to arrive at TD-targets, with a special case being \u03bb-returns for TD-learning. Autodidactic returns are a form of weighted returns where the weight vector is learned alongside the value function. These returns are state-dependent and denoted as w(s t). They can be used to improve value function approximations. Autodidactic returns, denoted as w(s t), are used for learning better value function approximations through the TD(0) learning rule. In contrast to \u03bb-returns, autodidactic returns assign dynamic weights to returns based on the reward signal received by the agent. These weights are learned during the interaction with the environment, making them a generalization of \u03bb-returns. The agent can weigh n-step returns based on its confidence in the value function estimate V(s t+n). This confidence is denoted as c(s t+n) and the weight vector w(s t) can be computed using it. Previous works have explored weighing returns based on confidence, but quantifying and optimizing this certainty metric has been challenging. In this work, a simple and robust way to model confidence metrics is proposed, addressing the significance of high confidence values for states in improving value function estimates. The incorporation of truncated \u03bb-returns into the A3C framework, termed LRA3C, is suggested. The A3C algorithm is suitable for using weighted returns like \u03bb-returns, enhancing the value function estimation process. CARA3C is a modification of the A3C framework that uses autodidactic returns instead of normal n-step returns. It involves incorporating weighted returns into the network and includes a separate neural network for predicting confidence values. The network's parameters are updated based on the gradient of the loss function, leading to improved value function estimation. The CARA3C network calculates confidence values using a separate neural network and incorporates autodidactic returns for training. The policy improvement involves following sample estimates of the loss function's gradient, leading to improved value function estimation. The LSTM-A3C neural networks share all but the last output layer for representing the policy and value function. The LSTM BID3 controller is shared by the policy and value networks, with a new output layer predicting confidence values. Autodidactic returns affect the LSTM controller parameters, but moving the TD target towards the value function prediction can lead to unstable learning due to L2 loss. To prevent learning instability in the A3C network, gradients are blocked from flowing back from the confidence values computation to the LSTM layer. This ensures that the parameters for the autodidactic return do not influence the LSTM parameters during back-propagation, allowing the confidence network parameters to be learned independently. The confidence network parameters are learned independently from the LSTM outputs in the A3C network to prevent learning instability. General game-play experiments were conducted on 22 tasks in the Atari domain, with networks trained for 100 million time steps and hyper-parameters tuned on a subset of four tasks. Experiments were repeated thrice with different random seeds for robustness. The study ensured robustness by using the same random seeds for experiments and averaging results. Experimental details and hyper-parameter selection process are provided in Appendices A and G. Performance evolution is shown in FIG0, with more detailed graphs in Appendix C. CARA3C and LRA3C showed improvement over A3C in normalized scores. The A3C normalized score is calculated as p q. CARA3C and LRA3C show improvement over A3C, with CARA3C achieving over 4\u00d7 the scores obtained by A3C on average. CARA3C achieves a 67\u00d7 performance improvement in the task Kangaroo. The difference in weights evolves dynamically during episodes for CARA3C in four tasks. The difference in weights evolves dynamically during episodes for CARA3C in four tasks, showing significant improvement over A3C and LRA3C. This analysis highlights the unique weighting strategy of CARA3C, giving it an edge in many games. The use of dynamic Autodidactic Returns is further motivated by these results. The network assigns confidence values to states based on changes in value estimates during training. Initially, confidence is similar regardless of value changes, but as training progresses, higher confidence is given to states with smaller value changes. This confidence value helps quantify the network's certainty in value estimates, allowing for better target value bootstrapping. The network assigns confidence values to states based on changes in value estimates during training. Higher confidence is given to states with smaller value changes as training progresses. Confidence values help quantify the network's certainty in value estimates for better target value bootstrapping. The periodic nature of game frames is observed in the graphs due to recurring key states in the games. In Beam Rider, high confidence states are seen at the start of each wave with new enemies. In Kangaroo, a fruit falls periodically on the left side for points. The CARA3C agent learns to exploit these game features for success. The CARA3C agent identifies recurring states where the fruit is close to the kangaroo, enabling better estimation of value functions. This attention mechanism allows for better estimates in other states by bootstrapping with a focus on high confidence states. CARA3C uses an attention mechanism to improve value function estimates. Two methods are proposed for learning value functions more effectively than n-step returns. A comparison experiment was conducted with trained agents to analyze the value functions learned by the methods. CARA3C outperforms LRA3C and A3C in estimating the Value function, except in the game of Kangaroo where A3C and LRA3C perform well due to the policy being random. A straightforward way to incorporate \u03bb-returns into A3C is proposed, leading to the benchmarking of LRA3C. Confidence-based Autodidactic returns (CAR) are introduced, where the agent assigns dynamic weights to n-step returns. Experiments show the effectiveness of mixing multi-step returns with CARA3C or LRA3C. The efficacy of mixing multi-step returns with CARA3C or LRA3C is demonstrated, with CARA3C outperforming A3C in 18 out of 22 tasks. Autodidactic Returns allow the DRL agent to model confidence in its predictions, leading to improved performances. Different ways of modeling autodidactic weights can further enhance generalization for the agent. Weighted returns can improve generalization in how the agent perceives its TD-target. The proposed idea of CAR can be combined with any DRL algorithm where the TD-target is modeled in terms of n-step returns. The LSTM-variant of A3C algorithm was used for CARA3C and LRA3C experiments, with async-rmsprop algorithm for updating parameters. Entropy regularization with a \u03b2 of 0.01 was used to encourage exploration. The \u03b2 for entropy regularization was set at 0.01 after hyper-parameter tuning for CARA3C and LRA3C. The optimal initial learning rate was 10^-3 for both methods. The discounting factor for rewards remained at 0.99. The most important hyper-parameter for LRA3C is the \u03bb for the \u03bb-returns, tuned extensively. The best performing models had \u03bb = 0.9 and were trained for 100 million time steps. Evaluation was conducted every 1 million steps. Evaluation was conducted every 1 million steps of training, following a strategy to ensure fair comparison with baseline scores. The average game-play performance was evaluated for 100 episodes, each capped at 20000 steps, with results shown in FIG0 and raw scores in TAB1 of Appendix B. The latest agent's performance after training for 100 million steps was assessed, using a low-level architecture similar to previous studies. Visual depiction of the network used for CARA3C can be seen in Figure 1. The network architecture for CARA3C is based on BID8, with common parts shared with LRA3C. Both methods have convolutional layers, FC layers, and an LSTM layer with 256 neurons. The policy and value function are derived from the LSTM outputs. CARA3C uses different final output layers for policy, value function, and confidence value. The Actor and Critic share all layers except the final one. These design choices ensure fair comparisons to the baseline A3C model. The network architecture for CARA3C is based on BID8, with common parts shared with LRA3C. CARA3C uses different final output layers for policy, value function, and confidence value. Evaluation was done using the agent trained for 100 million steps, with scores averaged across 3 random seeds. Training curves for 22 Atari tasks show how CARA3C and LRA3C agents' performance evolves over time. This appendix presents expanded results from Section 4.2. The expanded results in this appendix show the dynamic changes in confidence values over the course of an episode, highlighting peaks and troughs in many games. The periodicity observed in some graphs is attributed to the nature of the games themselves, not the confidences learned. Figure 10 illustrates the evolution of confidence values over an episode."
}