{
    "title": "HJz6tiCqYm",
    "content": "In this paper, rigorous benchmarks for image classifier robustness are established. The first benchmark, ImageNet-C, expands on corruption robustness and identifies preferable classifiers for safety-critical applications. A new dataset, ImageNet-P, allows researchers to assess a classifier's robustness to common perturbations. The benchmark evaluates performance on common corruptions and perturbations, not worst-case adversarial perturbations. Minimal changes in relative corruption robustness are found from AlexNet to ResNet classifiers. Strategies to enhance corruption and perturbation robustness are discovered, including the effectiveness of a bypassed adversarial defense in providing substantial common perturbation robustness. These benchmarks aim to support the development of networks that robustly generalize. The human vision system is robust in ways that existing computer vision systems are not. Humans are not fooled by small changes in images or various forms of corruption like snow, blur, and pixelation. Achieving robustness in computer vision and machine learning is crucial for safety-critical applications. Most research focuses on adversarial examples, unknown unknowns, and data poisoning, but this study introduces datasets for other forms of robustness. The IMAGETNET-C dataset is introduced for input corruption robustness, containing 75 common visual corruptions applied to ImageNet. IMAGENET-P is created for input perturbation robustness with subtly differing ImageNet images. Current deep learning systems show room for improvement on IMAGENET-C, with new methods and architectures enhancing corruption robustness without accuracy loss. The stability of network predictions on perturbed images is measured, revealing existing networks' instability on common perturbations. Enhancing corruption robustness can also improve perturbation robustness, with recent architectures showing significant improvements in both areas. The Adversarial Logit Pairing defense enhances robustness against diverse perturbations. Adversarial images are clean images distorted to confuse classifiers, highlighting the need for networks that do not rely on spurious correlations. Adversarial distortions are crafted to confuse classifiers, serving as a worst-case analysis for network robustness. Adversarial robustness is often interchangeable with robustness in the literature, with new defenses quickly succumbing to new attacks. Adversarial perturbations exist for datasets with classification errors, requiring an increase in training set size for robustness. Some models suggest modifying the problem of adversarial robustness itself. Speech recognition research focuses on robustness to common corruptions like street noise and background chatter, rather than worst-case adversarial corruptions. Studies show that convolutional networks are more robust in noisy environments, and additional robustness can be achieved through pre-processing techniques. Some research suggests modifying the problem of adversarial robustness for increased real-world applicability. The fragility of convolutional networks is highlighted through various corruptions like impulse noise and Gaussian noise. Studies show that human vision is more robust than networks, even after fine-tuning on specific corruptions. Efforts to enhance robustness include finetuning on blurred images, but fine-tuning on multiple types of blur may only marginally improve performance. Additionally, fine-tuning on noisy images can lead to underfitting. BID61 and BID8 address underfitting in classifiers trained on noisy images by encouraging the softmax distribution to match clean images and using corruption-specific experts. They define corruption and perturbation robustness and suggest evaluating a classifier's corruption robustness in addition to its accuracy on clean inputs. The text discusses the difference between corruption robustness and adversarial robustness in classifiers. Corruption robustness measures average-case performance on corruptions, while adversarial robustness measures worst-case performance on small perturbations. Perturbation robustness evaluates a classifier's prediction stability in the face of minor input changes. Common corruptions and perturbations encountered in natural images are used to approximate these robustness measures. The text discusses common corruptions and perturbations in natural images, referred to as \"common\" corruptions. These corruptions are available in IMAGENET-C and IMAGENET-P benchmarks, with 15 diverse corruption types in IMAGENET-C. Each corruption type has five severity levels to simulate real-world variations. The IMAGENET-C dataset contains algorithmically generated corruptions applied to ImageNet BID7 validation images, available for download at https://github.com/hendrycks/robustness. Networks should not be trained on IMAGENET-C images but on datasets like ImageNet. Additional corruption types are provided for experimentation in CIFAR-10-C, TINY IMAGENET-C, IMAGENET 64 \u00d7 64-C, and Inception-sized editions. The IMAGENET-C dataset includes 75 corruptions applied to ImageNet validation images for testing network performance. IMAGENET-P tests classifier perturbation robustness, crucial for maintaining user trust and preventing erratic predictions. Perturbations can impact image optimization procedures like style transfer and decision explanations. The dataset includes noise, blur, weather, and digital distortions, with various difficulty levels and editions for different datasets. The IMAGENET-P dataset is designed for benchmarking network performance, with perturbation sequences generated from ImageNet validation images. It includes common perturbations like Gaussian noise and shot noise, with each frame in the sequence being a perturbation of the previous frame. The dataset aims to test classifier perturbation robustness and includes various difficulty levels and editions for different datasets. The IMAGENET-P dataset contains perturbation sequences generated from ImageNet validation images to test classifier robustness. Perturbations include motion blur, zoom blur, snow, brightness, translate, rotate, tilt, and scale perturbations. IMAGENET-C metrics evaluate classifier performance across five corruption severity levels. The IMAGENET-C dataset evaluates classifier performance on various corruption types and severity levels. The Corruption Error (CE) is computed to standardize error rates across different corruptions. This results in the mean CE (mCE), a more nuanced corruption robustness measure. The Relative mCE measures the relative robustness of a classifier when encountering corruptions, calculated by averaging Relative Corruption Errors. This provides insight into performance degradation in the presence of corruptions, contrasting classifiers with low clean error rates but high corruption error rates. The approach to estimate the \"Flip Rate\" of a network on perturbation sequences using IMAGENET-P perturbation sequences is discussed. The Flip Rate is standardized by the sequence's difficulty for increased commensurability, leading to the mean Flip Rate (mFR). No \"relative mFR\" is defined, and predicted class probabilities are not directly used due to differences in model calibration. The text discusses penalizing top-5 prediction inconsistencies in perturbation sequences by comparing permutations of ranked predictions. The method involves using permutations to compare top-5 lists and defining a measure to assess the similarity of top-5 predictions across perturbation sequences. The protocol proposed involves training an image recognition network on the ImageNet training set and other desired sets, evaluating it on IMAGENET-C or IMAGENET-P using specific metrics. Training with distortions and data augmentation is allowed, but training on corruptions or perturbations is discouraged. The study evaluates the robustness of current computer vision methods by testing on IMAGENET-C and IMAGENET-P datasets. Architecture improvements have led to better generalization to corrupted distributions, with models showing consistent improvements in representations over time. However, the increase in corruption robustness is mainly due to accuracy improvements. The study found that current classifiers are surprisingly bad at handling perturbations, with ResNet-18 having a high probability of flipping predictions. VGGNets, despite being worse at generalizing to corrupted examples, can be more robust on perturbed examples. Batch Normalization made VGG-19 less robust to perturbations but more robust overall. Histogram Equalization successfully standardizes speech data for robust speech recognition. Preprocessing with Contrast Limited Adaptive Histogram Equalization is effective for images, reducing the effect of some corruptions without worsening performance on most. CLAHE, or Contrast Limited Adaptive Histogram Equalization, improves the corruption robustness of a pre-trained ResNet-50 model by reducing the error rate and mCE. Multiscale architectures, such as Multigrid Networks, enhance corruption robustness by propagating features across scales at each layer. Multiscale Dense Networks (MSDNets) and Multigrid networks enhance corruption robustness but do not improve perturbation robustness. Multigrid networks outperform ResNets and MSDNets on noisy inputs, showing better suppression of pixel noise. ResNet-50 has a top-1 error rate of 23.9%, MSDNet has 24.6%, and Multigrid network has the same error rate. When evaluating all corruptions, ResNet-50 has an mCE of 76.7%, MSDNet has 73.6%. Feature aggregation in recent models like DenseNets and ResNeXts enhances representations compared to ResNets, as evidenced by a decrease in ImageNet error rates. Switching from ResNet-50 to DenseNet-121 decreases mCE from 76.7% to 73.4%, and switching to ResNeXt-50 drops mCE to 68.2%. Corruption robustness results are summarized in Figure 5. Corruption robustness results show that making recent models more monolithic can lead to greater robustness gains. Model size and feature aggregation also play a role in improving stability on corrupted inputs. For example, swapping to larger models like DenseNet-161 or ResNeXt-101 decreases the mCE, indicating increased robustness. Both model size and feature aggregation are key factors in improving corruption robustness. Larger models like DenseNet-161 and ResNeXt-101 show decreased mCE, indicating increased stability on corrupted inputs. Stylized ImageNet proposes a novel data augmentation scheme to reduce reliance on textural cues for classification. Stylized images rely less on textural cues for classification. Training a ResNet-50 on typical and stylized ImageNet images reduces mCE from 76.7% to 69.3%. Adversarial Logit Pairing (ALP) is a defense for large-scale image classifiers, providing perturbation robustness against various types of noise and perturbations. ALP improves robustness to small gradient perturbations and generalizes well, with a publicly available Tiny ImageNet ResNet-50 model fine-tuned with ALP showing a 41% and 40% relative decrease. In this paper, the authors introduced comprehensive benchmarks for corruption and perturbation robustness using new datasets, IMAGENET-C and IMAGENET-P. They found that despite architectural advancements, relative corruption robustness only saw minimal changes. The study also revealed unexpected instability in classifiers when faced with simple perturbations. In this work, methods like histogram equalization, multiscale architectures, and larger feature-aggregating models were found to improve corruption and perturbation robustness. Adversarial defense designed for adversarial \u221e perturbations showed even greater perturbation robustness. The study introduced new experiments, metrics, and datasets to study model robustness in real-world settings. Impulse noise corruption types were shown in varying severities. The Impulse noise corruption types in varying severities range from negligible to pulverizing. To simplify model validation, an additional form of corruption is provided for each of the four general types: speckle noise, Gaussian blur, spatter, and saturate. These corruptions are available for download at https://github.com/hendrycks/robustness. The original pixel intensity is larger. Gaussian blur is a low-pass filter where a blurred pixel is a result of a weighted average of its neighbors, and farther pixels have decreasing weight in this average. Spatter can occlude a lens in the form of rain or mud. Saturate is common in edited images where images are made more or less colorful. See FIG3 for instances of each corruption type. The function DISPLAYFORM0 is defined as \u03c3 = (\u03c4 (x)) \u22121 \u03c4 (x ) and the empty sum is understood to be zero. The deviation d computes the deviation between the top-5 predictions of two prediction lists. For simplicity, the deviation is found between the identity and \u03c3 rather than \u03c4 (x) and \u03c4 (x ). In consequence, d ((2, 3, 4, 5, 6 , . . . , 1)) = 5, d ((1, 2, 3, 5, 6, 4, 7, 8, . . .)) = 2, and d ((5, 4, 3, 2, 1, 6, 7, 8, 9 , . . .)) = 12. In the context of perturbation robustness for predictions, various measures can be used to assess the displacement of different ranked classes. These measures can include Zipfian assumptions about class relevance, logarithmic functions, or class probabilities provided by the model. However, comparing models based on these measures may be challenging due to calibration differences. Researchers may focus on perturbations that are more likely to cause significant changes in predictions. Researchers can improve perturbation robustness by comparing frames two frames ahead instead of just one. Code for this change is available at https://github.com/hendrycks/robustness. For non-temporal perturbations, larger noise sequences are provided. IMAGENET-C corruption relative robustness results can be found in BID32, IMAGENET-P mFR values in TAB7, and mT5D values in Stability Training. Stability training involves training on noisy images to improve network robustness. The method involves minimizing cross-entropy between noisy and clean image softmax distributions. The authors fine-tuned a ResNet-50 with stability training for noisy images, testing on IMAGENET-C. Despite not increasing robustness to unseen noise corruptions, perturbation robustness slightly improved. The best model had an mFR of 57%, compared to the original ResNet's 58%. Benchmarking robustness-enhancing techniques requires a diverse test set. The authors attempted to improve model robustness by using image restoration techniques, specifically denoising with the non-local means BID3 method. Despite aiming to enhance noise robustness, denoising actually increased the mCE from 76.7% to 82.1%. This unexpected result was attributed to the non-local means algorithm stripping images of subtle details, even when noise was absent. The authors attempted to enhance noise robustness using image restoration techniques, but denoising with the non-local means BID3 method unexpectedly increased the mCE. This was due to the algorithm removing subtle details from images. Additionally, 10-crop classification was used to improve prediction stability by processing multiple image crops through a network to produce more accurate predictions. In testing the hypothesis that smaller models may be more robust, a CondenseNet with sparse convolutions and pruned filter weights achieved a 26.3% error rate and 80.8% mCE. However, further pruning led to decreased performance and robustness, indicating that larger models of similar accuracy are more robust. Models designed for mobile devices, while smaller and simpler, may not necessarily be more robust than larger models. In testing model robustness, smaller models designed for mobile devices do not necessarily improve robustness. Machine learning aims to learn the fundamental structure of categories, such as broad categories like \"bird\" with many subtypes like \"cardinal\" or \"bluejay\". A test for subtype robustness involves generalization to unseen subtypes sharing essential characteristics of a broader type. The ImageNet-22K dataset is repurposed for investigating subtype robustness, selecting 25 broad types for the experiment. In testing model robustness, smaller models for mobile devices may not enhance robustness. The study focuses on learning broad categories and their subtypes, with a test for subtype robustness using the ImageNet-22K dataset. 25 broad types are selected, with seen and unseen subtypes tested for accuracy. The classifiers show a subtype robustness performance gap despite training on millions of images with 25 classes. Proposed architectures closely follow the trendline."
}