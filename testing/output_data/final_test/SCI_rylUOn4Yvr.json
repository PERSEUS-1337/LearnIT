{
    "title": "rylUOn4Yvr",
    "content": "In this work, the research question of how to train robust Deep Neural Networks (DNNs) in the presence of semantically abnormal examples is addressed. The proposed gradient rescaling (GR) method modifies the gradient magnitude to emphasize easier training data points in the presence of noise, improving generalization performance. Empirical results show that GR is highly anomaly-robust and outperforms state-of-the-art methods by a significant margin, such as achieving a 7% increase on CIFAR100 with 40% noisy labels. The proposed gradient rescaling (GR) method significantly outperforms standard regularisers in clean and abnormal settings, showing a 7% increase on CIFAR100 with 40% noisy labels. DNNs rely heavily on accurate semantic labels for training, but maintaining label quality becomes challenging with large datasets. Additionally, DNNs can memorize training data even with random labels, leading to struggles in training. DNNs struggle to discern meaningful data patterns and ignore semantically abnormal examples, making robustness against anomalies essential during training. Progress has been made in this area, with three key perspectives: 1) Examples weighting, such as knowledge distillation from auxiliary models, which can be challenging to implement effectively. Progress has been made in enhancing the robustness of deep neural networks against anomalies during training. This includes utilizing reliable auxiliary models, robust loss functions, and explicit regularization techniques. Anomalies can be categorized as out-of-distribution anomalies (e.g., irrelevant objects in an image) and in-distribution anomalies (e.g., mislabeled images). In this work, the focus is on the training examples for deep neural networks. Emphasis is placed on harder instances for faster convergence and better performance, especially in clean datasets. The concept of emphasis focus and spread is introduced for better analysis under different settings. When severe noise exists, DNNs learn simple patterns before memorizing abnormal ones. Anomalies are harder to fit and own larger gradient magnitude in the later stage. Emphasis should be on relatively easier examples for better training. Emphasis spread adjusts when emphasis focus changes. The text discusses adjusting emphasis spread based on emphasis focus in a unified example weighting framework. Gradient rescaling (GR) is proposed to modify the gradient magnitude of the logit vector in a network's last fully connected layer. This approach is motivated by gradient analysis of loss functions and can be connected to example weighting, robust losses, and explicit regularization. The text introduces Gradient Rescaling (GR) as a method to adjust the gradient magnitude in neural networks, providing a more direct and flexible way to modify optimization objectives. GR serves as emphasis regularization, different from standard regularizers like L2 weight decay and Dropout. Its effectiveness is demonstrated in various computer vision tasks with different network architectures. The text introduces Gradient Rescaling (GR) as a method to adjust the gradient magnitude in neural networks for better optimization. It shows GR's superiority over standard regularizers like L2 weight decay and Dropout in various computer vision tasks. The main contribution is the simultaneous consideration of emphasis focus and spread in examples weighting, along with robust losses minimization and explicit regularization techniques for training accurate DNNs in the presence of anomalies. Training robust and accurate DNNs in the presence of anomalies involves assessing example difficulty through loss, gradient magnitude, and input-to-label relevance score. Strategies include robust training, noise-aware modeling, and alternative optimization methods for label noise correction. Various methods have been developed for correcting noisy labels or empirical losses in deep neural networks. These methods require extra information or specific assumptions, such as human-assisted noise structure speculation or the use of an extra clean dataset. While some algorithms iteratively train the model and infer latent true labels, they may not be directly applicable to unknown diverse semantic anomalies. The study focuses on anomalies in deep learning, specifically addressing diverse or arbitrary abnormal examples that may be out-of-distribution. Unlike prior work on label noise, the proposed method, GR, is independent of specific loss functions and does not directly impact gradient back-propagation. The study introduces a new method, GR, for robust training in deep learning to handle diverse anomalies. It is independent of specific loss functions and does not directly impact gradient back-propagation. The method involves rethinking generalisation and sample weighting in popular losses like CCE, MAE, and GCE. The study introduces a new method, GR, for robust training in deep learning to handle diverse anomalies. It involves rethinking generalisation and sample weighting in popular losses like CCE, MAE, and GCE. CCE, MAE, and GCE are defined with their gradients, emphasizing the importance of gradient magnitude in updating model parameters. Different settings of GR are shown with varying emphasis focuses. The study introduces a new method, GR, for robust training in deep learning to handle diverse anomalies. GR is a unified sample reweighting framework that adjusts emphasis focus and spread by choosing proper \u03bb and \u03b2. In MAE, images with input-to-label relevance scores of 0.5 become the emphasis focus. GCE involves loss calculation and gradient adjustments based on input-to-label relevance scores. Zhang & Sabuncu (2018) applied a truncated version where examples with p i \u2264 0.5 are dropped during training. The study introduces a new method, GR, for robust training in deep learning to handle diverse anomalies. GR is a unified sample reweighting framework that adjusts emphasis focus and spread by choosing proper \u03bb and \u03b2. In MAE, images with input-to-label relevance scores of 0.5 become the emphasis focus. GCE involves loss calculation and gradient adjustments based on input-to-label relevance scores. Zhang & Sabuncu (2018) applied a truncated version where examples with p i \u2264 0.5 are dropped during training. The gradient is controlled directly to improve supervision information in the model. The study introduces a new method, GR, for robust training in deep learning to handle diverse anomalies. GR unifies the gradients of CCE, MAE, and GCE, adjusting emphasis focus and spread with hyper-parameters \u03bb and \u03b2. Emphasis focus is defined as the input-to-label score with the largest weight, while emphasis spread controls the distribution of weights over training examples. The study introduces a new method, GR, for robust training in deep learning to handle diverse anomalies. GR unifies the gradients of CCE, MAE, and GCE, adjusting emphasis focus and spread with hyper-parameters \u03bb and \u03b2. Emphasis spread is the weight variance over training instances in a mini-batch, with larger \u03b2 values chosen for higher \u03bb values. The transformation function g is designed as a monotonically increasing function, with g(\u00b7) = exp(\u00b7) chosen for its effectiveness. The exact loss format is an error function, and various existing cases are summarized. In this work, inspired by the analysis of CCE, MAE, and GCE, the focus is on exploring the impact of gradient magnitude during gradient back-propagation in deep networks. The study proposes rescaling the gradient magnitude to reduce the impact of noisy samples. Testing is done on CIFAR-10 and CIFAR-100 datasets, with potential future exploration of amending gradient directions. On CIFAR-10 and CIFAR-100 datasets, ResNet models are used for testing with specific configurations. Standard data augmentation techniques are applied, and classification accuracies are reported. The batch size is 128. Classification accuracies of CCE and GR on clean CIFAR-10 and CIFAR-100 are compared. GR can achieve competitive performance with CCE under clean data. Results show that noise-tolerant proposals perform similarly with CCE. Reimplemented results are shown in Table 2. Best performance for GR is observed when emphasis focus is 0. When emphasis focus is 0, GR shows the best performance. Treating all training examples equally yields competitive results when \u03b2 = 0. Symmetric label noise is common in large-scale datasets, making sample differentiation and reweighting more effective in the presence of noise. The study explores the impact of different emphasis focuses on noisy labels in training data. Different emphasis levels are tested, with higher emphasis leading to easier training. Setting a proper spread for each emphasis focus is crucial for optimal performance. The study analyzes the impact of emphasis focus on noisy labels in training data. Different emphasis levels are tested, with higher emphasis leading to easier training. Results show that CCE achieves the best accuracy on corrupted training sets but has worse final test accuracy than most models. Emphasizing on harder examples can lead to the worst final test accuracy. In applications with noisy training data, using CCE or focusing on harder examples can hurt a model's generalization. Emphasis focus should be adjusted based on noise rate to improve a model's robustness. Emphasis spread also plays a significant role in model performance when emphasis focus is fixed. In noisy training data applications, adjusting emphasis spread based on noise rate can enhance model robustness. GR can outperform CCE by modifying emphasis spread, preventing model collapse. Various competitors like FullModel, Forgetting, Self-paced, Focal Loss, MentorNet, and Reed Soft are compared using GoogLeNet V1. Results show GR with fixed hyperparameters \u03b2 = 8, \u03bb = 0.5 outperforms others under different noise rates. GR with fixed hyperparameters \u03b2 = 8, \u03bb = 0.5 outperforms state-of-the-art GCE, especially in severe label noise. Better generalisation is achieved with higher emphasis focus as noise rate increases. Overfitting and underfitting both lead to bad generalisation. Most baselines have been reimplemented for direct comparison. For direct comparison, experimental configurations are followed using ResNet-44. Training and evaluation are repeated 5 times with different random seeds. Competitors include D2L, GCE, and other baselines like Standard CCE, Forward, Backward, and Bootstrapping with soft or hard convex combinations for label correction. Results show that GR outperforms competitors, especially in severe label noise conditions. GR is simpler and does not require prior knowledge of noise-transition matrix. Bootstrapping for label correction is time-consuming. D2L achieves noise-robustness by restricting dimensionality expansion during training and is considered state-of-the-art. GCE is reimplemented for comparison as it outperforms MAE. SL boosts CCE symmetrically with a noise-robust counterpart. D2L estimates local intrinsic dimensionality every b mini-batches and checks for dimensionality expansion every e epochs, which is time-consuming. Clothing 1M dataset contains 1 million images with 61.54% reliable training labels. GR outperforms competitors in severe label noise conditions. The trend in the corresponding parameters is consistent with Table 3, where increasing r allows for better robustness by adjusting \u03b2 and \u03bb. The distribution of input-to-label relevance scores differs between CIFAR-100 and CIFAR-10 due to the former having 100 classes. The study compares methods that learn from noisy training data without auxiliary information, using ResNet-50 with specific settings like pretraining on ImageNet and SGD optimization with momentum and weight decay. The learning rate starts at 10 \u22123 and is divided by 10 after 5 epochs. Training terminates at 10 epochs. Standard data augmentation includes resizing and cropping images, with a batch size of 64. The noise rate is around 38.46%, with \u03bb = 1, \u03b2 = 16. Comparison with noise-robust algorithms like Standard CCE, Forward, S-adaptation, and Masking is done on Clothing 1M dataset. Results show that under real-world agnostic noise, GR outperforms the state-of-the-art algorithms. The burden of noise-transition matrix estimation in Forward and S-adaptation is heavy due to alternative optimization steps, making it non-trivial without sufficient data. Masking reduces the burden of estimation by exploiting human cognition but lacks competitive performance. Label Optimization, while time-consuming, learns latent true labels and model parameters iteratively. The MARS dataset contains 20,715 videos of 1,261 persons with a total of 1,067,516 frames. In the study, 1,261 persons were analyzed using a dataset of 1,067,516 frames. Anomalies were present in the data, including background noise and out-of-distribution persons. The training and testing sets consisted of videos from 625 and 636 persons, respectively. The results were evaluated using cumulated matching characteristics (CMC) and mean average precision (mAP). The study utilized the Caffe framework for analysis and trained GoogleNet V2 on appearance information without considering temporal data. The study analyzed 1,261 persons using a dataset of 1,067,516 frames. Anomalies were present, and the training/testing sets included videos from 625/636 persons. Results were evaluated using CMC and mAP. GoogleNet V2 was trained on appearance information without considering temporal data. The video representation is the average fusion of frames' representations. Training settings include a learning rate starting at 0.01, SGD optimizer, weight decay of 0.0005, momentum of 0.9, and batch size of 180. GCE was implemented with reported best settings. Testing involved L2 normalizing videos' features and calculating cosine similarity. Results are displayed in Table 7. Incorporating attention mechanisms, GR outperforms other methods like OSM+CAA in terms of effectiveness and simplicity. GR achieves the best mAP performance compared to standard regularizers like L2 weight decay and Dropout. With fixed parameters, GR surpasses Dropout+L2 and state-of-the-art D2L in accuracy. In Table 5, GR achieves 52.0% accuracy, outperforming standard regularisers. GR works best with L2 weight decay. Results on CIFAR-100 show GR's superiority. The study focuses on emphasizing training examples effectively using a gradient rescaling framework. Extensive experiments on different tasks using various network architectures demonstrate the effectiveness of gradient rescaling (GR) for emphasis regularisation. Out-of-distribution anomalies and in-distribution anomalies are identified, showcasing the practical application of GR. The derivation of softmax layer and cross-entropy loss function is also discussed. The derivation of softmax layer and cross-entropy loss function is discussed, along with the calculation of derivatives with respect to logits. The effectiveness of gradient rescaling (GR) for emphasis regularization is demonstrated through extensive experiments on various tasks and network architectures. Additionally, the performance of GR on small datasets with less than 5,000 data points is questioned. In Section 4.2, the problem of label noise on CIFAR-10 and CIFAR-100 is discussed. When noise rates are high, the number of clean training examples is significantly reduced. Comparison of gradient rescaling (GR) with other regularizers is done on a small-scale fine-grained visual categorization problem using the Vehicles-10 Dataset from CIFAR-100. The Vehicles-10 Dataset from CIFAR-100 consists of 10 fine classes of vehicles, with 500 training images and 100 testing images per class. The dataset is used to evaluate the impact of noise rate (r=0.2) on classification performance. Results show that using Gradient Rescaling (GR) improves performance compared to other regularizers. The study explores the impact of noise rate on model robustness using Gradient Rescaling (GR) with different emphasis focuses and spreads on CIFAR-100 dataset. Results show that increasing emphasis on less difficult examples improves model robustness, with the best \u03b2 and \u03bb values selected for each noise rate. The study demonstrates that as noise rate increases, adjusting \u03b2 and \u03bb can enhance model robustness, albeit at a smaller scale compared to CIFAR-10. The study focuses on increasing \u03b2 and \u03bb for better robustness on CIFAR-100 dataset. Results show that adjusting emphasis on easier examples improves generalization. Overfitting and underfitting both lead to bad generalization. Different emphasis focuses and spreads are displayed in the results. When \u03bb is larger, \u03b2 should also be larger. When \u03bb is varied, different values of \u03b2 were tested: 0.5, 1, 2, 4 for \u03bb = 0; 4, 8, 12, 16 for \u03bb = 0.5; 8, 12, 16, 20 for \u03bb = 1; and 12, 16, 20, 24 for \u03bb = 2."
}