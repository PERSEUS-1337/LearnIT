{
    "title": "BJGVX3CqYm",
    "content": "Recent work in network quantization has reduced the complexity of neural network inference, allowing deployment on devices with limited resources. This paper explores quantizing different layers with varying bit-widths using a differentiable neural architecture search framework. Experimental results show improved compression of ResNet on CIFAR-10 and ImageNet, with quantized models outperforming baseline models. Recent research has focused on low-precision inference of ConvNets to enable deployment on embedded and mobile devices with limited resources. While existing quantization methods typically use uniform bit-width assignment for all layers, exploring mixed precision quantization could optimize accuracy and efficiency. Mixed precision computation is supported on various hardware platforms, but its application to ConvNets has not been thoroughly investigated. In this work, the authors propose a differentiable neural architecture search (DNAS) framework to explore mixed precision quantization of ConvNets efficiently. The framework represents the architecture search space with a stochastic super net, aiming to minimize cost while maintaining accuracy. The DNAS framework uses a stochastic super net to find the optimal architecture parameters \u03b8 for ConvNets. The super net executes edges stochastically, with \u03b8 parameterizing the probability of execution. By training the super net with SGD, the optimal \u03b8 can be determined for optimal expected performance. The Gumbel SoftMax function is used to \"soft-control\" the edges during gradient computation. Using the Gumbel SoftMax function, the DNAS framework utilizes a stochastic super net to optimize architecture parameters for ConvNets. By controlling the edges, the gradient estimation of \u03b8 can be computed with a balance between bias and variance. This approach makes the super net fully differentiable and solvable by SGD, leading to state-of-the-art compression results for quantized ResNet models on CIFAR10 and ImageNet. The DNAS framework utilizes a stochastic super net to optimize architecture parameters for ConvNets, achieving state-of-the-art compression results for quantized ResNet models on CIFAR10 and ImageNet. It is a fast and efficient architecture search framework that can be applied to other problems, with quantization reducing model size and communication costs. Recent works have focused on quantizing weights and activations to reduce computational costs. Most works use the same precision for all layers, but mixed precision quantization is not explored much. Neural Architecture Search has become popular, with ENAS framework reducing computational costs by using a shared weight super network. The DNAS framework proposes a stochastic super net for training child networks independently. This approach differs from DARTS, which uses a deterministic super net. The idea of super net and stochastic super net is also used in other related works like BID18 and BID8 for exploring neural net architectures and model compression through AutoML. Neural architecture search is applied to model quantization, reducing bit-width for weights and activations. Mixed precision quantization allows flexibility in choosing different precisions for layers. The challenge is to maintain network accuracy while minimizing model size or computation cost. Neural architecture search is utilized for model quantization, reducing bit-width for weights and activations. The approach involves choosing different precisions for layers to balance network accuracy and minimize model size or computation cost efficiently. Neural architecture search involves training a neural network to convergence, which can take days, and dealing with a discrete search space with exponential complexity. To efficiently solve the problem, the idea of differentiable neural architecture search (DNAS) is discussed, where a super net representing the architecture space is constructed as a computational DAG. The text discusses how a neural net architecture can be represented by a subgraph, with nodes and edges selected based on an \"edge-mask\" vector. This allows for the conversion of a super net to a stochastic super net for efficient neural architecture search. The text discusses converting a super net to a stochastic super net for neural architecture search using a mask vector and parameter \u03b8. The stochastic super net is parameterized by \u03b8, and the expected loss is computed using Straight-Through estimation or REINFORCE. The text explains using the Gumbel Softmax technique for a differentiable loss function in neural architecture search. The optimization process involves solving for the optimal architecture-distribution parameter \u03b8 using stochastic gradient descent. Monte Carlo estimation is used for computing the gradient, with Equation FORMULA7 providing an unbiased estimation but with high variance. The Gumbel Softmax technique is used to address the high variance in gradient estimation for neural architecture search. It allows for \"soft\" sampling of edges using a continuous random variable, making it directly differentiable with respect to parameters. A temperature coefficient \u03c4 controls the behavior of Gumbel Softmax. The Gumbel Softmax technique uses a temperature coefficient \u03c4 to control the behavior of the edges. As \u03c4 \u2192 \u221e, edges are executed and outputs averaged, leading to biased but low variance gradient estimation. As \u03c4 \u2192 0, only one edge is sampled for execution, resulting in unbiased but high variance gradient estimation. An exponential decaying schedule is used to anneal the temperature during training, effectively stabilizing the super net training. The Gumbel Softmax trick stabilizes super net training by using an annealing temperature. A differentiable neural architecture search pipeline is proposed, alternating training of the super net's weight and architecture parameters. Training the weight optimizes candidate edges, while training the architecture parameter adjusts the impact of different edges on performance. The DNAS pipeline involves training the architecture parameter \u03b8 to sample edges with better performance and suppress those with worse performance. The dataset is split into X w and X \u03b8 for training, with a focus on generalization. The temperature \u03c4 for gumbel softmax is annealed in each epoch, and training of \u03b8 is postponed for N warmup epochs to ensure w is sufficiently trained. Sampled architectures are trained on X train and evaluated on X test. The DNAS framework is used to solve the mixed precision quantization problem by training architectures Q A. A super net is constructed for a ConvNet with candidate edges representing quantized convolution operators. The loss function encourages lower-precision weights and activations, with a weighting function to balance terms. Model size compression is achieved through this approach. The DNAS framework addresses mixed precision quantization by training architectures with a super net for a ConvNet. The loss function promotes lower-precision weights and activations, using a weighting function to balance terms. Model size compression is achieved through a cost function that considers parameters and bit-widths. In the DNAS framework, a cost function is defined to adjust the importance of the cost term in the loss function for quantizing models like ResNet20, ResNet56, and ResNet110 on CIFAR10 dataset. The focus is on reducing model size by quantizing weights while using full-precision activations. Mixed precision search is conducted at the block level, with the first and last layers left unquantized. A super net is constructed to match the target network's architecture, allowing for precision selection at the block level. In the DNAS framework, a super net is constructed to match the target network's architecture, allowing for precision selection at the block level. Different precisions are chosen for quantizing weights, with results compared to full-precision models. The experiment results are summarized in Table 1, showing accuracy and model size compression rates. Comparisons are made with previous methods using ternary weights for network layers. Our most accurate and efficient models in the DNAS framework outperform full-precision counterparts by up to 0.37% with 11.6 - 12.5X model size reduction. The most efficient models achieve 16.6 - 20.3X compression with less than 0.39% accuracy drop. Compared to previous methods, our model shows up to 1.59% better accuracy. Precision assignment for ResNet20 models is compared, with the most efficient model skipping the 3rd block in group-1. Accuracy vs. compression rate of ResNet110 architectures is plotted, showcasing the effectiveness of our models. In the DNAS framework, the most accurate and efficient models surpass full-precision counterparts by up to 0.37% with significant model size reduction. The most efficient models achieve 16.6 - 20.3X compression with minimal accuracy drop. Precision assignment for ResNet20 models is compared, with the most efficient model skipping the 3rd block in group-1. The accuracy vs. compression rate of ResNet110 architectures is plotted, showing the effectiveness of the models. In the DNAS framework, experiments were conducted to compress model size and computational cost by quantizing weights and activations. The DNAS search was efficient, taking less than 5 hours on 8 V100 GPUs to finish the search on ResNet18. Model size compression results are reported in TAB2, with \"MA\" denoting the architecture with the highest accuracy and \"ME\" denoting the most efficient. In experiments to compress model size and computational cost, DNAS framework quantized weights and activations. Results in TAB2 show models outperform full-precision by up to 0.5% with 10.6-11.2X size reduction. Efficient models achieve 19.0 to 21.1X size reduction while maintaining competitive accuracy. Less accurate models still match full-precision accuracy with 21.1X smaller size, using label-refinery to boost accuracy of quantized models. The experiment on computational cost compression using mixed precision quantization for ResNet on ImageNet demonstrates that searched models maintain accuracy despite high compression rates. Comparison is made with other methods like PACT, DoReFA, QIP, and GroupNet, showing the effectiveness of the proposed approach. The experiment focuses on mixed precision quantization for ResNet on ImageNet, achieving high compression rates while maintaining accuracy. Comparison with other methods like PACT, DoReFA, QIP, and GroupNet shows the effectiveness of the proposed approach, which is a neural architecture search for determining layer-wise bit-widths. The DNAS framework efficiently explores the NAS problem through gradient-based optimization, achieving high compression rates for ResNet on CIFAR10 and ImageNet. Quantized models with significantly smaller model size or computational cost outperform baseline models. DNAS is a general architecture search framework with various applications beyond mixed precision quantization. The function quantizes a continuous value to its nearest neighbor in a bounded activation function for CIFAR10 experiments. The dataset contains 50,000 training images and 10,000 testing images classified into 10 categories. The model is trained on 80% of the training set for weights and 20% for architecture parameters. The super net is trained for 90 epochs with a batch size of 512 using SGD with momentum and weight decay. For architecture parameters, an Adam optimizer is used with a learning rate of 5 \u00d7 10 \u22123 and weight decay of 10 \u22123. The cost function from equation (11) is utilized with \u03b2 set to 0.1 and \u03b3 to 0.9. Gumbel Softmax functions are controlled with an initial temperature of T 0 = 5.0 and a decaying factor \u03b7 of 0.025. Every 10 epochs, 5 architectures are sampled from the distribution P \u03b8 and trained for 160 epochs with cutout used in data augmentation. ImageNet experiments involve 1,000 classes with 1.3M training images and 50K validation images, scaled to have a shorter side of 256 pixels. Training a super net on ImageNet can be computationally expensive, so 40 categories are randomly sampled from the training set instead. The super net weights are trained using SGD with momentum for 60 epochs, with a batch size of 256 for ResNet18 and 128 for ResNet34. The initial learning rate is set to 0.1 and reduced with a cosine decay schedule, while the momentum is set to 0.9. Architecture parameters are optimized using an Adam optimizer with a learning rate of 10 \u22123 and weight decay of 5 \u00d7 10 \u22124. Other parameters such as cost coefficient \u03b2, cost exponent \u03b3, T 0, and decay factor \u03b7 are also set accordingly. Architecture parameters are trained separately by sampling 2 architectures from the distribution P \u03b8 every 10 epochs. The super net weights are trained on 40 randomly sampled categories from ImageNet using SGD with momentum for 60 epochs. Architecture parameters are optimized separately by sampling 2 architectures from the distribution P \u03b8 every 10 epochs and trained for 120 epochs with SGD and label-refinery BID0."
}