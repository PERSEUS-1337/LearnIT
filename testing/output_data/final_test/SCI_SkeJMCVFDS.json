{
    "title": "SkeJMCVFDS",
    "content": "Supervised learning with irregularly sampled time series has been a challenge for Machine Learning methods due to dealing with irregular time intervals. A novel method called Time Embeddings represents timestamps as dense vectors using sinusoidal functions, improving performance for LSTM-based and classical machine learning models, especially with very irregular data. This method was evaluated on predictive tasks from the MIMIC III dataset of electronic health records. The Time Embeddings (TEs) method introduces a novel way to represent time as dense vector representations using sinusoidal functions. This approach improves the expressiveness of irregularly sampled data, allowing models to estimate time intervals more effectively. The Time Embeddings (TEs) method introduces a novel way to represent time using dense vector representations with sinusoidal functions. It improves the expressiveness of irregularly sampled data for estimating time intervals. The method was evaluated with the MIMIC-III dataset using various models like LSTM and SelfAttentive LSTM for tasks such as in-hospital mortality prediction and length of stay. Other simpler models like linear regression and logistic regression were also tested with and without TEs. The problem addressed in this work is how machine learning methods can learn representations from irregularly sampled data in various fields such as electronic health records, climate science, ecology, and astronomy. Some approaches treat irregularity as missing data, with time axis discretization into fixed intervals. Lipton demonstrated that binary indicators of missingness and observation time delta can enhance Recurrent Neural Network models more effectively than imputation, despite the sparsity of binary masks. Recent research has focused on improving machine learning methods for irregularly sampled data. Shukla & Marlin introduced a neural network model to interpolate missing data without time discretization. Bahadori & Lipton proposed data augmentation based on temporal clustering. Che et al. developed a GRU model, GRU-D, to handle missing data decay. Bang et al. used LSTM cell states to improve the decay concept. The concept proposed in this work aims to improve the decay concept using LSTM cell states. It suggests using continuous cyclic functions to describe exact time moments, allowing for linear calculation of time between irregular observations without cumulative sum or fixed-length time discretization. Time Embeddings are dense representations that avoid sparsity, while Positional Embeddings were initially introduced to enhance Convolutional Neural Network performance with temporal data. The Positional Embedding (PE) was introduced to improve Convolutional Neural Networks' handling of temporal data by incorporating numerical representation of order into the embedding latent space. The Transformer network later utilized PE to address the issue of order modeling in neural networks based on attention modules, using sinusoidal functions to represent relative input positions. This approach allows the model to learn relative positions effectively. The positional embedding for irregular positions is inspired by the Transformer position representation. It involves discretizing sinusoidal functions based on irregular timestamps, making data time representative. The dimension of time embeddings (TEs) can be parameterized, with a maximum time defining the limit of representation. The relation between maximum time and TE dimension can be a limiting factor. The main advantages of using Time Embeddings (TEs) include not needing optimizable parameters, enabling linear computation of time delta between TEs for better long-term dependency recognition, and ensuring all TEs have the same norm to avoid large values. The algorithms were evaluated on in-hospital mortality and length of stay prediction tasks using the MIMIC-III dataset. The next section will cover data acquisition, preprocessing, test results, and discussion. The method performance was assessed using the MIMIC-III benchmark dataset, with sequences extracted from in-hospital stays within the first 48 hours. The dataset included 17,903 training samples and 3,236 test samples for in-hospital mortality after 48 hours, and 35,344/6,225 for length of stay after 24 hours. Variables were normalized to zero mean and unit variance, and categorical variables were encoded using one-hot encoding. Labels for length of stay were changed from hours to days for easier interpretation. Additionally, part of the observed test data was randomly removed to create a more irregular dataset. The models were trained using PyTorch on a P100 with batch size of 100 and AdamW optimizer with amsgrad. Five-fold cross-validation with 10 runs on each fold was performed. The best performing model was selected based on validation performance for in-hospital mortality and length of stay. Mean and standard error of evaluation measures in the test set were reported. The proposed method was compared with binary masking with time interval indicators and regular LSTM. The neural models used in the study included a Self-Attentive LSTM with specific parameters such as dimension (d T E ) of 32 and maximum time of 48 hours. The models were connected to a 3-layer Multi-Layer Perceptron (MLP) with specific neuron configurations. Self Attention was implemented with uni-directional LSTMs, with attention size (d a ) of 32 and number of attentions (r) set to 8. The MIMIC III data used in the study consisted of multivariated series, and Time Embeddings (TEs) were not combined. The study proposed using Time Embeddings (TEs) in two ways with neural models, including a Self-Attentive LSTM. Results showed that TEs improved model performance for in-hospital mortality and length of stay tasks, surpassing vanilla LSTM performance. The study introduced Time Embeddings (TEs) to enhance model performance for in-hospital mortality and length of stay tasks. TEs improved LSTM error rates, especially with binary masking. The method also showed enhancements in non-recurrent models like linear regression and MLP, indicating its effectiveness in representing time in irregularly sampled time series data. The study introduced Time Embeddings (TEs) to improve model performance for irregular time series data. TEs showed enhancements in various models tested, including recurrent neural networks and classic machine learning methods. Despite being outperformed by binary masking in some tests, TEs are still considered a viable option, especially for very irregular time series and high dimensional data. The method has a promising future and is expected to be extended to improve other types of irregular time-continuous data. The code for TEs results will be publicly available in the future."
}