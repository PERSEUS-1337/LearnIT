{
    "title": "SJ1fQYlCZ",
    "content": "In machine learning, Curriculum learning and Self-paced learning involve organizing training samples by difficulty levels. A study found that adding samples randomly can also yield good results. Comparisons with traditional training methods show that the proposed random method performs better and is as effective as Curriculum and Self-paced learning. This suggests a new training approach that eliminates the need for determining difficulty levels. Curriculum learning involves organizing training samples by difficulty levels to optimize non-convex objectives. Various studies have proposed different curriculum strategies for language model adaptation and computer vision tasks. While these methods show improved generalization performance, it is uncertain if they guarantee the best curriculum. A work-specific curriculum may not be applicable to other tasks. A method suggested by BID10 allows the learner to determine the difficulty of samples at each stage, making it adaptable to different applications. Self paced learning, combined with Curriculum learning by BID9, allows the learner to choose easy or difficult samples. BID7 introduced automatic syllabus selection for neural networks. BID12 suggested learning simple tasks before complex ones for better results. Adding noise to sample ordering can improve learning performance. BID8 prioritized easy and diverse samples over conventional methods. Emphasizing uncertain samples by BID3 improves SGD training accuracy. BID0 explored variations of Self paced learning for slightly better performance. In this study, the researchers explored the impact of sorting samples in curriculum and anti-curriculum learning strategies. They found that adding new samples in stages without a specific order can lead to better results. Experimenting with different ordering types, they concluded that starting with a small training set and gradually adding samples improved the methods. The study compared a new method of adding samples without a specific order to Curriculum learning and Self-paced learning strategies. Curriculum learning involves following a predetermined curriculum during training, while Self-paced learning allows the network to determine sample difficulty levels. The methods were compared using paired T-test, with results showing the effectiveness of the new method without ordering. In this study, an ensemble method was used to automatically determine the difficulty of samples. The training set was grouped for Curriculum learning and Anti-curriculum learning based on difficulty levels. Curriculum learning involves sorting samples and dividing them into stages, while Anti-curriculum learning starts from the hardest group. The training process involves sorting the training set by difficulty and dividing it into groups for Curriculum learning. Starting from the hardest group, training progresses by adding groups one by one until the easiest samples are included in the final stage. The process is outlined in Algorithm 1, with Curriculum learning starting from random weights in the first stage. Anti-curriculum learning follows a similar process but in reverse order. Curriculum learning involves starting with random weights and updating them at each stage based on the minimum found in the previous stage. Self-paced learning helps determine difficulty levels in this process. Training begins with random samples, with easy labeled examples used to update weights in subsequent steps. The network is trained with more examples in each step, with all training set given in the final stage. This is achieved by adjusting the self-paced parameter. In Self-paced learning, the number of samples added at each stage is equal to Curriculum Learning. Samples are sorted by loss at each stage, with consistent samples chosen for the next stage. For SPL-Inversed, outlying samples are prioritized. Training starts with random samples, and incompatibility with the model space is considered for selecting samples in the next stage. In Self-paced learning, samples are selected based on loss at each stage, with consistent samples chosen for the next stage. The algorithm for SPL involves randomly ordering the training set, determining the number of stages and samples to add, and using a non-convex objective function for training. In Self-paced learning, samples are selected based on loss at each stage, with consistent samples chosen for the next stage. The SPL algorithm involves randomly initializing parameters and increasing the number of training samples at each step. The common point between SPL and its inverse versions is the incremental addition of new groups for better performance. Training with accumulating groups rather than in a meaningful order is important for optimization. In Random ordered growing sets (ROGS) method, the unsorted training set is divided into groups with equal samples. Training starts with the first group and adds a new group at each stage without a specific order. The algorithm uses solutions from previous stages as initial weights. The examined methods aim to find optimum weights using DISPLAYFORM0 objective. Different methods use varying strategies for selecting samples, such as pre-sorted sets or non-ordered sets. The loss function is defined with parameters and local minimums are identified. Geometric representations are used to visualize the optimization process. The point \u03b8 B is a definite local minimum with a skewed distribution of derivatives in the loss function. More than half of the instances are less complex than average, as shown in the geometric representation. The point \u03b8 B is a local minimum with a skewed distribution of derivatives in the loss function. For more than half of the instances, the derivatives are less than average complexity. Taking a subset with k instances from the training set, the expected value for the derivative of the loss function at \u03b8 B is negative. This probability is high and always negative for E[ s (\u03b8 B )]. When training with batch gradient descent on a subset with k instances, the expected value for the derivative of the loss function at \u03b8 B is less than zero. When training with batch gradient descent on a subset with k instances, a better local minimum is obtained for that subset compared to training with all samples. Continuing optimization with all samples provides a better local minimum for the entire training set. When training with batch gradient descent on a subset with k instances, a better local minimum is obtained for that subset compared to training with all samples. Continuing optimization with all samples provides a better local minimum for the entire training set. Optimization on different subsets of the surface (\u03b8) can lead to different stopping points, with the possibility of avoiding worse points and obtaining better local minima through training with growing sets. To optimize the surface of the whole training set, different subsets of the surface can provide better local minima. A 3-layer artificial neural network with 10 neurons in the hidden layer was trained using stochastic gradient descent with momentum. The network was tested on 36 data sets from the UCI repository, with sample sizes ranging from 57 to 20000. Each data set was divided into 5 folds for training. Results of the comparisons between different learning methods and Baseline are presented in TAB1. The comparisons were made using 20 error rates obtained with 4x5 fold cross validation on various data sets. The results show the win/tie/loss information for each method against Baseline, with details on the number of samples, features, classes, and average Baseline MSE provided in Appendix B. Training with ROGS method outperforms Baseline in 17 data sets, showing optimization by using growing subsets. SPLI method is the best with wins in more than half of the data sets. CL and SPL methods show robustness in noisy data sets, prioritizing easy examples for safer training. In experiments comparing different training methods, it was found that prioritizing easy examples for safer training led to better performance. Instead of ordering samples by difficulty, adding samples randomly at each stage yielded similar results with various learning approaches. The success of Curriculum learning and Self-paced learning approaches was attributed to training with growing training sets, rather than following a specific order. In experiments, prioritizing easy examples for safer training led to better performance. Implementation results showed that both easy-to-hard and hard-to-easy ordered methods can be successful. Ordering of samples is not crucial for optimization, focusing on shortening the distance between local minima is more beneficial. Training with growing sets can overcome saddle points and find better minima in high dimensional functions. Ensemble method used to determine sample difficulty for curriculum learning resulted in faster neural network training than SPL. ACL and SPLI performed poorly in high error rated data sets. Noisy examples given at the beginning may have a greater impact on output. Inverse versions of CL and SPL methods showed better performance. The study compared the performance of different methods in handling noisy and big data sets. SPLI method showed the most success against Baseline, resembling pool-based active learning. The necessity of valuable-example-based curriculum over easiness-based-curriculum was highlighted for future work. Previous experiments focused on specific domains, while this study demonstrated the methods' performance across various data sets. The study proposed a new method that does not require determining difficulty levels like CL and SPL. By analyzing weight changes during training iterations, a feature was identified that helps avoid local minimums. Adding new groups at each stage allows for larger steps from the previous minimum. The method outperformed baseline in reaching the minimum faster when using the entire training set. The new method outperformed the baseline by avoiding local minimums through adding new groups at each stage, allowing for larger steps towards the minimum. Training with growing sets without ordering can also lead to better minimums."
}