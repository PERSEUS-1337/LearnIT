{
    "title": "S1CChZ-CZ",
    "content": "We introduce Active Question Answering, framing QA as a Reinforcement Learning task. An agent learns to reformulate questions to improve answers by probing a black box QA system with multiple question variations. The system is trained end-to-end using policy gradient to maximize answer quality and outperforms state-of-the-art models on the SearchQA dataset. The agent learns non-trivial question reformulation strategies resembling information retrieval techniques. Users' information seeking activities evolve with web and social media sources. They seek direct answers to complex questions, requiring critical assessment and synthesis. Natural language offers various ways to formulate questions in the face of uncertainty. The agent AQA learns to reformulate questions and aggregate responses to maximize the chance of getting correct answers from a backend QA system. It uses a sequence-to-sequence model trained with reinforcement learning to probe the black-box environment effectively. AQA combines reinforcement learning with a convolutional neural network to improve question reformulation and answer selection. It outperforms BiDAF by 11.4% in F1 score and other query reformulation benchmarks. The AQA agent outperforms other query reformulation benchmarks by adapting its language to improve responses in machine-machine communication. Through qualitative analysis, it is found that the agent's question reformulations differ from natural language paraphrases but can learn transparent policies such as term re-weighting and morphological simplification. This ability may be due to the focus on relevance in current machine comprehension tasks. The curr_chunk discusses the use of paraphrasing in natural language processing, specifically in question reformulation. Various methods such as dependency parsing trees and neural translation models have been explored. The approach presented is a direct neural paraphrasing system, different from previous work that used pivoting through a second language. The goal is to generate full question reformulations while optimizing end-to-end performance metrics. Reinforcement learning is utilized in natural language understanding for various tasks, such as learning control policies for games and dialogue generation. Policy gradient methods are used to address limitations in word-level optimization, allowing for the use of sequence-level reward functions like BLEU. RL training helps prevent exposure bias by optimizing the agent through policy gradient methods. Our work utilizes policy gradient to optimize our agent for end-to-end question answering quality. Previous works include training a semantic parser for querying a knowledge base and query reduction networks for multi-hop common sense reasoning. Our approach differs by generating complete sequence reformulations for question-answering tasks. Active QA is related to fact-checking research, where database queries are perturbed to estimate support for claims. In Active QA, questions are semantically perturbed to improve answers. The AQA model interacts with a black-box environment by sending multiple question variants to find the best answer. The AQA model generates reformulations of questions and sends them to an environment for answers. The selection model then chooses the best answer from the candidates provided by a competitive neural question answering model, BiDAF. The environment returns answers based on token-level F1 score as a reward, and the agent has no access to its parameters. This setup allows interaction with various information sources like images and structured data. The reformulator model is a sequence-to-sequence model that reformulates utterances in the same language. Training relies on pre-training for multilingual translation and using signals from the environment for adaptation. Access to rewards for answers during training helps in addressing the lack of high-quality training data for monolingual paraphrasing. The selection model predicts the best answer by comparing F1 scores of variants. Pre-trained embeddings are used for query, rewrite, and answer tokens. A CNN and max-pooling are applied, followed by a feed-forward network. The model is trained on the QA task, with BiDAF as a fixed environment. Training both agent and environment jointly could improve performance, but the focus is on the agent learning to communicate in natural language. The aim is for the agent to learn to communicate using natural language with an environment it has no control over. The agent returns the best answer for a given question by maximizing a reward, typically the token level F1 score. The question is generated by a policy, and the goal is to maximize the expected reward of the answer under the policy. The policy is a sequence-to-sequence model that assigns probabilities to possible questions. The reward is optimized directly with respect to the policy's parameters. The policy is optimized for reward using Policy Gradient methods with Monte Carlo sampling. Gradients for training are computed using REINFORCE to reduce variance. Entropy regularization is used to prevent collapse onto sub-optimal policies. The final objective includes a regularization weight \u03bb. Training involves beam search or sampling for question rewrites in the reformulation system. The policy is optimized for reward using Policy Gradient methods with Monte Carlo sampling. Gradients for training are computed using REINFORCE to reduce variance. Entropy regularization is used to prevent collapse onto sub-optimal policies. The final objective includes a regularization weight \u03bb. Training involves beam search or sampling for question rewrites in the reformulation system. Many rewrites of a single question are issued to the QA environment, and a neural network is trained to pick the best answer from the candidates based on F1 scores. FFNNs, LSTMs, and CNNs were evaluated, with CNN chosen for computational efficiency and accuracy. The policy is pre-trained with a paraphrasing Neural MT model. A multilingual translation system was developed using a Neural MT model that can translate between several languages by adding special tokens to indicate source and target languages. This model can perform zero-shot translation and learn a single encoder and decoder for multiple languages. This allows for translation between language pairs for which it has not been trained. The multilingual translation system uses a Neural MT model to translate between languages by adding tokens for source and target languages. Zero-shot translation is possible, but bridging through a pivot language may improve performance. Training on multilingual data followed by monolingual data helps close the performance gap. BiDAF is trained on SearchQA data, selecting answer spans from context snippets limited to the top 10 for efficiency. The model was trained using the Adam optimizer for 4500 steps with a learning rate of 0.001 and batch size of 60. Pre-training of the reformulator was done using the multilingual United Nations Parallel Corpus v1.0 BID34, containing 11.4M sentences aligned across six UN languages. A multilingual training corpus of 30 language pairs was created, yielding 340M training examples for the zero-shot neural MT system BID11. Tokenization was done using 16k sentence pieces, and a bidirectional LSTM encoder and 4-layer stacked LSTM decoder with attention were used. The model trained on 400M instances with the Adam optimizer and a learning rate of 0.001 and batch size of 128, but resulted in poor quality. The model trained with a learning rate of 0.001 and batch size of 128 had poor quality. To improve, training was resumed on a smaller dataset from the Paralex database, filtering out noisy pairs with a Jaccard coefficient above 0.5. After processing, 1.5M pairs remained out of the original 35M. The refined model showed better quality, generating improved paraphrases. After training on a smaller dataset from the Paralex database, filtering out noisy pairs, the model showed improved quality. The reformulator was pre-trained and optimized with SGD for 100k RL steps, using an entropy regularization weight of \u03bb = 0.001. The selection model used supervised learning to generate 2M (question, rewrite, answer) triples for the SearchQA dataset. The training phase focused on latency, running inference and updates on CPU and the BiDAF environment on GPU. The selection model generated 2M (question, rewrite, answer) triples for the SearchQA dataset using pre-trained embeddings. A CNN-based model encoded the strings into vectors for binary output indicating performance relative to other reformulations. The training data was used thrice for BiDAF model training, reinforcement-learning tuning, and selector training to prevent overfitting. After training the selector, the generalization gap between training and validation set errors for BiDAF and AQA-Full remains around 3.4 F1 and 3.9 F1 respectively. Training AQA on BiDAF's dataset shows minimal additional overfitting. The final model is evaluated using the test set. Results are compared against a modified pointer network (ASR) and the original BiDAF model without the reformulator. Additionally, a system (MI-SubQuery) is implemented to generate reformulation candidates for evaluation. The MI-SubQuery system selects top N subqueries based on mutual information to generate reformulations for evaluation. The system is compared against a monolingual NMT system and human performance on SearchQA. Various variants of AQA are evaluated using reformulations for each query in the evaluation. The AQA system generates query reformulations for evaluation, with N=20 reformulations per query. Different AQA variants like TopHyp, Voting, MaxConf, and CNN are compared based on answer selection methods. Results are shown in Table 1, with metrics like exact match (EM) and F1 scores computed on token level. SearchQA is more challenging than other recent QA tasks like SQuAD for both machines and humans. BiDAF's performance drops by 40 F1 points on SearchQA compared to SQuAD, but it still outperforms the Attention Sum Reader network by 13.7 F1 points. Using the top hypothesis improves F1 score by 2.2 on the test set. Heuristic selection methods like Voting and Max Conf further boost performance by leveraging BiDAF's confidence in its answers. The study shows that a trained selection function improves performance significantly, closing the gap between BiDAF and human performance. The benchmarks also improve when generating multiple candidates and using a dedicated CNN selector. The AQA CNN system outperforms other methods by about 3%. Additionally, selecting the answer with the highest F1 score from available reformulations shows potential for maximum performance. The AQA agent can learn various sub-optimal policies, such as converging to deterministic policies by emitting the same meaningless reformulation for any input question. This can lead to local optima due to strong priors in the environment, but entropy regularization can address this issue. Another sub-optimal strategy is generating minimal changes to the input to stay close to the original question, which can be successful in achieving competitive performance. The AQA agent can learn non-trivial reformulation policies that differ significantly from sub-optimal strategies. The agent's language evolves during training via policy gradient, analyzing input questions and reformulations on the SearchQA dataset. The original Jeopardy! clues have been preprocessed for the sequence-to-sequence reformulation model. The sources for the sequence-to-sequence reformulation model resemble keyword-based search queries. SearchQA questions have an average of 9.6 words with low term repetition. Query Clarity (QC) is computed as a measure of query performance. Base-NMT rewrites differ from the top hypothesis generated by the pre-trained NMT reformulation system. The Base-NMT reformulation system improves structural language quality by reinserting dropped function words and wh-phrases, resulting in more fluent rewrites that are syntactically well-formed questions. However, there is a decrease in query clarity and lower DF terms possibly due to a domain mismatch. The AQA-QR generates the top hypothesis after policy gradient training. The AQA-QR rewrites outperform the original SearchQA queries by 2% on the test set. The top hypothesis starts with the prefix \"What is name\" in 99.8% of cases, suggesting a focus on named entities in answers. The AQA-QR rewrites show improvements over SearchQA queries, with a focus on named entities in answers. The rewrites are less fluent but contain more repeated terms and informative context. Morphological variants are present in 12.5% of cases, with a preference for singular forms over plurals for better matching with context. The study evaluates the paraphrasing abilities of a model in relation to QA quality by testing 3 variants of a reformulator: Base-NMT, Base-NMT-NoParalex, and Base-NMT+Quora. The models are assessed on the MSCOCO validation set using beam search to compute the top results. The study evaluates the paraphrasing abilities of a model in relation to QA quality by testing 3 variants of a reformulator: Base-NMT, Base-NMT-NoParalex, and Base-NMT+Quora. The Base-NMT model performs at 11.4 BLEU, while Base-NMT-NoParalex performs poorly at 5.0 BLEU. Training on additional monolingual data improves BLEU score slightly to 11.6, but adding Quora training does not have a significant effect on QA performance. The study evaluates the paraphrasing abilities of a model in relation to QA quality by testing 3 variants of a reformulator. Training on additional monolingual data improves BLEU score slightly, but adding Quora training does not have a significant effect. AQA-QR reformulator has a BLEU score of 8.6, below Base-NMT models, yet outperforms others in the QA task. Training the agent starting from Base-NMT+Quora model yields comparable results. BID13 trained chatbots that negotiate via language utterances, finding that language diverges without incentive for fluency in the reward function. The study evaluates the paraphrasing abilities of a model in relation to QA quality. AQA learns to re-weight terms and modify surface forms, adapting to deep QA architectures. It can generate semantically nonsensical surface term variants. Repetitions can directly increase the model's performance. The study evaluates how AQA learns to ask BiDAF questions by optimizing language for better ranking of candidate answers. It suggests that current machine comprehension tasks focus on pattern matching and relevance modeling, resembling document retrieval systems. This may lead to the implementation of sophisticated ranking systems trained to sort text snippets. The study introduces a new framework called active question answering (AQA) to enhance question answering by perturbing input questions. It consists of a question reformulator, a black box QA system, and a candidate answer aggregator. Experimental results show the effectiveness of the approach in improving answer quality. Future work will focus on developing AQA further and exploring the sequential aspects of information seeking tasks using end-to-end reinforcement learning. The study introduces active question answering (AQA) to enhance question answering by perturbing input questions. It includes a question reformulator, a black box QA system, and a candidate answer aggregator. Experimental results demonstrate the approach's effectiveness in improving answer quality. Future work will focus on developing AQA further and exploring the sequential aspects of information seeking tasks using end-to-end reinforcement learning. The curr_chunk focuses on information seeking tasks framed as end-to-end RL problems, closing the loop between the reformulator and the selector."
}