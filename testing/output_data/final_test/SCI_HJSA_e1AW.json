{
    "title": "HJSA_e1AW",
    "content": "In this work, the normalized direction-preserving Adam (ND-Adam) algorithm is proposed to address issues with updating weight vectors in deep neural networks. ND-Adam aims to improve generalization performance by controlling update direction and step size more precisely, bridging the gap between Adam and SGD. In classification tasks, generalization performance is enhanced by regularizing softmax logits. Optimization algorithms like Adam and SGD play a crucial role in improving generalization. Various adaptive variants of SGD, such as Adagrad, Adadelta, RMSprop, and Adam, have been developed to automatically adjust the learning rate based on parameter statistics. This work aims to bridge the gap between SGD and Adam in terms of generalization performance by identifying problems in Adam and showing how they are partially avoided by using SGD with L2 weight decay. The text discusses the problems with Adam optimization and how they are addressed by using SGD with L2 weight decay. It highlights differences in update mechanisms and the impact on generalization. The proposed solution is the normalized direction-preserving Adam (NDAdam) algorithm, which aims to improve generalization performance. The text discusses the issues with Adam optimization and how they are resolved by using SGD with L2 weight decay. The proposed solution is the normalized direction-preserving Adam (ND-Adam) algorithm, which aims to enhance generalization performance by adapting the learning rate to each weight vector and explicitly normalizing each weight vector. The proposed methods, ND-Adam and regularized softmax, improve the generalization performance of Adam by enabling more precise control over parameter updates, learning rates, and learning signals. Regularization techniques like batch normalization and L2-regularization applied to the logits further enhance classification tasks. Adam is a method that maintains a running average of the first and second moments of the gradient for n trainable parameters. It corrects biases towards zero during initial time steps and updates each parameter using a global learning rate. The magnitudes of parameter updates are invariant. Adam is known for its invariant parameter update magnitudes, but it may lead to worse generalization in DNNs compared to SGD. Recent studies suggest that the generalization power of DNNs is influenced by the training algorithm, with simple SGD often outperforming adaptive methods like Adam due to different update directions for hidden units. The direction missing problem arises in Adam due to individually adapted learning rates, impacting the convergence of DNNs. Even without batch normalization, ill-conditioning of parameterization can affect networks using linear rectifiers as activation functions. L2 weight decay is a regularization technique used with SGD in DNNs to improve generalization performance. It is viewed as a form of weight normalization, rather than introducing a Gaussian prior on weights. L2 weight decay in neural networks acts as weight normalization, affecting generalization performance. It stabilizes the input weights of hidden units during training by forming a negative feedback loop. The gradient magnitude of the L2 penalty is proportional to the square of the weights, leading to a stable equilibrium value. When L2 weight decay is applied in neural networks, it normalizes the weights and stabilizes the direction of weight changes during training, regardless of the magnitude of the weights. This helps prevent overfitting and improves generalization performance. The normalized direction-preserving Adam (ND-Adam) algorithm improves optimization by controlling the effective learning rate of input weights. It divides trainable parameters into two sets, updating them differently to enhance training. The normalized direction-preserving Adam (ND-Adam) algorithm optimizes training by controlling the learning rate of input weights efficiently. It reduces memory overhead compared to Adam and addresses direction missing and ill-conditioning problems. The algorithm jointly solves these issues and relates to other normalization schemes. The ND-Adam algorithm optimizes training by adapting the global learning rate for input weights, preserving the direction of the gradient for each vector. It extends the learning rate adaptation scheme from scalars to vectors, reducing memory overhead and addressing direction missing and ill-conditioning problems. The ND-Adam algorithm optimizes training by adapting the global learning rate for input weights, preserving the direction of the gradient for each vector. It extends the learning rate adaptation scheme from scalars to vectors, reducing memory overhead and addressing direction missing and ill-conditioning problems. The weights of an upper layer converge to the span of its input vectors, requiring lower layers to converge first. This phenomenon aligns with the manifold hypothesis, suggesting real-world data is presented in a low-dimensional manifold. The explanation is related to the manifold hypothesis, suggesting that real-world data concentrates on lower-dimensional manifolds. Activation functions like ReLU, sigmoid, and tanh are only activated by certain input vectors. Constraining input weights in a subspace encourages hidden units to form local coordinate systems on the manifold, leading to good representations. The ill-conditioning problem arises when changes in input weight magnitude can be compensated by other parameters without affecting the outcome. The ill-conditioning problem in deep neural networks can lead to inconsistent training and difficulty in control. Without proper regularization like L2 weight decay, the magnitude of weight changes can decrease, causing the network to converge to sharp minima which generalize poorly. L2 weight decay can help alleviate this issue by implicitly addressing the ill-conditioning problem. The ill-conditioning problem in deep neural networks can be alleviated by L2 weight decay, which normalizes weights. However, controlling the effective learning rate remains a challenge. To address this, a method restricts the L2-norm of each weight to 1 and optimizes its direction on a unit sphere. This approach aims to tackle the ill-conditioning problem more effectively. Spherical weight optimization normalizes weight vectors on a unit sphere to address the ill-conditioning problem in deep neural networks. By explicitly normalizing the weight vectors, updates only change their directions, maintaining a constant norm of 1. The text discusses how updates to weight vectors in spherical weight optimization only change their directions, keeping magnitudes constant. This allows for precise control over the learning rate through a single hyperparameter, \u03b1 v t. The normalization scheme maintains a desirable property of Adam BID15 by keeping the gradient normalization intact. The proposed spherical weight optimization normalizes weight vectors and projects gradients onto a unit sphere for precise learning rate control. It ensures robustness to improper weight initialization and requires an extra scaling factor for nonlinear activation functions. The activation of hidden units in weight normalization is determined by a scaling factor and bias, aiming to accelerate SGD optimization. Unlike spherical weight optimization, weight normalization does not directly normalize the weight vector of each hidden unit. Both schemes can combine weight and scaling factor into a single equivalent weight vector at inference time. Combining weight normalization and mean-only batch normalization can mitigate the issue of slow training in upper layers due to changing input distributions. In linear rectifier networks, scaling factors can be removed without altering network function. Batch normalization standardizes the input, allowing for stable weight updates in upper layers. Spherical weight optimization is combined with batch normalization for improved stability. The softmax function is commonly used for multi-class classification tasks but can lead to varying learning signals. When using cross entropy as the loss function with one-hot target vectors, the prediction is correct if the arg max of the logits matches the target class. Logits can be scaled together without changing predictions, but the cross entropy and its derivatives will vary with the scaling factor. For Adam and ND-Adam, the relative magnitudes of the gradients matter more than the absolute magnitudes. When the magnitude of the logits is small, softmax encourages the logit of the target class to increase while penalizing other classes equally. On the other hand, when the magnitude of the logits is large, softmax penalizes only the largest logit of the non-target classes. This can happen without the scaling factor, for example, by varying the norm of the weights of the softmax layer. When the magnitude of the logits is small, softmax penalizes non-target classes equally, regardless of the difference in values. However, it is more reasonable to penalize logits closer to the target class, which are more likely to cause misclassification. On the other hand, when the magnitude of the logits is large, softmax ignores other non-target classes and only penalizes the largest logit. To address this, two methods are proposed to ensure the logits are not too small or too large. One method is to apply batch normalization to the logits, setting a single hyperparameter \u03b3 C for all classes instead of trainable variables \u03b3 c. The text discusses the use of a single hyperparameter \u03b3 C for all classes in batch-normalized softmax (BN-Softmax) to find a better trade-off between extremes. Additionally, L2-regularization with a hyperparameter \u03bb C is proposed to address the issue of logits growing too large. Empirical evidence is provided for the analysis, evaluating ND-Adam and regularized softmax on CIFAR-10 and CIFAR-100 datasets. The text discusses the use of L2 weight decay in training a wide residual network (WRN) on the CIFAR-10 dataset. Modifications to the WRN architecture include replacing the last fully connected layer with a convolutional layer. The effective learning rate variations in different hyperparameter settings are shown in a figure. The text discusses the comparison of generalization performance between SGD, Adam, and ND-Adam when training a wide residual network on CIFAR-10 and CIFAR-100 datasets. ND-Adam allows for more precise control of weight updates by adjusting the learning rate for weight vectors. In experiments with Adam and ND-Adam, the learning rate is adjusted to match that of SGD. L2 weight decay has a significant impact on SGD but not on Adam. Default values for \u03b21 and \u03b22 are used for Adam and ND-Adam. A cosine learning rate schedule is found to improve performance. Test accuracies of ND-Adam surpass vanilla Adam and match SGD at convergence. Training includes data augmentation but no dropout. trainable scaling parameters of batch normalization are experimented with. The training losses of Adam drop dramatically early on, with test accuracies increasing rapidly. However, Adam tends to get stuck in bad local minima that do not generalize well. ND-Adam shows slightly better performance on CIFAR-10 but worse on CIFAR-100. Overall, ND-Adam and SGD show similar generalization performance with comparable effective learning rates. BN-Softmax improves generalization performance over Adam and ND-Adam, with consistent enhancements observed across all three algorithms. Removing scaling factors slightly boosts performance with ND-Adam but degrades it with SGD. Setting \u03b3 C to 2.5 for CIFAR-10 and 1 for CIFAR-100, BN-Softmax significantly enhances performance in this setting. In a comparison between SGD, Adam, and ND-Adam on CIFAR-10 and CIFAR-100 datasets, ND-Adam shows the best generalization performance. Differences in implementation details such as nonlinearities and skip connections impact performance. L2-regularization applied to weight vectors, scales, and biases of batch normalization in the original implementation leads to better generalization. Reimplementing ND-Adam in PyTorch for further comparison reveals subtle but important differences. In this experiment, different hyperparameter settings were used for SGD and ND-Adam. L2-regularization was applied to biases for both, with \u03bb set to 5e\u22124 for SGD and 5e\u22126 for ND-Adam. Only ND-Adam used L2-regularized softmax with \u03bb C = 0.001. The performance of SGD for WRN-28-10 was slightly better than the original implementation due to modifications described."
}