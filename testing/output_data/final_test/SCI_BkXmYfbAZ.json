{
    "title": "BkXmYfbAZ",
    "content": "Existing deep multitask learning (MTL) approaches typically align shared layers in a parallel ordering, limiting the types of shared structure that can be learned. A study comparing parallel ordering with permuted ordering of shared layers found that a flexible ordering can lead to more effective sharing. This has led to the development of a soft ordering approach in deep MTL, where shared layers are applied in different ways for different tasks. Deep MTL with soft ordering has shown superior performance compared to parallel ordering methods in various domains, indicating that the strength of deep MTL lies in learning general building blocks that can be adapted to different tasks. In multitask learning (MTL), auxiliary data sets are utilized in BID4 to enhance overall performance by leveraging common patterns across tasks. Deep learning has led to state-of-the-art systems in various domains, prompting the development of deep multitask learning techniques. These techniques have been applied in vision, natural language, speech, and reinforcement learning settings. While these approaches improve performance over single-task learning, they are typically limited to training a few closely-related tasks. However, from a Kolmogorov complexity perspective, transfer learning should always be beneficial as tasks share commonalities. Even seemingly unrelated tasks like vision and NLP can benefit from sharing without the need for an adaptor task. Deep multitask learning in deep learning models aims to improve performance by sharing learned transformations across tasks. Existing approaches are limited by the assumption that sharing occurs through parallel ordering of layers, restricting the kind of sharing that can happen between tasks. This paper investigates the necessity of parallel ordering of layers for deep multitask learning (MTL) and introduces permuted ordering as a more flexible alternative. The increased ability of permuted ordering to integrate information across tasks is analyzed, leading to the development of a soft ordering approach to deep MTL. Classical and column-based approaches to deep MTL are also discussed. The paper explores the concept of soft ordering in deep multitask learning, which allows for the application of shared layers in different ways at various depths for different tasks. This approach improves performance compared to single-task learning and fixed order deep MTL methods. MTL involves discovering generalizable modules that can be assembled in different ways for various tasks. Soft ordering enables deep MTL to capture diverse structural regularities across tasks, unlike fixed order methods. This section provides a classification of existing deep MTL approaches. The section presents a classification of deep MTL approaches based on how learned parameters are shared across tasks. Classical approaches involve adding output neurons to predict auxiliary labels for related tasks, acting as regularizers for the hidden representation. Many deep learning extensions follow a similar approach, learning a shared representation at a high-level layer, followed by task-specific decoders. In deep MTL approaches, parameters can be shared across tasks using different methods such as task-specific input encoders and column-based approaches. Column-based approaches assign each task its own layer of task-specific parameters at each shared depth, with mechanisms for parameter sharing between tasks. Negative effects of sharing in column-based methods can be due to mismatches between features required at the same depth for dissimilar tasks. Additionally, supervision at custom depths can be utilized to describe a hierarchy of tasks. Supervision at custom depths integrates supervised feedback from tasks in a hierarchy. One approach learns a task-relationship hierarchy during training, with parameters shared across matching depths. Recurrent methods reintegrate information from earlier predictions. Universal representations share all tasks. In this case, a method was applied to various vision tasks, showing the effectiveness of a small number of scaling parameters in adapting layer functionality. The core assumption behind Multi-Task Learning (MTL) is that regularities across tasks lead to learned transformations that can be shared. The method aims to leverage shared core model parameters and task-specific parameters to achieve strong performance across tasks. The core assumption of Multi-Task Learning (MTL) is that regularities across tasks result in learned transformations that can be shared to improve generalization. Methods reviewed in Section 2.1 assume that subsequences of the feature hierarchy align across tasks, with sharing occurring only at aligned depths. Tasks are learned jointly, with sharing happening at consecutive depths. Task-specific encoders and decoders are used, along with learned weights at shared depths, following the parallel ordering assumption. The assumption in Multi-Task Learning (MTL) is that weight tensors are similar and compatible for sharing at each shared depth. However, as more tasks are added, it becomes challenging for each layer to represent features for all tasks. The paper explores how parallel ordering limits the potential of deep MTL by enforcing strong constraints on layer usage. This constraint can be tested for necessity to develop more flexible methods. The baseline deep MTL model for each task is given by a task-specific permutation of layers, which challenges the necessity of parallel ordering in deep MTL. This approach allows for flexibility in layer application order, potentially improving performance. The study explores training layers in different orders for multiple tasks, leading to models with varied layer assemblies. Comparing permuted and parallel ordering on random tasks, the experiments aim to fit two tasks accurately using shared affine layers. The study compares permuted and parallel ordering of shared affine layers for fitting two tasks accurately. Results show that in both linear and nonlinear cases, permuted ordering does not lose accuracy compared to single-task models. The study compares permuted and parallel ordering of shared affine layers for fitting two tasks accurately. Results show that in both linear and nonlinear cases, permuted ordering does not lose accuracy compared to single-task models. The learned permuted layers successfully adapt to different task orderings, shedding light on the conditions that make this possible. The architecture allows flexible sharing of layers across tasks and depths, with shared weight layers and nonlinearity. The matrix trace constraint is invariant under cyclic permutations, with traces of F i equal in expectation. Task-specific scaling parameters can help adapt learned layers to specific tasks. In the context of flexible layer sharing across tasks, task-specific scaling parameters can adapt learned layers to specific tasks. This approach has been utilized in different ordering settings to reduce constraints and encourage sharing between tasks. The number of task-specific parameters remains constant regardless of the task size, promoting scalability and maximal sharing. This concept is integrated into the soft ordering approach discussed in the following section. The soft ordering approach introduces a flexible way for jointly trained models to learn layer application while learning the layers themselves. Task-specific scaling parameters are used to adapt learned layers to specific tasks, promoting scalability and sharing between tasks. This approach adds only D 2 scaling parameters per task, independent of layer size, and enforces a soft ordering via softmax. The soft ordering approach introduces a flexible way for jointly trained models to learn layer application while learning the layers themselves. It uses task-specific scaling parameters to adapt learned layers to specific tasks, promoting scalability and sharing between tasks. This formulation subsumes any fixed layer ordering and can be learned jointly with other parameters via backpropagation. Dropout is recommended after each shared layer to increase generalization capacity. Empirical tests on various datasets compare soft ordering against fixed ordering MTL and single-task learning. The experiments evaluate soft ordering in multitask learning on various tasks including MNIST, UCI, Omniglot character recognition, and facial attribute recognition. Different ordering methods are compared, such as single task, parallel ordering, permuted ordering, and soft ordering. The ability of multitask methods to exploit intuitively related tasks with disparate input representations is assessed. The experiments evaluate soft ordering in multitask learning on various tasks including MNIST, UCI, Omniglot character recognition, and facial attribute recognition. Each task aims to distinguish between two randomly selected digits using different ordering methods like parallel, permuted, and soft ordering. Results show that the flexibility of order can aid in scaling to more tasks, with soft ordering integrating information across tasks effectively. The experiment evaluates soft ordering in multitask learning on diverse tasks from popular UCI classification datasets. Results show that soft ordering significantly outperforms parallel and permuted methods in integrating information across seemingly unrelated tasks. The model eventually exploits regularities in disparate domains with flexible layer ordering. The Omniglot dataset consists of fifty alphabets for character recognition tasks, making it a useful benchmark for multitask learning. It allows analysis of performance based on the number of tasks trained jointly and evaluates the ability of soft ordering to compose layers differently for various tasks. Omniglot is also used to test deep generative models due to its inherent composibility. The experiment evaluates soft ordering of convolutional layers in models BID38 for multitask learning on the Omniglot dataset. Results show that soft ordering consistently outperforms other deep MTL approaches, regardless of the number of tasks or amount of training data. The experiment evaluates soft ordering of convolutional layers in models for multitask learning on the Omniglot dataset. Soft ordering significantly outperforms single task and fixed ordering approaches, showing improvement in training data without being affected by task or model complexity. Permuted ordering performs worse than parallel ordering due to the common feature hierarchy induced by deep vision systems. Soft ordering allows for the discovery of different uses for each layer at varying depths. Soft ordering of convolutional layers significantly outperforms single task and fixed ordering approaches in multitask learning on the Omniglot dataset. The usage of each layer is correlated with depth, revealing an innate hierarchy in convolutional networks that soft ordering can discover. This approach scales well to real-world tasks requiring specialized components like convolutional layers. The experiment investigates how a soft ordering approach can exploit relationships between high-level concepts in a shared hierarchy, as seen in the behavior of convolutional layers in different contexts. The CelebA dataset contains 200K color images with binary labels for 40 facial attributes. A ResNet-50 model is used for parallel and soft order models, with shared embedding layers and unshared dense sigmoid layers. The robustness of learning is tested with and without an additional facial landmark detection task. Soft order models are also tested with and without a fixed identity layer. The inclusion of a fixed identity layer in soft order models can improve consistency of representation across contexts. Results show that a soft ordering model outperformed a parallel ordering model in a deep learning system, achieving a test error of 8.79. This demonstrates the value of soft ordering in enhancing performance. Including landmark detection yielded a marginal improvement to 8.75, while parallel ordering slightly degraded performance, indicating that soft ordering is more robust. The identity layer improved performance to 8.64, with increased consistency of representation across contexts. Soft order models achieve a significant improvement over parallel ordering, boosted by including the identity layer. The addition of landmark detection showed a slight improvement in performance, but the inclusion of the identity layer slightly diminished this improvement. The flexibility provided by the identity layer may offset the regularization from landmark detection. Previous studies have shown that adaptive weighting of task loss, data augmentation, ensembling, and a larger vision model can also lead to significant improvements. These enhancements are expected to complement the benefits of soft ordering. Further improvements can be achieved by combining these methods with soft ordering. The models learn a \"soft hierarchy\" of layer usage, with certain layers being used more at higher depths. The success of soft layer ordering suggests that layers learn functional primitives with similar effects in different contexts. An experiment using generative visual tasks is conducted to explore this idea. The experiment involved using four shared dense ReLU layers of 100 units each, with a linear encoder shared across tasks and a global average pooling decoder. Task models are distinguished by learned soft ordering scaling parameters. Results show that layers learn functional primitives, with similar effects observed across different contexts. The soft ordering approach in this paper is a small step away from the parallel ordering assumption. It can be extended to more general structures and applied to training and understanding functional building blocks. The soft-ordering architecture is a new type of recurrent architecture designed for Multi-Task Learning, with different scaling parameters applied at different depths for different tasks. Soft ordering induces a type of recurrence that does not require sequential task input or output. Recurrent methods can reduce the size of S below O(T D 2) and can be extended to shared layers with internal recurrence, such as LSTMs. Generalizing the structure of shared layers is important for modern deep architectures with varying layer structures. Soft ordering requires a higher-level recurrence and can be applied to any deep architecture with many layers. Soft ordering in deep architectures involves reshaping inputs to maintain the same shape at each depth. This can be extended to soft ordering over modules with more general structure, enhancing modularity. Shared trained layers in permuted and soft ordering exhibit more general functionality than fixed location layers, benefiting different tasks. Shared trained layers in permuted and soft ordering exhibit more general functionality than fixed location layers, benefiting different tasks by allowing models to learn how to apply building blocks for new tasks without further training. This approach aligns well with real-world complexity and the reuse of functional primitives. The paper discusses the benefits of using shared layers in permuted and soft ordering for different tasks, allowing models to learn how to apply building blocks without additional training. Soft ordering outperforms parallel ordering in integrating information across diverse tasks. Deep multitask learning can be improved by using soft ordering instead of parallel ordering methods. Experiments conducted with the Keras deep learning framework and Tensorflow backend show that processing all tasks simultaneously simplifies training and leads to faster convergence. Shared encoders ensure that inputs in each batch are the same across tasks. In experiments with deep multitask learning, soft ordering outperformed parallel ordering methods. Cross-entropy loss was used for classification tasks, with validation loss calculated as the sum of per task losses. Different ordering methods trained equivalent core layers. Input pixel values were normalized between 0 and 1, and a dropout rate of 0.5 was applied. Each setup was trained for 20K iterations with 64 samples per batch for each task. Digits were randomly selected without replacement to define tasks. In experiments with deep multitask learning, soft ordering outperformed parallel ordering methods. Digits were selected without replacement to define tasks, with 45 possible tasks. Input features were scaled between 0 and 1, and a dropout rate of 0.8 was applied. Models were designed with four core layers, each a 2D convolutional layer with ReLU activation and kernel size 3 \u00d7 3. The model consisted of four core layers, each with a 2D convolutional layer using ReLU activation and a 3x3 kernel size, followed by a 2x2 maxpooling layer. The number of filters for each layer was set at 53, with a dropout rate of 0.5 applied after each core layer. The input was zero-padded to a shape of 105x105x53 to match the shared layers. Tasks were evaluated by training the first k tasks in a random ordering for 5000 iterations. The first k tasks were trained jointly for 5000 iterations, with each batch containing k random samples from a fixed ordering of tasks. The dataset used for training, validation, and testing contained \u2248160K images for training, \u224820K for validation, and \u224820K for testing, with 20 images of each of approximately \u224810K celebrities. ResNet-50 weights were initialized with pre-trained imagenet weights, and image preprocessing was done. The Keras framework was used with pre-trained imagenet weights for image preprocessing. Facial landmark detection output is a 10-dimensional vector normalized between 0 and 1. Mean squared error was the training loss, with a dropout rate of 0.5. Models were trained with RMSProp after validation loss convergence via Adam. The predictions for a fixed model were generated at each pixel location, denormalized, and mapped back to the pixel coordinate space using a mean squared error loss. Task models are distinguished by their learned soft ordering scaling parameters, and the joint model acts as a generative model. The behavior of a specific layer at depth for a task was visualized by sweeping across different values. The softmax activation in each task's depth was replaced with a sigmoid. Changing a single layer's weight at depth four had no noticeable impact due to the global average pooling decoder."
}