{
    "title": "SJg3T2EFvr",
    "content": "In this work, simple methods for generating static lookup table embeddings from pretrained contextual representations are introduced. These static embeddings outperform Word2Vec and GloVe on word similarity and relatedness tasks, providing insights for downstream tasks using these embeddings or the original contextual models. In this study, the authors introduce methods for generating static embeddings from pretrained contextual models, which outperform Word2Vec and GloVe on word similarity tasks. They also demonstrate the potential for analyzing social bias in word embeddings, highlighting inconsistencies in current techniques. The code and distilled word embeddings are publicly released to support reproducible research in the NLP community. Recent innovations in NLP have focused on enhancing embedding quality by incorporating syntax, morphology, subwords, subcharacters, and context. Pretrained word representations have become widely used in NLP tasks, showcasing the success of representation and transfer learning. These representations can be categorized into static and dynamic methods, with static methods like Word2Vec, GloVe, and FastText providing fixed representations post-training. Dynamic methods like CoVe, ELMo, and BERT produce word representations based on the context in which the word appears, unlike static methods such as Word2Vec and GloVe which provide fixed representations post-training. Static embeddings have advantages over dynamic embeddings in terms of speed, computational resources, and ease of use. This dichotomy has led to a proposal for a mechanism to convert dynamic representations to static representations, yielding higher quality static embeddings from pretrained contextual models like BERT, GPT-2, RoBERTa, XLNet, and DistilBERT. The proposal suggests converting dynamic embeddings from models like BERT, GPT-2, RoBERTa, XLNet, and DistilBERT to higher quality static embeddings. This approach allows for the generation of ever-improving static embeddings that can track advances in contextual word representations. Additionally, distilling static embeddings from dynamic ones enables the use of a wider range of analysis tools to better understand the original contextual embeddings, including identifying biases related to gender, race, and religion. In this study, methods are employed to identify gender, racial, and religious bias in distilled representations. The evaluation reveals inconsistencies in existing measures of social bias and discrepancies in bias estimates for different pretrained models and layers. The focus is on comparing these embeddings against widely used static embeddings like Word2Vec and GloVe. Word2Vec and GloVe are prominent static embeddings used due to their high-quality representations and easy accessibility. BERT is the most prominent contextual model considered, along with other alternatives like ELMo, GPT, Transformer-XL, XLNet, RoBERTa, and DistilBERT. Results are primarily reported for the bert-base-uncased model, with complete results for the bert-large-uncased model in the appendices. The BERT model utilizes subword pooling and context combination operations to compute a single representation for a word in different contexts. The tokenization process involves word-level tokenization and potentially breaking down words into subwords using the WordPiece algorithm. The BERT model uses subword tokenization with WordPiece algorithm to compute word representations across contexts. Different pooling mechanisms are considered to compute vectors, and contextual representations can be converted into static ones using specified contexts and combining resulting representations. The BERT model uses subword tokenization with WordPiece algorithm to compute word representations across contexts. To improve representational quality, multiple contexts are sampled from a large corpus, and a pooling strategy is applied to aggregate the representations. The static embeddings are evaluated on word similarity and relatedness datasets. The BERT model utilizes subword tokenization with WordPiece algorithm for word representations. Static embeddings are evaluated on word similarity datasets like RG65, WS353, SIMLEX999, and SIMVERB3500. The evaluation is based on the correctness of ranking similarity/relatedness using Spearman \u03c1 coefficient. The impact of choices in f and g 2 on embedding performance from bert-base-uncased is studied. The performance of static embeddings using the Aggregated strategy, particularly with g = mean pooling, outperforms the Decontextualized strategy. Mean pooling at the subword level also shows the best performance. The trend of f = mean, g = mean being optimal holds for most pretrained contextual models. Performance tends to increase with the number of total contexts for all words (N) for bert-base-uncased and bert-large-uncased, as well as other pretrained models. In the largest setting with N = 1000000, bert-large-uncased embeddings from the best performing layer outperform Word2Vec and GloVe. There is a preference for the first quarter of the model's layers, with a drop-off in performance thereafter. In the evaluation of lexical semantic understanding, the best-performing layer in a pretrained model shifts towards later layers as more contexts are considered. This variance in performance among layers may impact downstream use, as later layers are typically preferred by practitioners. Combining stable static embeddings from an early layer with contextual example-specific embeddings from a later layer is suggested for improved performance. The results suggest potential benefits in studying the integration of static and dynamic methods, with a preference towards later layers for larger values of N. The correlation between dataset difficulty and layer-wise optimality is intriguing, particularly in datasets ordered chronologically. The SIMVERB3500 dataset yields the best performance for embeddings from intermediary layers, with tendencies that generalize to other pretrained models. Results show that models with the same distillation procedure can have radically different performance in distilled embeddings. The results suggest that different pretrained models excel in various aspects of lexical semantics and context representation. It is important to consider multiple pretrained models in analyses as performance may not generalize across all models with similar Transformer architectures. DistilBert-6 outperforms BERT-12 on three out of four datasets despite being distilled from it. RoBERTa does not consistently outperform BERT models in static embeddings comparison. In this work, the performance of static embeddings from different pretrained models on word similarity and relatedness tasks is evaluated. The study also delves into the social bias encoded within static word representations, aiming for a more generalizable understanding of bias in machine learning and natural language processing. In this study, multiple axes of social bias (gender, race, religion) and computational methods for quantifying bias are considered. Existing NLP literature has focused on gender bias, but different computational specifications yield varying results. Caution is advised in interpreting bias results based on the definitions used. Low bias scores in embeddings do not necessarily indicate unbiasedness, suggesting the need for more nuanced definitions. Bolukbasi et al. (2016) introduced a gender bias computation definition. The study considers multiple axes of social bias and computational methods for quantifying bias. Existing NLP literature has mainly focused on gender bias, but caution is advised in interpreting bias results. Bolukbasi et al. (2016) introduced a gender bias computation definition that relies on a set of (male, female) word pairs to compute a gender direction. This definition is challenging to apply beyond gender and difficult to generalize to multi-class settings. The study introduces a different definition for computing binary bias that is not limited to gender, using representative words for each protected class. This definition is more general and does not rely on the first principal component explaining a large fraction of the variance. The study introduces a new definition for quantifying multi-class bias, utilizing representative words for each protected class. This definition is more general and does not depend on the first principal component explaining a large fraction of the variance. The study transparently reports social bias in existing static embeddings and those computed, analyzing bias for gender, race, and religion across different pretrained models. Results for binary gender, two-class religion, and three-class race are exhaustively reported, following previous research. The study analyzes social bias in pretrained models for gender, race, and religion. Results show variation in bias across layers and disagreement in bias measures for the same social attribute. The analysis does not include intersectional biases. The study examines bias in pretrained models for gender, race, and religion. There is variation in bias across layers and disagreement in bias measures for the same social attribute. The analysis does not consider intersectional biases. The analysis highlights the inconsistency in bias quantification measures for word embeddings, showing that bias scores are closely tied to the definition used. This has implications for interpreting debiasing procedures and understanding social biases in representations. The analysis reveals the variability in bias quantification measures for word embeddings, impacting the interpretation of debiasing procedures and social biases in representations. Comparing bias estimates across pretrained models, conflicting scores are observed, necessitating the retention of all definitions and social attributes for comparison. Target words provided by N prof are considered for clarity, with results for adjectives in Table 8. Preprocessing normalization of embeddings is not performed, making bias scores using bias GARG-EUC incomparable. The analysis highlights the variability in bias quantification measures for word embeddings, affecting the interpretation of debiasing procedures and social biases. Comparing bias estimates across pretrained models shows conflicting scores, necessitating the retention of all definitions and social attributes for comparison. Preprocessing normalization of embeddings is not done, making bias scores using bias GARG-EUC incomparable. Additionally, bias BOLUKBASI may not be reliable, and different bias definitions show varying scores for distilled static embeddings. The analysis reveals conflicting bias scores among different bias quantification measures for word embeddings, impacting the interpretation of debiasing procedures. Despite challenges in comparing bias between distilled and static embeddings, it is evident that existing bias definitions significantly influence bias scores. Akbik et al. (2019) introduced an Aggregated strategy for modeling global information in static embeddings, achieving state-of-the-art results on NER datasets. The curr_chunk discusses the use of additional features to improve performance on NER datasets, suggesting potential improvements in pretrained model and layer choices. It also compares pooling methods and mentions a different approach for converting representations into static embeddings. The curr_chunk discusses limitations in quantifying bias in sentence encoders due to using synthetically constructed templates for static embeddings. These concerns are exacerbated by poor representational behavior in Decontextualized embeddings. The curr_chunk discusses the limitations of quantifying bias in sentence encoders using the Aggregated strategy. It suggests that pooling over more semantically-meaningful contexts can provide a more faithful estimator of bias. The study contrasts its approach with Hu et al. (2019) who use a similar method for diachronic sense modeling. The curr_chunk highlights the potential for improving performance by pooling over more sentences, using bert-large-uncased, and considering layer choice for tasks relying on lexical understanding. The curr_chunk discusses the benefits of integrating pretraining in a lightweight manner for tasks relying on lexical understanding. It suggests that additional sense-annotated or weakly sense-annotated sentences would be beneficial. Model compression and knowledge distillation are well-studied techniques that have been recently applied for similar purposes. Several approaches have been proposed to yield lighter pretrained sentence encoders and contextual word representations. The curr_chunk discusses the importance of lightweight pretraining for resource-constrained settings and the environmental impact of large NLP models. It also addresses social bias evaluation in NLP through geometric similarity between embeddings. Our bias evaluation focuses on intrinsic bias in embeddings, considering gender, race, and religion, unlike prior work that primarily focused on gender. Recent studies have explored bias in sentence encoders and contextual models, such as gender bias in ELMo and BERT. Our study examines a broader range of biases than previous research, including gender bias in sentence encoders like ELMo and BERT. We emphasize the importance of reporting bias values for different layers of the model, as bias is not evenly distributed throughout model layers. Additionally, we provide insights for downstream practitioners on simple and sophisticated mechanisms to reduce bias in pretrained contextual word representations. This work introduces a new approach to understanding contextual word representations through proxy analysis, focusing on social bias. It suggests that post-processing techniques like dimensionality reduction could be valuable for improving large multi-layered networks like BERT. Additionally, the study explores various biases in sentence encoders and offers strategies to mitigate bias in pretrained contextual word representations. Future work may consider choosing the corpus D from the target domain for downstream tasks as a lightweight domain adaptation strategy. Providing sentence-length contexts can facilitate model comparison, but models like Transformer-XL or XLNet may benefit from larger contexts. Simple procedures for converting contextual word representations into static word embeddings have shown to outperform Word2Vec and GloVe when applied to pretrained models like BERT. The study demonstrates that embeddings from models like BERT outperform Word2Vec and GloVe, providing insights into pretrained models and social biases. The analysis reveals biases in popular contextual representations and questions existing bias quantification protocols. All data, code, and visualizations will be publicly released. In this work, the authors focused on intrinsic evaluation experiments for word similarity and relatedness tasks. They did not consider lexical understanding via word analogies due to concerns about validity. Results are sensitive to parameter selection, but in their setting, the impact of most parameters was exhaustively reported. The authors focused on intrinsic evaluation experiments for word similarity and relatedness tasks, not considering word analogies. Concerns about validity were addressed by exhaustively reporting the impact of most parameters. The study avoided preprocessing choices that could create discrepancies between embeddings used in intrinsic evaluation and downstream tasks. The computed embeddings from the pretrained model were used directly throughout the work. Rubenstein & Goodenough (1965) introduced a dataset of 65 noun-pairs showing strong correlation with human validation. The authors focused on intrinsic evaluation experiments for word similarity and relatedness tasks, not considering word analogies. They used PyTorch with pretrained contextual word embeddings. Various datasets were introduced to improve semantic similarity quantification, with a focus on verbs in SIMVERB3500 by Gerz et al. (2016). The authors utilized PyTorch with pretrained contextual word embeddings from the HuggingFace pytorch-transformers repository. Tokenization was done using corresponding tokenizers for each model. They introduced N as the total number of contexts used in distilling with the Aggregated strategy, filtering sentences in D containing at least one word in V. This approach limits the number of candidate sentences based on the most frequent word in V. The authors used PyTorch with pretrained contextual word embeddings from HuggingFace pytorch-transformers repository. They introduced N as the total number of contexts used in distilling with the Aggregated strategy, filtering sentences in D containing at least one word in V. The frequency of the least frequent word in V impacts the number of examples containing a word, with a fixed number of examples for each word in bias evaluation. The embeddings in the bias evaluation are drawn from layer X 4 using f = mean, g = mean as best performing embeddings. Gender-paired tuples were taken from Bolukbasi et al. (2016). Attribute sets and target sets were taken from Garg et al. (2018) to study social bias. Multi-word terms were removed from the lists. Social bias within static embeddings from different pretrained models with respect to a set of adjectives was analyzed. The analysis focused on social bias within static embeddings from various pretrained models using a set of adjectives. Parameters were set as f = mean, g = mean, N = 100000, and the layer X 4 was used in distillation."
}