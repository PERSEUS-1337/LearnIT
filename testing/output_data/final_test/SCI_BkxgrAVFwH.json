{
    "title": "BkxgrAVFwH",
    "content": "In the field of Generative Adversarial Networks (GANs), designing a stable training strategy is a challenge. Wasserstein GANs have improved stability by introducing Wasserstein distance but still face instability. A new framework called Wasserstein-Bounded GAN (WBGAN) enhances WGAN-based approaches by adding an upper-bound constraint to the Wasserstein term. WBGAN can measure distribution differences effectively, even when distributions have minimal overlap. Experimental results show that WBGAN stabilizes and speeds up convergence in training various WGAN-based models. Generative Adversarial Networks (GANs) involve a generator and discriminator competing to produce realistic data. GANs can create synthetic data in various forms like images, language, and music. However, training GAN models can be challenging due to issues like vanishing gradients and mode collapse. Researchers focus on stabilizing training procedures to improve GAN quality. Researchers aim to enhance GAN quality by stabilizing training procedures. WGAN-GP, a variant of GANs, addresses gradient disappearance by using a gradient penalty term instead of weight clipping. This method results in more stable training and success in various generating tasks. In response to the instability of WGANs, a new framework called Wasserstein-Bounded GAN (WBGAN) is proposed to improve training stability by controlling the Wasserstein term. Previous works focused on enhancing the gradient penalty term but overlooked the bottleneck of the Wasserstein term. The WBGAN strategy dynamically balances the Wasserstein loss and gradient penalty loss to ensure stable training. Wasserstein-Bounded GAN (WBGAN) aims to balance the Wasserstein loss and gradient penalty dynamically for stable training. It can be applied to various WGAN variants like WGAN-GP, WGANdiv, and WGAN-GPReal. Testing on the CelebA dataset shows improved training stability and convergence. The Wasserstein Generative Adversarial Network (WGAN) uses the Kantorovich-Rubinstein duality to formulate its objective, with a critic and generator parameterized by neural networks. WGAN enforces a 1-Lipschitz constraint on the critic using weight clipping, but this may lead to oversimplified functions and unsatisfactory results. Several studies proposed different forms of gradient penalty as a regularization term for the Wasserstein Generative Adversarial Network (WGAN). The generalized loss function includes a Wasserstein term and a gradient penalty term, posing a tradeoff between the two objectives. The Wasserstein Distance between empirical distributions is approximated using discrete optimal transport, with probabilistic couplings defined between the distributions. The Sinkhorn Algorithm approximates the Wasserstein distance to reduce computational costs for linear programming. It uses a cost matrix and entropy to find the optimal coupling between distributions. Compared to Wasserstein distance, Sinkhorn distance offers a more efficient solution. The Sinkhorn distance restricts the search space of joint probabilities for efficiency. It uses a Lagrange multiplier to compute a unique solution with lower cost than the original Wasserstein distance. In WGAN variants like WGAN-GP, the critic aims to maximize the unbounded Wasserstein term while satisfying the gradient penalty. However, the Wasserstein term often escalates to irrational values during the initial stages. During initial training, the critic function in WGAN variants like WGAN-GP may not satisfy the Lipschitz constraint, leading to instability in optimization and poor image generation performance. To address this, setting an upper-bound for the Wasserstein term helps improve numerical stability. This modification allows for both the Wasserstein term and gradient penalty to be optimized in a balanced manner, preventing one from dominating over the other. The choice of the upper-bound should be carefully balanced to avoid limiting the critic function's capacity or causing excessive escalation of values. The proposed bounded strategy in WGAN variants like WGAN-GP aims to improve numerical stability by setting an upper-bound for the Wasserstein term. This allows for balanced optimization of the Wasserstein term and gradient penalty, preventing one from dominating over the other. Different methods, such as Sinkhorn distance, can be used to estimate the value of the bounded term \u010e W. The 1-Wasserstein distance between real distribution Pr and generated distribution Pg can be estimated using the Sinkhorn distance in WBGAN. The computation of Wasserstein distance is replaced by Sinkhorn distance due to heavy computational costs. The algorithm involves learning rate, batch size, critic iterations, gradient penalty weight, Sinkhorn distance weight, and other hyper-parameters for training. The goal is to obtain trained parameters \u03b8 and \u03c6 that have converged. The Sinkhorn distance is used in WBGAN to compute the distance between real distribution Pr and generated distribution Pg. It is faster than linear programming solvers and allows for gradient back-propagation. The generator function G \u03c6 produces samples from noise input z, while the critic function D \u03b8 evaluates the generated samples. The optimal generator G \u03c6 is found by solving for the Sinkhorn distance with a balancing hyper-parameter \u03bb s. The flowchart of training WBGAN with Sinkhorn distance involves real data distribution Pr, generated data distribution Pg, and their empirical distributions. Proposition 1 states that as E[W1(Pr, Pg)] approaches 0, W1(Pr, Pg) is forced to 0. Using d\u03bb(Pr, Pg) to constrain the Wasserstein term is reasonable. Most GANs measure distribution distance based on probability divergence. Eq. 8 is validated in this context. The text discusses probability divergences and the validation of Eq. 8 as a valid divergence. It includes definitions, remarks, and proofs regarding the use of Eq. 8 as a probability divergence between two distributions. The text also mentions the optimization of Eq. 8 even in extreme cases where distributions have no intersection. The text discusses the validation of Eq. 8 as a valid divergence using gradient descent. It compares WBGAN to other GAN variants and introduces WBGAN-GP, WBGAN-div, and WBGAN-GPReal by adding bound constraints to existing WGAN variants. Different network architectures like DCGAN and BigGAN are used in the study. Different network architectures are utilized in the study, including DCGAN and BigGAN. BigGAN is a conditional GAN architecture with class conditioning passed to the generator. The discriminator is conditioned using cosine similarity between its features and learned class embeddings. Spectral norm is used in BigGAN, while the self-attention module is omitted for simplicity. The Fr\u00e9chet Inception Distance (FID) is chosen for quantitative evaluation. The study investigates mid-resolution image generation on the CelebA dataset using DCGAN. FID curves are obtained directly from generators without using moving average strategy to diagnose oscillating properties of different methods during training. The study compares the stability and performance of WBGAN-based methods with WGAN-based baselines during training. WBGAN-based models show improved stability and superior performance, with lower FID values compared to WGAN-based models. WGAN-div experiences unexpected FID fluctuations, WGAN-GPReal does not achieve FID convergence, and WGAN-GP shows slower convergence compared to WBGAN-GP. Additionally, a stronger backbone using BigGAN is investigated for generating face images on the CelebA dataset. The study evaluates the stability and performance of WBGAN-based methods compared to WGAN-based baselines. WBGAN-GP shows superior convergence and FID values, while WGAN-div and WGAN-GPReal fail to converge. Results are shown in training curves and tables, with generated face images displayed in figures. Stabilization of Wasserstein term and generator loss is analyzed, with additional results using BigGAN provided in the appendix. The study evaluates the stability and performance of WBGAN-based methods compared to WGAN-based baselines. WBGAN-GP shows superior convergence and FID values, while WGAN-div and WGAN-GPReal fail to converge. Results are shown in training curves and tables, with generated face images displayed in figures. Stabilization of Wasserstein term and generator loss is analyzed, with additional results using BigGAN provided in the appendix. A new term named the generator loss is computed, showing more stable G loss terms in WBGAN-based approaches compared to WGAN-based approaches. An ablation study is conducted to investigate the contribution made by different components of WBGAN using DCGAN on the CelebA dataset. Four configurations are compared, showing the impact of different loss terms on the training process of GAN. In this section, the study evaluates the performance of different WGAN-based methods, including WGAN-GP and WGAN-GP+D-bound+G-Sinkhorn. The FID curves show that adding D-bound stabilizes the training process, while integrating both D-bound and G-Sinkhorn results in a smooth and fast descent of the FID curve. The evaluation is done on higher-resolution images using the CelebA-HQ dataset with BigGAN as the backbone. When working with larger images, the accuracy of estimating Sinkhorn distance decreases due to smaller batch sizes. Alternative bounds are considered to constrain the critic, with constant values of 10 for WGAN-GP, 5 for WGAN-div, and 3 for WGAN-GPReal used based on empirical study. These values are heuristically chosen to stabilize the Wasserstein term in the training process. The Wasserstein term curves for different baselines are provided, showing FID curves and quantitative results in Fig. 7 and Table 2. WBGAN-GP has a similar convergence rate to WGAN-GP, WBGAN-div is slightly better than WGAN-div, and WBGAN-GPReal outperforms WGAN-GPReal. Sinkhorn distance is one way of upper-bound estimation, but other types can be used. Estimating Wasserstein distance between high-resolution image distributions remains a challenge. Refer to Fig. 13 and Fig. 14 in Appendix F for generated face images by different approaches. The paper introduces a general framework called WBGANs to stabilize the training process and improve performance in high-resolution image generation tasks. It discusses the benefits of using a bounded Wasserstein term for training stability and presents an instantiated bound estimation method via Sinkhorn distance. Theoretical analysis is provided, but setting a better bound for higher resolution tasks remains an open topic. The paper introduces WBGANs to improve high-resolution image generation using a bounded Wasserstein term. It discusses a method for estimating bounds via Sinkhorn distance and provides theoretical analysis. The need for a better bound for higher resolution tasks is highlighted as an open topic. The paper introduces WBGANs for high-resolution image generation with a bounded Wasserstein term. It discusses estimating bounds using Sinkhorn distance and provides theoretical analysis. The need for improved bounds for higher resolution tasks is emphasized."
}