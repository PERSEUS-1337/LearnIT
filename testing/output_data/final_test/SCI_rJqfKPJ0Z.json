{
    "title": "rJqfKPJ0Z",
    "content": "A breakthrough in AI has been achieved through deep neural networks, but they are vulnerable to adversarial attacks. The C&W attack is a robust method, but feature squeezing and ensemble defense can counter these attacks. A new method called Centered Initial Attack (CIA) limits perturbation to a preset threshold. The Centered Initial Attack (CIA) method limits perturbation to a preset threshold, making it robust against various defenses. With a maximum perturbation of 1.5% on any pixel, around 80% of attacks fool the defense, increasing to nearly 100% with a 6% perturbation. The difficulty of defending against CIA attacks is highlighted in the paper's last section. Deep neural networks (DNN) have become highly effective in various domains due to the increase in data volumes and GPU capacities. However, they are vulnerable to adversarial attacks, which can be dangerous in critical systems like autonomous vehicles and IT networks. Adversarial attacks on deep neural networks (DNN) pose a threat to critical systems like autonomous vehicles and IT networks. Various methods have been proposed to craft adversarial examples, including box-constrained L-BFGS, fast gradient sign method (FGSM), and Jacobian-based Saliency Map Attack (JSMA). These attacks aim to manipulate pixel intensity or gradients to deceive DNN-based intrusion detection systems. Evolutionary algorithms and robust attacks like Carlini and Wagner's L2-attack have been developed to create adversarial examples for deep neural networks. These attacks aim to deceive DNN-based intrusion detection systems by manipulating pixel intensity or gradients. Defense mechanisms such as feature squeezing and adversarial training have also been proposed to counter these attacks. The investigation revealed that various defense mechanisms, including gradient masking, dropout, Bayesian networks, statistics, and principal components, can be defeated by C&W attacks. Feature squeezing was also found to be vulnerable, with JPEG encoding as a potential but untested defense for images. Ensemble defenses and attacks transferability between models remain unexplored areas in the field. In the presence of an oracle, defense mechanisms can be defeated by C&W attacks. Clipping is necessary when perturbation is limited, degrading attack quality. A new attack called Centered Initial Attack (CIA) is introduced to address these issues. An example comparing CIA and C&W L2 attack is provided, showing the impact of clipping on a targeted guitar image. The Centered Initial Attack (CIA) outperforms the C&W attack in generating successful attacks with 96% confidence. However, the effectiveness of C&W is reduced to 88% when clipping is applied to limit perturbation. The paper discusses mathematical formulations, the CIA strategy, application against various defenses, guidelines to mitigate CIA attacks, and future research directions. A neural network is a function F(x) = y that depends on model parameters like weights and biases. The input x can be a vector or array, and the output y is calculated using the softmax function for m-class classifiers. The predicted class C(x) is determined by the component with the highest probability in the output vector. In crafting adversarial examples, the goal is to manipulate the input x to produce a predicted class C(x) that differs from the correct class Cc(x). This can be achieved through non-targeted or targeted attacks, using a loss function to maximize the probability of misclassification. The perturbation \u03b4 is constrained within a certain domain to ensure the adversarial example stays within bounds. Existing approaches involve generating adversarial examples through iterations and clipping to respect the perturbation constraint. The Centered Initial Attack method aims to find the center of the domain for each component of the input vector, ensuring it stays within specified bounds. This involves considering three cases to define the center, using a continuous differentiable function to represent each component. The CIA attack method uses arrays to optimize the loss function without constraints on the variable r, which is initialized as zero. Different continuous functions can be used for g, with tanh being a common choice. This method allows for defining maximum perturbations between components and limiting crafting to specific portions of an image. The crafting method allows for limiting modifications to specific parts of an image, using a zero max perturbation on other regions. Gradients masking is not desired due to it being a clipping operation. An example of partial crafting is shown in Figure 3, where only a 50px band on the top and right sides is altered. Adam BID15 is the fastest optimizer for crafting attacks in experiments. In experiments, the crafting method allows for limited modifications to specific parts of an image using zero max perturbation. The Adam optimizer is used for crafting attacks. CIA is not limited to images and can be used for any data with bounded continuous features. Targeted attacks are considered against various defense strategies, including ensemble defense, feature squeezing, and JPEG encoding. White box attacks are focused on, with full access to defense model parameters. Transferability between models is noted in other works. In order to test the robustness of CIA in attacking multiple classifiers simultaneously, the five best classifiers on ImageNet dataset are considered: Inception V3 (IncV3 a), Inception V4 (IncV4), InceptionResnet V2 (IncRes a), adversarially trained Inception V3 (IncV3 b), and adversarially trained InceptionResnet V2 (IncRes b). These classifiers have an accuracy of around 80% on the ImageNet dataset. The experiment focuses on attacking the IncV3 b classifier and evaluates the success rate of targeted attacks. The experiment focused on attacking the IncV3 b classifier and analyzing the success rate of targeted attacks. The results showed that while transferability was nonexistent, there was a slight increase in misclassification rate, particularly with IncV3 a. It was found that attacking multiple classifiers simultaneously was more effective, with an optimization using a sum of losses related to each classifier. The results of this approach were displayed in TAB2. The success rate of targeted attacks against the voting ensemble is high, around 80% for \u2206 = 4.0 and approaching 100% for \u2206 = 16.0. Attacking an ensemble defense using CIA is effective when complete access to all defense models is available. Transferability to an unknown model while attacking four among the five is limited but not negligible (more than 30%). A defense approach using smoothing filters BID6 was developed to counter attacks by removing sharp changes in crafting adversarial examples. Spatial smoothing with a 3x3 filter was found to be effective in maintaining defense accuracy. Different smoothing strategies like Gaussian and mean filters were considered, with the mean filter chosen for its simplicity in calculations. The filter can be replaced by a convolution layer for defense. The defense approach using spatial smoothing filters was developed to counter attacks by removing sharp changes in crafting adversarial examples. A convolution layer can replace the filter for defense, resulting in a new network represented by a new function F. Targeted attacks against one network showed a high success rate, indicating the defense's ineffectiveness. Transferability between models was found to be nonexistent, raising doubts about the defense's use of spatial smoothing. The defense approach using spatial smoothing filters was developed to counter attacks by removing sharp changes in crafting adversarial examples. However, attacks showed a success rate of only 3.2% and a misclassification rate of 18.2% when tested. This result contradicts the belief that spatial smoothing is an effective defense. To address this issue, a hybrid network using both filtered and non-filtered inputs is proposed for a more robust attack. The experiment results using a hybrid loss function showed a high success rate of 98.4% with filter-based defense and nearly 100% with no filter defense. The attack was found to be robust regardless of filter usage. Additionally, results from an ensemble defense using hybrid losses showed high success and misclassification rates, especially when no filters were used. The experiment results showed high success rates for attacks with filter-based defense, especially when no filters were used. Assigning a greater weight in the hybrid loss equation improved the attack rate. JPEG encoding adversarial examples before classification was effective in countering most attacks. The results of the attack against IncV3 showed robustness in non-targeted attacks with different compression quality values. The experiment results demonstrated high success rates for attacks against filter-based defense, particularly without filters. Increasing the weight in the hybrid loss equation enhanced the attack rate. Using JPEG encoding for adversarial examples before classification effectively countered most attacks. The results against IncV3 showed resilience in non-targeted attacks with varying compression quality values. However, targeted attacks were less successful, with a maximum success rate of 20% at Q = 80 and 0% at Q = 20. It is important to note that using Q = 20 as a defense would significantly reduce classifier accuracy. Attempts were made to enhance attack success by approximating JPEG transformations, but accurately modeling JPEG encoding proved challenging due to its non-differentiable nature. The Y component is more important in an image than the Cb and Cr components. Cb and Cr can be downsampled without affecting human eye receptivity. Removing high frequencies after a DCT of an image does not significantly impact quality. Steps like dividing the image into blocks and quantization are not considered. The approach to approximating JPEG involves transforming RGB images to YCrCb space and filtering each component using a convolution layer. After transforming RGB images to YCrCb space and filtering each component using convolution layers, the results of crafting attacks against IncV3 using this approximation are similar to previous results. The approximation of JPEG encoding is not accurate enough to give strong attacks and needs improvement. The defense against attacks needs improvement, with a focus on finding transformations that are not easily approximated. Using multiple well-performing classifiers with limited accuracy variance transformations like spatial smoothing can decrease success rates of attacks. However, attacking all classifiers except IncV4 shows limited transferability, with success rates almost reaching 100% against the attacked classifiers. In this paper, a new strategy called CIA is introduced for crafting adversarial examples with a maximum perturbation smaller than a fixed threshold. The CIA attacks have shown robustness against defenses like feature squeezing, ensemble defenses, and JPEG encoding. Future work includes investigating the transferability of CIA attacks to the physical world and exploring partial crafting attacks that consider the content of the data, such as hiding imperceptible perturbations in images."
}