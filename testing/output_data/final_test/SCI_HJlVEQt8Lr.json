{
    "title": "HJlVEQt8Lr",
    "content": "The Recurrent Attention Model (RAM) is examined from an active information sampling perspective, drawing on neuroscience research on visual attention. The original RAM model only incorporates one of the three suggested motives for active information sampling strategies. The original RAM model has three key weaknesses, which are addressed by adding two extra terms to the objective function. The modified RAM shows faster convergence, enables dynamic decision making per sample without accuracy loss, and generalizes better on longer sequences of glimpses. The Recurrent Attention Model (RAM) is a neural network that takes glimpses of small image patches to actively sample new information. Unlike other attention mechanisms like Transformer and BERT, RAM focuses on obtaining new information rather than processing fully observed data. However, RAM has three weaknesses: slow training convergence, and does not. The Recurrent Attention Model (RAM) has slow training convergence and does not support dynamic number of glimpses per sample. By adding two extra terms inspired by neuroscience research, RAM achieves faster convergence and incorporates additional motives for decision-making strategies. The Recurrent Attention Model (RAM) combines a recurrent neural network with reinforcement learning to utilize visual attention for classification. It outperforms a convolutional neural network baseline by adaptively selecting patches of images and processing only the selected patches. The modified RAM generalizes better to longer sequences of glimpses and enables decision-making with a dynamic number of glimpses without loss of accuracy. The Recurrent Attention Model (RAM) utilizes visual attention for classification by actively obtaining new information for decision making. RAM is trained end-to-end using the REINFORCE rule and maximizes a simple reward for the final prediction action. Glimpse actions are indirectly rewarded if they provide useful information for correct predictions, similar to pure-exploration bandits. The Recurrent Attention Model (RAM) uses visual attention for decision making, which is not well addressed in deep learning, reinforcement learning, and neuroscience research. Weaknesses of RAM include slow convergence during training and a fixed number of glimpses for each input sample. RAM uses a fixed number of glimpses for each input sample, but researchers aim for automatic termination when predictions are certain. They trained a separate network to predict termination, allowing RAM to dynamically predict with confidence. RAM struggles with generalization when using a larger number of glimpses, showing decreased accuracy in experiments with different glimpse settings. In a study on attentional learning, Gottlieb (2018) highlighted the importance of active sampling through visual attention and gaze for decision making. Three motives for implementing active sampling policies were suggested: increasing task rewards, reducing belief state uncertainty, and anticipating positive or negative outcomes. In the context of RAM for classification, additional terms are proposed to represent the motives of savoring or dread. The original objective function of RAM aims to maximize the reward of correct classification. The MNIST dataset with M sample-label pairs is denoted as {X, Y} = {x, y}M, where K = 10 classes. The RAM architecture parameter is \u03b8, with the output softmax vector as f\u03b8(x) and the classification prediction as a\u015dy = argmax i f\u03b8(x)i. The original objective function maximizes the reward of correct classification under the distribution of all possible action sequences. Two tricks are employed to maximize the objective function, which is non-trivial. The policy maximizes the original objective function by using the REINFORCE rule and training an auxiliary network to reduce variance. The new objective function includes terms to model uncertainty and predict classification errors. The policy maximizes the original objective function by using the REINFORCE rule and training an auxiliary network to reduce variance. The new objective function includes terms to model uncertainty and predict classification errors. The predicted difference as self-uncertainty, denoted as u(x), is minimized by intrinsic utility to anticipate outcomes equally across all classes. This preference encourages the output softmax vector to be close to a prior distribution. The introduction of self-uncertainty in the modified RAM encourages the output softmax vector to be close to a uniform distribution, helping to prevent over-confident predictions when there is insufficient information. This regularization is especially beneficial during early training stages when the information sampling policy may not be optimal. The self-uncertainty also allows for dynamic decision-making in test-time and enables a dynamic number of glimpses for different samples based on uncertainty measures. The modified RAM introduces self-uncertainty to prevent over-confident predictions and enable dynamic decision-making at test-time. It uses upper and lower bounds for each class, terminating when the highest class's lower bound exceeds the rest's upper bounds. The number of glimpses taken by RAM is controlled by the exploration rate \u03b2, with more glimpses taken for larger \u03b2 values. The model is evaluated on the MNIST dataset with N=6 glimpses during training for optimal test-time accuracy. The modified RAM introduces self-uncertainty for dynamic decision-making at test-time. Testing shows that adding new terms to the objective function leads to faster convergence. The performance of the modified RAM remains stable even with a large number of glimpses, unlike the original RAM. Adding both new terms improves convergence significantly, while adding only the uncertainty term shows minimal improvement. The modified RAM with self-uncertainty improves prediction accuracy with more glimpses, stabilizes performance, and converges faster. Dynamic exploration rate does not significantly impact performance, supporting the hypothesis that some samples require fewer glimpses. The intrinsic term effectively stabilizes prediction accuracy even with minimal training epochs. The l2-norm of internal states in the original RAM increases with more glimpses, while the modified RAM with intrinsic term remains stable. The modified RAM with intrinsic term stabilizes prediction accuracy even with minimal training epochs, while the l2-norm of internal states in the original RAM increases with more glimpses."
}