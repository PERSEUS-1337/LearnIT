{
    "title": "ryezBaNKDB",
    "content": "In this paper, learnable higher-order operations are introduced as building blocks for capturing higher-order correlations in video recognition. The study shows that successful visual classification architectures belong to the higher-order neural network family. Experimental results demonstrate the effectiveness of higher-order models in video recognition tasks, achieving state-of-the-art performance on Something-Something and Charades datasets. Effective video recognition architectures should not only recognize the appearance of objects but also understand actions arising from object motions. Effective video recognition architectures should not only recognize the appearance of objects but also understand actions arising from object motions. Recognizing actions involves understanding how objects relate in space and time, such as distinguishing left to right motion from right to left motion based on hand movements against the background. Classifying pull and push actions is more complex, requiring knowledge of the relative positions of the hand and object resulting from hand movements. The need for recognizing patterns in spatiotemporal context is crucial for understanding actions involving objects. Conventional convolutions struggle to capture the variety of variations in action classes due to fixed filter parameters. To recognize every object-in-context pattern, detailed filters are necessary, potentially leading to a large number of parameters. However, object-in-context patterns are related through a higher-order structure, such as pushing an iron or pulling a pen, affecting spatio-temporal relations. The text discusses the importance of recognizing object-in-context patterns in spatiotemporal relations. It proposes a feature extractor that can learn these patterns and suggests using context-dependent filters instead of fixed filters in convolution operations. This approach aims to capture the variations in visual patterns of target objects in different contexts more effectively. The proposed model replaces fixed filters with context-dependent filters to capture spatiotemporal contexts effectively. Tested on four benchmark datasets for action recognition, the model shows significant advantages over existing methods. The proposed model for action recognition replaces fixed filters with context-dependent filters, showing significant advantages over existing methods on benchmark datasets. The experiments demonstrate the effectiveness of the models, achieving results on par with or better than current state-of-the-art algorithms. Many video-action recognition methods are based on high-dimensional encodings of local features, such as histograms of oriented gradients (HOG) and histograms of optical flow (HOF). Researchers have explored the use of 2D ConvNets and 3D ConvNets for video recognition tasks, with some focusing on saving computation by replacing 3D convolutions. Several methods have been explored to save computation in video recognition tasks, including replacing 3D convolutions with separable or mixed convolutions. Additionally, an inflation operation has been introduced to convert pre-trained 2D models into 3D. Contextual information is crucial for action recognition, with various approaches reviewed for incorporating contextual information in object recognition. Various methods have been utilized to incorporate contextual information in action recognition, such as exploiting natural dynamic scenes, modeling spatio-temporal relationships, and learning discriminative space-time feature neighborhoods. Conditional Random Field (CRF) models have also been used for object and action recognition, including frameworks that incorporate hidden variables for part-based object recognition and exploiting feature relationships in videos captured by cameras. The CRF-based approach leverages relationships among features in videos from different viewpoints for video analysis. The second-order model involves analyzing video feature maps using spatio-temporal filters with filter parameter values computed through a higher-level function. These H-blocks can be part of a larger model composed entirely of H-blocks or intermittently included between conventional convolutional layers. The blocks work on video feature maps to produce output maps, which can be either the original video or the output of prior processing. The H-blocks in the model analyze video feature maps using spatio-temporal filters to generate output maps. The input video feature map is denoted as X, while the output map is denoted as Y. The output map is derived from the input map through a second-order relation Y = f(X, g(X; \u0398)), where f is a function relating the input and output maps and g generates parameters. The model uses H-blocks to analyze video feature maps with spatio-temporal filters to generate output maps. The input map X is transformed into the output map Y through a second-order relation using functions f and g. The convolutional network structure is described, with convolution operations and activation functions applied to the convolution outputs. The H-block maintains a similar structure for processing the feature maps. The H-block in the model uses position-dependent filter parameters computed by an upper-level function. The filter parameters are represented as W = {W p,q } and are derived through convolutions, with each W p,q restricted to a diagonal matrix. This approach is similar to depth-wise convolution, reducing the total number of parameters required. The context field for convolutional filters is defined by the receptive field R, capturing spatio-temporal patterns with shared weights \u0398. The parameters for estimating x p,q are obtained through a convolution operation from X, with \u0398 q t representing the filter parameters. The total number of parameters is C 2 in \u00d7 |R| \u00d7 |R |, where |R| is the number of elements in R. In the context field, R captures information and features are extracted from |R| positions using a CNN-based second-order block with multiple layers of convolutions. The CNN is designed to have a receptive field larger than R, requiring fewer parameters than a simple convolution model. For example, a 2-layer CNN with 3x3 filters provides a context field of 5x5. The CNN model with two layers, each computed by a 3x3 filter, provides a context field of 5x5 using only 18 parameters. Sharing parameters across the CNNs for different values of R greatly reduces the number of parameters required. The model is tested on various datasets to demonstrate its effectiveness. The ConvNet in the V2 dataset uses 9 C-dimensional vectors concatenated into a C \u00d7 3 \u00d7 3 filter to extract features. The backbone model is based on the ResNet-50 Inflated 3D architecture for fair comparison with previous results. Pseudo-3D convolutions are used to implement the ConvNet for a large context field. The ConvNet in the V2 dataset utilizes pseudo-3D convolutions with different context fields. The three P3D convolutions have varying input and output channels, with the last convolution being a group convolution to reduce parameters. The activation function used is the scaled exponential linear unit (SELU), and softmax is used for normalization in the last convolution layer. Models are trained from scratch, with input frames resized to 256\u00d7320 dimensions before training. Following (Wang & Gupta, 2018), input frames are resized to 256\u00d7320 and cropped to 224\u00d7224 pixels for training. The model is trained with 8-frame input clips at 12 FPS on a 4-GPU machine for 30 epochs, then fine-tuned with 32-frame input at 6 FPS on an 8-GPU machine for 45 epochs. Mini-batch stochastic gradient descent with momentum of 0.9 and weight decay of 1e-4 is used for optimization. Cross entropy loss is used for Something-Something V1, V2, and Kinetics-400 datasets, while binary sigmoid loss is used for Charades datasets. Inference involves resizing input frames to 256\u00d7320 and randomly sampling 40 frames. At the inference stage, input frames are resized to 256\u00d7320 and randomly sampled in 40 clips of 32-frame inputs at 6FPS. The final predictions are based on the averaged softmax scores of all 40 clips. The Something-Something V1 dataset contains 86K training videos, 12K validation videos, and 11K testing videos with 174 classes. Ablation results on the validation dataset show the network performance improvement when adding one H-block on res3, with diminishing returns on deeper stages. The study discusses how spatiotemporal correlation weakens as the network goes deeper, with higher-order information becoming less important. Adding one H-block after the first and second bottleneck in res3 leads to better accuracy compared to adding it in res3-3 and res3-4, indicating that spatiotemporal contexts weaken at different positions within the same stage. Adding H-blocks in different positions within the network can capture more meaningful spatiotemporal contexts and lead to better results. Multiple higher-order blocks in ResNet-50 improve performance by capturing comprehensive contextual information. Adding H-blocks with different kernel sizes and context fields can influence the improvement in classification accuracy. A kernel size of 3x3x3 showed the best results, while larger or smaller kernels led to lower accuracy. The size of context fields also plays a role in performance, with different context fields showing similar improvements. Smaller context fields may face optimization difficulties due to large spatial sizes. Adding H-blocks with different activation functions and context fields can impact classification accuracy. Smaller context fields may lack precise information, while larger ones could be redundant. The choice of activation function does not significantly affect performance. Comparison with state-of-the-art approaches is done on the test set of Something-Something V1. Our model achieved a top-1 accuracy of 46.7% on the Something-Something V1 dataset, surpassing all existing RGB-based methods. It can capture both appearance and motion information simultaneously, as shown in Figure 3. Our higher-order network can learn to find important relation clues instead of focusing on appearance information compared with I3D backbones. Kinetics-400 dataset contains 246k training videos and 20k validation videos for a classification task involving 400 human action categories. Our model achieved a top-1 accuracy of 77.8% and top-5 accuracy of 93.3% on the validation set, outperforming state-of-the-art methods. The Charades dataset consists of 8K training videos and 1.8K validation videos with 157 action classes. The baseline I3D ResNet 50 approach achieves 31.8% mAP, while the best result NL I3D + GCN achieves higher accuracy. Our method improves mAP by 5.1% (36.9% mAP) by adding H-blocks to res3 and res4 stages in the I3D Res50 backbone. Another 0.2% gain (37.1% mAP) is achieved by adding H-blocks to the res2 stage. The effectiveness of H-blocks is highlighted. The models are also tested on the Something-Something V2 dataset, which has 22K videos and 174 classes. Our higher-order ResNet 50 achieves 62.6% Top 1 accuracy when adding five higher-order blocks to res3 and res4 stages. Comparisons with previous results are shown in Table 4. In this paper, higher-order networks are introduced to action recognition, utilizing H-blocks to model position-varying contextual information. The proposed networks achieve state-of-the-art results on various datasets without fine-tuning, showcasing their effectiveness in context modeling. The model's versatility extends beyond visual tasks, making it applicable to tasks where context influences input feature interpretation. Future work will explore the benefits and extensions of the higher-order model in different scenarios. The curr_chunk discusses the benefits of a higher-order model and its extensions in various visual, text, and cross-modal tasks. It includes details on context field factorization and the backbone ResNet-50 I3D model."
}