{
    "title": "ryxtWgSKPB",
    "content": "Machine learning models can successfully learn to model quantum experiments by predicting output state characteristics for given setups without computing the states themselves, showing significant improvement over random search. Machine learning models, particularly LSTM architectures, can efficiently design complex quantum experiments by predicting output states for given setups without the need to compute the states themselves. This approach enables faster search and automated design of multiparticle high-dimensional quantum experiments. Designing complex quantum experiments involves determining the sequence of elements that quantum particles pass through. High-dimensional quantum states are crucial for various applications in quantum technologies. Automated design procedures, like the algorithm MELVIN, are necessary due to the complexity of determining the setup for desired final quantum states. The algorithm MELVIN uses optical elements to randomly generate sequences, calculate resulting quantum states, and identify interesting states. It has been used in laboratory experiments and a reinforcement learning approach has also been applied. LSTM networks are now being investigated for learning quantum optical setups and predicting resulting quantum states. Millions of setups generated by MELVIN are used to train the neural networks, making deep learning approaches the preferred choice. Cluster cross validation is used to evaluate the models. The OAM of a photon is characterized by an integer representing the shape and handedness of the photon wavefront. A three-particle quantum state can have different OAM values for each photon. Quantum states are evaluated based on their entanglement and dimensionality, represented by the Schmidt rank vector (SRV). State |\u03a8 is maximally entangled with equal amplitudes for all terms. The setup is labeled \"positive\" if its output state is maximally entangled and obeys further restrictions, otherwise labeled \"negative\". LSTM networks are trained to predict entanglement and SRV characteristics from experimental setups without predicting the quantum state itself using binary cross entropy for classification and regression. The SRV label is represented by a 3-tuple (n, m, k) where n \u2265 m \u2265 k. The network predictions for distribution parameters are \u03bb, p, q. The Schmidt rank value predictions are n = \u03bb, m = p\u03bb, k = pq\u03bb. Marginals of the joint probability mass function are considered to obtain the log-likelihood objective for a data point x with label (n, m, k). The sequence processing model involves training two networks for entanglement classification and SRV regression separately to avoid incorporating correlations between the two. Marginals of the joint probability mass function are used to obtain estimates for n, m, and k. The target values \u0177 can be estimates for y E or y SRV. The dataset consists of various setups with optical elements fed into a network. An LSTM with 2048 hidden units is used along with a component embedding space. The goal is to extrapolate to unseen SRVs by clustering the data based on leading Schmidt rank n. The dataset is clustered by leading Schmidt rank n to extrapolate to unseen SRVs. Samples with n \u2265 9 are moved to an extrapolation set, while the rest are divided into a training set and a conventional test set. The test set estimates generalization error, while the extrapolation set evaluates the model's performance on higher Schmidt rank numbers. The dataset is clustered by leading Schmidt rank n to extrapolate to unseen SRVs. Cluster cross validation (CCV) is used to evaluate the model's performance on higher Schmidt rank numbers. Seven folds correspond to Schmidt ranks 2 to 8, while samples with n = 1 and n = 0 are considered negative cases. The 4,300,268 samples with n < 2 are divided into two folds at random for training. The LSTM network's performance is evaluated based on true positive rate (TPR), true negative rate (TNR), and hit rate (HR). Interesting setups are those classified as positive. The data is split based on the leading Schmidt rank n, with samples where n \u2265 9 forming the extrapolation set for out-of-distribution exploration. The remaining samples (n < 9) are randomly split for testing. The training set is used for cluster cross-validation to identify true positives and false positives based on interesting classifications. The LSTM network is trained using stochastic gradient descent with momentum 0.5 and batch size 128, sampling mini-batches with equal positive and negative samples. The leading Schmidt rank vector number n is used as a class label for balanced SRV regression. Models are trained with early stopping after a certain number of weight update steps. Hyperparameter search was conducted in advance on a similar dataset. Figure 4 displays TNR, TPR, and rediscovery ratio for specific thresholds. The TNR and HR for fold 0,1 are 0.9996 and 0.659 respectively. Model performance is influenced by parameters \u03c4 and r, as shown in Figure 5 with varying sigmoid thresholds and SRV radii. The TNR approaches 1 for restrictive parameter choices, while too loose choices result in negligible advantage over random search. The models perform well with a decent compromise between TNR and TPR, reflected in a hit rate of 0.736 on average. The entanglement classification has a training BCE loss value of 10.2, with TNR and TPR values of 0.9271 and 0.9469 respectively. The SRV regression has a training loss value of 2.247, an accuracy of 93.82% with r = 3, and a mean distance between label and prediction of 1.3943. The test error for SRV is 2.24, with an accuracy of 0.938 and a mean distance of 1.40. Our experiments show that LSTM-based neural networks can model properties of complex quantum systems beyond entanglement and Schmidt rank. The next step involves using generative models like GANs and beam search for automated design of multiparticle quantum experiments. LSTM-based approaches have been successful in generating sequences in adversarial settings. The LSTM-based approaches utilize reinforcement learning to address gradient propagation issues in softmax outputs. Beam search can be implemented through a discriminative or generative approach, with the former maximizing belief in positive outcomes and the latter training on positive samples for next element prediction. Beam search aids in approximating the most probable sequence given initial conditions. Automated design procedures using LSTM-based neural networks can predict characteristics of high-dimensional quantum states without explicit knowledge of quantum mechanics. The network can extrapolate well to unseen data, allowing for the automated design of complex quantum experiments using generative machine learning models."
}