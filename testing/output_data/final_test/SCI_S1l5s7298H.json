{
    "title": "S1l5s7298H",
    "content": "The algorithm introduced is for low-rank decomposition, aiming to find a rank-$k$ matrix $A'$ that minimizes the approximation loss $||A- A'||_F$. It uses a training set of input matrices to optimize performance and replaces the random matrix $S$ with a learned matrix of the same sparsity to reduce error in computing low-rank approximations. Our experiments demonstrate that a learned sketch matrix can significantly decrease approximation loss compared to a random matrix $S$ across various data sets. Mixed matrices, with some rows trained and others random, also show improved performance while maintaining worst-case guarantees. The success of modern machine learning has expanded its application beyond traditional AI, with a focus on enhancing the efficiency of standard algorithms through fine-tuning based on input distribution properties. This learning-based approach to algorithm design has garnered significant attention for its potential to boost efficiency. In recent years, machine learning has shown potential in improving algorithmic efficiency for processing data streams. By tailoring algorithms to specific data distributions and training on past instances, better results can be achieved. This approach has been successful in compressed sensing, where random matrix selection can recover key coefficients of a vector. Compressed sensing involves using random matrix selection to recover key coefficients of a vector. This method can be improved by using samples from the distribution of the vectors. Low-rank decomposition is another problem that can be solved using random projections, which is widely used in data analysis, machine learning, and statistics. Multiple algorithms developed over the last decade use the \"sketching\" approach, which involves efficiently computable random projections to reduce problem size before low-rank decomposition. This method makes computations more space and time efficient, allowing for the recovery of a rank-k matrix through SVD on a random matrix. The sketch length is typically small, enabling storage in little space or efficient communication. The SVD of the matrix can be computed efficiently, reducing overall computation time. Our study focuses on improving the accuracy of low-rank decompositions using learned sketch matrices, which can outperform purely random matrices. The goal is to make sketch-based algorithms more efficient by reducing sketch length while maintaining accuracy. Our results show that learned sketch matrices can significantly enhance the accuracy of low-rank decompositions compared to random matrices. Based on a training set of matrices Tr = {A1, ..., AN} sampled from distribution D, a matrix S* is computed to minimize empirical loss using the Sarlos-Clarkson-Woodruff algorithm. The optimized sketch matrix S* can reduce approximation loss significantly compared to a random matrix, sometimes by one order of magnitude. However, using learned sketch matrices may sacrifice worst-case guarantees in algorithms. Using mixed sketch matrices, combining trained and random rows, with the SCW algorithm maintains worst-case performance guarantees of the random part while reducing approximation loss compared to purely random matrices. This approach offers improved performance for input matrices that deviate from the training distribution. Mixed random matrices offer improved performance for matrices from the training distribution and worst-case guarantees otherwise. The SCW algorithm computes the best rank-k approximation of a matrix A using a sketch matrix S, providing a quicker and more economical approach. The algorithm uses a sparse matrix S with one non-zero entry per column to reduce space usage. A learning-based approach computes S using backpropagation to minimize the rank-k approximation loss. The algorithm optimizes a sparse matrix S using stochastic gradient descent to improve loss. The SVD implementation is made differentiable by representing it as individual top singular value decompositions. The explicit form of the loss function and gradients is not feasible due to computational complexity. The algorithm optimizes a sparse matrix S using stochastic gradient descent to improve loss. The explicit form of the loss function and gradients is not feasible due to computational complexity. The method uses the autograd feature in PyTorch to compute gradients with respect to the sketching matrix S. The main question addressed is whether optimizing the sketch matrix S can enhance the performance of the sketching algorithm for low-rank decomposition on natural matrix datasets. Different methods for computing S are implemented and compared. The algorithm optimizes a sparse matrix S using stochastic gradient descent to improve loss. Different methods for computing S are implemented and compared, including random, dense random, learned, mixed (J), and mixed (S) matrices. The algorithm optimizes a sparse matrix S using stochastic gradient descent to improve loss. Different methods for computing S are implemented and compared, including random, dense random, learned, mixed (J), and mixed (S) matrices. Various datasets were used to test the performance of the methods, including videos (Logo, Friends, Eagle), hyperspectral images from HS-SOD dataset, and matrices from TechTC-300 dataset for text categorization. The study evaluates the quality of sketching matrices by testing them on different input matrices. The error of the sketch is defined as the optimal approximation loss on a collection of matrices. To avoid imbalance, matrices are normalized. Testing different methods on various datasets shows that learned sketching matrices outperform random or dense random matrices, with a 20\u00d7 improvement in test error for video datasets and more than 2\u00d7 improvement for other datasets. In Table 2, the performance of mixed sketching matrices is compared with random and learned sketching matrices. Mixed sketching matrices generally yield better results than random matrices and sometimes perform similarly to learned matrices. Training only half of the sketching matrix can still provide good results, while using the remaining random half can offer worst-case guarantees."
}