{
    "title": "SJlt6oA9Fm",
    "content": "In this paper, the focus is on the information-preserving nature of identity connection in bottleneck structures with residual connections in deep convolutional neural networks. The proposed Selective Convolutional Unit (SCU) enables a convolutional layer to have channel-selectivity by redistributing computations to important channels. SCU improves parameter efficiency by pruning unimportant channels and rewiring parameters to important channels during training. Experimental results demonstrate the effectiveness of SCU in various modern CNNs with bottlenecks. Recent advancements in convolutional neural networks (CNNs) have led to the development of more advanced architectures, such as ResNet with identity connections. These models can now scale to over a thousand layers or channels without overfitting. The Selective Convolutional Unit (SCU) enhances parameter efficiency by redistributing computations to important channels, resulting in model compression and improved accuracy across various CNN architectures. Recent advancements in CNN models have led to the development of more advanced architectures like ResNet and DenseNet, which utilize identity connections for parameter efficiency. To address computing resource constraints, recent literature focuses on network pruning, weight quantization, adaptive networks, and resource-efficient architectures. Identity connections help reduce representation dimension while preserving information, making them useful in modern CNN architectures. In this paper, state-of-the-art mobile-targeted architectures like SqueezeNet, ShuffleNet, MobileNet, and CondenseNet emphasize designing efficient bottlenecks. The proposed Selective Convolutional Unit (SCU) aims to efficiently utilize parameters as a bottleneck through convolutional operations and re-distributing computations to selected channels. SCU includes operations to de-allocate unnecessary input channels and re-allocate them for improved efficiency. The proposed Selective Convolutional Unit (SCU) aims to increase CNN efficiency by pruning or rewiring parameters on-the-fly while learning them, similar to how the human brain's hippocampus learns. A novel metric called expected channel damage score (ECDS) is used to select channels for deallocation or reallocation. The Selective Convolutional Unit (SCU) improves CNN efficiency by reallocating channels based on the expected channel damage score (ECDS). This method introduces spatial shifting bias, a scaling layer with sparsity-inducing regularization, and enhances convolutional kernels. SCU is tested on various CNN models like ResNet, DenseNet, and ResNeXt, showing consistent improvements in model size and efficiency. Results consistently show that SCU improves bottleneck efficiency in model size and classification accuracy. For example, SCU reduces error rates of DenseNet-40 model by using fewer parameters on CIFAR-10/100 datasets. SCU also enhances the efficiency of a CondenseNet BID17 model, outperforming NASNet-C BID54. There is a focus in the literature on pruning parameters during neural network training, but less progress on rewiring pruned parameters for maximum utility. In this paper, a new method of rewiring for parameter efficiency is proposed, along with a new architectural framework that allows for pruning and rewiring in a single training pass. This approach aims to achieve a balance between model compression and accuracy improvement by adjusting the calling policy of dealloc and realloc. The work introduces a novel direction in neural network optimization. The paper introduces a new method for parameter efficiency through pruning and rewiring in a single training pass, focusing on the calling policy of dealloc and realloc. It presents the Selective Convolutional Unit (SCU) as a generic architectural unit for bottleneck CNN architectures, discussing its structure, channel-selectivity metric, and integration into network training and inference. The concept of bottleneck structures in modern CNNs, particularly the residual function in ResNet BID11, is also explored. The bottleneck structure is a common approach in designing deep CNN models to reduce computation when dealing with large input sizes. It involves placing a bottleneck that maps the input into a lower dimension before processing it further. This method requires an identity connection to prevent information loss. Various architectures like DenseNet, PyramidNet, and DPN have adopted this idea using different aggregation functions. Designing R-F -W is a common way of handling large features in modern CNNs. The majority of CNNs use inefficient design of the bottleneck R, leading to high computation costs. For example, in models like ResNet and DenseNet, a significant portion of parameters are dedicated to modeling R, which can be inefficient. In this paper, the focus is on improving the efficiency of R in CNNs by reducing parameters through channel pruning and enhancing expressivity. The SCU architecture aims to learn channel-selectivity through dynamic pruning and rewiring during training, showing improved parameter efficiency. The identity connection's information-preserving nature benefits from dynamic pruning, especially in bottleneck structures. The SCU architecture introduces channel-selectivity through dynamic pruning and rewiring, enhancing parameter efficiency in CNNs. It includes special operations for channel control, such as deallocation and reallocation, to improve computational efficiency. The SCU architecture introduces channel-selectivity through dynamic pruning and rewiring, enhancing parameter efficiency in CNNs. Special operations like deallocation and reallocation are designed to be function-preserving, allowing for safe manipulation of input channels. The Channel Distributor (CD) is a key component that rebuilds the input by removing unnecessary channels based on the Expected Channel Damage Score (ECDS). The Channel Distributor (CD) in the SCU architecture rebuilds the input by discarding unnecessary channels and emphasizing important ones. It re-indexes and blocks the input channel-wise, allocating more parameters to better process specific channels. To prevent parameter degeneration, spatial shifting biases are considered for each channel. The Channel Distributor (CD) in the SCU architecture enhances input diversity by reallocating channels with spatial shifting biases. CD(X) i selects the pointed channel with bias b i or 0 if the gate g i is closed. The size of CD(X) matches X, utilizing shifting operations for spatial dimensions with continuous real values b h and b w learned through gradient-based methods. The Noise Controller (NC) component enhances the training of SCU by inducing channel-wise sparsity through a channel-wise re-scaling layer. It uses a Bayesian pruning approach to impose sparsity-inducing regularization on parameters, aiding in recovering damage from channel pruning. The Noise Controller (NC) component enhances SCU training by inducing channel-wise sparsity through a Bayesian pruning approach. Parameters are treated as random variables with a prior, leading to sparsity in the model. Channel-wise multiplicative noises are introduced, following design choices from BID35. The ECDS criteria aim to preserve function while pruning by estimating changes in outputs when channels are damaged. It utilizes BN layer to approximate ECDS without expensive computations, enabling efficient training. The ECDS criteria use the BN layer to estimate changes in outputs when channels are damaged, without expensive computations. The formula for ECDS(S) involves terms measuring input channel activity, NC amplification, and total impact. The SCU method in a CNN model captures low-magnitude and low-contribution channels by training parameters (V, W) through alternating phases of SGD and realloc. Stochastic variational inference is used to incorporate stochasticity from NC, allowing SCU to learn Bayesian parameters jointly with others. The SCU method in a CNN model uses stochastic variational inference to learn Bayesian parameters jointly with others via SGD. Dealloc and realloc are called on demand during training to adjust model compression. Noise from a NC is re-parametrized with a standard normal distribution for stochastic variational inference. The final loss minimized for a minibatch involves channel de-allocation and re-allocation. Dealloc selects slices based on a threshold, setting certain values to 0, while realloc selects channels based on another threshold, reallocating them if space is available. After training a SCU S, closed channels can be safely removed to create a compact unit. This allows for selecting channels instead of obstructing, leading to smaller dimensions in subsequent layers. SCU is trained to select a subset of inputs for bottleneck operations. For NC, efficient inference can be achieved by replacing noise with constants. In experiments, SCU is applied to various CNN architectures with bottlenecks on CIFAR-10/100 and ImageNet datasets. Models like ResNet, DenseNet, and ResNeXt are considered, with bottlenecks replaced by SCU. Different training cases are explored, including dealloc and realloc options. The appendix provides more details on experimental setups. SCU is applied to CNN models with bottlenecks on CIFAR-10/100 and ImageNet datasets, including ResNet, DenseNet, and ResNeXt. Different training cases are explored, such as dealloc and realloc options. SCU consistently improves the original models by utilizing de-allocated parameters to improve accuracy. It can also enhance accuracy even without using dealloc or realloc. The regularization effect of stochastic NC acts as a dropout-like layer, allowing for a targeted trade-off between compression and accuracy improvement. SCU-based models achieve model compression and accuracy improvement by adjusting dealloc and realloc policies. Results on CIFAR-10/100 and ImageNet show consistent improvements. SCU can also be used to design new efficient architectures by focusing on bottlenecks within the structure. SCU-based models focus on bottlenecks within the structure to improve efficiency. They adopt components from CondenseNet BID17, such as increasing growth rate and group convolution, to create a new model called CondenseNet-SCU. This model utilizes group convolution for F and is trained using dealloc to maximize computational efficiency. Comparisons with state-of-the-art CNNs show promising results. Our model, CondenseNet-SCU-182, outperforms state-of-the-art CNNs in model compression and accuracy improvement, including NASNet-C. With 6.29M total parameters, 5.89M are devoted to bottlenecks, highlighting the importance of reducing overhead for better efficiency. Ablation studies on SCU components CD, NC, and show promising results. The study evaluates the impact of key components CD, NC, and ECDS in the proposed SCU using DenseNet-SCU-40 model for CIFAR-10. Spatial shifting is introduced to enhance input diversity, with results showing that spatial shifting improves model performance compared to just reallocating parameters. The inclusion of NC in SCU promotes sparsity in the model. The study examines the sparsity-inducing effect of NC in SCU using DenseNet-SCU-40 model. Comparing models with and without NC, it is observed that NC leads to more deallocation of channels, resulting in better error rates. The stochastic regularization effect of NC is highlighted, although the importance of SBP for efficient deallocation is questioned. The study evaluates the efficiency of de-allocation in SCU using ECDS, comparing it with SBP. Three variants of M1 with different de-allocation policies are tested, showing that ECDS is more effective than SBP. M3 could not de-allocate any channels, while M4 and M5 had worse error rates than M1 when de-allocating competitive numbers of channels. These results confirm the effectiveness of ECDS in efficient de-allocation. The study confirms that ECDS is a more effective de-allocation policy than SBP in SCU. Channel-selectivity in CNNs focuses on bottleneck architectures and has potential for other tasks like interpretability, robustness, and memorization. Bayesian inference involves inferring \u03b8 from a dataset D with prior knowledge in a probabilistic model. Bayesian inference updates the posterior belief on \u03b8 using a prior distribution p(\u03b8) and dataset D. Variational inference approximates p(\u03b8|D) with a parametric distribution q \u03c6 (\u03b8) to minimize KL-divergence. BID22 proposed a Monte Carlo estimator for intractable expectations in complex models. Structured Bayesian pruning (SBP) is an example of incorporating stochastic variational inference into deep neural networks. It involves a dropout-like layer with a noise vector \u03b8, approximating the posterior p(\u03b8|D) with a fully-factorized truncated log-normal distribution. The posterior p(\u03b8|D) is approximated by a fully-factorized truncated log-normal distribution q \u03c6 (\u03b8), with the noise vector \u03b8 re-parametrized using a non-parametric uniform noise \u03b5 i \u223c U(\u03b5|0, 1). SBP explicitly addresses the issue of preventing improper KL divergence between q \u03c6 (\u03b8) and p(\u03b8). SBP optimizes \u03c6 = (\u00b5, \u03c3) and weights W of a neural network using stochastic variational inference. The final loss L SBP is optimized with a scaled KL-divergence term to balance sparsity and accuracy. SBP starts from a pre-trained model and re-trains it, inducing sparsity with a log-uniform prior. Neurons with signal-to-noise ratio (SNR) below 1 are selected and removed after re-training. C BAYESIAN PRUNING AND IDENTITY CONNECTIONS SCU requires careful training-time removal of input channels for channel de-allocation and reallocation. When optimizing neural networks like SBP, careful channel removal is crucial to avoid bad local minima. This issue is more pronounced in Bayesian neural networks, leading to premature pruning of channels due to uncertain weights. Strategies to address this include using pre-trained networks or implementing a \"warm-up\" strategy with rescaled KL-divergence terms. In optimizing neural networks like SBP, careful channel removal is essential to prevent bad local minima. The problem is more significant in Bayesian neural networks, where premature pruning of channels can occur due to uncertain weights. While strategies like pre-trained networks or a \"warm-up\" approach with rescaled KL-divergence terms have been used, this paper suggests that identity connections can greatly alleviate optimization difficulties by preserving input information even when parameters are pruned. This implies that networks with identity connections not only scale up network architectures but also reduce their size. The paper suggests that identity connections can help optimize neural networks like SBP by preserving input information during channel removal. This can prevent premature pruning of channels in Bayesian neural networks. The assumption is that networks with identity connections not only scale up architectures but also reduce their size. The formula for ECDS(S i ) is derived using the linearity of expectation and the assumption that BN(CD(X; W CD ); W BN ) i,h,w \u223c N (\u03b2 i , \u03b3 2 i ) holds in modern CNNs. The paper discusses the benefits of identity connections in optimizing neural networks like SBP by preserving input information during channel removal. It also introduces the ECDS formula derived from the linearity of expectation and the assumption of BN(CD(X; W CD ); W BN ) i,h,w \u223c N (\u03b2 i , \u03b3 2 i ) in modern CNNs. The experiments focus on a randomly chosen hidden layer in a DenseNet-40 model, analyzing the empirical distribution of hidden activations from the CIFAR-10 test dataset. The observations suggest that the assumptions in Proposition 1 are reasonable, even when the model is not trained, indicating structural properties of CNNs like equivariance on translation or the central limit theorem. The formula in Proposition 1 is valid throughout training. V consists of (\u03c0, g) in CDs, while W is split into W NC and W C. The noise \u03b8 from a NC can be re-parametrized with a standard normal noise \u03b5. Stochastic variational inference BID22 is used for optimization with a minibatch-based stochastic gradient method. The final loss minimized for a minibatch involves a sampled vector from a fully-factorized standard normal distribution. The final loss minimized for a minibatch involves a sampled vector from a fully-factorized standard normal distribution. An extra regularization term can be added for non-Bayesian parameters. Truncated distributions on a compact interval are used to bypass issues, but this adds computational overheads, especially for large models like ResNet or DenseNet. Truncations are not taken on q \u03c6 (\u03b8) and p(\u03b8) due to practical considerations. The study approximates truncated distributions of q \u03c6 (\u03b8) and p(\u03b8) on a large interval, replacing D KL (q \u03c6 (\u03b8) p(\u03b8)) with \u2212 log \u03c3 for optimization. This regularization technique does not significantly impact the performance of SCU. Experiments are conducted on CIFAR-10 and CIFAR-100 datasets containing 60,000 RGB images for classification tasks. The CIFAR datasets consist of 60,000 RGB images for classification tasks, with CIFAR-10 having 10 classes and CIFAR-100 having 100 classes. ImageNet dataset has 1.2 million training images and 50,000 validation images with 1,000 classes. Models are trained using stochastic gradient descent with Nesterov momentum and a cosine learning rate schedule. Training includes data preprocessing following specific guidelines and lasts for 300 epochs with a weight decay of 10^-4. For ImageNet models, training lasts 120 epochs with a batch size of 256. Different weight initializations are used based on the model. A predefined calling policy for dealloc and realloc is followed during training, with specific thresholds set for each. For CIFAR-10/100 models, b i is re-initialized via realloc with a random sample from [-1.5, 1.5] \u00d7 [-1.5, 1.5] pixels. Weight decay on each b i is set to 10^-5 separately. For ImageNet models, b i is fixed unless re-initialized via realloc with a sample from [-2.5, 2.5] \u00d7 [-2.5, 2.5] pixels. The reallocation scheme improves efficiency of SCU in models with multiple bottlenecks separated by average pooling layers. The models are divided into stages with average pooling layers for down-sampling. Each stage has N bottleneck blocks. Minor differences exist between the resulting models and the original papers. ResNet and ResNeXt models use explicit average pooling layers for down-sampling, while DenseNet models do not include a 1x1 convolutional layer between stages. The models used have a 1x1 convolutional layer between stages, referred to as the \"compression\" layer in DenseNet. Despite this difference, the models perform as well as the originals."
}