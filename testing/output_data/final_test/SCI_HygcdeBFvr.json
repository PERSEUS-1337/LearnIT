{
    "title": "HygcdeBFvr",
    "content": "Generative models for singing voice are typically focused on singing voice synthesis, creating voice waveforms from musical scores and lyrics. This work explores singing voice generation without pre-assigned scores and lyrics, experimenting with free singer, accompanied singer, and solo singer scenarios. The challenges are outlined, and a pipeline is proposed involving source separation, transcription models, adversarial networks, and customized metrics for evaluation. The task of computationally producing singing voices, known as singing voice synthesis (SVS), typically involves generating voice waveforms from musical scores and lyrics. Researchers aim to create synthesis engines that sound natural and expressive like a real singer. Progress in SVS has closely followed advancements in text-to-speech (TTS) synthesis. However, the traditional approach of singing based on pre-assigned musical scores and lyrics may not capture the full range of human singing activities. Singing voice synthesis typically involves generating voice waveforms from musical scores and lyrics. However, teaching a machine to sing without pre-assigned scores and lyrics is a new challenge. The machine has to sing without any knowledge of human language or musical scores, making it a difficult task. The machine learns to create vocal melodies without musical scores or lyrics, making it different from Text-to-Speech (TTS). Three types of singing voice generation tasks are considered: free singer with random noises input, accompanied singer singing over instrumental music, and solo singer using noises to generate 'inner ideas' for singing. This work contributes to unconditioned or weakly conditioned tasks in generating singing voices. This work expands the spectrum of singing voice generation by introducing unconditioned or weakly conditioned tasks. It allows for more freedom in output generation and can utilize a larger training set without the need for labeled and aligned data. This approach may lead to establishing a universal model for singing voice generation. The proposed accompanied singer is one of the first attempts in this direction. The proposed accompanied singer aims to generate singing voices with accompaniment by directly generating the mel-spectrogram representation, bypassing the step of generating scores. This approach addresses the challenges of unsupervised tasks in singing voice generation. The proposed tasks involve unsupervised learning of music structure directly from audio signals using a novel GAN-based architecture. WaveRNN is used as the vocoder to generate audio waveforms, with a focus on mel-spectrogram generation for singing voice. The study utilizes GRUs and dilated convolutions in the generator and discriminator to model music patterns. Vocal source separation is used to prepare data for training accompanied singers. The pipeline for training accompanied singers is illustrated in Figure 2. Having a vocal separation model allows for using multiple audio files for training data, but may lead to artifacts in singing voice generation models. The singing voice generation models may suffer from artifacts of the source separation model. There is no single \"ground truth\" for the accompanied singer, leading to a one-to-many relationship between model input and output. The pipeline involves using source separation to train generators and discriminators in a GAN for singing voice generation. The machine cannot be asked to generate a specific singing voice in response to an accompaniment track for diversity and artistic freedom. The proposed tasks involve using conditional GAN to generate singing voices with high quality and diversity, vocal-like timbre, and emotion expression. Customized objective and subjective metrics are proposed for performance evaluation, including measuring the matchness between the generated vocal track and the accompaniment track. Reproducibility is a major issue, especially for subjective tasks. Reproducibility is a major issue for subjective tasks. The study focuses on using copyright-free instrumental music as conditional signals for testing singers. The generated singing voices will be made available for model comparison, along with the testing conditions and open-source code. The models have various potential applications, such as using the accompanied singer as a backing vocalist and the free singer as a sound source. Samples of the generated singing voices and a Jazz Hiphop song created using the free singer can be found online. A free singer can be listened to at https://bit.ly/2QkUJoJ. The singing voices from a free singer may not sound good, but they should sound like singing voice. Training a free singer involves modeling a distribution P(Y), where Y is a matrix representing features. An accompanied singer generates singing voices that match the accompaniment track. In training an accompanied singer, a model is used to generate vocal tracks that harmonize with, but do not duplicate, the accompaniment track. Mel-spectrograms are used for the vocal track features, while pitch features are extracted from the accompaniment track using a transcription model. The solo singer generates an 'inner idea' and sings according to that, learning a joint distribution P(Y|I)Q(I) where I is a matrix representing the idea sequence. The inner idea can take the form of a chord progression. The distribution Q is modeled by an autoregressive recurrent network for chord progression generation. The solo singer can be seen as a combination of an idea generator and an accompanied singer. The solo singer, acting as both an idea generator and accompanied singer, utilizes information from accompaniment tracks in various forms. To address the complexity of audio spectrograms and lack of supervised data, a new adversarial net is proposed with GRUs, dilated convolutions, and feature grouping. This approach has been successful in blind musical source separation tasks. In a source separation task, a model learns to decompose different sources from a mixture signal. The output spectrograms should be distortion-less and of high audio quality. A stack of GRU, dilated convolution with feature grouping, and group normalization is used as a building block in singer models to consider accompaniment information. The 'D2 block' from previous research uses dilated GRU and weight normalization. The 'D2 block' from previous research used dilated GRU and weight normalization. However, in our study, we found that using group normalization and plain GRUs can help singer models converge easier. Our new blocks, called 'G3 block', consider accompaniment information and can generate voices of arbitrary length. We address the one-to-many issue and absence of supervisory signals by designing a GAN architecture for score and lyrics-free voice generation. Special attention is given to three components in our model. In our study, we focus on voice generation using a GAN architecture. Two existing GAN models for audio generation are discussed, with attention to network architecture, input noises, and discriminator loss function. Generators use 2D convolutions and transposed convolutions with a vector input noise, while discriminators compress input matrices to a single value for prediction. Our study focuses on voice generation using a GAN architecture. Existing models struggle with generating variable-length output due to the need for transposed convolution layers. Our proposed architecture uses G3 blocks and convolutions without strides for both generators and discriminators. Instead of a single noise vector, our models take a sequence of noise vectors with the same length as the desired output. Each column is sampled independently from a Gaussian distribution. The output for each frame depends on the entire sequence of noise vectors due to recurrent GRUs in the model. The discriminator in the GAN model uses recurrent GRUs to enforce consistency in generated frames. Different loss functions are experimented with, including vanilla GAN, LSGAN, and BEGAN. BEGAN's discriminator acts as an autoencoder to reconstruct input, while GAN and LSGAN use a classifier to distinguish real from generated samples. The loss functions for the discriminator and generator in the GAN model are defined based on the feature sequences of real and generated vocal tracks. A variable \u03c4 is introduced by BEGAN to balance the power of the discriminator and generator. Empirical comparison shows that the BEGAN-based approach, G3BEGAN, performs the best. Additionally, an auto-regressive RNN is used to create a chord progression. The auto-regressive RNN chord generator, trained on the Wikifonia dataset, aims to freely generate rhythmically correct chord progressions across different tempo values, time signatures, and keys. Tempo, time signatures, and key information are encoded as the initial hidden state of the RNN, with the chord vector from the last time step concatenated with the beat position as input. The RNN chord generator, trained on Wikifonia dataset, creates chord progressions for testing the solo singer. Mel-spectrograms are used as acoustic features, computed using librosa package, and passed to a WaveRNN vocoder for audio generation. The vocoder is trained from scratch with 3,500 vocal tracks separated from a music collection. The focus is on Jazz music, using a source separation model based on Liu & Yang's architecture. The MUSDB dataset is used for training. The study utilizes the MUSDB dataset for training a source separation model to isolate vocal and accompaniment tracks. Additional Jazz piano solo audio is collected to augment the dataset, resulting in the ability to separate vocal and piano tracks from any song. Female and male Jazz vocal tracks are obtained from 17.4 hours and 7.6 hours of Jazz songs, respectively, and are used for training the model. Sub-clips with less than 40% vocals are removed, resulting in 9.9-hour and 5.0-hour training data for female and male Jazz vocals. The study trains models on female and male Jazz vocals using 9.9-hour and 5.0-hour training data, respectively. Validation sets of 200 and 100 sub-clips are reserved. Models are trained for 500 epochs, with evaluation at the 500th epoch for GAN and LSGAN, and the best convergence rate for BEGAN. Pitch-related information from accompaniment tracks is used to condition vocal track generation. A piano transcription model is implemented to transcribe the separated piano track for this purpose. The study implements a piano transcription model with G3 blocks for transcribing the separated piano track into 88-dimensional frame-wise pitch. The model shows slightly worse note F1 score but better note precision compared to a previous model. Singer models are trained with or without piano accompaniment, and performance evaluation is conducted using collected data. The study collects Jazz music from Jamendo for performance evaluation, applying source separation to extract piano tracks. Chord progressions are generated for evaluation, establishing baselines using objective metrics and existing SVS systems. The study utilizes Sinsy and Synthesizer V as SVS baselines for synthesizing singing voices. Sinsy is used for both objective and subjective tests, while Synthesizer V is used for subjective tests only due to limitations in batch processing MIDI files and lyrics. Lyrics are chosen as multiple 'la' for Synthesizer V, and melodies are transcribed using CREPE from singer training data. The study utilizes Sinsy and Synthesizer V as SVS baselines for synthesizing singing voices. They transcribe vocals from singer training data using CREPE and convert it to MIDI format. Piano transcription from Jamendo testing data is also done, with the piano part transcribed and processed using the skyline algorithm to extract a melody line. Objective evaluation metrics are proposed for assessing the performance of the singer models, including vocalness to determine if an audio clip contains singing voices. Listening to the generated results is encouraged, but objective evaluation is still important for model development and gaining insights into the generation result. The JDC model is used to detect singing voices in audio clips by analyzing vocal activation and pitch contour. Vocalness is determined by the proportion of vocal frames in an audio clip. Average pitch is estimated using CREPE and JDC pitch detection models. The tool is applied to non-silence parts of audio to analyze singing voices excluding accompaniment. The average pitch is computed using the state-of-the-art monophonic pitch tracker CREPE and JDC models. The models do not contain meaningful lyrics, so the baselines also use non-meaningful lyrics like 'la'. Non-silence frames are identified using the librosa function 'effects.signal to frame nonsilent.' The singer models were evaluated using objective measures, including singing-accompaniment matchness. A melody harmonization RNN was used to measure matchness by computing the likelihood of observing a chord sequence given a melody sequence. The average log likelihood across time frames was used as the matchness score. The harmonization model considered symbolic sequences and CREPE was used to transcribe the generated voices. The singer models were evaluated using objective measures, including singing-accompaniment matchness. The model uses CREPE to transcribe voices and Madmom to recognize chord sequences. Results show that the average pitch of generated voices is close to training data, with female voices higher than male voices. Sinsy singing voices tend to have overly high pitches. The models score lower in vocalness compared to Sinsy and training data, but not significantly. Accompaniments in the training set have low vocalness. The accompanied singers score higher than the random baseline and the free singer as expected. The matchness scores of the accompanied singers are close to that of the singer training data. Female singer models learn better than male counterparts, possibly due to the larger training set. The study evaluated three versions of accompanied female singers using mean opinion scores (MOS) from user studies. The final G3BEGAN model scored lower in sound quality, vocalness, expression, and matchness compared to Sinsy and Synthesizer V models. The second user study compared the G3BEGAN model with two existing SVS systems. In the first user study, the 'final' model is compared against two early versions with less training epochs. 39 participants rate singing generated for three accompaniment tracks in a quiet environment with headphones. No post-processing is applied to the audio. The second study is similar but includes five different accompaniments. The second user study includes five different accompaniments and synthesized singing voices. Results show that the model improves with more training epochs, with room for improvement in Sound Quality. Comparing with other systems, Synthesizer V performs the best overall. The study compares Synthesizer V and Sinsy in terms of matchness and expression. Synthesizer V has an advantage in matchness due to synthesized singing voices, while Sinsy's voices do not always align with scores. Synthesizer V also has higher audio quality, which may contribute to its overall score. The study compares Synthesizer V and Sinsy in terms of matchness and expression. Synthesizer V's higher audio quality seems to boost its overall score. HMMs have been effective for the task, with Nishimura et al. (2016) reporting improved naturalness using deep neural nets instead. Many neural network models have since been proposed, with Nishimura et al. (2016) using fully-connected layers to map symbolic features to acoustic features for synthesis. The output features include spectral and excitation parameters, which can be converted into audio using the MLSA filter. Various models have extended this approach, such as using convolutional and recurrent layers for learning the mapping between input and output features. Neural vocoders like WaveNet have also been explored for improving naturalness in audio synthesis. In recent studies, advancements in synthesizing singing voices have been made using models that predict mel-spectrograms directly from lyrics and pitch labels. Techniques like adversarial loss and attention modules have been utilized. Speaker encoders have also been incorporated for multi-singer synthesis. Another model, Synthesizer V, combines deep learning with sample-based concatenation. However, generating singing voices without score or lyrics remains unexplored. The focus is on training models to predict mel-spectrograms without hand-crafted features. In this paper, a novel task of singing voice generation is introduced, focusing on models that do not use musical scores or lyrics. Three singing schemes are proposed: free singer, accompanied singer, and solo singer. A BEGAN based architecture using GRUs and grouped dilated convolutions is presented to generate singing voices in an adversarial way. The models aim to generate variable-length audio, a capability not extensively studied before. The evaluation of singing voice generation models shows room for improvement in audio quality, but success in humanness and emotion expression. Future work includes exploring new ideas like letting the model extract relevant information itself and investigating advanced settings for timbre and expression control. The generator in G3BEGAN uses a stack of two G3 blocks and supports generating chord progressions with various options for keys, tempo, time signatures, and chord qualities. Conditions like key, tempo, and time signatures are encoded into a 40-dimension representation for chord generation. The model in G3BEGAN uses a 40-dimension vector for chord generation, with 3 stacked GRU layers and 512 hidden variables. Training data includes leadsheets from the Wikifonia dataset, augmented by rotating keys for a total of 80,040 leadsheets. The datasets used in the study include Jazz music and other genres, with different processing models applied. The melody harmonization model is adapted from a chord generator, using Wikifonia dataset for training. It generates chord sequences from melody sequences, incorporating chroma representation. Real pairs of melody and chord progression have a matchness score of -7.04\u00b12.91, compared to -13.16\u00b13.72 for randomly paired melodies. Examples of generated singing voices' spectrograms can be seen in Figures 3 and 4. The spectrograms of generated singing voices are shown in Figures 3 and 4. Different GAN losses were experimented for free singer (female), including BEGAN, vanilla GAN, and LSGAN. The output of discriminators in vanilla GAN and LSGAN is a single real/fake value. The discriminators in vanilla GAN and LSGAN have an extra average-pooling layer in the output compared to BEGAN. BEGAN model shows higher Vocalness and better convergence. Generated singing voices from GAN and LSGAN models are mostly noise. Samples of spectrograms show salient pitch contours, with male singers singing lower pitches on average. The male singers in the generated singing voices have lower pitches on average, which can be used to evaluate model convergence."
}