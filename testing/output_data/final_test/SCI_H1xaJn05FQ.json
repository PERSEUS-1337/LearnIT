{
    "title": "H1xaJn05FQ",
    "content": "In this paper, the authors introduce Sliced-Wasserstein Auto-Encoders (SWAE) which use the geometric properties of optimal transport and Wasserstein distances to shape the latent space distribution without the need for adversarial training or a specified likelihood function. The auto-encoder loss is regularized with the sliced-Wasserstein distance between the encoded training samples and a samplable prior distribution. The proposed formulation provides similar capabilities to Wasserstein Auto-Encoders (WAE) and Variational Auto-Encoders (VAE) with a simple implementation. Extensive error analysis and performance on benchmark datasets are provided. The optimal transport problem provides a way to measure distances between probability distributions by morphing one distribution into another. Wasserstein distances, arising from this problem, are true distances that metrize a weak convergence of probability measures. They have gained interest in the learning community due to their unique properties. In this paper, Sliced-Wasserstein auto-encoders (SWAE) are introduced as a new type of auto-encoders for generative modeling. They minimize the sliced-Wasserstein distance between the distribution of encoded samples and a samplable prior distribution. This approach avoids adversarial training in the encoding space and is not limited to closed-form distributions, while still benefiting from a Wasserstein-like distance measure in the latent space. Our approach offers a simple numerical solution to the computationally expensive Wasserstein distance problem. Despite concurrent papers on sliced-Wasserstein distance in generative modeling, our method is novel and distinct. Unlike other methods, we only require a small number of slices (O(10)) for distribution matching in the latent space, as opposed to O(10^4) slices. Additionally, our approach does not rely on adversarial training like other methods. Sim\u015fekli et al. (2018) propose a parameter-free generative modeling approach using sliced-Wasserstein flows. They define the probability density function for input data and use Random Variable Transformation to obtain the density of encoded samples. This method offers a simple solution to the computationally expensive Wasserstein distance problem without relying on adversarial training. In line with VAEs and WAEs, the goal is to encode input data into latent codes for recovery and to match the encoded samples' probability distribution with a prior. The decoder maps latent codes back to the original space, aiming to minimize dissimilarity between input and decoded distributions. Various dissimilarity measures are reviewed for auto-encoders. The existing dissimilarity measures for distributions p X and p Y are briefly reviewed. Different measures, including f-divergences and Wasserstein distances, have been used to compute the dissimilarity between the two distributions. The Fenchel conjugate of the convex function f is utilized to minimize the dissimilarity, leading to a min-max problem equivalent to adversarial training in generative modeling. The OT problem and Wasserstein distances are used to define a distance between distributions p X and p Y in Wasserstein-GAN. The problem is reformulated as a min-max optimization solved through adversarial training. The existence of pairs x n and y n = \u03c8(\u03c6(x n)) is utilized to minimize the discrepancy between p X and p Y. The text discusses minimizing dissimilarity between distributions p Z and q Z by maximizing log-likelihood or minimizing KL-divergence. KL-divergence has limitations for distributions on non-overlapping manifolds, common in neural networks' hidden layers. Various alternatives exist in the literature to address limitations in measuring discrepancy between distributions p Z and q Z. In their WAE paper, two approaches were proposed: GAN-based and MMD-based methods. The GAN-based approach defines a discriminator network to classify samples from 'true' and 'fake' distributions. The paper proposes a min-max adversarial optimization for learning \u03c6 and D Z to classify samples from 'true' and 'fake' distributions. It suggests using the Wasserstein distance between p Z and q Z, utilizing the Kantorovich-Rubinstein duality. The sliced-Wasserstein metric is proposed to measure the discrepancy between the distributions. The sliced-Wasserstein metric is used to measure the discrepancy between distributions p Z and q Z. It eliminates the need for training an adversary network or choosing a data-dependent kernel, providing an efficient and stable implementation. The major difference between learning auto-encoders and GANs lies in the correspondences between high-dimensional point clouds, simplifying the problem for auto-encoders. The sliced-Wasserstein metric measures the discrepancy between distributions without the need for adversarial networks or data-dependent kernels. It simplifies the problem for auto-encoders by handling lower-dimensional point clouds through nonlinear dimensionality reduction. The approach introduces the Sliced Wasserstein auto-encoder (SWAE) by reviewing necessary equations for Wasserstein and sliced-Wasserstein distances. The Wasserstein distance can be defined using diffeomorphic mappings between probability densities pX and pY. Efficient optimization techniques have been proposed to solve this problem, with a closed-form solution available for one-dimensional probability distributions. The cumulative distributions PX and PY can be used to calculate the Wasserstein distance. The sliced-Wasserstein distance is a closed-form solution that simplifies the calculation of Wasserstein distance. It is used for computing barycenters of distributions and point clouds, defining positive definite kernels for distributions, and as a measure of goodness of fit for GANs. The sliced-Wasserstein distance simplifies calculating Wasserstein distance by comparing one-dimensional marginal distributions using the Radon transform and Fourier slice theorem. It is used for computing barycenters, defining kernels, and assessing GANs. The sliced Wasserstein distance simplifies calculating Wasserstein distance by comparing one-dimensional marginal distributions using a closed-form solution. It is a true metric that induces the same topology as Wasserstein distance on compact sets. The SW distance has theoretical guarantees for transportation plans and maps, with an upper bound for p \u2265 2. In our paper, we focus on p = 2, where \u03b1 p,d = 1/d. In the Numerical Implementation Section, a numerical experiment compares W 2 and SW 2, confirming a proposed formulation for the SWAE. The sliced-Wasserstein distance is used to measure the difference between distributions, avoiding additional optimization steps while maintaining favorable characteristics of the Wasserstein distance. The sliced-Wasserstein distance is utilized to measure the discrepancy between probability densities p Z and q Z, maintaining the favorable characteristics of the Wasserstein distance. The numerical details of the approach involve estimating the Wasserstein distance using the midpoint Riemann sum method and empirical distribution functions when only samples are available. The Glivenko-Cantelli Theorem is used to show the convergence behavior of the empirical distribution functions. The Wasserstein distance can be approximated by sorting x m s and y m s and then calculating. The convergence behavior is achieved via Dvoretzky-Kiefer-Wolfowitz inequality bound. Rates of convergence for the p-Wasserstein metric have been extensively studied in mathematics and statistics communities. The p-Wasserstein distance rates of convergence are discussed, with results for E(Wp(pX,M , p X )) and (E(W p p (p X,M , p X ))) 1 p shown for p = 1. Similar bounds can be found for Wp with stricter assumptions on pX and pY. The empirical density p X,M can be estimated from samples x m \u223c p X. Marginal densities are obtained through integration over the unit sphere in R d. The sliced-Wasserstein distance minimization requires a Monte Carlo scheme for approximation. Global minimum for SW c (p Z , q Z ) is also a global minimum for each. Fine sampling of S d\u22121 is necessary. The effect of the number of random slices on approximating the SW distance was investigated using a simple experiment with two random multi-variate Gaussian distributions in a d-dimensional space. The Wasserstein distance for the distributions has a closed form solution, serving as the ground-truth for the analysis. The study investigated the effect of the number of random slices on approximating the SW distance between two Gaussian distributions in a d-dimensional space. The SW distance was measured for M = 1000 samples using different random slices. The experiment was repeated a thousand times for each L and d, with results showing that the scaled SW distance closely follows the true Wasserstein distance. Variance of estimation increases with higher dimensions d and decreases with more random projections L. The study focused on approximating the SW distance between Gaussian distributions in a d-dimensional space by using a large number of random projections. The proposed method involves sampling from a uniform distribution, updating parameters using gradient descent, and solving a min-max problem iteratively. Each iteration of SWAE costs O(LM log(M)) operations. In experiments, three image datasets were used: MNIST, CelebA, and LSUN Bedroom. Different architectures were employed for each dataset, such as auto-encoders for MNIST and DCGAN for CelebA and LSUN. The algorithm was tested on shaping the latent space of the encoder using the MNIST dataset. The study conducted experiments using different datasets and architectures to shape the latent space of the encoder. SWAE successfully embedded the dataset into the latent space while enforcing a close match between two distributions. The convergence behavior of SWAE was compared with WAE-GAN, showing promising results. The loss function remains the same for both SWAE and WAE-GAN. While WAE-GAN generates better random samples for MNIST, it struggles with matching distributions. Mode-collapse issues are observed in GANs. Computational time is similar for both methods. The distribution in the 64-dimensional latent space was set to Normal. FID score statistics at final iteration of training show that CelebA face and LSUN bedroom datasets have higher variations compared to MNIST, requiring a 64-dimensional latent space. Sliced Wasserstein auto-encoders (SWAE) were used with a 64-dimensional latent space for CelebA and LSUN Bedroom datasets. SWAE allows shaping the distribution of encoded samples without adversarial training. Comparison with WAE was done, and interpolation in the latent space was demonstrated. Sliced Wasserstein auto-encoders (SWAE) were used with a 64-dimensional latent space for image datasets like MNIST, CelebA face, and LSUN Bedroom. SWAE demonstrated competitive performance without relying on additional adversarial training, making it suitable for transfer learning and domain adaptation algorithms. The method showed sample convergence behavior compared to WAE-GAN, showcasing its efficiency in encoding target domains in a latent space. The adversarial loss in the latent space does not fully cover the distribution, similar to the 'mode collapse' problem in GANs. SWAE provides a better match between distributions without adversarial training. Comparing Jensen-Shannon divergence with Wasserstein distance, W1(p, q\u03c4) and JS(p, q\u03c4) are shown as functions of \u03c4 in FIG6. The Jensen-Shannon divergence fails to provide a useful gradient when distributions are on non-overlapping domains. To maximize/minimize similarity/dissimilarity between p Z and q Z, we can use the Wasserstein distance measured via the Kantorovich formulation of optimal mass transport problem. The encoding of p X into latent space Z and decoding to p Y provides a unique decomposition of the optimal coupling between p X and p Y. The optimal coupling between distributions pX and pY can be different from \u03b3(x, y) = \u03b4(y \u2212\u03c8(\u03c6(x)))pX(x). This leads to equation 3, where taking the infimum with respect to \u03c6 and \u03c8 results in a non-zero value. The global optima for both Wc(pX, pY) and W\u2021c(pX, pY) is \u03c8(\u03c6(\u00b7)) = id(\u00b7). Trained SWAEs with different latent space dimensions show that a low dimensionality may not contain enough information for crisp image reconstruction. Increasing the dimensionality of the latent space in SWAEs leads to crisper image reconstruction. The sliced Wasserstein distance is used as a measure of goodness of fit, and a discriminant subspace is calculated to separate \u03c8(z) from \u03c8(\u03c6(x)). Linear discriminant analysis is not suitable due to having only two classes, so penalized linear discriminant analysis (p-LDA) is used instead. p-LDA is a method that combines LDA and PCA to solve an objective function involving within class covariance matrix, data covariance matrix, and an interpolation parameter alpha. By using the triangle inequality and empirical convergence bounds, p-LDA can be expressed in a simplified form."
}