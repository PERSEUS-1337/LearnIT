{
    "title": "BJgNJgSFPS",
    "content": "Capsule networks are limited by their parameter-heavy layers and lack of equivariance guarantees. A new variation aims to address these issues by focusing on learning part-whole relationships more efficiently and utilizing a framework that encodes pose-variations. This alternative framework involves projecting into a space-of-variation for each capsule type using a trainable, equivariant function defined over group-transformations. This approach enhances the prediction phase of routing in capsule networks. In this phase, type-homogeneous group-equivariant convolutions of shallower capsules are used to increase parameter-sharing benefits. An equivariant routing mechanism based on degree-centrality is introduced. The model is shown to be equivariant, preserving compositional representation under transformations. Experiments on object-classification datasets demonstrate increased transformation-robustness and general performance compared to capsule baselines. Visual objects are described as instances of class-dependent spatial grammars, with production rules specifying valid type-combinations, inter-geometry, and behavior under transformations. Capsule networks aim to build spatially-aware representations in a multi-class setting by using a parse-tree structure with weighted parent-child edges to denote aggregational relationships. This approach helps mimic latent compositionalities in objects and improve transformation-robustness in object-classification tasks. Capsule networks use vector-valued activations called 'capsules' to represent components of a visual scene at different semantic levels. Deeper capsules are constructed from shallower ones using a combination of backpropagation-based learning and consensus-based heuristics to accurately parse visual scenes and capture spatial relationships. The mechanism of creating deeper capsules involves receiving pose predictions from shallower capsules through trainable neural networks to capture part-whole relationships. These predictions are aggregated through a process called 'routing' to ensure activation only when the capsules are in the right spatial relationship. Multiple routing algorithms, such as dynamic routing, exist for this purpose. Capsule networks offer an alternative to CNNs for learning compositional structures in visual scenes. Unlike CNNs, capsule networks explicitly learn spatial relationships and do not rely on pooling layers for transformation-invariance. This allows them to accurately detect compositional structures by capturing simpler to more complex features as depth increases. Research in capsule networks is still in its early stages, with challenges to overcome before they can rival CNNs. One key issue is the scalability of current capsule-network models to deep architectures, due to the need for unique neural networks to model pair-wise relationships between capsules. This design limitation hinders the potential of capsule networks for broader applicability. The design of capsule networks is criticized for being expensive and inefficient due to not all pairs of capsule-types having significant relationships. This inefficiency leads to poor scalability and limits the ability to model complex objects accurately. To build deeper architectures and more expressive layers, improvements in the prediction phase are necessary. The lack of theoretical guarantees on equivariance in capsule networks is a key issue, as they often rely on heuristics for learning transformation-robust spatial relations. Our model addresses this by encoding capsule types with vector-valued functions, improving scalability and accuracy in modeling complex objects. Our model, called 'space-of-variation' networks (SOVNET), encodes capsule types with vector-valued functions to learn transformation-invariant compositional relationships. The number of trainable networks for a layer depends on the number of capsule-types. The choice of prediction networks and routing algorithm is crucial for guarantees on learning spatial relations. The study introduces group-equivariant convolutions (GCNN) for prediction in capsule networks to ensure equivariance and improve object-structure awareness. Additionally, an equivariant degree-centrality based routing algorithm is proposed to enhance predictions by treating each capsule prediction as a vertex in a graph. The paper introduces a graph-based framework for a scalable capsule-network model, utilizing equivariant convolutions and a degree-centrality-based routing algorithm to aggregate predictions. The method prioritizes predictions that align with the majority, following the principle of routing-by-agreement, while maintaining equivariance and preserving the capsule-decomposition of inputs. The paper presents a graph-based framework for a scalable capsule-network model using equivariant convolutions and a degree-centrality-based routing algorithm. It includes proof-of-concept experiments on various datasets to demonstrate the adaptability of SOVNET architectures to geometric perturbations and their overall performance. The architecture is defined in terms of transformation-equivariance and modeling capsules as functions over transformation groups. The paper introduces a graph-based framework for a scalable capsule-network model using equivariant convolutions and a degree-centrality-based routing algorithm. It focuses on transformation-equivariance and modeling capsules as functions over transformation groups, aiming for equivariance in pose and activation. The notion of equivariance is formalized in terms of preserving transformations from input-space to output-space without loss of information. The model restricts equivariance using the operator L g instead of group-representations, with the operator \u2297 describing the change in representation space specific to the deep learning model used, such as capsule networks and SOVNET. The capsule network model introduces a graph-based framework using equivariant convolutions and a degree-centrality-based routing algorithm. Capsule-types are defined as an output of an agreement-based aggregation of predictions from the preceding layer. A general framework for routing procedures is provided in Algorithm 1, which builds deeper capsules using a weighted sum of predictions. Different methods can be used to obtain the weights for combining predictions meaningfully. The GetWeights function in capsule networks determines the activation of capsules based on consensus among predictions, following the routing-by-agreement principle. The Agreement function evaluates this consensus, with specific model instantiation using group-equivariant convolutional filters and correlation operators. The weights in the routing method are softmaxed degree-scores of affinities among predictions, and capsule activation is normalized using the 'squash' function from dynamic routing. The new routing algorithm introduced serves as an alternative to existing iterative strategies like dynamic routing, with no hyperparameters. The SOVNET layer in Algorithm 2 is group-equivariant with respect to transformations. The operator \u2297 encapsulates the degree-routing procedure with prediction networks for functional depiction of capsule types. The SOVNET layer in Algorithm 2, denoted by the operator \u2297, is group-equivariant with respect to transformations. Equivariance guarantees the preservation of detected compositionalities in a SOVNET architecture, leading to greater robustness in handling data transformations. The SOVNET layer in Algorithm 2 is group-equivariant, preserving detected compositionalities. A capsule-decomposition graph formalizes these compositionalities, connecting capsules based on routing weights. This graph represents the strengths of compositional relationships within the SOVNET model. The capsule-decomposition graph in a SOVNET model is designed to maintain isomorphism between the graph of an input and its transformed variations. This graph captures the compositional relationships within the model by connecting capsules based on routing weights. The input x is represented as G(x) = (V(x), E(x)), where V(x) and E(x) represent the vertex-set and edge-set. The capsule-decomposition graph in a SOVNET model maintains isomorphism between input and transformed variations. The routing procedure is equivariant with respect to the group G, leading to isomorphic results for SOVNET models using Algorithm 2. This section describes the experiments comparing SOVNET architectures to other capsule network baselines. The SOVNET architecture is compared to other capsule network baselines in terms of transformation robustness and classification performance. The first layer of capsules in SOVNET uses a modified residual block with SELU activation and group-equivariant convolutions. More details can be found in the github repository. The primary capsule layer in SOVNET utilizes group-convolution layers and modified residual blocks to generate meaningful capsule-activations without routing. This design reduces the number of prediction networks and trainable parameters, enhancing the network's performance. The SOVNET architecture utilizes group-convolution layers and modified residual blocks in each hidden layer, enhancing prediction mechanisms. Unlike DeepCaps, which uses regular convolution, SOVNET employs degree-routing for improved performance on datasets like MNIST, FashionMNIST, KMNIST, SVHN, and CIFAR-10. The output capsule-layer in SOVNET follows a similar design as the hidden layers, utilizing group-convolutional implementation for predictions. The prediction mechanism in SOVNET utilizes group-convolutional implementation for making class predictions based on the maximum 2-norm of capsule-activations. P4-convolutions are used for robustness to translations and rotations in the first set of experiments, while p4m-convolution is used in the second set for greater ability to learn from augmentations. The architectures are identical except for this difference. The architectures are identical except for the use of different convolutions in SOVNET. Margin loss and reconstruction loss were used to train the networks, with values adjusted for better predictions during training epochs. Reconstruction loss guides the capsule network to build meaningful capsules. The capsule network in SOVNET is trained using margin loss and reconstruction loss, with the latter guiding the network to create meaningful capsules. The loss is weighted by 0.0005, and Adam optimizer is used with an exponential learning rate scheduler. Experiments conducted on SOVNET show its ability to preserve compositionalities under transformations, indicating potential robustness to changes in viewpoint. The model is tested for its capability to handle both train and test-time affine transformations. SOVNET is compared to other capsule networks in handling train and test-time affine transformations on MNIST, FashionMNIST, and CIFAR-10 datasets. Different variations of train and test-splits are created for each dataset, resulting in 25 accuracies per dataset per model. SOVNET is tested against DeepCaps by resizing CIFAR-10 images to 64x64 and applying translations and rotations. SOVNET outperforms other capsule network baselines in handling affine transformations on MNIST, FashionMNIST, and CIFAR-10 datasets. Experiments show higher accuracy and robustness to data transformations compared to Capsnet, EMcaps, DeepCaps, and GCaps. Code from various sources was used for implementation. Additionally, SOVNET was tested against a group-equivariant convolution network (GCNN) and several capsule and convolutional baselines. SOVNET outperforms capsule and convolutional baselines in handling geometric transformations on KMNIST and SVHN datasets. Results show superior performance with standard augmentation techniques. SOVNET is more robust to extreme transformations compared to ResNet models. On CIFAR-10 experiments, SOVNET learns from train-time transformations better than DeepCaps, outperforming it in most cases. However, DeepCaps performs significantly better on untransformed data, generalizing to test-time transformations better due to its higher number of parameters. SOVNET performs better with train-time geometric transforms compared to DeepCaps, which struggles with extreme transformations. GCaps outperforms SOVNET on extreme transformations but struggles with sustaining performance under more extreme train-time perturbations due to its explicit geometric parameterization. While GCaps may yield better results under mild conditions and on simple datasets, the convolutional nature and greater depth of SOVNET allow it to capture more complex features effectively. The greater depth of SOVNET allows it to learn better from complex training scenarios, making a case for deeper models with more expressive prediction mechanisms. G-Caps performs poorly on the CIFAR-10 dataset due to its geometric-based routing, which is not effective for capturing non-geometric features like texture. This observation leads to a hypothesis for improving model performance. The importance of equivariance in neural networks for computer vision is highlighted, with references to AlexNet and group-equivariant convolution. The need for a neural network model that can guarantee equivariance is emphasized. The group-equivariant convolution was proposed to improve performance by increasing parameter-sharing. Various models exhibiting equivariance to different transformations have been introduced, such as rotation-equivariance and translation-equivariance. A general theory of equivariant CNNs was developed to show that convolutions with equivariant kernels are the most general class of equivariant maps between feature spaces. Capsules were proposed as an efficient alternative to group-equivariant convolutional networks, allowing for a linear increase in dimension to achieve equivariance. Most capsule architectures focused on aspects like routing and general architecture, rather than formalizing this concept. In group-equivariant capsules, the idea of efficient equivariance was formalized, changing from preserving the action of a group on a vector space to preserving the group-transformation on an element. However, these models may struggle with compositionalities beyond spatial geometry due to potential representational inefficiencies. SOVNET lacks transformational efficiency but captures non-geometric structures well using convolutions. SOVNET, a scalable equivariant model for capsule networks, utilizes group-equivariant convolutions and degree-centrality routing to preserve compositional structures under transformations. Experimental results demonstrate its superior performance compared to capsule network and convolutional baselines on various classification datasets. Future work aims to explore the tradeoff between transformational efficiency and representation of non-geometric compositional relations. The curr_chunk discusses the role of the routing algorithm in the optimality of the capsule-decomposition graph and the potential wider application of SOVNET to different domains. It also explains the concept of a group representation in relation to a group action on a vector space. The curr_chunk explains the definition of a homomorphism in a vector space and introduces a GCNN prediction network for a SOVNET layer. It also presents the 2-norm of a vector and provides a proof for a theorem related to equivariant convolution. The curr_chunk discusses the equivariance of Algorithm 2 steps, showing that predictions and DegreeScore procedure are equivariant. It highlights the equivariance of maps, non-linearities, and softmax function in obtaining degree-scores. The proof concludes by stating the equivariance of product and sum of equivariant maps. Theorem B.2 pertains to an L-layer SOVNET with activations routed by Algorithm 1. The curr_chunk discusses the equivariance of a routing procedure belonging to Algorithm 1 with respect to group G. It shows that the capsule-decomposition graphs of transformed inputs are isomorphic. The proof involves the bijection between capsule-decomposition graphs and the equivariance of the model. Two experiments were conducted to verify the capsule decomposition-graphs. The experiments conducted verified that the capsule decomposition-graphs of transformed and untransformed images are isomorphic. Capsule-activations and degree-scores were analyzed for different image variations to confirm the mapping defined in the proof. The experiments verified that capsule decomposition-graphs of transformed and untransformed images are isomorphic. Capsule-activations and degree-scores were analyzed for different image variations to confirm the mapping defined in the proof. The results in Table 6 and Table 7 show zero error and unchanged accuracies under transformations for which SOVNET exhibits equivariance. The SOVNET architecture achieves the highest accuracy on MNIST images compared to other recent capsule network models, with an accuracy of 97.01%. When trained with translations and rotations, it achieves a state-of-the-art accuracy of 99.20%. The CAPSNET model achieved an accuracy of 99.20% on augmented versions of MNIST and FashionMNIST, outperforming previous models. The SOVNET architecture on CIFAR100 achieved an accuracy of 71.55%, a significant improvement over the STARCAPS model. Experiments comparing SOVNET with ResNet models on MNIST and FashionMNIST showed promising results. The models were tested on various transformed versions of the test-splits, with results shown in Table 10. SOVNET compares with much deeper CNN models, but further testing on complex datasets and deeper SOVNET models is needed for a better understanding of their performance. Algorithm 1, provided below, evaluates the relative importances of predictions for deeper capsules. Formalization of these concepts into a general framework for routing is essential to cover all possible notions of relative importance. The text discusses the need for a formalization of routing algorithms for SOVNET to cover various notions of relative importance. It provides examples of GetWeights and Agreement functions, focusing on degree-centrality based routing. The DegreeScore procedure assigns weights based on normalized degree centrality scores, emphasizing routing-by-agreement principle. The text discusses routing algorithms for SOVNET, emphasizing the principle of routing-by-agreement. It compares the proposed algorithm with a dynamic routing algorithm, highlighting the difference between \"attention-based\" and \"agreement-based\" routing. The weight associated with a prediction depends on other deeper capsules. The modified general procedure for routing algorithms in SOVNET involves GetWeights taking all predictions as parameters and returning routing weights. The weights cij(g) depend on deeper capsule routing weights. Activation in capsules is denoted by the 2-norm, with dynamic routing not using a separate value for this. Theoretical results and algorithms allow for generalization to other groups with equivariance and preservation of compositionality under equivariant group-convolution. Example includes translation group Z2 and correlation operation, proven equivariant. n-dimensional correlation on Zn is equivariant to translations in n-dimensions. The n-dimensional correlation operator is equivariant with respect to Zn and the group representation L. The degree-centrality based algorithm uses discrete convolutions and remains equivariant when applied to continuous groups like SO(n). The correlation of functions on SO(3) is equivariant to transformations in SO(3) with respect to the group representation L R. The routing algorithm remains equivariant with convolutions despite approximations. The dot-product for SO(3) functions is proven equivariant. The preservation of compositionality is ensured by considering the infinite graph G(x). The routing algorithm remains equivariant with convolutions despite approximations, proven by using the same mapping between vertices and the equivariance of the routing procedure."
}