{
    "title": "BJeMeiCVd4",
    "content": "Deep reinforcement learning algorithms require large amounts of experience to learn a task, while meta-reinforcement learning faces challenges in sample efficiency and task uncertainty. A new off-policy meta-RL algorithm is developed to address these issues by disentangling task inference and control through online probabilistic filtering of task variables for efficient exploration. Our method improves sample efficiency in meta-RL by 20-100X and outperforms prior algorithms on various benchmarks. Learning new tasks quickly is challenging due to the need for extensive data, but exploiting common task structures can help in faster adaptation. Our approach integrates online inference of probabilistic context variables with off-policy RL algorithms to improve sample efficiency in meta-RL. It addresses the challenge of adapting from off-policy data during meta-training, aiming to achieve both adaptation and meta-training data efficiency. Our method, PEARL, integrates probabilistic context variables with off-policy RL algorithms for improved sample efficiency in meta-RL. It adapts quickly by sampling context variables, updating beliefs about tasks, and achieving structured exploration through uncertainty reasoning. PEARL demonstrates 20-100X improvement in meta-training sample efficiency on six continuous control meta-learning environments. Our model, PEARL, enhances sample efficiency in meta-RL by integrating probabilistic context variables with off-policy RL algorithms. It adapts rapidly to new tasks through structured exploration and achieves significant improvement in meta-training sample efficiency on six continuous control meta-learning environments. The actor and critic are meta-learned jointly with the inference network, optimized with gradients from the critic and an information bottleneck on Z. De-coupling data sampling strategies for context and RL batches is crucial for off-policy learning. Prior work has used probabilistic models for meta-learning, adapting model predictions using latent task variables. In RL, the focus is on composing skills via the learned embedding space, while our approach is on adapting to new tasks through structured exploration. BID6 adapts via gradient descent and explores via sampling from the prior in a distribution of tasks p(T ), each being a Markov decision process (MDP). Tasks consist of initial state distribution, transition distribution, and reward function. The meta-training process learns a policy that adapts to tasks by conditioning on past transitions. At test-time, the policy must adapt to new tasks from p(T) using latent probabilistic knowledge. The text discusses how meta-training involves learning to infer a latent probabilistic context variable Z for a new task, and optimizing the policy based on samples from the posterior over Z. An inference network is trained to estimate the posterior p(z|c) by predicting the task state-action value function. The text discusses meta-training by inferring a latent context variable Z for a new task and optimizing the policy based on samples from the posterior over Z. The inference network is trained to estimate the posterior p(z|c) by predicting the task state-action value function. The variational lower bound training objective involves a unit Gaussian prior over Z and the Bellman error for a state-action value function conditioned on z. The inference network should capture task-relevant information without modeling irrelevant dependencies and be permutation invariant for fully observed MDPs. The text discusses meta-training by inferring a latent context variable Z for a new task and optimizing the policy based on samples from the posterior over Z. To keep the method tractable, Gaussian factors are used for a Gaussian posterior. Prior work involves posterior sampling for exploration, where a distribution over MDPs is modeled for efficient task exploration. PEARL meta-learns a prior over Z to capture the distribution over tasks. Sampling z's allows for exploration and hypothesis testing even when immediate task feedback is lacking. The text discusses the challenges of designing off-policy meta-RL algorithms due to the mismatch in data distribution between meta-training and meta-test. Off-policy methods struggle with learning effective stochastic exploration strategies required for meta-RL. Off-policy RL methods struggle with optimizing state distributions, while policy gradient methods have direct control over actions. An off-policy meta-RL method uses a probabilistic model to train the encoder separately from the policy. The context z is treated as part of the state, with exploration provided by uncertainty in the encoder. The actor and critic are trained with off-policy data from a replay buffer, using a sampler to train the encoder. The algorithm is built on top of the soft actor-critic algorithm (SAC), optimizing parameters of the inference network q(z|c) jointly with the actor and critic. The inference network is trained using gradients from the Bellman update for the critic. PEARL is evaluated on six continuous control environments, showcasing sample efficiency and performance. PEARL, a meta-RL method, outperforms existing policy gradient methods by being 20-100x more sample efficient. It leverages off-policy data during meta-training and achieves state-of-the-art performance. Despite attempts to adapt recurrent DDPG, it was unsuccessful. Performance is evaluated through online adaptation at the trajectory level, with results reported after two trajectories. Our approach, PEARL, demonstrates superior performance by being 20-100x more sample efficient compared to previous policy gradient methods. It effectively explores sparse reward MDPs using posterior sampling, as shown in a 2-D navigation task with shaped rewards. In a comparison to MAESN and BID6, our approach shows superior adaptability to new sparse goals with fewer trajectories and samples needed for meta-training."
}