{
    "title": "HkxOoiAcYX",
    "content": "We investigate the evolution of internal representations in deep neural network training to understand the compression aspect of the information bottleneck theory. The theory suggests a rapid fitting phase followed by a slower compression phase, where the mutual information I(X;T) decreases. Previous studies observed compression of estimated mutual information in different DNN models, but the true I(X;T) is either constant or infinite. This work clarifies the discrepancy between theory and experiments by introducing a noisy DNN framework where I(X;T) is a meaningful quantity dependent on network parameters. This framework serves as a good proxy for the original DNN in terms of performance and learned representations. In a study on deep neural network training, the compression aspect of the information bottleneck theory is explored. A noisy DNN framework is introduced to estimate mutual information I(X;T) and observe compression in various models. The compression is driven by the clustering of hidden representations of inputs from the same class in the T space. This work clarifies past observations of compression and highlights the geometric clustering of hidden representations as the true phenomenon of interest. Recent work by BID10 utilizes the Information Bottleneck framework to analyze DNN learning dynamics. The study observes that as DNN layers progress, irrelevant information about the input X is gradually shed, leading to a compression phase that contributes to the generalization performance of DNNs. Additionally, it is suggested that compression is not inherent to DNN training, with the type of nonlinearities used playing a role in whether compression occurs. The study by BID10 examines DNN learning dynamics using the Information Bottleneck framework. It notes that as DNN layers progress, irrelevant information about input X is gradually shed, leading to compression for generalization. However, the plots of I(X; T) evolution across training epochs are misleading as the true mutual information is either infinite or a constant independent of DNN parameters. The mutual information in deterministic DNNs remains constant regardless of parameters, with an injective mapping from X to T for certain nonlinearities. Compression observed in BID10's study is not due to changes in mutual information, but rather a discretization issue in approximating I(X; T). The discrepancy between theory and experiments arises from the unjustified discretization of neuron values in the computation of I(X; Bin(T)). The activity of T is divided into user-selected bins, which are highly sensitive to bin size. Compression results based on this binning method are observed in various cases. To track the flow of information in DNNs, a stochastic parameterized channel is needed, with intrinsic stochasticity in the DNN operation. Several criteria are identified for this framework to provide meaningful insights into practical systems. The proposed stochastic DNN framework adds i.i.d. Gaussian noise to each neuron's output, ensuring the data processing inequality is satisfied and reflecting true operating conditions. This approach aims to relate mutual information to learned internal representations, with the noise being centered and isotropic. The proposed stochastic DNN framework adds i.i.d. Gaussian noise to each neuron's output, ensuring the data processing inequality is satisfied. Experimental results show that the addition of noise does not significantly affect the DNN's learned representations and performance for certain variances. Randomness during training has been used to improve neural network performance in various ways. The estimation of I(X; T) is decomposed into simpler instances using a sampling technique in Section 3. Theoretical analysis shows that a new estimator for differential entropy over noisy DNNs requires exponentially many samples in dimension d. A proposed approach using I(X; Bin(T)) outperforms general-purpose estimators, converging at O(log n) d/4 / \u221a n. During training of small DNN classifiers, the rate of general-purpose differential entropy estimators shows compression in many cases. This compression is related to data transmission over AWGN channels, where representations of inputs from the same class cluster together and become indistinguishable at the channel's output, decreasing the transmitted information. Deeper layers in the DNN tighten these clusters, improving the representation of X for Y. The text discusses how clustering in deterministic DNNs improves the representation of X for Y. It explains that clusters of inputs in learned representations typically form in both noisy and deterministic DNNs. The binned mutual information measures clustering, not compression, by hidden representations. The geometric clustering of hidden representations is the fundamental phenomenon of interest, with future work aiming to test its connection to generalization performance. The text discusses training convolutional neural networks (CNNs) on MNIST dataset with different levels of internal noise, including no noise and dropout. The models were evaluated based on total validation errors, showing mean \u00b1 standard deviation over multiple random seeds. The study aimed to explore the relationship between noisy and deterministic DNNs in current machine learning practices. The CNNs were trained with varying levels of internal noise, including dropout instead of additive noise. Performance was evaluated on the validation set, and cosine similarities between internal representations were analyzed. Results showed that small amounts of additive noise had minimal impact on classification, while dropout significantly improved performance. The histograms indicated that noisy and dropout models learned similar representations to the deterministic model. Increased noise led to higher dissimilarity, especially in internal layers. The study analyzed the impact of noise on CNNs, showing that small amounts had minimal effect on classification, while dropout improved performance. Noisy and dropout models learned similar representations to the deterministic model, with higher noise leading to increased dissimilarity, especially in internal layers. Mutual Information was used to describe the stochastic map induced by noisy DNNs. The study focuses on estimating mutual information between variables X and T in a complex distribution using a sample propagation estimator. The goal is to estimate entropy terms h(p T ) and h(p T |X=x) in high dimensions, which proves challenging. The estimator approximates true entropy using a known Gaussian mixture, leveraging the ability to propagate samples up DNN layers and known noise distribution. The study aims to estimate entropy terms h(p T ) and h(p T |X=x) in high dimensions using a sample propagation estimator. It leverages a known Gaussian mixture to approximate true entropy, utilizing DNN layers and a noise distribution for sample propagation. Monte Carlo integration is used for numerical evaluation due to the lack of a closed-form expression for the entropy of a Gaussian mixture. The study utilizes a sample propagation estimator to estimate entropy terms in high dimensions. It leverages a Gaussian mixture to approximate entropy, using DNN layers and a noise distribution for sample propagation. Monte Carlo integration is employed for numerical evaluation due to the lack of a closed-form expression for the entropy of a Gaussian mixture. The SP estimator for entropy in high dimensions requires exponentially many samples, but its absolute-error risk converges satisfactorily. It involves computing the differential entropy of a Gaussian mixture using Monte Carlo integration for numerical approximation. The unbiased approximation of the expectation achieves a negligible error for the SP estimator. Practical guidelines are provided for selecting noise standard deviation and number of samples for estimating I(X; T) in a classifier. Internal noise can serve as a regularizer, and sometimes a higher noise value is needed for accurate estimation. The number of samples needed for accurate estimation of mutual information can vary depending on factors like layer dimensionality and data availability. The theoretical bound for selecting the number of samples may be too pessimistic in practice, so empirical testing and adjustments are necessary to ensure computational feasibility without compromising estimation accuracy. Estimation accuracy was maintained through empirical testing, with a concrete example showing the need for a large number of samples. Theoretical bounds were adjusted for computational feasibility without compromising accuracy. The connection between compression and clustering was explored from an information-theoretic perspective. The information-theoretic perspective on the connection between compression and clustering is illustrated through the concept of mutual information in an AWGN channel. By reducing point spacing in the input constellation, the distinguishability of symbols under Gaussian noise is directly impacted. Training a neuron using mean squared loss and gradient descent with specific parameters further demonstrates this concept. The behavior of I X; T (k) is illustrated using squared loss and gradient descent with a fixed learning rate. The Gaussian mixture p T (k) is plotted across epochs, showing the convergence of Gaussians as they meet the tanh boundary. The mutual information trend reflects the merging of Gaussians into the first one, indicating a connection between clustering and compression. The X \u22121 Gaussians merge in two stages as w grows, becoming indistinguishable for larger \u03b2. Observations from minimal examples hold for larger networks, showing compression of mutual information in noisy networks is driven by clustering of internal representation. Deterministic networks also cluster samples despite constant I(X; T). Two DNNs considered are the small, fully connected network (SZT model) and a convolutional network for MNIST classification (MNIST CNN). Selected results are presented with additional details in the supplement. The FCN model BID10 for binary classification of 12-dimensional inputs using a fully connected architecture was tested with different nonlinearities. The saturation of tanh nonlinearities led to clustering of Gaussians, reducing mutual information. Regularization with orthonormal weight matrices reduced compression. The results are shown in FIG3. The scatter plots in FIG3 show that most neurons do not saturate, leading to no clustering observed at later training stages. Compression can occur without saturation, impacting classification accuracy. The relationship between compression and generalization performance is complex, with test loss increasing while compression occurs in some layers but not others. Further examination of this topic is warranted in future research. In future work, the focus is on clustering as a source of compression in neural networks. Histograms of pairwise distances between samples' representations show tight clustering in unnormalized models but not in normalized models. The discrete entropy of Bin(T) is used to measure clustering, with the number of bins, B, as a tuning parameter. The discrete entropy of Bin(T) is used to measure clustering in neural networks, with the number of bins, B, as a tuning parameter. The distribution of hidden representations in bins reflects the level of clustering, with a clear correspondence between H Bin(T) and I(X; T) observed in SZT models. The discrete entropy of Bin(T) is used to measure clustering in neural networks, with the number of bins, B, as a tuning parameter. H Bin(T) reveals different clustering granularities depending on the bin size. In the MNIST CNN model with dropout, H Bin(T) remains approximately ln(10000) = 9.210 even with only two bins, showing near-injective behavior. In the MNIST CNN model, histograms of pairwise distances between samples in different layers show that fully connected layers 3 and 4 effectively separate within-class and between-class distances after one epoch of training, while convolutional layers 1 and 2 are more generic. In the context of the MNIST CNN model, the analysis of internal representations reveals clustering in a larger convolutional network. The study reexamines the compression aspect of the Information Bottleneck theory, showing correlations between I(X; T) and H Bin(T) as additional measures for clustering. Pairwise distances between samples demonstrate evidence of clustering in the network. The study reexamines the compression aspect of the Information Bottleneck theory in deterministic DNNs with fixed weights. It identifies clustering of learned representations as the source of compression and demonstrates that past compression-related experiments were measuring this clustering through binned mutual information. Binning-based measures, while not accurately estimating mutual information, are useful for tracking changes in clustering, the true effect of interest in deterministic DNNs. Further study of geometric phenomena driven by DNN training is recommended. The study explores geometric phenomena in DNN training to understand learned representations and their connection to generalization. It focuses on the mutual information between random variables A and B, with a joint distribution P A,B. The scenario of interest involves a discrete variable A and a continuous variable B, simplifying the mutual information calculation. The differential entropy of a continuous random variable C is also defined in the context of the discussion. The mutual information in C as DISPLAYFORM3 can be expressed as DISPLAYFORM4, with the conditional differential entropy of B given A denoted by h(B|A). An example is provided to illustrate the relation between clustering and compression of mutual information using non-saturating nonlinearities. The network is trained with specific parameters to show the connection between Gaussians' motion and mutual information. The first layer clusters elements in X1/4, with the output negated using w2 < 0. The second layer gradually raises the clustered bundle X1/4 by growing b2, causing elements to split and coalesce. Mutual information of the layers shows merging and spreading of elements in X1/4. The evolution is tracked by bounds on I X; T (k). The experimental details for the SZT model in Section 5 include regularization of network weights with an orthonormality constraint. Additional results in FIG0 show insights into clustering and compression phenomena for tanh and ReLU nonlinearities, especially under high variance additive noise. The additive noise with high variance leads to mild compression effects, as shown in the histograms and scatter plots. Increasing the noise parameter results in wider Gaussians, causing clusters to merge and reduce mutual information. In contrast, smaller noise parameters maintain distinguishable Gaussians within clusters. In the low-beta regime, clustering Gaussians and compression are strongly related. However, in the presence of large noise, these phenomena decouple. Results for ReLU activation without weight normalization show almost no compression, attributed to weight matrix regularization. The reduction in compression with ReLU activation is due to weight matrix regularization. While clustering at the origin promotes compression, the spread of Gaussians along positive axes counterbalances this effect, leading to a constant mutual information profile. The weight-normalized ReLU behavior in FIG0 is similar to FIG0, with bounded weight growth and reduced saturation around the origin. Mutual information increases for layers 4 and 5, then flattens as Gaussians move away from the origin but remain bounded, decreasing clustering density. A slight compression occurs when Gaussians can't move further along positive axes. Results for a synthetic spiral data example with a SZT model architecture are presented in this section. The architecture of the MNIST CNN models used in Sections 2 and 5 in the main paper is described in detail. The CNNs were trained using PyTorch BID28 version 0.3.0.post4 and follow a standard architecture with two convolutional layers, two fully connected layers. The results show a connection between clustering and compression, with an estimate of entropy revealing clustering granularity. The architecture of the MNIST CNN models includes two convolutional layers, two fully connected layers, batch normalization, Tanh() activation function, Gaussian noise or dropout, and max-pooling. The first convolutional layer has 16 output channels with 5x5 kernels, while the second has 32 output channels. The fully connected layers have 128 and 10 outputs respectively. The MNIST CNN models include two convolutional layers, two fully connected layers, batch normalization, Tanh() activation function, Gaussian noise or dropout, and max-pooling. The fully connected layers have 128 inputs and 10 outputs. Training uses cross-entropy loss with stochastic gradient descent, 128 epochs, and 32-sample minibatches. Data augmentation is applied to the training set by translating, rotating, and shear-transforming each example. The MNIST CNN models consist of convolutional and fully connected layers with batch normalization, Tanh() activation, Gaussian noise, and dropout. Training involves cross-entropy loss, stochastic gradient descent, and data augmentation. Eight models are trained with different noise conditions, ensuring comparable internal representations for cosine similarity measurement. At test time, models are deterministic without noise or dropout layers. In the main paper, layers are defined based on specific steps in the model. The entropy estimators aim to estimate h(p S * \u03d5) using samples from S \u223c p S while knowing \u03d5. Performance guarantees for the SP estimator are stated in this section, with detailed proofs available in a referenced work. The minimax absolute-error risk is calculated over a set of distributions, with an estimator \u0125 based on empirical data S n. The subgaussian random variable X satisfies tail conditions or super-exponential moment properties. The subgaussian norm X \u03c82 is defined as the smallest constant K 2. This requirement is naturally met by noisy DNN frameworks. It includes discrete distributions over a finite set and distributions of the random variable S = f(T^-1) in a noisy ReLU DNN. The input to the network is subgaussian, which allows for an upper bound on the formula in Definition 1. The constant K2 depends on various factors including the network's weights, biases, depth of hidden layers, input subgaussian norm, and noise variance. This assumption is satisfied by the distribution of X considered. The sample complexity is shown to be exponential in d. The sample complexity is exponential in d, with a lower bound for estimation in large dimensions. The exponent \u03b3(\u03b2) decreases with larger \u03b2 values, favoring estimation. Theorems 2 and 3 establish the inevitability of exponential sample complexity and provide lower bounds for small noise variances in any dimension. Theorem 3 holds for all \u03b2 < 0.08 in d = 1. The SP estimator's expected absolute error decays like O \u221a n for all dimensions d, with explicit constants showing exponential dependence on dimension. The absolute-error risk of the SP estimator over the class Fd is bounded for all n sufficiently large. The SP estimator's expected absolute error decays like O \u221a n for all dimensions d, with explicit constants showing exponential dependence on dimension. The estimator over the class Fd is bounded for all n sufficiently large, with explicit and implicit upper bounds on the minimax absolute-error risk. The SP estimator achieves fast convergence rates for estimating differential entropy over DNNs with bounded activation functions. Theorem 1 provides convergence rates for networks with unbounded nonlinearities. The estimator over the nonparametric class Fd,K is bounded for all sufficiently large n. The class F(SG)d,K is general and includes Fd when K \u2265 1. Theorem 5 provides an upper bound on minimax risk under the setup of Theorem 1. The expressions with K = 1 have the same convergence rates, and their constants are very close. Faster convergence rates than 1 \u221a n cannot be achieved for parameter estimation under absolute-error loss. The convergence rate of the SP estimator is near minimax rate-optimal for parametric estimation, as established in Theorems 1 and 5. The error of the mutual information estimator vanishes as n \u2192 \u221e, and the results provide worst-case convergence rates over a nonparametric class of distributions. In practice, convergence may be faster than these worst-case rates. The bias of the SP estimator cannot be empirically tested, even with multiple estimations consistently producing similar values. A lower bound on the number of samples needed to avoid biased estimation is presented, with the bias bounded by a given \u03b4 > 0 as long as n \u2264 k. The number of estimation samples n should be at least 2 for negligible bias. The mutual information estimator requires n \u2265 2 for negligible bias. It involves computing the differential entropy of a Gaussian mixture, approximated using MC integration. The method is presented for an arbitrary Gaussian mixture without specific notation. Monte Carlo integration is used to compute h(g), where h(g) = -E log g(\u00b5_i + Z) for each i \u2208 [n]. The Q-function is defined as Q(x). The MSE bounds for tanh and ReLU networks scale linearly with dimension d. Empirical results compare the SP estimator to KDE-based and kNN-based estimators. The distribution P of S is a mixture of Gaussians truncated to [-1, 1]. The kernel width for the KDE estimate was chosen via cross-validation, varying with both d and n; the kNN estimator and \u0125 SP (S n) require no tuning parameters. The KDE estimate is highly sensitive to the choice of kernel width, with optimized values shown in the curves. Both the kNN and KDE estimators converge slowly, degrading with increased d, worse than \u0125 SP."
}