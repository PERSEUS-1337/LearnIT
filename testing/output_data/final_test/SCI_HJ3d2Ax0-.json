{
    "title": "HJ3d2Ax0-",
    "content": "The paper analyzes the impact of depth on the ability of recurrent networks to express correlations over long time-scales. A new measure called the Start-End separation rank is introduced to quantify the information flow across time supported by the network. The Start-End separation rank measures the ability of deep recurrent networks to model long-term dependencies by correlating different parts of the input sequence. Deep networks support exponentially higher separation ranks compared to shallow networks, showing an advantage in modeling correlations as the input sequence extends. This attribute can be extended to other RNN architectures like LSTM networks. Recurrent Neural Networks (RNNs) have become the prominent architecture for modeling sequential data, including language modeling, neural machine translation, and speech recognition. Recurrent Arithmetic Circuits (RACs) merge the hidden state with the input via Multiplicative Integration, supporting long-term correlations in inputs. The ongoing empirical effort to apply recurrent networks to complex tasks includes enhancements like LSTM networks and deep recurrent networks, which show superiority over shallow ones in processing information hierarchically at every time-step. Deep recurrent networks model correlations corresponding to longer time-scales than shallow ones, implying depth advantages in complexity and temporal capacity. The paper addresses this issue by showing depth efficiency in recurrent networks, highlighting their ability to handle varying inputs. The paper discusses the depth efficiency in recurrent networks, emphasizing their ability to handle varying input sequence lengths. It introduces a recurrent arithmetic circuit (RAC) to explore the relationship between depth and the modeling of long-term dependencies. The paper introduces Recurrent Arithmetic Circuits (RACs) as a depth-efficient model for handling long-term dependencies in neural networks. RACs are connected to theoretical models like Convolutional Arithmetic Circuits and Multiplicative RNNs, showing improved performance over existing RNN models. The study also links RACs to the Tensor Train decomposition for obtaining results. In section 3, the paper introduces the Start-End separation rank as a measure of a recurrent network's ability to model long-term dependencies. The separation rank measures the distance from separability in analyzing the long-term correlations of a function over sequential inputs. The paper introduces the Start-End separation rank as a measure of a recurrent network's ability to model long-term dependencies. It shows that deep networks have exponentially higher separation ranks than shallow networks, allowing them to capture more elaborate input dependencies over longer periods of time. Additionally, the separation rank of deep recurrent networks grows exponentially with sequence length, while shallow networks are limited in modeling long correlations. The paper introduces the concept of Start-End separation rank to measure a recurrent network's ability to model long-term dependencies. Deep networks have exponentially higher separation ranks than shallow networks, allowing them to capture more elaborate input dependencies over longer periods of time. Additionally, the separation rank of deep recurrent networks grows exponentially with sequence length, while shallow networks are limited in modeling long correlations. The study presents a quantitative conjecture on the advantages of depth in recurrent networks and introduces Recurrent Arithmetic Circuits (RACs) as a class of networks with similar architectural features to standard RNNs. Sequential data processing in recurrent neural networks (RNNs) involves mixing information from previous time-steps with new data. Recurrent Arithmetic Circuits (RACs) utilize algebraic properties to model long-term dependencies in inputs. The framework of shallow recurrent networks is described for sequence classification tasks, with the output being class score vectors. The shallow recurrent network with hidden channels is used for sequence classification tasks, outputting class score vectors. The input is mapped to vectors in an initial encoding step, and the network's output at each time step is determined by learned parameters and a non-linear operation. The newly introduced class of RACs uses Multiplicative Integration for merging input and hidden state, with each layer in deep recurrent networks acting as a recurrent network receiving the hidden state of the previous layer as input. The output of the depth L recurrent network with R hidden channels in each layer is constructed using learned parameters. The input and hidden weights matrices are used for calculations, with a non-linear operation determining the type of deep recurrent network. The newly presented class of RACs is considered a good surrogate for common RNNs. The newly presented class of Recurrent Arithmetic Circuits (RACs) is seen as a good surrogate for common RNNs due to structural similarities and performance advantages over existing RNN models. The algebraic properties of RACs are utilized in the following sections to explore their potential in recurrent networks. In the following sections, the algebraic properties of Recurrent Arithmetic Circuits (RACs) are used to analyze the benefits of depth in recurrent networks. The Start-End separation rank is introduced as a measure of information flow across time in recurrent networks, tied to the concept of grid tensors for analyzing long-term temporal dependencies. Depth in recurrent networks is shown to exponentially enhance their ability to model complex temporal relationships. The Start-End separation rank quantifies a function's distance from separability with respect to two disjoint subsets of its inputs in recurrent networks. It is defined as the minimal number of summands that together give the function, where each summand is separable with respect to the \"Start\" and \"End\" inputs. The Start-End separation rank quantifies a function's distance from separability with respect to two disjoint subsets of its inputs in recurrent networks. It is connected to various applications such as chemistry, particle engineering, and machine learning. The separation rank is linked to the L2 distance of the function from separable functions and is also tied to quantum entanglement measures for many-body quantum systems. If the Start-End separation rank of a function is 1, then the function is separable and cannot model any interaction. The Start-End separation rank quantifies a function's distance from separability in recurrent networks. If the separation rank is equal to 1, the function is separable and cannot model any interaction between inputs arriving at different points in the sequence. The higher the separation rank, the more the function models dependency between the beginning and end of the input sequence. Deep RACs support exponentially larger Start-End separation ranks compared to shallow RACs, making them better suited for modeling long-term temporal dependencies. The use of grid tensors helps evaluate these separation ranks by representing tensors as multi-dimensional arrays with different modes and dimensions. The tensor product is a fundamental operator in tensor analysis denoted by \u2297, which combines two tensors to create a new tensor. Matricization of a tensor A w.r.t. the partition (S, E) arranges tensor elements into a matrix. Shallow RACs compute the score of a class c at time T using a recursive function given by equations 1 and 3. The shallow RAC weights tensor, essential for computing class scores, can be expressed in closed form as a Tensor Train (TT) construction. It involves tensor products related to the Multiplicative Integration property of RACs and the multiplication of hidden states by hidden weights at each time-step. The shallow RAC weights tensor, known as a Tensor Train (TT) decomposition, is analogous to a Matrix Product State (MPS) Tensor Network in quantum physics. Grid tensors are a form of function discretization, where the function is evaluated on a large grid and stored in a tensor. The grid tensors of functions realized by recurrent networks help calculate separation ranks and determine the benefits of network depth. The tensorial structure of a function realized by a shallow RAC can be tied to its separation rank through its grid tensor. If initial mapping functions are linearly independent and measurable, template vectors exist to satisfy certain conditions. The grid tensors of functions realized by recurrent networks help calculate separation ranks and determine the benefits of network depth. The equality between the Start-End separation rank and the rank of the matrix obtained by the corresponding grid tensor matricization is established. This equality is useful in proving main results, as grid tensors are a tool to bound the separation rank. The computation performed by a deep RAC involves data duplication and cannot be written in a closed tensorial form like that of a shallow RAC. This implies that the equality shown in claim 1 does not hold. The relation between a function's Start-End separation rank and the rank of the matrix obtained by the corresponding matricization is discussed. Claim 2 provides a lower bound on the Start-End separation rank of functions realized by deep RACs, showing an exponential enhancement compared to shallow RACs. The paper discusses the exponential enhancement of Start-End separation rank in deep recurrent networks compared to shallow ones. Theoretical contributions include a result separating memory capacity between deep and shallow networks, a quantitative conjecture on memory capacity, and Theorem 1 stating correlations modeled in deep networks. The correlations modeled between the beginning and end of input sequences in deep recurrent networks are exponentially more complex compared to shallow networks. This is due to the enhanced ability of deep networks to model long-term temporal dependencies in sequential input. Theorem 1 demonstrates depth efficiency in deep recurrent networks, showing that shallow networks would require exponentially more parameters to implement the same function. This is due to the enhanced ability of deep networks to model long-term temporal dependencies in sequential input. Theorem 1 demonstrates depth efficiency in deep recurrent networks, showing that shallow networks would require exponentially more parameters to implement the same function. This is due to the enhanced ability of deep networks to model long-term temporal dependencies in sequential input. The functions realized by deep networks represent more elaborate correlations over longer periods of time compared to shallow networks. Shallow recurrent networks are much more restricted in modeling long-term correlations, as indicated by the increasing Start-End separation rank in deep networks with sequence length T. The rank of a shallow RAC function is equal to the rank of the corresponding shallow RAC weights tensor matrix, which can be proven using the TT-decomposition. The rank is bounded by the matrix dimension, and the min-cut separating S from E in the Tensor Network graph represents the rank of the matrix obtained by matricizing any tensor according to a partition. The TT-decomposition in eq. 7 implies that the min-cut in the Tensor Network graph is equal to the TT-rank R. Claim 2 states that the Start-End separation rank of a depth L = 2 RAC function is lower bounded by the rank of the corresponding grid tensor matricization matrix. The rank is bounded by the matrix dimension, and finding an example where the rank exceeds the lower bound suffices to prove the theorem. The weight assignment results in a matricized grid tensor with a rank achieving the upper bound, as shown in Theorem 1. The theorem provides a lower bound on the Start-End separation rank of depth L = 2 recurrent networks, separating deep from shallow networks exponentially. Trivial weight assignments in higher layers impact the Start-End separation rank. The weight assignment in deep recurrent networks impacts the separation rank. A conjecture suggests that memory capacity grows exponentially with network depth, supported by a combinatorial perspective. Tensor Networks visualize this computation, with details in the appendix. The conjecture proposes that memory capacity increases exponentially with network depth, illustrated through Tensor Networks. The computation graph showcases the weight matrices' values and the integration of inputs in a depth-dependent manner. The basic unit connecting \"Start\" and \"End\" inputs is highlighted in red to estimate the separation rank of a deep recurrent network. The lower bound on the Start-End separation rank of a depth L > 2 recurrent network is estimated by finding a specific instance of network parameters. A specific assignment of network weights is identified, where the Tensor Network resembles a basic unit connecting \"Start\" and \"End\" raised to the power of its repetitions in the graph. This basic unit involves a simple computation represented by a grid tensor with a rank upper bounded by a certain value. Conjecture 1 suggests an exponential separation in memory capacity between recurrent networks of different depths, beyond the advantage already proven for deep networks over shallow ones. The number of repetitions of the basic unit connecting \"Start\" and \"End\" in the deep RAC Tensor Network graph is equal to a specific value for any depth L. This result can further enhance the understanding of the benefits of depth in recurrent networks. In this paper, the authors explore the advantages of depth in recurrent networks, focusing on the concept of 'time-series expressivity' to quantify the memory capacity of these networks. They introduce the Start-End separation rank as a measure of the ability of recurrent networks to model long-term temporal dependencies. This metric aims to capture correlations in convolutional networks and address the remarkable performance of recurrent networks on long input sequences. The proposed measure, Start-End separation rank, quantifies the ability of recurrent networks to correlate incoming sequential data over time. Analyzing Recurrent Arithmetic Circuits, it was found that deep RACs show exponential increase in separation rank with input sequence length, while shallow RACs do not. Depth in recurrent networks significantly enhances their ability to model long-term dependencies, utilizing tools from measure theory, tensorial analysis, combinatorics, graph theory, and quantum physics. The analysis extends to other architectural features in modern recurrent networks, including different variants of LSTM networks. The observation that correlations in vanilla shallow recurrent networks do not adapt to sequence length highlights the potential for new insights. Shallow layers are linked to short time-scales like phonemes and words, while deeper layers support longer correlations. Theoretical analyses may lead to practical applications, as shown in experiments by Hermans and Schrauwen (2013). The curr_chunk discusses the role of different layers in recurrent networks in modeling temporal correlations, with a focus on the number of hidden channels in deep convolutional networks. The conjecture presented suggests that the Start-End separation rank of recurrent networks grows exponentially with depth, leading to enhanced memory capacity. This work is seen as a significant step towards developing new methods for improving recurrent networks. The curr_chunk introduces Tensor Networks (TNs) as a method to match recurrent network architecture to temporal correlations in sequential data. It discusses TN construction for shallow and deep Recurrent Autoencoders (RACs) and presents a conjecture on the growth of RACs' memory capacity with depth. The text discusses the growth of RACs' memory capacity with depth using Tensor Networks (TNs), where each node corresponds to a tensor with edges representing different modes of the tensor. The weight of each edge, known as bond dimension, is equal to the dimension of the tensor mode. The connectivity properties of a TN are then presented. The text explains the connectivity properties of a Tensor Network (TN), where edges represent operations between tensors. Contracted indices are edges connecting nodes, while open indices have one loose end. The entire TN can be calculated by summing over contracted indices. An example shows multiplying a vector by a matrix in a TN, resulting in an order 1 tensor. The shallow recurrent network computes the output at time T using a temporal concatenation of a unit cell, involving input and hidden weights matrices contracted with input and hidden state vectors. The final component is the \u03b4 tensor, an order 3 tensor. The \u03b4 tensor in the shallow recurrent network is represented by a triangular node and defined by a recursive relation involving matrix multiplication. This tensor plays a key role in defining the operation of the network. The restricted \u03b4 tensor in the shallow RAC is crucial for element-wise multiplication. The tensor network representing the shallow RAC weights tensor A can be simplified using a Tensor Train (TT) decomposition, reducing the number of parameters needed. The Tensor Train (TT) decomposition of rank R is used in tensor analysis. The shallow recurrent network can be analyzed using min-cut analysis for information flow. The computation of a deep recurrent network using TNs is more complex due to reusing hidden states. The Tensor Train (TT) decomposition of rank R is utilized in tensor analysis. The complexity of computing a deep recurrent network using Tensor Networks (TNs) increases due to the reuse of hidden states. The operation of duplicating a vector in TNs is impossible to represent, as stated in Claim 3. The complexity of computing a deep recurrent network using Tensor Networks (TNs) increases due to the reuse of hidden states. Claim 3 states that duplicating a vector in TNs is impossible to represent. However, a workaround is to duplicate the input data itself to model the duplication present in the network computation. The TN construction of deep RACs involves duplicating input data to bypass claim 3's restriction. The TNs grow exponentially in size as the recurrent network depth increases, but the actual network size grows linearly. The TNs are a theoretical tool for analysis, not a suggested implementation scheme for deep recurrent networks. The exponential growth in TN size for deep recurrent networks allows for modeling intricate correlations over longer periods. Input duplication process involves duplicating segments of TN to calculate hidden state vectors. The input duplication technique in deep recurrent networks involves inserting f(x1) twice to achieve the same hidden state h1,1 twice in the tensor network. This leads to a fractal structure with self-similarities, as shown in the example for L=3 in FIG6. The duplication of intermediate hidden states adds complexity to the network. The fractal structure of deep recurrent networks involves duplicating TNs representing RACs of depth L-1, leading to increased complexity with depth. This construction allows for investigating the impact of network depth on modeling long-term dependencies. The conjecture on the Start-End separation rank of deep RACs is motivated by this analysis, utilizing TN visualizations for a formal representation. The conjecture on the Start-End separation rank of deep RACs relies on finding a specific instance of network parameters to establish a lower bound. Claim 2 and lemma 1 are combined to show that the rank of the matrix obtained by grid tensor matricization is a lower bound on the separation rank. Lemma 1 states that finding an example where the rank of the matricized grid tensor exceeds the desired lower bound confirms the conjecture. The text discusses finding a weight assignment that separates layers in a neural network, simplifying computations in deeper layers. An example of a tensor network corresponding to a deep RAC is shown, and the rank of the matrix is evaluated for Start-End separation. The text discusses the impact of graph segments involving only \"Start\" set indices on the matrix rank under certain conditions. It shows how these segments affect the effective TN in a deep RAC after a certain number of time-steps. The dependence of the TN on time-step indices outside the basic unit may increase the rank. The effective TN is determined by the number of repetitions of the basic unit separating \"Start\" and \"End\" indices. The text proves a claim about the exponential increase in the number of repetitions of a basic unit in the TN graph with the depth of the RAC. It focuses on the computation performed by an RAC with specific parameters and the occurrences of the basic unit in layer L = 1. The contribution of deeper layers and the tensor V reflect the impact of the \"Start\" set indices. The text discusses the exponential increase in the number of repetitions of a basic unit in the TN graph with the depth of the RAC. It focuses on the computation performed by an RAC with specific parameters and the occurrences of the basic unit in layer L = 1. The contribution of deeper layers and the tensor V reflect the impact of the \"Start\" set indices. The number of multiplications implemented by the chain of products in the expression is calculated to determine the repetition of the basic unit in the computation. The text discusses the exponential increase in repetitions of a basic unit in the TN graph with the depth of the RAC. It focuses on the computation performed by an RAC with specific parameters and the occurrences of the basic unit in layer L = 1. The contribution of deeper layers and the tensor V reflect the impact of the \"Start\" set indices. The number of multiplications implemented by the chain of products in the expression is calculated to determine the repetition of the basic unit in the computation. The lower bound presented in conjecture 1 is obtained by considering a rank R matrix raised to the Hadamard. The functions of the respective decomposition to a sum of separable functions are defined, and the matricization of the grid tensor is given. The proof strategy outlined in section 4 is followed to prove theorem 1, showing an exponential advantage. The text discusses the exponential advantage of deep recurrent networks over shallow ones in modeling long-term dependencies, as shown in theorem 1. The construction of a shallow RAC using a Tensor Network is detailed, with the weights represented by a Matrix Product State Tensor Network. The bond dimension of the MPS is denoted by R. The bond dimension of the MPS in the Tensor Network is denoted by R. According to Levine et al. (2017), the rank of the matrix obtained by matricizing any tensor is equal to a min-cut separating certain partitions in the Tensor Network graph. The minimal cut in the MPS Tensor Network is equal to the bond dimension R, unless R > M T/2, in which case the minimal cut contains the external legs. Claim 2 states that the Start-End separation rank of a depth L = 2 RAC function is lower bounded by the rank of the corresponding grid tensor matricization. The rank of a matrix in a Tensor Network is related to the bond dimension R. To prove a theorem, it is necessary to show that the rank of a matrix is achieved for all configurations of network weights except for a negligible set. By choosing specific template vectors and a non-singular matrix F, this assignment satisfies the theorem. In the assignment, we define matrices and vectors using specific notation. We set values for W I,1 and W I,2 based on matrix Z and identity matrix. The initial hidden state values are chosen to have no impact on calculations. The output for class c after T time-steps is determined. The grid tensor is evaluated using template vectors, with substitutions made. The grid tensor's form is determined based on the defined matrix Z. The grid tensor is defined using specific notation and template vectors. It is split into two expressions, with the left part mapped to a vector and the right part mapped to a matrix. The matrix B can be written as a sum of rank-1 matrices, showing linear independence. The matrix B can be written as a sum of rank-1 matrices, showing linear independence. Applying claim 6 on the entries of B yields a specific form, where the maximal reward over all possible initial states is attained at a certain state q. Lemma 3 proves the existence of a value of \u2126 such that for every sequence of colors d, the maximal reward is solely attained at state q for all values of z except a finite set. This leads to the conclusion that there exists a value of z for which rank (B) = N, and the theorem follows. Claim 5 states that if a polynomial mapping A achieves a certain rank at a point x, then the set of points where A has a lower rank has zero measure. This implies that showing a specific assignment of recurrent network weights can establish a lower bound on the rank of the grid tensor matricization for all weight configurations except a set of Lebesgue measure zero. Theorem 1 states that providing a specific assignment achieving the required bound is sufficient to prove the theorem. For a matrix with polynomial entries in x, if a single contributor to the determinant has the highest degree of x, the matrix is fully ranked for all x values except a finite set. The lemma confirms that the determinant of a matrix with polynomial entries is not zero if its leading term has a non-vanishing coefficient. This helps achieve the required grid tensor matricization rank. The vector rearrangement inequality establishes a useful relation for non-negative numbers. The rearrangement inequality, known as \u03c3(i) = j \u21d0\u21d2 a(i) = a(j), holds for each component j \u2208 [R] of the vectors. It is proven that there exists a hard inequality for all permutations \u03c3 \u2208 SN where \u03c3 = IN. This is derived from the conditions of achieving equality in the rearrangement inequality. The lemma ensures that the matrix \u016a satisfies the conditions and is fully ranked. An identity is shown to simplify a complex expression, leading to a contribution less than \u03c1*."
}