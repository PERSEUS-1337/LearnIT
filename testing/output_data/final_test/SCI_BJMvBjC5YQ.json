{
    "title": "BJMvBjC5YQ",
    "content": "Deep Neutral Networks(DNNs) require large GPU memory for training on image/video databases. A novel training approach called Re-forwarding reduces memory usage by storing tensors at selected vertices during the first forward pass and computing missing tensors during backward passes. This trade-off reduces memory usage without compromising performance in testing. The Re-forwarding method reduces memory usage during DNN training by storing tensors at selected vertices during the forward pass and computing missing tensors during backward passes. This approach cuts down up to 80% of training memory on popular DNNs like Alexnet, VGG, ResNet, Densenet, and Inception net. The memory cost of popular backbone DNNs like AlexNet, VGG, and ResNet increases quadratically with input image resolution and network depth. For example, ResNet101 requires around 5000 MB for a median size input tensor. Video-based DNNs face even greater memory issues, with input tensors containing 64 frames. Training DNNs with large databases and big learning rates can require batch sizes up to 64. In training DNN compositions like GANs, memory issues are addressed through approaches such as better single GPUs and parallelization among multiple GPUs. Existing efforts involve saving all tensors during forward and using them to compute gradients during backward, while reforwarding saves a subset of tensors during the first forward and conducts \"Re-forward\" to compute tensors for gradients during backward. To address memory issues in training DNN compositions like GANs, a fundamental approach is proposed to trade-off between memory and computation power of GPUs. Recent affordable GPUs with limited memory but improved cores and FLOPS can train heavy DNNs by saving tensors at a subset of layers during the first forward and conducting extra local forwards to compute missing tensors during backward. To reduce memory usage during training of DNN compositions like GANs, a method called Re-forwarding is proposed. This involves saving tensors at a subset of layers during the first forward pass and conducting extra local forwards to compute missing tensors during backward. This approach leads to significant memory reduction. Researchers have also explored techniques for distributed computation to alleviate memory pressure on single GPU processors, but these methods do not reduce the total memory cost of DNNs. Other approaches involve optimizing the computation graph of DNNs and performing liveness analysis to reduce memory usage on finite hardware. The computation graph of DNNs describes tensor dependencies among layers. Liveness analysis manages memory by recycling garbage. These ideas originated from compiler optimization and are widely adopted by deep learning frameworks like Theano, MXNet, Tensorflow, and CNTK. Techniques for efficient data swapping between CPU and GPU exist but may incur extra I/O time without reducing total memory cost. Our approach optimally reduces training memory for both linear and arbitrary computation graphs by manipulating high-level tensors, making it applicable to any DNNs and their compositions. Our approach optimally reduces training memory for both linear and arbitrary computation graphs by manipulating high-level tensors, making it applicable to any DNNs and their compositions. The computation graph of DNNs describes tensor dependencies among layers. Liveness analysis manages memory by recycling garbage. These ideas originated from compiler optimization and are widely adopted by deep learning frameworks like Theano, MXNet, Tensorflow, and CNTK. Techniques for efficient data swapping between CPU and GPU exist but may incur extra I/O time without reducing total memory cost. All previous techniques are compatible with our approach and can further improve the memory efficiency of DNN training. Re-forwarding is formulated as DISPLAYFORM0 with terms for memory cost of stored tensors and maximal re-forward cost. For Linear Computation Graphs (LCGs), the solution involves finding the shortest path in the Accessibility Graph to minimize total cost. This approach optimally reduces training memory for various DNNs and is compatible with existing techniques for memory efficiency. The solution for Linear Computation Graphs (LCGs) involves constructing an Accessibility Graph to find the shortest path and minimize total cost. The algorithm skips unnecessary loops and max terms to optimize the solution. The time complexity of the algorithm is O(N^4). The time complexity of Algorithm 1 for Linear Computation Graphs is O(N^4). Theory and algorithms for DNNs with Arbitrary Computation Graphs (ACG) are presented, focusing on acyclic directed graphs. The optimal solution involves dividing the ACG into end-to-end segments with minimum memory cost. The segments are narrowed down to those with one source vertex and one target vertex for simplification. The approach for DNNs with Arbitrary Computation Graphs (ACG) involves dividing the ACG into end-to-end segments with minimal memory cost. The key assumptions are that segments have two endpoints and multi-input operations can compute gradients without using current input values. This approach is optimal for ACGs. The approach for DNNs with Arbitrary Computation Graphs (ACG) involves dividing the ACG into end-to-end segments with minimal memory cost. The approach is optimal for ACGs, where closed sets are defined based on specific properties. The detailed complexity analysis and proofs are in the appendix due to space limitations. Closed sets in the ACG can be split into different cases, denoted by s ij and s 1 ij. A splitting vertex in a closed set is defined by the existence of s it and s tj, with s ij = s it \u222a s tj \u222a {v t}. Closed sets in the ACG can be split into different cases based on the presence of splitting vertices. A closed set is splittable if it has at least 1 splitting vertex, defined as Closed Set Type 1, while a closed set with 0 splitting vertices can be divided into branches, defined as Branched Closed Set (Type 2). The division of closed sets involves linear segments separated by splitting vertices for Type 1, branches for Type 2, and maximal split for Type 3. The investigation focuses on the division of closed set Type 3, excluding trivial divisions. Division tree is a representation of a computation graph, where the root node is the whole computation graph, the leaf nodes are all the single tensors in the computation graph, and for a non-leaf node, its children are the members of its division. The computation graph can be reorganized into a division tree with 3 types of closed sets, forming a non-leaf node as a closed set. The division tree organizes a computation graph where non-leaf nodes are closed sets and children are divisions. The root node is the entire graph, leaf nodes are single tensors. The tree allows divide-and-conquer for optimal solutions, proven to be unique and complete in Theorem 1. The division tree organizes a computation graph where non-leaf nodes are closed sets and children are divisions. The root node is the entire graph, leaf nodes are single tensors. The tree allows divide-and-conquer for optimal solutions, proven to be unique and complete in Theorem 1. The search for optimal solutions for ACGs involves solving sub-problems using Algorithms 2-4, leading to the final solver presented as Algorithm 5. Algorithm 2 identifies splitting vertices in closed sets, while Algorithm 3 determines if a closed set is branched. These algorithms help classify closed sets and their properties efficiently. Algorithm 4 addresses finding the maximal split in closed sets type 3. It identifies possible closed sets within a set and uses a property to determine membership in the maximal split. The time complexity is O(N^4). Algorithm 5 is the solver for ACGs, building a division tree and using a greedy approach for optimal solutions. Algorithm 4 addresses finding the maximal split in closed sets type 3 by proposing a greedy idea to never expand a closed set unless its cost exceeds the max term. This approach ensures that the cost will never be smaller than unexpanded. The algorithm identifies possible closed sets and uses a property to determine membership in the maximal split. Algorithm 4 Find the maximal split of a non-branched s ij with 0 splitting vertex. For vertices {v} with paths from v k and paths to v t, form closed sets s kt with these vertices. If no s ab exists such that s kt s ab s ij, add s kt to the maximal split. Expand children with cost larger than current max term, solving linear segments. Evaluate Re-forwarding on neural networks with linear structures like Alexnet. The text discusses neural networks categorized into linear and non-linear structures, with computation graphs built for each network. A comparison is made between Re-forwarding and regular training approaches, with Re-forwarding working on arbitrary computation graphs. Additionally, a customized network called \"CustomNet\" is introduced, where even the manual version of Chen's approach is not applicable. Our approach works on all networks, with the computation graph visualized in the appendix. Experiments were conducted in Pytorch, measuring training time for both the regular and our approach. Results show significant memory reduction with reasonable time overheads: 26% space off and 40% time overhead for Alexnet, around 40% space off and time overhead for Vgg series, and up to 80% space off with only 39% time overhead for the deepest Resnet152. Re-forwarding is a fundamental approach that explores the trade-off between memory and computation power of GPUs. It saves tensors at a subset of layers during forward and conducts extra local forwards for backward, allowing for training heavy DNNs with finite GPU memory. This approach achieved significant space reduction with reasonable time overheads for various network architectures. The text discusses techniques such as distributed computing, GPU/CPU swapping, computation graph optimization, and liveness analysis. It explains the concept of splitting vertices in a closed set and provides lemmas to support the arguments. The text also mentions the independence of a closed set and the proof that any member of a maximal split cannot be a subset of another closed set. Lemma 6 states that any member of a maximal split cannot be a subset of another closed set. The proof involves analyzing the source and target vertices of the maximal split, ensuring independence and identifying splitting vertices within the closed set. The maximal split can have at most 2 members overlapping with s. If there are 0 or 1 member overlapping, it violates the definition of maximal split. If there are 2 members overlapping, it still violates the definition. The maximal split can have at most 2 members overlapping with s. If there are 0 or 1 member overlapping, it violates the definition of maximal split. If there are 2 members overlapping, it still violates the definition. This lemma is proved by showing that overlapping members cannot have the same splitting vertex. If a non-branched s ij has at least 1 vertex but has 0 splitting vertex, then its maximal split has length > 2. The uniqueness of the division is proven by discussing the division uniqueness of closed set types 1, 2, and 3. The splitting vertex set of a closed set type 1 is unique, and any additional division would require a branch member. The division tree's closed set must have at least 1 vertex, ensuring the maximal split has a length greater than 2. The division tree's completeness is proven by discussing the division completeness of closed set types 1, 2, and 3. The maximal split of a closed set in the division tree is unique, ensuring the completeness of the division. The completeness of the division tree is proven by discussing the division completeness of closed set types 1, 2, and 3. The division of closed set type 1 is complete when the head and tail vertices are in different members. This is ensured by the endpoint vertex having connections with other closed sets. The division of closed set type 3 is completed by forming edges in the accessibility graph, with a time complexity of O(N^4) and space complexity of O(N^2). Traversing ancestors and descendants to get {v in} and {v out} costs O(N) time. The time complexity of Algorithm 2 is O(N^2) with a space complexity of O(N). The most time-consuming part of Algorithm 3 is from step 5 to step 13, with an overall time complexity of O(N^2). The space complexity for Algorithm 3 is O(N). Algorithm 4 involves forming closed sets and checking connections, with a time complexity of O(N^4) and a space complexity of O(N^4). Algorithm 5 has a time complexity of O(N^4) for step 1 and complexity related to building a division tree in step 2. In Algorithm 4, closed sets are formed and connections checked with a time complexity of O(N^4). The division of closed sets in the tree is determined by Algorithms 2, 3, and 4, with varying time complexities. The overall time complexity for getting the division of an arbitrary closed set is O(N^3). The overall time complexity of Algorithm 5 is O(N^4) and the overall space complexity is also O(N^4). The computation graph determines the complexity of the ACG solver, with linear graphs having O(N^4) complexity and non-linear graphs having O(N^2) complexity. The LCG solver is called in step 6, with other steps being O(1) complexity. The runtime of ACG Solver (Algorithm 5) for each network is listed in TAB10, measured on a single core of CPU i7-8700. Parallelization on multiple CPU cores can significantly reduce the runtime. Despite concerns about runtime for deep networks, it is relatively small compared to training processes. Optimal solutions for popular networks will be released online, eliminating the need to run ACG solver. In comparing measured memory cut off with theoretical memory cut off in TAB10, it is noted that the former is slightly lower due to assumptions made during implementation. Some operations only require storing small tensors for backward, such as batch normalization. Computation graphs of various networks are visualized, showing the solutions of different approaches in green and red. The cost of each vertex in the graphs represents the size of the tensor during forward propagation. The cost of each vertex in the computation graphs represents the size of the tensor during forward propagation. For example, in Alexnet, the input tensor size is [1, 3, 224, 224], and after 2D convolution and relu, it becomes [1, 64, 55, 55]."
}