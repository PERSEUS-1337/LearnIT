{
    "title": "Skld1aVtPB",
    "content": "This work applies anomalous pattern detection techniques to neural networks to detect anomalous inputs, including adversarial noise. Subset Scanning methods are used to identify systematic patterns in the activation space of neural networks. Anomalous pattern detection techniques are applied to neural networks to detect adversarial noise in images. By leveraging common anomalous patterns in adversarially noised images, increased detection power is shown as the proportion of such images in a test set rises. Detection power and accuracy results are provided for targeted adversarial noise added to CIFAR-10 images using the Basic Iterative Method attack. The goal is to quantify, detect, and characterize data generated under alternative systems, such as fraudulent records or disease outbreaks. This paper explores using subset scanning to detect anomalous patterns in neural network activations, specifically focusing on finding high-scoring subsets of images and nodes in hidden layers. The goal is to quantify, detect, and characterize data generated under alternative systems, such as fraudulent records or disease outbreaks. The paper demonstrates using nonparametric scan statistics to quantify the anomalousness of a subset of images in neural network activation space. It measures the deviation between the subset's activations and those of background inputs, dealing with high-dimensional data. Subset scanning is effective in capturing systematic deviances in high-dimensional data, such as neural network activation space, making it a valuable tool for unsupervised anomalous-input detection. This method can be applied to any input and neural network architecture, offering broad potential for detecting anomalous patterns beyond just images. Additionally, subset scanning identifies nodes where deviations occur, providing further insights not fully explored in the paper. The paper explores the subset of nodes with higher-than-expected activations in images to characterize anomalous patterns. It also focuses on detecting targeted adversarial noise added to inputs to change labels, emphasizing the ability to detect noise across multiple images simultaneously. This approach relies on the idea that targeted attacks create a systematic anomalous pattern across multiple noised images, leading to a subset of inputs sharing higher-than-expected activations at similar nodes. This work applies subset scanning techniques to detect anomalous patterns in neural network data, specifically focusing on adversarial noise detection in images as a group. Detection power increases significantly when targeted images make up 8%-10% of the data, reaching near-perfect detection at 14%. Subset scanning treats pattern detection as a search for the most anomalous subset of observations, showing success where other heuristic approaches may fail. Subset scanning techniques have been shown to succeed in detecting anomalous patterns in data, where other heuristic approaches may fail. \"Top-down\" methods search for globally interesting patterns and identify sub-partitions to find smaller anomalous groups, while \"Bottom-up\" methods look for individually anomalous data points and aggregate them into clusters. Treating the detection problem as a subset scan maximizes detection power, with Linear Time Subset Scanning (LTSS) allowing for efficient maximization over all subsets of data without exhaustive search. Linear Time Subset Scanning (LTSS) allows for efficient maximization over all subsets of data without exhaustive search. Nonparametric scan statistics (NPSS) are used to detect anomalous patterns in data, making no parametric assumptions about activation distribution in a neural network. NPSS requires baseline data to compute empirical p-values for evaluation inputs, comparing them to a baseline distribution from natural inputs free of anomalies. The Nonparametric scan statistics (NPSS) method searches for subsets of data in evaluation inputs to detect anomalous patterns. It computes empirical p-values for activation values in neural networks, comparing them to a baseline distribution from natural inputs. The p-value p_ij measures the anomaly of activation values in images, with a simplifying assumption to consider only the upper bound of the p-value range. The Nonparametric scan statistics (NPSS) method converts activations into p-values to detect anomalies in evaluation images. It aims to identify high-scoring subsets through a search procedure maximizing a nonparametric scan statistic. This involves processing p-values from evaluation images to find the subset with extreme activations. The NPSS method converts activations into p-values to detect anomalies in evaluation images. It aims to identify high-scoring subsets through a search procedure maximizing a nonparametric scan statistic, utilizing well-known goodness-of-fit statistics like the Kolmogorov-Smirnov test and the Berk-Jones test statistic. The NPSS method utilizes the Berk-Jones test statistic to detect anomalies in evaluation images by comparing observed and expected proportions of significant p-values. NPSS satisfies the linear-time subset scanning property, allowing for efficient and exact subset scanning. The linear-time subset scanning (LTSS) property allows for efficient maximization over subsets of data by prioritizing top-k records based on a given score function. This method aims to find the most anomalous subset of images for a given subset of nodes, utilizing a function to measure image priority based on p-values. Additionally, the process can be reoriented to identify anomalous subsets of nodes for a given subset of images. The linear-time subset scanning (LTSS) property enables efficient maximization over subsets of data by prioritizing top-k records using a score function. This method focuses on identifying anomalous subsets of nodes or images by measuring their priority based on p-values. By iteratively optimizing over subsets of nodes and images, a local maximum of the score function can be efficiently computed. Multiple random restarts can be performed to approach the global maximum. LTSS enables efficient optimization of NPSS to reach the global maximum with high probability. Machine Learning models are vulnerable to adversarial perturbations in input data, leading to misclassification. Various methods exist to enhance neural networks' robustness to adversarial noise, including retraining with altered loss functions and supervised detection approaches. Our work approaches the problem as anomalous pattern detection in an unsupervised manner, without prior knowledge of attacks or labeled examples. It does not rely on data augmentation or specialized training techniques, and complements existing defenses. For example, subset scanning methods can easily detect patterns that require extreme perturbations to change class labels. This approach is similar to Feinman et al. (2017), who use a kernel density estimate over background activations from the last hidden layer. Our novel subset scanning approach detects anomalous patterns at the node-level and across multiple inputs simultaneously using a ResNet20 neural network trained on CIFAR-10 images. The model achieved a test accuracy of 0.9183 and focuses on the final convolutional layer of the last residual block. Future work will explore the impact of model classification accuracy on detecting anomalous patterns within its activations. The final convolutional layer of the last residual block in a ResNet20 neural network contains 64 filters with 8x8 nodes each, totaling 4096 nodes. An analysis was conducted on the activations at these nodes, using a subset scanning approach to detect anomalous patterns. Adversarial experiments were performed using a subset of validation images to generate background activation distribution, with the remaining images used to create \"Clean\" and \"Adversarial\" groups for targeted attacks. The Basic Iterative Method (BIM) adversarial attack was utilized for the experiments. The Basic Iterative Method (BIM) adversarial attack was used to generate targeted attacks on a ResNet20 neural network. The attacks were controlled by a parameter that determined the extent of pixel changes in the image. The attacks were conducted over 10 steps with small increments to make the pattern subtler and harder to detect. The attacks were successful with near 100% success rates, except for a few failed attempts that were removed from the dataset. Each targeted attack group contained approximately 900 images. The study generated targeted attacks on a ResNet20 neural network using the Basic Iterative Method (BIM). Each attack group contained around 900 noised images. Test sets were created with 500 images, including clean and adversarial images from different groups. Adversarial images made up 6-14% of the test sets for each target class label, with a special case of drawing from all groups. Each case was repeated 100 times for analysis. The study conducted targeted attacks on a ResNet20 neural network using the Basic Iterative Method (BIM), generating around 900 noised images in each attack group. Test sets of 500 images were created, consisting of clean and adversarial images from different groups. Adversarial images accounted for 6-14% of the test sets for each target class label, with a special case of drawing from all groups. Each case was repeated 100 times. The test sets were scanned to determine the most anomalous subset of images, with results reported in Table 1. Detection Accuracy was measured by precision and recall, while Detection Power was assessed by modeling the null hypothesis of no adversarial noise present in 100 test sets containing only clean images. The study analyzed the detection power of a method by comparing distributions of scores from scanning 500 clean images to those with noised samples. ROC curves were generated for different target classes and proportions. Detection Power results were reported, including experiments with individual image scanning. Precision and recall were not reported for individual cases. The study focused on detecting adversarial noise across multiple images simultaneously. Detection power was measured by comparing distributions of scores from scanning clean images to those with noise. Results showed higher detection power for groups of images compared to individual images, with nearly perfect detection at a 10% proportion of noised images. The study found that detection power is higher for groups of images compared to individual images, with nearly perfect detection at 12% and above. The scanning method identifies a consistent anomalous pattern across multiple noised images targeting a single class, but lags behind in the \"All\" category. The study shows that precision is consistently lower than recall in detecting anomalous images with targeted noise. This is attributed to the presence of both noised and natural images of the same class in the test set, leading to decreased precision. Additionally, a static parameter setting for the scanning function may also contribute to the relatively low precision. The recall measurements in the experiments show exceptionally high values due to a large, static \u03b1 max value. The experiments were conducted in an unsupervised form with \u03b1 max set arbitrarily at 0.5. Increasing the number of noised images should decrease the recall rate. Subset scanning demonstrates adaptability by maintaining strong recall despite an increase in the number of noised images. This work introduces an unsupervised anomaly detector for neural networks, based on detecting anomalous patterns in the activation space. This work introduces a novel approach using subset scanning methods to detect anomalous patterns in neural network activations. Nonparametric scan statistics are applied to identify the most anomalous subset of node activations and inputs, operating on three levels of anomalousness. The optimization process in subset scanning methods for detecting anomalous patterns in neural network activations focuses on identifying subsets with the most anomalous p-values. This approach operates on three levels of anomalousness, with the highest level involving finding inputs with higher-than-expected activations at specific nodes. Subset scanning method efficiently identifies high scoring subsets of images and nodes in a large search space of neural network activations. It can detect targeted adversarial noise and shows increased detection power when considering images with noise as a group. This work is the first to consider adversarial noise detection across a group. This work introduces a method for detecting adversarial noise across a group of images, which is a novel approach compared to existing defenses that focus on individual images. The method can identify systematic differences in affected images and is performed unsupervised, showing potential for various neural network applications. Implementation details are provided for readers interested in replicating the experiments. The paper provides implementation details for detecting adversarial noise across a group of images, focusing on reproducible results and using vanilla settings. Code for the experiments is available on Github, with specific details on sorting activations from background images and calculating p-value ranges efficiently."
}