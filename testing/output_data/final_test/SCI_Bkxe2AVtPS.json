{
    "title": "Bkxe2AVtPS",
    "content": "Training with larger number of parameters while maintaining fast iterations is a popular strategy for improving Deep Neural Network (DNN) models. A novel methodology called Shifted and Squeezed FP8 (S2FP8) is introduced for training DNNs using 8-bit floating point numbers. This method allows for a larger effective memory and increased computational speed, working well for models like ResNet50, Transformer, and NCF without the need for fine-tuning loss scaling parameters or specific layer precision requirements. Two learnable statistics - shifted and squeezed factors - are introduced for DNN tensors. The DNN tensors utilize shifted and squeezed factors to adjust their range optimally in 8-bits, minimizing loss. Deep neural networks have achieved state-of-the-art performance in various tasks, leading to interest in reducing memory and energy consumption. Training with reduced precision formats like 16-bit is effective, but 8-bit precision training remains a challenge. Current methodologies for FP8 training require specialized accumulation techniques. In this work, a novel 8-bit floating point format called shifted and squeezed FP8 (S2FP8) is proposed for training deep neural networks. S2FP8 eliminates the need for loss scaling and offers advantages over previously proposed 8-bit training methodologies. This format is hardware-friendly, consumes less power, and simplifies the training process by adjusting during forward and backward passes. S2FP8, a novel 8-bit floating point format, adjusts gradients, activations, and weights during model training without needing FP32 precision for the first and last layers. It outperforms previous 8-bit approaches in image classification, translation, and recommendation models, matching FP32 accuracy without additional tuning. The success of 32-bit floating point data in deep neural network training has spurred interest in lower precision training, leading to advancements in techniques like loss scaling and stochastic rounding. Studies have developed techniques like loss scaling and stochastic rounding for effective training in 16-bit, supported by hardware. Stochastic rounding is crucial for model convergence even in simple convolutional neural networks. Google's bfloat16 format has the same number of exponent bits as FP32, leading to its success without requiring hardware-intensive techniques. While 8-bit formats offer performance advantages, convergence is challenging due to accuracy loss in backpropagated gradient values. Wang et al. (2018) demonstrated training models in FP8 but used FP16 for matrix multiplications and convolutions. Training models with matrix multiplications and convolutions in FP8, using FP16 with chunk-based accumulations and stochastic rounding hardware. Success demonstrated with FP8, accumulating in FP32 and using loss scaling techniques on various networks. Need for efficient loss scaling, rounding hardware, and restriction on some layers being in higher precision emphasized. Zhou et al. (2016) quantized weights, activations, and gradients of AlexNet to 1, 2, and 6 bits respectively, maintaining first and last convolution layers in full precision. Demonstrated use of integers for training LeNet-5. The FP8 format with 2 bits of mantissa and 5 bits of exponent is narrow and has lower accuracy. It lacks generalizability to other models and requires custom tuning. No out-of-the-box solution exists for training deep learning models with FP8 without the need for tuned loss scaling techniques. The FP8 format has a narrow range and lower accuracy compared to FP32. Tensor distributions in neural networks vary widely, requiring techniques like loss scaling to fit within the FP8 range for training. Loss scaling is crucial for training neural networks in the FP8 format, as it modifies gradients to prevent convergence issues. However, weights and activations can also exceed the FP8 range, impacting convergence. The drawback of loss scaling is the need for manual adjustment and empirical tuning, especially for models like Transformers that require dynamic adjustments. To address the challenges of training neural networks in low-precision formats like FP8, a new 8-bit floating point format is proposed. This format stores tensors using N FP8 numbers along with two factors (squeeze and shift) to adjust the distribution and precision. This approach eliminates the need for manual scaling adjustments and tuning, making it easier to train models without modifications or hyperparameter adjustments. The new tensor format for training neural networks in low-precision FP8 numbers ensures that Y has zero mean and values less than 15, ideal for FP8. This format simplifies the training process without manual scaling adjustments. The new S2FP8 format is used for training neural networks, with master weights in FP32 and updated using S2FP8 gradients. Accumulations in the GEMM kernel are in full FP32 precision. \u03b1 and \u03b2 provide extra degrees of freedom for each tensor, capturing a wide dynamic range. They can be interpreted as parameters of a distribution generating tensor values. \u03b1 and \u03b2 converge to approximately 5 and 21, respectively. When using S2FP8 for training, forward and backward GEMM's only use S2FP8. The S2FP8 format is used for training neural networks with master weights in FP32. \u03b1 and \u03b2 values represent the distribution of each converged tensor. Statistics stabilize in the last third of training, indicating convergence. Comparison is made with baseline FP32 and FP8 training on various networks and datasets. For experiments on MovieLens 1 Million dataset, ResNet and NCF models were used with consistent hyperparameters across FP32, FP8, and S2FP8 evaluations. S2FP8 was simulated by inserting truncation functions throughout the network to mimic low-precision training. The truncation function computes mean and maximum magnitude of tensors, calculates \u03b1 and \u03b2, and truncates the tensors accordingly. The study evaluates S2FP8, a low-precision training method, on Residual Networks with varying depths on CIFAR-10 dataset. Results show that S2FP8 almost matches FP32 baseline performance and sometimes even surpasses it. FP8 without truncation does not converge well. S2FP8 is also tested on ImageNet dataset with promising results. In training the network on 4 GPUs, standard parameters were used with 90 epochs, batchsize of 256, and SGD with momentum of 0.9. The initial learning rate was 0.1 and decreased by a factor of 10 after epochs 30, 60, 80, and 90. Results in Table 2 and Figure 6 show that S2FP8 approaches the FP32 baseline closely. FP8 without truncation diverges and does not converge well. Loss scaling of 10,000 can help FP8 converge, but S2FP8 does not require this due to improved quantization. Stochastic rounding slightly improves precision but requires tuning and keeping some layers in full precision. S2FP8, a quantization method, showed promising results on a small Transformer model without the need for hyperparameter tuning. In comparison, FP8 required extensive loss scaling tuning to reach similar performance. The BLEU score for translation tasks on the English-Vietnamese dataset with Transformer Tiny was 25.3 for both FP32 and S2FP8, while FP8 scored lower at 21.3. This highlights the value of S2FP8 as an out-of-the-box method for users. The Neural Collaborative Filtering (NCF) network uses embeddings for users and items from the MovieLens dataset, passed to an MLP network for user-item interaction. Matrix-multiplication operations are key. S2FP8, FP32, and FP8 without loss scaling are compared on MovieLens 1 Million dataset. S2FP8 easily reaches baseline without tuning, while FP8 falls short. S2FP8 is a new data type that easily reaches baseline without tuning. It requires its own circuitry in a tensor processing engine, but the added overhead is minimal. To convert FP32 tensors into S2FP8, two hardware components are needed: one to calculate tensor statistics and another to adjust the exponent and mantissa of tensor elements before truncating them into 8-bit placeholders. Shift and squeeze factors are applied to achieve this conversion. The novel 8-bit floating point data type S2FP8 allows training neural networks in an 8-bit format without the need for loss scaling tuning or complex rounding techniques. It uses shifted and squeezed factors to adjust the range of tensors before truncation, providing competitive performance compared to FP32 baselines. This eliminates the restriction of maintaining the first and last layers in FP32, making it a viable option for various networks. The FP8 data type has a range and precision indicated in Figure A1, with a normal representable range from 2^-14 to (1 - 2^-3) * 2^16. The convergence of ResNet-50 with the CIFAR-10 dataset using Loss FP32 S2FP8 is shown in Figure A2."
}