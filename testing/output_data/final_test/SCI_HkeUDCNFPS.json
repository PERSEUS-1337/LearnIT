{
    "title": "HkeUDCNFPS",
    "content": "Hierarchical reinforcement learning (HRL) methods divide tasks into hierarchies to handle action-reward correlation over long time horizons. A novel HRL framework, TAIC, learns temporal abstraction from past experience without task-specific knowledge. It formulates the problem as learning latent representations of action sequences and regularizes the latent space with information-theoretic constraints to maximize mutual information between variables and state changes. The algorithm effectively learns temporal abstractions, leading to faster convergence and improved sample efficiency in RL tasks. Hierarchical RL decomposes problems into sub-goals, enabling the composition of low-level actions into high-level abstractions. This approach addresses challenges in real-world tasks with large search spaces and sparse rewards. The curr_chunk discusses the challenges of learning task structures and temporal abstractions automatically in Hierarchical Reinforcement Learning (HRL). Various strategies have been proposed, including finding sub-goals based on statistical methods and learning temporal abstractions with deep learning. However, many methods still require a predefined hierarchical policy structure or task-specific knowledge. The curr_chunk introduces a general HRL framework called TAIC, which enables agents to learn temporal abstractions without task-specific knowledge. It formulates the problem as learning a latent representation of action sequences and proposes a novel approach to regularize the latent space using information-theoretic constraints. The learned abstract representations (options) facilitate RL at a higher level and knowledge transfer between tasks. The TAIC framework introduces a method for learning temporal abstractions in RL using Recurrent Variational AutoEncoders (RVAE) and regularization on the option space. Experimental results show that the learned temporal abstractions improve RL training and enable efficient knowledge transfer between tasks. In the early 1990s, various methods were proposed for task abstraction in reinforcement learning. These included recurrent neural networks generating sub-goals, an evaluator predicting rewards, and a RL machine using sub-goal sequences. The options framework and Hierarchies of Abstract Machines were also introduced to address the task hierarchy problem. Additionally, the MAXQ method focused on value function decomposition. Many of these early approaches assumed predefined task hierarchies by human experts. Recent work in automatic task decomposition has shifted towards learning temporal abstraction using deep learning methods. Frans et al. developed a two-layer hierarchy of policies, while Andreas et al. defined policy sketches for annotating tasks with sub-tasks. Kulkarni et al. presented hierarchical-DQN integrating hierarchical action-value functions, and Bacon et al. combined the options framework with policy gradient. These advancements aim to address the continuous control problem and improve task hierarchy in reinforcement learning. In the realm of automatic task decomposition, recent efforts have focused on learning temporal abstraction using deep learning techniques. Bacon et al. (2017) introduced an option-critic architecture that combines the options framework with policy gradient. Our framework proposes learning temporal abstraction without predefined sub-goals, unlike previous methods. The SeCTAR algorithm utilizes RVAE to learn a latent representation from state sequences in trajectories. Our work differs from existing literature by learning abstractions instead of just combining primitive actions, which proves to be more beneficial for hierarchical reinforcement learning training. In this paper, the authors apply RVAE to model the abstraction of action sequences in the context of MDP problems. They propose the TAIC framework, which focuses on learning temporal abstraction without predefined sub-goals, unlike previous methods. The framework aims to learn abstractions instead of just combining primitive actions, which is beneficial for hierarchical reinforcement learning training. The TAIC framework models options as continuous random variables, representing sequences of actions. It defines initiation condition I(o) as the entire state space, sub-policy \u03c0(o) mapping o to actions, and termination condition \u03b2(s, o) controlling option length. This differs from traditional options where each represents a sub-policy. In TAIC, a meta-policy \u03c0 \u2126 selects sub-policies at longer time horizons. The TAIC framework transforms discrete options into continuous options by defining them as continuous random variables with infinite values. The meta-policy \u03c0 \u2126 outputs the random variable o, allowing for constraints on the option space to group similar consequences together. The goal is to learn temporal abstractions o and corresponding sub-policies to improve training efficiency in reinforcement learning tasks. Learning latent representations o from action sequences is a key focus. The recurrent variational autoencoder (RVAE) is used to learn latent representations from action sequences with variable lengths. It approximates the posterior p(o|a 0...k ) by optimizing a lower bound on the log-likelihood, improving the identification of hidden features in the inputs. The recurrent variational autoencoder (RVAE) approximates the posterior p(o|a 0...k ) by optimizing a lower bound on the log-likelihood. It uses two networks, an encoder E and a decoder D, to model the Gaussian distribution of q(o) and minimize reconstruction and KL losses. The decoder D in RVAE outputs 0...k based on input option o. The reconstruction loss encodes action sequences with L2 distance in action space. Options should be encoded closely together for upper-level task solving. The goal is to extract latent variables for RL training, not precisely reconstructing action sequences. Options in RL training should encode the consequence of action sequences by maximizing mutual information with state changes. This involves maximizing mutual information between the option and two states, while minimizing mutual information between the option and individual start and end states. This is achieved by minimizing the summation of two conditional entropy terms. Equations 4 and 5 aim to minimize conditional entropy and maximize conditional entropy, respectively. Equation 6 and Equation 7 conflict with each other, representing a trade-off in encoding state changes. The final constraints combine these equations. The system architecture includes neural networks for E, D, F, P, and RL. The gradients of losses L rvae, L adv, and L pred are back-propagated through the encoder E. The constraints are approximated with neural networks, and a predictor network P is utilized for Equation 8. Equation 8 introduces a predictor network P that predicts the end state given the start state and option. The predictive loss L pred is minimized to optimize the constraint, with gradients backpropagated into the encoder E. Another network F is used to recover the start and end states based on the option code, trained competitively with E. E is pushed to encode options in a way that F cannot recover the states, leading to regularization of the encoder. The encoder E is regularized by extra gradients from predictor network P and recovery network F. Four networks (E, D, F, P) collaborate and compete in training on experience set \u039b. The learned latent representation o captures action consequences, allowing for training the agent at a higher level with HRL policy outputting options decoded by D. Three termination condition methods are compared: Fix-length, Term-output, and Term-predict for applying learned options in HRL training. The decoder D in the HRL policy outputs options after a fixed number of actions, regardless of state. Another output is added to learn option codes of varying lengths, with a termination signal determining the end of a sequence. This method provides more choices for the HRL in stable and safe states. The HRL policy uses a predictor network to determine the length of action sequences based on the environment dynamics. The termination condition, called Termpredict, adjusts the sequence length depending on the state's familiarity. This approach allows for more flexibility in choosing sequence lengths based on the state's stability. The training algorithm sets a threshold on prediction loss, terminating output when it exceeds the threshold. Options learned are more robust to state changes but rely on a good predictor. The length of the sequence depends on the hyperparameter \u03b4. Using a Semi-MDP framework, an HRL agent is trained at a higher level with continuous options. The policy gradient algorithm is used for continuous options, applicable to discrete problems with discrete actions. The advantage function A(s t , o t ) is designed as the accumulative reward subtracting a baseline function in high-level policy planning. The experiments employ the PPO algorithm for policy gradient over options and actions. The temporal abstraction framework is applied to robotic control tasks in MuJoCo, showcasing the concept's proof in a 2D navigation task. In experiments using the PPO algorithm, a one-layer LSTM with 64 hidden units is implemented for experience and decision-making, with three-layer MLPs for reinforcement learning. Different learning rates and loss balancing weights are used. The framework is tested on a 2D navigation task to visualize the learned option. The environment for the experiment is a 10m by 10m room with obstacles, where state and action are continuous. The goal is to navigate from start to goal locations, with a reward of 100 for reaching the goal. A flat PPO agent is used to collect experience, and an RVAE network is trained on the action sequences. Interpolation between two options is shown in Figure 5 (a), demonstrating the RVAE's ability to capture direction. The RVAE effectively captures the direction and curvature of sequences, producing smoother versions of the original sequences. Interpolations between sequences smoothly transition in the option space. Each dimension in the option space encodes different information, controlling aspects like the ending point and curvature of action sequences. Results may vary during training, with some properties mixed. The experiment evaluates the ability to recover from sudden environmental changes using the TAIC algorithm compared to flat RL. Results show TAIC consistently recovers in all trials, while flat RL only finds the goal in 20% of trials. In the robotic control domain, the goal is to learn a control policy for the robot to run forward as fast as possible. Two ways of evaluating the TAIC framework are presented: visualizing the option space qualitatively and applying the learned option to HRL training for performance comparison. A visualization method is proposed to evaluate the option space correlation. The correlation between options and state changes is visualized using t-SNE to show improved coupling with information-theoretic constraints. HRL performance using TAIC outperforms flat PPO in 3 out of 4 benchmark tasks, as seen in Figure 7. In the HarfCheetah and Ant tasks, TAIC starts higher and converges faster due to positive rewards from random policy over option space. HRL policy is updated less frequently than flat RL due to multiple time steps for options. In Walker2d task, our method does not outperform flat RL possibly due to agent instability. Decoder D in current setup acts as open-loop controller, vulnerable to unstable states. Option model can be extended to close-loop sub-policies in future studies. Different termination conditions have pros and cons, with Fix-len being simpler and more stable in most cases. The Fix-len termination condition is simpler and more stable. Term-output and Term-predict conditions yield better results in some cases but require more parameter tuning. Selecting experiences with higher rewards helps the agent learn faster. The TAIC framework efficiently transfers past experiences to new tasks. Options learned from the MoJoCo Ant-v1 task are applied to novel tasks. In the Ant turn task, the agent is rewarded for making left turns. The TAIC framework efficiently transfers past experiences to new tasks, achieving higher rewards and quick convergence in novel tasks. It outperforms the flat RL method in solving tasks without task transfer treatment. Transfer learning is also applied to more challenging tasks. The TAIC framework outperforms flat RL in transferring past experiences to new tasks, showing higher rewards and quicker convergence. Transfer learning on flat RL does not bring significant benefits, indicating that sharing weights between tasks is less efficient. The TAIC framework transfers high-level abstracted knowledge in the form of options, avoiding performance degradation seen in other methods. This paper introduces a general HRL framework for learning temporal abstraction from action sequences, formulating the problem as learning latent representations (options) over action sequences and providing a solution for regularization and constraint addition to improve representation learning. The TAIC framework introduces a general HRL framework for learning temporal abstraction from action sequences. It provides an efficient tool to transfer knowledge between tasks and can be applied with various RL optimization algorithms. Future studies could explore jointly optimizing RL tasks and options to improve performance. Future studies will focus on learning a closed-loop sub-policy beyond the RNN decoder in the TAIC framework. Additionally, applying the framework to discrete problems and exploring other RL algorithms like DQN and SAC could provide more insights for further improvement."
}