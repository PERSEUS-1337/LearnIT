{
    "title": "Bygadh4tDB",
    "content": "Stochastic neural networks with discrete random variables are important for their expressivity and interpretability. Monte Carlo gradient estimation techniques are used for training these models due to the inability to directly differentiate and backpropagate. Efficient stochastic gradient estimators like Straight-Through and Gumbel-Softmax work well for shallow models but struggle with increasing complexity. In this work, the focus is on stochastic networks with multiple layers of Boolean latent variables. The framework of harmonic analysis for Boolean functions is used to analyze these networks and derive an analytic formulation for bias in the biased Straight-Through estimator. A new gradient estimation algorithm called FouST is proposed, which outperforms state-of-the-art biased estimators and is faster than unbiased ones. FouST is the first gradient estimator capable of training very deep stochastic neural networks with up to 80 deterministic and 11 stochastic layers. The curr_chunk discusses the challenges of using discrete random variables in deep learning due to the limitations of backpropagation. It introduces Monte Carlo gradient estimation as a solution, highlighting the trade-offs between biased and unbiased gradient estimates. Various solutions to reduce variance in unbiased estimators have been proposed recently. In this work, the focus is on biased estimators for Boolean random variables in complex neural networks. The goal is to provide a gradient estimator that works for any deep or wide neural network architecture without using continuous relaxations or quantizations. The framework of harmonic analysis of Boolean functions is repurposed for this purpose. The study focuses on biased estimators for Boolean random variables in complex neural networks. It introduces the framework of harmonic analysis of Boolean functions to analyze discrete stochastic neural networks and their gradients. A low-bias gradient estimator, FouST, is presented for Boolean latent variables, with three bias reduction steps. Additionally, the gradient estimator used with DARN is shown to be a strong baseline for gradient estimation in large and complex models with many stochastic layers. FouST is a gradient estimate algorithm for training deep stochastic neural networks with Boolean latent variables. It can be used in complex neural networks with multiple layers of Boolean random variables. The algorithm is based on Harmonic Analysis for Boolean functions on the n-dimensional Boolean cube. An example of a Boolean function in this setting is a generative neural network f : z \u2192 y with a factorized latent distribution. The goal is to learn or approximate the latent distribution given input data x, i.e., p(z|x). Basic operations in Harmonic Analysis of Boolean functions are introduced, with further details in Appendix A. The Fourier expansion of Boolean functions f and g is defined using basis functions \u03c6 S, which are orthonormal and derived from the n-dimensional Boolean cube. The expansion allows for the computation of Fourier coefficients using inner products, with the cardinality of S determining the degree of the expansion. The Straight-Through gradient estimator is used in training models with a loss function. It approximates the gradient by computing a biased approximation using Harmonic Analysis of Boolean functions. The bias in the estimator is quantified by analyzing the Fourier expansion of Boolean functions. The REINFORCE gradient estimator connects with the degree-1 Fourier coefficients of Boolean functions. Bias quantification is done through the analysis of Fourier expansion. The proof sketch outlines the steps to derive the relation between Fourier coefficients under the unknown distribution and the uniform Bernoulli distribution. It involves deriving Taylor expansions for the true gradient and the Straight-Through gradient estimator, and comparing them to prove the lemma. The REINFORCE gradient is connected to the degree-1 Fourier coefficients of Boolean functions for bias quantification through Fourier expansion analysis. The Taylor expansions of the true gradient and the Straight-Through gradient are compared to show their relationship. The true gradient focuses on odd Taylor coefficients, while the Straight-Through gradient is the expectation of a specific equation in the i-th dimension. The FouST algorithm presents a gradient estimate for deep Boolean latent models, focusing on bias reduction steps in the Straight-Through estimator. The algorithm is inspired by Harmonic Analysis and aims to reduce bias under the uniform Bernoulli distribution. Sampling from p i\u21921/2 instead of p(z) helps eliminate extra bias due to non-zero expectation terms in higher-order harmonics. The algorithm FouST aims to reduce bias in the Straight-Through estimator for deep Boolean latent models by sampling from p i\u21921/2 instead of p(z) to eliminate extra bias from higher-order harmonics. This involves importance sampling and correcting coefficients using moments of the uniform distribution. The algorithm FouST aims to reduce bias in the Straight-Through estimator for deep Boolean latent models by sampling from an auxiliary variable u and exploiting higher moments of the uniform distribution to reduce bias. This involves using the \"Bernoulli splitting uniform\" trick and comparing equations 14 and 13. Further details are provided in Appendix C.1. The algorithm FouST aims to reduce bias in the Straight-Through estimator for deep Boolean latent models by sampling from an auxiliary variable u and exploiting higher moments of the uniform distribution to reduce bias. Equation 14 shows that pure terms match the true gradient in equation 13, while mixed terms have coefficients that differ. Using smaller intervals for random samples can help manage the bias-variance trade-off. The \"Bernoulli splitting uniform\" relies on the continuous variable u conditioned on the binary sample to reduce bias. The \"Bernoulli splitting uniform\" trick utilizes an auxiliary variable u to reduce bias in deep Boolean latent models. It does not relax like Gumbel-Softmax and leads to an unbiased estimator with increased variance for univariate f. The choice of input representation affects bias, with Fourier coefficients differing based on the representation used. The final forms of the gradients are provided, with details in Appendix C.2. The final forms of the gradients are given in Appendix C.2. Under the p i\u21921/2 distribution, the degree-1 Fourier coefficients are presented in equation 15, showing a reduction in bias compared to equation 7. The algorithm described in algorithm 1 is a Straight-Through gradient estimator with bias reduction steps, using a single sample for gradient estimation. Monte Carlo gradient estimators for training models with stochastic variables can introduce bias. Monte Carlo gradient estimators for training models with stochastic variables can be biased or unbiased. The REINFORCE algorithm is an unbiased estimator but has high variance gradients. For continuous variables, the reparameterization trick by Kingma & Welling reduces variance. Control variate schemes like NVIL, MuProp, and REBAR are used for variance reduction in discrete variables. The REBAR method uses the Gumbel-Softmax trick for unbiased gradient estimates, while RELAX extends this with an auxiliary network and continuous relaxations. Biased estimators like the Straight-Through estimator and GumbelSoftmax estimator are also used for training models with stochastic variables. In this work, biased Straight-Through gradient estimators are analyzed for bias reduction using Fourier expansions of Boolean functions. Fourier expansions are commonly used in computational learning theory for various applications. The authors are the first to explore Fourier expansions for reducing bias in biased stochastic gradient estimators. The authors validate FouST on toy and generative models using variational autoencoders. They train models on various datasets and compare against other estimators. Results are consistent, and details on architectures and hyperparameters are provided in the appendix. The authors validate FouST on toy and generative models using variational autoencoders. They train models on various datasets and compare against other estimators. Results show that FouST outperforms other biased gradient estimators in both datasets and architectures, especially the Straight-Through estimator. FouST outperforms the StraightThrough estimator in bias reduction for optimized neural network functions. REBAR, while not directly comparable, shows worse test ELBO compared to FouST on MNIST with two stochastic layers. FouST also excels in training stochastic ResNets on CIFAR-10, surpassing other biased estimators. FouST outperforms other biased estimators in training stochastic ResNets on CIFAR-10, achieving a score of 5.08 bits per dimension. It requires a single sample for estimating gradients and has similar wallclock times as other biased estimators. On MNIST, the unbiased REBAR is significantly slower than biased estimators for MLPs with stochastic layers. FouST allows for efficient training of neural networks with Boolean stochastic variables. It is evaluated on more complex architectures like wide S-ResNet-40-2-800 and deep S-ResNet-80-11-256, showing promising results. FouST enables efficient training of neural networks with Boolean stochastic variables on complex architectures like wide S-ResNet-40-2-800 and deep S-ResNet-80-11-256. Training with existing unbiased estimators is intractable in this setup. FouST outperforms DARN in achieving better training ELBO's, allowing for scaling up the complexity of stochastic neural networks in terms of stochastic depth and width. The text discusses differentiation of partial derivatives on continuous functions and discrete derivatives on Boolean functions. It presents a proof following O'Donnell's work, using two representations of a Boolean function: Fourier expansion under a uniform Bernoulli distribution and a polynomial representation in zi. The text presents a detailed proof of Lemma 2, showing the equivalence between discrete and partial derivatives in the context of Boolean functions represented through Fourier expansion and polynomial representation. The proof of Lemma 2 demonstrates the equivalence between discrete and partial derivatives in Boolean functions represented through Fourier expansion and polynomial representation. The Taylor expansions for the true gradient and Straight-Through gradient estimator are derived and compared, leading to the conclusion that the REINFORCE gradient for the i-th dimension is given by a specific formula. The Taylor expansions of the true and Straight-Through gradients are also discussed, focusing on the coefficients and terms involved. The true gradient and Straight-Through gradient estimators are compared in terms of their Taylor expansions. The final expression for the true gradient with respect to pi involves the expected sum of odd Taylor coefficients. The dependence on zi and \u00b5i is not explicitly shown in the final expression, unlike in the Straight-Through gradient expansion. The expectation under pi\u21921/2 causes a term to vanish, resulting in low-bias gradient estimates. The final term in equation 37 vanishes, resulting in low-bias gradient estimates for a bivariate function. A two-dimensional z and bivariate f (z1, z2) with a Taylor expansion are described. The \"Bernoulli splitting uniform\" trick involves sampling from a uniform Bernoulli distribution and taking a uniform sample with specific conditions. Moments of the uniform distribution simplify to specific values. The gradient under binary sampling shows a decaying trend for mixed terms, reducing bias but increasing variance. To manage bias-variance trade-off, smaller intervals for uniform samples are chosen. Functions with greater dependence on mixed degree terms benefit from this approach. The Fourier basis is independent of input representation, but the choice of representation affects bias. Using {\u22121/2, 1/2} n as input representation changes Fourier coefficients. Taylor series of f in terms of h(zi) is written, showing odd terms decayed by inverse powers of 2. The ablation experiments on the importance-corrected Straight-Through method showed that scaling alone improves optimization for both MNIST and OMNIGLOT models, while noise alone helps in the case of MNIST but results in a worse ELBO for OMNIGLOT. Combining noise with scaling gives an improvement in both cases, indicating the effectiveness of the proposed modifications. The encoder and decoder networks consist of MLPs with one or more stochastic layers, each preceded by 2 deterministic layers with a tanh activation function. The network architecture includes deterministic and stochastic layers with specific activation functions, learning rates, and noise intervals. ResNets are used with stacks of layers, each containing regular residual blocks and at most one stochastic layer. Downsampling is done once per stack, with two layers per stack for CIFAR. Different learning rates and scaling parameters are chosen for optimization. For mini-ImageNet, downsampling is done thrice. The learning rate is chosen from a range of values. FouST scaling parameter and uniform interval scaling factor are also selected for optimization."
}