{
    "title": "HyBbjW-RW",
    "content": "This paper explores open loop search methods for hyperparameter optimization, such as grid search, random search, and low discrepancy sequences. These methods are predetermined and can be generated before evaluating configurations. $k$-DPPs are proposed for hyperparameter optimization via random search, promoting diversity compared to uniform random search. An approach is described to transform hyperparameter search spaces for efficient use with $k$-DPPs. A novel Metropolis-Hastings algorithm is introduced for sampling from $k$-DPPs defined over spaces with a mix of discrete and continuous dimensions. Experiments show significant benefits over uniform random search in scenarios with limited training budgets for supervised learners. Hyperparameter values such as regularization strength, model family choices, procedural elements, and data preprocessing choices play a crucial role in model performance. Hyperparameter optimization is crucial in machine learning, with choices like gradient descent step sizes and data preprocessing impacting success. Search methods vary between open loop, like random selection, and closed loop, like Bayesian optimization. These methods suggest configurations for training models and evaluating validation losses. This paper explores the landscape of open loop methods for hyperparameter optimization in machine learning. It discusses the advantages of closed loop methods but highlights the resurgence of interest in open loop methods due to the time-consuming nature of training modern deep learning models and the cost efficiency of cloud resources. Tradeoffs in open loop methods are identified and discussed. The paper discusses the landscape of open loop methods for hyperparameter optimization in machine learning, emphasizing the advantages of closed loop methods. It highlights the resurgence of interest in open loop methods due to the time-consuming nature of training modern deep learning models and the cost efficiency of cloud resources. The text explores tradeoffs in open loop methods, focusing on diversity-promoting methods like k-DPP random search. The paper discusses open loop methods for hyperparameter optimization in machine learning, contrasting them with closed loop methods like Bayesian optimization. It mentions the release of open source implementations of their hyperparameter optimization algorithm and the MCMC algorithm. The focus is on the efficiency and complexity of different optimization techniques. Recent research has explored using Determinantal Point Processes (DPPs) for optimizing hyperparameters in Bayesian optimization. Instead of sampling a single point at each iteration, a set of diverse points is sampled from a DPP over a small region of the space, allowing for easy parallelization within one iteration. Recent research has explored using Determinantal Point Processes (DPPs) for optimizing hyperparameters in Bayesian optimization. While this can lead to easy parallelization within one iteration, the overall algorithms are still sequential. Configuration evaluation methods have been shown to perform well by adaptively allocating resources to different hyperparameter settings. They initially choose a set of hyperparameters to evaluate, partially train models for these hyperparameters, compare them, and allocate more resources to the best-performing models. These algorithms produce high-quality models in the end. Recent trends have renewed interest in open loop methods for hyperparameter optimization tasks like deep networks. Random search has been shown to be competitive with sophisticated closed loop methods, inspiring further research. The concept of star discrepancy is important in numerical integration, quantifying the property of a sequence in higher dimensions. The Sobol sequence, a low-discrepancy sequence, is known to outperform random and grid search in hyperparameter optimization. However, it is only suitable for continuous spaces, not discrete dimensions. Another method, the DPP, has received less attention in this field. The Sobol sequence is effective in hyperparameter optimization for continuous spaces but not for discrete dimensions. DPPs have been less explored in this area. BID2 studied a DPP formulation with a star discrepancy between Sobol and random for all k values. Their theoretical approach motivates further exploration of DPPs for optimization. BID3 noted that random sequences with large gaps can impact optimization performance. The Euclidean norm is used as a surrogate for optimization performance, with the Sobol sequence showing superiority in star discrepancy. The Sobol sequence outperforms in star discrepancy, while DPP slightly edges out Uniform. All methods are comparable in distance to the center. Practitioners often define search space, where small bounds may lead to optimal solutions at edges or corners. Biasing sequences towards corners can be beneficial in some cases, contrary to low discrepancy sequences. DPP samples points based on distance, favoring corners, unlike Sobol and random sequences. This corner sampling behavior is common in Bayesian optimization schemes. In Bayesian optimization schemes, DPP tends to outperform uniform at random and Sobol in distance to the origin. The study delves deeper into DPPs and k-DPPs for hyperparameter tuning problems, focusing on sampling from a domain of values. In Bayesian optimization, DPP defines a probability distribution over subsets of Y based on dissimilarity. L-ensembles focus on quality and diversity of elements in Y. Quality q i, featurized representation \u03c6 i, and similarity kernel K are key components. Hyperparameter settings are discussed in Section 4.3. In Bayesian optimization, DPP defines a probability distribution over subsets of Y based on dissimilarity. Hyperparameter settings are fixed with q i = 1, but closed loop methods could use q i to adapt the DPP's distribution over time. k-DPPs are distributions over subsets of Y of size k. Sampling algorithms for k-DPPs are discussed, including a Metropolis-Hastings algorithm as a fast alternative. The proposed algorithm allows sampling from base sets with mixed discrete and continuous dimensions. Algorithm 1 initializes a set and samples indices from it, while Algorithm 2 samples points directly from the base set B when continuous dimensions are present. Algorithm 2 efficiently samples from a k-DPP over a space with mixed discrete and continuous dimensions by computing only the principal minors of L needed for relevant computations on the fly. It requires less computation and space compared to Algorithm 1, even with discrete dimensions in B. The vector \u03c6 i encodes y i as an attribute-value mapping for hyperparameters, with fixed segments assigned to each attribute. Numerical hyperparameters are rescaled to [0, 1] to prevent dominance in similarity calculations, while categorical hyperparameters are encoded with m elements for m values. The approach involves using a one-hot encoding for categorical hyperparameters with m values, computing similarity with an RBF kernel, and adjusting \u03c3 2 to affect model properties. Real-world hyperparameter search spaces often have tree structures, like the number of layers in a neural network. Binary hyperparameters, like regularization, are encoded similarly with one-hot encoding. When setting the regularization strength to \"off,\" it is equivalent to setting it to zero. Higher-level design decisions can be treated as hyperparameters, such as choosing between different types of models like logistic regression, convolutional neural networks, or recurrent neural networks. Gaussian processes are commonly used in hyperparameter optimization, with sampling from a DPP with a kernel being equivalent to sampling proportional to the posterior variance of a GP defined with the same kernel. In hyperparameter optimization experiments, k-DPP-RBF, uniform sampling, and Bayesian optimization are compared. Samples drawn using Algorithm 1 and Algorithm 2 are compared against random sampling. As k increases, all methods approach the true optimum. The experiments focus on hyperparameters affecting performance in a convolutional neural network for text classification. The study focuses on hyperparameter optimization for a convolutional neural network used in text classification. The model architecture includes a convolutional layer, max-over-time pooling layer, and fully connected layer with softmax. Three hyperparameters are considered: L2 regularization strengths, dropout rates, and learning rates. The goal is to find the optimal settings for improved accuracy in sentiment analysis. The study explores hyperparameter optimization for a convolutional neural network in text classification. It compares different sampling methods, including k-DPP-RBF and Bayesian optimization technique (BO-TPE), to find the best models with fewer iterations. Surprisingly, BO-TPE performs the worst despite using additional information, possibly due to an exploration/exploitation tradeoff. The study analyzed hyperparameters for CNN in text classification, comparing sampling methods like k-DPP and BO-TPE. BO-TPE's tradeoff leads to more local search before exploring fully, affecting global optima. BID24 identified six key hyperparameters for CNN stability in sentence classification. The study compared sampling methods for hyperparameter optimization in CNN text classification, finding that k-DPP-RBF outperforms uniform sampling. They identified key hyperparameters for stability in sentence classification, emphasizing the impact of learning rate. The study demonstrated the effectiveness of constructing k-DPPs over hyperparameter search spaces, showing improved performance over random sampling. Increasing the difficulty of the optimization problem resulted in greater benefits from k-DPP-RBF compared to uniform sampling. The study showed that constructing k-DPPs over hyperparameter search spaces led to improved performance over random sampling. An open-source implementation of the method is available."
}