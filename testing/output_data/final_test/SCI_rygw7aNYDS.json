{
    "title": "rygw7aNYDS",
    "content": "In this paper, the large-sample behaviors of Q-value estimates are investigated with closed-form characterizations of asymptotic variances. Confidence regions for Q-value and optimal value functions are efficiently constructed, along with policies to minimize estimation errors. A policy exploration strategy based on estimating relative discrepancies among Q estimates is proposed, showing superior performance in numerical experiments compared to benchmark approaches. The study focuses on the classical reinforcement learning problem where the agent aims to maximize accumulated discounted rewards in a Markov decision process environment with uncertain dynamics. The paper investigates statistically efficient methodologies to quantify errors and uncertainties in Markov decision processes. It focuses on large-sample behaviors of estimated Q-values and optimal value functions, showing asymptotic convergence to computable distributions as data sizes increase. The goal is to obtain better policies by minimizing estimation errors. The paper aims to provide accurate confidence regions for quantities related to the optimal policy, complementing existing error bounds. It also focuses on designing exploration policies using tight error estimates, particularly in the pure exploration setting for applications like autonomous driving. The paper proposes an efficient strategy for exploration in training optimal policies offline, focusing on optimizing worst-case estimated relative discrepancy among Q-values. This approach, termed Q-OCBA, utilizes Q-value estimates and randomized policies to achieve optimal budget allocations. Our technique, Q-OCBA, utilizes Q-value estimates and randomized policies to achieve optimal allocation, consistently outperforming other exploration policies in selecting the best policy and generating tight confidence bounds for value estimates. It resolves technical challenges in previous work by Mannor et al., allowing for generalization of variance results to Q-values and optimal value functions. Our results utilize an implicit function theorem applied to the Bellman equation to obtain gradients for Q-values and translate to the optimal value function. This allows for generalization to constrained MDP and approximate value iterations. The work is related to dynamic treatment regimes in medical decision-making but distinguishes itself with infinite-horizon results on optimal value and Q-value. The non-unique policy case corresponds to the \"non-regularity\" concept in DTR. The paper discusses the non-unique policy case in dynamic treatment regimes, where true parameters close to decision boundaries lead to sensitive policies. It presents results on large-sample behaviors, exploration strategies, and experimental findings in an infinite horizon discounted reward MDP setup. Generalizations to constrained MDP and approximate value iteration are also included in the Appendix. In an MDP setup, the state space S, action space A, random reward R(s, a), transition probability P(s|s, a), discount factor \u03b3, and initial state distribution \u03c1 are defined. The reward and transition probability distributions are unknown to the agent. Stochasticity assumption is made regarding the reward with finite mean and variance. A policy \u03c0 maps states to action probabilities. The value function associated with a policy \u03c0 is defined as V * = V \u03c0 * and Q-value is denoted by Q(s, a). The Bellman equation for Q takes the form for any (s, a) \u2208 S \u00d7 A. The optimal policy \u03c0 * is unique and deterministic under Assumption 2. The text discusses the statistical quantities arising from data, including sample mean and variance of the reward, empirical transition matrix, and sampling covariance matrix. It also introduces the estimate of Q, called Qn, which is the empirical fixed point of T\u03bc R,n, Pn. Additionally, it mentions the focus on empirical errors due to noises in the data. The text focuses on empirical errors due to noises in collected data and assumes MDP or Q-value evaluation can be done offline. It presents results on the asymptotic behaviors of Qn and V*n, making an assumption on the exploration policy \u03c0 to gather data. The assumption is that the Markov chain with transition probability P\u03c0 is positive recurrent, with a unique stationary distribution denoted as w. The text discusses the asymptotic normality of Qn under exploration policy \u03c0, assuming positive recurrent Markov chain with transition probability P\u03c0. It introduces notations like \"\u21d2\" for convergence in distribution and N(\u00b5, \u03a3) for multivariate Gaussian distribution. The dimension of vectors N(\u00b5, \u03a3), I, and ei are clarified, and algebraic derivations involve re-arranging vectors. Theorem 1 states that under certain assumptions, Qn is a consistent estimator of Q, with an explicit form of the asymptotic variance \u03a3 derived from the delta method. The variances of parameter estimates involve \u03c3^2R(i, j) and \u03a3Pi,j, proportional to sample size w(i, j). Leveraging Theorem 1 establishes the asymptotic normality of V*n and \u03c7*n. Corollary 1 establishes the asymptotic normality of V*n and \u03c7*n under certain assumptions. Additionally, using an implicit function theorem on the Bellman equation, the results are derived for optimal value functions, providing distributional statements. This approach generalizes results for fixed policies and offers an alternative route through perturbation analysis on the linear program representation of the MDP. The perturbation analysis on the linear program representation of the MDP provides gradient information for V* and Q. Theorem 1 and Corollary 1 allow for statistical inference, constructing confidence regions for subsets of Q-values. A key quantity for exploration policies is the difference between actions in a state. When the optimal policy for the MDP is not unique, estimated Q and V* may vary between optimal actions. In situations where the optimal policy for the MDP is not unique, estimated Q and V* may fluctuate between optimal actions, leading to a complex large-sample behavior. Theorem 2 states that in such cases, there are multiple distinct matrices {Gk} and a non-Gaussian limit distribution. This non-Gaussian behavior is due to the sensitivity to perturbation direction, resulting from solution non-uniqueness in the MDP's LP representation. This phenomenon is similar to the \"non-regularity\" concept in DTR, where estimation noises near decision boundaries can cause a 1/\u221an-order bias. The policy is sensitive to estimation noises and exhibits a 1/ \u221a n-order bias. When the optimal policy is non-unique, a bias arises as shown in Theorem 2. Generalizations for constrained MDP and approximate value iteration are also discussed. Exploration policies are designed to minimize the probability of selecting a suboptimal policy. A strategy is proposed to maximize the worst-case relative discrepancy in Q-value estimates. The procedure aims to maximize the minimum relative discrepancy among Q-value estimates, focusing on the difficulty in obtaining the optimal policy due to estimation errors. The criterion aims to make the problem \"easiest\" by minimizing the probability of suboptimal selection between close Q-values for different actions. The Q-OCBA procedure aims to minimize suboptimal selection between close Q-values for different actions by using an exponential decay rate controlled by h ij. The criterion requires model primitives Q, P, and \u03c3 2 R for optimization and parameter update, with data collected through a Markov chain on exploration actions. The Q-OCBA procedure minimizes suboptimal selection between close Q-values for different actions using an exponential decay rate controlled by h ij. Data is collected through a Markov chain on exploration actions to derive a characterization for admissibility of policies. Optimizing over admissible policies is equivalent to optimizing over stationary distributions, which is more tractable due to the linear structure of W. The set W \u03b7 = W \u2229 {w \u2265 \u03b7} is used in practice to ensure closedness of the set. Algorithm 1 describes Q-OCBA. The Q-OCBA procedure minimizes suboptimal selection between close Q-values for different actions using an exponential decay rate controlled by h ij. Algorithm 1 describes Q-OCBA with two stages. Criterion (7) can be modified based on the decision goal, such as obtaining the best estimate of \u03c7 * by considering min w\u2208W\u03b7 \u03c3 2 \u03c7. Additional experiments are showcased in the Appendix. The sequential updating rule for exploration in Q-OCBA involves running a policy for a certain number of steps in each batch. The closed-form characterization of \u03a3 allows for estimation of coefficients c ij (s, a) using plug-in estimators with data collected in earlier stages. In Q-OCBA, plug-in estimators are used to estimate coefficients cij(s, a) with data from earlier stages. Numerical experiments support large-sample results and compare Q-OCBA performance against benchmark methods using the RiverSwim problem. Rewards are given at boundary states, and a policy with 0.8 probability of swimming right is used. The coverage rates of constructed 95% CIs are shown for small and large m values, with accurate CI coverages observed for large sample sizes. The efficiency of the exploration policy is then investigated by comparing Q-OCBA with K = 2 to four benchmark policies. The study compares different exploration policies, including random exploration, UCRL2, and PSRL with varying parameters. A two-stage implementation is used for all policies, with an initial warm start for better performance. The probabilities of obtaining the optimal policy are compared, showing the importance of the warm start for UCRL2 and PSRL. In numerical experiments, Q-OCBA outperforms other methods in policy selection with small and large data sizes. Benchmark policies perform worse with larger values of r L due to misalignment between Q-values and exploration needs. Q-OCBA focuses on efficient exploration to minimize incorrect policy selection. Our goal is to minimize incorrect policy selection by utilizing variance information from the first stage. Additional results on large-sample behaviors for constrained MDPs and estimations based on approximation value iteration are presented. The constrained MDP setting for budgeted decision-making and safety-critical applications is considered. The aim is to maximize long-run accumulated discounted reward while constraining the long-run accumulated discounted cost. Data is assumed to come in as before, with observations on the incurred. We estimate the cost and focus on the estimation error of the optimal value in constrained MDPs. An optimal policy involves a \"split\" policy with a randomization probability called the mixing parameter \u03b1*. The randomization probability in constrained MDPs is referred to as the mixing parameter \u03b1*. The optimal policy may involve randomization between two actions at a specific state, denoted by \u03b1*. This occurs when the constraint is non-binding, leading to an unconstrained scenario. In constrained MDPs, the mixing parameter \u03b1* determines randomization between actions. When the constraint is non-binding, it reduces to an unconstrained scenario. If the constraint is binding, \u03b1* must be chosen to ensure equality, leading to additional noise in the estimation of V*. Updating a large state space using T \u00b5 R ,P (.) may be computationally infeasible, so approximate value iteration with a dimension-reducing mapping M is used. In constrained MDPs, a dimension-reducing mapping M is used for approximate value iteration to update a large state space efficiently. The mapping includes an \"inherit\" and \"generalization\" mapping, with a fixed point Q M derived from the operator M \u2022 T \u00b5 R ,P (\u00b7). Large-sample error estimates are obtained under the assumption of a well-defined metric on the state space S. Assumptions are made on the generalization map M g to guarantee the existence of Q M, including being a max-norm non-expansion mapping in S. Unique properties of Q M and the subset S 0 are also defined. The text discusses the properties of Markov Chains with transition probability P \u03c0 and positive recurrent communication classes. It also mentions the empirical estimator Q M n and the estimation quality of Q-values, V * and \u03c7 * in numerical experiments. Additionally, it explores the use of dimension-reducing mapping M in approximate value iteration for constrained MDPs. In this section, additional numerical results are provided for Tables 1 and 2 in the main paper, focusing on the estimation quality of Q-values. The coverage rates converge to 95% as the number of observations increases, with consistent behaviors across individual estimates. A sample size of 5 \u00d7 10^4 (or lower) is sufficient to elicit asymptotic results. In this section, additional numerical results are provided for Tables 1 and 2 in the main paper, focusing on the estimation quality of Q-values. A sample size of 5 \u00d7 10^4 (or lower) is enough to elicit asymptotic results in Theorem 1 and Corollary 1. Tables 6, 7, and 8 compare CI coverage rates when the state space is large, with different values of \u03c0(1|s). The coverage convergence for approximate update is generally slower compared to exact update. Sample size of 5 \u00d7 10^4 is not enough for approximate update to obtain the nominal coverage. Tables 6, 7, and 8 demonstrate varying convergence rates to nominal coverage based on different \u03c0(1|s) values. Faster convergence is observed at \u03c0(1|s) = 0.85, reaching 95% coverage at n = 10^5. In contrast, \u03c0(1|s) = 0.8 requires n = 10^7 for 95% coverage, while \u03c0(1|s) = 0.9 struggles to converge even at n = 10^7. Discrepancies in estimates of Q, V*, and \u03c7* are evident when coverage deviates from the nominal rate. Caution is advised in such cases. In Q-OCBA, the second-stage exploration policy aims to maximize the worst-case relative discrepancy among all Q-value estimates to achieve accurate coverage. It is recommended to solve min to derive the optimal second-stage exploration policy for obtaining the best estimate of \u03c7* with a short confidence interval. Comparison with benchmark strategies shows the effectiveness of this approach for different scenarios. In Q-OCBA, the strategy aims to shorten the confidence intervals of estimates by comparing it with pure RE and \u03b5-greedy strategies. Results show that Q-OCBA achieves nominal 95% coverages for both Q-values and \u03c7* with significantly shorter confidence intervals. Q-OCBA leads to much shorter confidence intervals compared to RE and \u03b5-greedy strategies, with the shortest CI lengths in all settings. For example, when r L = 2, the CI length derived by Q-OCBA is at least 80% less than those derived by other methods. Q-OCBA also performs more stably than \u03b5-greedy, especially for different values of r L. Q-OCBA outperforms pure RE and \u03b5-greedy strategies, with at least 40% shorter CI lengths for \u03c7 * estimates. The performance of \u03b5-greedy varies with r L, as the exploration \"preference\" changes with Q-values. This misalignment results in different performances for \u03b5-greedy. NA indicates unvisited (s, a) pairs. In this section, the proofs of the main results are presented. By treating P as an N-dimensional vector, the partial derivatives of F are denoted. P is interpreted as a transition matrix of a Markov Chain, and the implicit function theorem is applied to the equation F(Q, \u00b5 R, P) = 0. The implicit function theorem is applied to the equation F(Q, \u00b5 R, P) = 0, resulting in a unique continuously differentiable function \u03c6. The partial derivatives of \u03c6 satisfy certain conditions, and the delta method is used to show convergence. The asymptotic normality of \u03c7 \u03c0 * n follows from the continuous mapping theorem. The asymptotic normality of the estimated value function under a given policy is established using the implicit function theorem. Corollary 2 provides conditions for the transition matrix and a diagonal matrix, with a unique continuously differentiable function \u03c6\u03c0. The proof of Theorem 2 establishes the asymptotic normality of the estimated value function under a policy using the implicit function theorem. It involves a unique continuously differentiable function \u03c6\u03c0 and the LP representation of the MDP problem. The dual problem's variables represent occupancy measures, and non-unique optimal policies lead to non-unique optimal solutions, causing degeneration in the primal problem. In this case, multiple choices for basic variables at the optimal solution exist. Perturbations in coefficients along intersecting hyperlanes affect the objective value. Directions can be partitioned into subsets, impacting the LP optimal value by fixing basic variables. Gradient vectors corresponding to directions are denoted as G \u03c1 k. The argument focuses on the LP with objective value s \u03c1(s)V (s), which can be repeated for each V (s) by setting \u03c1(s) = e s. The directional Jacobian of V with respect to P, \u00b5 R is defined by taking n \u2192 \u221e, resulting in the continuous mapping theorem. The LP representation of the constrained MDP is used to define x s,a as the occupancy measure. Solving for an optimal solution (x * s,a ) s,a in the LP leads to the translation of the solution. The LP representation of the constrained MDP defines x s,a as the occupancy measure, which is solved for an optimal solution. The optimal policy is obtained from this solution, with a unique and non-degenerate LP. Perturbing the parameters does not lead to negativity for non-basic variables. Two cases are considered based on the binding of the first constraint, leading to either a deterministic optimal policy or a perturbed policy with retained constraints. The analysis focuses on the perturbation of parameters in the LP representation of the constrained MDP. It considers cases where constraints remain binding, leading to deterministic or perturbed optimal policies. The optimal policy is split at a state and satisfies a specific condition, with mixing parameter \u03b1* being a function of \u00b5C and P. The value function V* can be seen as a function of \u00b5R and P. The value function V* is a function of \u00b5R, \u00b5C, and P, with Jacobians denoted by \u2207\u00b5R,\u00b5C,P V* and \u2207\u00b5R,\u00b5C,P,\u03b1 V*. The notation distinguishes between multi-dimensional Jacobian matrices and scalar Jacobians. The value function V* is a function of \u00b5R, \u00b5C, and P, with Jacobians denoted by \u2207\u00b5R,\u00b5C,P V* and \u2207\u00b5R,\u00b5C,P,\u03b1 V*. The derivation of \u2207\u00b5R,\u00b5C,P V* (\u00b5R, P, \u03b1*) and \u2207\u00b5R,\u00b5C,P L \u03c0* (\u00b5C, P, \u03b1*) follows the same line of analysis as G\u03c0 and H\u03c0 V in the proof of Corollary 2. Theorem 4 is proven by changing the distribution of [\u00b5R,n, Pn] S\\S0 without affecting QMn, assigning auxiliary random variables to \u00b5R,n and Pn, and defining independent random variables for each i /\u2208 S0. The stationary distribution w is obtained by letting i \u2208 S in transition matrix P \u03c0w."
}