{
    "title": "B1g0QmtIIS",
    "content": "Reservoir computing is a powerful tool for explaining how the brain learns temporal sequences, such as movements. A network can learn complex sequences with a reward-modulated Hebbian learning rule when combined with a dynamic working memory network. This approach offers a biologically plausible interpretation of learning rules and paradigms, performing as well as FORCE learning for complex temporal sequences. Reservoir computing, also known as liquid computing or echo-state networks, is a challenging framework for computational brain models. It involves a reservoir of rate units with strong, random connections and a linear readout for learning temporal sequences. While the FORCE rule leads to excellent performance on tasks like motor movements, it is biologically implausible due to rapid and large synaptic updates. Reward-modulated Hebbian learning rules offer a more realistic alternative, but struggle with learning complex tasks when feedback is delayed. Combining a reservoir network with a structured network storing a continuous variable as a \"bump\" in an attractor facilitates reward-modulated Hebbian learning. This dynamic working memory serves as input to the reservoir network, overcoming structural weaknesses and allowing efficient learning with a delay in the feedback signal. Our architecture combines an attractor network with a reservoir network to perform multiple tasks. The attractor network generates a two-dimensional neural trajectory, which shapes the dynamics of the reservoir network to produce a high-dimensional output. The attractor network consists of 2500 neurons evolving with time constants for firing rates, external input, and adaptation variables. The weight matrix J = J s + J h creates stable activity patterns in a reservoir network by combining adaptation variables with input. The network approximates a target function with output z(t) using firing rates and readout weights. The architecture combines an attractor network with a reservoir network to perform multiple tasks. The learning rules in the curr_chunk regulate chaotic activity and implement a feedback loop using reward-modulated Hebbian rule. The update rule mimics gradient descent and is compared on target functions sampled from a Gaussian Process. Performance is measured with normalized cross-correlation between output and target on a single trial with frozen weights. The full network with attractor input and reward-modulated Hebbian learning learns faster and more reliably than reward-modulated Hebbian learning without the input from the attractor network. The full network achieves the performance of the FORCE rule after about 90 training trials for one-second signals, and after 200 trials for 10-second target signals when combined with input from the attractor network. The three-factor learning rule succeeds in learning complex tasks when combined with temporally structured input from the attractor network. Even with temporally sparse updates, learning is still possible, but input from the dynamic working memory is necessary for task achievement. In the absence of attractor input, learning fails. Delayed updates do not harm performance, achieving high cross-correlation in fewer than 100 trials with strong attractor network input. Different inputs generate unique neural trajectories exploitable by the reservoir network for learning multiple tasks. The network is trained to produce hand-written digits using attractor trajectories from the pre-processed MNIST dataset. The reservoir learns a single drawing for each class, with increased structural noise for more robust bump trajectories. The reward-modulated Hebbian rule masters 10 classes. The reward-modulated Hebbian rule achieves high accuracy in mastering hand-written digits, even with noise in the attractor network. Reservoir computing with dynamic working memory facilitates learning complex tasks with biologically plausible three-factor learning rules. The proposed network combines a bump attractor with reward-modulated learning to match the efficiency of FORCE. It suggests using a limited number of trajectories in the attractor network and potentially combining it with another input for increased capacity. This method could have real-world applications in reservoir computing and be a cost-effective alternative for learning in neuromorphic devices. The simulation details for neuromorphic devices included training without breaks in dynamics, testing with frozen weights, and restoring pre-training activity. The code for experiments is available at a specific GitHub link. Test functions were drawn from Gaussian processes, with parameters chosen to match target complexity. The attractor network parameters included time constants \u03c4 m = 30 ms, \u03c4 a = 400 ms, and adaptation strength s = 1.5. External input e was drawn from a Gaussian distribution N(1, 0.0025^2) for each neuron. The connectivity matrix J = Js + Jh had a noisy part drawn independently as (Jh)ij \u223c N(0, \u03c3^2/Nattr), with Nattr = 2500 and \u03c3 = 2 in most experiments. The reservoir network parameters included a time constant of \u03c4 = 50 ms and a total coupling strength of \u03bb = 1.5. The readout weights were initialized to zero, and the feedback weights were drawn from a uniform distribution. Neurons were arranged on a 2D grid with mutual excitation of nearby neurons and inhibition of distant ones. The bump center corresponded to the mean activity on the torus, and the center on the x-axis was calculated using a counterclockwise angle. The recurrent connections and weights in the reservoir were drawn independently. State and exploratory noise were generated from uniform distributions. Learning rules included low-pass filtering and a computed learning rate."
}