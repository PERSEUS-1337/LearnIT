{
    "title": "r1eiu2VtwH",
    "content": "Deep neural networks (DNNs) are widely used in machine learning for various tasks, but their advantage over gradient boosting decision trees (GBDT) in tabular data remains uncertain. A new architecture called Neural Oblivious Decision Ensembles (NODE) is introduced in this paper, which combines the benefits of ensembles of decision trees with end-to-end optimization and hierarchical representation learning. Extensive experiments comparing NODE to GBDT on multiple tabular datasets show promising results. The proposed NODE architecture outperforms leading GBDT packages on tabular datasets, showing its advantage in machine learning tasks. The PyTorch implementation of NODE is open-sourced, aiming to become a universal framework for tabular data. The recent rise of deep neural networks has significantly improved various machine learning tasks, but tabular data has not fully benefited from DNN power yet. Machine learning for tabular data has not fully benefited from the power of deep neural networks. While many works address this issue, deep learning approaches do not consistently outperform leading gradient boosted decision tree models. There is still no universal deep neural network approach that consistently outperforms GBDT models in Kaggle competitions. The NODE architecture, inspired by CatBoost, aims to improve deep learning solutions for tabular data problems. It generalizes CatBoost by making feature choice and decision tree routing differentiable, resulting in a fully differentiable architecture. The NODE architecture is fully differentiable and can be integrated into existing DL packages like TensorFlow or PyTorch. It allows for constructing multi-layer architectures similar to \"deep\" GBDT, trained end-to-end. The use of oblivious decision tables and entmax transformation are crucial for achieving state-of-the-art performance. In experiments, NODE consistently outperforms leading GBDT implementations on most datasets. The paper introduces a new DNN architecture, NODE, for machine learning on tabular data. It outperforms leading GBDT packages and has a PyTorch implementation available online. The rest of the paper reviews prior work, describes the NODE architecture, and presents experimental evaluations. Ensembles of decision trees like GBDT and random forests are popular for tabular data. Leading GBDT packages include XGBoost, LightGBM, and CatBoost, with CatBoost using oblivious decision trees. Oblivious decision trees (ODTs) are constrained to use the same splitting feature and threshold in all internal nodes of the same depth, making them weaker learners compared to unconstrained decision trees. However, in ensembles, ODTs are less prone to overfitting and synergize well with gradient boosting. ODTs allow for efficient inference by computing independent binary splits in parallel. In contrast, non-oblivious decision trees require evaluating splits sequentially. In contrast to non-oblivious decision trees, which require sequential split evaluation, oblivious decision trees (ODTs) use the same splitting feature and threshold in all internal nodes of the same depth. ODTs are weaker learners but are less prone to overfitting in ensembles and work well with gradient boosting. They allow for efficient parallel computation of independent binary splits. To address limitations in tree construction, recent works propose \"softening\" decision functions in internal tree nodes to make the tree function and routing differentiable. The entmax transformation is advocated for this purpose, offering advantages over previous approaches. The entmax transformation is proposed as an inductive bias in models for differentiable split decision construction in internal tree nodes, allowing for efficient feature selection and computation. In the context of multi-layer non-differentiable architectures, various works have explored the construction of architectures using nondifferentiable blocks like random forests or GBDT ensembles. While these approaches show some improvements over shallow models, they lack end-to-end training capabilities. In contrast, the importance of end-to-end training is emphasized, especially for DNNs designed for tabular data. The recent introduction of the Neural Oblivious Decision Ensemble (NODE) architecture has shown consistent outperformance of tuned GBDTs across various datasets. NODE utilizes differentiable oblivious decision trees (ODT) trained end-to-end through backpropagation. The architecture is described in detail in different sections, including the implementation of the NODE layer, the full model architecture, and the training and inference procedures. The model architecture includes a Neural Oblivious Decision Ensemble (NODE) layer composed of differentiable oblivious decision trees (ODTs). Each ODT splits data along splitting features and compares them to learned thresholds, returning one of 2^d possible responses. The output is a sum of leaf responses scaled by choice weights. The model architecture features a Neural Oblivious Decision Ensemble (NODE) layer with differentiable oblivious decision trees (ODTs). The ODTs determine the tree output through splitting features and comparison operators, which can be made differentiable using the \u03b1-entmax function for efficient training. The model architecture includes a Neural Oblivious Decision Ensemble (NODE) layer with differentiable oblivious decision trees (ODTs) using the \u03b1-entmax function for efficient training. The choice function is replaced by a weighted sum of features computed with \u03b1-entmax (\u03b1=1.5) over a learnable feature selection matrix. The Heaviside function is relaxed as a two-class entmax, and the final prediction is a weighted linear combination of response tensor entries with weights from a choice tensor. This relaxation is equivalent to the classic non-differentiable ODT when feature selection and threshold functions reach a one-hot state. The NODE layer in the model architecture consists of densely connected NODE layers, each containing several trees whose outputs are concatenated. The final prediction is obtained by averaging the outputs of all trees. The architecture can be trained alone or within a complex structure like fully connected layers organized into multi-layer architectures. Our architecture consists of densely connected NODE layers, where each layer uses a concatenation of all previous layers as input. It can learn both shallow and deep decision rules by allowing each tree to rely on outputs from previous layers. The final prediction is an average of all decision trees from all layers, with early layer outputs used as inputs for subsequent layers. The dimensionality of the outputs is not restricted to the number of classes. The training protocol includes data preprocessing to normalize features, initialization using data-aware techniques, and the use of an additional hyperparameter l for averaging predictions. The dimensionality of the outputs is not limited to the number of classes, with l typically ranging from 1 to 3. During training, feature values are observed in the first data batch, with scales initialized for samples in the linear region. Response tensor entries are initialized with a standard normal distribution. Model parameters are jointly optimized, using traditional objective functions and QuasiHyperbolic Adam for optimization. Model parameters are averaged over consecutive checkpoints and the optimal stopping point is chosen on the hold-out validation dataset. Inference involves computing the entmax function during training. During training, the entmax function and choice tensor are computed, with the option to pre-compute entmax feature selectors for more efficient inference. A comparison with GBDT packages is presented, along with ablation studies on the NODE architecture. Experiments are conducted on various datasets with the \u03b1 parameter set to 1.5 in the entmax transformation. The datasets used for comparison include Epsilon, YearPrediction, Higgs, Microsoft, Yahoo, and Click. Train/test splits were provided for all datasets, with 20% of the train set used for validation. Different loss functions were minimized depending on the dataset type, with classification error reported for classification datasets and mean squared error for regression and ranking datasets. The proposed NODE architecture was compared to Catboost and XGBoost as baselines. The authors used XGBoost and FCNN for comparison in two regimes: default hyperparameters and tuned hyperparameters. In the default regime, they did not tune hyperparameters and used default settings provided by GBDT packages. The only tunable parameter was the number of trees. The default architecture in the model contains a single layer with 2048 decision trees of depth six, inherited from CatBoost settings. NODE architecture benefits from end-to-end training via back-propagation. Tuned hyperparameters are optimized for NODE and competitors on validation subsets, with optimal configuration containing between two and eight NODE layers. Results are summarized in Table 1 and Table 2, reporting mean performance and standard deviations over ten runs. The proposed NODE architecture consistently outperforms CatBoost and XGBoost on tabular problems with default hyperparameters. Tuned NODE also outperforms competitors on most tasks, except for Yahoo and Microsoft datasets where XGBoost performs better. NODE should be extended to non-oblivious trees for future work. In the regime with tuned hyperparameters, FCNN outperforms GBDT on some datasets while GBDT is superior on others. The proposed NODE architecture shows universal performance, excelling in most tasks. Comparison with mGBDT and DeepForest reveals scalability issues and Out-Of-Memory errors on many datasets. In this section, we analyze the key architecture components that define our model. Differentiable decision trees require a function for feature selection and tree routing. We experimented with four options: Softmax, Gumbel-Softmax, Sparsemax, and Entmax, each with unique implications. Entmax with \u03b1=1.5 outperforms sparsemax, softmax, and Gumbel-Softmax in learning sparse decision rules with smoother optimization. It shows superior empirical performance across all datasets, unlike the other choice functions. Gumbel-Softmax struggles to learn deep architectures due to its stochastic nature, resulting in noisy responses in earlier layers. Feature importance decreases with depth, with the first layer being the most utilized. Deep layers can still produce important features, aided by the DenseNet architecture. The DenseNet architecture enhances the contribution of deep trees to the final response. There is an anticorrelation between feature importances and contributions, indicating earlier layers produce informative features while later layers focus on accurate prediction. Comparing NODE runtime to state-of-the-art GBDT implementations, training and inference times are reported for ensembles of 1024 trees on the YearPrediction dataset. In this paper, a new DNN architecture for deep learning on heterogeneous tabular data is introduced. The architecture involves differentiable deep GBDTs trained end-to-end via backpropagation. Extensive experiments show the advantages over existing competitors with default and tuned hyperparameters. Incorporating the NODE layer into complex pipelines trained via back-propagation is a promising research direction, especially in multi-modal problems. The NODE layer can be used in multi-modal problems to incorporate tabular data, similar to how CNNs are used for images or RNNs for sequences. Different hyperparameter optimization methods like Catboost, XGBoost, and FCNN are employed, with 50 steps of TPE optimization. The final hyperparameters are chosen based on the smallest loss on the validation set, with the number of trees determined by the validation set. Below are the hyperparameters and search spaces for Catboost."
}