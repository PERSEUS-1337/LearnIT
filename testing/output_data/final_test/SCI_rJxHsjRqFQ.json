{
    "title": "rJxHsjRqFQ",
    "content": "Recent approaches have shown the benefits of learning shallow network parameters in hyperbolic space. This work extends that by applying hyperbolic geometry to embeddings used in attention mechanisms for neural networks. By changing the geometry of object representations, the embedding space can be used more efficiently without increasing model parameters. Hyperbolic geometry can encode objects more effectively as the number of objects grows exponentially from the query. This method improves generalization in tasks like neural machine translation, graph learning, and visual question answering while keeping neural representations compact. The focus of this work is to endow neural network representations with suitable geometry to capture fundamental properties of data, including hierarchy and clustering behavior in complex networks. Hyperbolic geometry is introduced to encode objects more effectively as the number of objects grows exponentially, improving generalization in tasks like neural machine translation, graph learning, and visual question answering. Complex networks exhibit a scale-free structure with power law distribution on node degrees. Hyperbolic spaces represent hierarchical organization effectively. Hyperbolic geometry is well-suited for modeling relational data due to its properties. In hyperbolic space, the circumference and area of a disc grow exponentially with radius, contrasting with Euclidean space where growth is polynomial. The growth of volume in hyperbolic space allows for better separation of concepts at increasing semantic distances. In hyperbolic space, the volume grows exponentially, allowing for better separation of concepts at increasing semantic distances. The embeddings in hyperbolic space take into account the density in that particular region, correcting any noise introduced in the system. In hyperbolic space, embeddings are equally distinguishable regardless of density, correcting any noise introduced in the system. The connection between hyperbolic space and scale-free networks is highlighted, showing how heterogeneous topology implies hyperbolic geometry. Recent interest in learning non-Euclidean embeddings directly from data has grown in the machine learning community. Hyperbolic attention networks are proposed to increase neural network capacity without adding trainable parameters. Unlike previous methods, hyperbolic geometry is applied to activations of deep networks, improving generalization in tasks like neural machine translation and visual question answering. This approach utilizes hyperbolic operations for attention mechanisms efficiently. In the hyperboloid model of hyperbolic space, Nickel and Kiela (2018) proposed a method to learn shallow embeddings of graphs. Different models of hyperbolic space, such as the Poincar\u00e9 ball model, offer different coordinate systems for computation. The hyperboloid model is used in this paper for its unbounded nature, making it a convenient target for projecting into hyperbolic space. The Klein model is an efficient way to express hyperbolic aggregation operations. It is a subset of R^n where x < 1, and can be obtained from the hyperboloid model through projection. The geometric relationship between the Klein and hyperboloid models is illustrated in FIG5 of the supplementary material. The Klein model efficiently expresses hyperbolic aggregation operations, obtained from the hyperboloid model through projection. Graph neural networks incorporate message passing to capture relations between entities, while graph convolution networks use convolutions to learn graph representations. Relational reasoning models can be expressed through an attentive read operation. The attentive read operation is a key component in relational reasoning models, widely used in deep learning for various applications. It involves matching a query with keys to compute a scalar score and read values from memory locations. The operation can be broken down into matching and reading components. The attentive read operation involves matching a query with keys to compute attention weights and aggregating values using these weights. It can be seen as message passing on a graph, where nodes aggregate messages from neighbors. This operation is a primitive in many message passing neural network architectures and can be adapted to leverage hyperbolic geometry. Relation Networks (RNs) are a neural network architecture designed for reasoning about the relationships between objects. RNs operate on a set of objects by applying a shared operator to each pair of objects, augmented by global information. The result of each relational operation is passed through a global transformation, allowing for general learnable functions. Interpreting RNs as learned message passing on a graph, attention weights represent edge weights in the graph. The authors define scaled dot-product attention in the Transformer model as a way to perform attentive reads in parallel. They experiment with softmax and sigmoid operations for computing attention weights in hyperbolic models. In some applications like visual question answering, sigmoid attention weights are considered to prevent competition between attention weights over different entities. The attentive read operation from Section 3.1 is redefined as an operation on points in hyperbolic space, utilizing new matching and aggregation functions. Mapping network activations onto the hyperboloid allows interpretation as points in hyperbolic space, maintaining the rapid scaling behavior of hyperbolic space. Mapping neural network activations into hyperbolic space involves careful consideration due to the unique structure of hyperbolic space. Activations are mapped into the hyperboloid, which is the only unbounded model of hyperbolic space commonly used. Points in hyperbolic space are expressed in pseudo-polar coordinates, consisting of a radius and a direction vector. The coordinates are assumed to be normalized. In hyperbolic space, neural network activations are mapped onto the hyperboloid using a projection that increases scale exponentially. This preserves exponential growth in volume and maintains appropriate scaling properties. An attentive read operation is then built to operate on points in hyperbolic space, leveraging hyperbolic geometry in matching and aggregation steps separately. In hyperbolic space, neural network activations are mapped onto the hyperboloid using a projection that increases scale exponentially. An attentive read operation operates on points in hyperbolic space, leveraging hyperbolic geometry for matching pairs of points using hyperbolic distance and hyperbolic aggregation with the Einstein midpoint. In hyperbolic space, neural network activations are mapped onto the hyperboloid using a projection that increases scale exponentially. The Einstein midpoint is used for hyperbolic aggregation due to its properties resembling a weighted average in Euclidean space. The derivation of this operation is complex and not covered in the paper. Models are evaluated on synthetic and real-world tasks, with experiments showing clear results when the underlying graph structure is explicitly known. Experiments demonstrate the benefits of using hyperbolic geometry as an inductive bias in various tasks and architectures, including diagnostic visual question answering and neural machine translation. Hyperbolic attention in feed-forward networks, Transformer, and Relation Networks shows effectiveness, especially in compact representations for small models. The algorithm of von BID46 efficiently generates large scale-free graphs to test the model's predictive tasks. In this study, large scale-free graphs are generated to test the model's predictive tasks using Recursive Transformer (RT) models with hyperbolic and Euclidean attention. The RT model has similarities to Graph Attention Networks and is used for link prediction in graphs with 1000 and 1200 nodes. The hyperbolic RT model performs well in these experiments. The hyperbolic RT outperforms the Euclidean RT on tasks involving graphs of 1000 and 1200 nodes. Results are shown in FIG4. The goal is to predict the shortest path length between nodes, treated as a classification problem with a max pathlength of 25. Rejection sampling is used during training for a uniform path length distribution. Test paths are sampled uniformly at random. Activation scale distribution is visualized in FIG4. The model tends to use larger scales for larger graphs when training on graphs of size 100 and 400. A baseline comparison to the optimal constant predictor is done, which performs well due to the skewed path length distribution on the test set. Training data is generated online for both tasks, starting with smaller graphs and gradually increasing the number of vertices. The dataset generation procedure and curriculum scheme are detailed in the supplementary material. The investigation focuses on hyperbolic attention for relational modeling. Our models, incorporating hyperbolic attention, outperform the original Relation Nets on the Sort-of-CLEVR dataset with 99.2% accuracy. In low-capacity scenarios, the attention mechanism in hyperbolic space improves performance by 20% over standard RNs. Additionally, we evaluate our models on Citeseer and Cora datasets using a similar experimental setup. The study evaluated graph transduction tasks using Citeseer and Cora datasets with models incorporating hyperbolic attention. Results showed improved performance compared to original Relation Nets, with H-GAT achieving 83.5% accuracy on Citeseer and 72.9% on Cora. The experimental setup followed the protocol defined in BID43. The study compared the performance of hyperbolic attention models (H-GAT) against baseline GAT models on Citeseer and Cora datasets. Results showed improvements in accuracy over 100 random seeds, with H-GAT outperforming the original GAT model. Visualizations of hyperbolic embeddings were also presented. Additionally, Relation Networks with various attention mechanisms were trained on the CLEVR dataset for visual question answering tasks focusing on relational reasoning. In experiments comparing different attention mechanisms on the CLEVR dataset for relational reasoning tasks, hyperbolic attention with sigmoid consistently outperformed other models, achieving 95.7% accuracy. This outperformed the previous state-of-the-art model, RN, which achieved 95.5% accuracy. The Transformer model, known for neural machine translation, heavily relies on attention as its core operation. The Transformer model has been extended by replacing its scaled dot-product attention with hyperbolic attention. Different versions of the model were trained using various coordinate systems and attention normalization functions. Model sizes include tiny, base, and big, with varying numbers of layers, units, and attention heads. The base model has 6 layers of encoders and decoders, each with 512 units and 8 attention heads. Hyperbolic attention coupled with sigmoid activation function shows improvements over the Euclidean model, especially with restricted model capacity. The best model achieves a 28.52 BLEU score, outperforming the original version. This novel approach imposes inductive biases from hyperbolic geometry on deep neural networks. Our proposed hyperbolic attention mechanism, utilizing hyperbolic geometry in attention weight computation and value aggregation, improves performance on various tasks such as link prediction, shortest path length prediction, visual question answering, graph transduction, and machine translation. The gains are significant in smaller models, confirming that hyperbolic geometry leads to more compact representations. Future work could explore using hyperbolic geometry for activations in neural networks. In future work, using hyperbolic geometry as an inductive bias for neural network activation in memory is a potential direction. Different models of hyperbolic space are illustrated in FIG5, with one-to-one isometric transformations between them. The hyperboloid model is unbounded, while Klein and Poincare models are bounded in a disk. The algorithm by von BID46 is used, with parameters set to \u03b1=0.95 and edge_radius_R_factor=0.35. Code for operations in hyperbolic space will be released. Curriculum played a crucial role in training on scale-free graph tasks, with a curriculum approach used for LP and SPLP tasks. The graphs generated on a disk are sliced starting from a 30-degree angle, gradually increasing the slice size based on the number of lessons in the curriculum. Embeddings of query (q) and keys (k) in the hyperbolic matching function are visualized in Figure 6. A model trained with dropout has embeddings bounded in a smaller volume compared to a model without dropout. Off-policy DQN-like agent BID27 is trained with the HRT. TSP graphs are generated following the procedure in BID45. Hyperbolic networks are compared with and without dropout in Figure 4 (Right). The hyperbolic transformer networks perform better with implicit polar coordinates. The hyperbolic recursive transformer extends the transformer by tying parameters of self-attention layers. It uses representations of nodes from the encoder and decoder for predictions. Trees can be represented in hyperbolic and Euclidean geometry, preserving angles in hyperbolic space as the tree grows. The comparison between a hyperbolic recursive transformer with and without pseudo-polar coordinates on the travelling salesman problem shows that angles may not be preserved when the volume grows faster than the rate, resulting in curved lines in the Euclidean diagram."
}