{
    "title": "HJ1HFlZAb",
    "content": "Generative networks, particularly generative adversarial networks, are challenging to evaluate. This paper introduces a new method for assessing the quality of a generator by training a classifier with a mix of real and generated samples. The classifier is then tested on a labeled test dataset to compare its accuracy with a classifier trained on real data mixed with noise. Generative networks like GANs are difficult to evaluate. A new method involves training a classifier on a mix of real and generated data to assess the generator's performance. Experiments compare different generators from VAE and GAN frameworks on MNIST and fashion MNIST datasets. Visual assessment is commonly used to validate the realism of generated samples. The curr_chunk discusses the limitations of current methods for evaluating generative models and proposes a new approach that involves incorporating generated data into the training phase of a classifier before evaluation. This method aims to estimate both sample quality and global fit of the data distribution more accurately. Incorporating generated samples for training can improve efficiency and performance of generative models. Generative models can help regularize the model when data is limited, produce interpolations between images, and be used for data augmentation or producing labeled data to enhance training of discriminative models. Our method evaluates generative models by assessing their ability to fit the real data distribution using generated samples. This data augmentation benefits classifier training and serves as an evaluation tool for generative models. We compare classifiers trained on mixtures of generated and real data to assess generative models in different data settings. The next section discusses generative models, the use of generated samples, and their evaluation. A specific type of generative model, the variational auto-encoder (VAE), controls its latent space and learns to map it to the observation space. The VAE minimizes the KL divergence between the latent space distribution and a prior distribution. This characteristic makes the VAE suitable for generating new data. Generative adversarial networks (GANs) are models where a generator produces images from a distribution to fool a discriminator that distinguishes between real and generated images. GANs can create realistic samples but face training instabilities. Approaches like Wasserstein GAN (WGAN) enforce constraints on the discriminator to address these issues. Conditional neural networks, like Conditional Variational Autoencoders (CVAE) and Conditional Generative Adversarial Networks (CGAN), offer control over the generated outputs. Generative adversarial networks (CGAN) allow for control over the sample's class during generation, improving the quality and discriminative ability of generative models. This approach has been used for data augmentation by learning incremental operations through adversarial training, leading to better performance on various datasets. Our approach, similar to BID14, uses generative networks for data augmentation without learning transformations. We evaluate generative models using metrics like Parzen windows and log likelihood, as visual quality is not a reliable indicator of sample entropy. The method proposed can estimate both the quality and entropy of samples, using a discriminator to evaluate the internal representation of a generator. By applying the discriminator as a feature extractor in unsupervised representation learning, good accuracy was achieved on Cifar10 datasets. This approach provides insight into how the discriminator determines the authenticity of generated images. The method evaluates the quality of generator samples by using a deep convolutional neural network as a discriminator. It is not involved in the training process and ensures no bias in the generator. Parzen windows estimate the probability density function using simpler kernels like isotropic Gaussians. Parzen windows estimator uses a small window on each data point for smoothing, but may not accurately capture the true likelihood. Multi-scale structural similarity (MS-SIM) measures image details at different resolutions, commonly used in image compression. The MS-SIM measures image details at different resolutions to determine similarity between images of a certain class. It can provide insights on the entropy of the data point given its class. However, it may not differentiate between different modes of the distribution. Another approach is the inception score, which evaluates generative models using a conditional distribution learned by a classifier. The authors proposed the inception score as a measure of the variability of generated data, evaluating generative models based on conditional label distribution. Their approach aims to provide an unbiased indicator of the entropy of both P(Y|X) and P(X|Y) in a supervised training setup. The authors proposed the inception score as a measure of the variability of generated data in a supervised training setup. They used datasets like MNIST and Fashion-MNIST BID18 to evaluate different generative models, including conditional generative models using traditional generative neural networks. The authors compared generative models like VAE, WGAN, CVAE, and CGAN in a supervised training setup. They mixed generated samples with real data to create a dataset D gen, experimenting with different ratios of real and generated data using \u03c4 values ranging from 0 to 1. In a supervised training setup, generative models like VAE, WGAN, CVAE, and CGAN were compared by mixing generated samples with real data to create a dataset D gen. Different data augmentation techniques were tested, including isotropic Gaussian noise, random pixel dropout, and no augmentation as baseline. The classifier performance was evaluated on a validation set D valid, with early stopping implemented after 50 epochs. The best performing classifier was chosen and tested on D test for each \u03c4 value. The quality of the generative model was assessed by comparing the test score when \u03c4 = 0. The generative model's quality is evaluated by comparing the test score when \u03c4 = 0 with the best test score when \u03c4 > 0. The experiments vary the amount of data samples in D train to measure the regularization capacity of the generated samples over the classifier's training, indicating the generalization capacity. In the evaluation of the generative model's quality, the test accuracy is compared at different values of \u03c4. A low \u03c4 (< 0.5) indicates the generator can generalize by learning meaningful information about the dataset. If \u03c4 > 0.5 and accuracy is maintained, the generated data can replace the dataset in most parts of the distribution. When \u03c4 = 1, the classifier is trained only on generated samples, and if accuracy is better than the baseline, it means the generator has fit the training distribution. The accuracy of different generative models on fashion-MNIST is compared using data augmentation. VAE and CVAE can maintain baseline accuracy with generated samples, but accuracy decreases with only generated data. CGAN shows similar behavior, with worse degradation at \u03c4 = 1. WGAN improves accuracy for \u03c4 < 0.5 and maintains it at \u03c4 = 1. FIG0 also helps estimate entropy. Training one WGAN per class is the best solution to fit the complete distribution, regardless of the number of training data used. CGAN struggles with training difficulties and instability, leading to poor results. Tuning the parameter \u03c4 can improve accuracy for all generators, except CGAN on MNIST. This parameter serves as a tunable hyperparameter for data augmentation. The parameter \u03c4 serves as a tunable hyperparameter for data augmentation, impacting the generalization capacity of generators. Results in TAB1 summarize this by calculating the data augmentation capacity \u03a8 G for each generator G. The capacity is determined by the mean differences in accuracy between \u03c4-tuned mixtures and baseline accuracy across varying training data sizes. Operating at different scales helps estimate the generative model's ability to generalize. The results of TAB1 show if a generative model can perform data augmentation effectively. It helps in choosing the best model for a specific case, especially with low data amounts. Evaluating the generative model's quality with a discriminative model is a meaningful way to measure its ability to sample probable data points. This method was applied to image samples in the study. The study applied a method to evaluate the quality of generative models using image samples. Results showed that generative models are less prone to overfitting with a small number of training examples compared to discriminative models. Data augmentation induced by generated data was found to be effective with low training data amounts. WGAN was identified as the most efficient solution based on the evaluation performed on various datasets and generative models. Further experiments are needed to confirm WGAN as the most efficient solution for generative models. The impact of different sampling methods on data production and the potential for improving generator performance through sampling phase enhancements should be explored. Additionally, assessing generative model performance on labeled datasets by training a classifier on a mixture of generated and real data can provide insights into generalization abilities. The ability of generative models to create meaningful samples was demonstrated by adding generated data into the training set for better data augmentation. A data augmentation capacity \u03a8 G was computed for each model on image datasets, representing the generalization capacity. This method can be applied to various datasets and generative models with labeled data available. The overfitting capacity of a VAE was shown through additional results, indicating that increasing the number of training images improves variability in the generated samples. The generator generalizes better with fewer training images, showing an improvement in accuracy when using generated data for augmentation. \u03c4 represents the ratio of generated data to total data used for training."
}