{
    "title": "B1GAUs0cKQ",
    "content": "In this paper, variance layers are introduced as a different kind of stochastic layers in neural networks. Each weight of a variance layer follows a zero-mean distribution and is only parameterized by its variance. These layers can learn well, aid in exploration in reinforcement learning, and defend against adversarial attacks. Zero-mean parameterization leads to a better training objective compared to conventional Bayesian neural networks. Parameterization in deep neural networks is crucial for training objectives. Stochastic methods, such as variance layers, are used to reduce overfitting, estimate uncertainty, and improve exploration in reinforcement learning algorithms. Bayesian deep learning offers a principled approach to training stochastic models, reinterpreting existing training procedures as special cases of Bayesian models. In deep neural networks, stochastic methods like drop-connect and stochastic gradient descent are used to train models. One approach is to replace deterministic weights with random weights during training, learning a distribution over the weights instead of a single point estimate. This learned distribution is often discarded during inference, and only the expected values of the weights are used. This heuristic, known as mean propagation or the weight scaling rule, is widely used in practice. In our study, we explore a unique case of stochastic neural networks where weights have zero means and trainable variances. We introduce variance layers that store information only in the variances of weights, connecting them to conventional Bayesian deep learning models. This approach challenges the traditional weight scaling rule and shows competitive performance. Several popular Bayesian models converge to variance networks, showing that a less flexible posterior approximation can lead to better values of the variational inference objective. Variance networks perform well on deep learning problems, achieving competitive classification accuracy, robustness to adversarial attacks, and good exploration in reinforcement learning. Stochastic deep neural networks, which utilize random noise, have become popular. These networks involve parameters drawn from a parametric distribution during training. During training, parameters \u03c6 are adjusted to the data by minimizing the negative log-likelihood and an optional regularization term. Techniques like binary dropout, variational dropout, and drop-connect are used in training stochastic deep neural networks. The predictive distribution for these models is usually intractable but can be approximated using test-time averaging with K independent samples. To improve computational efficiency, weights are often replaced with their expected values. The weight scaling rule, despite being mathematically incorrect, performs well in practice. Symmetric weight distributions cannot store information about training data in their means. Layers with symmetric weight distribution result in random guess quality predictions, referred to as variance layers in neural networks. The curr_chunk discusses the visualization of objects activation samples from a variance layer with two neurons in a network trained on a four-class classification problem. It demonstrates that a variance layer can learn two different representations and explains the computation of a fully-connected layer with input and output neurons. The use of standard normal distributed random variables in stochastic layers is also mentioned. The curr_chunk introduces a Gaussian variance layer that stores information only in the variances of the weights, unlike conventional stochastic layers. It explains how the activations of this layer follow a Gaussian distribution and how this is used in the local reparameterization trick. The Gaussian variance layer in the curr_chunk encodes objects as zero-centered multidimensional Gaussians with different variances, making them easily discriminated by following layers. This is illustrated on a toy classification problem with four classes generated from Gaussian distributions. The fifth layer in the neural network is a bottleneck variance layer with two output neurons. Different classes are represented by different colors in the activations plot. The variance bottleneck layer can learn two types of representations, leading to near perfect performance. In one case, the same information is stored in both neurons, acting as in-network ensembling. In the other case, the information stored in the neurons is different, with each neuron being either activated or deactivated. The fifth layer in the neural network is a bottleneck variance layer with two output neurons, representing different classes. The activations can be either activated or deactivated, encoding information robustly. The variance of activations can encode the same information as the means, leading to high accuracy even with one sample. Test-time averaging improves accuracy further. Real tasks show similar results. The non-linearity in the distribution breaks symmetry, affecting expected activations. Visualizations of embeddings learned by a variance LeNet-5 architecture on the MNIST dataset are in Appendix D. The non-linearity in the distribution breaks symmetry, affecting expected activations. The model learns to distinguish activations of different variance, with variance networks using antisymmetric non-linearities. Different types of variance layers may exploit various noise distributions, storing information only in the variances of the weights. The weight scaling rule results in random guess performance when the mean of the weights is zero. The use of Gaussian variance layers in training a LeNet-5 architecture on the MNIST dataset resulted in high accuracy, outperforming Bernoulli or uniform variance layers due to the lack of local reparameterization trick for the latter. Gaussian dropout layers may converge to variance layers, showing promise in improving model performance. Gaussian dropout layers can converge to variance layers in practice. Doubly stochastic variational inference (DSVI) with the reparameterization trick is a special case of training with noise. The goal is to approximate the posterior distribution over weights by maximizing the variational lower bound, which consists of an expected log likelihood term and a KL divergence term acting as a regularizer. In the context of training with noise to approximate the posterior distribution over weights, the Gaussian dropout approximate posterior with a shared dropout rate \u03b1 is explored. Different prior distributions, such as the symmetric log-uniform distribution and Student's t-distribution, are considered. The KL-term for the Gaussian dropout posterior is a function of \u03b1 and can be estimated using one MC sample or accurately approximated. The Student's t-distribution with diminishing values of \u03bd can replace the improper log-uniform prior in neural networks, resulting in a proper posterior. This approach has been successfully applied to linear models and Bayesian neural networks without changes. In the case of the Gaussian dropout posterior, the optimal prior variance \u03bb 2 ij would be (\u03b1 + 1)\u00b5 2 ij, and the KL-term can be calculated accordingly. The KL-divergence between a zero-centered Gaussian and the prior is constant for log-uniform and ARD priors. For ARD, the KL is zero as the prior equals the approximate posterior. Gaussian dropout layers with these priors can converge to variance layers. The KL term decreases in \u03b1 and may grow to infinite values in practice. The approximate posterior N(\u00b5ij, \u03b1\u00b5^2ij) may grow to infinite values as \u03b1 increases. As \u03b1 approaches infinity, the Maximum Mean Discrepancy between the approximate posterior and its zero-mean approximation goes to zero. This result allows for replacing the learned posterior with its zero-centered approximation without affecting the model's predictions. During training, the Gaussian dropout rates start low and weights can be replaced with their expectations without loss of accuracy. As training progresses, dropout rates increase to infinity, storing all information in weight variances. This leads to a \"phase transition\" in network behavior, as seen in Figure 3. Results are consistent across different prior distributions. In Appendix B, different parameterizations of the Gaussian dropout posterior are explored to understand their impact on the variational lower bound. Contrary to expectations, simpler approximations result in better ELBO values, with the optimal KL term achieved when all \u03b1's are set to infinity. The optimal KL term is achieved when all \u03b1's are set to infinity in the Gaussian dropout posterior parameterizations. More flexible parameterizations make the optimization problem more difficult, potentially leading to poor local optima like sparse solutions. Experimental evaluation of variance networks shows usefulness in classification and reinforcement learning. Variance networks can be pruned up to 98% with minimal accuracy loss, showing resistance to adversarial attacks. Stochastic models were optimized with one noise/weight sample per step using PyTorch. Experiments included image classification tasks on MNIST, CIFAR-10, and CIFAR-100 datasets using the LeNet-5-Caffe architecture. The code is available at https://github.com/da-molchanov/variance-networks. The experiments used LeNet-5-Caffe architecture for MNIST dataset and VGG-like architecture for CIFAR-10/100. Variance networks showed similar test accuracy as binary dropout. Layers in variance networks had specific parameterization equations. The variance network posterior yielded high classification accuracy. In experiments using LeNet-5-Caffe for MNIST and VGG-like for CIFAR-10/100, variance networks achieved similar test accuracy to binary dropout. The posterior robustly yielded high classification accuracy, showing that weights with low variances can be ignored without accuracy degradation. Parameter noise in reinforcement learning can provide efficient exploration for algorithms. In this section, a proof-of-concept result for exploration with variance network parameter noise on two gym environments is provided. The policy gradient approach used a three-layer neural network with parameter noise and variance network policies. Stochastic gradient learning was performed using Adam. In this experiment, the robustness of variance networks to targeted adversarial attacks was studied using CIFAR-10 dataset on a VGG-like architecture. Adversarial attacks were built using the iterative fast sign algorithm with a fixed step length \u03b5 = 0.5, and the successful attack rate was reported. The approach was compared to dropout networks with test time averaging and deep ensembles. The experiment compared the robustness of variance networks to adversarial attacks on CIFAR-10 dataset using a VGG-like architecture. Variance networks learn only the variances of weights while keeping means fixed at zero, showing resistance to attacks. Variance ensembles, combined with deep ensembles, further improve network robustness. The success of variance networks in training deep neural networks raises counter-intuitive implications, showing they can withstand noise and store information using variances. Replacing random variables with expected values can significantly degrade accuracy. The landscape of the loss function is more complex than previously thought. In a similar model, weights or even a whole layer with an exactly zero signal-to-noise ratio (SNR) can be crucial for prediction and cannot be pruned by SNR alone. A more flexible parameterization of the approximate posterior does not necessarily improve the variational lower bound or approximate the posterior distribution better. Variance networks offer new insights into how neural networks learn from data and provide tools for building better deep models. The text discusses reparameterization and transformations in the context of Maximum Mean Discrepancy. It explains how linear transformations do not affect the norm or continuity of a function, and how rotations can be incorporated without changing the supremum. Integration over a standard Gaussian distribution is considered separately, and functions are denoted by their antiderivatives. The text discusses the Lipschitz property of a function F1(\u03b5) derived from f(\u03b5) and its deviation bound using integration by parts. The MMD bound approaches zero as \u03b1t increases, and the deviation of ensemble predictions is bounded by the softmax output interval. Learning curves for VGG-like architectures on CIFAR-10 with different prior distributions show equivalent performance. The text discusses the convergence of three priors to variance networks, with the Student's prior showing slower convergence due to noisy estimates. A LeNet-5 network on MNIST dataset with tanh non-linearities and no biases performs well with a variance layer. Average distributions of activations in the variance layer for different classes are shown, indicating robust discrimination using a linear model. The text discusses the expressions for the forward pass through fully-connected and convolutional variance layers with different parameterizations. It explains the energy levels of neurons for different classes and how samples with varying magnitudes can be distinguished in neural networks. The text discusses the square and square root operations being component-wise, with variables being 3D tensors. Optimization is done with respect to different parameters. The KL divergence is constant with respect to sigma."
}