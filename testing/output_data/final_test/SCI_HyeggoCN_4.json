{
    "title": "HyeggoCN_4",
    "content": "This paper explores transfer learning in natural language understanding tasks, focusing on the transferability of instances and parameters. It proposes an ensemble-based method for few-shot learning to improve performance and avoid negative transfer effects. The paper proposes a method for mitigating negative transfer in neural networks by using small recurrent networks trained on different subsets of the source task. This approach is incorporated into a target task for few-shot learning by adjusting a decaying parameter based on the slope changes of a smoothed spline error curve during training. Our proposed method improves few-shot learning compared to other transfer methods and competes with models trained with full supervision. Learning relationships between sentences is crucial in natural language understanding, especially in tasks like sentence-level semantic textual similarity and textual inference. Pairwise-based sentence classification/regression has been active in research on distributional compositional semantics using neural networks and word representations. Transfer learning is essential for tasks like textual entailment and paraphrasing. Negative transfer can hinder model performance on less similar tasks. Challenges with negative transfer in deep neural networks are not well explored. A proposed method addresses negative transfer by transferring models from source tasks to target tasks. The relevance of each subset per task is weighted based on model validation. The method proposed, called Dropping Networks, addresses negative transfer in deep neural networks by gradually transitioning from using ensemble models trained on irrelevant subsets of a source task to a single model trained on examples from the target task. This transition is guided by a decaying parameter chosen based on the slope changes of a smoothed spline error curve during training. The approach combines Dropout and Bagging techniques in neural networks. The approach involves using Dropout and Bagging in neural networks for regularization, weighting models in ensembles. Experiments focus on Natural Language Inference (NLI) tasks and Question Matching (QM) dataset. NLI infers if a hypothesis is true given a premise, while QM identifies pairs of questions with the same intent. Analysis is restricted to three datasets to study transfer between closely related tasks and less related tasks. Model averaging properties of negative transfer are shown. The study focuses on the model averaging properties of negative transfer in neural networks, showing benefits over Bagging and Dropout methods. It emphasizes the importance of addressing negative transfer effects in transfer learning to improve performance on target tasks without degrading original task performance. The proposed weighting scheme aims to enhance results by considering distant tasks with potential knowledge transfer. Transfer learning in neural networks has mainly focused on parameter-based transfer. Lower-level representations are found to be more transferable than upper-layer representations, reducing negative transfer effects. Previous work has shown the benefits of transfer in structured tasks and the use of hyperplane utility measures for faster convergence. Additionally, covariance matrices for informative Gaussian priors have been constructed for transfer from related tasks in binary text. The purpose of using a covariance matrix for informative Gaussian priors in binary text classification is to improve generalization from weakly informative priors. Transfer learning in neural networks, particularly in Computer Vision, has been studied extensively. Models like AlexNet allow for appending features to existing networks for fine-tuning on new tasks. The transferability of parameters in neural networks for NLP tasks has also been explored. The transferability of parameters in neural networks for NLP tasks was explored, focusing on tasks like SNLI and SICK. Lower level features are more general and useful for transfer, while the output layer is task-specific. It was noted that a large learning rate can change transferred parameters significantly. Transfer learning has been popularized by BID7 through domain agnostic neural language models. Lexical word definitions have also been used for transfer learning, reducing sample complexity for specific tasks. BID24 utilizes a Word Embedding Correlation model for scoring co-occurrence probabilities in Question-Answer pairs. The paper explores correlation scoring functions for word vectors and word co-occurrence modeling. It introduces a character-based intra attention network for NLI, showing improvement over Bi-LSTM networks. The architecture aims to use attention for sentence encoding pairs, utilizing Attention-based CNNs at different hidden layers. However, word ordering can be lost in CNNs when pooling is applied. The section describes a co-attention GRU network used for pairwise learning tasks, focusing on obtaining encoded representations for paired sentences. It addresses the limitations of using only the last hidden state as the context vector and introduces a cross-attention network to account for interdependencies between pairs of sentences. The section introduces a co-attention GRU network for paired learning tasks, addressing limitations of using only the last hidden state. It focuses on interdependencies between sentences by weighting words based on salience. The softmax function generates attention weights, producing a context vector as the sum of attention-weighted outputs. Accelerating learning and avoiding negative transfer are achieved by considering two approaches for voting parameters of a learned model. The text discusses methods to avoid negative transfer in learning tasks by adjusting weights based on similarity between models. It also introduces the use of smoothing splines to stabilize training. Additionally, it mentions the use of dropout and bagging techniques for model regularization, with a proposed method called Dropping that combines both approaches. This method is likened to Adaptive Boosting (AdaBoost) in terms of weight adjustment. The proposed method Dropping adjusts weights based on batch performance after Bagging, using Dropout to promote sparsity. It combines arithmetic and geometric mean model averaging to avoid negative transfer in large datasets. Soft attention is placed on model performance using a weighted vote, enhancing few-shot learning on target models. The proposed method Dropping adjusts weights based on batch performance after Bagging, using Dropout to promote sparsity. It combines arithmetic and geometric mean model averaging to avoid negative transfer in large datasets. Soft attention is placed on model performance using a weighted vote, enhancing few-shot learning on target models. In few-shot learning examples on T t, the weighted ensembled models from T s can be transferred and merged with the T t model, leading to faster convergence and more general features. The update rule decays the importance of T s Dropping networks as the T t neural network learns from few examples, with the prediction from few samples a l t being the single output from T l t. The slope of the error curve, \u03b3, is updated at regular intervals during training, with the rate of shift towards the T t model being proportional to the gradient of the error \u2207 xs for a set of mini-batches xs. The slope update occurs every 100 iterations, emphasizing the incorporation of past knowledge in the initial stages of learning. In the initial stages of learning, incorporating past knowledge is crucial. As the model specializes on the target task, reliance on prior knowledge decreases over time. Different smoothing approaches like kernel and spline models can be used to handle volatile training errors. The regularized least squares function for cubic smoothing splines is used with a penalty term to handle outliers. Splines are solved using least squares with a regularization term, and each subinterval represents the space that is adapted for over time. The standard cross-entropy loss is used as the objective in this approach. The proposed method requires 58% more computational time for training smaller ensembles compared to a larger global model. It addresses issues related to distance measures and parameter constraints, and allows for training on tasks with varying class numbers. Weighting models within the ensemble based on performance helps mitigate negative transfer problems. The proposed Dropping Network transfer method provides a large-scale corpus with 570K annotated sentence pairs, emphasizing informal reasoning and lexical semantic knowledge. The SNLI corpus addresses issues with previous datasets by grounding instances with scenarios for comparing contradiction, entailment, and neutrality between premise and hypothesis sentences. The MultiNLI dataset extends the SNLI corpus by including 10 distinct genres of written and spoken English, allowing for a more detailed analysis of machine learning model performance. It addresses issues such as temporal reasoning, belief, and modality that were not covered in the original SNLI corpus. The dataset also facilitates the evaluation of transfer learning across domains. The Question Matching task in NLU involves analyzing transferability between closely related datasets for semantic relatedness. It is important for Question-Answering systems and machine comprehension. Different hyperparameter settings were tested on models trained for 30,000 epochs using a dropout rate of 0.5 with Adaptive Momentum. The models are trained for 30,000 epochs with a dropout rate of 0.5 using ADAM optimization in a 2-hidden layer network. Transfer Learning involves transferring lower level features and fine-tuning upper layers. Evaluation is based on convergence rate and optimal performance, with a focus on speedup in early learning stages. Table 1 displays results from single-task learning on three datasets, with the ensemble model outperforming other networks. The co-attention network shows similar performance, especially on MNLI, reaching competitive levels with state-of-the-art models. Zero-shot learning results from the ensemble network are also showcased in FIG4. The ensemble network achieves high accuracy in zero-shot learning by averaging probability estimates from different models. Learning is mostly completed by 5,000-10,000 epochs. Transferring from SNLI to MNLI is more challenging than the reverse, with better performance in MNLI \u2192 SNLI. The QM dataset poses difficulties due to its dissimilarity, resulting in reduced test accuracy. Model weights are normalized in the zero-shot setting. The QM dataset has reduced test accuracy due to its dissimilarity to T t. The second approach involves few-shot learning performance with fixed parameters transferred from T t and fine-tuning the 2nd layer. Instances from each genre within MNLI are sampled at least 100 times, with a 3% random sample used for testing in SNLI and QM. Further tuning on the small T t batch leads to improvements, particularly in MNLI with a 2.815 percentage point increase in test accuracy. The proposed method transfers parameters from the Dropping network trained with a spline smoother to balance between the source ensemble and target model. 20 ensembles are transferred with a dropout rate of 0.5, and the performance does not decrease when transferring QM models to SNLI and MultiNLI. The proposed method involves transferring QM models to SNLI and MultiNLI using a weighting scheme with spline smoothing. The method shows improvement in transfer compared to standard methods, with a higher decay rate \u03b3 being more suitable for closely related tasks. This method only relies on one additional parameter \u03b3 and outperforms static hard parameter ensemble transfer. The proposed method combines neural network-based bagging with dynamic cubic spline error curve fitting to transition between source models and a single target model trained on only a few target samples. It overcomes limitations in transfer learning by avoiding negative transfer when transferring from more distant tasks, particularly in a few-shot learning setting. The paper empirically demonstrates learning complex semantic relationships between sentence pairs for pairwise tasks. The co-attention network and ensemble GRU network perform comparably for single-task learning."
}