{
    "title": "SJxIm0VtwH",
    "content": "Adaptive gradient algorithms are commonly used in training deep neural networks, but their success in min-max problems like GANs is not well understood. This paper bridges the gap by analyzing Optimistic Stochastic Gradient (OSG) for non-convex non-concave min-max problems and introducing Optimistic Adagrad (OAdagrad) as an adaptive variant. The paper introduces Optimistic Adagrad (OAdagrad) as an adaptive variant of Optimistic Stochastic Gradient (OSG) for non-convex non-concave min-max optimization. It establishes an improved adaptive complexity and shows that adaptive gradient algorithms outperform non-adaptive ones in GAN training due to the slow growth rate of the cumulative stochastic gradient. Adaptive gradient methods like Adagrad are popular for training deep neural networks due to their efficiency and minimal hyper-parameter tuning. However, there is debate on whether adaptive methods converge faster than non-adaptive methods like SGD in supervised deep learning tasks. Studies have shown that Adagrad may have slower convergence and worse performance compared to SGD in tasks like image classification using deep convolutional neural networks. The curr_chunk discusses the use of GANs in training deep neural networks, specifically focusing on the importance of using the Adam optimizer over non-adaptive methods like SGD. The paper aims to analyze the performance of Optimistic Stochastic methods in GAN training. The curr_chunk discusses the proposal of an adaptive variant named Optimistic Adagrad (OAdagrad) for solving nonconvex non-concave min-max problems, showing improved convergence rates in GAN training when the growth rate of the cumulative stochastic gradient is slow. The curr_chunk discusses solving stochastic optimization problems in GAN training by finding a saddle point in non-convex, non-concave functions, which is NP-hard. The focus is on finding a solution without assuming convexconcave objectives. Finding the first-order stationary point in non-convex, non-concave functions is the focus, aiming to satisfy necessary conditions for finding a local saddle point. Various iterative algorithms have been developed to achieve this goal with non-asymptotic guarantees, including deterministic and stochastic approaches. The extra-gradient method, for instance, requires two stochastic first-order oracles in one iteration, which can be computationally expensive. The inexact proximal point method has iteration complexity O( \u22126 ) for finding a first-order stationary point. Several studies have proposed single-call variants of the extragradient algorithm to avoid the cost of an additional oracle call. Some focus on convex settings, while others focus on non-convex settings. Previous work considered the min-max setting and GAN training, but convergence was only shown for specific problems. The gap in current research on min-max optimization is highlighted, with specific settings studied in previous works. The goal is to design stochastic first-order algorithms with low iteration complexity and cost, suitable for general non-concave min-max problems. Table 1 provides an overview of results, while Appendix B offers a comprehensive survey of related methods not covered in the table. The main focus is on designing algorithms for general non-convex non-concave min-max problems using variational inequalities. The SVI and MVI problems are defined, with a connection to min-max optimization. The main contributions include extending optimistic stochastic gradient analysis beyond the bilinear and unconstrained case. In the context of designing algorithms for non-convex non-concave min-max problems using variational inequalities, the analysis extends optimistic stochastic gradient (OSG) beyond the bilinear and unconstrained case. A variant of OSG achieves state-of-the-art iteration complexity for finding a first-order stationary point. Additionally, an adaptive gradient algorithm named Optimistic Adagrad (OAdagrad) is designed under the same conditions, showing better adaptive complexity. Our main innovation in Optimistic Adagrad (OAdagrad) is considering variable metrics based on data geometry for faster convergence in non-convex non-concave min-max games. This adaptive complexity surpasses non-adaptive methods like OSG, with improved performance demonstrated in GAN training on CIFAR10 data. OAdagrad outperforms Simultaneous Adam in sample quality due to the slow growth rate of cumulative stochastic gradient. In this section, formal definitions of variational inequalities and their relationship to the min-max problem are provided. Notations include a closed convex set X \u2282 R d, the euclidean norm, and the projection operator \u03a0 X. Stochastic gradient and stochastic first-order oracle are used interchangeably to represent noisy observations of T (x) in the min-max setting. Monotonicity and pseudo-monotonicity of operators are defined. In this section, formal definitions of variational inequalities and their relationship to the min-max problem are provided. Monotonicity and pseudo-monotonicity of operators are defined. A point x \u2208 X is called -first-order stationary point if T (x) \u2264 . Pseudo-monotonicity implies that MVI(T, X ) has a solution. In Appendix G, for the min-max problem, when F(u, v) is convex in u and concave in v, T is monotone. Solving SVI(T, X) is equivalent to solving the problem. Even though solving SVI(T, X) is NP-hard, we aim to find a first-order stationary point. Assumption (ii) states that MVI(T, X) has a solution, commonly used in non-monotone variational inequalities analysis. The section discusses the Optimistic Stochastic Gradient (OSG) algorithm, inspired by previous works. It aims to save costs and maintain iteration complexity in nonconvex minimization problems, particularly in learning neural networks using SGD. The algorithm is described in Algorithm 1, with mt representing the minibatch size for estimating the first-order oracle. The Optimistic Stochastic Gradient (OSG) algorithm, described in Algorithm 1, is a cost-saving approach for nonconvex minimization problems, especially in tasks like training GANs where stochastic gradient computation is expensive. OSG only computes stochastic gradient over the sequence of {z k}, making it more numerically appealing compared to the stochastic extragradient method. Before introducing Optimistic Adagrad, a quick overview of Adagrad is presented. Adagrad aims to solve a minimization problem with a model parameter w and a random variable \u03b6. The update rule involves constructing a variable metric based on history gradients information. Optimistic Adagrad (OAdagrad) is an adaptive variant of OSG, updating minimization and maximization variables simultaneously. OAdagrad inherits ideas from Adagrad for faster adaptive convergence. Optimistic Adagrad (OAdagrad) considers the unconstrained case with bounded weights and regularization techniques to ensure weight stability during training. Theorem 2 states that under certain assumptions, Algorithm 2 converges after N iterations. After N iterations of Algorithm 2 in Optimistic Adagrad (OAdagrad), the complexity is O(1/4) when \u03b1 = 1/2, matching Theorem 1. The minibatch size in Algorithm 2 can be any positive constant and independent of \u03b4, making it more practical than Theorem 1. The stochastic gradient is usually sparse, leading to \u03b1 < 1/2 and affecting the computational complexity. After N iterations of Algorithm 2 in Optimistic Adagrad (OAdagrad), the complexity is O(1/4) when \u03b1 = 1/2, matching Theorem 1. OAdagrad updates the discriminator and generator simultaneously, while Alternating Adam alternates between multiple steps of Adam on the discriminator and a single step on the generator. OAdagrad fits into the framework of Optimistic Adam with annealing learning rate. No convergence proof exists for Alternating Adam for non-convex non-concave problems, whereas our convergence proof for OAdagrad is provided. In the experiment, the effectiveness of Alternating Adam, OSG, and OAdagrad algorithms in GAN training using WGAN-GP on CIFAR10 data was verified. Different batch sizes were tested, and the learning rate was tuned accordingly. The Inception Score (IS) was reported for each algorithm. In the experiment, OAdagrad outperformed OSG and Alternating Adam in GAN training on CIFAR10 data. Inception Score (IS) was used to compare the performance of the algorithms. The growth rate of the cumulative stochastic gradient was studied using OAdagrad with different learning rates and a batch size of 64. In GAN training, OAdagrad showed superior performance compared to OSG and Alternating Adam on CIFAR10 data. The growth rate of cumulative stochastic gradient was analyzed, revealing slow growth in GANs. The polynomial degree varied between different GAN models, with faster convergence observed for OAdagrad. Additionally, GAN training on a large-scale dataset using Self-Attention GAN and ImageNet showed boundedness of both generator and discriminator through spectral normalization. The effectiveness of adaptive gradient methods in training GANs is highlighted, with OAdagrad outperforming Simultaneous Adam in quantitative metrics and sample quality generation. Future work will explore the potential benefits of training OAdagrad with larger batch sizes for improved results. Theoretical and empirical analysis of adaptive gradient methods in GAN training shows efficient algorithms for solving non-convex problems with faster convergence. Experimental results support the practical performance of adaptive gradient methods due to slow growth of cumulative stochastic gradient. During the training of WGAN-GP, CIFAR10 images were generated using three optimization methods (OSG, OAdagrad, Alternating Adam) with batch size 64. OAdagrad and Alternating Adam produced better images than OSG, showcasing the benefits of adaptive gradient methods in GAN training. Additionally, Self-Attention GAN (SA-GAN) generated ImageNet images using OAdagrad and simultaneous Adam, with OAdagrad producing higher quality images. In comparison to simultaneous Adam, OAdagrad produces superior quality images. The learning rates used are 0.001 for the generator and 0.00004 for the discriminator. Alternating Adam with the same learning rates as SA-GAN collapsed due to the different batch sizes. OAdagrad outperforms Simultaneous Adam in terms of Inception score and Fr\u00e9chet Inception Distance. Alternating Adam was not reported as it collapsed during training. Training For convex-concave min-max optimization, the extragradient method was first proposed by Korpelevich in 1976. Nemirovski extended the idea to mirror-prox under gradient Lipschitz condition, achieving O(1/N) convergence rate. Stochastic mirror-prox was analyzed by Juditsky et al. in 2011. Zhao developed a nearly-optimal stochastic first-order algorithm for strongly convex primal variables. Bach & Levy proposed a universal algorithm adaptive to smoothness and noise with optimal convergence rate. Many works analyze one-sided nonconvex min-max problems. In analyzing one-sided nonconvex min-max problems, various algorithms have been proposed by different researchers. Rafique et al. (2018) introduced a stage-wise stochastic algorithm for weakly-convex functions, while Lu et al. (2019) utilized a block-based optimization strategy. Lin et al. (2019) showed the effectiveness of (stochastic) gradient descent ascent for smooth functions, and Liu et al. (2020) applied deep neural networks to stochastic AUC maximization. AUC maximization with deep neural networks is transformed into a nonconvex-concave min-max problem. Various algorithms have been proposed to address this challenging problem, including the deterministic extragradient method and a stage-wise algorithm for weakly-convex and weakly-concave functions. Additionally, an alternating deterministic optimization algorithm has been designed for this problem. The text discusses a new stochastic optimization algorithm that differs from previous methods by not requiring bounded domain assumption, having lower iteration complexity, not assuming the PL condition, and being applicable to both stochastic and deterministic cases. The text discusses a new stochastic optimization algorithm that requires only one stochastic gradient calculation per iteration, achieving the same iteration complexity as previous methods. Various works have analyzed the convergence behavior of min-max optimization algorithms, with some showing asymptotic convergence. Various works (Nagarajan & Kolter, 2017; Grnarova et al., 2017; Yadav et al., 2017; Gidel et al., 2018; Mertikopoulos et al., 2018) focus on specific settings such as concavity and convex-concave objectives. Daskalakis et al. (2017) analyze convergence for bilinear problems, while Chavdarova et al. (2019) develop a variance-reduced extragradient method. Azizian et al. (2019) provide a unified analysis of extragradient for bilinear games. The paper analyzes extragradient for bilinear games in the strongly monotone case and intermediate cases, but lacks non-asymptotic convergence results for non-convex non-concave min-max problems. The proof involves closed and convex sets, optimal solutions, and bounds on the error term. The paper discusses extragradient for bilinear games in the strongly monotone case and intermediate cases, lacking non-asymptotic convergence results for non-convex non-concave min-max problems. It involves closed and convex sets, optimal solutions, and bounds on the error term. In experimental results on CIFAR10, the performance of OSG, Alternating Adam (AlterAdam), and OAdagrad is compared under the same minibatch size setting. In experimental results on CIFAR10, OAdagrad and Alternating Adam outperform OSG consistently. OAdagrad converges faster than Alternating Adam with large minibatch sizes, showing its benefits in such scenarios."
}