{
    "title": "rygfC0VKPS",
    "content": "Combining domain knowledge models with neural models has been challenging, but end-to-end trained neural models outperform other combinations. By composing domain models with machine learning models and using extrapolative testing sets, we create interpretable models that can predict complex systems. These models are data-efficient and capture non-stochastic behavior. The improved modeling paradigm is applied to simulated and physical systems for system identification. Various ways of combining domain models with neural models are explored for different systems. The ability to combine models is a promising direction for neural modeling, used to explain, predict, and control the real world. Traditional models include science/math equations, algorithms, simulations, and interpolative models like cubic splines. Stochastic noise models capture the unpredictable part of the signal, while domain/physical models predict output vectors given input vectors with adjustable parameters. Random noise accounts for the unmodeled non-deterministic part of the data. Traditional models capture the predictable part of data, while stochastic noise models account for the unpredictable part. However, traditional models have limitations in modeling complex systems with unmodeled degrees of freedom or systematic errors. Parameters of physical models can also be in error or time-dependent, making the usual deterministic-stochastic model inadequate. Neural models, such as neural networks, offer unique advantages in handling high-dimensional input-output relations with complex patterns. They do not require handcrafting basis functions and can describe unmodeled degrees of freedom, systematic errors, and nonstationary behavior. Combining neural modeling with traditional modeling can leverage the strengths of both approaches and address the limitations of traditional models in capturing complex systems. The advantages of traditional and neural models can be combined to address limitations by creating hybrid models through boosting, ensemble, and cyclical autoencoder. This involves assumptions about relationships between systems and noise, with different model classes and loss functions used for various stages. Extrapolative testing is also introduced to test modeling power, along with a focus on producing truly stochastic residuals in hybrid models. In this work, novel loss functions are used to enhance the quality of hybrid models for system identification. The models decompose into deterministic, predictable, and stochastic components, handling complex systems. Previous studies have explored combinations of traditional and neural models, such as extended Kalman filtering, multilayer perceptron, Kohonen neural networks, hybrid ARIMA neural network, and liquid state plus physics model hybrid. However, these approaches do not incorporate the latest machine learning techniques like boosting. The latest machine learning approaches, specifically boosting, are discussed as a modeling paradigm. Previous work showed that end-to-end neural models without domain knowledge often have lower MSE. This study addresses issues like composing hybrid models, incorporating prior knowledge, interpretability, and extrapolation vs interpolation. Various model composition approaches are explored. Various approaches for composing models are discussed in this work. Jacovi et al. (2019) used a blackbox physics model to generate supervised data for a neural model, while Innes et al. (2019) propagated errors through parametric physics models. Sahoo et al. (2018) introduced specific functional neurons for model computation, and Amos & Kolter (2017) constructed a nonlinear least squares layer. Zhou et al. (2017) incorporated a physics model by adapting inputs using a neural network. Incorporating a physics model by adapting inputs with a neural network, the study by Zhou et al. (2017) did not address training protocols, interpreting the neural component, or investigating statistical properties of residuals. To form hybrid models, potential methods include sequential, parallel, and cyclic training, as discussed in the study. The model composition procedure involves training parameters of domain/physics models using a selected loss function. This is followed by using residuals and previous inputs to train another model, iterated multiple times. The boosting method combines physics/deterministic models with neural network models to refine predicted outputs. The final residuals are attributed to stochastic models. The sequential method involves fitting models to residuals using stage-specific loss functions, with domain models applied first followed by neural models. In parallel modeling, various models are trained simultaneously to minimize system-model loss. A hybrid method selects a model with a loss function for each training stage and learns a second decoder NN model for cyclical training. The method involves cyclical training for each stage, similar to the sequential method. It improves training and provides an inverse model related to deep equilibrium layers. Different ways to compose models exist, but these methods directly connect with the additive decomposition. The loss function for each stage must be selected, with the MSE function sufficient for domain model training. However, MSE does not guarantee compatibility with the stochastic model's assumptions. The stochastic model assumes IID random variables with residuals compared using an autocorrelation function. A Ljung-Box loss function ensures uncorrelated residuals and compatibility with the model. Decorrelation loss functions control residual correlations in a hybrid model trained using domain and NN models. Different loss functions are used for each stage of learning to drive outputs towards desired goals. Training and testing data selection is crucial, with previous work sampling the testing set from the same input. The testing set is usually sampled from the same input region as the training set, leading to lower MSE in large parameter neural models. However, this does not measure the extrapolation ability of domain models. A new method involves creating an interpolating test set by sampling from the convex hull at different points than the training set, and an extrapolating test set by sampling outside the training convex hull. This helps identify models that generalize to larger input domains and discriminate against those that memorize data. High parameter models perform well for interpolative but not for extrapolative data. Performing end-to-end testing of all models simultaneously is a significant alternative. Performing end-to-end testing of all models simultaneously is a significant alternative to training in stages. This approach combines models in parallel or sequentially, selects a training set, and uses a loss function to train each stage. The process is repeated with complementary models and different loss functions to ensure consistent decomposition. This modeling procedure is applied to complex phenomena in order to maintain interpretability and efficiency in GPU usage. The modeling process for complex phenomena involves system identification for mechanical problems and time series prediction. Various problems are simulated, including the inverse pendulum and double pendulum, with Gaussian noise added. Data sets consist of state, actuation, and residuals, with outputs being the next time step residual values. The system's state includes angle, angular velocity, and acceleration, while actuation time series include training sets. The modeling process involves system identification for mechanical problems and time series prediction. The actuation time series include training and testing sets with restricted ranges. Various models are trained using the training set, including a physics model, linear state space model, dense neural network, recursive neural network, and cyclical model. The inverted pendulum swingup problem is a classic control issue addressed in the modeling process. The model introduces errors to ensure capture by other models, with inputs including cos \u0398(t), sin \u0398(t),\u0398(t), U (t) and outputs cos \u0398(t + 1), sin \u0398(t + 1),\u0398(t + 1). Models are trained on a dataset of 25000 points with control actions between -1.0 to 1.0, and performance is evaluated on both interpolative and extrapolative datasets. The performance of various models on a dataset is compared using root mean square residuals (RMSE) in db = 20log 10 |RSM E|. Dense NN and RNN outperform the nonlinear physics model. Boosting with a linear state space model reduces median but variation remains large. Boosting with dense NN and RNN significantly reduces median and variance of residuals. Cyclical autocorrelation helps reduce interpolative testing errors. LJB loss boosting results in worse MSE due to minimizing residual correlations. Models show greater variation in extrapolative testing set. The models exhibit greater variation in the extrapolative testing set. Physics models outperform linear and neural network models, validating their ability to extrapolate better. Boosting physics models with neural networks improves performance. MSE for extrapolative data sets is larger than interpolative data sets. The results confirm that hybrid domain models augmented with NN models provide better fits for extrapolative data sets. The double pendulum simulation demonstrates the ability of hybrid modeling to capture unmodeled degrees of freedom. The RMSE for the physics model is significantly reduced by boosting it with neural models, particularly the RNN. The hybrid model, combining physics and neural networks, improves the autocorrelation function of residuals. The RNN successfully models the oscillation of the second pendulum, reducing amplitude oscillations. Using the LJB loss function results in residuals that closely resemble noise added to the simulation. The hybrid modeling approach successfully captures structured disturbances in simulations and physical systems. A DC motor coupled to a rotary encoder with a 3D-printed shaft demonstrated the ability to control speed and direction changes using Pulse Width Modulation (PWM). Data collection involved varying speed and polarity of the motor to collect position and velocity data from the encoder. The controller was programmed to vary the speed and polarity of the motor, collecting data inputs and model outputs. Different models were used, including linear state space, RNN, and dense NN. The best performance was seen with a linear/physics model boosted by an RNN for interpolative data. For extrapolative data, the RMSE was larger, but boosting by a NN, especially the RNN, helped capture the delay of the backlash with a smaller MSE. The RNN captures the delay of the backlash with a smaller MSE. The domain model picks up behavior, the RNN captures the backlash, and the LJB loss function enforces residual matching. Boosting domain models with neural models using whitening loss functions results in consistent, interpretable models for both simulated and real systems. This approach combines domain knowledge, interpretability, and data efficiency with neural models to model complex phenomena, particularly for control. Hybrid models using boosting and novel whitening objective functions capture complex system behavior by decomposing unmodeled non-stochastic components. These models address structure noise and can be used for stability bounds in control systems. Trained on Keras with TensorFlow backend, models are saved based on performance on a validation dataset. Hyperparameter discussion is included. The validation dataset is created from the Interpolative convex set. The initial learning rate is 0.01 and is halved if the validation loss plateaus for 10 consecutive epochs. Early Stopping occurs if there is no reduction in validation loss for 30 consecutive epochs. Adam optimizer is used, models are trained with mini batch size of 512, and Batch Normalization is applied at the input stage. Dense models have 10 Hidden Layers with 256 weights each, while RNN models have 10 Hidden layers with 50 recurrent weight blocks each. Results for the interpolative double pendulum are shown in Fig. 9."
}