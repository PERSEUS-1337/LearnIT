{
    "title": "BkgZxpVFvH",
    "content": "Origin-Destination (OD) flow data is crucial for transportation studies and ride-sharing platforms. A new LSTOD model with a CNN filter and attention structure improves prediction accuracy by 6.5% over existing methods. The LSTOD model with a CNN filter and attention structure improves prediction accuracy by 6.5% over the second best model. Spatial-temporal prediction of large-scale network-based OD flow data is important for traffic flow control, urban routes planning, infrastructure construction, and policy design on ride-sharing platforms. Predicting dynamic demand flow data helps platforms optimize resource utilization, improve user experience, and design better order dispatch and fleet management policies. Efforts have been made to develop traffic flow prediction models, with traditional statistical and machine learning methods dominating before the rise of deep learning. Deep learning technologies have significantly improved OD flow prediction by extracting non-linear correlations among OD flows. Deep learning technologies have greatly enhanced OD flow prediction by capturing non-linear latent structures. Various approaches, such as modeling the city as an image and using graph-based neural networks, have been explored to improve spatial-temporal relationships in traffic flow prediction. The aim of this paper is to introduce a hierarchical Latent Spatial-Temporal Origin-Destination (LSTOD) prediction model using CNN-based architectures to extract complex spatial-temporal features of OD data. A Vertex Adjacent Convolution Network (VACN) is introduced to capture related OD flows that share common vertices, improving traffic flow prediction. The paper introduces a temporal gated CNN (TGCNN) integrated with VACN to capture the evolutionary mechanism of dynamic OD flow systems. A periodically shifted attention mechanism is used to capture long-term periodicity, and short-term and long-term representations are combined for prediction. This is the first proposal of purely convolutional structures to learn both short-term and long-term dependencies in ride-sharing platforms. The paper introduces a novel VACN architecture to capture graph-based connections among OD flows and a periodically shift attention mechanism for long-term periodicity in TGCNN. Experimental results show LSTOD outperforms state-of-the-art methods in OD flow prediction, with significant improvements in RMSE. The paper introduces a VACN architecture to capture graph-based connections among OD flows and a periodically shift attention mechanism for long-term periodicity in TGCNN. The goal is to predict future OD flow maps given historical data, with varying short-term and long-term input data. Increasing the length of input data improves prediction accuracy but also increases training time. The paper introduces a VACN architecture and a periodically shift attention mechanism for long-term periodicity in TGCNN to predict future OD flow maps. Increasing input data length improves accuracy but also increases training time. The LSTOD model reformulates short-term OD networks into a 4D tensor and concatenates long-term snapshots into a 5D tensor to capture network structures and temporal dependencies. The model includes an end-to-end framework constructed by CNN modules and a multi-layer architecture VACN for extracting network patterns. The paper introduces a VACN architecture and a periodically shift attention mechanism for long-term periodicity in TGCNN to predict future OD flow maps. The VACN architecture includes a novel multi-layer design to extract network patterns and a special ST-Conv block for spatio-temporal representations. The architecture efficiently utilizes long-term information by measuring similarities with short-term data. Standard CNNs may disregard connections between neighboring OD flows in the graph space, as shown in Figure 2. The VACN architecture introduces a novel multi-layer design and a special ST-Conv block for spatio-temporal representations. It efficiently utilizes long-term information by measuring similarities with short-term data. The OD flows covered by a single CNN filter may be topologically far away from the target one in the graph. To better understand the differences between VACN and vertex-based convolutions like GCN or GAT, the concept of line graphs is introduced. Each node in the line graph corresponds to an edge in the original graph. The concept of line graphs in L(G) corresponds each node to an edge in G, with each edge in L(G) mapped to a pair of edges in G. The learned representation of target edges is a weighted sum from the adjacency matrix and transposed adjacency matrix. VACN propagation for OD flow from v i to v j involves weights and elementwise activation functions. Temporal gated CNN is used instead of RNNs in LSTOD for temporal representation. The TGCNN architecture addresses the limitations of RNNs by using 3D convolution kernels and a gated linear unit for efficient training. It processes spatial representations of OD flows over time, allowing for parallel computations and faster training speeds. The TGCNN architecture uses 3D convolution kernels and a gated linear unit for efficient training, processing spatial representations of OD flows over time. The spatial-temporal convolutional block (ST-conv block) captures spatial-temporal features with a 'sandwich'-structure architecture and VACN operator, reducing computation complexity and memory consumption. The TGCNN architecture utilizes 3D convolution kernels and a gated linear unit for processing spatial representations of OD flows over time. The ST-conv block captures spatial-temporal features with a 'sandwich'-structure architecture and VACN operator, reducing computation complexity and memory consumption. The output is computed by applying TGCNN kernels and a multi-layer VACN operator to each timestamp, reducing the temporal length of the input data. The TGCNN architecture uses 3D convolution kernels and a gated linear unit to process spatial representations of OD flows over time. To address long-term temporal periodicity in the data, a modified periodically shifted attention mechanism is proposed to work with CNN-based ST-Conv blocks. This approach captures cycling patterns in the data while reducing computational complexity by selecting a small set of timestamps from each previous day. The proposed modified periodically shifted attention mechanism works with CNN-based ST-Conv blocks to capture long-term temporal patterns in OD flow data. It reduces temporal length and applies TGCNN layers to build spatial-temporal representations for each day, leading to the creation of long-term spatial-temporal features. The model concatenates short-term and long-term spatial-temporal representations, applies a fully connected layer with a sigmoid function for predictions, normalizes data for training, and uses L2 loss for optimization. The LSTOD model uses L2 loss for training and is optimized with BPTT and Adam. The architecture is implemented using Tensorflow and Keras. Experiments are conducted using demand flow data from a ride-sharing platform in two big cities. The study utilized demand flow data from a ride-sharing platform in two big cities, A and B, received from 04/01/2018 to 06/30/2018. N=50 hexagonal regions with the largest customer demands were selected to build 2500 OD flows. The dataset was split into training and testing sets, with the training set covering data from 04/01/2018 to 06/16/2018 and the testing set covering data from 06/17/2017 to 06/30/2017. The study analyzed OD flow data from a ride-sharing platform in cities A and B, focusing on 2500 OD flows with a 4:1 ratio. Predictions were made for 50 OD flows in 30-minute intervals using various methods like HA, ARIMA, SVMR, and LSM-RN. Evaluation was done using Rooted Mean Square Error. In this study, latent models like ARIMA and LSM-RN were compared for predicting OD flow data without external covariates. Hyperparameters were tuned for optimal performance, with specific values obtained for each model. The SRCN model had an optimal kernel size of 11x11 for spatial-based CNN kernel. Short-term OD flow sequence length was set to 9, long-term data covered the three most recent days, and day-level time series length was set to 5 to capture periodicity shifting. The study compared different models for predicting OD flow data without external covariates. The LSTOD model outperformed all other methods in terms of prediction RMSE on testing data for city A. The model used a two-layer architecture to extract spatial patterns and set the filter size to 64 for all deep learning layers. The LSTOD model outperforms all other methods in predicting OD flow data on the testing set, showing significant improvement over the second best method 'SRCN'. Deep learning models exhibit more stable performance compared to traditional methods, with LSTOD efficiently controlling estimation variance. This highlights the benefits of using spatial-temporal architecture and long-term periodicity in modeling OD flow networks. The LSTOD model performs better on city B compared to baseline methods, as long-term periodic patterns are more significant in city B. Results for City B are in Table 3. The complete LSTOD model outperforms variants without attention mechanisms, capturing day-wise periodicity shifts for improved accuracy. Figure 3 compares model predictions with true values. The complete LSTOD model outperforms variants without attention mechanisms, capturing day-wise periodicity shifts for improved accuracy. The distribution curves of day-wise RMSEs show the predictive and stable nature of the complete LSTOD model, especially for unusual cases. Additional experiments on hyperparameter configurations are detailed in Section E of the appendix. In an experiment comparing VACN to standard CNN, VACN outperformed in capturing hidden network structure of OD flow data. The study used N = 50 sub-regions of city A to build dynamic OD flow matrices. VACN covered 200 pixels per snapshot, while standard CNN had a maximum receptive field of 15x15. Different kernel sizes were tested, with standard CNN achieving the best performance with RMSE = 2.64 on testing data. The standard CNN achieved the best performance with RMSE = 2.64 on testing data using a filter size of 11 \u00d7 11, which was higher than VACN with RMSE = 2.54. RMSE increases with larger receptive fields, as spatial correlations among related OD flows are smoothed. The experiment highlights the limitations of standard CNN filters in capturing spatial correlations without considering topological connections from a graph perspective. Batch normalization was used in the VACN component, with a batch size of 10 and an initial learning rate of 10 \u22124. In this section, the LSTOD model's performance is analyzed based on different hyperparameters of input OD flow data, specifically p1 and p2. The best RMSE of 2.41 is achieved with (p1, p2) = (7, 5), showing that shorter time ranges capture shifts more effectively. Different p1 values under p2 = 5 consistently outperform those under p2 = 7. The shift can be captured within a short time range, while a longer sequence may smooth the significance. Table 4 shows detailed prediction results for each data setting."
}