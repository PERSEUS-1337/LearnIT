{
    "title": "ryazCMbR-",
    "content": "Coding theory is a key discipline in modem technology, driven by human ingenuity with occasional breakthroughs. This paper explores automating the discovery of decoding algorithms using deep learning, focusing on RNN architectures for sequential codes. The study demonstrates that creatively designed RNN architectures can decode well-known codes like convolutional and turbo codes with near-optimal performance on the AWGN channel. Reliable digital communication relies on robust codes for efficient decoding under noisy conditions. Landmark codes like convolutional, turbo, LDPC, and polar codes have significantly impacted modern communication technologies. These codes are essential for various applications, including cellular phones and global cellular networks. The canonical setting for coding theory is point-to-point reliable communication over the AWGN channel. The goal is to design new, computationally efficient codes that improve the state of the art in correct reception over the AWGN setting. The focus in coding theory is on designing robust codes for reliable communication over various channel models, beyond the AWGN setting. Progress in developing new codes for multi-terminal settings like the feedback channel and interference channel has been driven by human ingenuity. The transition from convolutional codes to polar codes took over 4 decades, but deep learning is now showing promise in learning complex algorithms from data alone. Deep learning methods can play a crucial role in solving coding theory challenges, such as vast code spaces and efficient encoding. Generalization across block lengths and data rates is essential for high reliability in communication. The performance of neural decoders on a wide range of channel SNRs is crucial for designing codes. Recent deep learning works focus on decoding known codes using data-driven neural decoders, with challenges in generalizing to longer block lengths and data rates. The main challenge is to limit oneself to a class of codes that neural networks can naturally handle. In this paper, the focus is on sequential encoding and decoding schemes like convolutional and turbo codes, which can be efficiently decoded using recurrent neural network (RNN) architectures. The advantage of these schemes is the ability to encode arbitrarily long information bits at various coding rates. The goal is to decode convolutional codes on the AWGN channel using RNN architectures, with the Viterbi decoder being a key component for optimal bit error decoding. Neural network decoding, specifically using RNN architectures, has shown near-optimal performance in bit error rate (BER) and block error rate (BLER) when decoding convolutional codes on the AWGN channel. Training the RNN decoder at a specific signal-to-noise ratio (SNR) and short information bit lengths (100 bits) allows for strong generalization capabilities across a wide range of SNR and block lengths (up to 10,000 bits). The specific training SNR is closely related to the Shannon limit of the AWGN channel for code rate, providing strong information theoretic support to empirical results. RNN convolutional decoders can decode turbo codes at comparable BER to state-of-the-art turbo decoders on the AWGN channel, showing robustness to variations in the channel model. Neural networks can adapt to bursty AWGN channel variations in OFDM cellular systems and radar interference, outperforming heuristics. Sequential coding schemes are attractive for mobile standards and achieve near information theoretic limits. The neural decoder for convolutional codes achieves optimal classification accuracy by using a recurrent network with binary input sequences and vector states. The encoder operates on a rate-1/2 Recursive Systematic Convolutional (RSC) code, producing binary vector outputs known as transmitted bits or codewords. The output is a two-dimensional binary vector where the state of the next cell is updated from an initial state of 0. The 2K output bits are sent over a noisy channel, with decoding referring to finding the maximum a posteriori estimate. Efficient iterative schemes are available for finding the MAP estimate for convolutional codes, with two MAP decoders based on error criteria: bit error rate (BER) or block error rate (BLER). BLER counts the fraction of blocks wrongly decoded. The Viterbi algorithm and BCJR algorithm are efficient methods for finding the optimal MAP estimate for convolutional codes. These algorithms rely on the recurrent structure of the encoder, represented as a hidden Markov chain (HMM). By using dynamic programming, the exact posterior distributions can be computed in linear time, crucially depending on the encoder's structure. Neural networks can efficiently compute posterior distributions for decoding convolutional codes without specifying the underlying probabilistic model. The goal is to train a model to accurately decode received bits using dynamic programming and the right architecture. The goal is to train a model for accurate sequence-to-sequence classification using a novel neural decoder for rate-1/2 RSC codes called N-RSC. The model's parameters are denoted as W, and the output sequence f W (y) estimates posterior probabilities. The weights W are trained to minimize the error with respect to a specified loss function. The encoder for accurate sequence-to-sequence classification uses bidirectional GRU, a 2-layer architecture, and batch normalization. Unidirectional GRU and single layer bi-GRU do not meet the desired performance. The accuracy depends on the number of layers, as shown in Table 1. Batch normalization is crucial for achieving target accuracy. Two novel training techniques are proposed to improve model accuracy. Two novel training techniques are proposed to improve model accuracy significantly. The first technique introduces a novel loss function guided by efficient dynamic programming, reducing the number of training examples needed. The second technique utilizes a python implementation of BCJR to compute posterior distributions accurately. The text discusses using a python implementation of BCJR to compute posterior distributions accurately and proposes a guideline for choosing training examples to improve accuracy. It suggests using training data with noise level matched to test SNR for rate-1/2 RSC code, and provides a general formula for rate-r codes in Appendix D. The text discusses using a python implementation of BCJR to compute posterior distributions accurately and proposes a guideline for choosing training examples to improve accuracy. It suggests using training data with noise level matched to test SNR for rate-1/2 RSC code, and provides a general formula for rate-r codes in Appendix D. The N-RSC is trained on randomly generated data for rate-1/2 RSC code of block length 100 over AWGN channel with proposed training SNR of min{SNR test , 1}. The decoder is trained with Adam optimizer, achieving optimal performance of the optimal dynamic programming (MAP decoder) almost everywhere. Surprisingly, the neural decoder trained on length 100 codes can be applied directly to codes of length 10,000 while still meeting optimal performance. The proposed neural decoder can generalize to unseen codewords and longer block lengths, with excellent performance in terms of Bit Error Rate (BER). Training with different methods yields similar BER performance. Complexity is also an important metric for evaluating decoder performance. The paper introduces an alternative decoding methodology that can achieve excellent BER performance. The methodology discussed in the current chunk highlights the potential for excellent Bit Error Rate (BER) performance. It mentions the use of innovative ideas from computer vision to reduce circuit complexity, such as distilling knowledge in large networks and binarization of weights and data. The complexity comparison of different decoders is briefly touched upon, noting that it is a complex topic beyond the scope of a single paper. The proposed neural decoder for turbo codes shows promising performance on the AWGN channel, outperforming standard turbo decoders in some cases. Unlike traditional message-passing decoders, the neural decoder utilizes convolutional neural networks to achieve this improvement. The complexity of decoders like Viterbi, BCJR, and neural decoders is discussed, with the neural decoder requiring a quadratic number of multiplications in the dimension of hidden states. The neural turbo decoder outperforms message-passing decoders on the AWGN channel. It is both adaptive and robust, tested on various non-Gaussian channel models. Training and architectural choices are similar to convolutional codes, explored in detail in Appendix B. The neural turbo decoder is robust when tested on non-Gaussian channel models. Gaussian noise is known to be the worst case noise in terms of capacity. Viterbi decoding for convolutional codes is naturally robust, while turbo decoder is vulnerable to adversarial attacks. The neural decoder shows more robustness compared to turbo decoder. The neural decoder is more robust to t-distributed noise than the turbo decoder. It is important for the learning algorithm to be robust against differing noise distributions. The neural decoder significantly outperforms the turbo decoder in experiments with t-distribution noise. The neural channel decoder outperforms the standard Turbo decoder due to more accurate LLR calculations and adaptivity to varying noise distributions. The neural network can learn decoding algorithms even in scenarios with complex noise patterns. The channel model describes how bursty noise can interfere with LTE in next generation wireless systems, attracting attention in the communications systems community. The Turbo coding decoder fails under this model because the location of bursty noise is a latent variable that needs to be decoded along with the message bits. The bursty noise is a latent variable that needs to be jointly decoded along with the message bits. To combat this noise model, the neural decoder is fine-tuned on bursty noise, termed the bursty neural decoder. Two heuristics, erasure-thresholding and saturation-thresholding, are used. The performance of the AWGN neural decoder and standard turbo decoder on this problem is demonstrated. The action of the neural decoder trained under bursty noise is interpreted using a simplified model. The bursty noise is decoded jointly with message bits using the bursty neural decoder, fine-tuned on bursty noise. The neural decoder's performance is compared to the standard turbo decoder, showing differences in confidence levels during decoding. The BCJR algorithm exhibits high confidence levels leading to error propagation, while the RNN decoder is more modest in its confidence assessment. In this paper, it is demonstrated that RNN architectures can learn Viterbi and BCJR decoding algorithms through end-to-end training samples. The RNN architectures show potential in decoding existing codes and potentially learning new codes on the AWGN channel. This could improve the current state of the art in coding techniques. The paper demonstrates that RNN architectures can learn Viterbi and BCJR decoding algorithms through end-to-end training samples, showing potential in decoding existing codes and potentially learning new codes on the AWGN channel to improve coding techniques. The state-of-the-art includes turbo, LDPC, and polar codes, with a new look at multi-terminal communication problems like relay and interference channels. Neural decoders can be trained to decode various convolutional codes, including non-recursive and non-systematic codes, as well as rate-1/2 RSC codes with different state dimensions. The neural network architecture used for convolutional codes is shown in Figure 8. Different architectures were used for codes in Figure 8(a) and 8(b), with varying hidden units and network types. Training examples and parameters differed for each code, with performance metrics displayed in Figure 8. The neural decoder trained on one SNR and block length showed good generalization capabilities. The neural decoder trained on one SNR (0dB) and short block length (100) can generalize to decoding as well as a MAP decoder under different SNRs and block lengths. The BER and BLER performances of the neural decoder for convolutional codes demonstrate its generalization capability. A neural decoder for turbo codes is provided using multiple layers of the neural decoder introduced for RSC codes. The turbo code example includes two rate-1/2 RSC encoders with an interleaver performing random permutation. The encoder output sequence is redundant, so it is discarded, and the remaining sequences are transmitted over an AWGN channel. The iterative turbo decoder, using the RSC MAP decoder as a building block, refines the belief on the codewords until convergence. The N-Turbo neural decoder for turbo codes, called N-Turbo, uses a deep layered architecture with N-BCJR layers. Training the deep layers of recurrent architecture is challenging, so we propose training each layer separately and then initializing the deep layered neural decoder of N-Turbo from these trained models. The N-BCJR architecture is a new type of N-RSC that can take a flexible bit-wise prior distribution as input, unlike previous N-RSC models customized for a uniform prior distribution. The N-RSC proposed is customized for a uniform prior distribution, with an architecture similar to N-RSC but with input size of 3 and LSTM instead of GRU. Training examples are generated from turbo codes, with a blocklength of 100 at fixed SNR -1dB using mean squared error as a cost function. Non-zero priors are generated using a triplet of input prior probability, input sequence, and output of the BCJR layer. The model includes 6 layers of BCJR decoder with interleavers, with the last layer being the final output. The proposed N-Turbo decoder utilizes 6 layers of BCJR decoder with interleavers, training with binary crossentropy loss. End-to-end training involves 1,000 examples of length 1,000 turbo encoder. Performance shows N-Turbo matching turbo decoder accuracy for block length 100, with potential higher accuracy at SNR=2. N-Turbo generalizes well to unseen codewords with limited training examples. In this section, the performances of various recurrent neural networks in decoding rate-1/2 RSC code and learning BCJR algorithm with non-zero priors are compared. Results show that 2-layered GRU outperforms 1-layered RNN and single-directional RNN in terms of Bit Error Rate (BER). Matching SNR levels for training and testing data is not reliable, as indicated in FIG2. The analysis predicts the optimal training SNR for different coding rates, proposing a threshold based on the rate. Training the neural decoder for RSC encoders of varying rates shows different thresholds for each rate. The analysis predicts optimal training SNR for different coding rates, showing thresholds based on the rate. Training neural decoder for RSC encoders of varying rates reveals different thresholds for each rate. The empirical best SNR for training at each testing SNR for various rate r codes closely aligns with theoretical predictions. When the testing SNR is below the threshold, the target bit error rate is around 10^-1 to 10^-2, with many examples near the decision boundary. Matching training examples to these boundary examples maximizes sample use. However, when the testing SNR is above the threshold, the target bit error rate is much smaller, around 10^-6, with only a small proportion of examples near the boundary. Matching training SNR in this case would waste examples, so training examples from SNR 10 log 10 (2^(2r) - 1) should be near the boundary. The capacity achieving random codes for AWGN channels can be used to estimate how closely codewords can be packed at a given test SNR. The Gaussian capacity rate is given by 1/2 log(1 + SNR), which determines decision boundaries based on the rate. The desired threshold is set based on this information."
}