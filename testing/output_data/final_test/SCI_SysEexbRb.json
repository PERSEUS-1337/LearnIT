{
    "title": "SysEexbRb",
    "content": "In this paper, the focus is on understanding loss functions for training neural networks theoretically. The analytical forms of critical points for square loss functions in linear neural networks are characterized, along with conditions for achieving global minimum. The landscape properties of loss functions for linear neural networks and shallow ReLU networks are also explored. The focus of this paper is on understanding loss functions for training neural networks theoretically. It is noted that linear networks have no spurious local minimum, while one-hidden-layer nonlinear networks with ReLU activation function do have local minimum that is not global minimum. Deep neural networks have become popular in various fields, and there is a growing interest in developing theoretical understandings of neural networks, including generalization error and landscape properties of loss functions. The landscape properties of loss functions for neural networks, such as critical points being global minima, local minima, or saddle points, play a crucial role in optimization algorithms. Previous research has focused on understanding these properties for various neural networks, showing conditions for equivalence between local and global minima under different assumptions. The focus of this paper is on characterizing the sufficient and necessary forms of critical points for broader scenarios, including shallow and deep linear networks with no assumptions on data matrices and network dimensions, and shallow ReLU networks over certain parameter space. The paper focuses on characterizing critical points for linear networks with one hidden layer, providing necessary and sufficient conditions for global minimum. It also establishes new landscape properties around these critical points, offering simpler proofs for existing understanding. The paper establishes landscape properties for linear networks with one hidden layer, proving that every local minimum is a global minimum and other critical points are saddle points. It provides new proofs for characterizing the landscape around critical points without assumptions on parameter dimensions and data matrices. The matrix factorization problem satisfies these properties, and for deep linear networks with square loss function, necessary and sufficient characterizations of critical points and global minimizers are established. The study categorizes critical points in neural networks and proves equivalence between local and global minimums. It provides a full characterization of critical points for one-hidden-layer networks with ReLU activation function, including cases with one hidden unit. The results are applied to concrete scenarios. Analytical forms of critical points for loss functions of neural networks have been studied in previous work. The authors in BID1 provided an analytical form of critical points for linear networks with one hidden layer. BID13 established a sufficient condition for critical points of a generic function. Our results offer necessary forms of critical points for deep linear networks. Properties of critical points in linear and complex-valued autoencoders have been studied, showing equivalence between local and global minimums. Recent work has also explored deep linear networks, establishing conditions for critical points to be global minimums. Regularization effects on critical points in two-layer linear networks have been investigated for nonlinear neural networks as well. Nonlinear neural networks have been extensively studied, with research showing that every local minimum is also a global minimum under certain conditions. Multi-layer nonlinear networks with a pyramidal structure have been shown to achieve zero loss for critical points of full column rank when the sample size is less than the input dimension. Additionally, the loss surface of deep nonlinear networks has been connected to the Hamiltonian of the spin-glass model, with further research eliminating some assumptions and characterizing the distribution of local minimums. BID11 and BID17 studied different aspects of local minimums in neural networks, showing equivalence between local and global minimums under certain conditions. BID6 found that most local minimums are global minimums in a one-hidden-layer nonlinear network. Tian (2017) explored critical points in a two-layer ReLU network with Gaussian input data. Geometric curvature BID10 established gradient dominance conditions for deep linear networks. The regularity condition around global minimizers for deep linear, deep linear residual, and shallow nonlinear networks has been studied by various researchers. Local strong convexity properties have been established for overparameterized nonlinear networks with one hidden layer and quadratic activation functions. Additionally, the local linear convergence of the gradient descent method with tensor initialization has been shown. The volume of sub-optimal differentiable local minima in one-hidden-layer nonlinear networks is exponentially vanishing compared to global minima. Saddle points in deep neural networks have been investigated using results from statistical physics and random matrix theory. In this section, linear neural networks with one hidden layer are studied. Input data matrix X and output data matrix Y are used to learn a model mapping from X to Y via a linear network with one hidden layer. Weight parameters A 2 between the output and hidden layers are denoted as A 2 \u2208 R d2\u00d7d1. The square loss function of a linear network with one hidden layer is studied, without assumptions on parameter dimensions or data matrix invertibility. The full singular value decomposition of \u03a3 is defined, with r distinct positive singular values. The square loss function of a linear network with one hidden layer is studied, without assumptions on parameter dimensions or data matrix invertibility. The full singular value decomposition of \u03a3 is defined, with r distinct positive singular values. Values \u03c3 1 > \u00b7 \u00b7 \u00b7 > \u03c3 r > 0 with multiplicities m 1 , . . . , m r , respectively, and hasm zero singular values. Theorem 1 characterizes the necessary and sufficient forms for all critical points of L, involving matrices L 1, V, and C. The matrix C captures the invariance of the product A 2 A 1 under an invertible transform, and L 1 captures the degree of freedom of the solution set for linear systems. The analytical forms in eqs. (1) and (2) allow for constructing critical points of L by specifying choices of L 1, V, C that fulfill the condition in eq. (3). Choosing L 1 = 0 guarantees eq. (3), yielding a critical point for any invertible matrix C and block matrix V. For nonzero L 1, a proper V can be fixed to solve the linear equation on C in eq. (3) to obtain the form of a corresponding critical point. The analytical structures of the critical points have direct implications on global optimality conditions and landscape properties. The parameters {p i} r i=1 characterize how well the learned model captures important principle components of the data, reducing the loss function value as suggested by Proposition 1. The parameters {p i} r i=1 characterize the model's fit to the data in terms of the loss function value. Proposition 2 establishes conditions for a critical point to be a global minimizer, based on the number of nonzero singular values in the data matrix \u03a3. The rank determines the global minimum achievement. A global minimizer can have a non-full rank A2 capturing all nonzero singular values. A1 must be full rank in case 1, and A1 is also full rank if assumptions on network size and data matrices in BID1 are adopted. Parameters {pi} naturally divide critical points of L into non-optimal and optimal order categories. Non-optimal order involves a matrix V where pi < mi and pj > 0. Optimal order has rank(A2) < min{d2, d1} and a matrix V specified in Theorem 1. The critical points of L can be categorized into non-optimal and optimal order. Non-optimal order critical points have a perturbation with lower function value, while optimal-order critical points have a perturbation that achieves a lower function value. The loss function L categorizes critical points into non-optimal and optimal order. Non-optimal points have a perturbation with lower function value, while optimal-order points achieve a lower function value. Any non-global-minimum critical point has a descent direction, making it not a local minimizer. Therefore, any local minimizer must be a global minimizer. Additionally, any point in X has an ascent direction when the output is nonzero, ruling out the existence of local/global maximizers in X. Non-global-minimum critical points in X are saddle points with both descent and ascent directions. The loss function L distinguishes between non-optimal and optimal order critical points, where non-optimal points can be saddle points. In the case of singular data matrices, saddle points can be non-strict. The perturbation scheme focuses on capturing smaller singular values to perturb non-optimal critical points. The perturbation scheme focuses on perturbing smaller singular values to reduce the loss function and preserve the rank of A 2. The proof introduces a new technique that removes restrictions on parameter dimensions and data matrices, requiring careful choices of perturbation parameters and manipulations of matrices. In this section, deep linear networks with \u2265 2 layers are studied. Weight parameters between the layers are denoted as A k \u2208 R d k \u00d7d k\u22121 for k = 1, . . . , . The square loss function of deep linear networks is of interest, with perturbations increasing the rank of A 2 to capture additional singular values and reduce the loss function further. The square loss function of deep linear networks is studied with weight parameters denoted as A k \u2208 R d k \u00d7d k\u22121 for k = 1, . . . , . Critical points of L D are characterized by matrices A, with individual parameters recursively expressed via equations. The forms of the individual matrices can be obtained by applying the recursive equations. The square loss function of deep linear networks is studied with weight parameters denoted as A k \u2208 R d k \u00d7d k\u22121 for k = 1, . . . , . The first condition in eq. FORMULA13 guarantees that the product of parameter matrices factorizes into individual parameter matrices. The set of critical points is uncountable, but Theorem 3 provides ways to obtain some critical points. Setting L k = 0 allows for obtaining the form of critical points for any invertible C k and proper V k. The parameters determine the value of the loss function at critical points and specify the analytical form for global minimizers. The analytical form of global minimizers for deep linear networks is discussed in two propositions. For non-full rank parameter matrices, the global minima correspond to full rank parameter matrices under certain assumptions. Critical points of the loss function are analyzed to understand the relationship between local and global minima. The landscape of critical points in deep linear networks is analyzed to understand the relationship between local and global minima. Different subsets of non-global-minimum critical points are identified, including deep-non-optimal and deep-optimal orders. The critical points of the loss function in deep networks are more complex due to depth, compared to shallow linear networks. The landscape of critical points in deep linear networks is analyzed to understand the relationship between local and global minima. The loss function has specific properties for deep-non-optimal and deep-optimal order critical points, with perturbations achieving lower function values. Every local minimum is also a global minimum for these critical points, and every critical point in X D is a saddle point. The landscape of L D for deep linear networks is similar to that of shallow linear networks. The study focuses on nonlinear neural networks with one hidden layer using ReLU activation function. The weight parameters between layers are denoted as A2 and A1. The approach does not handle non-global minimizers, leaving open the problem of perturbing intermediate weight parameters in deep networks. The study examines nonlinear neural networks with one hidden layer using ReLU activation. The weight parameters are denoted as A2 and A1, with input and output data denoted as X and Y. The square loss function is of interest, with conditions for critical points being global minimums explored. The parameter space is partitioned into disjoint cones, with a focus on the set of cones K I\u00d7J satisfying specific conditions. Within these cones, the loss function reduces to that of a shallow linear network. The study focuses on nonlinear neural networks with one hidden layer using ReLU activation. The weight parameters A2 and A1, along with input and output data X and Y, are considered. Critical points of the loss function are characterized by specific matrices and singular values. The study examines critical points of the loss function in nonlinear neural networks with one hidden layer using ReLU activation. These points are characterized by specific matrices and singular values, such as L1, V, and C. In a special case with one unit in the hidden layer, the parameter space is divided into disjoint cones. Critical points within K{1}\u00d7J can be characterized by matrix 1. The critical points of the loss function in nonlinear neural networks with one hidden layer using ReLU activation can be characterized by a unit vector v supported on singular values of \u03a3J, a scalar c, and a matrix 1. Proposition 7 describes the existence and forms of these critical points over the parameter space. If a critical point exists in K{1}\u00d7J with v supported on the i-th singular value of \u03a3J, it achieves a local minimum. The critical points of the loss function in nonlinear neural networks with one hidden layer using ReLU activation can achieve local minima based on the singular values of \u03a3J. Local minima that are not global minima can exist for L N, as shown in Proposition 8 with a concrete example. The landscape of the loss function in nonlinear neural networks differs significantly from that of linear networks. This paper provides a detailed analysis of critical points for square loss functions in various types of neural networks. It is shown that linear networks do not have spurious local minima, unlike nonlinear networks with ReLU activation functions. The paper analyzes critical points for square loss functions in different neural networks. Linear networks lack spurious local minima, unlike nonlinear networks with ReLU activation. Future research aims to characterize critical points for deep nonlinear networks and explore the landscape properties around them. The paper discusses critical points for square loss functions in neural networks, focusing on the differences between linear and nonlinear networks with ReLU activation. Future research will explore critical points for deep nonlinear networks and analyze the landscape properties around them. The analysis involves singular values and projections in the context of the networks' structures. The paper discusses critical points for square loss functions in neural networks, focusing on differences between linear and nonlinear networks with ReLU activation. The analysis involves singular values and projections in the context of the networks' structures. The proof involves equations and matrices to derive the desired form of A1 and A2, satisfying certain conditions for optimization. The paper discusses critical points for square loss functions in neural networks, focusing on differences between linear and nonlinear networks with ReLU activation. The analysis involves singular values and projections in the context of the networks' structures. To satisfy certain conditions for optimization, it is shown that a critical point (A 1 , A 2 ) satisfies specific equations and forms. The critical points for square loss functions in neural networks are discussed, focusing on differences between linear and nonlinear networks with ReLU activation. Specific conditions for optimization are shown, where a critical point (A 1 , A 2 ) satisfies certain equations and forms. The global minimum value is achieved by selecting singular values in a decreasing order. If (A 2 , A 1 ) is a global minimizer, the global minimum can be achieved without needing a full rank A 2. The text discusses non-optimal-order critical points in neural networks, showing perturbations to achieve certain equations. The construction of perturbation matrices A1 and A2 satisfies specific conditions, leading to the conclusion that certain equations hold for the point (A1, A2). The text discusses perturbations to achieve specific equations for non-optimal-order critical points in neural networks. The construction of perturbation matrices A1 and A2 satisfies conditions leading to certain equations holding for the point (A1, A2). The text discusses perturbations to achieve specific equations for non-optimal-order critical points in neural networks. It involves constructing perturbation matrices A1 and A2 to satisfy conditions for certain equations to hold at the point (A1, A2). The process involves utilizing orthonormal matrices and simplifying trace terms to calculate function values. The text discusses perturbations to achieve specific equations for non-optimal-order critical points in neural networks by constructing perturbation matrices A1 and A2. It involves simplifying trace terms to calculate function values at the point (A1, A2). The text discusses perturbations to achieve specific equations for non-optimal-order critical points in neural networks by constructing perturbation matrices A1 and A2. It involves constructing a product matrix A equivalent to matrices B, defining an orthonormal block matrix S, and perturbing matrices to satisfy certain equations. The text discusses perturbations to achieve specific equations for non-optimal-order critical points in neural networks by constructing perturbation matrices A1 and A2. It involves constructing a product matrix A equivalent to matrices B, defining an orthonormal block matrix S, and perturbing matrices to satisfy certain equations. With the above equation, the function value at this perturbed point is evaluated, showing changes in diagonal elements of a matrix. A deep-optimal-order critical point is considered, leading to the identification of a perturbation that achieves a lower function value. The text discusses perturbations to achieve specific equations for non-optimal-order critical points in neural networks. It concludes that every local minimum is a global minimum for two types of critical points, and every critical point in X D is a saddle point."
}