{
    "title": "HJcSzz-CZ",
    "content": "In few-shot classification, algorithms are trained with only a few labeled examples. Recent progress has focused on meta-learning, where a model is trained on episodes with small labeled training sets and test sets. This work advances few-shot classification by incorporating unlabeled examples in each episode, including situations with distractor classes. Novel extensions of Prototypical Networks are proposed to utilize unlabeled examples in classification. Prototypical Networks are enhanced with the ability to use unlabeled examples in training to improve predictions. Experiments on Omniglot, miniImageNet, and a new split of ImageNet show the effectiveness of this approach. Deep learning methods have achieved significant advancements in tasks like speech recognition and object recognition due to large labeled datasets, but struggle with problems lacking labeled data. Few-shot learning aims to improve generalization on problems with small labeled training sets, addressing the gap between human and machine learning abilities. It focuses on few-shot classification, where only a few labeled examples per class are available. This approach is crucial for tackling problems with scarce labeled data and has seen increasing interest in deep learning developments. Few-shot learning involves meta-learning to transfer knowledge from a pool of various classification problems with large labeled data to new classification problems with few labeled examples per class. The aim is to improve generalization on problems with small training sets, such as distinguishing between unseen classes like goldfish and shark. This approach is essential for addressing problems with limited labeled data and has gained interest in deep learning advancements. In this paper, the aim is to improve few-shot learning by incorporating unlabeled data from classes to learn representations for, along with data from distractor classes. The goal is to generalize the setup by considering scenarios where new classes are learned with additional unlabeled data, addressing the challenge of transferring knowledge to new classes never seen before. In this study, the focus is on few-shot learning with the challenge of transferring knowledge to new classes never seen before. The presence of distractor classes adds a realistic level of difficulty. The research proposes benchmarks for evaluation and explores novel extensions of Prototypical Networks for semi-supervised few-shot learning. In few-shot learning, the episodic paradigm simulates test scenarios with limited labeled data for new classes. Prototypical Networks are used for this purpose, and recent progress includes semi-supervised variants that outperform purely supervised approaches. In few-shot learning, models are trained on K-shot, N-way episodes using a small subset of N classes from the training data. The model is updated to minimize loss on a test set, encouraging good generalization for new examples. This approach is known as learning to learn or meta-learning. Prototypical Network BID20 is a few-shot learning model that uses support and query sets to extract prototype vectors from each class and classify inputs based on their distance to the prototypes. It is simple yet achieves state-of-the-art performance by learning an embedding function that maps examples into a space where examples from the same class are close. Prototypical Networks use a neural network to map examples into a space where examples from the same class are close. The parameters lie in the embedding function, and prototypes are computed for each class by taking a per-class average of embedded examples. These prototypes define a predictor for new examples, assigning probabilities based on distances. The loss function updates the network by minimizing the average negative log-probability of correct class assignments. Training involves iterating over episodes and performing gradient descent updates. Generalization performance is measured on test set episodes. In few-shot learning, the generalization performance is evaluated on test set episodes using a predictor from Prototypical Networks. The semi-supervised setting includes a training set with labeled support set examples and an unlabeled set. Models are trained to predict labels for query set examples in episodes. Figure 2 illustrates the semi-supervised few-shot learning setup, where training involves support set S, unlabeled set R, and query set Q. The goal is to generalize good performance on the query set using labeled items in S and unlabeled items in R. Unlabeled items in R can be relevant to the classes or distractors. The model lacks ground truth information on whether unlabeled examples are distractors. In the semi-supervised few-shot learning setup, Prototypical Networks are extended to leverage unlabeled examples in set R for refining prototypes. The refined prototypes are used for classifying query examples based on proximity, with the average negative log probability used for training. In semi-supervised few-shot learning, Prototypical Networks are extended to refine prototypes using unlabeled examples. The refinement process adjusts cluster locations to better fit both support and unlabeled sets, borrowing from soft k-means for cluster assignments. This approach avoids non-differentiability of hard assignments and starts with regular Prototypical Network prototypes. The refinement process in semi-supervised few-shot learning adjusts cluster locations using unlabeled examples based on their Euclidean distance. Multiple refinement iterations were tested but found no improvement beyond a single step. The approach assumes unlabeled examples belong to one of N classes but aims for robustness against distractor classes. Soft k-means distributes soft assignments across all classes, but adding unlabeled distractor classes can interfere with the refinement process. To prevent this, an additional cluster can be added to capture distractors, with a prototype centered at the origin. Incorporating length-scales to represent within-cluster distances for the distractor cluster, the model learns the length-scale of the distractor cluster. However, modeling distractor unlabeled examples with a single cluster may be too simplistic as they could cover more than one natural object category. This complexity was reflected in experiments where distractor examples were sampled from multiple categories. The improved variant proposes modeling distractors as examples not within the area of legitimate class prototypes by incorporating a soft-masking mechanism on unlabeled examples' contribution based on their distance to prototypes. Soft thresholds and slopes are predicted for each prototype to refine the soft k-means clustering process. The improved variant introduces a soft-masking mechanism for unlabeled examples based on their distance to prototypes, allowing the model to include or ignore certain examples during training. This process is entirely differentiable and a single refinement step is found to be effective. The approach combines ideas from few-shot learning, semi-supervised learning, and clustering for improved performance. The best methods for few-shot learning involve episodic training within the metric learning framework. Previous work includes Deep Siamese Networks, Matching Networks, and Prototypical Networks. These methods aim to learn an embedding function that groups examples of the same class together and separates different classes. The distances between embeddings are used for classification. Another related approach is Matching Networks used in an active learning framework. BID1 employ Matching Networks in an active learning framework for few-shot learning, where the model selects unlabeled items to add to the support set before classifying the query set. Other approaches include meta-learning agents acquiring ground-truth labels and training generic neural architectures for accurate predictions. In this work, Prototypical Networks are extended for few-shot learning due to their simplicity and efficiency. The approach involves training a classifier on the initial set and then adding confidently predicted unlabeled items to the training set. This process is similar to a soft k-Means extension to Prototypical Networks, where soft assignments match the classifier output. The algorithm involves re-feeding a Prototypical Network with a new support set augmented with soft self-labels from the unlabeled set. Our algorithm extends Prototypical Networks for few-shot learning by re-feeding with a support set augmented with soft self-labels from the unlabeled set. It is related to transductive learning, where the base classifier is refined by unlabeled examples. To avoid memorizing labels, a separate unlabeled set is used. The goal is to apply k-Means in the presence of outliers to correctly discover and ignore them, preventing a bad partition of the true data. Our contribution to semi-supervised learning and clustering involves transferring knowledge from a training set to a new test set. We evaluate our model on three datasets, including Omniglot, a dataset of handwritten characters. The images are resized and rotated, resulting in a total of 6,492 classes. The miniImageNet dataset consists of 6,492 classes, split into training, validation, and testing sets. tieredImageNet is a proposed dataset for few-shot classification with 608 classes grouped into 34 categories based on the ImageNet hierarchy. Each category contains between 10 and 30 classes. The tieredImageNet dataset has 34 categories with 10-30 classes each, split into 20 training, 6 validation, and 8 testing categories. Unlike miniImageNet, tieredImageNet ensures distinct training and testing classes, providing a more realistic few-shot learning scenario. The tiered structure prevents high-level categories like \"musical instrument\" from being split between training and test classes. The tieredImageNet dataset has a tiered structure with 34 categories and 10-30 classes each. It may be useful for few-shot learning approaches that utilize hierarchical class relationships. Additional splits are created for each dataset to separate images into labeled and unlabeled sets. For Omniglot and tieredImageNet, 10% of images per class are labeled, while for miniImageNet, 40% are labeled. Average classification scores are reported over 10 random splits, with uncertainty computed using standard error. The training episodes are constructed by sampling N classes randomly from the training classes, selecting K images from the labeled split of each class for the support set, and M images from the unlabeled split for the unlabeled set. Optionally, distractors can be included by sampling H other classes and M images from the unlabeled split. The labeled/unlabeled split results in using less label information compared to previous work on the datasets, making the published numbers an upperbound for the performance of the semi-supervised models in this study. In the experiments, N classes are sampled for training episodes, with K images from the labeled split and M images from the unlabeled split. Distractors can be included by sampling H other classes. Test episodes follow a similar process, with N classes sampled from the test set. The dataset splits and class assignments can be found in Appendices A and B. In the experiments, N classes are sampled for training episodes with K images from the labeled split and M images from the unlabeled split. Distractors can be included by sampling H other classes. Test episodes follow a similar process with N classes sampled from the test set. The dataset splits and class assignments can be found in Appendices A and B. The three semi-supervised models are compared with two baselines, one being a purely supervised Prototypical Network and the other using semi-supervised refinement of prototypes at test time. Model hyperparameters are detailed in Appendix D. Results for Omniglot, miniImageNet, and tieredImageNet are presented in TAB2 and 5, with FIG3 showing the performance of models on tieredImageNet using different values for M. Additional results comparing the ProtoNet model to baselines and analysis of the Masked Soft k-Means model can be found in Appendix C. Our proposed models outperform baselines in all three benchmarks, especially in nondistractor settings. In scenarios with distractors, Masked Soft k-Means shows promising results. Masked Soft k-Means demonstrates robust performance in scenarios with distractors, achieving close to optimal results. Test accuracy improves as the number of items in the unlabeled set per class increases. Models trained with M = 5 show the ability to generalize and acquire better representations through meta-training. The proposed semi-supervised few-shot learning paradigm includes an unlabeled set in each episode, extending to situations with novel classes. The text introduces a new dataset, tieredImageNet, with novel classes for few-shot classification. Several extensions of Prototypical Networks are proposed, showing improvements in semi-supervised settings. Future work includes incorporating fast weights BID0 BID5 for different embedding representations. Experiments on the Omniglot dataset used a specific train/test split, with a separate validation split for hyper-parameter selection. Models were trained on the train split with various alphabets. The tieredImageNet dataset contains various high-level categories with 10 to 30 classes each. Classes with multiple parent nodes were removed to ensure separation between training and test categories. Test categories were selected to have varying levels of dissimilarity from training categories. The tieredImageNet dataset includes categories like \"working dog\" which are similar to training categories, while others like \"geological formation\" are different. The list of categories and dataset statistics can be found in TAB4. Train categories include various animals, birds, instruments, tools, and garments. The curr_chunk discusses validation and test categories for few-shot learning baseline results using labeled/unlabeled splits. It also mentions the use of CNNs to extract features for the baselines. The curr_chunk provides experimental results on few-shot classification using Regular ProtoNet, showing significant improvement compared to baseline methods. It includes test accuracy values with different numbers of unlabeled items and mask output distribution of the Masked Soft k-Means model on Omniglot dataset. The hyperparameter settings used for training are also mentioned. The training process involved 20K updates with a starting learning rate of 1e-3, decayed every 25K updates by half. ADAM BID11 was used for optimization, and a single hidden layer MLP with 20 units and tanh non-linearity was employed. Hyperparameters were not tuned, suggesting potential for better performance with a more thorough search."
}