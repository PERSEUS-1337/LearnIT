{
    "title": "rkYTTf-AZ",
    "content": "Machine translation has recently achieved impressive performance through deep learning and large-scale parallel corpora. This work explores translating without parallel data by mapping sentences from monolingual corpora in two languages into a shared latent space. The model learns to reconstruct in both languages from this space, achieving high BLEU scores on two datasets and language pairs without using any parallel sentences at training time. Recent advances in deep learning and the availability of large-scale parallel corpora have led to impressive performance in machine translation on several language pairs. However, these models require massive amounts of parallel data, which can be costly and often nonexistent for low-resource languages. Leveraging monolingual data has been explored to improve machine translation quality in a semi-supervised setting, with back-translation being a notable data-augmentation scheme proposed. BID30 proposed a data-augmentation scheme called \"back-translation\" using an auxiliary translation system to generate translations from a monolingual corpus. Other methods include augmenting the decoder with a language model and adding an auxiliary auto-encoding task on monolingual data. These approaches still require tens of thousands of parallel sentences. Translation methods have relied on labeled information from related language pairs or other modalities. One exception is a method that reduces machine translation to a deciphering problem for short sentences. Models are trained for sentence reconstruction and translation in both directions. In this paper, the model is symmetric and aims to train a general machine translation system without supervision. The key idea is to build a common latent space between languages and learn to translate by reconstructing sentences in both domains. The model aims to train a machine translation system without supervision by reconstructing sentences in both source and target domains. It uses back-translation and adversarial regularization to ensure the latent representations have the same distribution, leading to iterative improvements in translation quality. The approach is fully unsupervised, starting with a na\u00efve word-by-word translation model. The unsupervised translation model is based on word-by-word translation using a bilingual lexicon derived from monolingual data. It encodes sentences from both languages into the same feature space, allowing for translation in either language. Despite not matching supervised approaches, the model achieves remarkable performance, with results on the WMT dataset comparable to supervised systems trained on 100,000 sentence pairs. Experimental results are presented in section 4, showing high BLEU scores on the Multi30K-Task1 dataset. The translation model proposed in this section consists of an encoder and a decoder responsible for encoding and decoding source and target sentences. The model uses a single encoder and decoder for both domains, with the only difference being the choice of lookup tables for different languages. The set of words in the source and target domains are associated with learned word embeddings, allowing for translation between the two domains. The translation model includes an encoder that computes hidden states using word embeddings and a decoder that generates an output sequence based on these hidden states. The encoder and decoder are shared between source and target languages, with parameters specific to each language. The translation model utilizes a sequence-to-sequence model with attention BID0, consisting of a bidirectional-LSTM encoder and LSTM decoder with 3 layers each. The decoder generates output words iteratively based on the highest probability, using a context vector from the encoder states. The decoder in the translation model has 3 shared LSTM layers for both source and target encoder/decoder. Attention weights are also shared. Sentences are generated using greedy decoding. The model is trained on datasets from different domains by reconstructing sentences from noisy versions. The model initially uses an unsupervised na\u00efve translation model before training the encoder and decoder iteratively. The encoder and decoder are trained by minimizing an objective function to reconstruct and translate noisy input sentences. A discriminator is learned to align the latent distribution of sentences in source and target domains. The encoder/decoder generate new translations iteratively until convergence. At test time, they can be used for machine translation despite the lack of parallel data. Training an autoencoder for sentences is straightforward with the sequence-to-sequence model. Training an autoencoder for sentences involves adding noise to input sentences to prevent the model from simply copying words. By using a Denoising Auto-encoder strategy and a stochastic noise model, the objective function aims to reconstruct the corrupted version of sentences, with a measure of discrepancy between the sequences. The noise model used involves adding two types of noise to input sentences: dropping words with a certain probability and shuffling the sentence by applying a random permutation. The permutation is generated by sorting a random vector, with a tunable parameter determining the level of shuffling. The noise model involves adding noise to input sentences by dropping words with a certain probability and shuffling the sentence with a random permutation. The method can return any permutation, and using specific parameters can generate permutations similar to noise observed in word-by-word translation. The word dropout and input shuffling strategies have a critical impact on results, with using both simultaneously yielding the best performance. The objective is to constrain the model to map input sentences from one domain to another, ultimately interested in at test time. The objective is to learn the encoder and decoder to reconstruct x from a corrupted version C(y). The cross-domain loss is defined as the sum of token-level cross-entropy losses. The encoder should output features in the same space regardless of the input language to enable the decoder to decode in any language. The encoder and decoder aim to reconstruct a corrupted input regardless of language, with a cross-domain loss defined by token-level cross-entropy. A discriminator is trained to classify source and target sentence encodings, ensuring strong structure in feature space for effective translation. The encoder and decoder aim to reconstruct corrupted inputs regardless of language, with a cross-domain loss defined by token-level cross-entropy. A discriminator is trained to classify source and target sentence encodings, ensuring strong structure in feature space for effective translation. The discriminator produces a binary prediction about the language of the input, with the encoder trained to fool the discriminator. The final objective function includes hyper-parameters weighting the importance of auto-encoding, cross-domain, and adversarial loss. The training algorithm involves minimizing the generator loss to update the translation model and the discriminator loss to update the discriminator. The iterative process starts with an initial translation model, which is used to translate monolingual data for the cross-domain loss function. At each iteration, a new encoder and decoder are trained by minimizing the loss function, resulting in a new translation model. This process is jump-started by making word-by-word translations using a parallel dictionary learned through an unsupervised method. The algorithm proposed by BID4 involves using a parallel dictionary learned through an unsupervised method to improve translation quality. The encoder maps translations into a cleaner representation, while the decoder predicts noiseless outputs. This process enables better back-translations and produces less noisy translations iteratively. The hyper-parameters are selected based on a criterion. The proposed algorithm involves selecting hyper-parameters based on a surrogate criterion correlated with translation quality. It uses a two-step translation process to evaluate model performance, averaging BLEU scores over original inputs and reconstructions. The model with the highest average score is selected. The model selection criterion M S(e, d, D src , D tgt ) is used to determine when to stop training and select the best hyper-parameter setting. The Spearman correlation coefficient between the criterion and BLEU scores on the test set is high, indicating its effectiveness in model selection. However, models selected with the unsupervised criterion may have slightly lower BLEU scores compared to models selected using a small validation set. In this section, datasets, pre-processing, baselines, and empirical validation are described. Experiments focus on English-French and English-German language pairs on three datasets. The English-French dataset consists of 30 million sentences after filtering. Monolingual corpora are built from 15 million random pairs. The English monolingual dataset is created from the selected English sentences. The English and French monolingual datasets do not overlap, ensuring no exact correspondence between examples. A validation set of 3,000 sentences from the training corpora is used for model selection. Results are reported on the newstest2014 dataset. For English-German, two monolingual training corpora of 1.8 million sentences each are created, with testing on the newstest2016 dataset. In the study, the training and validation sets were divided into monolingual corpora, resulting in 14,500 monolingual source and target sentences in the training set, and 500 sentences in the validation set. The vocabulary size was limited on specific datasets. Two baselines were used: word-by-word translation and word reordering using an LSTM-based language model. In this study, a baseline method was used for word reordering using an LSTM-based language model trained on the target side. The method involved considering pairwise swaps of neighboring words and selecting the best swap iteratively. This approach was applied to the WMT dataset with a large monolingual data for training the language model. The performance of this method served as an upper-bound for word generation without word replacements. Additionally, a supervised learning model was considered, trained with supervision using cross-entropy loss on original parallel sentences. Word embeddings were trained on the monolingual corpora using fastText BID1 to initialize the embeddings of the model. The study utilized fastText BID1 for word embeddings on monolingual corpora and an unsupervised method by BID4 for bilingual dictionary inference. High-quality embeddings and dictionaries were obtained with accuracy rates of 84.48% and 77.29% on French-English and German-English. Word vectors were learned on Wikipedia using fastText for Multi30k datasets due to small training corpora. The discriminator architecture consisted of a multilayer perceptron with Leaky-ReLU activation functions. The model utilized a discriminator with Leaky-ReLU activation functions and a smoothing coefficient. Training details included using Adam for the encoder and decoder, RMSProp for the discriminator, alternating updates, and setting lambda values. The model outperformed baselines in BLEU scores for word-by-word translation into English. Our model surpasses baselines in BLEU scores for word-by-word translation into English, achieving a BLEU score of 27.48 and 12.10 for the en-fr task after one iteration. It even outperforms oracle reordering on some language pairs, suggesting correct word substitutions. With further iterations, the model achieves impressive BLEU scores of 32.76 and 15.05 on Multi30k-Task1 and WMT datasets for English to French translation. Comparison with supervised approaches shows the unsupervised model matching the performance of a supervised NMT model trained on about 100,000 parallel data. The unsupervised approach achieves comparable performance to a supervised NMT model trained on 100,000 parallel sentences, showing promise for low-resource languages. Results suggest potential for semi-supervised translation models in the future. Using a phrase-based machine translation system, BLEU scores of 21.6 and 22.4 are obtained for en-fr and fr-en, outperforming the supervised NMT baseline. Further improvements could be made by incorporating BPE. The quality of unsupervised machine translation can be enhanced by using BPE to remove unknown words. Iterative learning process improves the model's quality in Multi30k-Task1 dataset language pairs. The obtained model shows high quality after the first iteration. The iterative learning process improves the quality of unsupervised machine translation on the Multi30k dataset. Performance gains diminish after iteration 3, indicating convergence. Ablation study shows the importance of different components in the system. The ablation study on the Multi30k-Task1 dataset reveals that the unsupervised word alignment technique is crucial for model performance. Using a back-translation dataset or pretrained embeddings significantly impacts BLEU scores in English-French translation tasks. The adversarial component improves system performance significantly, with up to 5.33 BLEU difference in the French-English pair of Multi30k-Task1. Ensuring similar distribution of latent sentence representations across languages is crucial for benefiting from cross-domain loss. Removing auto-encoding loss or input sentence corruption degrades performance. A related work is the style transfer method with non-parallel text by BID32. The authors of BID32 propose a style transfer method using a sequence-to-sequence model with a discriminator. They train the encoder and decoder to reconstruct the input while fooling the discriminator. Two discriminators are used for the source and target domains. The decoder is trained using Professor forcing to ensure similarity between training and inference dynamics. Other works, like BID38, also use adversarial training to learn invariant representations. The authors propose a method to train an encoder and model for predictions, with a discriminator to remove bias from latent codes. This leads to better generalization on tasks. BID14 trained a variational autoencoder with a structured code for sentence attributes. A discriminator classified generated sentences, improving the decoder. The decoder in this work uses a differentiable model to optimize model selection metrics. By alternating training between source-to-target and target-to-source translation, they efficiently converge without the need for reinforcement learning-based approaches. In image translation tasks, various approaches like CoGAN, variational autoencoders, and generative adversarial networks have been used to map two image domains without paired supervision. Techniques like cycle consistency loss and Fader Networks architecture have also been employed to ensure accurate image translation. The Fader Networks architecture is used in image translation tasks to remove specific attributes from latent states. In this paper, a new approach to neural machine translation is presented, where a translation model is learned using monolingual datasets only. The model iteratively improves based on a reconstruction loss and uses a discriminator to align latent distributions. The approach presented in this paper for neural machine translation involves learning a translation model using monolingual datasets only. It utilizes a reconstruction loss and a discriminator to align latent distributions, demonstrating effective translation without supervision."
}