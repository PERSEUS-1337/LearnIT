{
    "title": "B1eZCjA9KX",
    "content": "We present a sequence-to-action parsing approach for the natural language to SQL task, using non-deterministic oracles to train models that achieve an execution accuracy of 83.7% on the WikiSQL dataset. When combined with execution-guided decoding, the model sets a new state-of-the-art performance at 87.1%. This approach has applications in health care, financial markets, and business process management. The goal is to allow users to interact with databases through natural language, known as NL2SQL. Understanding the semantics of natural language statements and mapping them to SQL is key. This problem was previously understudied due to lack of annotation, leading to a more difficult learning problem. The recent release of large-scale annotated NL2SQL datasets has sparked interest in solving the problem of mapping natural language to SQL queries. Existing approaches fall into two categories: sequence-to-sequence neural models and modularized models. The former struggle with labeling a single ground truth query when multiple equivalent queries exist, while the latter use a sequence-to-set approach to predict table columns. The second class of models use a sequence-to-set approach to predict table columns in the query and then predict the rest independently for each column. This approach avoids ordering issues but makes it difficult to leverage inter-dependencies among conditions. A sequence-to-action parsing approach is developed for the NL2SQL problem, filling SQL query slots with actions from an inventory. Non-deterministic oracles are proposed for training incremental parsers, allowing for multiple correct action continuations from a partial parse to account for logical form variations. The model combines the advantages of a sequence-to-sequence model and a SELECT statement. Our modularized model improves performance on the WikiSQL dataset by 2.1% compared to traditional static oracles. By combining our approach with execution-guided decoding, we achieve a new state-of-the-art performance with 87.1% test execution accuracy. Experiments on the ATIS dataset confirm the applicability of our models to other NL2SQL datasets. Our goal is to generate SQL queries from natural language questions. The WikiSQL dataset queries follow a specific SQL structure with SELECT agg selcol WHERE col op val (AND col op val) format. The order of conditions is irrelevant due to the semantics of AND. The input to the parser is denoted as x. The parser generates an executable SQL query y from input x, consisting of a question w and table schema c. Parsing decisions are made incrementally based on a learned policy, moving the parser through states until a complete logical form y is extracted. Training optimizes the parameterized policy by predicting subsequent actions probabilistically. The semantic parser optimizes a parameterized policy to generate an executable SQL query from input. The parser moves through states based on a learned policy until a complete logical form is extracted. During inference, a greedy approach is taken to pick the highest scoring action at each step. The parser states and actions are defined, along with the neural-network model architecture. A structured representation of a full parse is shown, with actions like selecting text spans and terminating the parsing process. The example query translates to a sequence of actions. The decoder model is designed to generate a logical form from input data, with a focus on context-sensitive representations and a probability distribution over parser actions. The challenges include the dynamic set of valid actions and the context-dependent nature of parser decisions, addressed through an LSTM-based framework. The LSTM-based decoder framework addresses challenges by scoring candidate actions individually and normalizing scores into a probability distribution. Dependencies between parser decisions and input data are captured through a dot-product attention mechanism. The model architecture includes intra-sequence LSTMs, self-attention, and cross-serial attention for context-sensitive representations. Each word is mapped to an embedding and processed by a bi-directional LSTM. Column headers are handled similarly with word embedding lookup and bi-LSTM. Self-attention is then applied to contextualize the initial representations. The model architecture involves self-attention and cross-serial attention to contextualize representations for natural language questions and SQL queries. Ambiguities arise from the ordering of filtering conditions and different SQL queries producing the same results, leading to multiple correct reference transition sequences for training instances. The model architecture uses self-attention and cross-serial attention to contextualize representations for natural language questions and SQL queries. Ambiguities in the ordering of filtering conditions lead to multiple correct reference transition sequences for training instances. To address this, non-deterministic oracles are used to explore alternative correct action sequences during training. The text-to-SQL parser training suffers from the \"order-matters\" issue when the oracle is non-deterministic, allowing for different correct action sequences. Prior solutions include reinforcement learning and a sequenceto-set approach, but these methods have drawbacks such as optimization instability and increased training time. Our model design leverages non-deterministic oracles to address the \"order-matters\" issue in capturing inter-dependencies among clauses for predicting filtering conditions. It combines an incremental approach with a modularized approach for higher-quality training signals. The model accepts all possible continuations at intermediate steps, regardless of their linearized positions. Our model leverages non-deterministic oracles to learn implicit column names in natural language queries, addressing parser errors caused by missing explicit mentions. This approach allows for the prediction of special column name \"ANYCOL\" and expands predictions into disjunctions of filtering conditions applied to all columns during execution. Our model uses non-deterministic oracles to predict column names in natural language queries, including the special column name \"ANYCOL\". This allows for expanding predictions into disjunctions of filtering conditions applied to all columns, improving parser accuracy. The model aims to optimize execution accuracy by using pre-trained GloVe embeddings and type embeddings for natural language queries and column names. Only tokens appearing at least twice in the training data are retained in the vocabulary before the embedding layer. Features indicating word appearances in column names are embedded into 4-dimensional vectors and concatenated with word embeddings before being input into the biLSTMs. The model utilizes pre-trained GloVe embeddings and type embeddings for natural language queries and column names. Word appearances in column names are embedded into 4-dimensional vectors and concatenated with word embeddings before input into the biLSTMs. The training includes a batch size of 64 with a dropout rate of 0.3 and Adam optimizer with a default initial learning rate of 0.001. The model achieves comparable results with state-of-the-art models and shows improvement with non-deterministic oracles during training. The model's execution accuracy drops due to the use of ANYCOL option for column choice. Non-deterministic oracles addressing \"order-matters\" have minimal impact on model performance, while adding ANYCOL significantly improves execution accuracy from 81.8% to 83.7%. The incremental parser uses a greedy decoding strategy. The model's execution accuracy drops due to the use of ANYCOL option for column choice. Non-deterministic oracles addressing \"order-matters\" have minimal impact on model performance, while adding ANYCOL significantly improves execution accuracy from 81.8% to 83.7%. The incremental parser uses a greedy decoding strategy. A natural extension is to expand the search space using beam search decoding. The decoder maintains a state for the partial output, which consists of the aggregation operator, selection column, and completed filtering conditions until that stage in decoding. The execution-guided decoder retains the top-k scoring partial SQL queries free of runtime exceptions and empty output. The execution-guided decoder achieves an 87.1% accuracy on the test set, setting a new state of the art. Performance of the static oracle model with execution-guided decoding is comparable to the non-deterministic oracle model but requires a larger beam size. Experiments on the ATIS dataset show the model's ability to generalize to diverse SQL structures. The reduced dataset consists of 933 examples, with 714/93/126 examples in the train/dev/test split, respectively. Models trained with static and non-deterministic oracles achieve 67.5% and 69.1% accuracy on the test set. The use of non-deterministic oracles during training improves performance, especially for smaller datasets like ATIS. WikiSQL is a large-scale dataset with annotated pairs of natural language queries and SQL forms. SQL syntax is weaker than previous datasets like ATIS, BID25, BID3, and GeoQuery. WikiSQL has diverse questions. NL2SQL is a type of semantic parsing mapping natural language to logical forms. Neural semantic parsing treats it as a sequence generation problem, achieving state-of-the-art results on datasets like WikiSQL. The models proposed by BID4 and BID33 aim to generate tree outputs and use sketches for guidance in semantic parsing. BID33 explicitly incorporates grammar constraints during decoding, while training oracles are commonly used for syntactic parsing to find optimal actions in sub-optimal parsing states. Our work introduces non-deterministic training oracles for incremental semantic parsing, specifically for the NL2SQL task. By utilizing non-deterministic oracles during training, our model achieves an execution accuracy of 83.7% on the WikiSQL dataset, outperforming the current state of the art by 2.3%. Additionally, incorporating execution-guided decoding further improves the accuracy to 87.1%, setting a new state-of-the-art on the test set. Our work achieved a new state-of-the-art execution accuracy of 87.1% on the test set by using non-deterministic oracles for training incremental semantic parsers. This approach involves identifying multiple correct transition sequences and determining possible continuations for intermediate states. Promising results were shown on WikiSQL and filtered ATIS datasets, with potential for extension to more complex NL2SQL tasks and other semantic parsing domains."
}