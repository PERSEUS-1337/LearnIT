{
    "title": "SylPMnR9Ym",
    "content": "Intelligent agents can learn to represent the action spaces of other agents through visual observation. Stochastic video prediction is used to capture scene dynamics and learn a latent variable that disentangles action structures. The model, called Composable Learned Action Space Predictor (CLASP), shows potential in capturing action spaces in complex visual settings. In a semi-supervised setting, learned representations perform comparably to fully supervised methods on action-conditioned video prediction and planning in complex visual settings, requiring fewer action labels. The focus is on learning an agent's action space from unlabeled visual observations, highlighting the importance of perceptual learning in acquiring and using action representations. Infants observe adults walking before attempting to walk themselves, which helps them acquire representations of actions. Unsupervised learning from sensory experience plays a critical role in this process. Reinforcement learning offers an alternative approach to learning action spaces and consequences. Recent advancements in RL suggest efficient learning of natural, goal-directed walking behavior. Recent breakthroughs in model-free and model-based reinforcement learning suggest that end-to-end training can be used to learn mappings between sensory input and actions. Methods for sensorimotor learning from purely visual data involve using latent composition to recover actions from passive data without requiring labels. The model learns to represent actions in sequences identically by training a representation to capture the dynamics of the scene and its compositional structure. The learned action space z recovered by our method captures the structure of the action space, facilitating learning from passive observations where action information is not available. This method may be useful for imitation learning and reusing action representations between systems with different effectors and goals. In this work, a model is developed to learn an agent's action space from unlabeled videos, enabling direct planning in latent space. The representation obtained can be used for efficient subsequent learning with a small number of labeled action sequences. This representation is similar to those found in the parietal and premotor areas of the cortex, crucial for flexible motor control. In contrast to traditional unsupervised learning of dynamics, this work focuses on disentangling action information from the environment's state. It builds on stochastic video prediction methods and trains the representation to be minimal, focusing on dynamic sensory input properties. The curr_chunk discusses training a representation to be composable and capture the structure of an agent's actions through unsupervised learning of an agent's action space using a stochastic video prediction model. This is a novel approach that enforces disentangling of individual action representations from static content to capture sensory dynamics. The method involves training a stochastic video prediction model to learn a minimal and composable representation of actions that is independent of scene content and visual characteristics. This representation can be used for action-conditioned video prediction and planning, requiring significantly fewer labeled videos compared to supervised methods. The Information Bottleneck (IB) objective function can be optimized with a variational approximation similar to the VAE objective, leading to disentangled representations. Applying the IB principle to temporal models enforces minimality of the representation, distinct from methods focusing on static content and pose disentanglement. Our model learns factors of variation purely from passive temporal visual observations, making it applicable even in environments where access is limited or costly. Recent works have utilized temporal information for representation learning using LSTM and CNN architectures to predict future frames and actions. Various modifications have been proposed to improve unsupervised learning with video data. The focus of this work is on learning action-conditioned predictive models for video prediction, inspired by stochastic video prediction methods. The model aims to discover the space of possible actions from video data, capturing the multimodal distribution of future images. In this work, the focus is on developing recurrent latent variable models based on variational autoencoders to capture the multimodal distribution of future images. The proposed approach aims to learn unsupervised representations for an agent's action space, building on sensorimotor abstractions for behavior in reinforcement learning and robotics. Previous work has focused on hand-crafted abstractions and hierarchies for sensorimotor mappings, with methods for aggregating low-level controls into higher-level representations for planning and reinforcement learning. In this section, the architecture for learning a minimal and composable action representation is described. A variational video prediction model is used to learn a latent representation of the change between past and current frames, without considering labeled actions at this stage. In Sec. 3.2, an unsupervised method is introduced to impose composability of the latent for structured representation in CLASP. A bijective mapping between latent representation and control output is learned with a small number of labeled data points (Sec. 3.3). The method involves a recurrent latent variable model for video prediction based on a temporal extension of conditional VAE. The learned bijective mapping is utilized for action-conditioned video prediction and planning in the learned action space (Sec. 4.2 and 4.3). The generative model in Fig. 2 outputs a latent variable at each timestep. The model generates the next frame by taking the mean of the conditional distribution. An additional network approximates the posterior of the latent variable to optimize the log-likelihood. The standard VAE formulation does not constrain the information in the latent variable, so a reformulation is proposed for learning a minimal representation. The VAE objective is reformulated using the Information Bottleneck (IB) to minimize mutual information between action representation and input frames while maximizing reconstruction ability. A Lagrange multiplier balances the two components, with a higher value leading to more efficient representations. The variational IB provides an approximation with an additional constant. The video prediction objective for the model is derived by aggregating over a sequence of frames. The video prediction objective for the model is to learn action representations by associating the latent variable z t with the distribution of changes between frames. This is inspired by representing z t with the agent's actions in video data. The objective encourages the model to learn action representations for composability. The model aims to learn action representations for composability by using a procedure that involves defining a random variable, composing latent samples into trajectories iteratively, and maximizing an objective function using a multilayer perceptron. The model aims to learn composable action representations by using a variational approximation method with a unit Gaussian prior distribution. This encourages minimal representation for trajectories composed of action representations, promoting efficient composition. The training objective combines two objectives to achieve Composable Learned Action. Our approach, CLASP, learns a minimal and disentangled latent representation z for control. We use a bijective mapping from action-annotated frame sequences to determine the correspondence between z and action u. This step requires less data compared to models with full action supervision. Our approach, CLASP, learns a bijection between latent representation z and actions u, enabling direct correspondence without relying on standard image-based motion representations like optical flow. This allows for action-conditioned video prediction and planning. Our approach, CLASP, enables action-conditioned video prediction and planning with fewer labels than supervised approaches. Experiments were conducted on simulated reacher and real-world robot pushing datasets. The reacher dataset involves a robot arm rotating with random angular distances between images. The dataset consists of 100,000 training and 4,000 test sequences. Our method learns a latent space with a clear correspondence to the ground truth actions, while the baseline model fails to produce a disentangled representation. Variations of the dataset include varying backgrounds from the CIFAR-10 dataset and varying robot appearance. The BAIR robot pushing dataset comprises 44,374 training and 256 test sequences of 30 frames each, with actions defined as differences in the spatial position of the end effector in the horizontal plane. The learned representation can be used for action-conditioned prediction and visual servoing, with comparisons made to the original model of Denton & Fergus (2018) and fully supervised approaches for performance evaluation. To evaluate our method's performance, we compare it to fully supervised approaches using BID17 for the reacher dataset and Finn & Levine (2017) for the BAIR dataset. For planning, we also compare to a model by Agrawal et al. (2016) that learns forward and inverse dynamics with direct supervision. Metrics include absolute angular position for the reacher dataset and change of end effector position for the BAIR dataset, capturing direct consequences of actions. In visual servoing, we measure angular distance to the goal state at the end of servoing. In visual servoing, the angular distance to the goal state is measured at the end of servoing. The learned action space structure is inspected by training CLASP on the reacher dataset and visualizing the representation. Principal Component Analysis (PCA) is used to find the two-dimensional subspace with maximal variability, with the first PCA dimension capturing 99% of the variance due to the robot having one degree of freedom. Our method successfully recovers the structure of possible actions of the robot. Table 1 shows the results of action-conditioned video prediction. The model can recover action representations and be used for transferring actions between sequences with different initial states. Additionally, the model is capable of action-conditioned video prediction using ground truth sequences annotated with actions. The model trained with the composability objective on the reacher dataset successfully performs the task of action-conditioned video prediction, with performance similar to the fully supervised model. On the BAIR dataset, the model outperforms the baseline model, reducing the performance gap. Our approach outperforms the baseline model, reducing artifacts in generated frames. Results show promise for complex, real-world interactions. The learned action space can be used for planning tasks like visual servoing, similar to Finn & Levine (2017). Our planning algorithm, based on Model Predictive Control (MPC), plans trajectories in the latent space z using MLP act to retrieve corresponding actions. The controller refines action trajectories with the Cross Entropy Method (CEM) and selects the trajectory closest to the goal for execution. Servoing terminates once the goal is reached or maximum steps are executed, measuring distance using cosine distance between VGG16 representations. The baseline by Agrawal et al. (2016) follows a different procedure. The study demonstrates the effectiveness of learning from passive observations in a reacher environment. Results show that the agent can accurately reach the target and plan trajectories using a small number of action-labeled training sequences. The model outperforms supervised baselines in low-data scenarios and remains competitive in abundant data settings. Our model performs well in low-data scenarios and remains competitive in abundant data settings, as shown in Table 3 in the Appendix. Experiments on different visual variability in the reacher dataset demonstrate the robustness of our approach. The model can reliably discover patterns in sequences with variations not seen during training. Our model demonstrates robustness to visual changes and can learn from videos with different visual conditions or agents, requiring significantly less data compared to supervised baselines. This suggests that our method is effective for passive learning from diverse video sources. The text chunk discusses learning the structure of an agent's action space from visual observations alone by imposing minimality and composability on a latent variable for stochastic video prediction. This approach offers a data-efficient alternative to fully supervised methods and can be used for action-conditioned video prediction and planning. The representation is insensitive to static scene content and visual characteristics, achieving promising results in realistic settings. The architecture used is similar to SVG-FP of Denton & Fergus (2018). The text discusses learning the structure of an agent's action space from visual observations by utilizing a neural network architecture similar to SVG-FP of Denton & Fergus (2018). Input images are encoded using a CNN, decoded with transposed convolutions, and generated using a LSTM network. The model observes past input frames, generates corresponding latents and predicts future images. At test time, latents are sampled from a prior distribution. The text discusses a stochastic video prediction model that utilizes a neural network architecture similar to SVG-FP of Denton & Fergus (2018). The model does not observe ground truth frames in the future during training but takes its own predicted frames as inputs. This allows the network to generalize to observing generated frame encodings at test time. The approach omits re-encoding, saving computation. The model is conditioned on five images and rolls out ten future images. The model is conditioned on five images and generates ten future images with a resolution of 64 \u00d7 64 pixels. The image representation dimension is 128, and the learned representation dimensions are 10. Different architectures are used for the reacher and BAIR datasets, with varying network configurations for different components. Wider and deeper architectures did not improve performance for certain networks. The latent space in experiments had a simple representation or was entangled with static content, making a more powerful network unnecessary. Latent samples used for trajectory representation were C = 4. The leaky ReLU activation function was used in networks. Models were trained on a high-end NVIDIA GPU for 4 hours on the reacher dataset and one day on the BAIR dataset. The bottleneck parameters \u03b2 z and \u03b2 \u03bd were chosen based on practical effectiveness. The latent space should be set to the highest value for high-quality image samples. Choosing the right value is crucial to avoid inferior quality samples. The problem of determining \u03b2 is common in stochastic video prediction methods. Algorithm 1 is used for visual servoing, sampling latent sequences and using a video prediction model. The cost of an image trajectory is defined as the cosine distance between feature representations of the target image. The text discusses the average absolute angle error for action-conditioned video prediction using different models. It compares the performance of various models in predicting image trajectories based on feature representations. The Cross Entropy Method is mentioned in the update step for improving predictions. In the update step of the Cross Entropy Method (CEM) algorithm, trajectories are ranked based on cost, and a Gaussian distribution is fitted to the latents z. Latent sequences are sampled and mapped to output control actions using a learned mapping. The algorithm terminates after a specified number of servoing steps. Parameters for visual servoing experiments are listed in Tab. 5. The visual servoing experiments involved using a hand-engineered algorithm to determine absolute angle values from grayscale images of the reacher environment. The algorithm accurately detected angles with errors mostly below 5 degrees, as shown in a histogram in FIG3. The model output quality is suitable as surrogate ground truth, achieving similar performance with a neural network regression approach. Errors are attributed to discretization effects at low image resolutions. The method's robustness is tested with different static backgrounds from the CIFAR-10 training set. Training is done on visual observations first, followed by training the networks on a small set of data. The model accurately predicts background images and controls the agent's actions. It disentangles static scene content from dynamic arm movements and consistently represents actions across different backgrounds. Trajectory transplantation between backgrounds further validates the learned latent representation. Our model achieves almost perfect accuracy in learning representations of actions disentangled from static content like background and agent appearance. Trajectory transplantation between different environments validates the learned latent representation. Additional videos can be found at: https://daniilidis-group.github.io/learned_action_spaces/. The method can learn from agents with varying visual appearances but a common action space, demonstrated by varying parameters of the reacher arm in the dataset. The reacher arm's visual characteristics, thickness, and length are evaluated in a dataset with 72 configurations. Training on a novel dataset shows comparable performance regardless of visual appearances. The model learns a latent representation capturing shared action space among agents. The model learns a latent representation capturing shared action space among agents, enabling trajectory transplantation between different agents. The stochastic video prediction model can capture the stochasticity of data by generating plausible future sequences. The model learns a latent representation capturing shared action space among agents, enabling trajectory transplantation between different agents. The stochastic video prediction model can capture the stochasticity of data by generating plausible future sequences. Only five of ten predicted frames are shown for clarity. Our method produces sequences perfectly aligned with the ground truth, while the baseline often produces images with two different robot arms and other artifacts. The baseline model's latent space structure visualization shows entangled action representations with the absolute position of the reacher arm. The model aims to capture shared action space among agents for trajectory transplantation. Additional generated videos can be viewed at the provided link. The baseline model's latent space structure reveals entangled action representations linked to the reacher arm's position. The representation is minimal, a linear transformation of the optimal actions, suggesting the importance of composability in learning a disentangled action representation."
}