{
    "title": "HyxGB2AcY7",
    "content": "This paper explores how learning contingency-awareness and controllable elements in an environment can improve exploration in reinforcement learning. An attentive dynamics model (ADM) is developed to identify controllable elements in observations, such as the character's location in Atari games. The ADM is trained to predict agent actions and the learned information is used for exploration. Combining actor-critic algorithm with count-based exploration using this representation achieves impressive results on challenging Atari games with sparse rewards, achieving a state-of-the-art score of >11,000 points on Montezuma's Revenge without expert guidance. The success of reinforcement learning algorithms in complex environments relies on balancing exploration and exploitation. Recent interest has focused on effective exploration strategies for high-dimensional state spaces and sparse rewards. Deep neural networks have been successful in RL as function approximators and representation learning tools. Our study achieved a state-of-the-art score of >11,000 points on Montezuma's Revenge without expert guidance, showcasing the power of contingency-awareness in tackling exploration challenges. Recent studies have explored using neural network representations for exploration in reinforcement learning, particularly in count-based exploration with neural density estimation. The challenge lies in constructing an optimal representation for exploration that goes beyond visual similarity, focusing on what is relevant to solving the Markov Decision Process (MDP). In exploring reinforcement learning, recent studies have focused on constructing optimal representations for Markov Decision Processes (MDP) beyond visual similarity. This work investigates learning a more intuitive and interpretable high-level abstraction through contingency awareness and controllable dynamics, emphasizing the agent's understanding of environmental dynamics and self-localization. Contingency awareness is crucial for intelligent organisms and systems, as it involves self-localization in the state space. This concept is important in neuroscience and psychology, as seen in studies on grid cells showing how organisms navigate and remember paths based on their location. Contingency awareness is essential for intelligent agents, involving self-localization and self-awareness. It is a powerful mechanism for exploration in reinforcement learning, as demonstrated in the context of 2D Atari games. For example, in FREEWAY, only the chicken sprite is controllable, highlighting the importance of understanding contingent regions. In this study, the focus is on exploring how contingency awareness can be learned without external annotations or supervision. An algorithm is introduced to automatically learn and utilize this information to enhance exploration in a 2D ALE environment. The use of an attentive dynamics model (ADM) helps predict the agent's actions and approximate its position in the environment without the need for additional supervision or hand-crafted features. This self-supervised learning approach does not require an environment simulator or supervision labels. Our methods improve A2C performance on hard-exploration Atari games without the need for hand-crafted features, supervision labels, or an environment simulator. We achieve state-of-the-art results on sparse-reward games like MONTEZUMA'S REVENGE by combining our exploration strategy with PPO. Our contributions highlight the importance of learning contingency awareness for efficient exploration in challenging RL problems. The novel instance of attentive dynamics model using contingency and controllable dynamics improves localization abilities in challenging Atari environments. Strong performance is achieved on sparse-reward games like MONTEZUMA'S REVENGE, confirming the power of contingency awareness in tackling exploration problems in reinforcement learning. Self-localization is emphasized, with the discovery of grid cells motivating research on self-aware agents for spatial navigation tasks. The presence of grid cells correlates with high performance. Grid cells are correlated with high performance in navigation and can be extended to abstract spaces. Various approaches to self-localization include SLAM and dynamics models for learning feature representations and intrinsic rewards. Learning representations through auxiliary tasks is also explored. Our approach focuses on discovering controllable aspects using an attention mechanism for better interpretability, different from other methods that use exploration bonuses or visitation density models. The approach focuses on discovering controllable aspects using an attention mechanism for better interpretability, unlike other methods that rely on exploration bonuses or visitation density models. Key feature information is extracted from observations for counting states, while exploration strategies in RL use intrinsic motivation to encourage the agent to search for novel states. An instance of attentive dynamics model is developed to predict actions based on consecutive input frames. The inverse dynamics model focuses on determining controllable regions in observations to classify actions. A spatial attention mechanism is used to identify relevant parts of the observation. Convolutional feature maps are computed from observations to estimate action logits for classification. The model utilizes a shared multi-layer perceptron to derive logits for action classification. The model utilizes a shared multi-layer perceptron to derive logits for action classification, with attention masks computed to indicate controllable parts of the observation. The attention probabilities are used to linearly combine logits from all regions for stable performance. Training involves optimizing with cross-entropy loss with respect to ground-truth actions taken by the agent. The attention probability \u03b1 t (i, j) should be high on regions predictive of the agent's actions. Learning to localize controllable entities in a self-supervised way without extra supervision is challenging due to unpredictable actions and shifting data distribution during online training. To address issues with data and action distribution shifts, an attention entropy regularization loss is used to encourage high entropy in attention distribution. Additionally, separate cross-entropy losses are applied to train logits for each grid cell independently. These measures aim to improve model adaptability and localization of controllable regions. The model uses attention entropy regularization and cross-entropy losses to improve adaptability and localization of controllable regions. It also incorporates count-based exploration and an exploration bonus to enhance learning from unseen observations. The model incorporates count-based exploration and an exploration bonus to maximize rewards and enhance learning from unseen observations. The procedure involves increasing counter values for encountered states during training. Key questions include the usefulness of contingency awareness for exploration and the performance of the proposed strategy compared to other methods. The proposed exploration strategy, A2C+CoEX, outperforms the A2C baseline on six out of 8 Atari games, achieving higher rewards and better policy convergence. The A2C+CoEX technique significantly outperforms the A2C baseline on six out of 8 games by incorporating a count-based exploration bonus and ground-truth location information from the game's RAM. The A2C algorithm uses 16 parallel actors with 5-step rollout to collect experience. Episodes end when the game ends, not when the agent loses a life. The ADM takes 160x160 observation frames as input and uses a 4-layer CNN to produce a feature map with a 9x9 grid size for location prediction. In sparse-reward problems, a discrete context representation is introduced to summarize the high-level visual context in which the agent currently lies. This representation clusters input frames using a simple clustering method, assigning different contexts to different clusters. The cumulative reward from the beginning of the episode is calculated to be instrumental for future states of the environment. The cumulative reward from the beginning of the episode can provide high-level behavioral context for the agent, guiding optimal behavior changes. For count-based exploration, including cumulative reward as part of state abstraction leads to better performance. For count-based exploration, utilizing the location and context of the agent in the current observation, along with cumulative environment reward, leads to better performance. The learning curves of proposed methods on Atari games are shown in FIG1, with results summarized in Table 1. Hyper-parameter search was conducted to balance environment reward and exploration bonus reward for A2C+CoEX+RAM and A2C+CoEX, yielding consistent results. The proposed exploration strategy improves scores on hard-exploration games compared to vanilla A2C. A2C+CoEX+RAM shows significant improvement by encouraging agents to visit novel locations. A2C+CoEX with attentive dynamics model and observation embedding clustering also outperforms A2C baseline, especially on specific games like FREEWAY and QBERT. The proposed method A2C+CoEX+RAM outperforms other count-based exploration methods on 5 out of 8 games. DQN-PixelCNN achieves state-of-the-art performance on difficult sparse-reward games. Comparison with A3C+ shows the effectiveness of A2C+CoEX. Performance of ADM in learning controllable dynamics is evaluated using the average distance between predicted and ground-truth agent locations in a grid. The ADM quickly captures the agent's location within a 9x9 grid in Atari games without supervision. The predicted location is usually 1 or 2 grid cells away from the ground-truth location. When encountering a novel scene, the average distance temporarily increases but then drops as the model learns. Videos of the agents playing and localization information are provided as supplementary material. In Atari games like MONTEZUMA'S REVENGE, VENTURE, HERO, and PRIVATEEYE, high-level visual context is used for exploration. Different visual contexts lead to distinguishable embeddings for accurate clustering. Evaluation compares the discrete representation to ground-truth room numbers. Adjusted Rand Index measures similarity, with a value between 0 and 1. The curves of the Adjusted Rand Index are shown in FIG4. The proposed exploration algorithm is evaluated on Atari games like MONTEZUMA'S REVENGE and VENTURE, where the discrete representation as room number is effective. However, for games like HERO and PRIVATEEYE, clustering embeddings is more challenging due to similar rooms. The algorithm is tested using sticky actions to prevent memorization, and trained with Proximal Policy Optimization (PPO) using 128 parallel actors for experience collection. The method PPO+CoEX achieves a score of 11,618 on MONTEZUMA'S REVENGE at 500M environment steps, showing improved performance compared to vanilla PPO. Another approach called \"Exploration by Random Network Distillation\" achieves similar results with a different philosophy. This paper explores the use of an attentive dynamics model (ADM) to discover controllable dynamics. The current approach explores the use of an attentive dynamics model (ADM) to improve exploration in challenging sparse-reward environments, achieving significant improvements on difficult video games. The method focuses on controllable dynamics and employs spatial attention and modelling of the dynamics. Future work could involve using ADM to learn an effective abstraction of both controllable and uncontrollable dynamics. Attention-based dynamics models (ADM) can be used in forward, inverse, or combined mode to generalize learning in different RL environments. The inverse dynamics model allows for some error and could be obtained through embedding learning techniques. Different visual characteristics in RL environments may require varying forms of attention-based techniques. The focus is on learning contingency-awareness in 2D video games using attention and dynamics models. The proposed method utilizes an attentive dynamics model (ADM) to provide contingency-awareness for an RL agent in 2D environments, enabling self-localization and achieving strong results in sparse-reward Atari games. State-of-the-art results of >11,000 points on MONTEZUMA'S REVENGE were reported without expert demonstrations or supervision. The approach is seen as applicable to more complex 3D environments in the future. The presented high-level concept involves actor-critic training using on-policy samples and an attentive dynamics model for contingency-awareness in RL agents. The approach aims to achieve strong results in sparse-reward Atari games and is considered a stepping stone towards universal algorithms for various RL environments. In the exploration process, the agent's location and room number are extracted from RAM for count-based exploration. The learning curves in FIG7 demonstrate that A2C+CoEX+RAM sets the upper bound performance. A method to obtain observation embedding is detailed, where observations are flattened and projected to a 64-dimensional embedding. Observations are clustered based on a threshold value \u03c4, with specific values listed in TAB6 for different games with room changes. If the distance between the current embedding and the cluster mean is less than \u03c4, the embedding is assigned to that cluster. In the exploration process, observation embeddings are clustered based on a threshold value \u03c4. If the distance between the current embedding and the cluster mean is less than \u03c4, the embedding is assigned to that cluster. New clusters are created if the distance exceeds the threshold. In the exploration process, observation embeddings are clustered based on a threshold value \u03c4. If the distance between the current embedding and the cluster mean is less than \u03c4, the embedding is assigned to that cluster. New clusters are created if the distance exceeds the threshold. Clustering results for VENTURE, HERO, PRIVATEEYE, and MONTEZUMA'S REVENGE show observations from the same room are assigned to the same cluster. A simple ablation study on the learning objectives of ADM is conducted, evaluating its performance under different combinations of loss terms. In comparing different training methods for ADM, including variations with different loss terms, the average distance between ground-truth and predicted locations is shown. Results on MONTEZUMA'S REVENGE show little difference between variants, while on FREEWAY, additional loss terms help improve performance by reducing suboptimal behavior. In an ablation study, the A2C+CoEX agent with the ADM showed improved performance on MONTEZUMA'S REVENGE and FREEWAY games. Different loss objectives were compared, with the full ADM variant working best. The minimal training objective of ADM also performed reasonably well, but combining it with other loss terms led to more stable performance. Results of the ablation study on state representation were summarized, showing the maximum mean score achieved over 100M environment steps. In an additional ablation study, different baselines for count-based exploration using state representation \u03c8(s) were considered. Variants include using context embedding only, context embedding with cumulative reward, and contingent region information with context embedding. A baseline with random coordinates instead of contingent region information performed similarly to the context embedding with cumulative reward baseline. Experimental results are summarized in TAB7 and FIG0. The experimental results in TAB7 and FIG0 show that variants without contingent regions performed significantly worse in most games compared to variants with contingent region information. The variants with contingent region information worked best and were comparable to each other, showing the effectiveness of learned contingency-awareness information in count-based exploration."
}