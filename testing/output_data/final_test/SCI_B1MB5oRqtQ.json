{
    "title": "B1MB5oRqtQ",
    "content": "Building upon recent success in deep reinforcement learning, this work proposes adapting the replay buffer concept to on-policy algorithms. The method generalizes Q-value and advantage functions for data from multiple policies, using trust region optimization and trainable covariance matrix to improve results compared to existing algorithms like TRPO or ACKTR. The recent development in reinforcement learning has seen advancements in deep learning techniques, particularly in solving vision-based control tasks like Atari 2600 games and continuous control problems. Many state-of-the-art methods utilize the actor-critic architecture, separating the actor for policy and the critic for approximating rewards. A new method proposes adapting the replay buffer concept to on-policy algorithms, improving results compared to existing algorithms like TRPO or ACKTR. Reinforcement learning algorithms can be categorized into on-policy and off-policy approaches. On-policy methods evaluate the target policy with the exploration strategy incorporated into the policy itself, while off-policy methods separate the exploration strategy from the target policy. Off-policy methods commonly use replay buffers to store outcomes for learning. Off-policy methods in reinforcement learning use replay buffers to store previous policy outcomes for learning. Deep Q-Networks (DQN) combined with experience replay have shown end-to-end learning on Atari 2600 games. An extension of DQN, Deep Deterministic Policy Gradient (DDPG), handles continuous action spaces by introducing exponential smoothing of target actor and critic weights for stability. Generalised Advantage Function is proposed to improve policy gradient variance, and Asynchronous Advantage Actor Critic model (A3C) parallelizes exploration using differently trained actors. State-of-the-art on-policy methods like TRPO aim to evaluate policies without separate exploration strategies, but suffer from poor sample efficiency compared to off-policy methods. TRPO introduces trust region policy optimization to control policy evolution speed over time. The TRPO method aims to control policy evolution speed using Kullback-Leibler divergence. However, it lacks sample efficiency compared to off-policy methods like DDPG. Solutions include using second-order natural gradient methods and modifying objective functions in methods like PPO and ACER. Recently, methods like interpolated policy gradient and off-policy trust region method Trust-PCL have emerged to combine benefits of both on-policy and off-policy reinforcement learning approaches. These methods aim to optimize stability and efficiency by utilizing a weighted sum of stochastic and deterministic policy gradients, as well as exploiting off-policy data within trust regions optimization framework. Unlike traditional on-policy methods, these approaches do not discard all policies except the last one, leading to improved performance in reinforcement learning tasks. The curr_chunk discusses a novel reinforcement learning algorithm that combines replay buffers with trust region optimization for improved sample efficiency. It highlights the method's ability to use on-policy data with replay buffers and the benefits of using a single set of hyperparameters for better performance compared to state-of-the-art methods. The paper aims to ensure repeatability of experiments and acceptance by the community. The paper introduces a novel reinforcement learning algorithm that combines replay buffers with trust region optimization for improved sample efficiency. It emphasizes using on-policy data with replay buffers and the benefits of a single set of hyperparameters for better performance. The authors aim to ensure experiment repeatability and community acceptance by releasing their source code. The discounted return BID3 is defined as per BID19. The advantage function A \u03c0, value function V \u03c0, and Q-function Q \u03c0 are defined accordingly. Learning a policy through unconstrained maximization can lead to unpredictable changes. The algorithm's convergence rate and complexity are important considerations. The algorithm's convergence rate and complexity make direct application impractical. Trust Region Policy Optimisation (TRPO) replaces unconstrained optimisation with constrained optimisation. Many state-of-the-art methods use second order natural gradient based actor-critic optimisation to address parametrisation dependency. The Fisher information matrix is used for actor-critic optimization, but its computation is intractable due to the large number of parameters. Approximations like the K-FAC method are used to optimize the policy. In this section, a new concept of replay buffers is proposed to combine on-policy data with data from previous policies, aiming to overcome restrictions on policy distribution stationarity for stochastic policy gradient methods. This approach differs from previous methods that only use data from the current policy iteration. The proposed algorithm introduces replay buffers to combine data from multiple policies, overcoming restrictions on policy distribution stationarity for stochastic policy gradient methods. Generalized Q-function, value function, and advantage function are defined for multiple policies, facilitating the formalization of the algorithm and theorems. The probability of transition between states using a policy is also considered in the algorithm. The gradient for a set of policies is determined by a specific equality. The proposed algorithm introduces replay buffers to combine data from multiple policies, overcoming restrictions on policy distribution stationarity for stochastic policy gradient methods. The gradient for a set of policies is determined by a specific equality, where joint parameters \u03b8 and bias functions are used. The algorithm involves updating value function estimator parameters and estimating advantage functions for each path in the replay buffer. Parameters of the policy are then updated for a specified number of iterations. During the algorithm, data is collected for each path until the termination state is reached, with policy actions sampled from a Gaussian distribution. The obtained data is saved in a policy replay buffer, and the value function is trained using Adam optimizer with a sum-of-squares loss function. Target values for the value function are computed for every state. During Stage 4, the Generalised Advantage Estimator is used to estimate the advantage function A \u03c0 (s t , a t ) with defined parameters k, \u03bb, and \u1e7c \u03c0 (s t ). This results in the generalised advantage function estimator. The proposed advantage function estimator is similar to BID20's approach. The proposed advantage function estimator modifies the estimator to avoid estimating multiple value functions, diminishing the replay buffer idea. The difference between the estimators is dependent on the difference in conventional and generalised value functions, showing that closer policies result in smaller bias. The policy function is approximated using the K-FAC optimiser with a constant step size, without adapting trust region radius or optimisation algorithm parameters like ACKTR. The output parameters include the diagonal of the policy covariance matrix, with elements restricted to MIN COV EL and MAX COV EL values for efficient optimization. Policy gradient estimation is defined with the substitution of likelihood ratio, using parameters derived from the latest policy for the replay buffer. Linear barrier function is added for constrained optimization in the function \u03c1(\u03b8). The linear barrier function is added for constrained optimization in the function \u03c1(\u03b8), with \u03b1 > 0 as a parameter. The approach aligns with theoretical prepositions and uses actual constrained optimization methods. The network architecture is based on OpenAI Baselines ACKTR implementation, with the only difference being the addition of diagonal covariance matrix outputs in the policy network. Experimental evidence compares this method with on-policy ACKTR, PPO, and other approaches. The method introduces a fixed value for trust region radius and a trainable policy distribution covariance matrix, reducing hyperparameters. Results for ACKTR on tasks HumanoidStandup, Striker, and Thrower are not included due to divergence. PPO results are from baselines implementation PPO1. Figure 2 compares results for different replay buffer sizes. The study compares performance with different replay buffer sizes, showing improvements in most cases compared to no replay buffer. The proposed method outperforms DDPG in various tasks, with stable performance in tasks where DDPG failed. The paper introduces a new approach combining replay buffers and on-policy data for reinforcement learning, showing significant improvements on tasks from the MuJoCo suite BID25. It proposes a fixed hyperparameter for trust region parameters and a trainable diagonal covariance matrix, paving the way for using replay buffers and trust regions in reinforcement learning. The approach, initially designed for continuous tasks, can also be applied to discrete reinforcement learning tasks like ATARI games. The method introduced combines replay buffers and on-policy data for reinforcement learning, with fixed trust region parameters and a trainable diagonal covariance matrix. The K-FAC optimizer parameters were used for experiments, while default parameters from Tensorflow were used for the Adam algorithm. The neural network architecture consists of two fully connected layers with 64 neurons each. The derivation and difference between estimators are discussed in the context of the GAE. The derivation and difference between estimators in the context of the GAE is given by \u2206\u00c3 \u03c0n (s t , a t ) = (1 \u2212 \u03bb)(\u03b3\u2206V 1 + \u03bb\u03b3 2 \u2206V 2 + \u03bb 2 \u03b3 3 \u2206V 3 + . . .)"
}