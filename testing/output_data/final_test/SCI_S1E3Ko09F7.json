{
    "title": "S1E3Ko09F7",
    "content": "Instancewise feature scoring is a method for model interpretation that provides importance scores for each test instance's features. Methods based on the Shapley score are fair but computationally complex, especially for large datasets and complex models. Two algorithms with linear complexity have been developed for instancewise feature importance scoring on black-box models in graph-structured data settings. The methods are related to the Shapley value and Myerson value from cooperative game theory and have been demonstrated on language and image data. In this paper, instancewise feature importance scoring is studied as a method for interpreting predictions of black-box models. The algorithms compare favorably with other methods in terms of accuracy and human evaluation, particularly in language and image data. Interpretability is crucial in fields like medicine, finance, and criminal justice when using black box models. The method yields importance scores for each instance, aiding in understanding model predictions. The importance scores associated with features can vary across instances, providing explanations for specific predictions. Research has focused on scoring input features for model interpretation, with recent work using Shapley value from game theory. The Shapley value, originating from cooperative game theory in 1953, is used to distribute total surplus fairly among players in predictive models. Despite its conceptual appeal, computing Shapley values is computationally challenging. Various methods, such as Monte Carlo approximation and sampled least-squares with weights, have been proposed to address this complexity. This paper suggests approaching the problem of explanation within a model-based paradigm, where explanations are framed in terms of a model. In a model-based paradigm, explanations are framed using the Shapley value for fair distribution of total surplus among players in predictive models. Two methods, L-Shapley and C-Shapley, are proposed for instancewise feature importance scoring in a graph structure framework. These methods reduce the number of model evaluations linearly in the number of features. The relationship between feature scoring measures and the C-Shapley value is demonstrated, along with its connection to the Myerson value in cooperative game theory. Feature scoring methods are applied to various models for language and image data, showing favorable comparisons to existing algorithms for feature importance scoring. The study focuses on models trained for prediction using input feature vectors and output variables. The importance score v x (S) is introduced for predicting a response variable y \u2208 Y based on a feature vector x \u2208 X. It represents the negative expected bits needed to encode the model output, with a coding-theoretic interpretation. The score is calculated using a conditional distribution P m (\u00b7|x) and subset S of input features x S. The importance score v x (S) is a measure of how important a feature subset S is to the predicted class, based on the model's output distribution. It is calculated using a conditional distribution and subset of input features. The score is related to the mutual information between the feature subset and the predicted class. The importance score v x (S) measures the significance of a feature subset S in predicting the class, using a modified score based on the expected log probability of the predicted class given the features in S. Estimating the conditional distribution involves estimating the conditional probability functions P m (y | x S) using empirical averages or plug-in estimation methods. The plug-in estimation method is used to estimate the conditional probability functions P m (y | x S) by sampling feature vectors and defining new vectors based on a reference vector. This method is computationally efficient and works well with appropriate reference points, such as using padding index for language data and average pixel strength for vision data. The importance of feature interactions in quantifying feature importance for vision data is highlighted. An example of sentiment analysis on a movie review demonstrates the significance of considering feature interactions. The word \"not\" is emphasized as an important feature for predicting negative sentiment, showcasing the importance of looking beyond individual features. The Shapley value is a way to measure the interaction of a feature with other features in a subset. It calculates the marginal contribution of a feature to a subset by comparing the importance of all features with and without it. This method provides a scalar measure for feature importance by aggregating these marginal contributions over all subsets containing the feature. The Shapley value calculates the marginal contribution of a feature to a subset by comparing the importance of all features with and without it. It provides a scalar measure for feature importance by aggregating these contributions over all subsets containing the feature. The Shapley value calculates feature importance by comparing the marginal contribution of a feature to subsets. Various approximation methods have been developed to reduce computational complexity, such as Monte Carlo approximation and weighted linear regression. Sampling-based approximations are commonly used in practice. The main contribution of this paper is to address the challenge of high variance in sampling-based approximations for large-scale predictive models with a large number of features. The proposed approach in a model-based paradigm considers the contribution of features to the response variable respecting the structure of an underlying graph. This method can be complementary to sampling-based or regression-based approximations of the Shapley value. The paper proposes a method to speed up the computation of L-Shapley and C-Shapley values by combining different methods. Features in applications can be associated with graph nodes, where distances between features are based on the graph structure. Distant features in the graph have weak interactions and may have little effect on Shapley value computation. Modified forms of Shapley values, such as L-Shapley, are introduced in this section. The paper introduces modified forms of Shapley values, called L-Shapley and C-Shapley values, which can be computed more efficiently by excluding weak interactions in structured data. These values provide good approximations to the original Shapley values under certain probabilistic assumptions. Features are associated with graph nodes, where distances between features are based on the graph structure. The proposed algorithms aim to approximate Shapley values by considering the graph structure of features. The L-Shapley score perturbs neighboring features to evaluate importance, focusing on nodes closer in distance on the graph. The L-Shapley estimate of order k on a graph perturbs neighboring features to evaluate importance, controlling error under certain assumptions. The choice of k is based on computational considerations, with a proposed C-Shapley algorithm to reduce complexity. In an example, evaluating the importance of \"not\" may lead to evaluating the model on a rare word subset. C-Shapley is a proposed algorithm that estimates the importance of features in a model by using a new coalitional game approach. It involves evaluating subsets of features connected in a graph to determine their influence. The error between C-Shapley and the Shapley value can be controlled under certain statistical assumptions. For text data, C-Shapley is equivalent to evaluating n-grams. In Section 4, the L-Shapley and C-Shapley values are studied, showing their relationship to Shapley values and Myerson values in cooperative game theory. Absolute mutual information is introduced as a measure of dependence between features. Absolute mutual information is defined as a measure of dependence between random variables X and Y. It is more stringent than mutual information, with the absolute conditional mutual information also being defined. Theorems 1 and 2 relate L-Shapley and C-Shapley values to the Shapley value in models obeying a Markovian structure. Theorem 1 states that the expected error between the L-Shapley estimate and the true Shapley-value-based importance score is bounded by 4\u03b5. Theorem 2 discusses the expected error between the C-Shapley estimate and the true Shapley-value-based importance score, bounded by 6\u03b5. It also explores the relationship between the C-Shapley value and the Myerson value in coalitional games. The Myerson value, introduced by Myerson (1977), characterizes a coalitional game over a graph G using a score function that satisfies decomposability. The Myerson value is the unique quantity that satisfies decomposability, additivity, equal contributions, and monotonicity properties. The decomposability condition in our setting is equivalent to assuming that the influence of disconnected feature subsets is additive at sample x. C-Shapley of order k = d is the Myerson value over G. Partitioning subsets S into connected components, the Myerson value is equivalent to equation 6. Methods for approximating the Shapley value can speed up the evaluation of L-Shapley and C-Shapley values. Monte Carlo approximation can be used for L-Shapley, combined with sampling-based methods for feature interaction in a large neighborhood N k (i). Regression-based methods like KernelSHAP can be used to sample feature subsets and estimate Shapley values, reducing computational complexity. This approach can be combined with modified Shapley values to further simplify the evaluation process for C-Shapley values on different graph structures. We evaluate L-Shapley and C-Shapley on real-world data sets for text and image classification, comparing them with KernelSHAP, SampleShapley, and LIME for feature importance scoring on black-box models. Our focus is on model-agnostic interpretation, omitting methods specific to certain model classes. The objective is the log probability of the predicted class for all methods. Text classification is a common task in natural language processing, where documents are categorized into predefined groups. L-Shapley and C-Shapley are evaluated on various neural models for text classification, including word-based CNNs, character-based CNNs, and LSTM recurrent neural networks. The experiments involve different datasets, with details provided in tables. Each word's importance score is highlighted using RGB colors in the analysis. The importance of each word is highlighted with RGB colors based on its score. A bidirectional LSTM model achieves 70.84% accuracy on the Yahoo! Answers Topic Classification Dataset. Different models are evaluated with zero paddings as the reference point. L-Shapley and C-Shapley consider word interactions in text classification tasks. The study evaluates the performance of different model interpretation algorithms on various datasets. Results show that L-Shapley and C-Shapley outperform other algorithms on different datasets such as IMDB, AG's news, and Yahoo! Answers. The evaluation metric involves masking top features and analyzing the change in log-odds scores. C-Shapley outperforms other algorithms on Yahoo! Answers with LSTM, followed by LIME. L-Shapley, SampleShapley, and KernelSHAP do not perform well for LSTM model due to long n-grams. Importance scores visualized on Example ( ) from IMDB. More visualizations in Appendices. Each pixel as a single feature for MNIST and CIFAR10. Reference points chosen for all methods. LIME and L-Shapley not used for comparison. C-Shapley outperforms other methods on MNIST and CIFAR10 datasets by evaluating all image patches with n \u2264 4. The decrease in log-odds scores is shown as top pixels are masked, with C-Shapley consistently performing better. The model's reasoning is visualized by important pixels chosen by C-Shapley, which align with digits from the opposite class. Results of human evaluation show that C-Shapley outperforms other methods in producing interpretable results on MNIST and CIFAR10 datasets. The visualization of important pixels chosen by C-Shapley align with digits from the opposite class and distinguish between deers and horses in CIFAR10. More visualization results are available in the Appendix. The study uses human annotators on Amazon Mechanical Turk to compare L-Shapley, C-Shapley, and KernelSHAP on IMDB movie reviews. They aim to determine if humans can make decisions based on top words alone and if masking the top words affects decision-making. 200 movie reviews are sampled and assigned to five annotators each, who classify the sentiment into five categories. Texts include raw reviews, top ten words ranked by different methods, and reviews with masked top words. The study compares L-Shapley, C-Shapley, and KernelSHAP on IMDB movie reviews using human annotators on Amazon Mechanical Turk. They aim to determine if humans can make decisions based on top words alone and if masking the top words affects decision-making. Reviews are categorized based on the consistency between true labels and human annotators' labels, with an average of 14% words masked for L-Shapley and C-Shapley, and 31.6% for KernelSHAP. The study compared L-Shapley, C-Shapley, and KernelSHAP on IMDB movie reviews using human annotators. Results showed that humans were more consistent and confident when presented with top words. C-Shapley performed the best in terms of consistency, agreement, and confidence. However, masking top words led to more mistakes and uncertainty in human judgment. The experiments demonstrated that key words convey attitudes towards a movie and the algorithms accurately identify key words. We proposed L-Shapley and C-Shapley algorithms for instancewise feature importance scoring, outperforming other methods on black-box models for text and image classification. The datasets used include IMDB movie reviews and AG news corpus with Char-CNN, segmented into four classes. Yahoo! Answers corpus was also utilized with LSTM. The Yahoo! Answers Topic Classification Dataset has ten categories with 140,000 training samples and 5,000 testing samples. The MNIST dataset contains images of handwritten digits with 12,000 training images and 1,000 testing images. A subset of the CIFAR10 dataset with deers and horses has 10,000 training images and 2,000 testing images. A CNN model trained on MNIST achieves 99.7% accuracy on the test set. The CNN model trained on a subset of the CIFAR10 dataset achieves 96.1% accuracy on the test data set. It consists of two convolutional layers with 8 and 16 filters, followed by a max-pooling layer. Another CNN model modified from AlexNet achieves 96.1% accuracy on the test data set with six convolutional layers and two dense linear layers. Lemma 1 states a combinatorial equality for positive integers, proving it using the binomial theorem. The theorem is then proven by splitting the analysis into two cases, extending the L-Shapley estimate for feature subsets. The Shapley value is defined for subsets A, partitioned based on U S (A). The expected error between \u03c6 S X (i) and \u03c6 X (i) can be upper bounded by a certain equation, involving the size of V S (A). By applying Lemma 1, the expected error can be further simplified and bounded. The expected error is upper bounded by a certain equation involving the size of subsets partitioned based on U S (A). By applying Lemma 1, the error can be further simplified and bounded. The text discusses the expected error in partitioning subsets based on U S (A) and V S (A). By applying Lemma 1, the error can be bounded. The study focuses on the correlation between feature ranks from different algorithms and true Shapley values using Kendall's \u03c4 and Spearman's \u03c1 metrics. The study examines the correlation between different algorithms' feature ranks and true Shapley values using Kendall's \u03c4 and Spearman's \u03c1 metrics. Results show that L-Shapley has the highest rank correlation with the Shapley value, followed by C-Shapley. The proposed algorithms show a slight performance gain over KernelSHAP and SampleShapley, with bias decreasing as neighborhood radius increases. The complexity of L-Shapley grows exponentially with neighborhood radius, while C-Shapley remains consistent. The experiment evaluates the variance of SampleShapley and KernelSHAP in the setting where the sample size is linear in the number of features. The study seeks a nonparametric approach to measure the variability of sampling-based algorithms by colorizing the words with the largest and smallest scores based on the predicted class. Red words indicate positive attitude for a positive prediction, while blue words indicate negative attitude. The experiment evaluates the variance of SampleShapley and KernelSHAP in the setting where the sample size is linear in the number of features. Red words indicate positive attitude for a positive prediction, while blue words indicate negative attitude. The corresponding RGB entries are linearly interpolated based on importance scores. The lighter a color is, the less information with respect to the prediction the corresponding word is. Positive reviews praise the performance, production design, costumes, and writing. Negative reviews criticize the acting, premise, and director. The director is defending the movie, which was previously unknown before appearing on Cable TV. The actors, who were in House Party 3 as children, are now grown up. Despite being shocked by some scenes, the director Chris Stokes did a decent job for his first movie. The words with the largest and smallest scores are colorized based on importance. Global warming is causing longer summers and less freezing winters, potentially leading to flooding in Wales. The United States Congress is spending money on various unnecessary projects, such as seafood waste research and the Arctic Winter Games. There are opportunities to make cuts in spending to secure borders. The curr_chunk discusses the need to make cuts in spending to secure borders, the issue of language use among Filipinos, raising money for bail, and the role of the ACLU in promoting free speech while maintaining decency in society. The curr_chunk discusses the importance of decency in society and criticizes the shift towards extreme liberalism in politics. It also briefly mentions the various roles of a holder of important jobs such as head priest, chief academic officer, local government officer, and military police officer. The curr_chunk discusses a prison entertainment, music, and a band registering to play in 2006."
}