{
    "title": "HJg8stY2oB",
    "content": "Many models based on the Variational Autoencoder aim to achieve disentangled latent variables. A pruning mechanism is introduced to automatically determine the intrinsic dimension of the data, improving disentanglement. The method is validated on MPI3D and MNIST datasets, showing advancements in disentanglement, reconstruction, and robustness. The code is available on GitHub for further exploration. In this paper, an orthogonal mechanism is introduced to enhance disentanglement and robustness in state-of-the-art models, particularly focusing on the dimensionality of the latent representation. Excessive or deficient latent dimensions can hinder achieving optimal disentangled representations. An approximated L0 regularization is proposed to prune the latent representation vector, leading to the development of Pruning Variational Autoencoders (PVAE) framework. In this challenge, the pruning mechanism is applied to the DIP-VAE model for seeking the intrinsic dimension of the latent representation. The goal is to compress the network and achieve a balance between terms to control each dimension using binary masks. The DIP-VAE model applies a pruning mechanism to find the latent dimensionality. The KL divergence terms decompose across dimensions due to factorized distributions. L0 regularization is used with samples drawn from the variational posterior. The total loss includes a term for the probability of positive values and bounds for a stretched range. The DIP-VAE model applies a pruning mechanism to find latent dimensionality using L0 regularization and factorized distributions. The mask vector m is clamped to [0, 1] and modelled with Bernoulli distributions. Masks are obtained through a sigmoid-like function with stretched range [\u03b6, \u03b3]. The encoder outputs means and variances of q(z|x) for alignment with VAE. The L0 regularization in the pruning mechanism improves the performance and robustness of vanilla DIP-VAE by adjusting the means and variances of q(z|x) to switch off dimensions during training. The KL divergence is adjusted to prevent numerical instability, and hyperparameters are based on default settings. The proposed PVAE introduces a pruning mechanism that enhances disentanglement benefits on MPI3D and MNIST datasets. The L0 regularization improves model robustness and captures better-disentangled representations with optimal size. The model outperforms other state-of-the-art methods in terms of disentanglement and reconstruction, even approaching the intrinsic dimension for various datasets. The PVAE outperforms other methods in disentanglement and reconstruction. Default parameters are in Table 1. Adam optimizer with a learning rate of 10^-4 is used. The model generalizes well on MNIST and CelebA. A Pruning Joint VAE (PJVAE) is adopted to capture discrete features. Pruning shows advantages on MNIST, especially with deviating initialization. PJVAE is robust to initialization with enough latent space. The Pruning Joint VAE (PJVAE) outperforms DIP-VAE in disentanglement and reconstruction on the MNIST dataset. PJVAE shows robustness to different initializations and has advantages in Total Correlation (TC). The model is initialized with an additional 10-value categorical variable for fair comparison. PJVAE also exhibits lower reconstruction error compared to VAE and DIP-VAE."
}