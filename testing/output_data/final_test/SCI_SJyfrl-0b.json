{
    "title": "SJyfrl-0b",
    "content": "Representation learning in Deep Learning has led to significant advancements in tasks like Neural Machine Translation, Question Answering, and Speech Recognition. New methods for learning node and edge representations in graphs have been proposed, with some based on the SkipGram algorithm. A novel method for generating node embeddings in graphs using a limited number of permutations over a node's immediate neighborhood is introduced in this paper. This approach, known as ego-centric representations, outperforms state-of-the-art methods in link prediction and node classification tasks across six different datasets. The field of Natural Language Processing (NLP) has seen advancements through algorithms that learn word representations, such as word embeddings used in neural machine translation. These embeddings are faster than baselines for generating node embeddings in large graphs, improving predictions and inferences on nodes and edges. In this paper, a new method called Neighborhood Based Node Embeddings (NBNE) is proposed for generating node embeddings in large graphs. NBNE, based on the SkipGram algorithm, outperforms DeepWalk and Node2Vec for link prediction and node classification tasks, being significantly faster. The study introduces Neighborhood Based Node Embeddings (NBNE) as a new method for generating node embeddings in large graphs, outperforming DeepWalk and Node2Vec in link prediction and node classification tasks. The focus on the \"predictable\" parts of the graph improves effectiveness and efficiency, addressing challenges in learning node embeddings in highly connected graphs. Many definitions of similarity in graphs involve first and second order proximity. First-order proximity suggests connected nodes should have similar properties, while second-order proximity indicates nodes with similar neighborhoods should share common characteristics. Earlier works used matrix representations and dimensionality reduction techniques to obtain node embeddings, but faced challenges with large graphs. Recent techniques dynamically learn node representations based on first and second order proximities. Recent works focus on using non-linear techniques to find representations for specific types of graphs. TriDNR utilizes a graph structure with node content and labels, TEKE and KR-EAR find representations for entities in knowledge graphs, metapath2vec finds node representations in heterogeneous networks, and LINE finds node representations based on first and second-order graph proximities. Structural Deep Network Embedding (SDNE) is another method for node representation. The method DeepWalk generates random walks on each vertex in a graph to create sentences for training with the SkipGram algorithm, resulting in node embeddings with a time complexity of O(|V| log |V|). Node2Vec also uses random walks with SkipGram for node embeddings. Node2Vec BID13 is a method that uses random walks biased by parameters p and q to create node embeddings. It is not efficient for densely connected graphs due to its time and memory dependencies on the graph's branching factor. In comparison to DeepWalk, Node2Vec BID13 requires a semi-supervised approach with labeled nodes to choose the best parameters. NBNE is more effective than Node2Vec and DeepWalk, using a different sentence sampling strategy based on a node's neighborhood. It is efficient for both dense and sparse graphs, with faster training time. The context of a word in graphs is complex, with NBNE focusing on second-order proximities in nodes' neighborhoods for representation. In NBNE, nodes' representations are defined by their neighborhoods, with similar neighborhoods leading to similar representations. The method creates sentences based on node neighborhoods, facing challenges like sentences with all neighbors from a highly connected node being less useful. To address this, small sentences with k neighbors each are formed using random permutations of neighborhoods. Algorithm 1 presents the code for generating sentences based on node neighborhoods, with the user able to select the number of permutations n to balance training time and dataset size. The algorithm forms a set of sentences where each word represents a node from the graph, training vector representations by maximizing the log probability of predicting a node given another node in a sentence. The algorithm generates sentences based on node neighborhoods, training vector representations by maximizing the log probability of predicting a neighbor given a node. It uses a window of size k equal to the sentence size, with probabilities learned using feature vectors and optimized through stochastic gradient ascent with negative sampling. The algorithm creates node embeddings by maximizing predictability of neighbors, resulting in similar representations for nodes with similar neighborhoods. Adjusting parameter k balances first and second order proximity in the SkipGram algorithm. Overfitting can occur with large n values on graphs with few edges per node, but can be mitigated by sequential training. The algorithm creates node embeddings by maximizing predictability of neighbors, resulting in similar representations for nodes with similar neighborhoods. To avoid overfitting, training on increasing values of n and testing on a validation set is recommended. The best value of n is chosen based on precision on the validation set, with early stopping implemented. DeepWalk and Node2Vec are used as baselines for link prediction and node classification tasks. Node2Vec and DeepWalk were used as baselines for link prediction tasks, trained and tested alongside NBNE. Parameters were set as proposed in BID13, with a window size of 10, walk length of 80, and 10 runs per node. Node2Vec was tuned on validation set with grid search. NBNE was evaluated on synthetic graphs and author name disambiguation task. Comparison with SDNE can be found in Appendix D. Link prediction estimates link likelihood based on node attributes and observed links using similarity metrics like Common Neighbors. The study proposed training a logistic classifier based on node embeddings for link prediction tasks. A sub-graph with randomly selected edges was used to obtain node embeddings, which were then used to train a logistic regression model. The model was evaluated using a test set to predict new links, and 10-fold cross-validation was performed to assess the results. The logistic regressions were trained and tested using available edges and negative samples. Results showed NBNE outperforming DeepWalk and Node2Vec on various datasets in terms of AUC scores for Link Prediction tasks. In DBLP, NBNE achieved the best AUC score, with significant improvements in training time compared to Node2Vec and DeepWalk. The dataset analyzed contained a large graph with over 300,000 nodes and 1 million edges. NBNE showed the biggest improvements on sparser networks of medium size, such as Astro, PPI, and Wikipedia datasets, with training times significantly faster than Node2Vec and DeepWalk. Node classification task involves inferring the classification of unknown nodes in a partially labeled graph using node embeddings trained with NBNE. Logistic classifier was trained on 80% labeled nodes, validated on 5%, and tested on 15%. Results were statistically validated with 10 random seed initializations. Best results were achieved with n = 5 for Blog and Facebook datasets, and n = 10 for DBLP dataset, significantly faster than DeepWalk and Node2Vec. NBNE's embeddings were evaluated on Blog, PPI, and Wikipedia datasets using 10 random seed initializations. Results in terms of Macro F1 scores and training times are shown in TAB2. NBNE showed statistically similar results to baselines on PPI and Wikipedia, with a significant 22.45% gain on the Blog dataset. It was more than 10 times faster than DeepWalk and [300 \u223c 600] times faster than Node2Vec. NBNE did not show statistically worse results on any dataset and had significantly faster training times compared to DeepWalk and Node2Vec. NBNE's embeddings offer significantly faster training times compared to Node2Vec, with the quality depending on embedding size (d) and number of permutations (n). Larger n values (n > 100) improve results for highly connected graphs, while smaller n values (n = 1) decrease results. AUC scores versus embedding sizes on the Facebook link prediction task are shown in FIG0, with n = 10 providing robust values. TAB3 indicates that graphs with more edges per node benefit from larger n values. The number of permutations (n) plays a crucial role in the performance of algorithms like NBNE, Node2Vec, and DeepWalk. Larger n values are beneficial for graphs with more edges per node, while smaller n values are suitable for graphs with a smaller branching factor. Training time is linear with embedding size and number of permutations, with Node2Vec being slower than DeepWalk. NBNE with n < 10 is the fastest algorithm, while NBNE, Node2Vec, and DeepWalk have a time complexity of O(|V | log |V |). The proposed node embedding method NBNE has a time complexity of O(|V | log |V |) and outperforms Node2Vec and DeepWalk in various datasets. It focuses on learning node's immediate neighbors, making it efficient and easy to compute for large graphs. NBNE is a node embedding method with a time complexity of O(|V | log |V |) that outperforms DeepWalk and Node2Vec in terms of training time. It focuses on learning immediate neighbors of nodes, leading to faster learning and more stable representations. Six graph datasets were used to evaluate NBNE, with Facebook and Astro BID15 being among them. In the Astrophysics category in Arxiv, authors collaborate on papers. The Protein-Protein Interactions (PPI) network for Homo Sapiens is analyzed for assortativity properties. The study includes quantitative and qualitative analyses on homophily and assortativity based on different parameters. The analysis focuses on the trade-off between first and second order proximities based on the choice of k. Different ways to quantify homophily in a graph are discussed, including relational auto-correlation and dyadicity/heterophilicity. The assortativity properties of six graphs are presented, showing a broad range of characteristics. Notably, PPI, Wikipedia, and Blog graphs exhibit negative degree assortativity, indicating a tendency for nodes to connect with those of different connectivity degrees. In the analysis, different graphs show varying assortativity properties. Facebook, Astro, and DBLP have positive degree assortativity, while PPI and Blog datasets exhibit positive label assortativity. Wikipedia, on the other hand, shows negative label assortativity. The influence of permutations on homophily and overfitting in learned representations is further examined. The PPI dataset box plots in FIG4 demonstrate how embeddings overfit to specific graph structures for larger values of n, with removed edges showing a distribution more similar to non-edges than to training edges. The cosine distances for Facebook nodes show a larger separation for n = 5, justifying the algorithm's choice. For DBLP, the largest separation is seen for n = 10 in the cosine distances. The box plots in FIG4 show cosine distances for the Blog dataset, revealing a larger separation for n = 10 due to negative degree homophily. Similar trends are observed in the PPI and Wikipedia datasets, with varying intensity. The choice of n depends on the graph size and structure, as seen with n = 10 for the DBLP dataset with the largest degree assortativity. The choice of n for link prediction depends on graph size and structure, with a preference for a semi-supervised approach. The experiment on the PPI dataset with n = 1 shows the distribution of removed edges is closer to training edges than non-edges. The window size and number of neighbors are controlled by a variable k, balancing first and second order proximities in node embeddings. When choosing a smaller k, node embeddings focus on predicting their own neighbors, leading to closer representations for nodes with shared neighbors (second order proximity). In contrast, larger values of k result in nodes predicting not only their neighbors but also their neighbors' neighbors, increasing first order similarity. This is further analyzed by examining cosine distances between nodes at different graph distances using synthetic graphs like Barab\u00e1si-Albert, Erd\u00f5s-R\u00e9nyi, and Watts-Strogatz. The study analyzed node embeddings on synthetic graphs like Barab\u00e1si-Albert, Erd\u00f5s-R\u00e9nyi, and Watts-Strogatz. Results showed that with smaller k values, nodes two steps away had higher cosine similarity than nodes sharing an edge, indicating second order proximity. However, for larger k values, nodes sharing an edge had higher similarity, indicating first order proximity. The difference in similarity increased with higher k values. The study analyzed how graph sparseness and size affect the choice of permutations and window sizes in link prediction experiments on synthetic graphs like Watts-Strogatz and Barab\u00e1si-Albert. The box plots in FIG6 show that the difference in similarity increases with the value of k, with larger k values resulting in larger differences between similarities of nodes. In this section, the analysis focuses on the impact of a graph's size and sparseness on the choice of permutations (n) for link prediction experiments on Watts-Strogatz and Barab\u00e1si-Albert graphs. Results show a correlation between the number of vertices and branching factor of a graph with the optimal value of n. Smaller graphs benefit from fewer permutations, while larger graphs perform better with more permutations. Overfitting is observed in sparse graphs with n = 10 and n = 5, where results fall below the expected AUC score of 0.5. Results show that larger graphs benefit from a larger number of permutations (n = 10), with a branching size of b = 8 yielding better results. For Barab\u00e1si-Albert graphs, smaller branching factors perform better with n = 1, while larger graphs require more permutations for accurate predictions. The complexity of predicting edges in Barab\u00e1si-Albert graphs is also highlighted. In analyzing Barab\u00e1si-Albert graphs, it is observed that predicting edges in these graphs is challenging, especially for smaller branching sizes. Results show that the best AUC scores are around 70%, with a dependency on graph properties such as size and sparseness. The approach of choosing n on a per graph instance basis is supported, acting as a form of early stopping during training of node embeddings. The study also explores the impact of graph size and sparseness on results for different window and sentence sizes in the model. In the previous section, results for larger values of b are reported, showing that larger values of k usually produce better results but are more prone to overfitting, especially for larger sparse graphs. Further analysis on the representations' properties for different values of k could provide better motivation for its choice, but this is left for future studies. The study also mentions the potential interest in exploring if geometric operations between representations have meaningful interpretations, similar to Word2Vec algorithms. Additionally, Structural Deep Network Embedding (SDNE) is discussed as another algorithm for learning node embeddings in graphs, based on first and second order proximities using autoencoders. SDNE is an algorithm based on first and second order proximities, using autoencoders to learn compact representations. It has a time complexity of O(|V | 2) and can be highly parallelized. SDNE embeddings were compared with NBNE in terms of efficiency and efficacy, with training done on a K40 GPU and a 16 core linux server. The algorithm was run in a semi-supervised setting, with fixed values for \u03b1 and \u03b2. In this study, the authors fixed \u03b1 = 0.2 and \u03b2 = 10 for SDNE embeddings, while choosing \u03bd in a semi-supervised manner. They used an architecture with [10,300; 1,000; 128] nodes on each layer and tested it on Link Prediction and Node Classification tasks. Results showed that both NBNE and SDNE produced similar AUC scores, with slight differences on certain datasets. NBNE outperforms SDNE in terms of training time, with over three orders of magnitude faster training time on various datasets. On the Node Classification task, NBNE showed statistically better results on two datasets, with significant improvements in performance compared to SDNE. In this section, the results of NBNE outperforming SDNE in terms of training time are extended to include precision on training and test sets. NBNE shows statistically similar results to its baselines in node classification tasks on PPI and Wikipedia datasets, with a significant 13% improvement in performance. NBNE shows statistically similar results to its baselines in node classification tasks on PPI and Wikipedia datasets, with a significant 13% improvement in performance. Node classification results in the PPI dataset had the smallest precision among all datasets, with only approximately 14%. On all other experiments, NBNE presented either statistically better or similar results to its baselines, while showing much faster training times. Author name ambiguity is a challenging issue in scholarly digital libraries. Automatic solutions are needed to address cases where distinct authors publish works under similar names. An algorithm was tested using co-authorship networks, with embeddings learned for each author to calculate the probability of author-coauthor relationships. The experiment used an unsupervised algorithm called NBNE to disambiguate author names. DeepWalk and Node2Vec were not used due to the large size of the graphs analyzed. Results for the task were presented, showing the effectiveness of the algorithm with specific parameters. The experiment utilized the NBNE algorithm for author name disambiguation, achieving higher precision compared to the graph baseline for most authors."
}