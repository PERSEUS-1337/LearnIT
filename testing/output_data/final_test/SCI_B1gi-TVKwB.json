{
    "title": "B1gi-TVKwB",
    "content": "An algorithm is introduced for learning a predictive state representation with off-policy temporal difference (TD) learning to steer a vehicle with reinforcement learning. Three components are learned simultaneously: off-policy predictions as a compact state representation, behavior policy distribution for estimating predictions, and deterministic policy gradient for learning to act. A behavior policy discriminator is used to estimate important sampling ratios for learning the predictive representation off-policy with general value functions (GVFs). A linear deterministic policy gradient method is used to train the agent with only the predictive representations while predictions are being learned. The components are combined, demonstrated, and evaluated on steering a vehicle from images in the TORCS racing simulator environment. Predictive learning in steering from images is evaluated on unseen tracks to measure generalization. The proposed method outperforms DDPG, approaching non-vision based models. Predicting the future is crucial in machine learning and human cognition, with the brain being predictive of outcomes. Bridging predictive learning and control is a key challenge, with most approaches focusing on forward or backward models. This paper introduces a predictive control architecture using off-policy predictive learning called general value functions (GVFs) to predict relevant aspects of the environment from raw sensor data. The value of the GVF framework is not fully understood yet, but early work suggests compact and general representations. The objective is to better understand the value of GVFs in real-world applications. The research aims to understand the value of General Value Functions (GVFs) in real-world applications, based on the hypothesis that predictive representations are beneficial for generalization. GVFs, like RL, could enable anticipative behavior rather than reactive responses. Despite potential in robotics applications, there is limited understanding of how to utilize GVFs effectively. The research introduces a novel architecture combining predictive, adversarial, and reinforcement learning to steer a vehicle in a racing simulator. It addresses challenges in autonomous driving, such as characterizing behavior policy, useful predictions, and vehicle control. The classical bicycle model for steering requires knowing the vehicle's angle with respect to the road direction, while direct steering from images is a desired goal. The paper discusses different approaches to steering control in autonomous driving, including a two-point model and end-to-end learning. Challenges in robustness and generalization are highlighted, with solutions like augmented images and predicting road angles from images. The paper proposes a new approach for steering control in autonomous driving by predicting future road angles and lane centeredness at different time horizons, which are then used to choose steering angles. This method offers interpretability in the controller compared to end-to-end approaches, despite efforts to improve robustness. The environment is described by states, actions, and transition dynamics similar to a Markov Decision Process, but without a reward signal to maximize. The goal is to learn an estimator that predicts the return of a cumulant signal, defined by a continuation function. There are currently no algorithms to learn this through interaction with the environment, so the policy, continuation, and cumulant functions are typically defined by an expert. Cumulants are commonly scaled by a factor of 1 \u2212 \u03b3 in non-episodic predictions. A General Value Function can be approximated with a function approximator, such as a neural network, to predict the equation. The agent uses off-policy policy evaluation methods to learn the General Value Function (GVF) with a neural network approximator parameterized by \u03b8. The parameters are optimized with gradient descent minimizing a loss function that corrects for the difference between the target policy distribution \u03c4 and behavior distribution \u00b5. Bootstrapping is used to make predictions of the next state value, improving stability in the process. Bootstrapping is used with older network parameters to improve stability in learning GVFs. Importance resampling with a replay buffer is an alternative approach to using importance sampling ratios, proven to have lower variance. An efficient data structure for the replay buffer is the SumTree used in prioritized experience replay. Sampling a mini-batch from the replay buffer helps to decorrelate sample updates in deep function approximation. A behavior policy needs to be defined to adequately explore the environment when learning GVFs, which may be learned by RL, a random policy, or a human driver collecting data safely. An algorithm using the density ratio trick is proposed to learn the behavior policy distribution in an adversarial setting. The algorithm proposes using the density ratio trick to learn the behavior policy distribution in an adversarial way for problems with low dimensional action spaces like autonomous driving. It involves defining a probability density function for comparison, training a discriminator to distinguish between distributions, and computing the ratio of densities using the discriminator. The algorithm for training a GVF off-policy with an unknown behavior distribution involves estimating \u00b5(a|s) using a known distribution \u03b7(a|s) and implementing a uniform distribution over actions independent of state. It includes computing cumulant values, behavior density, importance sampling ratio, and performing gradient descent steps on state-action pairs. In the context of training a GVF off-policy with an unknown behavior distribution, the approach involves updating a behavior discriminator with modified minibatches. The goal is to find a policy that maximizes future return by using a predictive representation of states and fixed target policies for faster learning. This method simplifies policy and action-value functions and is beneficial in scenarios where obtaining the cumulant signal is costly, such as in autonomous driving. Training a neural network for lane control in autonomous vehicles involves mapping images from inexpensive cameras to predictions of lane centeredness and road angle. The agent learns to steer using deterministic policy gradient (DPG) and linear policy function approximation, resulting in a prediction-based PID controller. This approach can tackle problems with high temporal delay. The approach involves using PID control with integral and derivative terms added to the state space representation of the agent. Action-value and policy networks are trained based on DPG, where the action value approximates the expected return. The policy network produces actions maximizing the expected return, with parameters learned by DPG. The policy network is parameterized for interpretability, representing gain coefficients for a proportional controller. The action-value network Q \u03c0 (\u03c6(s), a) is a small neural network mapping state representations \u03c6(s) and action a to estimate action-value in the current state. In autonomous driving, knowing road curvature helps maintain lane centeredness. Future off-policy predictions of lane centeredness can predict deviation from true center, informing corrective actions needed to stay in lane. Lane centeredness \u03b1 and road angle \u03b2 are key predictions. The lane centeredness \u03b1 and road angle \u03b2 are important predictions for steering a vehicle. Road curvature is represented by future predictions of lane centeredness \u03b1 and road angles \u03b2 at different time horizons. The predictive state representation is given by the feature vector \u03c6(s). The policy network of DPG can be linear due to deviations from the desired lane centeredness and road angle in the predictions. This predictive learning approach is applied to the challenging problem of learning to steer a vehicle in TORCS Wymann et al. The study focuses on learning to steer a vehicle in the TORCS racing environment using a kinematic-based steering approach. A reward function based on vehicle speed and angle is used, with a scaling factor applied to reduce variance. The agents are trained on 85% of the available tracks, with a target speed of 50 km/h controlled by a PID controller. Testing is done on the remaining tracks to evaluate performance. The study evaluated the generalization performance of policies learned by agents in a racing environment. Testing tracks not used in training were used, excluding tracks with uneven roads. Different approaches were compared, including DDPG-Image, DDPG-ImageLowDim, GVF-DPG, and a front wheel steering model. Each method received a history of two images as input. The study compared different approaches in a racing environment, including DDPG-Image, DDPG-ImageLowDim, GVF-DPG, and a front wheel steering model. Results showed that DDPG-ImageLowDim performed best on test tracks, providing ground truth information to the agent. DDPG-Image, however, struggled to learn steering from images, oscillating rapidly between extreme left and right actions. The DDPG-ImageLowDim agent struggles to achieve the target speed due to extreme steering oscillations. It relies more on low dimensional lane information than images. The GVF-DPG approach controls the vehicle more smoothly, as shown in Figure 4. The GVF-DPG approach performs well on most test tracks, while the DDPG-Image fails on several tracks like dirt-4 and wheel-2. The DDPG-ImageLowDim agent struggles with extreme steering oscillations and relies on low dimensional lane information. The GVF-DPG approach controls the vehicle smoothly and performs well on most test tracks, except where the DDPG-Image fails. The GVF-DPG agent follows the classical controller well, suggesting that a predictive representation of lane centeredness and road angle achieves performance closer to a classical controller. More work is needed to improve generalization abilities. The learning curves for the agents show that the GVF-DPG agent's action-value function estimator has smaller and smoother curves due to constrained predictive state representation values. The predictive state representation is constrained between [-1, +1], acting as a regularizer for the agent. DDPG-ImageLowDim learning curve approaches GVF-DPG agent's low error. Predictors converge quickly, behavior estimator stabilizes during learning. Off-policy predictive representation learning method presented using an adversarial approach. Deep off-policy predictions can be learned with behavior policy estimation to predict lane centeredness and road angles from images. The GVF-DPG framework combines policy estimation and predictive representation to steer a vehicle in TORCS using only images. Results show smoother steering with better performance than DDPG and similar to kinematics model on some test tracks. This work demonstrates learning off-policy predictions and controller simultaneously despite evolving behavior policy and changing state representation. Vision-only steering controller shows potential with further development. The prediction-based vision-only steering controller shows potential for improving generalization in RL to new road environments using only images as input. Future work aims to study learning the question for predictive state representation and suggests collecting real-world human driving data to train predictions off-policy. The approach presented in this paper, GVF-DPG, combines policy estimation and predictive representation for steering a vehicle from images. The GVF-DPG approach uses an Ornstein Uhlenbeck process for exploration and learns 8 predictions for lane centeredness and road angle. Different temporal horizons are used for short-term, medium-term, and long-term predictions. All predictions share the same deep convolutional neural network. The predictors and behavior estimator in the GVF-DPG approach share a deep convolutional neural network with specific layer configurations. The policy network is linear, and the action-value network is a small 4-layer network. All networks use rectified linear unit activations except for the last layers. The GVF-DPG approach uses different learning rates for various networks, with the action-value network requiring a small rate for two time-scale learning. It is compared to DDPG baselines with varying information provided to the agent, showing challenges with vision-based approaches. The GVF-DPG agent cheats by having access to lane centeredness and road angle during training, which may not be available at test time. An Ornstein Uhlenbeck process is used for exploration. Target networks were not used in experiments as their removal improved learning. Identical network architectures were used for the policy and action-value networks of the DDPG agents. A separate branch of 12 neurons was used for low dimensional state information, merged with a fully connected layer of 512 neurons. Challenges in merging due to mismatching statistical properties were not significant. Future work aims to improve bridging different pieces of information. Hyperbolic tangent activation was used for the policy network output layer, linear activation for the action-value network output layer, and rectified linear activation elsewhere. Training involved randomly selecting a track using a priority sampling method. During training, a track is randomly selected using a priority sampling method, allowing the agent to interact with the environment until termination. Termination occurs when the agent leaves the lane or reaches the maximum number of steps. A probability distribution is used to select tracks based on difficulty, with higher performing tracks being sampled more frequently. The TORCS environment was modified to provide higher resolution grayscale images for better performance. The grayscale images in the training data were 128x64 pixels, allowing the agent to make long-term predictions. This is beneficial for policy gradient methods and predictive learning."
}