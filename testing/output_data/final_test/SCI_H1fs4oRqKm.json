{
    "title": "H1fs4oRqKm",
    "content": "Unsupervised monocular depth estimation has advanced with deep learning. Training with binocular stereo images is effective for obtaining data easily. However, depth prediction struggles with object boundaries due to occlusion handling issues. A novel method is proposed in this paper to address this by using disparity maps to create an occlusion mask. New networks with flipped stereo images are designed to improve learning of occluded boundaries, resulting in clearer boundaries and better evaluation results on KITTI dataset. Monocular depth estimation using deep learning is a growing research area with applications in navigation and scene understanding. Traditional cameras can serve as a cost-effective alternative to LIDAR sensors for accurate depth estimation. While supervised learning requires ground truth depth data, collecting large datasets with this information is challenging and expensive. Training with stereo images without depth labels is a viable option, with methods like exploiting left-right consistency proposed to improve accuracy. The paper proposes a new method to improve monocular depth estimation using stereo images. It addresses the issue of blurred boundaries by generating an occlusion mask during image warping. New networks are designed to fully exploit stereo image information for clearer boundaries. Our method improves monocular depth estimation by utilizing stereo images to learn clear boundaries for occlusion areas. Empirical evaluation on KITTI and Virtual KITTI datasets demonstrates its effectiveness. Single view depth estimation is challenging due to the ill-posed problem of reasoning depth from a monocular colored image. The approach of using RGB images as inputs and Lidar depth points as ground truth for monocular depth estimation is discussed. Various methods such as Make3d, CNN utilization, deep CNN with CRF exploration, and incorporating semantic segmentations are proposed to improve depth estimation accuracy. Additionally, a residual network with up-sampling module and reverse Huber loss is introduced to enhance the results. Several semi-supervised and fully unsupervised methods have been proposed to overcome the lack of high-quality labeled data for monocular depth estimation. BID2 introduced DeepStereo, a view synthesis based method, while BID24 proposed a method using probability distribution for disparity generation. BID5 introduced a warp-based method, improved by BID7 with a novel loss function. BID19 extended the network with multiple losses, and BID9 proposed a semi-supervised method leveraging sparse Lidar data. Several methods have been proposed for monocular depth estimation, including a semi-supervised approach by BID9 using sparse Lidar points and stereo pairs. BID15 improved this method by decoupling depth prediction into view synthesis and stereo matching. BID26 introduced a network for depth and camera pose prediction, enabling reconstruction of relative temporal images. BID16 implemented a 3D loss for consistency in estimated 3D point clouds and ego-motion across frames, combined with a 2D photometric loss. The implementation of Direct Visual Odometry (DVO) and a novel depth normalization strategy aims to address the issue of object motion in temporal sequence based training. Trinocular training with stereo pairs can alleviate this problem, but warp-based methods struggle with learning occluded areas. Following Godard et al, a monocular depth estimation network with a novel mask method is proposed for end-to-end training without ground-truth labels. This method outperforms Godard et al in result quality, especially on dense evaluation datasets like virtual-KITTI. The goal is to predict pixel-wise dense depth maps from single colored images. The monodepth estimation network uses left image as input to output disparity for reconstructing the right image during training. Various unsupervised methods have been proposed to overcome the challenge of acquiring large labeled data. The monodepth estimation network uses the left image to predict both left and right disparities simultaneously, ensuring left-right consistency for more accurate results. The model incorporates a left-right consistency loss and a refined encoder-decoder network to improve performance. Godard et al achieved state-of-the-art results in unsupervised depth estimation by introducing these enhancements. Godard et al achieved state-of-the-art unsupervised monodepth estimation results using rectified stereo pairs, outperforming supervised methods. Despite post-processing steps, blurred artifacts persist near occlusion boundaries, especially for objects with higher disparaties. Backward warping introduces undesirable duplicates and artifacts due to occlusions. In order to prevent unwanted artifacts and duplicates during training, a warping mask is used to block back-propagation. This helps avoid introducing high losses and ensures the network does not learn to blur in occluded regions. The final output shows the masked white regions, preventing the propagation of warping-induced artifacts. The algorithm designed automatically masks occlusion regions to block artifacts during back propagation. Disparity maps are used for sampling in bilinear process. The mask method ensures clearness by generating masks for reconstructed images. However, masks alone may not guarantee clarity as they block further learning in masked regions. To address the issue of artifacts induced by warping and the blockage of further learning in occluded regions, a refined network architecture and flip-over training scheme were introduced. By exploiting the definite nature of blurred regions, the learning process can be reactivated. Flipping input images horizontally and warping the output disparities back reveals the blurred regions on the unmasked side, as demonstrated in Figure 7. The flip-over scheme is used to reveal blurred regions on the unmasked side, which helps reactivate the learning process. Flipping input images horizontally and warping output disparities back show the blurred regions, leading to the deletion of the predicting branch of the right disparity and the addition of another encoder-decoder network for right images. The doubled encoder-decoder network allows for left-right consistency loss without slowing down test speed. Another network architecture with a shared encoder-decoder network achieves comparable results in half the training time. Training loss is similar to BID7, performed on four scales with appearance match, disparity smoothness, and LR consistency losses. The curr_chunk discusses appearance matching loss and disparity smoothness loss in the context of a flip-over training scheme to avoid masking regions during image reconstruction. The network utilizes a flip-over training scheme to avoid masking regions during image reconstruction. A new encoder-decoder branch is added to address mismatches in disparity prediction. L1 penalty is applied for left-right consistency, reducing artifacts. The network is based on BID7 and implemented in Tensorflow, with VGG16 or Resnet50 encoder and 7 upconvolutional layers. Modifications include predicting only one disparity and tuning channel numbers for improved performance. The model was trained on rectified stereo image pairs in KITTI and Cityscapes datasets, with data augmentation including flipping and color shifting. Evaluation was mainly done on KITTI split BID6, Eigen split BID1, and virtual-KITTI datasets BID4. Issues with KITTI 2015 evaluation, such as sparsity and defects, affected the results, with better performance seen on dense datasets like virtual-KITTI. The same split as BID7 high-quality disparity images from the official KITTI dataset was used for comparison. The high-quality disparity images from the official KITTI dataset are compared to projected velodyne laser points and CAD models in cars. Ground truth data is sparse, especially in occluded regions. Results show less superiority over Godard et al on KITTI stereo 2015 dataset. Our model achieves comparable results with Godard et al when trained for 50 epochs and superior results when trained for 100 epochs. Our network outperforms BID7 when trained for 100 epochs, suggesting it requires more time to converge. We use different hyperparameters from Godard et al and achieve better results on the sparse KITTI dataset. The test split consists of 697 images with ground truth depth generated from velodyne laser points. Training is done on 22600 stereo pairs. The evaluation results are presented under the Garg crop BID5, with uncropped results showing model superiority. Ground truths are captured by velodyne Lidar, reducing superiority over Godard et al. Evaluation is done on the Virtual-KITTI dataset containing 50 monocular videos with pixel-level ground truth depth. The Virtual-KITTI dataset, used for testing, lacks weather conditions covered in the training KITTI 2015 dataset. Evaluation results show model superiority with a maximum depth of 50 meters. Comparison with Godard et al highlights the model's performance. Our model outperforms others on virtual-KITTI with sharp boundaries on predicted depth maps. Even after cropping black edges, our model remains superior to BID7. We introduce an occlusion mask and flip-over training scheme for effective learning of object boundaries. Our method achieves state-of-the-art results using only stereo images and addresses difficulties in occluded areas introduced by image warping. Our method addresses the challenge of occluded areas caused by image warping. It can be combined with more accurate networks trained on trinocular data to improve accuracy. The shared weight network effectively handles the issue of blurriness in occluded regions. The shared weight network addresses blurriness in occluded regions, leading to comparable results with non-shared networks in half the training time. However, it is slightly inferior but still outperforms Godard FIG0. The model averages black edges in post-processing and uses a flip-over scheme to solve back-propagation issues, resulting in blurred predicted disparities. The model addresses blurriness in occluded regions by flipping the colored image as input and the output back as final disparities for warping. This prevents alignment with masks, allowing for clearer boundaries and improved learning process."
}