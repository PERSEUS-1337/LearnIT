{
    "title": "Skz-3j05tm",
    "content": "Domain specific goal-oriented dialogue systems typically require modeling three types of inputs: the knowledge-base, conversation history, and current utterance. Current models like Mem2Seq often overlook the rich structure of knowledge graphs and conversation sentences. To address this, we propose a memory augmented Graph Convolutional Network (GCN) for goal-oriented dialogues. Our model leverages entity relation graphs and dependency graphs to enhance word and entity representations, inspired by the success of GCNs in various NLP tasks. Goal-oriented dialogue systems have widespread applications in various domains like e-commerce, entertainment, and healthcare. They assist in tasks such as scheduling appointments and booking tickets. Using a memory augmented Graph Convolutional Network, our method enriches utterance representations by leveraging global word co-occurrence graphs. Experimental results on the DSTC2 dataset and code-mixed versions in four languages show that our approach outperforms existing methods. The advantage of domain-specific goal-oriented dialogues lies in their limited vocabulary, fixed templates, and exploitable domain knowledge. State-of-the-art methods use Recurrent Neural Networks to encode conversation history and current utterances, along with memory networks for knowledge base entities. These encodings are combined using an attention network to generate responses word by word. Recent works have shown that utilizing graph structures, such as dependency parse trees and Graph Convolutional Networks (GCNs), can improve the performance of goal-oriented dialog systems. By capturing interactions between words and entities in the knowledge base, these methods enhance the system's ability to generate responses accurately. Utilizing Graph Convolutional Networks (GCNs), entities in the knowledge base are encoded as nodes and relations as edges. A sequential attention mechanism computes an aggregated context representation from GCN node vectors of the query, history, and entities. In situations where dependency parse is not feasible, a co-occurrence matrix is constructed from the corpus to impose a graph structure on utterances by adding edges between words that co-occur frequently. This strategy serves as a substitute for dependency parsing. The experiments suggest a simple strategy as a substitute for dependency parse trees. Using Graph Convolutional Networks (GCNs), the model shows improvement in BLEU and ROUGE points on goal-oriented dialogues with code-mixed conversations in multiple languages. The contributions include incorporating structural information with GCNs, utilizing a sequential attention mechanism for context representations, and constructing contextual graphs for code-mixed conversations. The proposed model utilizes PPMI values to construct contextual graphs for code-mixed utterances and achieves state-of-the-art results on the modified DSTC2 dataset. Previous goal-oriented dialogue systems used neural networks for intermediate modules but lacked end-to-end trainability. To address this, a new version of goal-oriented dialogue dataset was released. BID3 released a goal-oriented dialogue dataset focusing on end-to-end neural models that reason over KB triples to generate responses directly from utterances. Various models like Memory Network BID34 and RNN variants were proposed to improve response generation. However, challenges remain in obtaining candidate response sets in new domains. Recently, there has been interest in enhancing encode-attend-decode models with structural information from sentence parses, treating them as graphs for various NLP tasks. This approach aims to integrate entity-relation graph structure of KBs and syntactic structure of utterances, unlike previous works that focused on generating responses without considering these aspects. The use of graph structures in NLP tasks has shown improvements in performance. Tree-LSTMs and Graph-LSTMs have been utilized for tasks like Neural Machine Translation and relation extraction. Graph Convolutional Networks have also emerged as effective methods for encoding knowledge graph structures, allowing for capturing multi-hop relationships between nodes. These approaches have been adopted for various NLP tasks such as semantic role labeling and document dating. Graph Convolutional Networks (GCNs) are used to incorporate dependency structural information and entity-entity graph structure in a neural model for goal-oriented dialogue. This approach also includes contextual co-occurrence information for code-mixed utterances. GCNs operate on a graph structure, computing node representations by considering the node's neighborhood. Multiple layers of GCNs can be stacked to capture neighbors at different distances from the node. Graph Convolutional Networks (GCNs) operate on an undirected graph where each node is represented by a feature vector. The output of a 1-layer GCN is a hidden representation matrix capturing interactions with 1-hop neighbors. Multiple layers of GCNs can be stacked to capture interactions with nodes at different distances. In directed labeled graphs, modified GCNs can operate over dependency parse trees by adding inverse dependency edges to allow information flow between nodes. This requires label-specific parameters and biases, resulting in a large number of weights per GCN layer. The authors propose a more efficient GCN formulation by using only three sets of weights and biases per layer based on the direction of information flow. They also simplify the bias handling and define the task of end-to-end goal-oriented dialogue generation. Each dialogue consists of user utterances, system responses, and accompanying KB triples. The task involves generating responses in a dialogue using knowledge graph structure and syntactic information. The model encodes dialogue history, current user query, and knowledge graph to generate appropriate responses. The query contains tokens and the model embeds each token for processing. The model encodes dialogue history, user query, and knowledge graph to generate responses. It computes contextual representations of tokens using bidirectional RNN and query specific GCN. The node representation in the GCN is computed in each hop. The model encodes dialogue history, user query, and knowledge graph to generate responses. It computes contextual representations of tokens using bidirectional RNN and query specific GCN. The node representation in the GCN is computed in each hop. The embedding of tokens in the history is computed using a bidirectional RNN, followed by the computation of a dependency parse tree for each sentence in the history. A history-specific GCN operates on the graph representation of the history, with edge direction specific weights and biases in each hop. BiRNN is used in the model to capture interactions between entities in the knowledge graph associated with the dialogue. A KB specific GCN refines entity representations to capture relations between entities. The node representation in each hop of the KB specific GCN is computed using specific weights and biases. Inverse edges are added to allow information flow in both directions. An RNN decoder is used to generate response tokens. The response generation process involves using attention mechanisms to obtain single representations from different layers of node vectors in the query-GCN, history-GCN, and KB-GCN. Parameters and attention scores are computed at each decoder timestep to create query and history aware representations of the knowledge base. The response generation process utilizes an attention mechanism to create query and history aware representations of the knowledge base. Parameters and attention scores are computed at each decoder timestep to achieve this. The response generation process uses an attention mechanism to create query and history aware representations of the knowledge base. Parameters and attention scores are computed at each decoder timestep. The loss for each timestep is calculated based on the probability distribution over the vocabulary. Structural information in encodings is captured using dependency parse trees, or in the absence of parsers, a word co-occurrence matrix can be used to extract a graph structure from the utterances. In this section, the datasets, hyperparameters, and models used in experiments are described. The DSTC2 dataset was originally based on restaurant reservation tasks with dialogue state annotations. Results are reported on a modified DSTC2 dataset where annotations are removed, and only raw utterance-response pairs with associated KB triples are present. Experiments with contextual graphs are also discussed. The curr_chunk discusses experiments with contextual graphs on code-mixed versions of modified DSTC2 dataset in four languages. Results of different models are compared, showing performance metrics such as accuracy and F1 score. The dataset was collected by code-mixing utterances in Hindi, Bengali, Gujarati, and Tamil. The same train, test, and validation splits as the original dataset were used for evaluation. The experiments involved using contextual graphs on code-mixed versions of the modified DSTC2 dataset in four languages. Different models were compared based on performance metrics such as accuracy and F1 score. The dataset included code-mixed utterances in Hindi, Bengali, Gujarati, and Tamil, with the same train, test, and validation splits as the original dataset. The models were optimized using Adam optimizer and tuned with specific learning rates and regularization techniques. The performance of RNN+GCN-SeA and GCN-SeA models was compared, along with experimenting with providing token embeddings directly to the GCNs. The text discusses different models for contextual graphs in code-mixed datasets, including GCN-SeA, RNN+CROSS-GCN-SeA, GCN-SeA+Freq, GCN-SeA+PPMI, and GCN-SeA+Random vs GCN-SeA+Structure. These models involve various graph structures and connections between words in a context. The text discusses different models for contextual graphs in code-mixed datasets, including GCN-SeA, RNN+CROSS-GCN-SeA, GCN-SeA+Freq, GCN-SeA+PPMI, and GCN-SeA+Random vs GCN-SeA+Structure. The models involve connecting edges between words in a context, with results evaluated using BLEU BID28 and ROUGE BID21 metrics. The entity F1 measure is used to evaluate the model's capability of injecting entities in the generated response. Results on En-DSTC2 are compared with previous works, showing differences in retrieval based models. Our model outperforms retrieval and generation based models, with a gain of 0.7 in per-response accuracy compared to the previous state-of-the-art model BID30. We also achieve gains in BLEU, ROUGE, and entity F1 points compared to current generation based models, especially on code-mixed datasets. The results show that RNN+GCN-SeA performs better across all languages, including En-DSTC2. The results show that RNN+GCN-SeA outperforms GCN-SeA across languages, including En-DSTC2. Increasing the number of hops in GCNs led to a decrease in performance due to small utterance length. PPMI based contextual graphs were slightly better than frequency based graphs, resulting in improved accuracy, BLEU, ROUGE, and entity F1 scores. The results show that RNN+GCN-SeA outperforms GCN-SeA across languages. Using Random Graphs in GCN-SeA did not contribute to performance gain, highlighting the importance of dependency and contextual structures. Ablations were conducted by replacing sequential attention with Bahdanau attention and experimenting with different encoder combinations. Structure aware representations were found to be beneficial in goal-oriented tasks. The study demonstrated the effectiveness of structure aware representations in goal-oriented dialogue, achieving state-of-the-art performance on the modified DSTC2 dataset and code-mixed versions. Graph Convolutional Networks (GCNs) were used to incorporate structural information from dependency and contextual graphs to enhance dialogue context and knowledge base representations. A sequential attention mechanism was proposed to combine query, conversation history, and KB representations. Additionally, the study showed that in the absence of dependency parsers for certain languages like code-mixed languages, word co-occurrence frequencies and PPMI values can be used to extract a contextual graph for improved performance. The curr_chunk discusses information about two restaurants, Pizza Hut Cherry Hinton and Restaurant Alimentum, including their post codes, cuisines, locations, phone numbers, addresses, prices, and ratings. The user asks for the address of Restaurant Alimentum. Restaurant Alimentum serves modern European food. The user asks for the address and phone number, which the bot provides. The phone number for Restaurant Alimentum is also given. The conversation ends with the user thanking the bot and saying goodbye."
}