{
    "title": "H1g0Z3A9Fm",
    "content": "Community detection in graphs can be solved using spectral methods or probabilistic graphical models. Recent research has unified these approaches for random graph families like the stochastic block model, identifying statistical and computational detection thresholds based on signal-to-noise ratio. By treating community detection as a node-wise classification problem, a novel family of Graph Neural Networks (GNNs) has been developed for supervised learning. These GNNs can match or exceed the performance of the belief propagation algorithm on binary and multiclass stochastic block models, even without knowledge of the underlying generative models. Graph Neural Networks (GNNs) have shown good performance on real-world datasets and have been augmented with the non-backtracking operator for improved results. The optimization landscape of using GNNs for community detection has been analyzed, showing that local minimum loss values are close to global minimum values. Graph inference problems cover a wide range of tasks and domains, from probabilistic graphical models to image segmentation on non-Euclidean domains, driven by practical applications and algorithmic complexity. Graph Neural Networks (GNNs) are powerful algorithms for graph inference tasks, utilizing neural networks to classify nodes, edges, or entire graphs. They learn linear combinations of graph operators with activation functions, allowing for parameter sharing and application to various input graphs. In this work, the focus is on community detection problems in graph neural networks (GNNs). The study compares GNNs to spectral methods and belief propagation for node classification tasks. The goal is to determine if GNNs can approximate or improve upon existing algorithms in supervised settings. In this study, modifications to the GNN architecture are proposed to incorporate edge adjacency information using the non-backtracking operator of the graph. This Line Graph Neural Network (LGNN) model shows improved performance in community detection problems compared to other methods, particularly in random graph families like the stochastic block model (SBM) and geometric block model (GBM). The study demonstrates the improved performance of GNN and LGNN models in community detection tasks compared to other methods, even in the computational-to-statistical gap. The gains can be achieved with linear LGNNs, which are parametric versions of power iteration algorithms. The focus on community detection tasks is due to its well-studied nature, making it suitable for comparing algorithms and generating synthetic datasets. The good performances of GNN and LGNN motivate further analysis. The analysis focuses on the optimization landscape of simplified and linear GNN models when trained with planted solutions of a given graph distribution. An upper bound on the energy gap between local and global minima is provided, showing that the optimization landscape is benign on large input graphs. An extension of GNNs operating on the line graph using the non-backtracking operator improves community detection. Results on the stochastic block model show detection thresholds reached in a data-driven manner, surpassing belief propagation in hard SBM detection regimes. Our results improve upon belief propagation in hard SBM detection regimes and the geometric block model. We analyze the learning landscape of GNN models, showing an \"energy gap\" phenomenon. Our model performs well on community detection with real-world datasets, focusing on node-classification tasks. The specific label of each node is only important up to a global permutation of communities. The model \u03a6 in GNN is designed to be equivariant to permutations of nodes and accepts graphs of variable size for the same set of parameters. Previous work has generalized convolutional neural networks on undirected graphs using the Laplacian's eigenbasis for classification. The GNN architecture by Defferrard et al. (2016) and Kipf & Welling (2016) use Laplacian generators for classification and embedding in graph signals. Our contribution focuses on community detection across a distribution of input graphs without initial labeling, unlike semi-supervised learning. Extensions of GNNs include modifications to activation functions, parameter sharing, and graph operators. The GNN architecture extends learning to graph edges and nodes. Recent work by Velickovic et al. (2017) and Vaswani et al. (2017) explore adjacency learning and attention mechanisms. Kondor et al. (2018) propose a GNN generalization capturing high-order node interactions. Our approach using the line graph aims to capture such interactions. Shamir (2018) analyzes energy landscapes in relation to local minima. In the recent paper by Shamir (2018), an energy bound on local minima in ResNets optimization is established. Exploiting properties of community detection, an energy bound dependent on random matrix concentration is produced. Zhang (2016) focuses on data regularization for clustering and rank estimation, using Bethe-Hessian-like perturbations to improve spectral methods on sparse networks. Yang & Leskovec (2012a) curate benchmark datasets for community detection, while (2012b) develop algorithms fitting the Affliation Graph Model to networks with overlapping communities. The section introduces the Graph Neural Network (GNN) architectures, including power graph adjacency and line graphs with the non-backtracking operator. The GNN is a flexible neural network architecture based on local operators on a graph. The final states of nodes in the graph are used to predict node-wise labels. The final states of nodes in G are used to predict node-wise labels using intrinsic linear operators of the graph. These operators act locally on x and can be represented as matrices. Different matrices like the adjacency matrix A and diagonal matrix D are defined, along with power graph adjacency matrices. The identity matrix I is also considered. The operators are used in an end-to-end training process with backpropagation and a nonlinear activation function like ReLU. The layer includes linear \"residual connections\" to ease optimization and increase model expressivity. Spatial batch normalization is used to mitigate instability during training. Initial states are set as node degrees for permutation equivariance in community detection. In our setup, spatial batch normalization prevents gradient blowup and performs orthogonalization relative to a constant vector, reinforcing the analogy with spectral methods for community detection. Spectral methods use power iterations on matrices like the Laplacian to predict community structure based on eigenvectors. Incorporating a family of operators into the neural network framework, the GNN can approximate and go beyond power iterations. The Krylov subspace generated by the graph Laplacian is not sufficient in the sparse regime, unlike the space generated by {I, D, A}. Adding multiscale versions of A increases the expressive power of each layer, but at the cost of computational efficiency. The network depth is chosen to be of the order of the graph diameter to ensure all nodes receive information from the entire graph, especially in sparse graphs with small diameter. In sparse graphs with small diameter, the GNN architecture offers excellent scalability and computational complexity. Social networks often have constant diameters or log(|V|), making the model suitable for large-scale graphs. Loopy belief propagation can approximate posterior inference in graphs with few cycles. The message-passing rules are defined over the edge adjacency graph. Upgraded GNN models incorporate non-backtracking structures for data-driven belief propagation. The upgraded GNN model utilizes non-backtracking structure for message-passing on the line graph, enabling directed propagation of information. This approach extends the GNN architecture to consider a second GNN on L(G), using matrices B and D B as adjacency and degree matrices. Edge features are updated based on the edge adjacency of G. The Line Graph Neural Network (LGNN) defines edge features updated based on edge adjacency of G. Edge and node features communicate using edge indicator matrices. The model utilizes skip linear connections and trainable parameters. The line graph has size 2|E| \u223c O(d|V|) for graph families with constant average degree d. Line graph construction can be iterated to generate a graph hierarchy capturing high-order interactions among nodes of G. The Line Graph Neural Network (LGNN) captures high-order interactions among nodes of G by learning directed edge features from the edge adjacency structure provided by the non-backtracking matrix on the line graph. Various approaches have been proposed for edge feature learning, including edge features over directed and typed graphs, learning edge features on undirected graphs using commutative functions, and learning directed edge features on undirected graphs using stochastic matrices as adjacencies. The LGNN learns directed edge features from an undirected graph using a non-backtracking matrix. It constructs oriented edge features from node features and propagates them through the graph. Different variations of LGNN include LGNN-L, which drops nonlinear activation functions, and LGNN-S, which connects edges in the line graph based on undirected edges in the original graph. In LGNN-S, edges of the original graph are connected in the line graph if they share a common node. The community labels are defined, and the output of the models at each node is interpreted as the conditional probability of belonging to a community. The loss function is defined with respect to a given graph instance, considering all possible permutations of the community labels. In experiments, the cross entropy loss is minimized over all permutations of communities. For large C, evaluating the loss function can be impractical, so a solution is to randomly partition labels into groups. This allows for an approximate loss value that involves a permutation group of size (C!). Overlapping communities can be accounted for by enlarging C and defining the permutation group accordingly. The optimization landscape of linear GNNs is studied in this section, focusing on the binary c = 2 case with simplified assumptions. The GNN models without nonlinear activations show gains compared to baseline algorithms in numerical experiments. The landscape is proven to have \"benign\" local minima under certain simplifications. The linear GNN setup differs from fully-connected neural networks in terms of output on the unit sphere and fluctuating landscape due to graph-dependent operators in the family F. The operators are non-commutative, but the generalized Krylov subspace generated by powers of F is considered. The landscape is specified by random matrices Yn, Xn \u2208 RM\u00d7M with Cholesky decomposition EXn = RnRTn. The concentration of relevant random matrices around their mean controls energy gaps between local and global minima of L. If certain conditions are met, the energy gaps can be controlled as \u03b4n \u2192 0. The main strategy of the proof involves considering the loss function Ln as a perturbation of a simpler landscape to analyze its invariance properties. Estimating spectral fluctuations of Xn, Yn involves analyzing the spectrum of non-commutative algebras. Open questions include the behavior of bounds in stochastic block models and the relationship between landscape analysis asymptotics and estimation difficulty. In community detection experiments, performance is measured by the overlap between predicted and true labels, maximizing this quantity over global permutations within a graph. Prediction accuracy in real-world datasets with overlapping and unbalanced communities is measured differently. The experimental setup for neural network models in community detection includes using Adamax with a learning rate of 0.004, 30 layers and 8 features for Sections 6.1 and 6.2, and 20 layers and 6 features for Section 6.3. GNNs and LGNNs have J = 2 in most experiments, except for ablation experiments in Section C.3. The stochastic block model is a random graph model with a planted community structure, where nodes are partitioned into communities based on assigned labels. In the binary case, the sparse regime is well understood and serves as a basis for comparing GNN and LGNN with optimal recovery algorithms. In comparing GNN and LGNN with optimal recovery algorithms, two learning scenarios were considered. The first scenario involved training models separately for different pairs of (p, q), while the second scenario trained a single set of parameters from a mixture of SBM with varying parameters. This setup demonstrated that the models were not simply approximating known algorithms like BP. Five different pairs of (p, q) were chosen in the first scenario, corresponding to different signal-to-noise ratios. The performance of GNN and LGNN models on binary SBM graphs with different signal-to-noise ratios (SNRs) was compared with baseline methods including BP and GAT. Both GNN and LGNN achieved similar performance to BP, with LGNN showing results close to BP's spectral approximations. The models also outperformed GAT in the task. Experiments were conducted in dissociative cases and with 3 communities, yielding similar results. Table 1 shows the performance of different models on 5-community dissociative SBM graphs with n = 400, C = 5, p = 0, q = 18/n, and an average degree of 14.5. For SBM with fewer than 4 communities, BP reaches the information-theoretic threshold, while for k > 4, a gap is conjectured between MLE estimators and polynomial-time estimation procedures. GNN models are used to explore generalizations of BP and improve detection performance in scenarios where the SNR falls within the computational-to-statistical gap. Table 1 presents results for the 5-community dissociative SBM with n = 400, p = 0, and q = 18/n. The SNR is above the information-theoretic threshold but below the asymptotic threshold for BP detection. GNN and LGNN models outperform BP, potentially reducing the computation-information gap. LGNN shows the best average test accuracy, outperforming GAT. The study of finite-size effects and asymptotic gains is left for future work. The curr_chunk discusses the comparison of different models on SNAP datasets, focusing on top quality communities and identifying edges that cross different communities. The resulting graph is connected, and the dataset is divided into training and testing sets. The dataset is divided into training and testing sets, ensuring no community overlaps. The comparison of GNN, LGNN, GAT, and AGM models on SNAP datasets shows similar performance, with GNN and LGNN outperforming AGMfit in community detection. In this work, data-driven models like LGNN-S and GAT outperform AGMfit in supervised community detection. The models achieve comparable performance to BP in binary SBM and surpass BP in the sparse regime of 5-class SBM. The introduction of graph operators, including power graph adjacency matrices and the line graph with non-backtracking matrix, contributes to this success. The optimization landscapes of simplified linear GNN for community detection are theoretically analyzed, showing a bounded gap between local and global minima. Our empirical results on non-asymptotic models for graph sizes show potential for understanding generalization properties as graph size increases. Questions arise on how signal-to-noise ratios affect energy landscapes and if network parameters have mathematical interpretations. The model's current limitation is the fixed number of communities assumption, with future research directions including extending to variable community detection cases. Future research directions include extending to cases where the number of communities is unknown or varies with graph size, as well as applications to ranking and edge-cut problems. The Cholesky decomposition of EX n is defined, and eigenvalues of symmetric matrices are denoted. Global minimum of the mean-field loss is denoted by \u03b2 g, and a bound extension is discussed. The text discusses bounding the \"energy gap\" |L n (\u03b2 l ) \u2212 L n (\u03b2 g )| by considering separate cases and applying a change-of-variable technique. It also relates the inequality to cos(\u03b3 l ,\u03b3 g ) to obtain an upper bound on [1 \u2212 cos 2 (\u03b3 l ,\u03b3 g )]. The text further discusses bounding \u03bb 1 (\u2207 2S n (\u03b3)) by bounding \u03bb 1 (Q 1 ) and Q 2. The text discusses bounding \u03bb 1 (Q 1 ) and Q 2 using orthonormal eigenvectors of a symmetric matrix. It also aims to bound the angle between vectors \u03b3 l and \u03b3 g by analyzing functions of \u00b5 n, \u03bd n, and \u03b4 n. Lemmas A.4 and A.5 provide further bounds on these quantities. Lemma A.6 provides bounds on various terms by applying generalized H\u00f6lder's inequality. The lemma introduces \u03b4 n = (64 + 63\u03b4) and aims to bound each term separately on the right-hand side. By summing up the bounds obtained, the lemma concludes with a final result. Lemma A.6 introduces bounds on terms using generalized H\u00f6lder's inequality, aiming to bound each term separately on the right-hand side. By summing up the bounds obtained, the lemma concludes with a final result. The proof of Lemma A.3 notations are used to show that Y n is positive semidefinite, leading to EY n also being positive semidefinite, and hence \u0100 n = R. Next, the first and third terms on the right-hand side of the inequality in Lemma A.1 are bounded. Finally, the desired lemma is obtained using the generalized H\u00f6lder's inequality. The adjacency matrix A models a system of elements with community structure. Nodes are assigned community labels, and the goal is to estimate these labels from the adjacency matrix. In the binary case, community labels can be set to \u00b11. Communities are associative, meaning nodes from the same community are more likely to be connected. The cost associated with cutting the graph between communities is measured by DISPLAYFORM49, which we aim to minimize under constraints. The community structure is connected to the graph Laplacian spectrum, providing a stable way to estimate community labels for each node. The Fiedler vector reveals important community information. The Fiedler vector, associated with the second smallest eigenvalue, provides key community information of the graph. Extracting eigenvectors at the edge of the spectrum using the power iteration method is of interest. Unrolling power iterations into a trainable neural network is similar to the LISTA sparse coding model. Graph Laplacian spectral approaches have limitations in sparsely connected graphs. In sparsely connected graphs, spectral approaches fail as eigenvectors of the graph Laplacian concentrate on nodes with dominant degrees, losing correlation with the community structure. To overcome this limitation, ideas from statistical physics are used to optimize label agreement in graphical models. This involves assuming a Markov Random Field with probability calculations that are computationally challenging. When dealing with Markov Random Fields (MRFs), the sum-product algorithm, also known as belief propagation (BP), can efficiently compute marginals in linear time on tree graphs. The beliefs in BP represent marginal distributions, and fixed points can recover MRF marginals. However, convergence is not guaranteed on non-tree graphs, requiring a generative model with correct parameters, which can be derived using expectation maximization. Certain sparse graphs, like Stochastic Block Models (SBMs) with constant degree, can approximate trees for successful BP application. Spectral clustering using the nonbacktracking matrix (NB) improves over traditional methods like Laplacian matrix L and adjacency matrix A. NB eigenvalues are confined to a disk in the complex plane, enhancing community detection by reducing high degree fluctuations. Spectral clustering with the nonbacktracking matrix (NB) improves community detection by utilizing eigenvalues correlated with the community structure. The Bethe Hessian matrix, defined by BH(r) := (r^2 - 1)I - rA + D, shows a one-to-one correspondence between fixed points of BP and stationary points of the Bethe free energy. Negative eigenvalues of BH(r) indicate phase transitions in the Ising model, leading to identifiable new clusters. The spectral method using the Bethe Hessian shows a one-to-one correspondence between fixed points of belief propagation and stationary points of the Bethe free energy. Negative eigenvalues of the Bethe Hessian indicate phase transitions in the Ising model, leading to identifiable new clusters. The GNN in Section 4 can express the algorithm of performing power iteration on the Bethe Hessian without needing a generative model. The stochastic block model (SBM) is a random graph model denoted by SBM (n, p, q, C) with associated function F : V \u2192 {1, . . . , C}. The stochastic block model (SBM) is denoted by SBM (n, p, q, C) with associated function F : V \u2192 {1, . . . , C}. Communities are assigned to vertices based on a generative model where vertices are connected with probability p if in the same community, and q if in different communities. Exact recovery is possible on SBM (n, p = a log n n , q = b log n n ) in the two-community case. In the sparse regime of constant degree, exact recovery is not possible on SBM (n, p = a n , q = b n). Detection is the best achievable outcome in this scenario, which is relevant for real-world applications with bounded degree and extreme sparsity. Spectral and SDP methods struggle due to fluctuations in the degree distribution. In the constant degree regime, methods like spectral and SDP struggle due to fluctuations in the degree distribution. Decelle et al. (2011) proposed the BP algorithm on the SBM, which yields Bayesian optimal values. The signal-to-noise ratio and Kesten-Stigum threshold play a crucial role in detection, with SNR > 1 allowing for polynomial time solutions by BP. For k = 2, SNR = 1 is both the computational and information theoretic threshold. For k > 4, non-polynomial time algorithms may be needed for some SNR < 1. The Geometric Block Model (Sankararaman & Baccelli, 2018) is a random graph with locally hyperbolic properties that make it treelike with high probability. It contrasts with random graphs with locally Euclidean geometry. GNN and LGNN show overlap performance on graphs generated by this model compared to spectral methods. The random graph is generated by sampling points from a Gaussian mixture model with means at distances apart and drawing edges between nodes based on a threshold. The model contains short cycles affecting loopy belief propagation. Other estimation algorithms based on motif-counting are motivated. GNN and LGNN performance on the binary GBM model is compared with spectral methods. The LGNN model outperforms spectral methods and baseline GNN on the SBM mixture experiment. The GNN model competes with or surpasses the Bethe Hessian, achieving state-of-the-art results despite not having access to the underlying generative model. The GNN model outperforms spectral clustering methods on the SBM mixture experiment by learning the optimal perturbation of the multiscale adjacency basis and nonlinear power iterations. The absence of certain operators in the model affects performance, highlighting the importance of power graph adjacency matrices. In the experiment, GNN models a, b, and c show that larger J does not always lead to better performance. Models c, d, and e with similar parameters as f achieve worse accuracy, highlighting the importance of the line graph structure for LGNN performance. Additionally, l performs worse than f, emphasizing the significance of the non-backtracking line graph over the symmetric line graph."
}