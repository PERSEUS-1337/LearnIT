{
    "title": "SkVRTj0cYQ",
    "content": "Federated learning is a privacy-protecting method where a trusted curator aggregates parameters optimized by multiple clients in a decentralized manner. The resulting model is distributed back to clients without sharing data explicitly. However, the protocol is vulnerable to differential attacks, revealing clients' contributions and data sets. To address this, an algorithm for client-sided differential privacy-preserving federated optimization is proposed to hide clients' contributions during training while balancing privacy loss and model performance. Empirical studies show that our proposed procedure can maintain client-level differential privacy with minimal impact on model performance. The increasing interest in security in machine learning is driven by the success of big data and deep learning. As machine learning services become more prevalent, privacy protection measures are crucial due to the potential risks of data misuse. Anonymization alone is often insufficient, and standard machine learning approaches may overlook privacy concerns and be vulnerable to attacks that compromise user privacy and data confidentiality. BID2 proposes a measure to assess the memorization of privacy-related data in privacy-preserving machine learning. Decentralized approaches like collaborative or federated learning involve multiple clients learning a model in a decentralized manner, with only learned parameters centralized by a trusted curator. However, additional measures are needed to preserve privacy effectively. In federated learning, clients can be identified through model updates, requiring further privacy measures. Differential privacy (dp) aims to prevent a learned model from revealing specific data points used during training. An algorithm incorporating dp into federated learning protects client data sets from being exposed during decentralized training. Our main contributions include demonstrating how a client's participation can be hidden in federated learning while maintaining high model performance and achieving client-level differential privacy with minimal impact on model performance. We also propose dynamically adapting the privacy-preserving mechanism during decentralized training to improve model performance, contrasting with centralized training approaches. In federated learning, gradients exhibit different sensitivities to noise and batch size compared to centralized learning. Communication between curator and clients is limited and vulnerable to interception. The challenge is to learn a model with minimal information exchange. Clients' data may be non-IID, unbalanced, and distributed. Federated averaging is a key algorithm. Differential privacy can be protected by incorporating a dp-preserving randomized mechanism like the Gaussian mechanism into the learning process. The Gaussian mechanism (GM) is a randomized mechanism used for differential privacy in federated learning. It adds Gaussian noise to approximate a real-valued function with sensitivity Sf. The mechanism ensures (\u03b5, \u03b4)-differential privacy, with \u03b4 representing the probability of privacy breach. The probability of privacy breach can be bounded by \u03b4 \u2264 5/4 exp(\u2212(\u03c3)\u00b2/2), where \u03c3 is fixed.\u03b4 accumulates with consecutive inquiries to the GM, emphasizing the need to protect privacy. The Gaussian mechanism (GM) in federated learning ensures (\u03b5, \u03b4)-differential privacy by adding Gaussian noise. \u03b4 accumulates with consecutive inquiries, prompting a privacy accountant to track it. BID0 introduced dp-SGD, a privacy-preserving algorithm that stops training once \u03b4 reaches a threshold. This approach aims to prevent the learned model from revealing sensitive data points. In federated optimization, the goal is to protect a client's entire dataset during decentralized training while maintaining high model performance. A randomized mechanism is used to approximate the averaging of client models, ensuring that a single client's contribution is hidden within the aggregation process. This mechanism involves random sub-sampling of clients in each communication round to distribute the central model only to a subset of clients. In federated optimization, a subset of clients with size m t \u2264 K is sampled to receive the central model w t. The clients optimize the central model on their data, resulting in distinct local models {w k } mt k=0. The difference between the local and central models is sent back as updates to the central curator. A Gaussian mechanism is used to distort the sum of updates, with scaled versions used to limit sensitivity and add noise for privacy. The Gaussian mechanism is used in federated optimization to distort the sum of updates from a subset of clients. The noise added is scaled to sensitivity S to prevent leakage of individual information. The distortion of the average is controlled by the noise variance, ensuring that learning progress is not hindered. Random sub-sampling and the Gaussian mechanism are both randomized mechanisms used to protect privacy in the optimization process. The moments accountant method by Abadi et al. BID0 is used to track privacy loss in federated optimization. The process involves sampling clients, distributing the central model, optimizing on local data, clipping updates, and averaging them. The central model in federated optimization is updated by averaging clipped updates and adding Gaussian noise. A privacy accountant evaluates privacy loss before starting a new communication round. The process stops when \u03b4 reaches a threshold based on the total number of clients K. The choice of threshold ensures privacy for many without revealing total information about a few. In federated optimization, the central model is updated by averaging clipped updates and adding Gaussian noise. Privacy loss is evaluated before each communication round, with the process stopping when a threshold based on the total number of clients is reached. The choice of threshold ensures privacy for many without revealing total information about a few. When choosing the clipping bound S, a trade-off exists between noise variance and preserving original contributions. The procedure involves calculating the median norm of unclipped contributions in each round. The choice of \u03c3 and m determines distortion and privacy loss, with smaller values resulting in lower privacy loss. In federated optimization, the choice of \u03c3 and m determines distortion and privacy loss. Privacy loss is smaller for smaller values of \u03c3 and m. The between clients variance Vc is defined as a measure of similarity between clients' updates. The variance of parameters throughout all clients is calculated to define Vc and the Update scale Us. The Update scale U s is defined by the variance of parameters across all clients. Algorithm 1 outlines a client-side differentially private federated optimization process, with parameters such as the number of clients, local mini-batch size, learning rate, and communication rounds. The goal is to achieve differential privacy with a specified threshold for probability. In a federated setting, the algorithm tests differential privacy with shards of the MNIST dataset assigned to clients. Each client receives two shards, limiting their training to two digits. Different scenarios with varying numbers of clients and data points are explored. Cross-validation grid search is conducted on parameters like number of batches, epochs, clients per round, and GM parameter \u03c3. During training, privacy loss is monitored using the privacy accountant, and training stops when \u03b4 reaches specific thresholds for different numbers of clients. The between clients variance is also analyzed during training. In cross-validation grid search, models with the highest accuracy below the \u03b4 bound are selected, with preference given to models requiring fewer communication rounds. The best models for different numbers of clients are listed in TAB3, showing accuracy, communication rounds needed, and communication costs. Communication costs are defined as the number of times a model is sent by a client during training. Benchmark results are also included in TAB3. During decentralized training, the accuracy of differentially private federated optimization models for various numbers of clients is compared. The impact of the number of participating clients on accuracy is shown in Fig. 2, with dots indicating when training stopped due to reaching the \u03b4-threshold. The between clients variance and update scale are also analyzed during training. The number of participating clients has a significant impact on model performance in differentially private federated optimization. For 100 and 1000 clients, model accuracy does not converge and remains below non-differentially private performance. However, accuracies of 78% and 92% for K \u2208 {100, 1000} are still better than individual training. In domains like hospitals, where K is in this range, jointly learned models can benefit clients while maintaining privacy. For K = 10000, the differentially private model almost matches non-differentially private accuracy, suggesting potential for scenarios with a large number of clients. In scenarios with many parties involved, differential privacy has almost no impact on model performance, such as in mobile phones and consumer devices. Increasing the parameter m t during training improves model performance. Lowering both m t and \u03c3 t in early communication rounds has minimal impact on accuracy gain but reduces privacy loss. However, in later rounds, a larger m t is necessary for accuracy, leading to higher privacy costs. This observation aligns with recent advances in information theory in learning. Recent advances in information theory in learning algorithms suggest that during training, there are two phases: label fitting and data fitting. In the label fitting phase, updates by clients are similar, resulting in low Vc and high Uc. As training progresses to the data fitting phase, Vc rises while Uc drastically shrinks, indicating convergence towards a local optima of the global model. This is shown in FIG3, highlighting the dependencies of Vc and Uc throughout the training process. The text discusses the importance of a balanced fraction of clients for representative updates, the impact of high Uc on early updates, and the feasibility of differential privacy on a client level. It also mentions optimizing privacy budgeting through data and update distribution analysis. Future work includes deriving optimal bounds for signal to noise ratio, investigating dataset dependency, and exploring applicability in bandwidth-limited settings. To explore the proposed approach's applicability in bandwidth-limited settings, we plan to investigate its effectiveness with compressed gradients like those suggested by BID8."
}