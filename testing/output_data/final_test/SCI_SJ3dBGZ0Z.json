{
    "title": "SJ3dBGZ0Z",
    "content": "Log-linear models, commonly used in machine learning and deep learning architectures, can be approximated in sub-linear time with strong concentration guarantees. The LSH Softmax method presented in this work utilizes Locality-Sensitive Hashing to enable sub-linear learning and inference of the softmax layer. By employing nearest neighbors and uniform samples, a well-concentrated gradient estimator is built. Additionally, an inference scheme using the Gumbel distribution allows for sub-linear time computations. In language modeling, Recurrent Neural Networks trained with LSH Softmax achieve comparable performance to exact softmax computation while requiring fewer computations. The softmax layer is a key component in various machine learning tasks like vision, language, speech, and videos. It transforms a feature vector into a distribution over the output space, modeling log-probabilities linearly. While offering modeling flexibility, inference and learning have linear runtime in the number of classes. Inference and learning in the softmax layer have linear runtime in the number of classes, requiring computation of un-normalized probabilities for every class. Large output space problems occur in NLP and computer vision, leading to proposed solutions like softmax probability approximation or exact probabilities for an approximate model. Inference and learning in the softmax layer have linear runtime in the number of classes. Proposed solutions include softmax probability approximation methods like Hierarchical Softmax, which clusters output classes for more efficient computation. Recent estimators based on nearest neighbor search have also been proposed for log-linear models. In this work, the estimators focus on Maximum Inner Product Search using Locality-Sensitive to retrieve large logits and address tail distribution with uniformly sampled classes. They offer strong theoretical guarantees and concentration bounds but were limited to toy settings. The authors extend these estimators for deep learning applications without compromising theoretical guarantees. They introduce a new deep learning layer and demonstrate its efficiency in language modeling tasks, showing significant improvements in perplexity and speed compared to existing methods. The authors introduce a new deep learning layer, LSH Softmax, based on Locality-Sensitive Hashing and the Gumbel distribution, with theoretical guarantees for sub-linear learning and inference. Empirical results show that LSH Softmax performs similarly to an exact softmax but with significantly fewer FLOPS. Neural networks models are built hierarchically using linear and non-linear transformations. Recurrent Neural Networks (RNN) extend this to long sequences by maintaining an internal state. The new LSH Softmax layer combines Locality-Sensitive Hashing and the Gumbel distribution for efficient learning and inference. The softmax layer in neural networks parameterizes a distribution over the output space Y using weight vectors and logits. It is crucial for inference and learning, requiring O(C) operations. The parametrization can be subject to vanishing or exploding gradients, making LSTM preferred for optimization. The Gumbel distribution, introduced by BID7, is defined by a cumulative distribution function. Sampling from this distribution involves sampling from a uniform distribution and applying a transformation. This distribution is useful for casting sampling as optimization. Nearest neighbor search is a common task in various fields, involving finding the closest vectors to a query in a fixed set based on a specified distance. Nearest neighbor search involves finding vectors closest to a query based on a specified distance. Maximum Inner Product Search (MIPS) aims to retrieve the vector closest to the query. Amortized sub-linear time can be achieved for large numbers of queries by using space partitioning techniques like Locality-Sensitive Hashing (LSH). LSH reduces the number of candidate vectors to evaluate by hashing similar vectors into the same bucket. The text discusses constructing a data structure for nearest neighbor retrieval in sub-linear time using a set of size N, a similarity measure, and a family of hash functions. It mentions utilizing LSH with hyperplane hash functions and applying a theorem for sublinear learning in deep learning models. Deep learning models are trained with a maximum-likelihood objective using back-propagation for optimization. Gradients are computed for parameters \u03b8 and \u03c6, with a bottleneck in large output spaces due to the O(|Y|) operations required. The bottleneck in large output spaces for deep learning models is addressed by BID26, which introduces a method to compute expectations in sub-linear time using an LSH structure. This allows for efficient computation of softmax gradients necessary for training. The BID26 method introduces a way to compute softmax gradients efficiently in deep learning models by using an LSH structure. The method allows for sub-linear time computation of gradients, but it requires online updating of the LSH structure to accommodate weight updates during training. This is achieved by performing sparse updates to maintain sub-linear runtime. The algorithm described in Proposition 4 shows how softmax computations can run in sub-linear time during training iterations. The BID26 method efficiently computes softmax gradients in deep learning models using an LSH structure. It involves sampling examples from D, finding nearest neighbors of h using MIPS, and re-hashing updated vectors. The softmax computations are split into retrieving nearest neighbors, computing forward/backward passes, and rehashing vectors, all of which are sub-linear operations. LSH Softmax can speed up training time and provide computational gains at inference time. It allows for sampling from conditional distributions, crucial for large-scale deployment. By leveraging a MIPS structure and Gumbel distribution, it enables sampling from log-linear models in sub-linear time. The Gumbel distribution is used to sample points efficiently from a set in sub-linear time. This inference scheme can be applied to any softmax model and runs faster on specialized hardware like GPUs. The likelihood is intractable, but the scheme guarantees sampling from the distribution with high probability. The Gumbel distribution enables efficient sampling from a set in sub-linear time, particularly beneficial for GPU implementation in training large models. LSH Softmax is easily implementable in modern deep learning frameworks, providing wall-clock gains in practice. The Gumbel distribution facilitates efficient sampling for GPU implementation in training large models. LSH Softmax is easily implementable in deep learning frameworks, offering practical time savings. Learning models are trained using minibatch optimization, with steps computed efficiently in the minibatch setting. The first step involves representing a batch of queries and hyperplanes in matrix form for GPU processing. The second step is parallelizable using multithreading on CPU, while the last step is GPU-friendly for gathering operations. The LSH structure facilitates efficient GPU processing by gathering appropriate vectors into a 3-d tensor. Implementing deep learning models with frameworks like BID0 BID19 abstracts gradient computation, saving time and reducing errors. The estimator can be easily integrated into these frameworks for seamless implementation. Our new layer LSH Softmax is implemented in deep learning frameworks, allowing for efficient computation of gradient estimators. It outperforms approximate softmax baselines and is applicable to various domains, including NLP and vision tasks with large output spaces. Language modeling involves estimating the probability of a sequence of words in a vocabulary. Non-parametric n-gram models with smoothing techniques may struggle with long histories due to the exponential number of sequences. Parametric models like RNNs have shown success in handling large output spaces. The LSH Softmax layer, implemented in deep learning frameworks, outperforms approximate softmax baselines in NLP and vision tasks with large output spaces. We evaluate our method on three standard datasets for Language Modeling: Penn TreeBank, Text8, and Wikitext-2. These datasets vary in vocabulary size and number of characters. Baselines include models trained with exact softmax and Biased techniques. Our models are trained with different softmax techniques including exact softmax, Biased Importance Sampled softmax, Negative Sampling, and standard Importance Sampling. Training involves SGD with gradient clipping and an initial learning rate of 20, annealed when validation perplexity plateaus. Models are trained for varying epochs on different datasets. Parameters for LSH Softmax, IS, and NS are chosen for fair comparison. LSH Softmax consistently outperforms approximate baselines in terms of perplexity and learning curves, showcasing its strength as an estimator. Approximate methods tend to plateau in performance, while LSH Softmax does not. The proposed estimator performs well on real-world tasks, and computation gains are evaluated for models with large output spaces. LSH Softmax outperforms approximate baselines in terms of perplexity and learning curves, showing its strength as an estimator. Computational gains are evaluated for models with large output spaces, with the softmax computation being a significant portion of the total computation. NS performs worse than LSH Softmax and deteriorates with increasing output space size. LSH Softmax outperforms importance sampling methods and shows consistent performance across different output space sizes. BID32 propose using LSH to retrieve largest logits for Softmax estimation, but not accounting for the tail can lead to biased gradients. BID26 and BID30 also explore similar methods but show worse performance when only considering top-k values or using LSH at each layer. LSH Softmax is a softmax approximation layer with theoretical guarantees and sub-linear cost for large output spaces. It outperforms other methods in NLP tasks like language modeling, offering closest perplexity to exact training and significant speed-ups. The LSH Softmax layer is a softmax approximation with theoretical guarantees and sub-linear cost for large output spaces. It outperforms other methods in NLP tasks like language modeling, offering closest perplexity to exact training and significant speed-ups. The plan is to release source-code with the camera-ready version for any architecture to choose this layer instead of softmax when the output space is large enough."
}