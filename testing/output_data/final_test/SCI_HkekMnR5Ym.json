{
    "title": "HkekMnR5Ym",
    "content": "In this paper, the authors discuss the trend of training neural networks to replace hand-crafted data structures for faster execution and better accuracy. They introduce the Neural Bloom Filter, a memory architecture for learning approximate set membership over a stream of data in one-shot via meta-learning. The Neural Bloom Filter is shown to be more compressive than Bloom Filters and other memory-augmented neural networks in scenarios of skewed data or structured sets. The Bloom Filter is a widely used data structure for set membership queries, storing sparse distributed codes in a binary vector. It allows for a controlled false positive rate due to hash collisions but never produces false negatives. Bloom Filters are commonly found in network security systems to block malicious IP addresses. Bloom Filters are used in various production systems for tasks like network security, databases, cryptocurrency, search, and program verification. They offer favorable compression and support dynamic updates, allowing for O(1) time insertion of new elements. While perfect hashing saves more space, it requires a polynomial-time pre-processing stage, limiting its applicability compared to Bloom Filters. In a database application with high write throughput, regenerating the data structure after each batch of writes is impractical. The focus is on the data stream computation model BID27, where input observations are ephemeral and can only be inspected a constant number of times. Developing a more compressive set membership data structure than Bloom Filters, applicable to dynamic or static sets, could greatly impact modern computing applications. Memory-augmented neural networks and meta-learning are used to address this problem. Memory-augmented neural networks and meta-learning are utilized to develop a more compressive set membership data structure than Bloom Filters. Prior studies have explored compiler optimization, computation graph placement, and data index structures like b-trees. By training a neural network over a fixed set of URLs with negative examples, a 36% space reduction over a conventional Bloom Filter is achieved. This approach requires iterating over the storage set multiple times to embed its salient features. Instead of learning from scratch, meta-learning memory-augmented neural networks are used to specialize in tasks with few examples, matching well with applications like Bigtable databases with many Bloom Filters. This allows for exploiting common redundancy in instantiated data structures. The main contributions of this paper include a new Neural Bloom Filter architecture for memory-augmented neural networks and an empirical evaluation on one-shot set membership problems. The Neural Bloom Filter outperforms classical Bloom Filters in space efficiency when there is a lot of structure in the query set elements. Approximate set membership allows for a false positive rate of at most 1%. The space requirement for approximate set membership of uniformly sampled observations is at least n log 2 (1) bits, which can be achieved with perfect hashing. This amounts to 6.6 bits per element, a significant space saving compared to storing raw or compressed elements. The Bloom Filter BID2 is a data structure that solves the dynamic approximate set membership problem with near-optimal space complexity. It uses k uniform hash functions to hash inputs and set corresponding bits to 1 in a binary string of length m. The Bloom Filter returns true for a query if all hashed locations are set to 1, with zero false negatives but possible false positives due to hash collisions. To achieve a low false positive rate with minimal space, one can set k = log2(1/\u03b5). Recurrent neural networks like LSTMs retain memory through the recurrent state, often tied to trainable parameters. Recent interest lies in enhancing neural networks with external memory, popularized by the Neural Turing Machine (NTM) and its successor, the Differentiable Neural Computer (DNC), as well as Memory Networks for question answering. Memory Networks store input embeddings in a memory matrix for content-based addressing. The Differentiable Neural Computer (DNC) and Neural Turing Machine (NTM) use a content-based addressing operation for reading from memory. They can write to memory based on content, temporal order, or unused memory. Increasing memory capacity can be achieved through sparse read and write operations or by compressing memory representations. Memory Networks can compress input individually but not jointly over multiple inputs. The Kanerva Machine BID36 tackles memory-wide compression using a distributed write scheme to jointly compose and compress its memory contents. It uses content-based addressing over a separate learnable addressing matrix A, instead of the memory M, to learn where to write. One approach to learning set membership in one-shot would be to use a recurrent neural network, such as an LSTM or DNC, to sequentially ingest elements to store and answer queries using the final state. The Neural Bloom Filter is a memory model that is compressive and scalable, using an additive write operation to update a memory matrix. It avoids the challenges of backpropagation through time (BPTT) and is more efficient than Memory Networks. The Neural Bloom Filter uses an additive write operation to update a memory matrix, allowing for parallel computation of gradients. Addressing memory slots is done via a softmax, with sparsification for efficiency. Sphering the addressing activations prior to the softmax improves performance. The final architecture includes a controller network for encoding input, a non-trainable linear layer for addressing softmax, and an addressing network for sphering transformations. ZCA transformation is used for decorrelation, and content-based attention is computed over memory using a non-learnable address matrix. The final architecture includes a controller network for encoding input and an addressing network for sphering transformations. Content-based attention is computed over memory via a non-learnable address matrix A, with a sparse softmax \u03c3 k for retaining top similarities. Memory writes are performed additively to top k addresses, inspired by the Bloom Filter, while reads involve retrieving top k addresses from memory and multiplying them element-wise with address weights. The addressing network allows for non-linear interactions between memory rows during read operations. The network configuration includes a 3-layer CNN for image input and an LSTM for text input. The addressing network also discusses the motivation for decorrelation and sphering, as well as its relation to uniform hashing. The model could be implemented for O(log m) time. In this section, we discuss space lower bounds for the approximate set membership problem, focusing on scenarios where neural networks may outperform classical lower bounds. The n log 2 (1/ ) lower bound assumes equal probability for all subsets and queries, but real-world applications like web cache sharing or spell-checking involve non-uniform sets and queries. This highlights the potential for neural networks to excel in scenarios with structured storage or query sets. A more general space lower bound for the approximate set membership problem can be defined using an information theoretic argument from communication complexity. This involves a two-party communication problem between Alice and Bob, where they agree on a shared policy to communicate about a set S and a query q. The maximum transcript size is shown to be greater than the mutual information between the inputs and transcript, indicating cases where less space may be needed than the classical lower bound, particularly when the entropy of the sets and queries is low. The experiments explore scenarios with varying levels of structure in storage sets and queries using four neural architectures: LSTM, DNC, Memory Network, and Neural Bloom Filter. A shared encoder architecture is used across all models, with specific setups for images and text. The training setup involves using a character LSTM with 128 hidden units for familiarity classification tasks. Meta-learning occurs through a two-speed process where the model quickly learns to recognize storage sets within a training episode and slowly improves this process by optimizing model parameters over multiple tasks. The comparison between a classifier, Memory Network, and Neural Bloom Filter in terms of space usage and false positive rate. The models are optimized for performance by sweeping over hyper-parameters to determine the smallest operating size at a desired false positive rate. The neural models aim to minimize false positive rates by using a backup bloom filter. This filter adds to the model's memory space for parity. The models are tested on set membership tasks using images from MNIST, with the input modality being images before moving to text. The neural models, including DNC, LSTM, and Neural Bloom Filter, outperform the classical Bloom Filter in set membership tasks using images from MNIST. Three different levels of inherent structure are experimented with in the sampling of sets and queries. Neural Bloom Filter outperforms classical Bloom Filter in set membership tasks with images from MNIST. Memory Network solves task with word size of 2, but DNC, LSTM, and Neural Bloom Filter show significant size reduction and better performance with 500 elements. Bloom Filter is preferable for less than 500 stored elements in non-uniform sampling task, but DNC, LSTM, and Neural Bloom Filter surpass it with 1000 elements. Uniform sampling task shows no structure in sampling of sets. The Neural Bloom Filter outperforms the classical Bloom Filter in set membership tasks with images from MNIST. It shows significant size reduction and better performance with 500 elements compared to DNC, LSTM, and Memory Network. The experiments reveal that the classical Bloom Filter cannot be matched by a meta-learned memory-augmented neural network when there is no structure to the data, but in cases of imbalanced or highly dependent sets, significant space savings are observed. The Neural Bloom Filter, in contrast to other memory-augmented neural networks, can organize input contents in memory. A study on the MNIST class-based familiarity task with 10 memory slots shows that the full model learns to assign specific classes to certain slots, while the write word encodes a unique token for each class. The Neural Bloom Filter organizes input contents in memory by encoding unique tokens for each class. It resembles Bloom Filters but stores unique tokens in memory slots instead of constant values. The model allocates semantic addressing codes for some classes and uses distributed addressing for others. The addresses are not linearly separable, leading to non-linear separation by the output MLP. The Neural Bloom Filter organizes input contents in memory by encoding unique tokens for each class, leading to non-linear separation by the output MLP. It corresponds to perfect hashing, mapping each input class to a unique memory slot. Inspired by database interactions, the setup emulates NoSQL databases like Bigtable and Cassandra, using string-valued row keys to index data. Bloom Filters determine query presence in stored sets, with a universe of alphabetically ordered strings sampled to represent SSTables. Models are trained with up to 200 unique tokens from the GigaWord v5 news corpus. The Neural Bloom Filter is trained with up to 200 unique strings from a universe of 2.5M words. Performance is best for larger false positive rates, with a 6\u00d7 space reduction compared to classical Bloom Filters for a storage size of 200. The LSTM shows smooth performance degradation between in-sample and out-of-sample sizes. When compared to Bloom Filters and Cuckoo Filters with a storage size of 5000, the Neural Bloom Filter shows a 3 \u2212 40\u00d7 space reduction. The LSTM was unable to learn the task with a sequence length of 5000, resulting in a 3 \u2212 40\u00d7 space reduction compared to classical Bloom Filters and Cuckoo Filters. The Neural Bloom Filter has higher query and insertion latencies but can match the maximum throughput of a Bloom Filter when run on a GPU. The Neural Bloom Filter uses a simple write scheme for large database tasks, optimizing additive write during training without BPTT. Various Bloom Filter variants exist, but few specialize to data distribution. The Neural Bloom Filter is a general solution for non-uniform data sets, allowing the encoder to learn data distribution statistics. Sterne (2012) proposes a neurally-inspired data structure using OR and AND gates instead of hash functions. The Neural Bloom Filter uses OR and AND gates to control false positive rates analytically. BID3 explores a neural familiarity module for familiarity mechanisms in the brain. BID22 uses a neural network for query classification to a fixed set S, requiring multiple epochs for a succinct representation. In contrast to neural networks, Mitzenmacher (2018a) proposes a memory-based approach for representing sets, focusing on empirical false positive rates. Mitzenmacher (2018b) suggests combining classical and learned bloom filters for efficiency, especially with larger false positive rates. Bloom Filters are considered more robust to data changes compared to neural networks. The paper introduces the concept of a Neural Bloom Filter as a potential replacement for traditional Bloom Filters. This model incorporates an external memory with adaptable capacity, avoids BPTT with a feed-forward write scheme, and learns to address its memory. The Neural Bloom Filter is seen as a promising option compared to popular memory models like DNCs and LSTMs, offering a more efficient compression method. The potential benefits of using a Neural Bloom Filter include the ability to train on offline datasets similar to future data, leading to efficient data structures for large databases. The space and time costs of the network can be offset by sharing it across multiple neural bloom filters and leveraging modern hardware for accelerated linear algebra operations. Further research is needed to explore the feasibility of implementing this approach in a production environment. A promising future direction would be to investigate the feasibility of implementing the moving ZCA approach in a production system. The moving ZCA involves computing moving averages of the first and second moment, updating a projection matrix \u03b8 zca during training, and fixing \u03b8 zca at evaluation time. The update process involves calculating singular values and updating \u03b8 every T steps to save computational resources. The decorrelation of s and sparse content-based attention with A can be seen as a hash function mapping s to indices in M. Gaussian samples in A are uniformly scattered across the surface of a sphere for moderate dimension sizes of s. The curr_chunk discusses the use of a continuous approximation for uniform hashing in Bloom Filters, allowing the network to learn query representations. Implementation tricks like using an approximate k-nearest neighbour index to avoid linear-time addressing operations are suggested for production systems. This approach differs from the moving ZCA method mentioned in the previous paragraph, which involves updating a projection matrix during training. The curr_chunk discusses using an approximate nearest neighbour index for memory-augmented neural networks to scale memory. By storing integer seeds instead of actual memory slots, the network can avoid storing fixed samples of random variables. This simplifies the architecture and reduces memory cost. The total memory cost of accessing rows in a Neural Bloom Filter with m memory slots is m bits. The Neural Bloom Filter's memory is often smaller than a classical Bloom Filter's memory, making dense matrix multiplication preferable in many cases. The model focuses on dense matrix operations and does not delve into specific optimization details. The model compares memory size at a given false positive rate. Neural networks output a probability p = f(x) and select an operating point \u03c4. The network outputs a memory state s to characterize the storage set. SPACE(f, \u03b1) is compared with SPACE(Bloom Filter, \u03b1). A backup Bloom Filter is used for false negatives, achieving a false positive rate of at most \u03b1. The overall false positive rate is (1-\u03b4), to achieve a rate of at most \u03b1 (1%), set \u03b4 = \u03b1/2. The total space for the backup bloom filter is equal to the number of false negatives, denoted nfn. For MNIST experiments, a 3-layer CNN with 64 filters followed by a two-layer feed-forward network with 64&128 hidden-layers was used. The Neural Bloom Filter has 243,437 trainable parameters, totaling 7.8Mb at 32-bit precision. The encoder architecture was not optimized for size. The Neural Bloom Filter used in the experiments has 419,339 trainable parameters totaling 13Mb. Optimization possibilities include using 16-bit precision, more convolution layers, or smaller feed-forward linear operations. Switching to a GRU or investigating temporal convolutions as encoders could also be beneficial. Hyper-parameter sweeps were conducted over a range of memory sizes for each task. The best model parameters were computed to minimize memory consumption for different tasks. Tasks included class-based familiarity, uniform sampling, and non-uniform instance-based familiarity. Training and evaluation were done on different sets, with specific sampling methods used for each task. The study focused on optimizing model parameters for various tasks such as class-based familiarity, uniform sampling, and non-uniform instance-based familiarity. Evaluation was conducted on training sets with specific sampling methods employed for each task. For the database task, a large number of unique tokens were shuffled and divided into training and test sets for analysis. The study optimized model parameters for tasks like class-based familiarity, uniform sampling, and non-uniform instance-based familiarity. Models were trained and evaluated on different sets. Sphering improved validation performance by preventing the model from fixating on specific rows in memory. This led to a more uniform query distribution, enhancing model performance. Sphering the query vector improved memory access uniformity, enhancing performance in the uniform MNIST sampling task. A comparison between classical Bloom Filter and Neural Bloom Filter showed that query latency is not a critical factor in Bloom Filter applications. The study focused on optimizing model parameters for various tasks and highlighted the trade-off between latency and space in neural networks. The Neural Bloom Filter network architecture utilizes an encoder LSTM with 256 hidden units and a two-layer 256-hidden-unit MLP for queries. Comparisons are made with an LSTM containing 32 hidden units. The network's single-query latency and throughput for batches of queries and inserts are benchmarked in TensorFlow without custom kernels or specialized code. The LSTM implemented in TensorFlow is benchmarked on both CPU and GPU, compared to a query-optimized Bloom Filter variant. The Neural Bloom Filter shows 5ms latency on the CPU, 400\u00d7 slower than the classical Bloom Filter. However, with multiple queries, batch operations improve latency to only around 20\u00d7 slower. When using a GPU, throughput increases to parity with the classical Bloom Filter, suggesting that a Neural Bloom Filter could be deployed in high query load scenarios without a significant decrease in throughput. The Neural Bloom Filter on the GPU maintains an insertion throughput of \u2248 58K with batchable operations, while the LSTM's maximum insertion throughput is only 4.6K on a powerful NVIDIA P6000 GPU. The NBF's additive write scheme proves beneficial for computational efficiency, unlike the LSTM's strictly sequential write scheme. This difference highlights the NBF's potential for high query load scenarios without a significant decrease in throughput. Training an RNN over sequences of length 5000 is difficult and slower than a Bloom Filter, even with accelerated hardware."
}