{
    "title": "r1exVhActQ",
    "content": "State-of-the-art deep neural networks (DNNs) with millions of parameters face challenges fitting into memory, increasing inference time and energy consumption. Connection pruning is a popular strategy for DNN model compression, but $\\ell_1$ regularization may not be as effective. Research revisits this strategy, showing that the number of non-zero elements in a pruned layer is bounded by the number of penalized prediction logits, regardless of regularization strength. Connection pruning is a popular strategy for compressing deep neural networks with millions of parameters. Research shows that successful pruning relies on an accurate optimization solver, with a trade-off between compression speed and prediction accuracy distortion controlled by regularization strength. Using simple $\\ell_1$ regularization with an Adamax-L1 solver can achieve competitive pruning ratios. Recent research has focused on compressing deep neural networks by trimming connections between neurons to reduce model size, addressing issues such as slow access times and high energy consumption. However, successful trimming algorithms lack theoretical guarantees, while theoretically-motivated approaches have been less competitive in practice. Theoretical understanding is needed to determine the effectiveness of pruning approaches on different tasks. Structured pruning strategies can speed up inference but may not achieve high pruning ratios. Regularized training is a natural strategy for pruning DNNs. Regularized training is a natural strategy for pruning DNNs, but recent investigations suggest it may not work well with non-shallow models. Theoretical analysis shows that 1 regularization in DNNs limits the number of non-zero parameters in each layer, emphasizing the need for an 1-friendly optimization solver for sparse solutions. Regularized training for pruning DNNs may not work well with non-shallow models. Theoretical analysis shows that 1 regularization limits non-zero parameters, requiring an optimization solver for sparse solutions. High-precision solvers can achieve high pruning ratios even with 1 regularization. Experimental results show competitive pruning with SGD pretraining and Adamax-L1. Regularized training for pruning DNNs may not work well with non-shallow models. Convolution layers with Relu activation, fully-connected layers with Relu activation, and commonly used operations like max-pooling are key components. Support Labels of a DNN X (J) are defined as indices of non-zero loss subgradient w.r.t. the prediction logit. Multiple Regression involves multiple real-valued labels. Multiple real-valued labels, such as location and orientation of objects in an image, can be represented as an N \u00d7 K matrix Y over N samples. In binary classification, labels are binary-valued and represented by a binary vector y \u2208 {\u22121, 1} N. Popular loss functions include logistic loss and hinge loss, with support labels defined as indices of non-zero loss subgradient. Hinge loss typically applies to a small portion of samples, coinciding with the concept of Support Labels. In multiclass or multilabel classification, labels are represented as a K-dimensional binary vector. Popular loss functions include cross-entropy and maximum margin loss. The maximum-margin loss typically has fewer support labels compared to cross-entropy. The section aims to address the DNN compression problem. The Deep-Trim task involves finding a compressed DNN with a limited number of non-zero parameters. This problem can be solved using 1 regularization under certain conditions, with the use of suitable optimization algorithms. The objective is to minimize a layerwise 1-regularized function for all layers of the DNN. The Deep-Trim task aims to compress a DNN with limited non-zero parameters using 1 regularization. The theorem states that most stationary points of the objective have non-zero parameters per layer bounded by the total number of support labels in the training set. The concept of general position in LASSO and sparse recovery is discussed, showing that a DNN can be trimmed without affecting training loss by choosing a small \u03bb. This is supported by Corollary 1, which guarantees compression without distortion in a DNN with limited non-zero parameters. In practice, it is crucial to choose optimization algorithms targeting high precision for convergence to the stationary point with sparse support in DNNs. Stochastic Gradient Descent (SGD) is known for its inaccuracy in optimization precision. Joint pruning of all layers has been observed to be as effective as layerwise pruning. In practice, joint pruning of all layers has been found to be as effective as layerwise pruning. A two-phase strategy is proposed for the non-convex optimization problem, where the first phase involves using Stochastic Gradient Descent (SGD) to optimize the objective function without aiming to reduce the number of non-zero parameters. In the first phase, the focus is on reducing the 1 norm of model parameters using SGD without decreasing the number of non-zero parameters. The second phase involves using an Adamax-L1 method to reduce the total number of non-zero parameters, achieving pruning results comparable to state-of-the-art methods. Various SGD-like optimizers such as Momentum, Adamax, Adam, and RMSProp are considered for optimization algorithms. The weight update process in SGD-like optimizers can be modified to include L1 penalty terms for sparsity. Combining techniques like SGD-L1 (cumulative) and Adamax-L1 (cumulative) with SGD can help achieve a sparse weight matrix. SGD-L1 (clipping) further enhances sparsity by dividing the update process into two steps, updating the L1 penalty term separately. The SGD-L1 (clipping) algorithm updates the L1 penalty separately in two steps, setting weights that change sign to 0. It can be seen as a special case of truncated gradient. SGD-L1 (cumulative) modifies this by using cumulative L1 penalty instead of standard L1 penalty to prevent weights from moving away from zero due to stochastic gradient noise. The update rule for SGD-like optimization algorithms can be written as DISPLAYFORM8 with the Adamax-L1 (cumulative) optimization algorithm. This method is compared to other state-of-the-art approaches for -regularized pruning in various experiments on different neural networks and datasets. The text discusses experiments with Resnet on CIFAR-10 and the trade-off for pruning Resnet-50 on the ILSVRC dataset. It compares different compression methods on the MNIST dataset with the LeNet-5 architecture, including pruning algorithms like Prune and DNS, as well as Variational Dropout and L1 Naive methods. Our method optimizes the 1-regularized objective in two phases (SGD and Adamax-L1). The LeNet-5 network achieves 99.2% accuracy without data augmentation. The compressed model maintains over 99% testing accuracy with significant weight and FLOP reduction. L1 Naive does not induce sparsity effectively, highlighting the importance of a L1-friendly optimization algorithm. Our method achieves significant weight and FLOP reduction on a VGG-like network with CIFAR-10 dataset. The Deep-Trim algorithm achieves a weight pruning ratio of 57\u00d7 and reduces FLOP by 7.7\u00d7 with minimal accuracy drop. Comparison with VD shows sparser weights in different layers. Notably, most remaining non-zero parameters and FLOPs are in block2 and block3 after pruning. In experiments with the CIFAR-10 dataset, the cross-entropy loss was employed with 500K support labels. The experiments were based on authors' code, tuning the coefficient of dropout regularization loss. Despite achieving a 48\u00d7 weight reduction ratio, the accuracy achieved was 92.2% instead of 92.7% as reported. The VGG-network's large parameter size allows for significant compression rates. The Deep-Trim algorithm was evaluated on a smaller Resnet-32 model trained on CIFAR-10 data, achieving a 33\u00d7 overall pruning ratio and 21\u00d7 reduced FLOP with a 1.4% accuracy drop compared to variational dropout. In this section, the pruning results of the method on VGG-16 with different samples are compared. The number of non-zero parameters after pruning increases with the number of samples, indicating a need for more model flexibility to fit the data. The theory analysis aligns well with the empirical results. In this work, the authors revisit the idea of pruning connections in DNNs through L1 regularization. They provide a theoretical analysis with small upper bounds on non-zero elements, suggesting the use of a high-precision optimization solver for better results. Experiments show state-of-the-art results with L1 regularization on different datasets and networks. Table 5 shows the architecture of Resnet-32, consisting of 3 convolutional blocks with downsampling in the first layer of each block. Global pooling and a fully-connected layer are applied after the convolutional layers. Each set of rows represents a residual block."
}