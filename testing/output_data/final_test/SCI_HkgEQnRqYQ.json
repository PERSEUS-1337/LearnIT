{
    "title": "HkgEQnRqYQ",
    "content": "The RotatE model is a new approach for knowledge graph embedding that can model various relation patterns such as symmetry, inversion, and composition. It defines relations as rotations in a complex vector space and uses a self-adversarial negative sampling technique for training. Experimental results show that RotatE outperforms existing models in link prediction on benchmark knowledge graphs. Knowledge graphs are collections of factual triplets representing relations between entities. They are used in various applications like question-answering, information retrieval, and recommender systems. Predicting missing links in incomplete knowledge graphs is a fundamental problem, leading to research on knowledge graph embedding for link prediction. This area has seen extensive studies on learning low-dimensional representations of entities and relations to predict missing links. Knowledge graph embedding methods aim to predict missing links by modeling connectivity patterns in knowledge graphs based on observed facts. These patterns include symmetry, antisymmetry, inversion, and composition of relations. Various approaches have been developed to capture these patterns for effective link prediction. The score functions of knowledge graph embedding models aim to model various patterns such as translations, three-way interactions, and symmetry. Existing models are limited in capturing all these patterns, prompting the search for a more comprehensive approach. RotatE is a knowledge graph embedding approach that models relation patterns using complex vector space and rotations. It effectively captures symmetric/antisymmetric relations by defining each relation as a rotation from the source entity to the target entity. The RotatE model effectively models symmetric/antisymmetric, inversion, and composition relation patterns using complex vector space and rotations. It remains scalable to large knowledge graphs and introduces a self-adversarial negative sampling technique for optimization. This technique can be applied to various knowledge graph embedding models and has been evaluated on four large benchmark datasets. The RotatE model outperforms existing approaches on large knowledge graph datasets and achieves state-of-the-art performance on various benchmarks. The p-norm of a complex vector is defined as v p = p |vi| p. Score functions for predicting missing links in knowledge graphs have been extensively studied. The methodology for knowledge graph representation involves defining a score function for triplets, where entities and relations are denoted as sets. The score function measures the salience of candidate triplets, aiming to score true triplets higher than false ones. Various models have been proposed, such as TransE, which represents relations as bijections between entities. RotatE combines advantages of TransE and ComplEx, modeling inversion, composition, and asymmetric relations. TorusE is a related model focusing on regularization, while RotatE emphasizes modeling and inferring composition patterns with higher representation capacity. This paper proposes the RotatE model for modeling and inferring multiple types of relation patterns in knowledge graphs. It implicitly learns relation patterns, providing meaningful embeddings for entities and relations. The problem of drawing negative samples for training knowledge graph embeddings is also addressed, with a proposed self-adversarial approach. The RotatE model is introduced to model relation patterns in knowledge graphs efficiently without the need for additional optimization components. It defines relations as rotations in a complex vector space and can infer important relation patterns such as symmetry, inversion, and composition. The text discusses the formal definitions of symmetry, inversion, and composition in knowledge graphs. It also analyzes existing models like TransE, TransX, DistMult, and ComplEx in inferring and modeling these patterns. HolE and ConvE are not included in the analysis. Our proposed model can model all three relation patterns by mapping entities to complex embeddings and defining functional mappings induced by relations as element-wise rotations. The modulus of each element of the relation is constrained to be 1, ensuring it is of the form e^i\u03b8. The proposed model, RotatE, represents relations as counterclockwise rotations in a complex vector space, allowing it to model and infer all three types of relation patterns. RotatE is the only model capable of capturing all three patterns, while TransE can handle all patterns except symmetry. RotatE is able to model and infer the symmetry pattern by using arbitrary vectors that satisfy certain conditions. It distinguishes entities with symmetric relations by using different embedding vectors. Negative sampling has been effective for optimizing distance-based models in knowledge graph and word embeddings. The negative sampling loss in RotatE optimizes distance-based models by sampling negative triplets using self-adversarial sampling, which considers the current embedding model. This approach improves efficiency by avoiding obviously false samples during training. The final negative sampling loss with self-adversarial training optimizes distance-based models by sampling negative triplets efficiently. Different approaches for negative sampling are compared on four knowledge graphs: FB15k, WN18, and FB15k-237. Key patterns for link prediction include symmetry/antisymmetry and inversion. The key patterns for link prediction on knowledge graphs like FB15k-237 and WN18RR involve symmetry/antisymmetry and composition. Hyperparameters are fine-tuned using Adam optimizer with ranges for embedding dimension, batch size, self-adversarial sampling temperature, and fixed margin. Regularization is not used in the optimization process. Relation embeddings are initialized uniformly between 0 and 2\u03c0 without regularization to prevent overfitting. Link prediction is evaluated in the filtered setting by ranking test triples against candidate triples not in the training set. Evaluation measures include Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits at N (H@N). A variant of RotatE serves as a baseline, where entity embeddings' modulus is constrained. This allows for investigating RotatE's performance without modulus information and with only phase. RotatE is compared to various models, including pRotatE, on FB15k and WN18 datasets. RotatE outperforms all models, with similar performance to pRotatE. The analysis shows consistency in modeling relation patterns across datasets. ComplEx performs well on FB15k, emphasizing the importance of modeling relation patterns for predicting missing links. On FB15k dataset, ComplEx outperforms TransE in inferring symmetry/antisymmetry and inversion patterns. DistMult achieves good performance by considering entity type differences in relations. On WN18, symmetry/antisymmetry patterns are prevalent. On WN18 dataset, ComplEx performs well due to symmetry/antisymmetry and inversion patterns. DistMult's performance decreases as it cannot model these patterns. TransE excels on FB15k-237 dataset with composition patterns, while ComplEx struggles. TransE struggles on WN18RR due to its inability to model symmetry patterns. TransE struggles on the Countries dataset as it cannot model symmetric relations well. Results from different models are compared in Table 6, and relation embedding phases are illustrated in Figure 2. The model is also evaluated on the Countries dataset BID4 BID24 to test link prediction capabilities for composition patterns. The Countries dataset is designed to test link prediction models for composition pattern modeling. It includes 2 relations and 272 entities. Tasks involve inferring composition patterns of increasing complexity. RotatE outperforms other models on the most difficult task, S3, based on AUC-PR metric. In this section, the focus is on verifying if relation patterns are implicitly represented by RotatE relation embeddings. The histogram of the phase of each element in the relation embedding is plotted, showing symmetry and inversion patterns. The embeddings of symmetric relations exhibit phases of either \u03c0 or 0, 2\u03c0, indicating a \u00b11 pattern. However, general relation embeddings do not show this \u00b11 pattern. The RotatE model analyzes inversion and composition patterns in relation embeddings. Inversion pattern requires conjugate embeddings of inverse relations, while composition pattern involves adding phases of composed relations. Results show implicit inference of basic patterns in relation embeddings. In this section, different negative sampling techniques are compared, including uniform sampling, self-adversarial technique, and the KBGAN model BID5. Results show that self-adversarial sampling is the most effective. Further experiments are conducted on TransE and ComplEx models to make a fair comparison with RotatE. The RotatE model outperforms TransE and ComplEx on FB15k and FB15k-237 datasets, as it can model all three relation patterns. However, TransE slightly outperforms RotatE on the Countries dataset for the S3 task due to the specific relation category 1-to-1. The RotatE model slightly outperforms RoateE on the S3 task in the Countries dataset due to the specific relation category 1-to-1. The TransE model does not suffer from its inability to model symmetric relations in the Countries dataset. ComplEx does not perform well on Countries as it cannot infer the composition pattern. Further investigation on RotatE's performance on different relation categories in the FB15k dataset is summarized. The results of RotatE on various relation categories in the FB15k dataset are compared with the KG2E KL BID12 approach, which models uncertainties in knowledge graphs. RotatE performs well on non-injective relations, especially many-to-many relations. The probabilistic framework KG2E KL(bern) BID12 consistently outperforms traditional knowledge graph embedding models, highlighting the importance of modeling uncertainties. Future work includes exploring uncertainties in knowledge graphs with RotatE. In our future work, we have introduced RotatE, a new knowledge graph embedding method that represents entities as complex vectors and relations as rotations in complex vector space. The RotatE model outperforms existing models on large-scale benchmarks and shows state-of-the-art results on composition pattern inference. We plan to further evaluate RotatE on additional datasets and incorporate a probabilistic framework to model uncertainties in entities and relations. Existing models struggle to capture all three relation patterns, with each model having its limitations. For example, TransE cannot model symmetry, TransX can handle symmetry/antisymmetry but struggles with inversion and composition, DistMult is challenged by asymmetric and inversion patterns, and ComplEx can infer symmetry and asymmetric patterns but not composition rules. These limitations are highlighted in the context of knowledge graph embeddings. The RotatE model addresses concerns in knowledge graph embeddings by efficiently handling relations between entities. It treats head and tail entities uniformly, allowing for easy acquisition of relation inverses. The model's distance function can be rewritten in polar form, providing insights into its behavior. The RotatE model improves knowledge graph embeddings by handling entity relations efficiently. It outperforms other models on the YAGO3-10 dataset, which focuses on entities with a minimum of 10 relations each. The model's distance function can be simplified to 2C sin, similar to TransE. RotatE outperforms state-of-the-art models on YAGO3-10. The best hyperparameter settings are listed in Table 13 for RotatE on various benchmarks. An ablation study on FB15k-237 shows that self-adversarial sampling improves performance for both RotatE and TransE models. Negative sampling loss is effective only on RotatE. The re-implementation of TransE also outperforms state-of-the-art models on FB15k-237. Average and variance of MRR results on different datasets show RotatE's stable performance across random seeds. RotatE demonstrates stable performance across random seeds, as shown by histograms of embedding phases in FIG2-5."
}