{
    "title": "HJxpDiC5tX",
    "content": "This work introduces a scalable solution for continuous visual speech recognition by creating a large dataset of text-video pairs of faces speaking. The system includes a video processing pipeline, a deep neural network for mapping lip videos to phoneme sequences, and a speech decoder that outputs words. It achieves a word error rate of 40.9%, outperforming previous lipreading approaches. Deep learning techniques have advanced lipreading, but often limited to narrow vocabularies and single-word classification. A novel method for large-vocabulary continuous visual speech recognition is introduced, showing substantial reductions in word error rate. The motivation behind this work is to assist people with speech impairments. Visual speech recognition can benefit patients with speech impairments. A novel lipreading system is proposed, utilizing a diverse dataset to transform raw video into word sequences. The system includes a data processing pipeline to create the Large-Scale Visual Speech Recognition dataset. This work aims to provide a scalable solution for lipreading and addresses the importance of this technology in medical applications. The dataset for visual speech recognition was created using computer vision and machine learning techniques, generating phoneme and lip frame sequences from YouTube videos. The system includes a data processing pipeline and a deep neural network for phoneme recognition. The curr_chunk introduces a new neural network architecture called Vision to Phoneme (V2P) for lipreading, optimized for predictive performance within computational limits. Techniques like group normalization are crucial for results. This approach combines visual speech recognition with word-level decoding, allowing for vocabulary extension without retraining the network. The trained lipreading model performs well under optimal conditions but not in other contexts, achieving a WER of 40.9%. Compared to professional lipreaders and previous state-of-the-art approaches, it outperforms with lower error rates. The model extends vocabulary without retraining the neural network, showcasing advancements in automated lipreading technology. The text discusses various approaches in lipreading technology, including using HMM models to predict sequences of visemes, words, and digits. These methods incorporate visual features extracted from mouth region images to improve performance in noisy environments. Additional works by different authors have also explored traditional speech, vision, and machine learning pipelines for lipreading. Recent advancements in deep learning have addressed challenges in generalization across speakers and extraction of motion features in lipreading technology. While most works focus on single-word classification using visual-only representations, multimodal audio-visual representations, or a combination of deep networks with traditional speech techniques, LipNet stands out as the first end-to-end model for sentence-level lipreading. It utilizes spatiotemporal convolutions and gated recurrent units (GRUs) trained with the CTC loss function, showing promising results on the GRID corpus. LipNet, utilizing CTC loss function, achieved low word error rates on the GRID corpus BID15 dataset. It outperformed human lipreaders and inspired similar architectures like BID72, BID30, and BID80. The \"Watch, Listen, Attend and Spell\" model introduced by BID80 combined sequence-to-sequence models with attention for audiovisual speech recognition. The authors evaluated WLAS using the LRS dataset, achieving 50.2% WER. They extended the work to multi-view lipreading, with WER of 62.8% for profile views and 56.4% for frontal views. BID2 presented LRS3-TED dataset, while BID0 used pre-learned features for seq2seq and CTC models, achieving WER of 57.9% and 61.8% on LRS3-TED. Other advances in the field were also noted. The V2P model achieved a Word Error Rate (WER) of 57.9% and 61.8%. It uses a network to predict phoneme distributions and is memory and computationally efficient. The data processing pipeline resulted in a larger and more diverse training dataset compared to previous efforts. The model can accommodate very large vocabularies without the need for retraining. The LSVSR dataset is significantly larger and more varied than previous datasets, with 3,886 hours of audio-video-text pairs and a vocabulary of 127,055 words. It is created using a data processing pipeline that makes use of large-scale parallel processing. The dataset used in the study is extracted from public YouTube videos, yielding 140,000 hours of audio segments paired with transcripts. Only about 2% of clips from YouTube satisfy the filtering criteria. The pipeline can be used with a trained model to predict word sequences from raw videos. The pipeline for predicting word sequences from raw videos involves a length filter and language filter. Segments from YouTube are limited to 1-12 seconds and filtered for non-English utterances. Utterances with fewer than 6 words are removed, and phoneme sequences are aligned using a standard forced alignment approach. Shot boundaries are detected and removed using a color histogram classifier, and faces are detected and tracked using FaceNet. The pipeline for predicting word sequences from raw videos involves filtering segments for non-English utterances, removing shots boundaries, and tracking faces using FaceNet. Speech segments are filtered based on video quality, frame rates, and face landmark smoothing is applied to reduce noise and improve performance. Canonicalization of faces is achieved by applying an affine transformation on landmarks from a reference model, followed by cropping around the lips. A speaking filter is then used to discard minor lip movements and non-speaking faces based on mouth openness, with low computational cost and high recall. This process was crucial for processing 16 years of audio-video-text pairs efficiently. The contribution was crucial for efficiently processing 16 years of audio-video-text pairs. V2P-Sync, a neural network architecture, verifies audio and video channel alignment using face landmark smoothing, 3D convolutions, and high resolution inputs. It utilizes longer time segments and spatiotemporal convolutions to extract temporal features, producing embeddings for audio and video inputs. The V2P model uses a neural network architecture to verify audio-video synchronization by comparing embeddings with a contrastive loss. Unlabeled data is used for training, with pairs filtered and fine-tuned for accuracy. The final model achieves 81.2% accuracy by processing audio-video pairs with sliding V2P-Sync on segments. The V2P model utilizes a neural network architecture for audio-video synchronization verification, achieving 81.2% accuracy by processing segments with sliding V2P-Sync. The model includes a 3D convolutional module for spatiotemporal feature extraction from video clips, aggregated over time with a temporal module to output phoneme distributions. Training involves the CTC loss function, and a decoder based on FSTs is used at test-time to generate word sequences from phoneme distributions. The vision module is based on a volumetric adaptation of VGG for motion feature extraction. The vision module in the V2P model focuses on spatiotemporal relationships in human visual speech recognition, with a receptive field of 11 video frames. To optimize GPU memory usage, the module consists of 5 convolutional layers with reduced filters in the first two layers and no spatial padding around the lips. The V2P model's vision module focuses on spatiotemporal relationships in human visual speech recognition with 11-frame receptive field. To optimize GPU memory usage, it uses 5 convolutional layers without spatial padding. Batch normalization is replaced with group normalization for more stable learning regardless of batch size. The temporal module in the model performs longer-scale aggregation of features over time using a stack of 3 bidirectional LSTMs with a hidden state of 768, interleaved with group normalization. The output is then fed through an MLP layer to produce a sequence of conditionally independent phoneme distributions. This model architecture differs from previous work by using LSTMs instead of GRU units and dropout. The LipNet model used GRU units and dropout, which performed poorly in preliminary experiments. LipNet has 3 convolutional layers and 3 GRU layers, while V2P is trained to predict phonemes instead of characters. CTC is a loss function for distributions over sequences of label tokens. CTC allows the model to output blank symbols and repeat consecutive symbols to align label sequences with temporal module sequences. The probability of observing a label sequence is obtained by marginalizing over all possible alignments. Autoregressive connections are prevented in CTC, so marginal distributions are produced at each timestep of the temporal module. CTC models use marginal distributions at each timestep of the temporal module, which are conditionally independent. To restore temporal dependency during testing, a beam search procedure is used with a language model. The rationale for using phonemes and CTC in speech recognition is to address uncertainty in input sounds and corresponding words. Previous work has shown issues with using CTC to model characters directly due to the conditional independence of CTC timesteps. The conditional independence of CTC timesteps leads to spurious modes in the distribution, causing issues in modeling character sequences. This is exemplified by homophones like \"fare\" and \"fair\" having equal probabilities under the maximum likelihood estimate, due to the many-to-many mapping of characters to words. The difficulty in mapping characters to words arises due to the many-to-many mapping, leading to uncertainty in visual speech recognition. Visually similar phonemes, called visemes, add to this uncertainty. Phoneme-to-viseme mappings may vary per speaker, making it challenging to incorporate this knowledge. Using phonemes allows the temporal model to focus on visual uncertainty, while word uncertainty can be handled by the decoder. The decoder described focuses on visual uncertainty, while word uncertainty can be handled separately. Phonemes are chosen over words or characters for performance reasons and easier model training, providing extra flexibility with CTC. The decoder allows for easy changes to vocabulary and language model, enabling visual speech recognition in specific domains without retraining. It uses a combination of three weighted finite state transducers for decoding phoneme distributions into word sequences. The decoder utilizes three weighted finite state transducers for converting phoneme distributions into word sequences. It employs a 5-gram language model with Katz backoff, consisting of 50 million n-grams and a vocabulary of one million. The system is evaluated on a test set containing 63,000 video frames and 7100 words, showing significant performance improvements over professional lipreaders and previous methods. Our system outperforms professional lipreaders and previous methods for visual speech recognition. Validation and test sets were created by removing blurry videos. Experiments were conducted with and without context, using modified clips with the whole head cropped. Lipreaders viewed the videos multiple times to measure difficulty. The lipreaders viewed videos multiple times to measure performance with and without context. Clips were selected based on transcript length, and lipreaders transcribed a subset of the test set. Performance was also evaluated using an audio speech recognition model and a character-level CTC architecture. The phoneme models in the study use a character-level lexicon instead of a phoneme-based one. LipNet is trained to predict phonemes with CTC and a FST-based decoding pipeline. V2P utilizes group normalization for small batches per worker. A comparison is made with a variant of the WAS sequence-to-sequence architecture that predicts character sequences. Training end-to-end exceeded GPU memory limitations, leading to adjustments in the implementation. The authors addressed limitations of modern GPUs by using a pretrained network for fixed convolutional weights. They replaced the 2D convolutional network with a 3D visual processing network for improved performance. Reranking was done using a 5-gram word LM after standard beam search decoding. V2P-FullyConv replaced LSTMs with dilated temporal convolution layers for a fully convolutional model. V2P-NoLM is identical to V2P. The LipNet model variants V2P-FullyConv and V2P-NoLM were tested, with V2P-NoLM using a dictionary of 100k words weighted by frequency during decoding. Results show LipNet performing comparably to professional lipreaders with word error rates of 86.4% and 89.8%. The V2P method presented in the professional context achieved a word error rate of 40.9%, significantly outperforming previous methods. The fully-convolutional network also showed improved performance compared to bi-directional LSTM. The V2P model outperforms previous methods with a word error rate of 40.9%. Removing the language model only results in a 13 WER drop to 53.6%. Predicting phonemes directly avoids the need for phoneme-to-viseme mappings. Errors commonly include occluded phonemes like /d/, /n/, /t/, and the vowel /@/. The V2P model demonstrates superior performance with a word error rate of 40.9%. Directly predicting phonemes eliminates the need for phoneme-to-viseme mappings. The approach outperforms a state-of-the-art model on the LRS3-TED dataset, even without specific training on that dataset. The V2P model outperforms TM-seq2seq on the LRS3-TED dataset, achieving WERs of 47.0 \u00b1 1.6 and 55.1 \u00b1 0.9 on filtered and unfiltered test sets respectively. The approach shows superior generalization and scalability in visual speech recognition. The scalable model for producing phoneme and word sequences from video clips significantly improves lipreading performance, potentially enhancing automatic speech recognition systems and benefiting speech impaired patients worldwide. Lip reading technology could provide alternative communication strategies for those with communication problems. Aphonia is the inability to produce voiced sound, often caused by injury or disorders of the larynx. Dysphonia is difficulty in speaking due to physical disorders of the mouth, throat, or vocal cords, with patients retaining some ability to speak. Spasmodic dysphonia, for example, causes breaks or interruptions in the voice. Patients undergoing tracheostomy procedures may experience aphonia or dysphonia, impacting their ability to communicate. This can be especially challenging in acute care settings where procedures are often unplanned, leaving little time to prepare patients for the loss of voice. Alternative communication strategies may be necessary for these patients. Some conditions like high spinal cord injuries can lead to tracheotomy, affecting limb function and alternative communication methods. Patients undergoing procedures for head and neck cancers may experience frustration from loss of voice, even after consultation with a speech therapist. While some patients may adapt to different communication methods, many struggle to achieve functional spoken communication. Patients with communication disabilities, such as speech impairment or aphonia, face challenges in clinical and community settings. In acute care, these patients may experience distress from sudden inability to communicate effectively, leading to potential quality of care issues. Discharged patients without the ability to speak or with poor speech quality also encounter difficulties in the community. Patients with communication disabilities, such as speech impairment or aphonia, face challenges in day-to-day life, limiting their independence and social functioning. Technology capable of lip-reading individuals with facial muscle movement but without audible speech could greatly improve their quality of life, enhancing social interactions and job opportunities. This technology could also benefit patients unable to speak or move their arms by improving communication speed compared to current approaches. The confusion matrix and insertion/deletion chart in FIG4 were computed by analyzing the edit distance dynamic programming matrix between predicted phoneme sequences and ground-truth. The diagonal is dominant, with some phoneme groups commonly confused due to visual similarity. The chart in FIG4 shows commonly omitted or erroneously inserted phonemes. TAB3 displays the insertions, deletions, and substitutions for the Audio-Ph and V2P models, showing higher rates when using the visual channel. TAB4 optimization details include batch size, batch normalization, and Adam BID28 with specific hyperparameters. The text chunk describes the architecture and hyperparameters of a neural network model, including convolutional and pooling layers, as well as fully connected layers. It also mentions momentum coefficients, numerical stability, a curriculum schedule for training, and image transformations for data augmentation."
}