{
    "title": "Byldr3RqKX",
    "content": "Deep generative models like GANs and VAEs are complex and function as black box models. Analyzing their modularity through counterfactual manipulation of internal variables helps understand their operation. Experiments with VAEs and GANs show that modularity between activation maps in generator architectures allows for meaningful transformations of generated images without additional training. Deep generative models like GANs and VAEs use complex neural architectures to learn data distributions. They relate to the mapping of scenes to images and have emerged as state-of-the-art approaches in computer vision. Inverse optics, also known as the converse mapping, is a key concept in computer vision algorithms. While forward optics can be described concisely with a set of equations based on physical parameters, inverse optics relies on prior assumptions and numerical solutions. This asymmetry has implications for efficiently manipulating and updating internal representations in interactions with the outside world. The complexity of deep architectures makes it challenging to interact with the generative process, as they mostly behave as black boxes. Modularity of generative models should be enforced to understand and manipulate representations, but directly influencing the learned generative process is difficult due to the network's complexity. Additional training or complex manipulation of the network's input or output may be required to achieve specific results. The complexity of deep architectures makes it challenging to interact with the generative process. Modularity of generative models should be enforced to understand and manipulate representations. Intervening on parts of the architecture implementing the generative function can help uncover a modular structure within the network. This modularity can be quantified and exploited in a causal framework to infer whether modules can be further disentangled. The principle of modularity in generative models allows for evaluating the role of internal variables using counterfactuals. This perspective extends the concept of disentangled representation in deep learning. Tools are introduced to analyze modularity in VAEs and GANs trained on human face datasets, revealing a modular structure in these systems. GANs trained on human face datasets show modularity with intermediate activation maps encoding different parts of generated images. Interpretability in CNNs has been extensively studied, focusing on discriminative networks. Efforts include finding optimal activation patterns for filters, correlating intermediate feature space with data features, and disentangling patterns detected by filters. Capsule networks have also been used to enforce modularity. The recent InfoGAN network and other works in disentanglement of latent variables in generative models can be seen as extrinsic disentanglement. Our intrinsic disentanglement perspective should complement such approaches and is not in direct competition. Our approach relates to modularity and invariance principles in the field of causality, particularly formalized by BID0. We introduce a general formalism to perform interventions inside a generative network to isolate a modular functional structure in deep networks. The text introduces a formalism for interventions in generative networks based on causal generative models. Causal models, described by Structural Equations, allow for robust predictions of interventions and counterfactuals. This formalism isolates a modular functional structure in deep networks, complementing approaches in disentanglement of latent variables. The text introduces a formalism for interventions in generative networks based on causal generative models. It discusses Structural Causal Models and their use in representing complex systems with interdependent modules. The model includes a distribution over latent variables, structural equations for endogenous random variables, and deterministic mappings for generating outputs. The latent variables take values on intervals with a non-vanishing density. The text defines a causal generative model where latent variables have non-vanishing density on intervals. It aligns with the probabilistic Structural Causal Model definition and describes the functional structure of generative models. CGMs have specificities compared to classical SCMs, such as deterministic mappings and probability distributions on exogenous variables. In contrast to classical SCMs, CGMs have unique features. Latent variables in CGMs correspond to exogenous variables in structural equations and are assumed to be \"observed\". Observed variables consist of output I and endogenous variables, representing internal computations between input and output. The granularity of partitioning internal values into endogenous variables is left open. The activation of a single neuron or channel is important in generative models to approximate the data distribution. The causal framework allows for defining interventions and counterfactuals in the context of CGMs. In the context of generative models, interventions and counterfactuals can be defined using a CGM. By replacing structural assignments with constant values for endogenous variables, the unit-level counterfactual output can be determined. This definition aligns with the concept of potential outcomes and can be extended to more complex assignments. Faithfulness in generative models considers interventions on internal variables and their impact on model outputs. To prevent artifactual outputs, functions may need restrictions on output values. Model modularity can take various forms, with the concept of disentangled networks being formally defined. Extrinsic disentanglement in generative models involves an endomorphism T on latent variables, ensuring that only a subset of variables is affected in the generated image. The sparsity of disentanglement is determined by the minimum size of this subset. This concept allows for interventions on the model, illustrated by a transformation that only impacts specific latent variables. In generative models, a transformation T affects only a subset of latent variables, leading to a 1-disentangled output. This concept allows for interpretable changes in generated objects while maintaining sparsity in disentanglement. The challenge is to reflect these changes in the content of objects while keeping disentanglement sparse. In generative models, a transformation T affects only a subset of latent variables, leading to a 1-disentangled output. This concept allows for interpretable changes in generated objects while maintaining sparsity in disentanglement. The intrinsic disentanglement of a CGM M with respect to endomorphism T and subset of endogenous variables E is illustrated in FIG0, showing how T affects values of endogenous variables. This directly relates to a causal interpretation of the generative model and its robustness to perturbation of subsystems. The absence of significant variations in Z 3 in the training set may lead to poor performance of the estimation algorithm. Transformations pairs emulate changes in unaccounted latent variables to assess the robustness of the fitted causal structure. The endomorphism assumption is crucial for generative models to ensure the output remains within the model's image. Faithful counterfactuals are relevant examples of this concept. Faithful counterfactuals are relevant examples in causal generative models. Proposition 1 states that if a counterfactual intervention is faithful, then the model is intrinsically disentangled with respect to the transformation. The faithfulness assumption ensures modularity for the subset indexed by E. Quantifying modularity in deep generative networks poses challenges as modules are not easily identifiable beyond successive layers. Statistical dependencies between nodes are deterministic, making it difficult to determine intrinsic disentanglement. Choosing transformations without priors on network organization is complex, as activations are constrained by synaptic weights and non-linearities. The precise arrangement of upstream synaptic weights and non-linearities in deep generative networks can impact the output. To avoid issues with internal variables leaving their domain, variations across samples are relied upon. By generating values from regular sampling of latent variables, hybrid samples can be created. This approach is particularly useful in feed-forward multilayer neural networks. The hybridization procedure involves taking independent samples of latent variables to generate original samples of the output. By recording tuples of values from different endogenous variables, a modular structure can be identified. This allows for the creation of hybrid samples by mixing features from the original samples and feeding them into the generator network. The hybridization procedure involves generating hybrid samples by mixing features from original samples of the output. This is done by intervening on endogenous variables and creating a counterfactual hybridization using latent samples in a CGM. The counterfactual hybridization framework assesses how internal variables affect the generator's output by quantifying causal effects. This is done by generating pairs of latent space vectors and calculating the mean absolute effect to estimate an influence map. The difference in the parenthesis represents a unit-level causal effect, similar to computing the average treatment effect. The counterfactual hybridization framework evaluates internal variables' impact on the generator's output by quantifying causal effects. This involves generating latent space vectors and calculating the mean absolute effect to estimate an influence map. The approach involves taking the absolute value of unit-level causal effects and averaging the results over different treatments. The challenge lies in selecting modules to intervene on, especially in networks with many units or channels per layer. A fine to coarse approach is proposed to extract intervention groups, particularly in convolutional layers. The counterfactual hybridization framework evaluates internal variables' impact on the generator's output by quantifying causal effects. Influence maps are created for individual output channels of convolutional layers, which are then grouped by similarity to define modules at a coarser scale. Representative influence maps for channels of a VAE trained on the CelebA face dataset suggest functional segregation, with some channels influencing finer face features while others affect the background or hair. Clustering of channels using their influence maps as feature vectors is done to achieve grouping in an unsupervised manner. The influence maps are pre-processed by smoothing and thresholding, then fed into a non-negative matrix factorization algorithm to obtain cluster template patterns. Each influence map is assigned a cluster based on the template pattern with the highest weight. The choice of NMF is justified by its success in isolating meaningful parts of images. Our approach successfully isolates meaningful parts of images in different components. We compared it to the classical k-means clustering algorithm on the CelebFaces Attributes Dataset using VAE and DCGAN architectures. The VAE structure includes coarse, intermediate, fine, and image levels. Original samples are generated by passing a real image through the encoder. The full procedure involves influence maps calculation and clustering of channels into modules. Architecture details are provided in the supplemental material. The previous section involved influence maps calculation, clustering channels into modules, and hybridization at the module level. Hybridization procedures are typically done at the output of the intermediate convolutional layer. Setting the number of clusters to 3 resulted in interpretable cluster templates for background, face, and hair. Cluster stability analysis confirmed this observation by maximizing label consistency between clustering outcomes. The clustering analysis showed that using 3 clusters resulted in high consistency (>90%) and outperformed k-means clustering. The average cosine similarity between matching clusters was .9, indicating strong similarity. Influence maps in FIG2 also supported these findings. The clustering analysis revealed high consistency with 3 clusters, outperforming k-means. Influence maps in FIG2 confirmed these results, showing how the magnitude of causal effects decreases with coarser modules. The causal effect decreases with the number of clusters and is weakly influenced by the layer intervened on. The influence of a module reflects the proportion of channels belonging to it. The magnitude of causal effects is distributed heterogeneously across modules, especially in layer 3. The heterogeneity of causal influence across modules is more pronounced in layer 3, leading to a poorer fit of the linear regression model. Hybridization of modules results in targeted feature replacement while maintaining overall image structure. Notable observations from these experiments are consistent with reasonable model parameter choices. By decreasing the \u03b2 parameter, the quality of reconstructed images improves, but with a slightly more artificial hybridization. The approach was also applied to GANs on the CelebA dataset, showing optimal results with three clusters. However, the GAN clusters are less aligned with high-level concepts. The paper introduces a methodology to assess modularity in deep networks, focusing on features of images that occur in specific parts. Preliminary experiments on the CIFAR10 dataset showed challenging clustering adjustments, with influence maps associated to foreground and background objects. The GAN clusters are not aligned with high-level concepts, but still produce visually good hybrid samples. The paper discusses the limitations of spatial alignment assumptions in datasets like CelebA and CIFAR10. It suggests using multidimensional techniques like Principal Component Analysis for a more general approach. Optimizing modularity in deep generative networks and generating relevant modules are areas for further research. The paper discusses limitations in datasets like CelebA and CIFAR10, suggesting the use of techniques like Principal Component Analysis. Improving modularity in generative models may require specific learning objectives and architecture choices. The VAE architecture for CelebA is shown in Fig. 4, with network hyperparameters in Table 1."
}