{
    "title": "SJa9iHgAZ",
    "content": "Residual networks (Resnets) are a prominent architecture in deep learning, with ongoing research to understand their properties. A recent view suggests that Resnets perform iterative refinement of features, which is further studied analytically and empirically. Residual architectures naturally encourage features to move along the negative gradient of loss during the feedforward phase. Empirical analysis shows that Resnets can perform both representation learning and iterative refinement, with lower layers focusing on representation learning and higher layers on refinement. Sharing residual layers without caution can lead to representation explosion and hinder generalization performance. Residual networks address the issue of vanishing and exploding gradients in deep neural networks by introducing residual layers. However, sharing these layers without caution can lead to representation explosion and hurt generalization performance. Various strategies such as better activations, weight initialization methods, and normalization methods have been proposed to alleviate this problem. Training deep compositional networks deeper than 15-20 layers remains a challenging task. Residual networks, introduced as a breakthrough in deep learning, can learn very deep networks and achieve top performance. Their unique additive framework allows for the removal or shuffling of blocks without significant impact. Two perspectives on Resnets have emerged: the ensemble view sees them as learning an exponential ensemble of shallower models, while the unrolled iterative estimation view suggests Resnet layers iteratively refine representations. Our work aims to deepen the understanding of Residual Networks (Resnets) by analyzing their iterative feature refinement perspective. We use Taylor's expansion to show that each residual block encourages representations to move along the negative gradient of the loss, leading to gradient steps for minimizing loss in hidden representation space. Empirical evidence supports this concept by measuring the cosine similarity between the output of a residual block and the gradient of loss with respect to hidden representations. In Residual Networks (Resnets), lower blocks focus on representation learning while higher blocks emphasize iterative feature refinement. Shortcut connections play a dominant role in representation learning, with most residual blocks refining features iteratively. The iterative refinement view suggests that deep networks can leverage parameter sharing for iterative inference. However, sharing a large number of residual blocks without loss of performance has not been achieved. Two ways of reusing residual blocks are studied: sharing during training and unrolling for more steps than trained. Naively sharing blocks leads to poor performance, with reasons explored and a preliminary fix proposed. Recent papers suggest Resnets are an ensemble of shallow networks, based on an unraveled view of Resnets. Residual networks, like Resnets, are seen as ensembles of shallow networks with an exponential number of paths between input and prediction layers. The mathematical formulation of Resnets shows similarities to LSTM, and successive layers cooperate to preserve feature identity. Resnets have also been studied from the perspective of boosting theory. Our work focuses on a precise definition of iterative inference, showing that a residual block approximates a gradient descent step in the activation space. This relates to the gap between boosting and iterative inference interpretations. Humans often make predictions with iterative refinement based on task difficulty. The visual cortex is believed to perform fast feedforward inference for easy stimuli and iterative refinement for complex stimuli through lateral connections within individual layers. The brain uses lateral connections within layers to iteratively update and make predictions on complex tasks. This mechanism involves recursive application of shared weights, similar to a recurrent model. Residual blocks in deep network models perform parameter sharing for iterative inference. Resnets with convolution layers followed by L residual blocks learn additive representations, contrasting traditional compositional networks. The first hidden layer is a convolution layer followed by L residual blocks with or without shortcut connections. Residual blocks transform representations recursively, expanding the loss function around each block to minimize the dot product between F(h_i) and other terms. The loss function in Resnets minimizes the dot product between F(h_i) and other terms, moving down the energy surface during iterative inference. Residual blocks resemble stochastic gradient descent. The Resnets loss function minimizes the dot product between F(h_i) and other terms, resembling stochastic gradient descent. The study aims to validate that residual networks perform iterative refinement and addresses questions about the behavior of residual blocks in Resnets. Four architectures are used for analysis, including the Original Resnet-110 architecture with 54 residual blocks in three stages. The Resnet architecture consists of 16 filters and 54 residual blocks in three stages, each with different filter sizes. Shortcut connections are inserted after certain blocks to change hidden space dimensionality. The model has a total of 1,742,762 parameters. Another architecture, Avg-pooling Resnet, repeats the blocks three times with average pooling layers reducing height and width after each stage. The avg-pooling architecture uses 150 filters in all convolution layers and has 12,201,310 parameters. Wide Resnet starts with a 3x3 convolution layer followed by 3 stages of four residual blocks with 160, 320, and 640 filters respectively, totaling 45,732,842 parameters. In this experiment, the classifier is validated by computing the cosine loss for Resnets on CIFAR-10. The results show consistently negative cosine loss for all residual blocks, especially for the higher ones. This suggests that features are refined by moving them in the direction that reduces the loss value for the corresponding data samples. The higher residual blocks in Resnets achieve more negative cosine loss, indicating iterative refinement of features by moving representations in the half space of the negative cosine loss. This behavior formalizes how residual blocks iteratively refine features. The 2 ratio of F i (h i ) 2 / h i 2 averaged across samples shows how significantly F i (.) changes the representation h i. In CIFAR-10, the first few residual blocks in single representation Resnet and pooling Resnet change representations significantly, while the higher blocks show less significant changes. This effect is not as drastic in original Resnet and wide Resnet architectures. The residual network suggests that lower residual blocks may need to change representations significantly when the network lacks enough compositional layers. Dropping individual residual blocks from trained Resnets shows their significance towards final accuracy.Adjacent blocks operate in the same feature space, allowing for this analysis. Dropping individual residual blocks from trained Resnets significantly affects final accuracy. Performance drops are drastic for single representation Resnet and pooling Resnet when dropping the first few residual layers. However, dropping higher residual layers has minimal effect. Validation accuracy gradually increases when adding more residual blocks during training. The study investigates the impact of adding more residual blocks in the last stage of architectures on accuracy. Individual residual blocks show small performance improvements, suggesting they affect samples near the decision boundary. Borderline examples, requiring minimal probability change to flip predictions, are analyzed in the last 5 blocks of the network using the final classifier. The experiment is conducted on CIFAR-10 with Resnet-110 architecture, showing the evolution of loss and accuracy on three groups. The study explores the impact of adding more residual blocks in the last stage of architectures on accuracy, focusing on borderline examples. The evolution of loss and accuracy on three groups is analyzed in CIFAR-10 using Resnet-110 architecture. While train loss drops uniformly across layers, test set loss increases after the last block, indicating a need for improved generalization in Resnets. Unrolling the last block for more steps is discussed as a potential area for future work. Unrolling the last block of a trained Resnet-110 for 20 extra steps led to activation explosion, which was controlled by adding a scaling factor. The study tracked the impact on loss and accuracy, showing an improvement in train set loss but an increase in test set loss. Additionally, there were 51 borderline examples on average in the test set. Unrolling the last block of a trained Resnet-110 for additional steps improved loss on the train set and maintained negative cosine loss on both train and test sets. Sharing top residual blocks in a shared version of the model could lead to similar iterative refinement. In a shared version of Resnet-110, sharing all residual blocks from the 5th block leads to bad performance, especially for deeper Resnets. The shared model has approximately 3 times fewer parameters than the unshared version. Naively sharing parameters of the top residual blocks results in overfitting and underfitting compared to Resnet-110. The shared Resnet-38 model experiences worse validation performance despite similar training accuracy due to exploding activations and gradients at initialization. To address this, a variant of recurrent batch normalization is introduced, leading to improvements over the baseline with a similar number of parameters. Ablation studies show that all modifications to the strategy are necessary to reduce the initial activation effectively. The unshared Batch Normalization strategy mitigates the exploding activation problem in Resnets, similar to the recurrent neural network issue of exploding gradients. Insights from recurrent networks optimization can be applied to future unrolled Resnets. Residual blocks in Resnets encourage representations to move in the direction of negative loss gradient, implementing a gradient descent. Residual blocks in Resnets encourage representations to move in the direction of negative loss gradient, implementing a gradient descent in the activation space. The study validates this theory experimentally on various Resnet architectures and explores sharing blocks in Resnet. It is found that Resnet can be unrolled to more steps than trained on, but training residual blocks with shared blocks leads to overfitting. A variant of batch normalization is proposed to address this issue, with further investigation left for future work. The developed formal view and practical results aim to aid analysis of models using iterative inference and residual connections. The study explores the concept of moving representations in the direction of negative loss gradient in Resnets through residual blocks. Experimental validation is done on various Resnet architectures, with findings on sharing blocks and overfitting. A variant of batch normalization is proposed to address overfitting. The experiments for CIFAR-100 dataset are reported, with plots showing accuracy, cosine loss, and 2 ratio metrics for each residual block. In this section, the study extends results from previous experiments on Resnets. It reports on the evolution of cosine loss and 2 ratio for Resnet-110 with naively shared Resnet and unshared batch normalization. The accuracy after individual residual blocks gradually increases from lower to higher blocks, while cosine loss remains consistently negative. The 2 ratio tends to increase as training progresses. The study extends previous experiments on Resnets, focusing on the evolution of cosine loss and 2 ratio for Resnet-110 with unrolled last block for 20 additional steps."
}