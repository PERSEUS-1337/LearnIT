{
    "title": "BJl_VnR9Km",
    "content": "In this paper, a Hierarchical Prediction Network (HPNet) model was developed to understand how spatiotemporal memories are learned and encoded in a representational hierarchy for predicting future video frames. The model is inspired by the mammalian hierarchical visual system and contains feedforward, feedback, and lateral recurrent circuits. It encodes spatiotemporal features of increasing complexity and integrates signals from different levels through recurrent gated circuits to generate predictions. The network learns by comparing incoming signals. The developed Hierarchical Prediction Network (HPNet) model processes data in blocks of video frames to learn movement patterns and make predictions. The network compares incoming signals with its predictions, updating its internal model to minimize errors. Neurophysiological evidence shows that neurons in the early visual cortex exhibit similar sensitivity, suggesting the importance of predictive self-supervised learning. The visual cortex may rely on predictive self-supervised learning for representational learning, with memories stored in sensory areas. Neurons in the visual cortex encode memories of object images and visual sequences, serving as internal models for predicting visual experiences. Learning to predict incoming visual signals is proposed as a key objective for representation learning. In self-supervised learning, signals drive representation learning in recurrent neural networks without the need for labeled data. Hierarchical models like PredNet and PredRNN++ in computer vision use predictive coding principles to efficiently predict video sequences. PredNet learns LSTM models at each level to predict errors in the visual system, building a hierarchical representation for prediction. The proposed model, HPNet (Hierarchical Prediction Network), utilizes a hierarchical network architecture with a fast feedforward path and a feedback path intersecting at each level through a gated recurrent circuit for long-range video predictions. HPNet utilizes a hierarchical network architecture with a gated recurrent circuit, such as LSTM, to integrate top-down, bottom-up, and horizontal information for prediction. Prediction errors are used to influence interpretation at each level. HPNet processes data in spatiotemporal blocks using 3D convolutional LSTM for better long-range video prediction performance. Our study demonstrates HPNet's effectiveness in predictive learning and its competency in long-range video prediction. Neurophysiological evidence shows that neurons in the early visual cortex exhibit sensitivity to memories of global movement patterns similar to units in HPNet. This suggests that predictive self-supervised learning is crucial for representation learning in the visual cortex. HPNet serves as a viable computational model for understanding computation in visual cortical circuits and developing a hierarchical cortical model for predictive learning of spatiotemporal memories. HPNet is a deep learning model that integrates ideas of predictive and associative coding, learning a hierarchy of feature representations in the feedforward path. It differs from predictive coding models by focusing on modeling features in the world like normal deep convolutional neural networks. PredNet, on the other hand, builds a hierarchy to model prediction errors of its own predictions. The predictive learning models like HPNet and PredNet focus on hierarchical feature representation for better video prediction. Unlike earlier models, they use local gated recurrent circuits at each level for synthesis of expectation, making them more powerful in solving computer vision problems. The idea of predictive learning in computer vision involves using incoming video frames as teaching labels to train recurrent networks. Self-supervised learning has been actively explored in this area, particularly in video prediction research. Various models, including autoencoders, DCNN, and hierarchy of LSTMs, have been developed, with some incorporating feedforward and feedback paths. PredRNN++ is considered the state-of-the-art hierarchical model for video prediction, utilizing a stack of LSTM for feedforward input. PredRNN++ is a hierarchical model for video prediction that utilizes a stack of LSTM layers to predict the next video frame. It functions similar to an autoencoder, with intermediate layers capturing abstract movement patterns and subsequent layers forming a feedforward network. While not claiming neural plausibility, PredRNN++ offers top performance in benchmark evaluations compared to other approaches. The novel neurophysiological experiment demonstrated prediction suppression effects in the early visual cortex of monkeys for well-learned videos, indicating neuronal sensitivity to memories of global movement patterns and scene context. The HPNet model consists of Cortical Modules representing visual areas along the primate visual system's ventral stream. The network includes a deep convolutional neural network (DCNN) and Long Short Term Memory (LSTM) modules for processing visual information. The HPNet model incorporates a deep convolutional neural network (DCNN) and Long Short Term Memory (LSTM) modules to process visual information. The DCNN and LSTM modules are stacked together, with the feedforward path performing convolution on input data and the feedback path utilizing LSTM modules to generate new hypotheses based on prediction errors. The HPNet model combines a deep convolutional neural network (DCNN) and Long Short Term Memory (LSTM) modules to process visual information. The LSTM at each time step integrates feature input to recover the representation at the current frame, allowing for fast computation on sparse input. The input data consists of a sequence of video frames or a spatiotemporal block, with each block containing 5 video frames. The LSTM in the HPNet model integrates feature input at each time step to recover the representation at the current frame. The convolutional LSTM BID41 has greatly improved performance in video prediction models by replacing the Hadamard product with convolution. Earlier models processed video sequences frame by frame, while the Block-to-Frame approach uses a block of frames to generate a predicted future frame. The Block-to-Block (B-B) approach in video prediction uses a spatiotemporal block to predict another block, improving long-range predictions by learning movement relationships. The 3D convolutional LSTM algorithm details are in Appendix A, trained by minimizing a loss function weighted sum of prediction errors. The non-linearity is set at the maximum pixel value in the algorithm. The model's performance is evaluated in video prediction using Moving-MNIST and KTH datasets. The impact of recurrent network structures on feedforward representation is investigated. Neuronal activities in the network model are compared with those in the visual cortex of monkeys for video sequence learning. The plausibility of HPNet is evaluated by comparing it with PredNet and PredRNN++. HPNet's performance is compared with PredNet and PredRNN++ for video prediction. Three versions of the network are implemented for comparison: Frame-to-Frame (F-F), Block-to-Frame (B-F), and Block-to-Block (B-B). The networks are trained using 40-frame sequences and their performance in predicting the next 20 frames is evaluated. The models tested for video prediction used four modules (layers) with the same number of feature channels in each layer. The kernel sizes were either 3\u00d73 (for F-F) or 3\u00d73\u00d73 (for B-F and B-B) for all models. The input for predicting future frames involved making the prediction of the last time step the next input. The models were trained and tested on GeForce GTX TITAN X GPUs using 3\u00d73 or 3\u00d73\u00d73 kernel sizes. Evaluation was based on Mean-Squared Error (MSE) and Structural Similarity Index Measure (SSIM) BID50. SSIM values range from -1 to 1, with higher values indicating greater similarity between predicted and actual frames. Video sequences from Moving MNIST 2 dataset were used, with 40-frame sequences extracted randomly 15000 times for training. The models were trained and tested on GeForce GTX TITAN X GPUs using 3\u00d73 or 3\u00d73\u00d73 kernel sizes. Evaluation was based on Mean-Squared Error (MSE) and Structural Similarity Index Measure (SSIM) BID50. The extraction process resulted in a training set of 10000 sequences, a validation set of 2000 sequences, and a testing set of 3000 sequences. Results of different models on the Moving-MNIST dataset are compared, showing B-F outperforming B-B in short term prediction but B-B excelling in longer range prediction. The 3D convLSTM learns relationships at movement levels, with B-F performing better than F-F, indicating the spatiotemporal block data structure provides additional information. Even F-F achieved better prediction results than PredNet, suggesting a feature hierarchy may be beneficial. The BID40 network outperformed PredRNN++ on the KTH video database, which includes sequences of human actions. The video clips were divided into training and test sets for evaluation. Different models were compared based on predicted frames, with F-F performing better than PredNet. The frames were center-cropped and resized for analysis. The BID40 network outperformed PredRNN++ on the KTH video database for human actions. Different models were compared based on predicted frames, with B-B performing best in long-range video prediction tasks. Hierarchical feedback in HPNet enhances representations in early Cortical Modules for global movement and image patterns. In neurophysiological experiments, monkeys implanted with multielectrode arrays were studied to observe the effect of unsupervised learning of video sequences on early visual cortical representations. The monkeys were shown 40 video clips of natural scenes with global movement patterns, with each clip lasting 800 ms. The monkeys were shown 40 video clips of natural scenes with global movement patterns, each lasting 800 ms. The clips were presented once in a random interleaved fashion in a block of trials, repeated 20-25 times each day. Among the 40 clips, twenty were the same each day (Predicted set) and twenty were different each day (Unpredicted set). The experimental design aimed to ensure that neurons experienced about 400 movie frames for each set, with temporal responses of all neurons to each set expected to be roughly the same. After three days of unsupervised training, neurons started to respond less to predicted movies than to novel movies around 100 ms post-stimulus onset. The familiarity suppression index of all neurons decreased as they became more familiar with the Predicted set, showing a prediction suppression effect. After three days of unsupervised training, neurons showed a prediction suppression effect as they became more familiar with the Predicted set. The effect increased and saturated around the sixth and seventh days of the experiment. Results were consistent across six repetitions in two monkeys. Neurons exhibited sensitivity to global movement patterns when video clips were shown through a smaller aperture, indicating adaptation beyond local receptive fields. After unsupervised training, neurons exhibited prediction suppression effect with global movement patterns. The network was trained with Predicted set for 2000 epochs, resulting in all types of units showing suppression effect. The prediction suppression effect was observed in all types of neurons in the hierarchy, with higher modules showing a stronger effect. Neurons representing features also exhibited prediction suppression, suggesting that predictive self-supervised learning may explain common observations in the visual cortex. The hierarchical prediction network model (HPNet) integrates predictive self-supervised learning with additional neural constraints to model the mammalian visual system's architecture. It incorporates spatiotemporal processing, feature hierarchy, and sparse convolution for state-of-the-art performance. The hierarchical prediction network model (HPNet) integrates predictive self-supervised learning with additional neural constraints to model the mammalian visual system's architecture. It incorporates spatiotemporal processing, feature hierarchy, and sparse convolution for state-of-the-art performance in long-range video prediction. The hierarchical interaction in HPNet introduces sensitivity to global movement patterns in the earliest module, similar to real cortical neurons in awake monkeys. This supports predictive self-supervised learning as crucial for representation learning in the visual cortex and suggests HPNet as a computational model for understanding cortical circuits in the hierarchical visual system. Further evaluations are needed to determine whether PredNet or HPNet is a better fit to biological reality. The 3D convolutional LSTM is used in the unit of spatiotemporal blocks with input video dimensions specified as c\u00d7d\u00d7h\u00d7w. The convolution kernel size is m \u00d7 k \u00d7 k, with a spatial stride of 1 and output size of n \u00d7 d \u00d7 h \u00d7 w. The model is defined by equations involving inputs X, cell states C, outputs H, and gates i, f, o. The 3D convolutional LSTM model uses spatiotemporal blocks with specific input video dimensions. The model involves equations with inputs X, cell states C, outputs H, and gates i, f, o. In the main text of the paper, figures show that stacking higher order modules in the hierarchy leads to more pronounced semantic clustering into movement classes. Linear decoding analysis reveals improved accuracy in distinguishing semantic clusters in different modules of the network. The 4-module HPNet achieves 63% accuracy in classifying movement classes, suggesting it learns semantic representations. Decoding results show PredRNN++ lacks strong semantic clustering. The network behaves like an autoencoder with two layers of feature abstraction. The hierarchical representation in PredNet does not contain semantic information about global movement patterns. Prediction suppression is observed in E, P, and R units in every module along the network, with more pronounced effects in E and P units. HPNet reproduces prediction suppression effects in IT neurons during testing stage for familiar and unfamiliar image pairings. Neural responses to expected second images in a familiar sequence order are weaker compared to unexpected sequence orders. The model produced similar results with lower responses for predicted pairs after training on image pairs for 2000 epochs. After training, the neural responses to the trained image pairs showed prediction suppression effects, with lower responses for the predicted second stimulus compared to the unpredicted second stimulus. This effect was observed in all three types of units in NPNet, although it was weaker in the R units. The results were consistent with previous observations and replicated findings from BID29. Our model can handle gaps between stimuli due to processing information in spatiotemporal blocks, unlike previous models that showed prediction suppression effects."
}