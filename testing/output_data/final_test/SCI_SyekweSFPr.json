{
    "title": "SyekweSFPr",
    "content": "Our work focuses on producing neural networks that excel in few-shot tasks while being robust to adversarial examples. We explore adversarial training, robust architectural features, pre-processing defenses, and compare robust meta-learning to transfer-learning for few-shot tasks. This analysis lays the groundwork for future defenses in safety-critical applications like facial recognition and copyright control. In applications requiring few-shot learning, conventional defenses against adversarial attacks may not be effective due to limited training data. Metalearning methods are utilized to create networks that can quickly adapt and learn from minimal data in scenarios like face recognition from few images or recognizing new objects from limited examples. Adversarial querying is a new approach in few-shot learning that exposes the network to adversarial attacks during the query step of meta-learning. This method produces a robust feature extractor without the need for adversarial training during fine-tuning. It outperforms other robustness techniques in terms of clean accuracy and adversarial robustness. In Section 4, adversarial querying is motivated by testing methods like adversarial fine-tuning and pre-processing defenses, which are found to be less effective than adversarial querying. Results show the superiority of adversarially queried models in terms of accuracy and robustness. Before the emergence of meta-learning, various approaches existed for few-shot problems, such as transfer learning and metric learning methods. Metric learning methods use nearest-neighbors in feature space for classification, avoiding overfitting to small training examples. These methods are computationally efficient when adding new classes at inference. Meta-learning algorithms create a \"base\" model that quickly adapts to new tasks by fine-tuning on support data. Each task involves a small subset of classes in a large dataset, with the number of examples per class in the support set called the shot. An iteration of training involves sampling tasks from a distribution and fine-tuning the base model in the inner loop. The base model is fine-tuned on support data from sampled tasks. In the \"outer loop\" of training, predictions are made on query data using the fine-tuned network, and base model parameters are updated to improve accuracy. Backpropagation is required through the fine-tuning steps. The meta-learning framework involves sampling tasks, fine-tuning the model in the inner loop, and validating the fine-tuned base model after training. During fine-tuning, MAML updates all parameters with SGD to minimize cross-entropy loss. R2-D2 and MetaOptNet only update the final linear layer, keeping the backbone network frozen at test time. R2-D2 uses a closed-form method instead of SGD. R2-D2 and MetaOptNet use differentiable solvers for linear problems, keeping the backbone network frozen at test time. ProtoNet constructs class prototypes for each task and assesses model robustness with adversarial perturbations. Adversarial perturbations are crafted using the projected gradient descent attack with an \u221e bound of 8/255 and a step size of 2/255. Adversarial training involves replacing clean examples with adversarial examples to create robust models. This method solves a minimax problem to find network parameters that maintain low loss. The authors explore robustness properties of transfer learning and propose ADML, a meta-learning algorithm designed for robustness through adversarial training. However, ADML is only compatible with the outdated MAML algorithm and is computationally expensive. In this section, the authors benchmark the robustness of existing meta-learning methods against a strong attacker using a 20-step PGD attack. Results show that prominent meta-learning algorithms are vulnerable to attacks, with MetaOptNet using SVM for fine-tuning. MetaOptNet uses SVM for fine-tuning, but even SVM fails to show robustness during testing. Adversarial training with 7-PGD on naturally trained MAML models is explored to improve robustness. Results on Mini-ImageNet and Omniglot show the impact on natural and robust test accuracy against a 20-step PGD attack. Adversarial fine-tuning on naturally trained MAML models shows limited robustness on Mini-ImageNet and Omniglot. Adversarial querying (AQ) is introduced as an algorithm for meta-learning to address vulnerability to adversarial examples. The text introduces adversarial querying (AQ) as a method for meta-learning to improve robustness against adversarial examples. It presents a minimax problem where a central parameter vector is fine-tuned on support data to minimize query loss against an attacker. The approach involves an alternating algorithm with steps for sampling data, fine-tuning, perturbing query data, and minimizing query loss. The method is tested on various data sets and meta-learning protocols, showing algorithm-agnostic behavior. In Section 4.5, the effects of backbone architecture and classification head on robustness of meta-learned models are analyzed. Adversarial querying is shown to generate networks robust to strong attacks. Few-shot learning methods with a non-robust feature extractor are found to break under attack, prompting consideration of using a robust feature extractor for transfer learning and meta-learning. In a comparison between robust transfer learning and meta-learning, backbone networks are trained using meta-learning algorithms and fine-tuned with a meta-learning head. The performance of these feature extractors is compared to those trained using adversarially queried meta-learning algorithms. The experiment directly compares feature extractors produced by robust transfer learning and robust meta-learning. The adversarial querying procedure attacks query data to assess performance on testing data after fine-tuning on support data. Low loss on perturbed query data indicates robustness. Attacking only support data is not effective for achieving robust meta-learners. Perturbing only query data is more computationally efficient in adversarial training, as attacking support data doubles the computational load. Therefore, focusing on attacking query data significantly impacts the robustness of the model. Attacking support data during training is not effective for robust meta-learning. Adversarially perturbing only query data is more computationally efficient and significantly impacts model robustness. Experiments show that attacking support data does not improve performance over adversarial querying in meta-learning tasks using MAML on Omniglot and Mini-ImageNet datasets. Adversarial querying is more effective than attacking support data for robust meta-learning. Networks trained with adversarial querying require fine-tuning during test time to maintain robust accuracy. Adversarial querying can also be used to create meta-learning analogues for other adversarial training variants, such as meta-TRADES. Meta-TRADES can slightly outperform initial adversarial querying in robust accuracy with careful hyperparameter choice. However, networks trained with meta-TRADES sacrifice natural accuracy. High performing meta-learning models like MetaOptNet and R2-D2 only update their last linear layer during fine-tuning. Retraining early convolutional layers leads to a drop in robust test accuracy, while retraining only the last layer improves natural and robust accuracy in adversarially queried models. The last layer update in adversarially queried meta-learners significantly improves natural and robust accuracy compared to standard AQ MAML algorithm. Fine-tuning only the last layer during the inner loop outperforms retraining early convolutional layers. In adversarial querying experiments, MetaOptNet is less robust than R2-D2, even when trained with the same backbone architecture. Ridge regression may be a more effective fine-tuning technique than SVM for robust performance. ProtoNet with R2-D2 backbone also performs worse than expected. In addition to adversarial training, architectural features like feature denoising blocks have been used to enhance robustness against targeted adversarial attacks on ImageNet. However, when deployed on small networks for meta-learning, denoising blocks do not improve robustness. After testing denoising blocks on small networks for meta-learning, it was found that they did not improve robustness. The best results were achieved by adding a denoising block after the fourth layer in the R2-D2 embedding network. The model was tested against various powerful adversarial attacks including MI-FGSM, DeepFool, and 20-step PGD with 20 random restarts. The adversarially queried model showed robustness against the strongest \u221e bounded attacker and 20-step PGD attack with 20 random restarts. DeepFool perturbed images outside the robustness radius enforced during adversarial querying. Additional experiments on CIFAR-FS can be found in Appendix A.6. The table compares 5-shot MiniImageNet results against DeepFool, MI-FGSM attack, and PGD attack with 20 random restarts. R2-D2 trained with adversarial-querying (AQ) outperformed adversarially trained transfer learning R2-D2. Recent works have proposed pre-processing defenses for sanitizing adversarial examples before feeding them into a classifier. However, these methods are ineffective in the few-shot regime. DefenseGAN, using a GAN to sanitize adversarial examples, has been compromised by recent attacks due to the limited expressiveness of GANs on complex datasets with high-resolution images. The DefenseGAN setup was found to be insufficient for CIFAR-FS, so a stronger ProGAN generator was substituted. The superresolution defense denoises data with sparse wavelet filters and performs superresolution, motivated by projecting adversarial examples onto the natural image manifold. Testing the superresolution defense using SRResNet showed that these methods are not well suited for the few-shot domain. Naturally trained networks for few-shot image classification are vulnerable to adversarial attacks. Existing robust transfer learning methods do not perform well on few-shot tasks, even when fine-tuned adversarially. There is a need for few-shot methods for adversarial robustness, particularly in the context of meta-learning. In the context of meta-learning, an algorithm-agnostic method called adversarial querying is developed to enhance the robustness of meta-learning models. It is found that fixing the feature extractor and only retraining the last layer during fine-tuning leads to the most robust models. The choice of classification head also significantly impacts robustness. This paper serves as a foundation for developing adversarially robust methods for few-shot applications. Various models are trained with specific parameters and training strategies to achieve robustness. The study introduces adversarial querying to enhance meta-learning model robustness. Fine-tuning with a learning rate of 0.01 for 10 steps per task is conducted. Results show R2-D2 with adversarial querying outperforms transfer learning models in CIFAR-FS tasks against various attacks."
}