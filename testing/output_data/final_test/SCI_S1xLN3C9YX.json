{
    "title": "S1xLN3C9YX",
    "content": "We propose a method for incremental learning of an embedding space for network architectures, facilitating architecture selection during compressed architecture search using Bayesian Optimization. Our algorithm outperforms random search and reinforcement learning, producing compressed architectures superior to state-of-the-art designs like ShuffleNet. The learned embedding space can be transferred to new settings without retraining. In this paper, the focus is on compressed architecture search, which involves automatically discovering compressed network architectures based on a given large network. The goal is to reduce the size of over-sized networks while maintaining similar performance, addressing the issue of wasted computational resources and enabling usage on embedded systems. The paper focuses on compressed architecture search to find efficient network architectures. Evaluating different architectures is costly, so a method to learn an embedding space over the domain of network architectures is proposed. This allows for the careful selection of architectures for evaluation in the network compression paradigm. In compressed architecture search, a teacher network is used to find a student network with fewer parameters while maintaining performance. Bayesian Optimization is employed with a kernel function in an embedding space to select architectures for evaluation. The goal is to automatically search for an efficient network architecture with minimal human effort. Various computationally efficient network architectures have been developed, but this approach aims to streamline the design process. Neural Architecture Search (NAS) is a current research topic focused on finding high-performance architectures within resource constraints. NAO and our work both map network architectures into a continuous space, but with different motivations. Our work aims to define a kernel function for complex architectures with skip connections and multiple branches. Our work is closely related to N2N BID1 and is developed based on Bayesian Optimization (BO). BO is a popular method for hyper-parameter optimization in machine learning, used to tune various network parameters. Our approach will be compared to other BO based NAS methods. The text discusses a Bayesian method for multi-objective optimization problems in neural network architecture search. It introduces a distance metric OTMANN for comparing network architectures with complex connections and proposes a BO based NAS framework called NASBOT. The OTMANN distance is designed based on factors influencing network performance, while the algorithm in the text learns the distance metric automatically based on actual performance. The algorithm automatically learns network architectures based on performance, optimizing in a high-dimensional latent space. It is related to previous works on deep kernel learning, aiming to incorporate the expressive power of deep networks into kernel functions. In this work, the focus is on finding a compressed network architecture with few parameters while maintaining performance similar to a teacher network. The optimization problem aims to evaluate how well a neural architecture meets requirements using a reward function based on compression ratio and validation performance. The algorithm learns network architectures in a high-dimensional latent space, incorporating deep network expressive power into kernel functions. The algorithm proposes a method to learn an embedding space over network architectures for prioritized evaluation using Bayesian optimization (BO) with a Gaussian process (GP) based approach. This allows for the selection of architectures to evaluate efficiently, considering the costly nature of evaluating f(x) for specific architectures. The algorithm uses Bayesian optimization with a Gaussian process to learn an embedding space for prioritized evaluation of network architectures. It selects architectures efficiently by evaluating a mean function and covariance function, computing a posterior distribution on the function, and maximizing the expected improvement acquisition function. The algorithm utilizes Bayesian optimization with a Gaussian process to learn an embedding space for evaluating network architectures efficiently. It addresses challenges in optimizing the kernel function for the neural architecture domain by proposing a learned embedding space and a feasible search space for maximizing the acquisition function. The kernel function is crucial for selecting network architectures during the search process. To compare complex architectures with multiple layers and connections, a mapping to a continuous embedding space using recurrent neural networks is proposed. The architecture embedding function generates embeddings based on configuration parameters, and the kernel function is defined using the RBF kernel with learned weights. The architecture embedding function h(\u00b7; \u03b8) is parameterized by the same weights \u03b8 as the learnable kernel function k(x, x ; \u03b8) based on the RBF kernel. A Bidirectional LSTM is used to handle diverse network architectures with multiple layers, branches, and skip connections by processing configuration information of each layer. The Bi-LSTM processes the configuration of each layer to obtain the architecture embedding, which encodes skip connections. The weights of the Bi-LSTM are learned during the search process to determine the architecture embedding function and kernel. The representation for layer configuration is applicable to skip connections used in ResNet but not to more complex connections between layers. The architecture embedding function h(\u00b7; \u03b8) and kernel k(\u00b7, \u00b7; \u03b8) are determined to control the GP prior and posterior distribution of the function value. The goal is to learn \u03b8 for consistency with the GP prior, resulting in an accurate posterior distribution characterizing the function's statistical structure. D represents the evaluated architectures, with f(x_i) being the value obtained for each architecture. The posterior probability of f(x_i) is conditioned on other evaluated architectures in D, with higher values indicating a more accurate posterior distribution. The value of DISPLAYFORM3 is crucial for accurately characterizing the statistical structure of function f and ensuring consistency with the GP prior. Learning \u03b8 involves minimizing the negative log posterior probability, which is a Gaussian distribution computed based on k(\u00b7, \u00b7; \u03b8). The weights \u03b8 are learned using backpropagation, and the next architecture to evaluate is determined by maximizing the acquisition function EI t (\u00b7) over the neural architecture domain X. It is important to consider only architectures smaller than the teacher network for a compressed architecture search. Maximizing EI t (\u00b7) over X is a non-trivial task. The search space for maximizing the acquisition function is constrained by the teacher network and defined based on it. It includes architectures obtained by layer removal, shrinkage, and adding skip connections to ensure smaller architectures than the teacher network are considered. In this work, we focus on smaller architectures than the given big network by removing layers and shrinking the size of layers, specifically the number of filters in convolutional layers. We find that only shrinking the number of filters already yields satisfactory performance. Adding skip connections is crucial for compressed network architectures to achieve similar performance to the teacher network. The study focuses on exploring a search space to manipulate the architecture of a teacher network. Random sampling of operations is used to optimize the acquisition function, with random sampling proving to be as effective as evolutionary algorithms. The thoughtful design of operations favors compressed architectures. The search algorithm utilizes a learnable kernel function for implementation. The study explores manipulating the architecture of a teacher network using a search algorithm with a learnable kernel function. To avoid overfitting, a multiple kernel strategy is proposed, inspired by the bagging algorithm, to encourage exploration of diverse architectures. Multiple kernel functions are trained on subsets of evaluated architectures in each step of the search process. The study proposes a multiple kernel strategy to explore diverse architectures by learning multiple architecture embedding spaces. Each kernel computes a posterior distribution to evaluate one architecture in the next step. Training kernels on subsets of evaluated architectures encourages exploration and can lead to better architectures. The weights of the kernels are learned from scratch without fine-tuning the Bi-LSTM from the previous step, resulting in fast training. The study introduces a multiple kernel strategy to explore diverse architectures by learning multiple architecture embedding spaces. The training of the Bi-LSTM is fast, evaluating hundreds of architectures during the search process. The algorithm is extensively evaluated with different teacher architectures and datasets, comparing the found architectures to ShuffleNet BID39. Transfer performance of the learned embedding space and kernel is also evaluated, along with an ablation study on design choices. Two datasets, CIFAR-10 and CIFAR-100, are used for evaluation. The study introduces a multiple kernel strategy to explore diverse architectures by learning multiple architecture embedding spaces. The algorithm evaluates architectures using different teacher networks like VGG-19, ResNet-18, ResNet-34, and ShuffleNet. Two baseline algorithms for comparison are random search (RS) and a reinforcement learning based approach, N2N. RS evaluates 160 architectures, while the proposed method runs 20 architecture search steps, generating 8 architectures for evaluation in each step based on different kernels. The proposed method evaluates 160 architectures in total during the search process, training each for only 10 epochs to save computation time. The top 4 architectures are fully trained and the best one is chosen as the solution. Kernel function parameters are learned by randomly sampling from the evaluated architectures. Compression results are summarized in TAB1, with 'Ratio' indicating the compression ratio and 'Times' showing the size ratio between the teacher network and the compressed network. The proposed method evaluates 160 architectures in total during the search process, training each for only 10 epochs to save computation time. The top 4 architectures are fully trained and the best one is chosen as the solution. Kernel function parameters are learned by randomly sampling from the evaluated architectures. Compression results are summarized in TAB1, with 'Ratio' indicating the compression ratio and 'Times' showing the size ratio between the teacher network and the compressed network. The size of the teacher network and the compressed network is compared, along with the value of f(x) to evaluate accuracy and compression ratio. Experiments on VGG-19, ResNet-18, and ResNet-34 show that the proposed method consistently finds architectures with higher f(x) values than baselines on CIFAR-10 and CIFAR-100 datasets. The proposed method successfully compresses architectures with higher accuracy than baselines on CIFAR-10 and CIFAR-100 datasets. Our algorithm outperforms RS in accuracy by 4% with fewer parameters. 'Ours -removal' significantly outperforms 'N2N -removal' in accuracy and compression ratio. ShuffleNet is used as a teacher network, and our search algorithm compresses it by 10.43\u00d7 and 4.74\u00d7 on CIFAR-10. The search algorithm successfully compresses 'ShuffleNet 1 \u00d7 (g = 2)' by 10.43\u00d7 and 4.74\u00d7 on CIFAR-10 and CIFAR-100 respectively while maintaining similar accuracy to the original teacher network. Comparisons are made with state-of-the-art manually-designed compact network architecture ShuffleNet, varying the number of channels and groups. Experiments on CIFAR-100 are summarized in TAB2. The compressed architectures found by the algorithm based on different teacher networks achieve higher accuracy than the original ShuffleNet models. Transferability of the learned embedding space and kernel is being studied to understand their effectiveness in different settings. In studying kernel transfer, a kernel learned in one setting is evaluated for generalization to a new setting. The process involves learning a kernel in the source setting, maximizing the acquisition function in the target setting based on the learned kernel, and evaluating the resulting architecture. Different settings include ResNet-18 on CIFAR-10, ResNet-34 on CIFAR-10, VGG-19 on CIFAR-10, and ResNet-18 on CIFAR-100. The study involves transferring a learned kernel from one setting to another, such as ResNet-18 on CIFAR-10, ResNet-34 on CIFAR-10, VGG-19 on CIFAR-10, and ResNet-18 on CIFAR-100. The process includes running a search algorithm in one setting and transferring the learned kernel to different settings to evaluate its performance. The results show that the learned kernel can transfer effectively to larger teacher networks, different architecture families, and harder datasets. The study demonstrates that a learned kernel can effectively transfer to new settings for architecture search without additional training. The proposed method using BO finds more efficient network architectures on CIFAR-10 and CIFAR-100 compared to baselines. The key contribution is learning an embedding space for network architectures, which can be transferred to new settings without training. The function f aims to find a network architecture with minimal parameters while maintaining performance. It needs to balance compression ratio and performance, favoring high performance with low compression. The reward function design in N2N BID1 is adopted for f, defined as DISPLAYFORM0 where C(x) is the compression. The function f aims to find a network architecture with minimal parameters while maintaining performance. It involves evaluating the compression ratio and validation performance of the architecture compared to a teacher network. During the search process, the architecture is only partially trained, and Knowledge Distillation (KD) is used for faster training. Fine-tuning is done without KD to assess the true performance of the architecture. The configuration of one layer is represented by a vector with dimensions indicating layer type and attributes. Edge information of the network is encoded as a directed acyclic graph. The nodes in a directed acyclic graph can be topologically sorted, allowing for indexing of each layer. The connection information in a network architecture can be described using this representation. The impact of the number of kernels K was studied through experiments on CIFAR-100 using ResNet-34 as the teacher network. Results showed that K = 4, 8, 16 yielded better results than K = 1, with similar performance among K = 4, 8, 16. In main experiments, K was fixed at 8. In experiments on CIFAR-100 with ResNet-34 as the teacher network, the impact of the number of kernels K was studied. Results showed that K = 4, 8, 16 yielded similar results, with K fixed at 8 for main experiments. Adding skip connections in the search space consistently outperformed not adding them, proving their effectiveness. In comparing randomly sampling (RS) and evolutionary algorithm (EA) to maximize the acquisition function EI t (x) for compressing ResNet-34 on CIFAR-100, EA is better at maximizing EI t (x) but slightly worse in final search performance compared to RS. While EA may find better solutions than RS, the function values of the evaluated architectures are usually similar. RS leads to more stable growth in function values compared to EA, so RS is chosen for simplicity in further experiments. Neural architecture search is an optimization problem in a high-dimensional space. Comparing the method of learning a latent embedding space for neural architecture to using TPE for compressed architectures shows differences in approach. TPE uses Gaussian mixture models to fit hyperparameter values, while the proposed method focuses on architecture configurations. Our method transforms architecture configurations into a learned latent embedding space for comparison. We focus on layer removal and shrinkage to find a compressed architecture. This results in a significant number of hyperparameters to tune. Our experiments on CIFAR-100 show that our method outperforms TPE in accuracy with a similar model size. In experiments with skip connections, our method outperforms TPE in accuracy for ResNet-18 and ResNet-34. The high-dimensional hyperparameter space makes direct optimization challenging. ResNet-34 results are not shown due to difficulties in training networks with many skip connections. Our method outperforms TPE in accuracy for ResNet-18 and ResNet-34 in experiments with skip connections. The high-dimensional hyperparameter space makes direct optimization challenging, especially when the original space is very high-dimensional. When optimizing the acquisition function, we randomly sample architectures in the search space by sampling operations to manipulate the teacher network's architecture. To maintain consistency between layers, we impose conditions such as only allowing layers with the same input and output dimensions to be removed for layer removal. When compressing ShuffleNet, layers are grouped based on input and output dimensions. Skip connections are added only when dimensions match. Modifications include inserting a 1x1 convolutional layer before average pooling, increasing parameters by 10% without affecting performance. When compressing ShuffleNet, layers are grouped based on input and output dimensions. Skip connections are added only when dimensions match. Modifications include inserting a 1x1 convolutional layer before average pooling, increasing parameters by 10% without affecting performance. The random search baseline can achieve good performance, but it is more unstable compared to our method. Our method shows a strong tendency for the function value of evaluated architectures to grow as more steps are taken, unlike random search. Our method has a higher chance of selecting architectures with high function values. In contrast to random search, our method efficiently selects architectures by leveraging learned architecture embeddings or kernel functions. We discuss different objective functions for learning the embedding space, focusing on maximizing predictive posterior probability. Additionally, we compare our approach to alternative choices, emphasizing the importance of training the LSTM with a Euclidean loss to predict architecture performance. The LSTM is trained by regressing the function value with a Euclidean loss. Adding a fully connected layer after the embedding predicts the performance of the input architecture. Maximizing the predictive posterior probability is considered a more suitable training objective than regressing the function value. Changing the objective function to the squared Euclidean distance was also tested, with results summarized in TAB8. The results show that maximizing the predictive posterior probability is more effective than using the Euclidean loss as the training objective. Maximizing the log marginal likelihood is another option but leads to worse results compared to the predictive GP posterior. The log marginal likelihood can cause numerical instability due to the covariance matrix's dimension, making the training objective go to infinity. By maximizing the predictive GP posterior instead of the log marginal likelihood, we avoid numerical instability and improve search performance even with smaller learning rates."
}