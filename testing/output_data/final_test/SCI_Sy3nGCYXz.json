{
    "title": "Sy3nGCYXz",
    "content": "Based on observations of singular value drops in fully connected layers and feature maps, a singular value decomposition (SVD) approach is proposed to estimate deep manifold dimensions in VGG19. Three ImageNet categories are chosen for analysis, determining local dimensions through tangent space. Gaussian noise augmentation method is found to be closer to intrinsic dimension, as it does not increase rank of feature matrix in augmented images. The study estimates the dimension of the deep manifold in VGG19 based on tangent spaces of maxpooling layers. Results show dimensions of different categories are similar and decrease rapidly through convolutional and fully connected layers, particularly in Conv5. This research offers insights into the structure of deep neural networks and reveals the organization within the black box. The analysis focuses on the high-dimensional feature space and assumes activation vectors lie on low-dimensional manifolds. The activation vectors of deep neural networks are on low dimensional manifolds in a high dimensional feature space. Manifold learning algorithms based on deep learning help understand the variability in data and estimate the intrinsic dimensionality of the data. This is crucial for determining the number of variables in a linear system or the degrees of freedom in a dynamic system. The problem of estimating the intrinsic dimensionality of a manifold is significant for many algorithms in manifold learning. Estimating the intrinsic dimensionality of a manifold is crucial for manifold learning, especially in AI. The rugged nature of manifolds in deep neural networks poses a challenge, requiring a considerable number of training examples to cover variations. Our work focuses on characterizing manifolds using tangent hyperplanes, which provide local directions of variations on the manifold. Our work focuses on exploring the local hyperplane dimension of the activation manifold in deep neural networks by creating artificial data clusters concentrated in regions of the local tangent hyperplane. Through SVD analysis, we found a significant drop in singular values for fully connected layers and convolutional layers, indicating variations in the dimensionality of the concatenated features. The dimension of the concatenated feature vector in convolutional layers closely matches the sum of dimensions on each feature map. The dimensions of different image categories are similar and decrease rapidly across layers. This study is the first to thoroughly explore manifold dimension in deep neural networks, aiming to enhance understanding and inspire further research in this area. In exploring deep manifolds formed by activation vectors of deep layers, few works have focused on this area. Manifold learning is mainly used in unsupervised learning to capture manifolds. It associates activation nodes with tangent planes to capture variations. Some works investigate learning a kernel matrix for high-dimensional data on a low-dimensional manifold. Another approach involves high-order contractive auto-encoders for capturing manifold structure. A novel approach using high-order contractive auto-encoders builds a topological atlas of charts characterized by principal singular vectors. Efficient algorithms like Singular Value Decomposition (SVD) and Principal Component Analysis (PCA) determine intrinsic dimensionality of data, with algorithm choice based on data geometry and expected outcomes. Improved manifold-learning algorithms consider more pre-knowledge of datasets. The curr_chunk discusses various methods for estimating the intrinsic dimensionality of datasets, including using multi-scale SVD and maximum likelihood principles. It highlights the lack of related work in determining the intrinsic dimensionality of deep manifolds in neural network feature spaces. The curr_chunk delves into determining the intrinsic dimensionality of a deep manifold in neural network feature space. It explains the strategy for finding the dimension of the local tangent hyperplane of the manifold using SVD. The challenge lies in accurately determining the dimension of the manifold dataset. The challenge in dimension estimation for a deep manifold in neural network feature space is influenced by noise, which can lead to incorrect results. Factors of variation can also impact the data, potentially leading to erroneous conclusions. To address these issues, deep neural networks extract high-level features from raw data to disentangle factors effectively. The solution proposed involves using a pre-trained deep neural network to extract abstract features from raw data, effectively disentangling factors of variation and eliminating irrelevant noise. This ensures that feature vectors lie on a noiseless manifold in high-dimensional feature space. Additionally, picture augmentation methods are introduced to further enhance the process. By introducing picture augmentation methods, a large cluster of feature vectors close to a local tangent d-dimensional hyperplane of the noiseless manifold is generated. The original SVD is then applied to this cluster to estimate the local dimension of the manifold. The set of image data points is used, with the original image classified by the neural network as a specific class with high probability. By introducing picture augmentation methods, a cluster of feature vectors near a local tangent hyperplane of the noiseless manifold is generated. The original image is classified by the neural network with high probability. The augmentation information introduced to the image can be divided into irrelevant noise and useful factors of variance. Activation vectors concentrate around a noiseless local tangent hyperplane of the manifold after the feed-forward process. The goal is to estimate the local tangent dimension of the manifold given the image data points and corresponding feature vectors. The goal is to estimate the local tangent dimension of the manifold by computing the SVD of the corresponding data in a specific layer. The singular values are used to determine the dimension, with a threshold value to identify significant values. If a certain ratio condition is met, the dimension is estimated, otherwise, it is based on the whole dimension. The estimation of the local dimension in a specific layer is based on the activation space dimension. This involves calculating the dimension of individual feature maps and then summing them up. The original dimension is obtained by considering all feature maps in a layer. Concatenated dimension is calculated by combining selected feature maps. FIG0 illustrates these concepts. In this paper, the authors use the pre-trained VGG19 model for their experiment, giving each layer a unique name for reference. Table 2 in the Appendix provides the name of each layer and its corresponding activation space dimension. Image augmentation is done using three categories from the ImageNet dataset: Persian Cat, Container Ship, and... The authors use the pre-trained VGG19 model for their experiment, with three categories from the ImageNet dataset: Persian Cat, Container Ship, and Volcano. They select three typical images for each category and apply augmentation methods like cropping, Gaussian noise, and rotation to generate similar images. These methods create data point clusters around the original images, with activation vectors concentrating around them. The authors used the pre-trained VGG19 model with three ImageNet categories: Persian Cat, Container Ship, and Volcano. Activation vectors concentrate around original images, showing similar trends in dimension through layers. Singular values drop dramatically at certain layers, as shown in FIG4. The dimension of the local tangent hyperplane on the manifold can be determined by a dramatic drop in singular values. The dimension grows slowly with the augmentation data size, showing a correlation between data size and estimated dimension. The dimension of the local tangent hyperplane on the manifold can be determined by a dramatic drop in singular values. The dimension grows slowly with the augmentation data size, showing a correlation between data size and estimated dimension. Additionally, a fairly small data set can be used to estimate the dimension of convolutional layers, with the estimated dimension closely matching the concatenated dimension. The estimated dimension closely approximates the concatenated dimension, allowing for the use of a small amount of images to estimate the original dimension. The local tangent hyperplane's dimension is restricted by the number of input images, with the estimated dimension being a summation that can approximate the manifold's dimension. The estimated dimension of the local tangent hyperplane can be approximated using a small dataset size, such as 8000 images. Three categories in ImageNet, Persian Cat, Container Ship, and Volcano, were analyzed for their dimensions in Conv5 and fully connected layers. The dimensions gradually decrease from Conv5 to fc8, with fc8 having a dimension of 1000 due to its direct link to final classification prediction. The dimensions of the three categories are similar and decrease rapidly. The dimensions of the three categories in the VGG19 neural network decline quickly inside the convolutional and maxpooling layers. Through experiments, a drop in singular values was observed in fully connected layers and feature maps. An efficient SVD based method was developed to estimate local dimensions of deep manifolds. Dimensions are similar for different images of the same category. Our study found that dimensions decrease rapidly across convolutional and fully connected layers in deep networks, supporting the low-dimensional manifold hypothesis. This exploration reveals the inner organization of deep networks and suggests the possibility of analyzing individual feature maps for dimensionality in convolutional layers."
}