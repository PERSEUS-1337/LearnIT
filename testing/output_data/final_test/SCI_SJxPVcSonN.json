{
    "title": "SJxPVcSonN",
    "content": "Machine learned large-scale retrieval systems require a large amount of training data for query-item relevance. This paper proposes leveraging user logs and implicit feedback to enhance relevance modeling. A two-tower neural net architecture is used to model query-item relevance with collaborative and content information. By introducing auxiliary tasks trained with richer implicit user feedback data, the quality and resolution of learned representations for queries and items are improved. Significant improvements were seen when applying these representations to an industrial retrieval system. The paper introduces a novel transfer learning model architecture for large-scale retrieval systems, focusing on retrieving the top-k most relevant candidates given a query and a set of candidate items. Large-scale retrieval systems aim to retrieve the top-k most relevant candidates given a query and candidate items. Efforts focus on better representation learning through machine learning models that map queries and items to an embedding space. Challenges include sparse relevance data and the cost of collecting users' opinions on item relevance. In this paper, researchers design human-eval templates with Likert scale questions for relevance and gather feedback through crowd-sourcing platforms. They address challenges such as noisy and biased feedback, and the need to learn relevance in a multi-modality feature space. The proposed approach leverages both explicit answers and implicit feedback from users to learn relevance effectively. The proposed model architecture, based on a two-tower deep neural network, refines query and candidate item representations using implicit and explicit feedback. It includes a shared-bottom architecture for transferring knowledge between tasks. The curr_chunk discusses a transfer learning framework that leverages rich implicit feedback to improve prediction accuracy for sparse explicit feedback in a large-scale retrieval system. The contributions include proposing the framework, designing a novel model architecture, and demonstrating significant improvements in real-world evaluations. Section 2 discusses related work in building large-scale retrieval systems. Section 3 introduces the problem and training objectives. Section 4 describes the proposed approach. Section 5 reports experimental results on a large-scale retrieval system. Finally, Section 6 concludes with findings on state-of-the-art industrial retrieval systems, multi-task learning, and transfer learning techniques. Retrieval systems are widely used in search and recommendation applications, transitioning from reverse index to machine-learned and neural network-based models. The curr_chunk discusses the limitations of current neural network based retrieval models in adapting to complex feature representations from multiple sources. It also mentions the challenges in applying full-blown ranking solutions to retrieval problems where thousands of candidates need to be identified. The curr_chunk introduces a novel framework combining the efficiency of a two-tower model with the capabilities of a multi-task DNN architecture for retrieval problems. It discusses transfer learning and weakly supervised learning in relation to the retrieval problem, defining it as returning the top-k relevant items from a query and candidate items corpus. The retrieval system is modeled as a scoring function selecting top-k scores for queries. Training data consists of query and item pairs with explicit or implicit user feedback. Implicit feedback like clicks is often used due to the cost of explicit feedback. The goal is to fit the scoring function based on examples and optimize machine learning based retrieval systems. In this study, retrieval systems utilize implicit feedback from user logs, like clicks, alongside explicit feedback. The main objective is to improve query and candidate representations to better approximate relevance. An auxiliary objective is introduced to capture user engagement, such as clicks or purchases. The goal is to jointly learn two objectives while sharing parameters, focusing on minimizing differences between predicted and ground truth relevance. The study focuses on utilizing implicit and explicit feedback in retrieval systems to improve relevance approximation. Explicit feedback is associated with user responses, while implicit feedback is treated as a multi-class classification task. The approach involves optimizing regression loss for explicit feedback and using softmax formulation for implicit feedback modeling. Joint optimization of these losses is achieved through maximum likelihood estimation. The proposed framework extends the two-tower model architecture by introducing a shared-bottom model for large-scale retrieval problems. It involves optimizing losses in FORMULA0 and (2) through multi-task learning and adopting a shared-bottom architecture. The model encodes query and item features into a k-dimensional embedding space and computes the scoring function as the dot product between the embeddings. The proposed framework extends the two-tower model architecture by introducing two sub-towers for explicit and implicit feedback tasks. The shared-bottom layers are used for both tasks, with training done sequentially for user engagement and relevance objectives. Stop gradients are applied to prevent overfitting, and only the top layer of the relevance sub-towers is needed for prediction. The experiments of our proposed framework on Google's large-scale retrieval system for relevant item recommendations involved training data with explicit and implicit feedback. Data was split for training and evaluation, with model performance measured using RMSE. The model, implemented in Tensorflow, utilized hyper-parameters for optimal performance. Transfer learning was studied, showing significant improvement in prediction quality. Transfer learning significantly improves prediction quality for sparse relevance tasks by avoiding over-fitting. Using implicit feedback leads to better results compared to explicit feedback alone. Collaborative and content information combined outperforms using collaborative information alone. Proper parameterization of auxiliary and main tasks is crucial for success, balancing capacity for learning high-quality representations with avoiding over-fitting. The proposed model architecture reflects these considerations. Our proposed model architecture for sparse relevance tasks differs from traditional pre-trained models by having separate hidden layers for each task. We use a two-stage training approach with stop gradients to address data skew issues. Future work includes exploring different types of user implicit feedback and model architectures, as well as tuning hyper-parameters for optimal task representation. AutoML techniques will be investigated for automating parameterization across tasks. The paper proposes a novel model architecture using BID26 techniques to automate learning proper parameterizations for query and candidate towers. By leveraging auxiliary tasks with rich implicit feedback, the two-tower neural network approach is extended to enhance sparse task learning. Significant improvements in relevance prediction on Google's large-scale retrieval systems are observed through joint learning with auxiliary objectives."
}