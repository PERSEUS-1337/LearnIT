{
    "title": "BJlSPRVFwS",
    "content": "Asynchronous distributed methods are used to reduce communication and synchronization costs in large-scale optimization. Little is known about their convergence guarantees for general non-smooth, non-convex objectives, especially in cases where closed-form proximal operator solutions are not available. These objectives are commonly found in the training of deep neural networks. In this paper, the first convergence analysis for asynchronous methods with general non-smooth, non-convex objectives is introduced. The analysis covers stochastic sub-gradient descent methods with and without block variable partitioning, momentum, and is adapted to modern hardware properties. Experimental validation is done in training deep neural network architectures, showing successful asymptotic convergence and exploring the impact of momentum, synchronization, and partitioning on performance. The optimization problem in machine learning involves nonconvexity, nonsmoothness, and the need for probabilistic analysis due to memory constraints. Research on parallel optimization algorithms for shared memory architectures has been influenced by seminal works, with subsequent refinements and expansions to nonconvex problems. In shared memory architectures, the assumption of equal probability for choosing parameter blocks in asynchronous computation does not reflect practical NUMA settings where cores control specific blocks based on previous iterates, creating probabilistic dependencies. In shared memory architectures, asynchronous computation in practical NUMA settings creates probabilistic dependencies between delay vector and block choice. This model is formalized in Cannelli et al., introducing a new algorithm for asynchronous parallel optimization. This paper focuses on parallel asynchronous stochastic subgradient descent for nonconvex nonsmooth objectives, like those in deep neural network training. Previous work by Zhu et al. addressed similar problems with asynchronous proximal gradient methods. Training a neural network without ReLUs or max pooling involves using smooth activation functions and an additional regularization term. When activations are nonsmooth, such as with ReLUs, handling non-smoothness becomes challenging. Stochastic uncertain iterate updates further complicate the optimization process, requiring stochastic estimates of subgradients. This poses difficulties in nonconvex optimization, especially in the presence of asynchronous computation in shared memory architectures. In stochastic optimization, approximating elements in a differential inclusion towards minimization of the objective function is a standard approach. Recent interest in stochastic approximation has been fueled by Deep Neural Network architectures. This paper provides the first analysis for nonsmooth nonconvex stochastic subgradient methods in a parallel asynchronous setting. In a parallel asynchronous setting, the study proves the convergence of generic stochastic subgradient methods for nonconvex nonsmooth functions. It also introduces the first convergence analysis for the momentum variant of the classical subgradient method, validated through numerical experiments on ResNet deep network training. The study explores different asynchronous updating strategies for ResNet deep network training, considering variations with and without write locks and block variable partitioning. It focuses on the convergence behavior of a minimization problem involving a locally Lipschitz continuous function with applications in machine learning. The problem involves optimizing parameters in deep neural networks based on input data and target values. In a distributed setting, algorithms aim to optimize a function decomposable into a finite-sum form using multiple processing cores. An inconsistent read scenario is analyzed where each core is allocated a block of variables to update, leading to potential issues with shared memory access. In a distributed setting, algorithms optimize a function using multiple cores. An inconsistent read scenario is analyzed where each core updates variables, leading to shared memory access issues. Stochastic approximation methods, including stochastic gradient descent, are applied to handle nonsmooth functions using subgradient methods. Mini-batches are chosen uniformly at random for computation at each iteration. The shared-memory system with multiple cores performs computations asynchronously. The stochastic subgradient algorithm is used for optimization, with mini-batches chosen randomly. The error distribution becomes Gaussian as data size and batch size increase. The shared-memory system with multiple cores performs asynchronous computations using the stochastic subgradient algorithm for optimization. The error distribution becomes Gaussian with increasing data and batch sizes. For the discrete time probabilistic model of computation, basic requirements must hold across cores to ensure convergence, even in the presence of faults or slowdowns. The probabilistic assumptions governing asynchronous update scheduling are left to the Supplementary Material. The stochastic approximation framework discussed in the next section ensures convergence by satisfying standard assumptions about stochastic sub-gradient estimates. Mini-batches are sampled uniformly and independently at each iteration, resulting in independent noise applied to the subgradient term. The stochastic subgradient estimates satisfy certain conditions, with a bias term that is zero if the function is continuously differentiable. Additional details on asynchronous stochastic updating are provided in the Supplementary material. In the stochastic approximation framework, the algorithm is redefined for analysis from a stochastic perspective. The step size is defined as a subsequence {\u03b3 l } for data block i at iteration k. Standard conditions for sampling procedures in stochastic gradient methods are implied, following the original Robbins-Monro method. In Stochastic Approximation, the algorithm redefines the standard approach by introducing real time into the model of asynchronous computation. It defines the real elapsed time between iterations for each block i as \u03b4\u03c4 k,i. The step-size sequence is assumed to come from an underlying real function. New \u03c3-algebras F k,i and F + k,i are defined to measure the random variables, indicating events up to and including the computed noisy update. The algorithm in Stochastic Approximation introduces real-time into asynchronous computation by defining the elapsed time between iterations for each block as \u03b4\u03c4 k,i. It also defines new \u03c3-algebras to measure events up to and including the computed noisy update. The algorithm introduces piecewise constant interpolations of vectors in real-time, ensuring that delays do not grow exponentially. Assumptions are made to guarantee convergence to a stationary point in the real-time process. The process defined for the iterate time scale approximates the path of a differential inclusion, defining stationary points of f(\u00b7). An invariant set for a differential inclusion is defined, and a theorem shows weak convergence on large intervals. The process approximates the path of a differential inclusion, defining stationary points of f(\u00b7). An invariant set is defined, and a theorem shows weak convergence. Problems in training deep neural networks with activation functions like log x, e x, max(0, x), or log(1 + e x) have invariants that converge. The differential inclusions ensure convergence to block-wise stationarity, where every stationary point is also blockwise stationary. The algorithm can be modified to update the entire vector without block partitioning. Experimental evaluation compares different algorithms, including WIASSM. Experimental evaluation compared algorithms WIASSM and WCASSM for stochastic subgradient methods. WIASSM uses lock-free updates while WCASSM uses locks for consistent writes. Convergence analysis required sparsity of x for HogWild! and AsySG-incon. Train accuracy and generalization trajectories were plotted for the methods, with SGD running a single process and asynchronous methods running 10 concurrent processes. In experiments comparing WIASSM and WCASSM for stochastic subgradient methods, it was found that WIASSM and WCASSM show better convergence per epoch than PASSM. The use of momentum correction significantly improves the convergence of PASSM. Asynchronous methods run 10 concurrent processes, while SGD runs a single process. The degree of asynchrony is directly related to momentum in the experiments. The presented Partitioned Asynchronous Stochastic Subgradient Method uses lock-free updates and hyper-parameters with decreasing step sizes. L2 penalty with weight-decay and an additional L1 penalty are applied. Momentum correction is explored with benchmarks comparing constant momentum of 0.9. In a shared-memory setting, communication cost savings are limited due to efficient data-parallelization. CIFAR10 dataset with RGB images was used for training and testing. A constant momentum of 0.9 was applied in the benchmarks. The experiments demonstrate that PASSM converges faster than other asynchronous methods due to block partitioning the model across processes, reducing synchronization costs and speeding up data processing. Increasing the number of processes in PASSM improves convergence per unit time, while the use of locks in WCASSM slows it down. PASSM shows better convergence with respect to wall-clock time and scalability with parallel resources. The CNN model ResNet18 has a blocked architecture of residual. ResNet18 is a CNN model with a blocked architecture of residual blocks totaling 18 convolution layers. The neural network training poses general nonsmooth nonconvex optimization problems. The system specification includes a NUMA workstation with 2 sockets, 10 cores each, running at 2.4GHz, and 4 Nvidia GeForce GTX 1080 GPUs. Processes are bound to individual CPU cores for fair evaluation of scalability with 5 and 10 processes. For evaluation, 5 and 10 processes are used on a NUMA workstation with 2 sockets and 10 cores each. Asynchronous methods are implemented using Pytorch library and Python multiprocessing framework in a multi-GPU environment. The nn.DataParallel() module is utilized for data-parallel computation with CNN instances allocated on GPUs. Stochastic subgradients are computed on each GPU and added to the central instance. The implementation exploits parallel resources by summing subgradients from each GPU and adding them to the central instance. Unlike other methods, PASSM partitions the model into blocks for computing stochastic subgradients, resulting in savings for layers closer to the output. The implementation of PASSM partitions the model into blocks for computing stochastic subgradients, with blocks containing different numbers of parameter components assigned to processes based on their proximity to the output. Test-accuracy vs time plots show faster convergence with PASSM compared to other asynchronous methods. However, a blocked architecture like ResNet does not offer significant computation-cost savings. The PASSM model partitions the model into blocks for computing subgradients, utilizing multi-GPU data-parallel implementation. Experimental observations are presented in Figures 1, 2, 3, and 4. The block partitioning design reduces optimization costs per process. However, in a blocked architecture like ResNet, PASSM may not offer significant computation-cost savings. In a blocked architecture like ResNet, PASSM misses out on saving subgradient computation cost through parallelization. It is crucial to compute subgradients independently for better performance and reduced memory traffic. Momentum correction improves convergence per epoch in the block partitioning approach. The convergence theory of asynchronous stochastic subgradient descent is analyzed in this paper, showing consistency with standard theory in stochastic approximation and asymptotic convergence. The paper analyzes the convergence theory of asynchronous stochastic subgradient descent, showing consistency with standard theory in stochastic approximation and asymptotic convergence. Numerical results indicate performance variabilities in different types of asynchrony. The relation of the probabilistic model of asynchrony to hardware properties is discussed, with a global counter indicating sequential updates and dependencies on random vectors. The paper discusses the convergence theory of asynchronous stochastic subgradient descent, highlighting performance variabilities in different types of asynchrony. It explores the relation of the probabilistic model of asynchrony to hardware properties, focusing on the dependencies on random vectors and the evolution of blocks and minibatches. The paper discusses the convergence theory of asynchronous stochastic subgradient descent, focusing on the use of diminishing step-size without global synchronization. Each core has its own local step size, and variable blocks are partitioned across cores for efficiency. The paper presents Algorithm 2 for asynchronous stochastic subgradient descent with local step sizes and partitioned variable blocks across cores. The convergence theory is discussed without global synchronization, emphasizing the use of diminishing step sizes. The paper discusses weak convergence of a sequence of processes {A k (\u00b7)} in the Skorohod topology, with tightness conditions ensuring continuity of limit processes. The paper discusses weak convergence of a sequence of processes {A k (\u00b7)} in the Skorohod topology, with tightness conditions ensuring continuity of limit processes. Convergence of a function f n (\u00b7) to f (\u00b7) in the Skorohod topology is equivalent to uniform convergence on each bounded time interval. The j-fold product space D j [0, \u221e) consists of real-valued functions on [0, \u221e) that are right continuous with left-hand limits. The proof of the main Theorem draws from Kushner & Yin (2003) and introduces momentum to the algorithm, with details on how to treat the distinctions in the proof. The paper discusses tightness conditions for weak convergence of a sequence of processes {A n (\u00b7)}. It shows Lipschitz continuity of subsequence limits with probability one, existing in the weak sense. The weakly convergent subsequence's limits are denoted as x i (t) = x i (\u03c4 i (t)), with a set-valued map S(x, T, \u03c6). The noise structure ensures the existence of L for all possible values of x, T, and \u03c6. The uniqueness of the trajectory lies in a specific ball around the limit point for t \u2265 T 1, which can be replaced by the trajectory lying in a ball around the invariant set. The supremum of T 1 associated with every possible subgradient exists due to the compactness and upper semicontinuity of the subgradient. Assumption 3.2 implies Theorem 4.1 and Theorem 5.3, showing that as \u03c3 \u2192 \u221e, x \u03c3 (\u00b7) converges to an invariant set of the differential inclusion."
}