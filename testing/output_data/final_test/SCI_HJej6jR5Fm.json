{
    "title": "HJej6jR5Fm",
    "content": "The proposed problem of guided segmentation aims to segment unannotated pixels by leveraging varying amounts of pixel-wise labels through guided networks. These networks extract guidance from different levels of supervision and optimize for fast, accurate, and data-efficient segmentation. The approach spans from few-shot to many-shot learning scenarios, comparing to full gradient optimization at both extremes. The segmentor efficiently utilizes varying levels of supervision to segment classes as the union of instances across images. It can be updated cumulatively with more supervision and is applicable to tasks like cellular microscopy and medical imaging. Learning new segmentation tasks typically requires a large amount of annotated data. Guided segmentation involves learning from varying levels of supervision to segment images quickly and generically across tasks. This approach can propagate supervision from a few annotated pixels to fully annotated images, enabling efficient segmentation of diverse image collections. Guided segmentation extends fewshot learning to the structured output setting by integrating supervision across images and segmenting unannotated images. It broadens the scope of interactive segmentation by extracting guidance from variable amounts of supervision through guided networks. Guided networks enable quick adaptation to new tasks without the need for extensive retraining. The model can incorporate expert guidance for defining new tasks or correcting errors, making it versatile for various inference scenarios. Guided segmentation allows for quick adaptation to new tasks without extensive retraining. The method is evaluated on various segmentation challenges, including interactive image segmentation, semantic segmentation, video object segmentation, and real-time interactive video segmentation. Novel experiments explore the characteristics and limits of guidance, comparing it with standard supervised learning in few-shot and many-shot scenarios for segmentation tasks. The model can generalize to guide tasks at different levels of granularity, such as meta-learning from instance supervision to guiding semantic segmentation of categories. The current text discusses guided segmentation for various segmentation tasks, including semantic segmentation and video object segmentation. Few-shot learning is highlighted for its data efficiency, requiring minimal annotations for new concepts. Methods focus on direct optimization for few-shot settings by synthesizing episodes and learning task models for inference. Our approach explores flexible settings for annotations and classes in segmentation tasks, inspired by episodically optimized metric learning. We design an efficient segmentation architecture based on Siamese networks and few-shot metric methods to retrieve support annotations for queries. Unlike existing meta-learning schemes, we investigate how a meta-learned model generalizes across task families with a nested structure, such as semantic segmentation after meta-learning on instance segmentation tasks. The unified view with guidance approach addresses semantic, interactive, and semi-supervised video object segmentation challenges. The guided segmentor can segment accurately from sparsely annotated pixels and performs multi-way inference, outperforming existing methods. It is feed-forward, quick, and achieves high accuracy even with extremely sparse annotations. This approach replaces online optimization with offline methods for real-time, interactive video object segmentation. The guided segmentor improves video object segmentation by transferring guidance across tasks and scaling with more annotations. It outperforms interactive methods by quickly propagating supervision between images and dividing input into annotated support and unannotated query for task completion. In guided segmentation tasks, the focus is on sparse annotations for efficiency. The setting involves considering the number of support pixel annotations and annotated support images, denoted as (S, P)-shot learning. Sparse annotations are practical and only require annotators to point to segments of interest, making data collection more efficient compared to dense masks. Mixed-shot and semi-supervised supports are considered for segmentation tasks with imbalanced classes and sparse annotations. In guided segmentation tasks, sparse annotations are used for efficiency, with a focus on mixed-shot and semi-supervised supports. The task involves input-output pairs sampled from a distribution, with annotated support images and unannotated query images. The annotations consist of point-label pairs, with the task outputs being the targets for segmentation on the queries. The model can handle various classes, but for simplicity, binary tasks are focused on. Our approach to guided segmentation involves extracting a task representation from semi-supervised, structured support and segmenting the query based on that representation. The task representation is defined as z = g(x, +, \u2212), and the query segmentation is guided by this representation as \u0177 = f(x, z). The design of the task representation and its encoder is crucial for handling the hierarchical structure of images, high dimensions, semi-supervised nature of support, and skewed distributions. We focus on designing deep networks for the guide and inference, using branched fully convolutional networks. The fully convolutional networks utilize a guide branch to extract task representation from support and an inference branch for segmenting queries. Episodic meta-learning is adapted for image-to-image learning, increasing episode diversity by sampling within and across segmentation task families. The task representation fuses visual information with annotations to determine segmentation in queries. Support is statistically dependent, semi-supervised, high dimensional, and class-skewed. Initial focus is on a binary task with (1, P)-shot. The support encoder is decomposed across receptive fields for local task representations. Early fusion strategy involves stacking image and annotations channel-wise at the input. Masking positive pixels instead forces invariance to context, potentially speeding up learning. Late Fusion (ours) resolves the learning and inference issues of early fusion by factorizing features and annotations in the guide architecture. Visual features are extracted from the image alone, annotations are mapped into masks, and both are fused using an element-wise product. Late Fusion (ours) factorizes visual and annotation branches using m(+) and m(\u2212) coordinates, fused by \u03c8 through element-wise product. This spatial relationship improves learning efficiency and inference time by quickly updating task representation with only recomputing masking. Late fusion architecture shares feature extractor \u03c6 for joint optimization, enhancing learning efficiency and task accuracy by 60% for video object segmentation. Inference time is reduced as only masking needs recomputation for new annotations incorporation. The late fusion architecture improves learning efficiency and inference time by quickly updating task representation with only recomputing masking for new annotations incorporation, making it suitable for real-time interactive video segmentation. The effect of pooling in an image with multiple visually similar objects is shown in FIG1 (right). The full (S, P)-shot setting requires summarizing the entire support with varying amounts of pixelwise annotations. Task representation is formed by averaging shot-wise representations, extending guidance to multi-way inference in experiments. Separate guides are constructed for each class, sharing efficiency in \u03c6 and differing only in masking. In a static segmentation model, inference is simply \u0177 = f \u03b8 (x) for output y, parameters \u03b8, and input x. Guided inference involves a function \u0177 = f (x, z) using guidance extracted from the support. Different forms of conditioning have been explored for low-dimensional classification and regression problems in few-shot learning literature. Metric learning with feature fusion is selected for optimization, similar to siamese architectures but directly optimizing the classification loss. During training, the model optimizes the classification loss by fusing features using a small convolutional network. The guidance branch is jointly optimized with the segmentation branch to \"learn to guide\" in supervised learning. The fused query-support feature is scored by the network for retrieval from support to query, with multi-way guidance for parallel inference. The model optimizes the parameters of the guidance and segmentation branches by minimizing the loss between the segmentation output and target. Tasks are defined by support and query images, with episodic optimization on sampled tasks. Support and query annotations are binarized and spatially sampled for training the network. The model optimizes parameters for guidance and segmentation branches by minimizing loss between output and target segmentation. Training involves episodic optimization on sampled tasks with binarized and spatially sampled annotations. The network is trained with varying support sizes and shots for efficiency and can handle sparse and dense pixelwise annotations with the same model. Transfer between task distributions is possible with guided networks. The guided segmentor is evaluated on various segmentation problems like interactive, semantic, and video object segmentation. Results show that these problems can be seen as instances of guided segmentation. Cross-task supervision and guiding with large-scale supports are experimented to understand guidance characteristics. Evaluation is standardized using the intersection-over-union metric for all tasks and masks. The backbone of the networks used for interactive, semantic, and video object segmentation is VGG-16 BID25, pre-trained on ILSVRC BID21, and cast into fully convolutional form BID24 for fair comparison with existing works. Fine-tuning and foreground-background segmentation are included as baselines for all problems to optimize the model and verify learning. Our approach for interactive image segmentation involves late-global guidance, which differs from the state-of-the-art DIOS method. We achieve more accurate results with extreme sparsity and faster updates. In video object segmentation, we evaluate our method on the DAVIS 2017 benchmark, extending it to sparse annotations to compare different methods. Our method achieves 33.3% accuracy for 80% relative improvement over OSVOS in the same time envelope, handling extreme sparsity with little degradation. Fine-tuning significantly improves accuracy but takes 10+min/video, while our method is 200\u00d7 faster at 3sec/video. Our guided segmentor can interactively segment video in real time using randomly-sampled dot annotations to measure accuracy. The accuracy-annotation tradeoff curve shows improvement in both image and pixel dimensions. The feedforward architecture is fast and easily updated for annotation changes, addressing the challenge of semantic segmentation with high intra-class variance. Our approach achieves state-of-the-art sparse results in semantic segmentation, rivaling dense results with just two labeled pixels. OSLSM is only defined for {0, 1} annotations and is incompatible with missing annotations. We map all missing annotations to negative for evaluation. The oracle is trained on all classes, with nothing held-out. In a novel examination of meta-learning with cross-task supervision, the study investigates whether meta-training on sub-tasks can address super-tasks. The guided segmentor is meta-trained on interactive instance segmentation tasks from all classes of PASCAL VOC BID6 and evaluated on semantic segmentation tasks from all categories. The experiment includes varying levels of support from semantic annotations and compares results to foreground-background as a baseline. In a study on meta-learning with cross-task supervision, the guided segmentor is trained on interactive instance segmentation tasks and evaluated on semantic segmentation tasks from all categories. Increasing semantic annotations improves accuracy, with guidance compared to standard supervised learning on a transfer task between disjoint semantic categories. This is the first evaluation of how few-shot learning scales to many-shot usage for structured output. In a study on meta-learning with cross-task supervision, the guided segmentor is trained on interactive instance segmentation tasks and evaluated on semantic segmentation tasks from all categories. Increasing semantic annotations improves accuracy, with guidance compared to standard supervised learning on a transfer task between disjoint semantic categories. The guided segmentor achieves 95% of supervised learning performance in a many-shot regime, shedding light on the spectrum of supervision from few-shot to many-shot settings. The text discusses guided segmentation and networks that reconcile task-driven and interactive inference by extracting guidance from supervision. It highlights the benefits of guidance in learning and inferring tasks without optimization, improving accuracy with more supervision, and segmenting new images without the supervisor. The data preparation section mentions the use of PASCAL VOC 2012 BID6 with additional annotations from SBDD BID12 for training and validation sets. For few-shot semantic segmentation, random sampling is used to sparsify dense masks, achieving performance similar to more complex strategies. Ground truth labels are remapped, and classes not part of the task are labeled as background. The process is illustrated in FIG2. The experimental protocol involves dividing PASCAL classes into sets for testing few-shot performance. Each set is meta-trained with binary tasks sampled from 15 classes. The text discusses meta-training a guided segmentor for few-shot video object segmentation using the DAVIS 2017 benchmark. Tasks are synthesized by sampling frames from videos, with the first frame densely labeled. The backbone networks used are VGG-16 BID25 pre-trained on ILSVRC BID21. The text discusses optimizing guided nets for segmentation tasks using VGG-16 BID25 pre-trained on ILSVRC BID21. The optimization procedure follows FCNs detailed in BID24, with specific parameters like learning rate, batch size, and weight decay. Intersection-over-union (IU) is used as a standard metric for segmentation tasks, with a formula provided for calculating IU of positives across different masks. The text discusses optimizing guided nets for segmentation tasks using VGG-16 BID25 pre-trained on ILSVRC BID21. The optimization procedure follows FCNs detailed in BID24, with specific parameters like learning rate, batch size, and weight decay. Intersection-over-union (IU) is used as a standard metric for segmentation tasks, with a formula provided for calculating IU of positives across different masks. The metric used makes performance comparable across tasks, independent of the number of classes, and does not include negatives for multi-class predictions and ground truth. Note that this metric is not directly comparable to the mean IU typically reported for semantic segmentation benchmarks."
}