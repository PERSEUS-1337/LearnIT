{
    "title": "BJij4yg0Z",
    "content": "Our work addresses the issue of generalization in machine learning, inspired by a study on deep neural networks memorizing random labels but still generalizing well. We find similar behavior in small linear models, attributed to Bayesian evidence penalizing sharp minima. Additionally, we suggest an optimal batch size for maximizing test set accuracy and propose that noise from mini-batches guides parameters towards favorable minima. This approach views stochastic gradient descent as a stochastic differential equation, with a noise scale factor identified as $g = \\epsilon (\\frac{N}{B} - 1). The paper discusses the optimal batch size for deep learning models, showing that it is proportional to the learning rate and training set size. Empirical evidence from training deep convolutional networks on ImageNet and CIFAR10 supports these findings. The study also highlights how networks can memorize training labels but fail to generalize when labels are randomized, sparking debate on learning theory. The results question why models work well if they can assign arbitrary labels. Increasing batch size while keeping learning rate fixed can decrease test accuracy. Different scaling rules for batch size and learning rate have been proposed. Authors suggest that broad minima may generalize better than sharp minima. Some argue that nonvacuous PAC-Bayes generalization bounds penalize sharp minima. Stochastic gradient descent finds wider minima with reduced batch size. In this work, it is shown that stochastic gradient descent (SGD) finds wider minima as batch size decreases. The curvature of a minimum can be increased by changing model parameterization. The results are not unique to deep learning and can be observed in a small over-parameterized linear model. SGD integrates a stochastic differential equation with noise scale g \u2248 N/B, where g is the learning rate, N is the training set size, and B is the batch size. There is an optimal batch size that maximizes test set accuracy, proportional to the learning rate and training set size. Bayesian model comparison is discussed in section 2, while section 3 replicates observations from BID27 in a linear model. An optimum batch size for maximizing test set accuracy is shown in section 4, with scaling rules derived in section 5. The \"generalization gap\" refers to the difference in test accuracy between small and large batch SGD training. The text discusses inferring a posterior probability distribution over parameters using Bayes theorem, with a focus on the cross-entropy of unique categorical labels. It mentions using a Gaussian prior and L2 regularization to minimize a cost function for predicting unknown labels of new inputs. Integrals are computed to approximate the probability of new labels, with an emphasis on the region near the minimum cost function value. The text discusses comparing different models by evaluating the evidence ratio, which minimizes a PAC-Bayes generalization bound. The evidence is computed by integrating out the parameters and is invariant to the model parameterization, estimated near the minimum cost function value. The evidence in model comparison is estimated near the minimum cost function value, controlled by the curvature and regularization constant. Minima with low curvature enforce Occam's razor, favoring simpler models. The Occam factor penalizes the amount of information the model must learn about the parameters to accurately model the training data. The evidence can be reframed in the language of information theory, comparing it against a null model that assumes labels are entirely random. This null model has no parameters, and the evidence is controlled by the likelihood alone. The evidence ratio, based on the log evidence ratio, supports the idea that broad minima generalize better than sharp minima. Rescaling parameters can affect Hessian eigenvalues, but must be done in conjunction with regularization coefficients to avoid changing the model. Evaluating evidence for deep networks is challenging due to the complexity of computing the Hessian for millions of parameters. Neural networks have many equivalent minima due to the ability to permute hidden units without altering the model. The evidence ratio supports the idea that broad minima generalize better than sharp minima. Rescaling parameters can affect Hessian eigenvalues but must be done with regularization coefficients to avoid changing the model. Evaluating evidence for deep networks is challenging due to the complexity of computing the Hessian for millions of parameters. Neural networks have many equivalent minima due to the ability to permute hidden units without altering the model. This issue is not a major limitation as studying evidence in simple cases can explain results in various scenarios, including deep neural networks generalizing well on training inputs with informative labels. In the study, 5000 MNIST images of ones are used for two tasks: one with randomized labels and the other with informative labels matching true MNIST labels. The model, with 784 weights and 1 bias, shows good generalization to the test set when trained on informative labels but perfectly memorizes random labels. The mean margin between training examples and the decision boundary decreases as the regularization coefficient is reduced, with a larger margin for informative labels. The mean margin is larger for informative labels than for random labels. Figure 2 shows the mean cross-entropy of model predictions on training and test sets, along with the Bayesian log evidence ratio. In the random label experiment, the model makes random predictions with high confidence. As regularization increases, the test set cross-entropy decreases to ln 2. The log evidence ratio indicates the model's plausibility decreases exponentially with weak regularization. The log evidence ratio decreases exponentially with weak regularization, indicating the model's low plausibility. Test cross-entropy falls below ln 2 as regularization increases, showing successful generalization. The model's predictions become overconfident with weak regularization but improve with higher regularization coefficients. The log evidence ratio, evaluated on random or informative labels, is strongly correlated with the test cross-entropy. As regularization increases, the log evidence ratio rises to zero while the test cross-entropy rises to ln 2. Bayesian model comparison explains the results in logistic regression. BID20 showed the largest Hessian eigenvalue increased when training on random labels in deep networks, implying falling evidence. Bayesian principles suggest that the generalization gap can be explained by adding isotropic Gaussian noise to the gradient during training, driving parameters towards broad minima with large evidence. This noise helps prevent SGD from converging to sharp minima, improving generalization. Bayesian principles propose that the \"generalization gap\" can be explained by adding noise to the gradient during training. This noise helps prevent SGD from converging to sharp minima, improving generalization. The test set accuracy often decreases as the SGD batch size increases, with an optimal batch size balancing the gradient and noise contributions. Linear models do not show a generalization gap due to their convex nature. A shallow neural network with 800 hidden units and RELU activations trained on MNIST without regularization is used for analysis. In a study comparing small batch to full batch training on MNIST, a generalization gap between the two is observed. Small batch training takes longer to converge but shows a clear difference in model accuracy compared to large batch training. The test cross-entropy for small batch training is lower at the end, indicating a trade-off between batch size and performance. The performance peak shifts to the right with increasing batch size, but overall performance decreases after 3. The best batch size is proportional to the learning rate. Both small and large batches show increasing cross-entropy, suggesting overfitting. The generalization gap between batch sizes decreases with L2 regularization. Test set accuracy converges with gradient updates. A peak in test accuracy is observed at an optimum batch size. The results show that there is an optimum batch size that maximizes test accuracy, consistent with Bayesian intuition. The peak in test accuracy is influenced by the tradeoff between depth and breadth in Bayesian evidence, which is controlled by the scale of random fluctuations in SGD dynamics. Three scaling rules predict how the optimal batch size depends on learning rate, training set size, and momentum coefficient. The performance peak shifts to the right with an increase in training set size. The best batch size is proportional to the training set size once it exceeds 20000. The gradient error is modeled with Gaussian random noise, and equation 11 is interpreted as the discrete update of a stochastic differential. The discrete update of a stochastic differential equation is related to SGD through a gradient update. The noise scale in SGD decreases with increasing batch size, leading to an optimal batch size. A similar analysis was done by BID23 near local minima. The noise scale in SGD decreases with increasing batch size, leading to an optimal batch size for training. The peak test accuracy achieved at a given learning rate does not begin to fall until a certain point, indicating no significant discretization error. Above this point, the discretization error dominates and the peak test accuracy falls rapidly. In figure 5b, the best observed batch size scales linearly with the learning rate, allowing for increased parallelism across GPUs and reduced training times. Various authors have proposed adaptive batch size adjustments and linearly coupling learning rate and batch size. Smith et al. empirically show the benefits of decaying the learning rate during training. In Smith et al. FORMULA0, it is shown that decaying the learning rate during training and increasing the batch size are equivalent. The test set accuracy increases with training set size, and the best batch size also scales linearly with training set size. This scaling rule can be applied to production models to accommodate growing datasets. Additionally, large batch training is expected to become more common over time. Our analysis shows that the noise scale in SGD with momentum is g \u2248 N B(1\u2212m), where m is the momentum coefficient. The optimal batch size increases with a higher momentum coefficient. Test set performance and batch size are plotted against momentum coefficients, showing good agreement with our scaling rule. A heuristic for tuning batch size, learning rate, and momentum coefficient is proposed in the appendix. Additionally, it is noted that deep neural networks and linear models can memorize random labels of the same inputs. Bayesian evidence, composed of cost function and \"Occam factor\", explains observations in labels memorization. Mini-batch noise drives SGD away from sharp minima, leading to an optimum batch size for maximizing test accuracy. Optimum batch size scales linearly with learning rate, training set size, and momentum coefficient. Bayesian preference for sampling parameter values from posterior is discussed. The text discusses sampling the posterior using the overdamped Langevin equation, which is similar to the stochastic differential equation of SGD. As t approaches infinity, the probability of sampling a parameter vector from the Langevin equation is proportional to e^(-C/T). The text discusses drawing posterior samples by repeatedly integrating the Langevin equation with isotropic noise. The step size is scaled by the training set size to reduce discretization error and obtain valid samples from the posterior. The text discusses how the evidence in favor of a model is proportional to the integral of the posterior over all parameter space, with a focus on local minima in non-convex models. In Bayesian posterior sampling, the probability of sampling from a local minimum is proportional to the evidence in favor of that minimum. This bias towards local minima with large evidence explains why a single posterior sample often has lower test error than the cost function minimum. In experiments with L2 regularization, full batch training took longer to converge than small batch training. Regularized full batch training took longer to converge than small batch training, but significantly reduced the generalization gap. Despite slightly lower test set accuracy, large batch regularized training achieved lower test cross-entropy. The test cross-entropy of regularized models remained stable without degradation, eliminating the need for early stopping. The difference between full batch and mini-batch gradients was approximated by a Gaussian random variable in section 5, with scaling rules derived and empirically verified. This assumption was motivated by the central limit theorem, suggesting gradient errors tend towards Gaussian noise with increasing batch and sample sizes. The central limit theorem suggests that gradient errors tend towards Gaussian noise with increasing batch and sample sizes, even if the gradient distribution may have heavy tails. The distribution of gradients after random initialization in deep networks may not be Gaussian, as shown in figure 9a. The distribution of gradient errors tends towards Gaussian noise with increasing batch and sample sizes, even if the gradient distribution may have heavy tails. Momentum simulates a generalized Langevin equation with structured fluctuations, where the damping coefficient and Gaussian noise play key roles. Theorem states that varying damping coefficient doesn't change parameter sampling probability in the limit t \u2192 \u221e. Langevin equation relates to momentum equations. Scaling batch size B \u221d N keeps random fluctuations constant. Additional scaling relation: B \u221d 1/(1 \u2212 m), where ef f = /(1 \u2212 m) is \"effective learning rate\". Heuristic proposed for tuning batch size, learning rate, and momentum parameter to maximize test accuracy and batch size for parallel training. To maximize test accuracy and batch size for parallel training, set learning rate to 0.1 and momentum coefficient to 0.9. Experiment with batch sizes on a logarithmic scale to find optimal size. Increase batch size by a factor of 3 while scaling learning rate and momentum coefficient until validation accuracy starts to fall. Retune batch size on a linear scale around optimal size. Increasing batch size on a linear scale around the optimal size can improve test accuracy, reduce hyperparameter tuning costs, and minimize the number of gradient updates needed for model training."
}