{
    "title": "ryeRn3NtPH",
    "content": "Adversarial Inductive Transfer Learning (AITL) is a method that uses adversarial domain adaptation and multi-task learning to address discrepancies in input and output spaces between source and target domains. It is applied in pharmacogenomics to predict drug response in patients using genomic information, bridging the gap between pre-clinical and clinical datasets. AITL is the first method to tackle both input and output discrepancies in this context. Deep neural networks have shown state-of-the-art performance in various fields, including genomics and medicine. Transfer learning addresses the challenge of training with limited data by leveraging knowledge from a source domain to improve performance on a target domain. Adversarial Inductive Transfer Learning (AITL) outperforms existing methods in pharmacogenomics, guiding precision oncology more accurately. Transfer learning methods aim to minimize the discrepancy between source and target domains using metrics like Jensen Shannon Divergence, Maximum Mean Discrepancy, and Margin Disparity Discrepancy. Transductive transfer learning improves generalization on an unlabeled target domain, while inductive transfer learning enhances generalization on a labeled target domain with different label spaces. Adversarial domain adaptation has shown promising results in addressing domain shift issues. Adversarial domain adaptation has been effective in addressing domain shift issues by minimizing the discrepancy between source and target domains. However, there is a need to explore adversarial adaptation for discrepancies in both input and output spaces. In the context of pharmacogenomics, where predicting response to cancer drugs is crucial, current methods often rely on pre-clinical datasets due to limited clinical data availability. In pharmacogenomics, discrepancies exist between cell line and patient datasets in drug response prediction. Cell lines lack an immune system and tumor microenvironment, leading to different response measurements. While cell lines use IC50 for response, patients use RECIST criteria. This results in a regression problem for cell lines and a classification problem for patients. Adversarial Inductive Transfer Learning (AITL) is proposed to address these discrepancies. Adversarial Inductive Transfer Learning (AITL) is introduced as the first method for inductive transfer learning that adapts both input and output spaces. It aims to address discrepancies in gene expression data between cell lines and patient datasets in pharmacogenomics. AITL learns features from both domains to predict drug response using a multi-task subnetwork. The multi-task subnetwork in AITL addresses output space discrepancy by using shared layers for classification and regression, assigning cross-domain labels to source samples. Adversarial domain adaptation tackles input space differences by making features domain-invariant. Class-wise discriminators further regularize features by receiving source and target samples from the same class label. In pharmacogenomics datasets, AITL outperformed existing transfer learning methods in predicting drug response, showing promise for precision oncology. A domain in this context is defined by a raw input feature space and a probability distribution, while a task is associated with a label space and a predictive function learned from training data. Transfer learning aims to improve generalization on a target task using knowledge from both a source domain and a target domain. It can be categorized into unsupervised, transductive, and inductive transfer learning. In unsupervised transfer learning, there are no labels in either domain, while in transductive transfer learning, the source domain is labeled but the target domain is not. In inductive transfer learning, the target domain is labeled, and the source domain can be labeled or unlabeled. There are three approaches: deep metric learning, few-shot learning, and weight transfer. Deep metric learning is suitable for any number of samples in the target domain, few-shot learning focuses on small sample sizes, and weight transfer requires a large number of samples. Few-shot learning is ideal for small sample sizes in the target domain, involving training a classifier with limited examples from new classes. Various methods like Prototypical Networks and adversarial learning have been proposed for few-shot learning to improve classification accuracy. In this section, Generative Adversarial Networks (GANs) are introduced, which aim to learn the distribution of input data through a minimax framework with a discriminator and a generator. Adversarial transfer learning works on matching class-invariant features from different domains. The majority of literature focuses on transductive transfer learning where the source domain is labeled and the target domain is unlabeled. Transfer learning involves using labeled data from a source domain to learn from an unlabeled target domain. Adversarial transductive transfer learning, also known as domain adaptation, is commonly used in various applications like image segmentation, classification, speech recognition, and domain adaptation. The goal is to make the features extracted from both domains similar to deceive discriminators. Precision oncology aims to personalize cancer treatment based on genomic data, but only a small percentage of patients currently benefit. Precision oncology aims to personalize cancer treatment based on genomic data, but only about 5% of patients can benefit due to the complexity of drug response. Pre-clinical pharmacogenomics datasets, such as cancer cell lines and patient-derived xenografts, serve as reliable proxies to study genomic associations with treatment response. These datasets allow screening with hundreds of drugs, which is not feasible for patients, and are often larger than patient datasets. Pre-clinical datasets are advantageous for developing computational methods for drug response prediction. Various methods, such as ridge regression and deep neural networks, have been used to predict drug response from genomic data. The goal is to improve the accuracy of drug response prediction by integrating multiple data types and using transfer learning between different domains. Adversarial Inductive Transfer Learning (AITL) aims to improve learning by utilizing source and target domains in pharmacogenomics. Source domain involves gene expression data from cell lines to predict drug response, while target domain uses data from patients to predict drug response in a different form. AITL addresses differences in probability distributions and tasks between the two domains. Adversarial Inductive Transfer Learning (AITL) addresses discrepancies in input and output spaces between source and target domains in pharmacogenomics. The proposed AITL method uses input data from both domains to make predictions for the target domain, resolves output space differences, and extracts salient features through a neural network with four components. The AITL method uses a multi-task subnetwork with shared and task-specific towers to address small sample size challenges in the target domain. It assigns cross-domain labels to source samples and uses adversarial learning to make features domain-invariant. Class-wise discriminators further reduce input space discrepancies. The AITL method utilizes a multi-task subnetwork to tackle small sample size challenges in the target domain. It employs adversarial learning to make features domain-invariant and class-wise discriminators to reduce input space discrepancies. The AITL cost function includes classification, regression, and adversarial losses, optimized end-to-end. The proposed method overview is shown in Figure 1, where the feature extractor learns features for source and target samples, the multi-task subnetwork makes predictions and assigns cross-domain labels, addressing output space discrepancies. The feature extractor component, denoted as f(.), learns salient features in lower dimensions for input data from either the source (S) or target (T) domain. It is a one-layer fully-connected subnetwork with batch normalization and ReLU activation. The extracted features are used to make predictions for target samples and address input space discrepancies. The multi-task subnetwork component is designed to make predictions for target samples and address domain discrepancies. It consists of a shared layer and task-specific towers for regression and classification tasks. The performance is evaluated based on binary-cross entropy loss for classification and mean squared loss for regression tasks. The component outputs predicted labels for target samples and cross-domain labels for source samples. The multi-task subnetwork adapts the output space of the source and target domains by assigning cross-domain labels to the source domain. It predicts IC50 values for cell lines and binary response outcomes for patients, with shared and task-specific layers for regression and classification tasks. The component addresses the discrepancy in input space by adversarial learning of domain-invariant features. A discriminator classifies features into their domain, while the feature extractor learns domain-invariant features to fool the discriminator. The goal is for the global discriminator to not distinguish between cell line and patient features. In the driving application, AITL reduces input domain discrepancy through class-wise discriminators. Features from target samples and source domain are sent to corresponding class-wise discriminators. The goal is to learn domain-invariant features for specific class labels. Class-wise discriminators like DCi are one-layer fully-connected subnetworks with Sigmoid activation. The adversarial loss ensures that the discriminator for responder samples cannot distinguish between cell line and patient features. In the driving application, AITL reduces input domain discrepancy through class-wise discriminators with Sigmoid activation function. The cost function is designed with adversarial regularization coefficients for global and class-wise discriminators. Various datasets were used as source and target domains, including GDSC, TCGA, clinical trial patient datasets, and PDX dataset. Gene expression data were obtained from different sources for analysis. In the supplementary material of (Gao et al., 2015), patient datasets with comparable drug response measures were selected. Preprocessing steps were applied to normalize gene expression and drug response data. Experiments were designed to compare AITL against baselines for drug response prediction without transfer learning or with adversarial transductive transfer learning. Based on patient/PDX datasets for different drugs like Bortezomib, Cisplatin, Docetaxel, and Paclitaxel, AITL was compared against ADDA and ProtoNet for transfer learning. The drugs have different mechanisms and are used for various cancers, such as Docetaxel for breast cancer and Bortezomib for multiple myeloma. The datasets selected cover various anti-cancer drugs for multiple myeloma patients. An ablation study was conducted on AITL components, including AITL-AD, AITL-DG, and AITL-DC, to assess their impact. AITL-AD lacks adversarial adaptation components, AITL-DG excludes the global discriminator, and AITL-DC omits class-wise discriminators. Baselines were trained and tested on the same data. The baselines were trained and tested on the same data using binarized IC50 labels. AITL utilized a gradient reversal layer for minimax optimization and underwent 3-fold cross validation to tune hyper-parameters based on AUROC. Training and validation were done on two folds of source and target samples, with the third fold used for testing. Hyper-parameters for AITL included the number of nodes in hidden layers, learning rates, and mini-batch size. The hyper-parameters tuned for AITL included the number of nodes in hidden layers, learning rates, mini-batch size, weight decay coefficient, dropout rate, number of epochs, and regularization coefficients. The final selected settings for each drug and method are provided in the Appendix. Adagrad was used for optimization, and different implementations were used for the baselines and other methods. AITL outperformed baselines in terms of AUROC and AUPR for all studied drugs, showing that addressing discrepancies in input and output spaces leads to better performance. AITL also achieved better AUROC compared to state-of-the-art methods like ADDA for most drugs. Addressing discrepancies in both input and output spaces leads to significantly better performance in AUROC and AUPR for most drugs compared to methods like ProtoNet and ADDA. AITL outperformed these methods in all metrics for all drugs, showing competitive performance against other methods like Geeleher et al. (2014) and MOLI. The AITL method outperforms ProtoNet and ADDA in addressing input and output space discrepancies for most drugs, except for Paclitaxel where AITL\u2212D G performs better due to the drug's heterogeneous target domain. This highlights the importance of input and output space adaptation for better prediction performance. The study found that ProtoNet and ADDA did not outperform the method of Geeleher et al. (2014) and MOLI baselines in pharmacogenomics due to limitations such as the depth of the backbone network and imbalanced training examples. ProtoNet's performance is hindered by the smaller sample size in pharmacogenomics compared to image classification, while ADDA's lower performance may be attributed to the lack of end-to-end training of the classifier. The study highlighted the importance of end-to-end training of the classifier and global discriminator in improving performance compared to ADDA in pharmacogenomics. Including more patient samples and drugs can enhance generalization capability. The use of multiple genomic data types is also beneficial, although not considered in this work due to data availability constraints. In pharmacogenomics, AITL can be used for transfer learning to predict long-term clinical labels using short-term clinical data. It can also aid in diagnosing rare cancers with small sample sizes by leveraging gene expression data from prevalent cancers like breast cancer. In pharmacogenomics, AITL is introduced as a new problem in transfer learning that addresses discrepancies in both input and output spaces. AITL utilizes a feature extractor to learn features for target and source samples, a multi-task subnetwork for output space discrepancy, and global and class-wise discriminators for input space discrepancy. AITL adapts gene expression data from cell lines and patients in pharmacogenomics. It can also be used for rare cancer diagnosis and predicting long-term clinical labels. AITL outperformed state-of-the-art baselines in terms of AUROC and AUPR for four different drugs. The results show the benefits of addressing discrepancies in both input and output spaces, making AITL beneficial for precision oncology. Future research directions for precision oncology include incorporating the TCGA dataset for unsupervised transfer learning to improve domain-invariant features between cell lines and cancer patients. There is potential to enhance prediction performance by considering the impact of chemical structures of drugs in genomic-level transfer learning. AITL currently borrows information indirectly between input domains, but there is potential to exchange this information more explicitly. Theoretical analysis on transfer learning in this context has not been explored and is left for future work. In future research, incorporating the TCGA dataset for unsupervised transfer learning to improve domain-invariant features between cell lines and cancer patients is suggested. Enhancing prediction performance by considering the impact of chemical structures of drugs in genomic-level transfer learning is also recommended. Additionally, explicitly exchanging information between input domains in AITL is proposed for improved results. Theoretical analysis on transfer learning in this context is identified as an area for future exploration. The curr_chunk discusses various clinical trials and studies involving different chemotherapy drugs such as Cisplatin, Docetaxel, and Paclitaxel targeting different entities like patients, cell lines, and animal models."
}