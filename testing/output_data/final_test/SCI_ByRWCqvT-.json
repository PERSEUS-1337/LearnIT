{
    "title": "ByRWCqvT-",
    "content": "This paper presents a new method for transfer learning by learning to cluster using similarity information. Pairwise constraints are used to reduce categorical information, enabling the learning of a similarity function and clustering network for domain adaptation and cross-task transfer learning. Two novel approaches are introduced for transfer learning based on this similarity function. Our approach focuses on learning a clustering network with a transferred similarity metric for training inputs and cross-task learning. We propose a framework to reconstruct semantic clusters and estimate their number using a robust clustering algorithm. Our method achieves state-of-the-art results on challenging cross-task problems like Omniglot and ImageNet, as well as cross-domain transfer tasks like Office-31 and SVHN-MNIST. The approach discussed focuses on domain adaptation and the use of transfer learning to address challenges in obtaining labeled datasets. It highlights the importance of transferring knowledge across tasks or domains, especially in cases where data distributions change dynamically. Many strategies have been explored in cross-task transfer learning, with a focus on classification across datasets with the same categories. In the computer vision community, transfer learning strategies have been widely adopted to apply features learned from a deep neural network to various tasks. Prior works in cross-task transfer learning typically require labeled target data, but when labels are unavailable, unsupervised approaches like clustering are used. This paper explores what can be transferred besides features to support cross-domain and cross-task learning, focusing on a learned similarity function for clustering. The curr_chunk discusses the use of a similarity prediction function in clustering to enable cross-task and cross-domain transfer in unsupervised learning. It proposes an objective function that combines deep neural networks with learnable clustering, showcasing the benefits of transferring knowledge through predictive pairwise similarity. This method allows for the integration of classification and domain discrepancy losses, demonstrating the flexibility and effectiveness of the approach. Transfer learning aims to leverage knowledge from the source domain to help learn in the target domain, focusing on performance in the target domain. The approach utilizes a similarity prediction function for clustering in cross-task and cross-domain scenarios with deep neural networks, achieving state-of-the-art results in various benchmarks. Our work explores transferring pairwise similarity as meta-knowledge to improve performance in the target domain. Cross-task Transfer Learning has shown improved performance in various vision tasks by leveraging features learned from ImageNet classification. Our work explores transferring pairwise similarity in an unsupervised setting to improve learning. We address domain shift by transferring extra information from an auxiliary dataset, leading to a performance boost. Constrained clustering algorithms utilize constraints, such as pairwise information, to learn a distance metric. The approaches for clustering can be categorized into four groups: metric learning with clustering algorithms like K-means, constraints-based clustering loss formulations, constraints for both metric learning and clustering objectives, and generic clustering algorithms like K-means, LSC, and LPNMF. These approaches are summarized in survey papers. Our approach involves a learnable clustering objective (LCO) and pairwise similarity for cross-domain transfer. The proposed clustering strategy in cross-domain transfer utilizes constrained clustering methods in a semi-supervised setting. Ground-truth constraints are sparsely available in this approach. The goal is to transfer knowledge from a source dataset with categorical labels to a target dataset where labels are unknown. The learning is unsupervised, and the scenario is divided into two cases. The scenario involves unsupervised learning with unknown target labels. It is divided into two cases: one with different categories for transfer across tasks, and the other with a domain shift where input data distributions differ. Domain adaptation approaches focus on the latter case, aligning with common benchmarks for evaluating transfer learning performance. An auxiliary dataset is used to split the source data, containing labeled data and potentially different categories. Transfer learning involves scenarios with different categories for transfer across tasks and domain shifts where input data distributions differ. In the cross-task scenario, only labeled auxiliary data A and unlabeled target data T are included. To address transferring across tasks with different categories, the categorization problem is first reduced to a same-task problem using transductive transfer learning. The cluster structure in the target data is then reconstructed using predictions from the transformed task. Transfer learning involves transferring across tasks with different categories and domain shifts. The categorization problem is transformed into a pairwise similarity prediction problem using labeled auxiliary dataset A and unlabeled target dataset T. A transformation function R is specified to predict similarities between data instances, which is then used to learn a pairwise similarity prediction function G. Finally, constrained clustering algorithms are used to infer the unknown target categories Y T from the predicted similarities. The problem setting involves unsupervised domain adaptation using labeled dataset A (ImageNet) and domains S and T from the Office-31 dataset. The goal is to improve classification on domain T by incorporating information from A, S, and T. The approach utilizes a learning objective inspired by constrained clustering, incorporating noisy predicted pairwise similarities to enhance performance. Inspired by constrained clustering, a contrastive loss is constructed for clustering using softmax classifier outputs. Output nodes represent probabilistic assignments to clusters, guided by pairwise similarity. KL-divergence is used to evaluate similarity between output distributions. The contrastive loss for clustering is constructed using KL-divergence to evaluate the distance between cluster assignment distributions of data instances. The cost function for similar pairs is symmetric, while dissimilar pairs are evaluated using a hinge-loss function. The total loss is defined based on a similarity prediction function. The total loss for clustering is defined as a contrastive loss using a learnable function G, trained with an auxiliary dataset A. This approach eliminates the need for defining cluster centers or applying predefined metrics on feature representations. Instead, divergence is calculated directly on cluster assignments, allowing for joint optimization of feature representation and clustering through back-propagation in deep neural networks. The use of deep convolutional neural networks is preferred for their efficiency in vision tasks, with network architecture inspired by BID39. We design a network architecture inspired by BID39 to predict image-level semantic similarity. Instead of using Siamese architecture, we add a pair-enumeration layer on top of the feature extraction network. The pair-enumeration layer concatenates all pairs of feature vectors within a mini-batch. The architecture includes a hidden fully connected layer and a binary classifier trained with standard cross-entropy loss. The network architecture for predicting image-level semantic similarity includes a pair-enumeration layer on top of the feature extraction network. The binary classifier is trained with standard cross-entropy loss using binary similarity labels. The inference process outputs predictions for all similarity pairs in a mini-batch, with a binarized probability threshold of 0.5. The pairwise similarity prediction network is denoted as G and works as a static function once learned. To efficiently utilize pair-wise information, a pair-enumeration layer is combined with equation 5. The constrained clustering network (CCN) for transfer learning across tasks combines the pair-enumeration layer with equation 5. The architecture uses deep neural networks to reconstruct semantic clusters and optimize LCO. In the cross-domain case, labeled source data enables the use of LCO for target data and classification loss for source data during training. The training procedure for cross-domain transfer involves mixing both source and target data in a mini-batch, applying different losses. The loss function for cross-domain transfer in a mini-batch is formulated as Lcd, utilizing Lcluster and Lcls from network f. The learned classifier can be directly applied to target data, picking the maximum probability for the predicted class. The loss function transfers knowledge through constraints to regularize the process. Our approach for cross-domain transfer involves transferring knowledge through constraints to regularize the learning of the classifier. The architecture has the flexibility to be combined with other domain adaptation strategies. The experiments include evaluations with four image datasets, covering both cross-task and cross-domain schemes. The Omniglot dataset contains 1623 different handwritten characters from 50 alphabets, separated into background and evaluation sets. The author evaluates the use of Omniglot bg as the auxiliary dataset and Omniglot eval as the target data. The goal is to cluster Omniglot eval without labels using a neural network with convolution layers. Inputs are resized and normalized, with mini-batches sampled from random characters. The loss function is two-class cross entropy. The Constrained Clustering Network (CCN) reconstructs semantic clusters in the target dataset using a supervised loss function optimized by stochastic gradient descent. It consists of convolution and fully connected layers, with the output nodes representing potential clusters. The network is trained end-to-end with randomly sampled images per mini-batch, and the G function used by LCO is fixed during optimization. The input data preparation for Omniglot eval involves using 100 images per mini-batch. The dense pairwise similarity predictions from G are utilized by LCO, with the only hyper-parameter being \u03c3 set to 2 for all experiments. Omniglot eval contains 20 alphabets with varying numbers of characters (20 to 47). The reconstruction performance is evaluated under different numbers of ground-truth clusters, testing scenarios where the number of characters (K) in an alphabet is known or unknown. In the experiment, the number of characters in an alphabet (K) is set to 100, larger than the largest dataset. Various constrained clustering algorithms are used to reconstruct semantic clusters, with a focus on robustness to noise. Pairwise constraints are provided to all algorithms, with 400 images and 160000 predicted similarities from G. The full pairwise similarities are presented in random order to all algorithms. In the experiment, various constrained clustering algorithms are evaluated using pairwise constraints. Baseline approaches include K-means, LPNMF, LSC, ITML, SKKm, SKLR, SKMS, CSP, and MPCK-means. Evaluation metrics include normalized-mutual information (NMI) and clustering accuracy (ACC). ACC measures the classification accuracy based on the mapping between predicted clusters and ground-truth labels. The experiment evaluated various constrained clustering algorithms using pairwise constraints. ACC is crucial for generating coherent clusters, with CCN achieving top performance on both metrics. CCN demonstrated strong robustness in unknown K scenarios, with 78.1% average accuracy. Compared to CSP, CCN outperformed with a large gap. MPCK-means worked well with known clusters but dropped dramatically at K=100. CCN achieved 94% clustering accuracy on Old Church Slavonic Cyrillic. The results demonstrate the feasibility of reconstructing semantic clusters using noisy similarity predictions. Utilizing pairwise constraints for metric learning and clustering loss optimization leads to better performance, with CCN outperforming other algorithms. Algorithms that optimize clustering loss with constraints show better robustness, while those focusing only on metric learning fall behind. Jointly optimizing metric and clustering is crucial for improved performance. The importance of jointly optimizing the metric and clustering for robustness against noisy similarity prediction is confirmed. The accuracy of G in the experiment is comparable to Matching-Net BID34. Binarized predictions show improved precision and recall compared to random guessing, but still pose challenges for constrained clustering. The robustness of constrained clustering is related to the density of pairs in a mini-batch. Gradients from wrongly predicted pairs may cancel out, leading to better clustering results. Predicting the number of clusters (N C) is challenging, but feasible with pairwise similarity information. Evaluation involves comparing the number of dominant clusters (N DC) with the true number of categories (N C gt) in a dataset. In comparison to the baseline approach SKMS BID1, our approach TAB5 estimates K automatically and achieves a score of 16.3. The feasibility of estimating K with predicted similarity is demonstrated by a difference of N DC d and N C gt d smaller or equal to 3 in 10 out of 20 datasets from CCN's prediction. The scalability of our approach is shown by applying it to the ImageNet dataset, where Resnet-18 is used for both G and CCN with pre-trained weights. CCN outperforms K-means, LSC, and LPNMF in clustering algorithms using Resnet-18's output. It achieves 73.8% ACC when K is known and 65.2% when unknown, surpassing K-means by a large margin. Office-31 BID26 dataset with images from Amazon, DSLR, and Webcam domains is used for domain adaptation evaluation. The G function learns semantic similarity from ImageNet with Resnet-18 backbone. Training process follows standard protocols for unsupervised domain adaptation using deep neural networks. Mini-batches consist of 32 labeled source samples and 96 unlabeled target samples. Target dataset has no labels and requires a sufficient mini-batch size. Our approach focuses on unsupervised cross-domain transfer, utilizing a loss function optimized by stochastic gradient descent. Implementations include CCN +/++, DANN (RevGrad), and JAN with a ResNet backbone. Results show CCN + outperforming DANN and JAN, reaching 77.5% accuracy. Combining CCN + with DANN further boosts performance, indicating the effectiveness of LCO in mitigating transfer issues. The effectiveness of LCO in mitigating transfer issues is further demonstrated by using a deeper backbone network, ResNet-34, achieving 77.9% accuracy with source-only, 81.1% with CCN +, and 82% with CCN ++. The dense similarity predictions benefit our approach, despite low precision for similar pairs. In this study, the authors utilized various datasets such as SVHN, MNIST, and Omniglot bg to train networks from scratch. They achieved top performance with 89.1% accuracy, showing a significant improvement over previous methods. The transfer of pairwise similarity predictions proved to be beneficial, leading to robust optimization of features and clustering outputs within a neural network. The experiments demonstrated strong advantages of cross-task and cross-domain transfer learning. The study showed significant benefits of using semantic similarity predictions for cross-task and cross-domain transfer learning, leading to state-of-the-art results across multiple datasets. The performance of the proposed framework relies on the robustness of constrained clustering and the similarity prediction function. Future work may explore learning the similarity function in challenging scenarios, such as when there are few categories in the source or a large domain discrepancy between source and target. The study was supported by the National Science Foundation and National Robotics Initiative. The similarity prediction function performance was evaluated using N-way tests with Omniglot and MNIST datasets. Using deeper pre-trained networks like Resnet-50 showed a significant performance boost compared to state-of-the-art domain adaptation algorithms. Our approach involves transferring more information from an auxiliary dataset due to memory limitations. We used a small dataset (MNIST) with a small network for clustering analysis. The MNIST dataset consists of handwritten digits with 60k training and 10k testing images. Raw pixels were normalized and fed into networks for training, which was run five times under each combination of factors. The clustering training involved running five times under different factors, with a mini-batch size of 256 and 235 mini-batches per epoch. Optimization was done for 15 epochs using stochastic gradient descent. The predicted cluster was assigned based on the clustering networks. To simulate performance variations, pairs were flipped based on designated recall percentages. The precision of similar/dissimilar pairs depends on the recall of each type, allowing control over the recall. The recall of similar and dissimilar pairs was gradually reduced from one to zero at intervals of 0.1 to evaluate clustering performance. The NMI score was robust to low similar pair recall, even below 0.5, while the effect of recall on dissimilar pairs was divided at 0.5. The clustering was more robust against noise in similarity prediction with higher recall values. In clustering, performance is robust to noise in dissimilar pairs if recall is above 0.5 but fails if below. Robustness to similar pair recall is desirable as it's easier to predict dissimilar pairs. The density of pairwise relationships is crucial for clustering robustness, with D = 1 using all pairs and D = 0.1 using only 1 out of 10 constraints. When density D = 0.1, only 1 out of 10 constraints is used, leading to better utilization of pairwise information in a mini-batch. Gradients from false similar pairs are overridden by true similar pairs, aiding faster convergence of loss. Decreasing density shrinks the bright region in FIG7. Enumerating full pairwise relationships has negligible computation time overhead on GPU. Memory consumption is limited as only predicted distributions need to be enumerated for clustering loss calculation. In MNIST experiments, clustering loss calculations require enumerating distributions. Increasing the number of output categories does not significantly affect clustering performance."
}