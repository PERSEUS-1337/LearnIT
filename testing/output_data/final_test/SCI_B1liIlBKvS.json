{
    "title": "B1liIlBKvS",
    "content": "Current literature in machine learning suggests that unaligned, self-interested agents struggle to develop an emergent communication channel. A new sender-receiver game was introduced to explore communication in partially-competitive scenarios, with a focus on evaluation. It was found that communication can emerge in such scenarios, with selfish communication linked to cooperation levels. Using LOLA improved stability and performance, especially in competitive scenarios. Additionally, discrete protocols were found to be more effective for learning cooperative communication than continuous ones. Multi-agent RL is commonly used for fully-competitive, zero-sum games. In partially competitive games, the focus is on finding the best learned agent or team at test time. Unlike fully cooperative games, where self-play can optimize rewards, general-sum games involve both common interest and conflict. Care must be taken in defining strategies in these games. In general-sum games, the best agent is not necessarily the one that outperforms all opponents, as cooperation may be necessary to achieve the highest reward. Agents must learn to coordinate with each other by training together or understanding opponent intentions at test time. This allows for comparing learned agents and optimizing rewards in partially competitive games. The curr_chunk discusses the need for ad-hoc comparison of learned agents at test time and the challenges in adapting to opponent intentions in self-play methods. It also mentions the analysis of learning algorithms in general-sum MARL for cooperation and resolving social dilemmas. Investigations of emergent communication have mostly focused on fully-cooperative games. Singh et al. (2018) studied learning in mixed cooperative-competitive scenarios, using parameter sharing between opponents. Their results in the competitive scenario involve masking out all communication. Previous attempts to learn emergent protocols in competitive games have failed unless more complex learning rules are adopted. Comparing learning algorithms trained together is crucial to avoid issues when comparing emergent communication agents with different protocols. In a study on learning in mixed cooperative-competitive scenarios, Singh et al. (2018) found that comparing learning algorithms trained together is essential to avoid issues when evaluating emergent communication agents with different protocols. To address this, a meta-learning approach is suggested to synchronize protocols between agents. The study aims to provide a critical analysis of existing beliefs and sets rigorous criteria for evaluating communication without proposing new architectures or learning rules. The curr_chunk discusses language emergence through a sender-receiver game, where players coordinate to transfer information effectively using cheap talk messages. The framework is influenced by economics, philosophy, and classical game theory. The curr_chunk discusses the feasibility of communication in a sender-receiver game using standard learning rules in RL, focusing on information transfer in communication protocols. This approach differs from classical game theory's equilibrium analysis and aligns with emergent communication and evolutionary signaling studies. In a modified sender-receiver game, agents with varying levels of conflict of interest communicate through messages to achieve their respective targets. The sender selects a target angle and outputs a message, while the receiver interprets the message to take an action. The goal is for the receiver's action to match the sender's target as closely as possible. In a modified sender-receiver game, agents communicate through messages to achieve their targets. The game is fully competitive with a loss between actions and targets. The game involves partially cooperative/competitive general-sum games with MLP agents and ReLU nonlinearities. Targets are sampled from a circle, and the sender outputs a message to the receiver. The sender outputs a message sampled from a categorical distribution over a vocabulary, and the receiver deterministically outputs an action. Errors are calculated using L1 loss on a circle, with training done through gradient descent in a stochastic computation graph style. Training involves 30 epochs of 250 batches, with a batch size of 64, and both agents are trained using Adam. Evaluation is done on a fixed test set of 100 equidistant points by taking the arg max of output distributions. Hyperparameter searches are conducted with Or\u00edon. Hyperparameter searches are conducted using Or\u00edon with a fixed budget of 100 searches, exploring agents' learning rates, hidden layer sizes, vocabulary size, and entropy regularization. Results are averaged over 5 random seeds and metrics over the last 10 epochs for stability. Communication evaluation is based on the sum of agents' L1 losses, with non-communication resulting in random guessing. The average loss for both players is the expected value of the loss, indicating information transfer in the emergent communication space. Lower loss values indicate more informative learned protocols. Evidence of information transfer does not necessarily mean effective communication has been achieved. The sender's rate is static, and the receiver learns the sender's initial mapping of targets to messages. The receiver can dominate by choosing a specific strategy, leading to optimal information transfer. Differences in communication need to be delineated to avoid manipulation between agents. Communication between agents can manifest as domination of one agent over the other, modeled as cue-reading or sensory manipulation. Communication requires both agents to benefit, implying cooperation. In competitive scenarios, distinguishing between manipulation and cooperation is challenging. The focus is on the emergence of cooperative communication, where both agents perform better than fully-exploited losses. In a partially competitive scenario, agents aim to optimize common interest and fair division of conflict-of-interest loss. Using L2 metric for hyperparameter search, agents learn to cooperatively communicate without special learning rules, contrary to current literature. The performance decreases proportionately to the bias, affecting information transfer with communication inversely proportional to conflict of interest. Results show a relatively fair protocol learned by agents, deteriorating in competitive scenarios. In competitive scenarios, communication becomes less effective as bias increases, impacting information transfer. Fully-exploited communication is dominant in cooperative games but requires cautious cooperation in competitive games. Training curves of best hyperparameters are shown for different bias levels. The training curve of the best hyperparameters found in previous experiments is shown, indicating stable communication learned through LOLA, a learning rule that promotes cooperative behavior. LOLA is tested in various configurations with a hyperparameter search, incorporating improvements from previous studies. LOLA on the sender is ineffective, but LOLA on the receiver and both agents leads to better performance, showing the necessity of cooperation in competitive scenarios. Comparing basic agents with LOLA agents reveals performance gains from both agents improving. 2-step LOLA slightly outperforms 1-step, but 3-step does not provide further improvement. The study shows that 2-step LOLA outperforms 1-step, but 3-step does not offer additional improvement. The gains in performance are attributed to both agents improving, not just one. The comparison between discrete and continuous communication suggests that real-valued scalar messages with Gaussian distribution lead to better learning with selfish agents. The study compares the performance of 1-step and 2-step LOLA in continuous communication. Results show that while continuous communication with Gaussian distribution leads to better learning, there is little evidence of cooperative communication between agents. The study compares the performance of 1-step and 2-step LOLA agents in communication. Results show a preference for cooperative behavior with discrete messages. Evidence is presented against the idea that selfish agents do not learn to communicate. Continuous communication achieves high information transfer, but gains in performance may be due to receiver manipulation. Hyperparameter runs show trends between continuous and discrete communication for 1-step LOLA agents. The study compares 1-step and 2-step LOLA agents in communication. Continuous communication leads to manipulation, while discrete communication encourages cooperation. The research highlights the importance of distinguishing between information transfer, communication, and manipulation. LOLA improves selfish communication by enhancing agents' performance and stability. Using a discrete communication channel promotes cooperative learning. Using a discrete communication channel promotes cooperative learning in contrast to continuous communication, which shows little evidence of cooperation. Selfish emergent communication combines competitiveness with cooperativeness, as agents must learn and use a protocol simultaneously, leading to cooperation even in competitive scenarios. Selfish agents in a communication protocol must learn to cooperate. The game can be fully cooperative or fully competitive based on the bias parameter. The sum of losses always equals 180\u00b0, making the game constant-sum and fully competitive. The fairness of the game is proven by minimizing losses between agents. The fairness of the Circular Biased Sender-Receiver Game is proven by minimizing L2 losses between agents, which is achieved when the action is halfway between both agents' targets."
}