{
    "title": "HklSf3CqKm",
    "content": "This paper focuses on dictionary learning, specifically sparse coding, and introduces a subgradient descent algorithm that can recover orthogonal dictionaries with random initialization. The algorithm is able to achieve this on a nonconvex L1 minimization formulation of the problem, with mild statistical assumptions on the data. The analysis also provides tools for characterizing landscapes of nonsmooth functions, which can be useful for training deep networks with nonsmooth activations. Preliminary experiments show that the algorithm works well in recovering orthogonal dictionaries. Dictionary learning (DL) involves finding a compact representation of data by learning a basis and coefficients. DL has various applications in image processing and computer vision. Many heuristic algorithms have been proposed to solve DL effectively. Recent research has started to shed light on when and how DL can be successfully solved. Under certain conditions, it has been shown that a complete basis can be recovered from the data if the coefficients are very sparse. Recent research has focused on recovering complete bases from data with very sparse coefficients. Methods based on nonconvex optimization with model-dependent initialization have limitations on real data. Convex relaxations using the sum-of-squares SDP hierarchy can recover overcomplete bases with linear sparsity but require expensive computation. Alternatively, a nonconvex problem with arbitrary initialization can recover complete bases in the linear sparsity regime. The first-order gradient descent with random initialization is shown to have similar performance as the second-order optimization method. Dealing with nonsmooth functions is a challenge, but the 1 norm is commonly used in practical deep learning for promoting sparsity in coefficients. Despite its nonsmoothness, the 1 norm allows for scalable numerical methods like the proximal gradient method and alternating direction method. Previous analyses have focused on characterizing the function landscape of a nonconvex formulation of deep learning. In this paper, the focus is on learning orthogonal dictionaries from data generated by a fixed unknown orthogonal matrix. The proposed approach involves solving a nonconvex and nonsmooth optimization problem to recover the matrix A. The use of the 1 norm formulation allows for neat analysis and practical algorithms, contrasting with previous methods that introduced substantial analysis difficulty. The paper focuses on learning orthogonal dictionaries from data generated by an unknown orthogonal matrix. It involves solving a nonconvex optimization problem to recover matrix A. The optimization landscape is shown to be benign with high probability, and a simple algorithm can recover A in polynomial time. The paper presents a poly(m, -1)-time algorithm for learning orthogonal dictionaries from data generated by an unknown orthogonal matrix. The algorithm runs Riemannian subgradient descent on a formulation with lower sample complexity O(n^4) and shows that each desired solution is a local minimizer with a large basin of attraction. The paper introduces a poly(m, -1)-time algorithm for learning orthogonal dictionaries from data generated by an unknown orthogonal matrix. It utilizes elements from nonsmooth analysis, set-valued analysis, and random set theory to study nonconvex, nonsmooth optimization problems. The algorithm emphasizes practicality by using the subgradient of the nonsmooth objective, making it cheaper to evaluate than the smooth objective. The algorithm utilizes Euclidean projection for computation efficiency. Nonsmooth analytic tools like the Clarke subdifferential are crucial for optimizing locally Lipschitz functions on Riemannian manifolds. Nonsmooth optimization techniques for locally Lipschitz functions on Riemannian manifolds have been developed, including new methods like gradient sampling. Convergence results have been adapted for nonsmooth problems, with a focus on local convergence rather than just global convergence. Recent work has focused on analyzing nonconvex nonsmooth problems, including regularized empirical risk minimization and phase retrieval with 1 loss. There is also a surge in studying one-hidden-layer ReLU networks, with local landscape characterizations requiring strong initialization procedures and global results achieved through new loss functions or PDEs. In modern machine learning and data analysis, nonsmooth functions are used to encode structural information like sparsity or low-rankness. Nonsmooth problems are prevalent in optimal control and economics, with applications in deep learning where nonsmooth activations like ReLU are used. The landscape of non-smooth dictionary learning is algorithm-independent and initialization conditions are satisfied by random initialization with high probability. The technical ideas around nonsmooth analysis, set-valued analysis, and random set theory are relevant to applications involving ReLU. Given an unknown orthogonal dictionary A, the goal is to recover A through observations of coefficient vectors sampled from the Bernoulli-Gaussian distribution. The focus is on the first-order geometry of the non-smooth problem setup. The focus is on the first-order geometry of the non-smooth objective in Euclidean space. The problem becomes non-convex when minimizing subject to a constraint. The Riemannian sub-differential on S n\u22121 is defined, and a point is stationary if 0 is in the sub-differential. Set-valued analysis is required due to the set-valued mapping of the subdifferential. The addition of two sets is defined as the Minkowski summation. The Minkowski summation defines the addition of two sets. The expectation of random sets extends this concept. The Hausdorff distance between sets is also defined. Basic properties of the Hausdorff distance are provided. Notations for vectors and matrices are explained. The main result is the recovery guarantee for learning an orthogonal dictionary. Theorem 3.1 presents a method for recovering an orthogonal dictionary with a desired accuracy using subgradient descent. The algorithm guarantees recovery with high probability and a bounded number of iterations. The proof involves partitioning the sphere into \"good sets\" and demonstrating strong directional gradients within these sets. The curr_chunk discusses the convergence of Riemannian subgradient descent and the recovery of basis vectors using an optimization procedure. It also mentions assumptions made for simplification purposes. The curr_chunk discusses the geometry of the expected objective E[f], characterizing the function value and subdifferential set of the population objective to identify local minimizers and saddles. It shows that the population objective has no \"spurious local minima\" and each stationary point is either a global minimizer or a saddle point. The curr_chunk discusses the identification of \"good\" subsets on the sphere containing global minimizers and excluding population saddle points. It defines subsets with benign geometry for optimization algorithms. The curr_chunk discusses optimization algorithms favoring specific basis vectors in certain regions of the sphere. It presents lower bounds on directional subgradients and how geometry affects the empirical objective function with sufficient samples. The concentration argument in the curr_chunk shows that the empirical subdifferential set closely approximates the population subdifferential set with high probability. This is important for understanding the lower bounds on directional subgradients in the optimization algorithms discussed in the prev_chunk. The curr_chunk discusses the properties guaranteed by Theorem 3.6, including the uniqueness of stationary points and convergence of the Riemannian subgradient descent algorithm. It also presents an upper bound on the norm of subdifferential sets. The curr_chunk discusses the convergence properties of the Riemannian subgradient descent algorithm, ensuring the recovery of one basis with specific initialization and parameters. The optimization result shows that Riemannian subgradient descent can find the basis vector e n with a specific initialization. A random initialization on the sphere is guaranteed to fall within one of the regions with high probability. This ensures convergence to e i or -e i with independent, uniformly random initializations. The Riemannian subgradient descent algorithm can recover all basis vectors with independent, uniformly random initializations on the sphere. The algorithm guarantees recovery of standard basis vectors with high probability, up to a certain accuracy level. The main result in Theorem 3.1 is obtained by applying a rotation argument to the matrix. A key challenge is establishing the uniform convergence of subdifferential sets in Proposition 3.5. The goal is to show that the difference between \u2202f (q) and E [\u2202f ] (q) is small uniformly over q \u2208 Q = S n\u22121. The subdifferential is set-valued and random, making it difficult to analyze the concentration of random sets. The Lipschitz gradient property does not hold, so the usual covering argument is ineffective. Points not covered can have significantly different subdifferential sets. The Hausdorff distance is crucial for concentration of subdifferentials as it is closely related to the support function of sets. For convex compact sets, the Hausdorff distance is the sup difference between their support functions. This relationship is convenient for bounding the difference of subdifferential sets along specific directions. The absence of gradient Lipschitzness in the problem is attributed to the Euclidean distance not being the appropriate metric. By partitioning the space according to sign patterns, the subdifferential set becomes Lipschitz on each subset. To address this, a stronger metric dE is proposed on the sphere, which is sign-pattern aware and averages subset angles between points. The proposed metric dE on the sphere is sign-pattern aware and averages subset angles between points. To show gradient Lipschitzness in dE, the covering argument is performed, and the covering number is bounded. The indicator is non-zero with probability at most , and the expectation is bounded by O(log(1/ )). The empirical subdifferential \u2202f is shown to have a small expectation by bounding the observed proportion of sign differences. The covering number in dE is reduced to the maximum length-2 angle over any consistent support pattern. The proposed metric dE on the sphere is sign-pattern aware and averages subset angles between points. The task is to construct an \u03b7 = /n 2 covering in d 2 over any consistent sign pattern. The \u03b7-covering number in d 2 is bounded by exp(Cn log(n/\u03b7)). Sorting the coordinates in p and focusing on consecutive size-2 angles after sorting yields a proper covering for all size-2 angles by \u03b7/n. The size-2 angles by \u03b7/n will cover all size-2 angles by \u03b7. The covering number is (Cn/\u03b7) O(n) = exp(Cn log(n/\u03b7)), with the true dictionary A as the identity and random orthogonal matrices. Experimenting with different (m, n) pairs and sparsity levels, we use the Riemannian subgradient descent algorithm to recover the dictionary. Riemannian subgradient descent successfully recovers the dictionary with a base size of m \u2265 Cn^2, regardless of sparsity level \u03b8. The sample complexity requirement for guaranteed recovery may be lower than O(n^4) as suggested by simulations, with an observed rate of O(n^2) matching results from other methods. This paper presents the first theoretical guarantee for orthogonal dictionary learning using subgradient descent on a natural 1 minimization formulation. The problem becomes harder as \u03b8 increases, with success transition threshold shifting to the right. Additional experiments on large-scale instances and real images are included in the appendices. There is an O(n^2) sample complexity gap between established results and simulations, indicating room for future work. The paper presents theoretical guarantees for orthogonal dictionary learning using subgradient descent on a 1 minimization formulation. The potential for further sample complexity improvement lies in utilizing second-order information and careful algorithm-dependent analysis. The analysis has the potential to handle overcomplete dictionaries, where each column makes the dictionary approximately 1-sparse. The paper discusses the challenges of handling nonsmooth points in nonconvex optimization problems, highlighting the disconnect between theory and practice. Existing theoretical results often lack clarity on dealing with nonsmooth points, leading to potential issues in practical numerical methods. The importance of addressing nonsmooth points is emphasized by other works in the field. The paper discusses the challenges of handling nonsmooth points in nonconvex optimization problems, emphasizing the disconnect between theory and practice. Kakade & Lee (2018) warn about potential issues in ignoring nonsmooth points in machine learning optimization. The Hausdorff metric is used to measure differences between nonempty sets in R^n. The metric obeys the triangular inequality for nonempty, compact subsets X, Y, Z \u2282 R^n. Lemma A.1 states properties for convex compact sets X, Y \u2282 R^n. Proposition A.2 discusses Talagrand's comparison inequality. Proposition A.2 discusses Talagrand's comparison inequality for a zero-mean random process on a subset T \u2282 R^n. It provides a deviation inequality for sub-Gaussian matrices, showing that points in the claimed set are stationary points. The text discusses how points in the claimed set are stationary points by choosing v \u2126 = 0 in Eq. (3.5). It shows that {\u00b1e i : i \u2208 [n]} are the global minima, while other q's are saddles with a tangent direction where they are local maxima. The text discusses the stationary points in the claimed set, showing that {\u00b1e i : i \u2208 [n]} are global minima, while other q's are saddles with a tangent direction where they are local maxima. Using reparametrization and Lemma B.2, E [f ] (q) is directionally differentiable with a strictly negative second derivative, implying q is a saddle point. Theorem 3.4 guarantees that 0 / \u2208 E [\u2202 R f ] (q) whenever q / \u2208 S, introducing the reparametrization w = q 1:(n\u22121) in the region S (n+) 0. The text discusses reparametrization in the region S(n+)0, showing that for all unit vectors v in Sn-1 and s in (0, 1), there is a direction of negative curvature. The proof involves direct calculations and the definition of h_v(t) = E[g](tv) for t in (0, 1). The proof involves reparametrization in the region S(n+)0, showing a direction of negative curvature for all unit vectors v in Sn-1 and s in (0, 1). By differentiating g via the chain rule and considering points of the form tv with t \u2264 1/ 1 + v 2 \u221e, a uniform lower bound is derived. This leads to the conclusion that for all q \u2208 S, Eq. (3.8) holds. The proof in Section 3.2 establishes a valid metric on subsets of Sn-1 based on a consistent support pattern. The triangular inequality holds for any subset with the same support pattern, ensuring that the metric is well-defined. The proof also shows that the metric satisfies properties such as symmetry and identity, making it a reliable measure for distances between points on Sn-1. The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern, ensuring the metric is well-defined. It partitions Sn-1 according to support, sign pattern, and ordering of non-zero elements to give an \u03b7-covering in the d2 metric. The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern, ensuring the metric is well-defined. It partitions Sn-1 according to support, sign pattern, and ordering of non-zero elements to give an \u03b7-covering in the d2 metric. For each configuration, a covering with the same support, sign pattern, and ordering is constructed. The task reduces to bounding the covering number of A n by induction, with a properly chosen set Q r,k providing a covering. The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern, ensuring the metric is well-defined. It partitions Sn-1 according to support, sign pattern, and ordering of non-zero elements to give an \u03b7-covering in the d2 metric. For each configuration, a covering with the same support, sign pattern, and ordering is constructed. The task reduces to bounding the covering number of A n by induction, with a properly chosen set Q r,k providing a covering. Consider the set DISPLAYFORM20. We claim that Q r,k with properly chosen (r, k) gives a covering of DISPLAYFORM21. Each consecutive ratio p i+1 /p i falls in one of these intervals, and we choose q so that q i+1 /q i is the left endpoint of this interval. Such a q satisfies q \u2208 Q r,k and DISPLAYFORM22. By multiplying these bounds, we obtain that for all 1 \u2264 i < j \u2264 n, DISPLAYFORM23. Take r = 1 + \u03b7/2n, we have r n\u22121 = (1 + \u03b7/2n) n\u22121 \u2264 exp(\u03b7/2) \u2264 1 + \u03b7. Therefore, for all i, j, we have pj /pi qj /qi \u2208 [1, 1 + \u03b7), which further implies that ((p i , p j ), (q i , q j )) \u2264 \u03b7 by Lemma F.4. Thus we have for all |\u2126|\u2264 2 that (p \u2126 , q \u2126 ) \u2264 \u03b7. (The size-1 angles are all zero as we have sign match.) For this choice of r, we have k = log R/log r and thus DISPLAYFORM24 (C.28) and we have N (A n,R ) \u2264 N n. Step 3 We now construct the covering of A n \\ A n,R. For any p \u2208 A n \\ A n,R, there exists some i such that p i+1 /p i \u2208 [R, \u221e), which means that the angle of the ray (p i , p i+1 ) is in between. The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern, ensuring the metric is well-defined. It partitions Sn-1 according to support, sign pattern, and ordering of non-zero elements to give an \u03b7-covering in the d2 metric. For each configuration, a covering with the same support, sign pattern, and ordering is constructed. The task reduces to bounding the covering number of A n by induction, with a properly chosen set Q r,k providing a covering. Consider the set Q r,k with properly chosen (r, k) gives a covering of A n. Each consecutive ratio p i+1 /p i falls in one of these intervals, and we choose q so that q i+1 /q i is the left endpoint of this interval. Such a q satisfies q \u2208 Q r,k and the angle of the ray (p i , p i+1 ) is in between [R, \u221e). The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern, ensuring the metric is well-defined. It partitions Sn-1 according to support, sign pattern, and ordering of non-zero elements to give an \u03b7-covering in the d2 metric. For each configuration, a covering with the same support, sign pattern, and ordering is constructed. The task reduces to bounding the covering number of A n by induction, with a properly chosen set Q r,k providing a covering. Consider the set Q r,k with properly chosen (r, k) gives a covering of A n. Each consecutive ratio p i+1 /p i falls in one of these intervals, and we choose q so that q i+1 /q i is the left endpoint of this interval. Such a q satisfies q \u2208 Q r,k and the angle of the ray (p i , p i+1 ) is in between [R, \u221e). A numerical constant C > 0 ensures an -net of size exp(Cn log n) for Sn-1 wrt dE. The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern, ensuring the metric is well-defined. It partitions Sn-1 according to support, sign pattern, and ordering of non-zero elements to give an \u03b7-covering in the d2 metric. For each configuration, a covering with the same support, sign pattern, and ordering is constructed. The task reduces to bounding the covering number of A n by induction, with a properly chosen set Q r,k providing a covering. Each consecutive ratio p i+1 /p i falls in one of these intervals, and we choose q so that q i+1 /q i is the left endpoint of this interval. Such a q satisfies q \u2208 Q r,k and the angle of the ray (p i , p i+1 ) is in between [R, \u221e). A numerical constant C > 0 ensures an -net of size exp(Cn log n) for Sn-1 wrt dE. When m \u2265 C \u22122 n, the inequality DISPLAYFORM34 holds with probability at least 1 \u2212 exp \u2212c 2 m. The process R is the sample average of m indicator functions, and the hypothesis class H is defined as x \u2192 1 sign p x = sign q x : (p, q) is an admissible pair. The VC-dimension of H is bounded, leading to concentration results for VC-classes. The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern, ensuring the metric is well-defined. Each set in the hypothesis class can be written as the union of intersections of two halfspaces. The VC-dimension of H0 is n + 1. By applying bounds on the VC-dimension of unions and intersections, we can set t = /2 and make m large enough to complete the proof. Proposition C.6 discusses pointwise convergence for a centered random process with sub-Gaussian increments. The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern. Proposition C.6 discusses pointwise convergence for a centered random process with sub-Gaussian increments, leading to the claimed result. Universal constants c, C are used throughout the proof, with an admissible net N for Sn-1 wrt dE. The triangular inequality for the Hausdorff metric is applied, bounding terms I, II, and III to complete the proof. The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern. Proposition C.6 discusses pointwise convergence for a centered random process with sub-Gaussian increments, leading to the claimed result. With probability at least 1 \u2212 exp(\u2212c 2 m), the number of different signs is upper bounded by 2m for all p, q such that d E (p, q) \u2264 . Setting t 0 = ct \u221a m and = ct \u03b8/log m, we have that m \u2265 Cnt is a sufficient condition. The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern. With probability at least 1 \u2212 exp(\u2212cm\u03b8t 2 /log m), the probability is further lower bounded by 1 \u2212 exp(\u2212cm\u03b8t 2 /log m). Define DISPLAYFORM0 (C.85) By Proposition 3.5, with probability at least 1 \u2212 exp(\u2212cm\u03b8 3 \u03b6 2 0 n \u22123 log \u22121 m we have DISPLAYFORM1 0 log (n/\u03b6 0 ). We now show the properties Eq. (3.12) and Eq. (3.13) on this good event, focusing on S (n+) \u03b60 but obtaining the same results for all other 2n \u2212 1 subsets by the same arguments. The proof establishes a valid metric on subsets of Sn-1 based on a consistent support pattern. With probability at least 1 \u2212 exp(\u2212cm\u03b8t 2 /log m), the probability is further lower bounded by 1 \u2212 exp(\u2212cm\u03b8t 2 /log m). By Theorem 3.4, it is shown that inf \u2202f (q), ej/qj - en/qn = -sup \u2202f (q), en/qn - ej/qj. This completes the proof for Proposition 3.7, demonstrating the metric property of the Hausdorff metric. The proof involves Proposition 3.5 and Lemma B.3, showing that with high probability, certain conditions are met. By combining results from different equations, it is concluded that a specific constraint is satisfied. The proof also utilizes the mean value theorem to establish a valid metric on subsets of Sn-1. The proof utilizes the mean value theorem to establish constraints on subsets of Sn-1, dividing the index set [n-1] into three sets and performing different arguments on each set. The subgradient g(q) is bounded by 2, leading to specific inequalities for different cases when \u03b7 is less than certain values. The proof establishes constraints on subsets of Sn-1 using the mean value theorem. The subgradient g(q) is bounded by 2, leading to specific inequalities for different cases when \u03b7 is less than certain values. The proof in the curr_chunk establishes bounds on sequences in S and utilizes various inequalities to derive a desired bound on the number of iterations. The proof in Lemma 3.9 establishes bounds on sequences in S with a focus on the volume of sets. By setting specific parameters, the probability of falling into these sets is analyzed, leading to implications for random initialization in the algorithm. The algorithm will find a signed standard basis vector with high probability after multiple runs. The probability of missing any standard basis vector is bounded by a certain formula. Lemma F.1 shows that for a specific random process, the values are bounded by universal constants. Lemma F.3 states that for n \u2265 3 and \u03b6 \u2265 0, a certain inequality holds true. The proof involves a lower bound derivation using a Taylor expansion. Lemma F.4 discusses the relationship between points in the first quadrant based on their coordinates. The angle between two points in the first quadrant is calculated using trigonometric functions. A faster optimization method, GRANSO, is tested for solving constrained nonsmooth problems. It successfully identifies a basis after 1500 iterations with CPU time of 4 hours on a two-socket Intel Xeon E5-2640v4. The GRANSO optimization method identifies a basis after 1500 iterations in 4 hours on an Intel Xeon processor, showing potential for solving large-scale problems efficiently. The experiment focuses on using complete dictionaries as sparsification bases for real images. Formulation (1.1) is solved multiple times with the BFGS solver, resulting in 64 vectors after pruning and removing correlated vectors. The experiment focuses on using complete dictionaries as sparsification bases for real images. After pruning and removing correlated vectors, a final complete dictionary of 64 vectors is obtained. The learned dictionaries for the test images show adaptability to image contents, with representation coefficients concentrated around zero in a zero-centered Laplace distribution. The experiment uses complete dictionaries as sparsification bases for real images. The sparsity levels of the coefficient vectors are calculated using the norm-ratio metric, with values of 5.9135 and 6.4339 for the two images. The complete dictionaries learned are reasonable sparsification bases for the natural images."
}