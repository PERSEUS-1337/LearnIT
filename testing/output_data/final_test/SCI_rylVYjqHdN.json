{
    "title": "rylVYjqHdN",
    "content": "The goal of multi-label learning (MLL) is to associate instances with relevant labels from a concept set. A new approach is proposed for weakly supervised MLL, focusing on limited fine-grained supervision and leveraging hierarchical relationships between concepts. This approach reduces the problem to a multi-label negative-unlabeled learning problem using the hierarchy, addressed with a meta-learning approach. Our proposed meta-learning approach assigns accurate pseudo-labels to unlabeled entries in multi-label learning (MLL), achieving superior classification performance. MLL addresses scenarios with a growing number of classes, splitting high-level concepts into finer-grained ones. This work focuses on designing an accurate fine-grained classifier with limited supervision, by actively collecting informative labels and exploiting them in the learning model. The model receives a dataset with coarse-grained and few fine-grained labels, aiming to construct a predictive fine-grained model. The problem of constructing a predictive fine-grained model with limited supervision falls under weakly supervised learning. Leveraging coarse-grained labels for building a fine-grained classifier involves learning with inexact supervision, while using fine-grained labels can be seen as a multi-label variant of learning with incomplete supervision. The combination of inexact and incomplete supervision in our problem has not received much research attention. In this work, the problem of constructing a predictive fine-grained model with limited supervision is addressed. A new model is proposed that leverages the hierarchical relationship between coarse-and fine-grained labels to tackle the challenges posed by inexact and incomplete supervision. The model learns labeling assignments for unlabeled entries, called pseudo-labels, to improve the learning process. The proposed method leverages pseudo-labels to guide parameter updates on the classifier, assigning accurate labels to unknown entries and outperforming other methods in learning fine-grained classifiers with limited supervision. The model refines the label set from coarse to fine-grained concepts, improving the learning process. The goal is to learn an accurate fine-grained classifier using a data set annotated with high-level labels and a small set with fine-grained labels. Fully-supervised learning utilizes fully-annotated examples but may not train a strong classifier due to the small number of examples. It also ignores the weak supervision provided by coarse labels. One way to leverage higher-level supervision for learning fine-grained concepts is by treating unknown fine-grained labels as missing entries and using Multi-Label Learning algorithms that can handle missing labels. BID9 proposed a method called LEML to optimize model parameters accurately recovering observed training labels. The objective focuses on observed entries contributing to the optimization problem. In the context of leveraging higher-level supervision for learning fine-grained concepts, one approach is to treat unknown fine-grained labels as missing entries. However, in this setting, observed fine-grained labels are mostly deduced from their parent labels, making it challenging to exploit weak supervision from relevant coarse labels. Another possible direction is through one-class multi-label learning methods, where unobserved entries are assumed to be the opposite class of observed ones. Without careful algorithm redesign or label distribution estimation, it is difficult to accurately recover the underlying ground truths for missing entries. In the context of leveraging higher-level supervision for learning fine-grained concepts, one approach is to treat unknown fine-grained labels as missing entries. Existing solutions have attempted to address this issue but may not fully exploit the benefits of unlabeled entries. A method is sought to leverage missing entries by assigning pseudo-labels to unknown entries, aiming for optimal classification performance on fine-grained concepts. To optimize classification performance on fine-grained concepts, a method is proposed to assign pseudo-labels to unknown entries in order to leverage missing entries effectively. This approach involves training a fine-grained classifier using a validation set with fully-annotated labels, aiming to determine optimal model parameters based on validation performance. The optimization challenge is addressed by drawing inspiration from recent works in meta-learning literature. In an optimization challenge inspired by meta-learning literature, an iterative approach is used to dynamically find the best pseudo-label assignments locally at each optimization step. The goal is to minimize validation loss after a single update step by learning the pseudo-label assignment. To reduce computational cost, a simple approximation is proposed by looking at the gradient direction of the validation loss with respect to the pseudo-labels. The method is tested on a multi-label image dataset MS COCO to justify its effectiveness. The proposed method is tested on a multi-label image dataset MS COCO to evaluate its effectiveness. It outperforms three baseline methods, including a standard fully-supervised learning model and two other approaches. Results show that the proposed method consistently achieves the best performance across different settings. Despite not utilizing partially labeled examples, the standard fully-supervised approach surprisingly outperforms the other baseline methods in many cases. The learning curves of different methods in Figure 3 show that LEML and OCC initially perform well compared to fully-supervised learning but suffer from overfitting later on. The performance drop in LEML is attributed to an abundance of negative entries, while OCC's overfitting is due to simplistic assumptions on missing entries. The method's ability to recover missing entries is demonstrated in Figure 4, highlighting the correlation between recovery rate and model performance. The pseudo-labels learned from a tailored method show improved recovery of missing entries compared to naive baselines. A strong correlation exists between recovery rate and model classification performance, with accurate pseudo-label assignment leading to better classification results. The method utilizes a meta-learning strategy for weakly supervised MLL problems, resulting in improved classifier performance. The tailored method improves classifier performance by enabling better classification results than other existing solutions."
}