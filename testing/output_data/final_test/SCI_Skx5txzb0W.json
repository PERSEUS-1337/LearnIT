{
    "title": "Skx5txzb0W",
    "content": "We propose a method to address the flaws in comparing deep learning architectures by introducing a normalized expected best-out-of-n performance (Boo_n) to account for stochasticity in model training. Replicating results in deep learning research is challenging due to insufficient information provided in many papers, which limits their scientific value and usefulness to industry. Details of the experimental setup can significantly influence the results and should be provided for replication. Most deep learning training methods are inherently stochastic, leading to variability in results. Statistical techniques are necessary to handle this randomness, but they are underused in research. Instead of addressing this variability, most papers only report the performance of the best single model. The best single model performance in deep learning is not robust under experiment replication due to result stochasticity. Statistical techniques are necessary for evaluating machine learning architectures, but most existing methods focus on comparing mean performance, leading to underuse of statistical methods in research. Statistical methods are underused in research due to the focus on the best model performance rather than the mean. Researchers often train multiple models and deploy the best one based on validation set evaluation, leading to a competitive atmosphere that prioritizes publishing only the best results. Estimating performance statistically requires costly repeated experiments, which researchers may avoid. Researchers may prefer investing in model tuning over re-running experiments to save costs, especially when reviewers prioritize high-performing models over statistically sound evaluation. In some cases, focusing on the typical model's performance is more relevant than the best model's performance, making the use of mean or median appropriate. However, instead of completely abandoning reporting the best model's performance, a proposal to estimate the expected best-out-of-n performance by running more experiments is suggested. The paper discusses the concept of expected best-out-of-n (Boo n) performance, which aims to address the issues with reporting the performance of the best single model in deep learning. It provides a method to empirically estimate Boo n and presents experimental evidence supporting its effectiveness. The paper also highlights the importance of Boo n in a ML researcher's toolbox alongside traditional measures like mean or median. In deep learning, performance is often reported based on the best single model selected from multiple instances of the proposed architecture. This practice can lead to issues due to population variance in experimental results. The paper highlights the issue of population variance in experimental results when training models on different samples from the architecture's performance distribution. The correlation between validation and test results is not perfect, leading to uncertainty in the expected best result. The more samples drawn from the population, the more extreme the best result is likely to be. The number of experiments run is crucial in determining the expected best result, leading to methodological flaws and lack of comparability between research teams. This incentivizes running more experiments, favoring those with greater computational power and shifting focus towards quantitative results rather than fair architecture comparisons. The concept of \"best model performance\" is ambiguous without specifying the size of the model pool. Even for metrics like accuracy, the best model may be so unlikely that it holds no practical significance. Therefore, best model performance is not a meaningful characteristic of the performance distribution. The text discusses the importance of focusing on how a model behaves generally rather than just citing a single instance of high performance. It emphasizes the need for falsifiability in scientific theories and highlights the limitations of relying solely on the performance of the best model. Despite problems, reporting the best model's performance remains common in some ML areas, especially in deep learning papers. Reviewers at top conferences tolerate this practice, as seen in papers on datasets like Children's Book Test and SQuAD, where statistical testing is often omitted. The study at ICLR 2017 did not use statistical testing or confidence intervals in their results. Deep learning research often lacks the use of statistical methods, as shown in a survey of ICLR 2017 papers. This highlights the need for a better method of reporting architecture performance. Traditional statistical measures like mean or median may not fully capture the performance of the best model in machine learning applications. Practitioners often train multiple models and deploy the best-performing one, making it more informative to report the performance of the best model rather than just the average. This corrected best-model measure can provide valuable insights in situations where the focus is on selecting the top model for deployment. The proposed best-model measure, Boo n, aims to improve comparability between models evaluated in different numbers of experiments by normalizing results. This approach helps estimate the expected best performance and addresses issues of statistical robustness. The expected best-out-of-n performance is suggested as a more appropriate measure when selecting the top model for deployment. The calculations for Boo n are not innovative statistically and closely related to Order Statistics. Calculating Boo n of a known theoretical probability distribution clarifies the relation between Boo n and performance distribution. Making assumptions about the family of the theoretical distribution is useful for long-term model deployment or when training a single model due to hardware constraints. The analytic calculation allows us to estimate Boo n for independent identically distributed random variables with a probability distribution P. This parametric estimator is useful when sample size is small, providing lower variance due to prior information. The maximum of independent random variables with a probability distribution can be estimated analytically using the cumulative distribution function. The expected value of the maximum can be calculated by differentiating the probability density function. Numerical estimates can be obtained using major computation packages like numpy, with specific values for the standard normal distribution provided as examples. Numerical estimates of Boo n N (\u00b5, \u03c3 2 ) can be obtained by estimating the parameters of the Gaussian distribution. For discrete performance distributions, a probability mass function can be used to calculate the expected best validation performance of n models. The best model is typically chosen based on validation performance, but the focus is on the corresponding test performance. To calculate the expected test performance of the best-validation model, follow the method described in the text. To calculate the expected test performance of the best-validation model chosen from a pool of size n, substitute the value of x in Equations 2 and 4 with the expectation of the test performance X test conditional on the validation performance x val. This can be done using a bivariate Gaussian distribution with mean \u00b5 test, standard deviation \u03c3 test, and test-validation correlation \u03c1. The exact performance distribution of the model is usually unknown, so parametric estimation is used based on samples from this distribution. The empirical performance distribution is estimated from samples to compare models, using a non-parametric estimator. The distribution assigns equal weights to each sample, approximating the true performance distribution. Ranking samples from worst to best allows for a weighted average of test results. The weighted average of test results is calculated in case of a tie in validation results. The estimator used does not assume any specific performance distribution, but if known, information can be added to improve the estimate. Parametric estimators can be used for different distributions, such as the Gaussian distribution. Boo n method resolves the issue of dependence on the number of experiments run. The Boo n method resolves the issue of dependence on the number of experiments run by choosing the number of experiments to normalize. The choice of n depends on the number of candidate models a practitioner would train before choosing the best one for a target application. Researchers should characterize the architecture's performance distribution to help readers deduce the value of Boo n for their chosen n. The Boo n method allows for flexibility in choosing the number of experiments to normalize, with a focus on selecting the best model for a target application. The main reporting metric tends to converge on each task, with subsequent papers following suit to remain competitive. Using n = 5 in experiments, the AS Reader model takes about 2 hours to train on a single GPU, making it a reasonable requirement for achieving overnight results. Boo n provides a single number estimate that may be noisy, but can be aggregated with other metrics for a wider population analysis. When comparing the quantitative performance of a new model against a baseline, it is important to use appropriate statistical methods such as significance testing or confidence intervals. These methods help disentangle the effect size from uncertainty associated with noise and sample size. Analytical methods like the t-test or standard normal quantiles can be used for theoretical distributions, but computational methods like Monte Carlo are needed when the performance distribution is unknown. The text discusses the use of statistical methods like Monte Carlo or Bootstrap for calculating confidence intervals when comparing model performance. Experiments were conducted to quantify performance variation in deep learning models, with results summarized briefly. More detailed analysis can be found in the Gitlab repository or in an iPython notebook. The study analyzed the performance variation of deep learning models, showing that random variation in performance cannot be ignored. The interquartile ranges of model accuracies were compared to published results, highlighting the impact of hyperparameter variability on result variance. The study analyzed the impact of hyperparameter variability on deep learning model performance. Results showed that random variation in hyperparameters significantly affected model accuracies. The method presented in the study aimed to address randomness due to parameter initialization. The study addressed the impact of hyperparameter variability on deep learning model performance, aiming to compensate for randomness in parameter initialization and data shuffling. Various articles confirm significant variation in model performance due to different random seeds. Boo 5 outperformed single models in reducing noise in performance scores, especially in random hyperparameter search scenarios. The study highlighted the impact of hyperparameter variability on deep learning model performance, showing that the best-model performance improves with the number of experiments. The validation performance is a good predictor of the test performance, with the expectation of the best single model performance increasing significantly with more training runs. This effect is explained in more detail in other sources and emphasizes the importance of refraining from using certain methods and disclosing the number of experiments conducted. The correlation between validation and test results is not always accurate, especially with fixed hyperparameters. Allowing hyperparameters to vary can significantly increase the correlation. Larger validation sets can also improve this correlation, indicating the degree of generalization from validation to test. The problem of increasing expected performance is relevant when there is a higher correlation between validation and test results. Honest separation of validation is crucial when choosing the best model based on reported performance. The need for honest separation of validation and test data is emphasized when choosing the best model. Gaussian kernel smoothing is used to expand the result pool, addressing limitations in reporting the best single model performance. However, the method does not fully compensate for improved results due to hyperparameter tuning, especially with more advanced optimization methods. The work focuses on addressing variability in optimization methods, particularly due to random initialization and data shuffling. The method proposed, Boo n, is useful for selecting a final model from a pool of candidates, providing more informative results than traditional aggregation methods like mean or median. Boo n is seen as a valuable addition to existing methodologies. The paper emphasizes the importance of fully characterizing performance distribution in optimization methods. While using a single number to describe performance is convenient for comparison, a more detailed approach is needed. The proposed method, Boo n, offers a more informative way to select the best model from a pool of candidates. The paper introduces Boo n as a method to select the best model from a pool of candidates, emphasizing the need for fully characterizing performance distribution in optimization methods. It criticizes the practice of reporting only the best single model performance in machine learning research, advocating for more informative and robust results through repeated evaluations. The paper introduces Boo n as a method to select the best model from a pool of candidates, emphasizing the need for fully characterizing performance distribution in optimization methods. It criticizes the practice of reporting only the best single model performance in machine learning research, advocating for more informative and robust results through repeated evaluations. Reviewers can play a crucial role in bringing about this change. Boo n is suggested as a way to properly evaluate the best-model performance. The expected maximum is expressed in terms of a standard normal and is linearly proportional to both the mean and standard deviation. Reporting the expected test set performance of a best-validation model is discussed. The paper discusses reporting the expected test set performance of a best-validation model using a Bivariate Normal Distribution. The test performance is distributed normally with conditional expectation. The authors downloaded pdfs of papers from ICLR 2017, extracted text, and searched for experiments using the grep command. In a study analyzing ICLR 2017 papers, 180 out of 194 papers were found to contain either \"EVALUA-TION\" or capitalized section headings, indicating an empirical component. Only 11 papers mentioned \"confidence interval\" and 11 others mentioned terms related to hypothesis testing. This suggests that while many papers have an empirical component, the use of specific statistical methods is less common. The experimental procedure employed a method to quantify result stochasticity in deep learning models. Open Source implementations of ResNet on CIFAR-100 dataset and AS Reader on CBT CN dataset were chosen for the experiments. Data was collected by repeatedly training the models. The models were trained on Ubuntu 14.04 using Nvidia Tesla K80 or GTX 1080 GPUs. Resnet was trained with default hyperparameters, including 5 residual units. Training used the 0.9 momentum optimizer, batch size 128, and data augmentation techniques. L2 regularization weight was set to 0.002, and training ran for 300 epochs. The AS Reader was trained in two different settings: fixed hyperparameters with embedding dimension of 128 and 384 hidden dimensions, and randomly chosen hyperparameters for each training instance. Training was done using Theano 0.9.0 and Blocks 0.2.0. Test performances of the evaluated models are shown in FIG2, with Resnet achieving a mean test accuracy of 68.41%. The mean test accuracy for Resnet was 68.41% with a standard deviation of 0.67% and a range of 67.31% - 69.41%. For AS reader with fixed hyperparameters, the mean was 63.16% with a standard deviation of 0.94% and a range of 61.52% - 64.60%. Random hyperparameter search had a mean of 61.26%, standard deviation of 2.48%, and values ranging from 56.61% to 64.01%. The results suggest that fixed hyperparameters follow a Gaussian distribution, while random search shows a negative skew. The study compared architectures on datasets BID21 and BID16, sorting results by test performance and calculating differences between models. Median differences were 0.86% for CIFAR-100 and 1.15% for CBT CN, smaller than two standard deviations. Performance variations due to random initialization and data shuffling are significant compared to improvements in performance. Gaussianity cannot be ruled out at 0.05 significance level. The study suggests that while the best model is usually selected based on validation performance, fixing hyperparameters weakens the correlation between validation and test accuracy. This implies that selecting the best validation model may be akin to randomly picking in terms of test performance. Therefore, better characterization of the test performance distribution is needed. The study highlights the challenge of selecting the best model based on validation performance when hyperparameters are fixed, leading to a weakened correlation between validation and test accuracy. This suggests that randomly picking a model may yield similar test performance results. Therefore, a more thorough understanding of the test performance distribution is necessary. The results show that increasing the number of experiments improves the expected performance of the best-validation model, making it an important explanatory variable. This can lead to results reported by different research teams being incomparable and giving an advantage to those running more experiments. Reporting the performance of the best single model may be unsuitable due to random variation in performance estimates when reproduced by other research teams. The research team attempts to replicate experiments but often obtains different estimates. More observations lead to a more precise estimate. Confidence intervals express uncertainty and indicate if enough experiments were conducted to reduce uncertainty. Constructing confidence intervals is straightforward if the distribution of the estimate is known. However, in some cases, the distribution is unknown. When the underlying distribution is unknown, Monte Carlo methods can estimate confidence intervals using Bootstrap or similar methods. Bootstrap involves sampling with replacement to create a distribution of estimator values, allowing for easy estimation of confidence intervals. The Bootstrap distribution converges to the true underlying performance distribution by estimating parameters and generating a simulated Monte Carlo sample. It can be used to calculate confidence intervals for estimators and relative improvements compared to a baseline. More details on constructing Bootstrap confidence intervals can be found in standard texts on computational statistics. The Bootstrap confidence intervals were calculated for Resnet and the AS Reader using B = 100,000. Results were plotted in FIG3, with a comparison of non-parametric and Gaussian parametric estimators in FIG4. The parametric estimator showed lower variance, advantageous if the performance distribution is Gaussian. However, bias can occur if the true distribution differs from the assumed theoretical distribution."
}