{
    "title": "r1GbfhRqF7",
    "content": "Detecting abrupt property changes in time series is a challenging problem. Kernel two-sample test is used for this task, but selecting kernels is difficult in practice. A novel approach called KL-CPD is proposed in this paper, which optimizes test power through a generative model. KL-CPD outperformed other methods in detecting change-points in real-world applications. Change-point detection (CPD) is a key task in time series analysis with applications in various fields such as medical diagnostics, industrial quality control, and financial market analysis. CPD aims to predict significant changing points in a temporal sequence of observations. The focus is on retrospective CPD using a sliding window approach over the time series input. The retrospective CPD BID36 BID23 allows a flexible time window for change-point detection without strong distributional assumptions. Various parametric methods exist, but nonparametric and kernel approaches offer robust performance without prior knowledge of data distributions or anomaly types. Nonparametric and kernel approaches offer robust performance for change-point detection without distributional assumptions. Kernel two-sample tests have been successful in time series CPD, with different test statistics proposed for hypothesis testing. The performance of kernel methods depends on the choice of kernels, with some studies optimizing test power for better kernel selection. In this paper, the authors propose KL-CPD, a kernel learning framework for time series change-point detection. They optimize a lower bound of the test power using an auxiliary generative model and present a deep kernel parametrization combining RNNs and RBF kernels for time series applications. Extensive benchmark evaluations show the effectiveness of the proposed framework. The authors introduce KL-CPD, a kernel learning framework for time series change-point detection. They optimize test power using an auxiliary generative model and combine RNNs and RBF kernels for applications. Extensive benchmark evaluations demonstrate the framework's effectiveness. The proposed method enhances kernel power and avoids performance degradation as data dimensionality increases. The goal is to detect a change-point in a sequence of d-dimensional observations. The experiment code and datasets are available at the provided link. In this paper, the authors introduce a kernel learning framework for time series change-point detection. They use the maximum mean discrepancy (MMD) to measure distribution discrepancy and detect change-points. The focus is on piecewise iid assumption for time series samples, with potential for extension to other settings in future work. The MMD and two-sample test are key components in this framework. The MMD distance is a nonparametric probabilistic distance used in two-sample tests. It is defined as the kernel mean embedding for two distributions P and Q. Estimating the MMD distance involves using finite samples from the distributions. The estimator for MMD has minimal variance among unbiased estimators. The MMD value is non-negative and equals 0 only if the distributions are the same. However, the estimator may not be 0 even if the samples are from the same distribution due to finite sample size. Hypothesis testing provides statistical guarantees on whether two sample sets are from the same distribution. The MMD distance offers statistical guarantees for comparing two sample sets' distributions. The hypothesis test defines null and alternative hypotheses, with a focus on maximizing test power by choosing the appropriate kernel. In time series CPD, P represents usual events, and Q represents events with change-points. In time series CPD, the MMD distance is used to compare distributions of usual events (P) and events with change-points (Q). Choosing the right kernel for optimizing test power is challenging due to limited samples from the abnormal distribution Q. Kernel learning may lead to overfitting, affecting performance. An example on the Blobs dataset demonstrates the impact of limited Q samples on kernel selection for Gaussian RBF kernels. When q = 1, P = Q. For q \u2208 {4, 6, 8, 10, 12, 14}, 10000 samples are taken for X, Y and 200 samples for \u1ef8. Two objectives for choosing kernels are considered: median heuristic and max-ratio DISPLAYFORM0 among 20 kernel bandwidths. The test power under false rejection rate \u03b1 = 0.05 is reported after repeating the process 1000 times. Optimizing kernels using limited samples \u1ef8 decreases test power compared to Y. This raises the question of how to optimize kernels with very limited samples from Q, even none in an extreme. Choosing kernels based on surrogate distribution G is beneficial when samples from Q are limited, as is often the case in time series CPD tasks. By mimicking Q with a covariance g = q - 2, the test power is significantly boosted compared to using sparse samples from Q. This approach suggests that optimizing kernels with surrogate distribution G leads to improved test results. Optimizing kernel with surrogate distribution G improves test power when samples from Q are limited. The effectiveness of kernel selection objective holds without autoregressive/RNN modeling to control Type-I error. Test threshold approximation under H0: P = Q, mMk(X, Y) converges asymptotically to a distribution dependent on unknown data distribution P. Evaluating test threshold c\u03b1 is challenging due to its dependence on k and P. Optimizing kernel with surrogate distribution G improves test power when samples from Q are limited. To construct G without any sample from Q, injecting random noise to P is a simple method, but may result in a sub-optimal G. Making G as close to P as possible is crucial to ensure the effectiveness of the optimization objective. To improve test power with limited Q samples, an auxiliary generative model G \u03b8 is learned to approximate P closely. Early stopping criterion is set for G \u03b8 optimization. A min-max formulation considers all possible k \u2208 K during G learning. Stochastic gradient descent is used in experiments. The objective is to find k with the highest test power, different from previous work aiming to find G \u03b8. In this section, a kernel learning framework for time series CPD is presented, focusing on compositional kernels that combine RBF kernels with injective functions to create a more expressive kernel for complex time series. The injective functions are parameterized by recurrent neural networks to capture temporal dynamics. The goal is to find k with the highest test power, contrasting with previous work aiming to find G \u03b8 to approximate P. The text discusses using recurrent neural networks (RNNs) to capture temporal dynamics in time series data. An injective function f is approximated by an auto-encoder using a sequence-to-sequence architecture. The function F is a decoder trained to minimize reconstruction loss. The implementation involves estimating f in an online fashion using consecutive windows in a mini-batch. The auxiliary generative model and deep kernel implementation details are also discussed. The text discusses using a generator g \u03b8 to model counterfeit samples based on historical data X \u223c P. It utilizes Seq2Seq architectures for encoding and decoding time series data. The deep kernel parametrization aims to maximize test power via backpropagation on \u03c6 using the form k = k \u2022 f \u03c6. The text introduces the deep kernel parametrization for maximizing test power through backpropagation on \u03c6. It also discusses the use of a Seq2Seq framework with a GRU layer for generator design. The proposed KL-CPD algorithm includes weight-clipping and stopping conditions based on kernel MMD. The section presents a comparative evaluation of the proposed KL-CPD algorithm and seven baselines on real-world datasets from various domains. Training is unsupervised, with labels used for hyperparameter tuning. Evaluation is based on ROC curves and AUC, where KL-CPD performs best on three out of four datasets. KL-CPD outperforms other methods on real-world datasets, except for the Yahoo dataset where it is 2% lower than ARGP. OPT-MMD performs poorly compared to KL-CPD, confirming the importance of data-driven kernel selection. RDR-KCPD and Mstats-KCPD are not as competitive as KL-CPD and real-time CPD methods. KL-CPD outperforms RDR-KCPD and Mstats-KCPD in detecting change points by leveraging RNN for context extraction and discriminative embedding. Using a fixed kernel like Mstats-KCPD may lead to inferior performance due to the difficulty in detecting versatile types of change points. The non-iid temporal structure in real-world applications highlights the importance of adopting RNN and controlling type-I error for model selection. Using RNN parameterized kernels (trained by minimizing reconstruction loss) provides an advantage over directly conducting kernel two-sample tests on original time series samples. Model selection is still necessary to determine RNN parameters. OPT-MMD, a kernel learning baseline, is inferior to KL-CPD, which introduces a surrogate distribution with an auxiliary generator. KL-CPD outperforms other RNN alternatives like LSTNet, indicating that the proposed framework using an auxiliary distribution is effective for kernel selection. The impact of different encoders on KL-CPD is further examined. The impact of different encoders on KL-CPD is examined, with MMD-codespace showing a mild improvement over MMD-dataspace. MMD-negsample outperforms MMD-codespace by injecting random perturbations, validating the proposed lower bound approach for kernel selection. KL-CPD optimizes M k (P, G) effectively, even with simple perturbed P as G. It uses an auxiliary generator g \u03b8 for more complex samples. Performance is demonstrated in FIG1, showing competitive AUC with slight decrease at w r = 5. Results on Bee-Dance dataset are omitted due to space limit. AUC degradation is severe in MMD-dataspace and MMD-codespace under low delay tolerance. Conditional samples from KL-CPD can be found in Appendix B.5. Controlled experiments with simulated datasets show representative change-point characteristics. KL-CPD achieves the best results in creating simulated datasets with jumping mean, scaling variance, and Gaussian-Mixtures characteristics. Retrospective-CPD outperforms real-time CPD methods, suggesting that low reconstruction error does not always lead to accurate CPD. Mstats-KCPD may not perform as well as KL-CPD due to assumptions about the reference time series distribution. The study compares different encoders' impact on MMD performance in low vs high dimensions. MMD-dataspace weakens with higher dimensions due to sample size limitations, while MMD-codespace and KL-CPD mitigate this issue by testing on a learned low dimension codespace. KL-CPD is a new kernel learning framework for two-sample tests that optimizes the lower bound of test power with an auxiliary generator. It combines RNNs' latent space with RBF kernels to effectively detect change-points in various real-world applications. The method outperforms strong baseline methods in retrospective CPD and maintains performance as data dimensionality increases. KL-CPD is a kernel learning framework for two-sample tests that optimizes test power with an auxiliary generator. It differs from MMD GAN in its interpretation and motivation, focusing on the lower bound of test power and variance of empirical estimates. The paper aims to find the most powerful k for hypothesis testing, using an auxiliary generative model G. While optimizing G towards the data distribution P, early stopping is adopted to prevent G from becoming identical to P. Bee-Dance 2 records bee movements in x, y dimensions, and angle differences, which is of interest to ethologists. The three-stages bee waggle dance is studied by ethologists to identify change points in bee movements for communication. Fishkiller 3 records water level oscillations from a dam in Canada, treating the beginning and end of each oscillation as change points. HASC 4 dataset focuses on human activity segmentation using accelerometers. Yahoo 5 dataset contains time series metrics of various Yahoo services. Yahoo 5 dataset includes time series data of Yahoo services with labeled anomalies. 15 representative time series sequences are selected after removing duplicates. Jumping-Mean and Scaling-Variance models are used to generate samples with change points inserted at specific intervals. The curr_chunk discusses Gaussian-Mixtures and various baselines in time series forecasting and change-point detection, including Autoregressive Moving Average (ARMA) and Autoregressive Gaussian Process (ARGP). It mentions sampling data between two Gaussian mixtures and defines change points every 100 time stamps. The curr_chunk discusses advanced neural network models for time series forecasting and change-point detection, including RNN, LSTNet, ARGP-BOCPD, RDR-KCPD, and Mstats-KCPD. These models utilize techniques such as GRU, CNN, f-divergence, and kernel maximum mean discrepancy for improved performance. The curr_chunk introduces the use of kernel maximum mean discrepancy (MMD) as a dissimilarity measure in various models for time series forecasting and change-point detection. Hyper-parameter tuning involves selecting time lag orders and window sizes, as well as learning kernel hyperparameters. Different models have specific parameter settings, such as hidden dimensions for GRU and choices for regularization parameters."
}