{
    "title": "SkT5Yg-RZ",
    "content": "In an unsupervised learning scheme, two versions of an agent, Alice and Bob, compete by proposing and completing tasks in reversible or resettable environments. Through a reward structure, they create a curriculum for exploration, reducing the need for supervised episodes in reinforcement learning tasks. This approach improves sample efficiency and can lead to higher rewards. In this paper, a novel form of unsupervised training for an agent is introduced to enable exploration and learning about the environment without external rewards. This training allows the agent to quickly learn new tasks within the environment. The novel training approach introduces self-play with two \"minds\", Alice and Bob, in reversible or resettable environments. Alice proposes tasks for Bob to learn, helping him understand the environment and learn tasks quickly. Training includes self-play and target task episodes. Alice and Bob engage in self-play and target task episodes to train the agent in a reversible environment. Alice sets tasks for Bob by altering the state of the environment, while Bob's policy is used to control the agent in the target task. Bob quickly learns to visit the flag in the target task as he is familiar with the environment from self-play. In reversible environments, Alice sets tasks for Bob by altering the state of the environment. Bob's goal is to return the agent back to the initial state to receive a reward. This self-play between Alice and Bob involves internal reward and allows the agent to be trained without needing supervisory signals from the environment. In self-play training, Bob learns to transfer efficiently between states, which helps him learn target tasks faster. The reward structure encourages Alice to challenge Bob without giving impossible tasks. The total rewards for Bob and Alice are denoted by R_B and R_A respectively, with time taken for tasks denoted by t_B and t_A.\u03b3 balances internal rewards with external rewards. See Algorithm 2 in Appendix A for more details. Alice and Bob's self-regulating feedback system constructs a curriculum for exploration by balancing internal and external rewards. Alice's optimal behavior is to find tasks just beyond Bob's capabilities to facilitate his learning. Both have policy functions taking state observations as input and outputting action distributions. Alice and Bob's policy functions take state observations as input and output action distributions. In Alice's case, the function is based on the initial and current state observations, while in Bob's case, it involves a target state. The agent's policy function for a target task is denoted as Target = \u03c0 B (s t , \u2205), where the second argument is set to zero. In some experiments, a third argument z \u2208 {0, 1} is used to indicate the episode type. In settings with tabular or neural network policies, Bob is trained to find a policy that navigates between states efficiently. The policy table for Bob is indexed by (s t , s * ), indicating a fast policy for reaching any state from any other with minimal steps. In a setting where Bob aims to find a fast policy for transitioning between states efficiently, a universal policy \u03c0 fast is optimal for Bob and has the smallest expected number of steps to reach a target state. If Alice and Bob's policies are in equilibrium, with Alice unable to improve without changing Bob, then Bob's policy is considered fast. In equilibrium, if Alice gets positive reward on a challenge, it means Bob is slower. Bob's policy \u03c0B is fast because if it wasn't, he could improve by using \u03c0fast. This contradicts the assumption that Alice always gets zero reward. Therefore, Alice never proposes challenges where Bob is slower, leading to self-play dynamics. Self-play dynamics naturally arise in reinforcement learning, where agents compete for internal rewards to learn about their environment. This approach differs from external reward-based games and has connections to generative adversarial networks (GANs) and adversarial dialogue generation models. The curr_chunk discusses the use of adversarial loss terms in combination with variational auto-encoders for accurate density modeling. It also mentions the concept of intrinsic motivation for self-supervised learning agents, including curiosity-driven exploration techniques. These methods aim to encourage exploration without specific target tasks or extrinsic supervision. The curr_chunk discusses techniques for encouraging exploration in reinforcement learning, focusing on the novelty of states and the agent's control over its environment. There is no explicit notion of novelty in the work, but the goal is to push the agent towards challenging states. The concept of empowerment, measuring the agent's control, is related to the work but not explicitly measured. The curr_chunk discusses automatic curriculum learning in machine learning, where Alice and Bob create their own curriculum without manual specification. The approach is unsupervised, requiring no training labels. The models are parameterized similarly to previous works using a value function that depends on state and goal. The novelty lies in how Alice defines the goal for Bob. The curr_chunk discusses differences in approach compared to related works, particularly in how tasks are proposed and communicated. The goals are sampled through Alice's actions, requiring the environment to be reversible or resettable. Other concurrent works involve forming implicit curriculums and generating distant start states from a goal. The curr_chunk discusses an adversarial framework for improving agent robustness with a \"random Alice\" strategy. Self-play experiments are conducted on various tasks, showing benefits in training efficiency. The self-play episodes are considered \"free\" as they do not rely on environmental rewards, similar to semi-supervised learning approaches. The experiments use policy gradient with a baseline for optimization. The baseline is computed using a neural network and depends on the state. The models are trained to maximize expected reward and minimize the distance between baseline value and actual reward. Model parameters are updated after each episode. The experiments use policy gradient with a baseline for optimization, where the hyperparameter \u03bb is set to 0.1. Two-layer fully-connected networks with 50 hidden units are used for the policy neural networks. Training is done using RMSProp. The environment for illustrating asymmetric selfplay involves M states arranged in a chain, with Alice and Bob having three possible actions each. In the experiments, policy gradient with a baseline is used for optimization. The environment involves M states in a chain with Alice and Bob having three actions each. Self-play task involves returning to initial state, while the target task requires reaching a specific state within a set number of steps. The curriculum from self-play efficiently trains the agent for the target task and encourages exploration of the state space as Bob improves. In the experiments, policy gradient with a baseline is used for optimization. The red curve corresponds to policy gradient with penalties for task completion. The magenta curve represents Alice's random policy, while the green curve involves policy gradient with an exploration bonus based on state visit counts. In the MazeBase environment BID25, experiments show that using self-play training helps Alice give Bob progressively harder problems, leading to performance improvements similar to count-based exploration. The weight \u03b1 is chosen to maximize success after 0.2M episodes, with different training methods showing varying success rates. The MazeBase environment BID25 features a 2D grid with various items, including a light switch, key, wall with a door, and a goal flag. In self-play, Alice navigates the maze and changes switch states before Bob takes over. In self-play, Alice navigates a maze changing switch states until she outputs the STOP action. Bob then takes control to reset everything to its original state. The agent succeeds quickly with self-play compared to target task-only training. The approach was applied to the Mountain Car task in RLLab, where the agent learns to build momentum by moving left and right alternately. The agent in the Mountain Car task learns to build momentum by moving left and right in a 1-D valley. The action space is discretized into 5 bins, with a secondary action head for STOP actions. The environment is asymmetric, making it easier to coast down the hill than to climb up. The approach is compared to VIME and SimHash methods in FIG3, with a focus on adjusting the separation between self-play and target tasks. Training was stopped for all methods except target-only at 5 \u00d7 10 6 episodes. The rate at which Alice interacts with objects during an episode drops as tasks involve more objects. The plot of Alice and Bob's reward strongly correlates with this interaction rate. As self-play progresses, Alice takes more time before handing over to Bob, indicating increasing task difficulty. In the SwimmerGather task, the agent must learn to swim towards green apples and away from red bombs. The observation state includes a 13-dimensional vector for the worm's location and joint angles, and a 20-dimensional vector for nearby objects. The agent controls the worm with two real values for each joint, and additional action heads are added for handling the second joint and a STOP action. In the SwimmerGather task, the agent controls the worm with two real values for each joint and additional action heads for handling the second joint and a STOP action. The output space is discretized, and success is defined as a difference in final locations of Alice and Bob less than 0.3. The self-play approach demonstrates faster learning compared to other methods in Reinforce and TRPO, with similar final values to SimHash. In the SwimmerGather task, the agent controls the worm with two real values for each joint and additional action heads for handling the second joint and a STOP action. The self-play approach demonstrates faster learning compared to other methods in Reinforce and TRPO, with similar final values to SimHash. With reversible self-play, learning is faster but converges to a comparable reward. Training directly on the target task using Reinforce without self-play resulted in total failure. The self-play approach was applied to a StarCraft: Brood War game setup where the agent controls multiple units to mine, construct buildings, and train new units without enemies to fight. The target task is to build Marine units by following a specific sequence of operations. In the SwimmerGather task, the agent controls the worm with two real values for each joint and additional action heads for handling the second joint and a STOP action. To optimize the task, the agent must mine minerals, build a barracks, and train Marine units. The agent receives a reward for each Marine built after 200 steps. Optimizing this task is complex due to finding an optimal mining pattern, producing the right number of workers and barracks, and building supply depots at the right time. During self-play, Alice and Bob control the workers and can try different actions. Alice and Bob control workers and can try different actions during self-play. Bob's success is based on the global game state, including units and mineral resources. Bob's objective is to match Alice's units and mineral production in the shortest time possible. The novel method described is asymmetric self-play, effective in both discrete and continuous input settings with function approximation. Our approach, asymmetric self-play, is effective in both discrete and continuous input settings with function approximation. It can generate curriculums and encourage exploration. The approach performs as well as state-of-the-art RL methods that incentivize exploration. Theoretical analysis shows that optimal agents can transition efficiently between reachable states using reward functions from FORMULA0 and FORMULA1. Pseudo code for training agents on self-play and target task episodes is provided. In self-play, Bob does not need to worry about invisible things like the state of the door when the light is off. The agent has full visibility of the maze when the light is on, but can only see the light switch when it's off. Hyperparameters for RMSProp are set to 0.97 and 1e \u2212 6, with other values shown in Table 1. Entropy regularization maximizes the softmax layer's entropy. In the target task, the agent and goal are always placed on. In the target task, the agent and goal are placed on opposite sides of the wall. The agent must turn on the light, toggle the key switch to open the door, pass through it, and reach the goal flag. Alice and Bob's policies are modeled by neural networks. The effectiveness of the approach depends on the similarity between self-play and target tasks. Changing the initial light probability during self-play episodes can adjust the similarity between tasks. Varying the probability affects how Alice and Bob interact in the game. When the light is off 30% of the time, reverse self-play performs well but repeat self-play struggles. This adjustment impacts Bob's task and the overall difficulty of the test task. When the light is off 30% of the time, reverse self-play performs well, while repeat self-play struggles. Varying the initial light probability can adjust the similarity between tasks, impacting Bob's task and the overall difficulty of the test task. The performance of self-play is shown in Figure 6, where the reduction in target task episodes relative to training purely on the target-task is illustrated. The text discusses the impact of varying the similarity between self-play and target tasks on performance. It shows that significant speedups are achieved when self-play is similar to the target task, but the effect diminishes when self-play is biased against the target task. The changes in behavior of the agents correlate with rewards, indicating the importance of task similarity in training. In a single training run, Alice hands over to Bob at different stages, showing varying distributions. TRPO experiment used step size 0.01 and damping coefficient 0.1. Batch has 50,000 steps, with 25% from target task episodes and 75% from self-play. Self-play reward scale \u03b3 is set to 0.005. Actor and critic networks are separate, with critic network having L2 weight regularization. Active units include worker units, command center, and barrack. Agent controls multiple active units in parallel, with each unit having specific local and global observations. In self-play, Bob perceives the global observation of his target state where the final global observation of Alice is visible. The action space of different unit types is shown in TAB2, with the number of possible actions being the same for all units. The more complex actions such as \"mine minerals\", \"build a barracks\", and \"build a supply depot\" have specific semantics. In self-play, Bob perceives the global observation of his target state where the final global observation of Alice is visible. The action space of different unit types is shown in TAB2, with the number of possible actions being the same for all units. Some actions are ignored under certain conditions, such as \"mining\" action if the distance to the closest mineral is greater than 12, or \"building\" and \"training\" actions if there are not enough resources. Additionally, actions that create a new SCV or a barracks are ignored if the number of active units has reached the limit of 10. \"Build\" actions will also be ignored if there is not enough room at the unit's location. An extra reward is given at every step for count-based exploration, with a specific value of \u03b1 = 0.1 found to work the best. Extending the episode length from 200 to 300 in an experiment showed that self-play still outperforms. In self-play, extending the episode length from 200 to 300 allows more time for agent development. The self-play method continues to outperform baseline methods. Alice and Bob aim to explore the state space, with Alice focusing on challenging tasks for Bob. In more realistic environments with function approximation, Bob and Alice may get stuck in sub-optimal solutions. In self-play, extending the episode length allows more time for agent development. With function approximation, Bob and Alice can get stuck in sub-optimal minima. Local policy updates may lead to challenges in finding better policies. Function approximation offers no guarantees compared to tabular policies. In self-play, extending the episode length allows more time for agent development. With function approximation, Bob and Alice can get stuck in sub-optimal minima. Local policy updates may lead to challenges in finding better policies. Function approximation offers no guarantees compared to tabular policies. Fully reversible dynamics can result in longer expected steps for completing challenges, leading to suboptimal policies. Multiple Alices with different policies could help correct this issue. In this work, Alice proposes tasks for Bob to do, limited by practical and effective constraints in restricted environments. The tasks incentivize efficient transitions and could potentially involve reward functions and task representations that encourage discovering statistics of states and state-transitions."
}