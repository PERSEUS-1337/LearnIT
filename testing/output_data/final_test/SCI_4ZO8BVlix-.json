{
    "title": "4ZO8BVlix-",
    "content": "Driving simulators are crucial for vehicle research, but current virtual reality simulators lack true presence. UniNet, a driving simulator powered by SUMO and Unity, allows users to interact with simulated traffic in mixed reality. Its modular architecture enables research in areas like vehicular ad-hoc networks and traffic management. A user study shows that UniNet enhances the sense of presence. Most driving simulators focus on driver training or safety research, but UniNet stands out for its immersive experience. The need for high-quality Virtual Reality (VR) driving simulators with a focus on user presence is long overdue. A driving simulator with traffic simulation is a strong tool for Vehicular Ad-Hoc Network (VANET) research, combining network simulation, application prototyping, and testing for networking research. The curr_chunk discusses the use of a virtual environment for VANET researchers to evaluate communication protocols and applications before real-world deployment. It introduces a system for Mixed Reality (MR) where users can interact as their own avatar in a virtual environment. Our system introduces users as their own avatar in a virtual environment using stereoscopic cameras and passthrough VR technology. It is designed to be compatible with existing VR systems and provides a high-quality driving experience by combining realistic vehicle dynamics with a traffic flow simulation platform. The system aims to create a true sense of presence in virtual environments, presenting challenges for developers that our design addresses effectively. UniNet faithfully simulates a 2018 Ford Focus, matching gear ratios, horsepower, top speed, acceleration, and suspension. VR devices have evolved over the past 55 years, with the Oculus Rift being one of the first commercial HMDs. VR hardware progress is leading to advancements in VR-ready applications and supporting software, with variations like Augmented Reality and Augmented Virtuality emerging. Augmented Reality (AR) and Augmented Virtuality (AV) are modern forms of VR that blend reality with virtual environments. The work focuses on developing UniNet, a driving simulator with VANET and MR capabilities, to bridge the gap in vehicle network research. The study aims to measure user presence in UniNet using a custom passthrough MR experience, hypothesizing that users will feel a heightened sense of presence compared to pure VR experiences. The notion of Virtual Reality (VR) dates back to French playwright Antonin Artaud in 1938. The term was popularized by Jaron Lanier in the 1980s, who discussed its applications beyond entertainment and gaming, including treating war veterans and performing surgeries. Virtual Reality (VR) has evolved over the years, with modern HMDs like the Oculus Rift bringing VR to the consumer market. The use of VR in driver training, particularly with youths with autism spectrum disorder, is being explored. MR visual displays merge real and virtual worlds on the \"Reality-Virtuality Continuum\", offering a unique VR experience. Augmented Reality (AR) technology is considered mixed reality. Augmented Reality (AR) technology is a form of mixed reality used for various applications, including educational displays and multiplayer games. AV, another form of MR, is less common than AR. Blissing et al. studied driving behaviors in VR and MR, using an Oculus Rift DK2 HMD with cameras to mimic drivers' eyes. Their study aimed to understand how drivers' behaviors are influenced by reality, VR, and MR. The study involved 22 participants experiencing different configurations while driving a real car. The study involved participants driving in various configurations including cameras in VR, driving with passthrough cameras and traffic cones superimposed (MR), and full VR. Participants drove 35% slower when wearing the HMD, showing that it may affect driving behavior. The distinction between 'Immersion' and 'Presence' is important, with immersion referring to sensor fidelity and presence to a user's psychological response in a VR system. Measuring a user's sense of presence is crucial to understanding user experience in a VR environment. The study by Insko et al. discusses methods for measuring user presence in a VR environment, including Behavioral, Subjective, and Physiological measures. Freeman et al. conducted a study using postural responses to events in VR to measure presence, comparing them with subjective measures. Subjective measurements are commonly used due to the subjective nature of presence as an emotional sensation. Questionnaires, such as the Bob G. Witmer presence questionnaire, are preferred for gathering subjective measures. The Bob G. Witmer presence questionnaire is commonly used to measure presence subjectively. Questionnaires are preferred despite relying on user memory. Physiological measurements, like Heart Rate Monitors, provide objective data but are not easily linked to presence changes and can be affected by external factors. Driving simulators are artificial environments used for research, including traffic and VANET studies. They are cost-effective and flexible tools that can simulate realistic driving scenarios. These simulators have evolved from early flight simulators and are now used for driver safety assessments, VANET research, and Human-Computer Interaction studies. Driving simulators are used for research in VANETs and HCI. Modern simulators have high-quality physics simulations that convert user interactions into signals for sensors. Lee et al. built a full motion driving simulator without VR technology, recreating driving cues for immersion. The new level of immersion achieved by a driving simulator inspired its title as a VR tool. Driving simulators have become more accessible, thanks to advancements in video game technology. The simulator is built using Unity, a powerful game engine that can be combined with SUMO for traffic generation. BiurrunQuel et al. developed a driver-centric traffic simulator by connecting Unity with SUMO. The authors established a connection between Unity and SUMO using TraCIAS for remote control. They focused on synchronization, NPC vehicle motion, and physics simulation in their simulator. Ropelato et al. used Unity for a VR driving simulator with AI traffic in a virtual city. Michaeler et al. proposed a system entirely within Unity, considering SUMO and network event simulators. The authors implemented a system in Unity to simulate Vehicle-To-Vehicle (V2V) communication using the Self-Organized Time Division Multiple Access (SOTDMA) protocol. Their simulation included bad reception, building interference, parsing road types from OSM, and generating traffic signs. Unity was also used for real-time 3D visualization of distributed simulations of VANETs, as seen in the works of Guan et al. The authors utilized ESRI City Engine to visualize a city for VANET simulations. SUMO, an open-source traffic simulation application, was used for microscopic traffic simulation with defined vehicle types and routes. Routes are paths along edges, corresponding to roads, and can support traffic lights and multiple lanes. Gonccalves et al. explored SUMO in conjunction with a serious game driver simulator for testing Advanced Driver Assistance Systems. The work relies on SUMO as a 'center-server' for multi-agent microscopic simulation and providing essential information to other systems. Initial research focused on mental workload and distractions' impact on driver performance. Tools exist to augment SUMO, generate routes, and modify road networks. Real-time tasks require a socket connection to SUMO, facilitated by the Traffic Control Interface (TraCI) API. TraCI enables connections between SUMO and other simulators like Unity, NS2, or NS3. The TraCI protocol has been implemented in various programming languages over the past decade, with the simulator utilizing modern C#. Our simulator, utilizing a modern C# implementation of TraCI from CodingConnected, supports multiple connections from different sources. It enhances immersion and user presence in VR driving simulators, allowing for user interaction with generated traffic. Additionally, an improved architecture connects Unity and SUMO, enabling two-way communication between vehicles and SUMO from UniNet. UniNet is a simulator that combines an industry-standard traffic generator with a high fidelity driving simulator to allow for real-time human interaction in continuous traffic simulation. This addresses the lack of two-way communication for human involvement in traffic simulation seen in current tools. The system supports two primary forms of human interaction and integrates tools like Esri CityEngine for city visualization. UniNet integrates tools like Esri CityEngine for procedural city generation, allowing for the study of traffic flow using real-world data and VR headsets for immersive experiences. UniNet's VR technology aims to enhance immersion by providing a sense of presence in VR, surpassing traditional visual feedback. The process of creating immersive scenes can vary in time from 30 seconds to 10 minutes. High budget simulators have innovative ways to immerse users, like using an actual car cockpit for driving simulations to maintain immersion. Our novel solution to enhance immersion in VR simulators is to use a stereoscopic passthrough camera to create a mixed reality system. This technology superimposes real-world controls onto the virtual car's dashboard, providing a more realistic driving experience. UniNet also includes audio feedback such as engine sounds and ambient noise to further enhance the simulation. The simulator, combining Unity and SUMO, can render hundreds of vehicles with user input, offering various applications. The current simulator can render hundreds of vehicles in a virtual city generated from real-world locations. Unity generates traffic sent to SUMO for vehicle updates, which are then rendered in VR. Initially built with Randomation Vehicle Physics, it later switched to Vehicle Physics Pro for realism. The integration of a vehicle physics library into the Unity 3D project was seamless, allowing focus on connecting SUMO and Unity. Dead-reckoning technique was used for traffic agents' movement to match SUMO's simulation. SUMO is an open-source traffic simulator capable of simulating thousands of agents on road networks. The integration of SUMO into Unity for traffic simulation was straightforward. A script was used to export the city map into a *.net.xml file, which was then cleaned up using NETEDIT. SUMO was configured to simulate vehicle movement only when instructed by an external process, with a simulated step duration of 20 milliseconds. Vehicles were added and rerouted using TraCI. After these steps, SUMO was considered fully configured. Using TraCI in C#, Unity was connected to SUMO to populate streets with cars and link each car with their agent. A custom off-road vehicle type was created for user driving, mapped to a VPP-powered car in Unity. A dummy car was added to prevent traffic agents from driving through the user's vehicle. Improved stop-sign behavior was implemented in SUMO through Unity. Improved stop-sign behavior was added to SUMO using Unity, reducing NPC vehicle speed to 0 when entering a bounding box. This fix prevents NPC vehicles from rolling through stop signs unnaturally, enhancing the driver's immersion. Additionally, an automatic city generator was developed, utilizing real-world map data for realistic road layouts and 3D model generation based on building footprints. The city generator uses real-world map data to create a representation of a city by downloading satellite imagery, land use maps, building footprints, and roadways. Algorithm 1 generates 3D meshes for roads, buildings, and terrain, with MapBox being used to download necessary maps. The Unity game engine is used for this process. The city generator converts geographic coordinates into Cartesian coordinates using Mercator projection to account for inaccuracies in distance measurements caused by the Earth's curvature. A scalar multiplier \u03b8 is calculated based on the city's bounding box center to scale coordinates accurately. This \u03b8 value is applied uniformly to all coordinates for simplicity and speed optimization in Unity. The city generator uses Mercator projection to convert geographic coordinates into Cartesian coordinates, with a scalar multiplier \u03b8 for accuracy. Heightmaps from MapBox are used to generate terrain features, and passthrough virtual reality is achieved with HD cameras for a stereoscopic view. In our simulator, we use two HD cameras for a stereoscopic passthrough experience. The cameras are mounted to the front of an Oculus Rift with fixed properties: IPD at 60mm and downwards angle at 5\u00b0 to compensate for virtual camera mismatches. The cameras are parallel to capture important objects in the lower FOV. The stereoscopic camera pair used in the simulator is a 60 fps, 2560 \u00d7 960 USB camera with a software-based solution for stereo convergence. The cameras are offset in 2D to adjust focus and match the stereo convergence of the virtual cameras in the headset. The camera has a latency of approximately 170 ms, compensated for inside the game engine by recording the headset's rotation in the virtual environment each frame. This allows the Oculus Rift and the virtual passthrough camera to align their 3D orientation. The Oculus Rift and virtual passthrough camera are synchronized for 3D orientation, reducing simulator sickness by compensating for camera latency. A faster processor could decrease latency further. A green screen chamber surrounds the driving simulator, using a real-time green screen algorithm on the GPU to present the virtual world in mixed reality. This allows users to see their arms and a real vehicle dashboard while driving in a virtual city. The algorithm for the green screen uses difference keying to composite the user onto a virtual environment in the simulator. This technique helps the user become immersed by layering the captured camera feed onto the virtual world. The study compared UniNet's MR system with two VR systems and a non-VR control in a user study to test presence in the virtual environment. 24 participants with VR or driving experience were recruited for the experiment. The simulator used a Logitech G920 racing wheel with force feedback, and the clutch pedal was removed for clarity as the chosen vehicle was automatic. Research supported the choice of a racing wheel with high-quality force feedback for immersion. Participants in the study had VR or driving experience, with ages ranging from 18-57 years old. Some participants required corrective lenses during the VR experience. The VR system used included an Oculus Rift CV1 headset with specific technical specifications. The Oculus Rift CV1 headset features a diagonal Field of View (FOV) of 110\u00b0 for each eye and 94\u00b0 horizontally. It uses constellation tracking for accurate position and rotation tracking. The green screen chamber surrounds the user with a FOV of approximately 220\u00b0 and includes LED flood lights for illumination. The green screen chamber is equipped with flood lights mounted on the top brace to ensure consistent lighting. The screen is curved to prevent shadows, crucial for real-time GPU implementation of the green screen algorithm. Cameras are mounted on the Oculus Rift CV1 with 960p resolution and 60 FPS, powered by a single 15 ft micro USB cable. The cameras are strategically mounted on the Oculus Rift to minimize coverage of tracking LEDs. A 3D printed mount with a stereoscopic camera tilted downward compensates for lower FOV. The mount matches the height of the user's eyes and includes a non-VR configuration with a custom triple monitor rig. The custom triple monitor rig, with a combined resolution of 5760 \u00d7 1080, was used for the non-VR trial. Participants completed questionnaires about their driving and virtual reality experience, followed by a Positive and Negative Affect Schedule (PANAS) questionnaire. Participants completed a 10-question negative scale on a 5-point Likert scale to measure affect before trials. They were then seated in the driver's seat of UniNet for the study, which involved completing trials and questionnaires. The Bob G. Witmer PQ with 21 questions was administered after each trial to analyze factors like Involvement, Adaptation/Immersion, Consistent with Expectations, and Interface Quality. The study evaluated workload using NASA-TLX, which assesses mental demand, physical demand, temporal demand, performance, effort, and frustration. PANAS questionnaire measured mood and affectivity after each trial. The study used a 4 \u00d7 4 mixed factorial design with different configurations including MR, VR with no avatar, VR with avatar, and triple monitor non-VR control. Dependent variables measured were reaction time, Presence Questionnaire score, NASA-TLX score, and PANAS score. The user study utilized WindRidge city, a compact city designed for autonomous simulation. The city was mapped in Unity editor, exported as a *.net.xml file, and imported into NETEDIT for use with SUMO. Road signs were adjusted to match local signs. Participants completed four trials in UniNet with auditory and visual navigation cues from a virtual GPS inside a virtual car. The user study involved completing four trials in a virtual car with a GPS system, encountering artificial traffic and events such as car crashes or jump-scares at the end of each route. The user study involved participants reacting to events like car crashes or jump-scares in a virtual car. Reaction times were measured by analyzing inputs to the vehicle. Tests showed no significant deviations in reaction times, suggesting no ordering effect. The study found no significant ordering effect, indicating successful counterbalancing. Significant differences were observed in reaction times between different configurations, as shown in box plots. The study analyzed NASA-TLX scores for different factors, with outliers found in Adaptation/Immersion and Interface Quality. Mixed ANOVA and Friedman tests were conducted on the scores. Participants showed interest and excitement towards the MR configuration, with minimal variation in negative emotions among configurations. Only 20% of participants felt negative emotions during the study. Participants in the study showed minimal negative emotions, with only 20% feeling distressed or irritable. In a follow-up interview, all 24 users preferred the MR configuration, attributing it to a heightened sense of presence and perceived impact on the vehicle. The MR configuration was preferred by all participants due to a heightened sense of presence and connection to the vehicle. Users did not mention issues with low pixel density or latency. Sound-cancelling headphones were noted as a major factor for immersion. Most users felt immersed even with the triple monitor setup. All participants expressed willingness to drive the simulator again. In a user study, participants preferred the MR immersion configuration for its sense of presence and connection to the vehicle. One participant experienced mild simulator sickness with the triple monitor setup but still chose the MR configuration due to its novelty. Feedback suggested adding motion feedback or haptics in the seat/pedals for an improved experience, which will be investigated in future works. The MR configuration was found to be more immersive in the study. The study compared MR and UniNet configurations in a virtual environment. Results were subjective, based on questionnaires and reactions to vehicle collisions. Subjective results supported the hypothesis, but behavioral results were inconclusive. Presence is subjective, making subjective measurements popular. In-simulator trials compared UniNet configurations and measured reaction times. The study compared different immersion configurations for VR simulations, including our technology combined with existing VR, triple monitor setup, and virtual reality with fake hands. Each configuration aimed to provide a unique user experience in the virtual environment. In our VR simulator configuration, the car wheel is turned by a virtual avatar instead of turning by itself. VR without hands is another option in many simulators, providing uninterrupted user experience. Trials were untimed, with users driving faster than the average speed of 40 km/h. Reaction times varied, with MR trial showing average above 1 second and triple monitor trial below 0.5 seconds. The reaction times in the trials varied, with the mixed reality trial showing an average above 1 second and the triple monitor trial below 0.5 seconds. Participants completed the Bob G. Witmer presence questionnaire after each trial, indicating a high level of involvement with the mixed reality immersion configuration. The study compared involvement levels in different virtual reality setups. Results showed that mixed reality was the most immersive, followed by VR with fake hands and VR without hands. The triple monitor setup was the least immersive. There was a significant difference in involvement between VR setups with and without hands. The NASA-TLX questionnaire assessed perceived workload after each trial. The NASA-TLX questionnaire assesses workload after each trial. Significant differences were observed in task load index between the triple monitor and VR with fake hands setups. The 'Performance' scale may have been biased by reaction time events, affecting self-perceived performance. The VR without hands setup had a simpler driving scenario, resulting in a lower score. Performance and frustration were the main factors analyzed. The study compared performance and frustration levels between Triple Monitor and MR immersion configurations. Significant differences were found in both factors, with MR showing better performance and lower frustration levels compared to Triple Monitor. These differences could be attributed to the lower FOV in the Triple Monitor setup, leading to worse performance at intersections and junctions. Overall, performance and frustration levels may indicate a heightened sense of presence in MR. The PANAS questionnaire was used to gauge participants' emotions, with positive mood higher at the start of the study compared to the triple monitor immersion condition. Positive mood was measured at M = 32.1, SD = 7.24 at the start and M = 30.2, SD = 9.51 during the triple monitor condition, with a significant p-value of 0.0074. The study found that participants experienced high levels of positive emotions, such as 'Interested' and 'Excitement', during the MR immersion configuration. Additionally, the VR with fake hands trial resulted in higher levels of 'Distressed' emotion compared to the MR trial, possibly due to the uncanny appearance of the virtual avatar. Overall, users reported a heightened sense of immersion in UniNet's MR immersion configuration. The study focused on user presence in a MR driving simulator and the development of MR technology within the reality-virtuality continuum. Participants experienced high levels of positive emotions during the MR immersion configuration, with a heightened sense of immersion reported. The user study aimed to increase user presence in the virtual environment using MR technology. The study involved four trials in UniNet with vehicle collision events to elicit behavioral responses. Results supported the study's hypothesis. The hardware and software for UniNet were designed and tested, utilizing Unity and SUMO for player and NPC vehicles control. The technology works with Oculus Rift and stereoscopic cameras for a passthrough VR experience. The green screen chamber creates a mixed reality experience. Steps were taken to address issues encountered, such as tracking and anchoring the camera stream. To reduce latency, the camera feed was projected based on the user's head orientation at the time of capture. The green screen was curved around the user with a cloth material to improve keying. LED flood lamps were fixed to the chamber for consistent lighting. The cameras were calibrated to match the properties of the virtual cameras and Oculus HMD cameras. Distortion was removed using a chessboard tracking pattern and OpenCV. Further research is needed for quantitative presence measurement. User studies should include more focused subjective and controlled behavioral measurements. The hardware for UniNet could be improved with time, with a limited FOV and camera resolution that can be enhanced. Research on latency and camera properties' impact on user presence in MR is crucial. Users experienced camera latency over 150 ms with no negative effects. UniNet has potential for networking research with VANET applications. Future work could explore enhancing the green screen algorithm for UniNet by using depth mapping technology from a stereoscopic camera rig to improve compositing users onto virtual environments. Additionally, studying user interaction in real time and implementing motion feedback through a full motion simulator could be beneficial for future iterations of UniNet. The study involved a semi-structured interview after participants experienced a virtual environment. Questions focused on the consistency of virtual experiences with real-world experiences, concentration on tasks, visual display quality interference, feeling of experiencing vs. doing, and willingness to drive the simulator again with different immersion configurations."
}