{
    "title": "HJG0ojCcFm",
    "content": "When autonomous agents interact, cooperation is essential for achieving goals. Agents can form teams, make joint plans, and execute them. However, self-interested agents need appropriate incentives for team formation. Existing multi-agent negotiation methods are limited to specific protocols, requiring human input or domain-specific data. To address this, a framework using deep reinforcement learning for training agents to negotiate and form teams is proposed. This method is experience-driven and does not assume a specific negotiation protocol. Evaluation shows that agents using this approach outperform hand-crafted bots in team-formation negotiation environments. Agents in team-formation negotiation environments beat hand-crafted bots and achieve fair outcomes predicted by game theory. The physical location of agents influences outcomes as multiple agents can coordinate actions to solve tasks effectively. Stakeholders with different abilities and preferences must negotiate to form teams aligned with individual interests and capable of achieving tasks. This problem is formalized as a team-formation negotiation task. Team-formation negotiation tasks involve agents collaborating to achieve a common goal by forming teams and making enforceable agreements. This problem is formalized as a negotiation task where agents must interact to form teams and agree on how to share the reward. Cooperative game theory is used to study interactions between agents in these environments. Weighted voting games model coalition formation in legislative bodies. Cooperative game theory predicts agreements negotiated by agents in such settings, with solutions like the core, nucleolus, and power indices. The Shapley value is a prominent solution studied for weighted voting games. The Shapley value BID49 is widely studied for weighted voting games and used to estimate political power. Researchers have borrowed ideas from cooperative game theory to design negotiating agents, but these bots are not generally applicable or scalable. Multi-agent reinforcement learning is proposed as an alternative paradigm for negotiating in complex environments, where negotiation protocols can be complex and variable. This approach allows individual agents to learn team formation tasks based on their interactions with others, rather than using hand-crafted algorithms. The RL approach is applicable to Markov games and similar to recent work in non-cooperative settings. Our work compares negotiating RL agents with solutions from cooperative game theory, focusing on team formation in multi-agent reinforcement learning. Previous approaches have mainly addressed communication rather than team formation, often involving only two agents. We evaluate our approach on a team formation negotiation task using a direct negotiation protocol. Agents trained via independent reinforcement learning outperform hand-crafted bots in team formation negotiation tasks. The reward distribution shows a high correspondence with the Shapley value solution from cooperative game theory, even in a spatial grid-world environment. The analysis includes definitions from cooperative game theory for context. A cooperative game involves a set of agents and a characteristic function mapping subsets of agents to utility values. In simple cooperative games, winning teams achieve the task while losing teams do not. An agent is pivotal if their removal turns a winning team into a losing one. The Shapley value is used to determine the contribution of each agent in a permutation. The Shapley value characterizes fair agreements in cooperative games by measuring the proportion of all permutations in which an agent is pivotal. Weighted voting games are described by a vector of weights and a quota, where a coalition wins if its total weight meets or exceeds the quota. The Shapley value is used to analyze political power in legislative bodies. Multi-agent Reinforcement Learning involves n-player Markov games with observation functions and individual rewards. In reinforcement learning, agents learn behavior policies independently to maximize long-term payoff. The proposed method trains agents to negotiate team formation under various protocols in a decentralized setting. A cooperative game framework is used to capture different agent combinations for task performance, with viable teams receiving rewards to be shared among members. The team formation process involves agents negotiating how to share a reward. Cooperative game theory helps determine how the reward is shared, while negotiation protocols govern how agents interact to reach an agreement. In a negotiation environment, agents control their share of the reward in a grid-world setting. Analyzing this Markov game with non-cooperative game theory reveals equilibrium strategies. Training RL agents in this environment is proposed as a solution to the computational complexity of solving such games. The experiments focus on a weighted voting game, exploring the relationship between RL agents' negotiation outcomes and cooperative game theoretic solutions. The negotiation outcome of RL agents in a cooperative game theoretic solution is compared using different negotiation protocols. The task involves team formation in a weighted voting game environment where agents aim to exceed a fixed quota to receive a reward. In a cooperative game setting, successful teams are assigned a fixed reward r, which must be divided among team members. Viable teams are those where the total weight of members exceeds a certain threshold q. Agents face the dilemma of agreeing on shares to form a team or risk missing out on rewards. Different negotiation protocols are used to determine team formation and reward distribution. In a non-spatial environment, agents negotiate over team formation and reward allocation using direct proposals. In a spatial environment, agents use a similar approach seen in recent work on multi-agent deep reinforcement learning. Agents take turns proposing agreements and allocating rewards based on public knowledge of the game's weights and threshold. The proposer agent selects a team of agents with non-zero rewards to make a proposal. Proposees must accept or decline the proposal, and if declined, a new round may begin with another proposer. If all proposees accept, the rewards are allocated as proposed; otherwise, the episode ends with zero rewards for all agents. The episode ended with agents failing to reach an agreement in a non-cooperative game environment. A new spatial negotiation environment called Team Patches was introduced, where agents can form teams in a grid-world with colored patches to negotiate for rewards. In the Team Patches environment, agents can form teams by moving into patches and setting their demand for a share of the reward. Teams are viable if the total weight exceeds a threshold, and demands are valid if they do not exceed the total available reward. In the Team Patches environment, agents form teams by moving into patches and setting their reward demands. An agreement is reached when a viable team with a valid reward allocation is found. If no agreement is reached after 100 steps, all agents receive 0 reward. In the Team Patches environment, agents form teams by moving into patches and setting their reward demands. Viable teams with valid reward allocations are formed based on weight thresholds and demand availability. Agents independently learn policies using SARSA(\u03bb) and a function approximator to minimize temporal difference error. The neural network in V-trace correction BID16 learns from 16 parallel copies of the environment with an on-policy algorithm. It uses a convolutional layer with 6 channels, kernel size 3, and stride 1, followed by a multi-layer perspective with 2 hidden layers of size 32. The policy and value function heads are linear layers. Experiments are based on a distribution D over weighted voting games, with boards consisting of weights, thresholds, and 5 agents. Boards are partitioned into train and test sets, with agents trained on the train set for 500,000 games and evaluated on the test set. The neural network in V-trace correction BID16 learns from 16 parallel copies of the environment with an on-policy algorithm. It uses a convolutional layer with 6 channels, kernel size 3, and stride 1, followed by a multi-layer perspective with 2 hidden layers of size 32. Experiments are based on a distribution D over weighted voting games, with boards consisting of weights, thresholds, and 5 agents. Bots trained for 500,000 games using train set boards are evaluated on the test set, requiring them to learn a general strategy for negotiating under different weights. The negotiation performance of RL agents trained in the framework is compared with hand-crafted bots in the Propose-Accept protocol. The text discusses the negotiation performance of bots in a weighted voting game scenario. Bots propose and accept offers based on weight-proportional or Shapley-proportional allocations. The Shapley-proportional bot sets the target allocation proportional to Shapley values. The study compares the performance of a team of 5 RL agents with a team of 4 RL agents and 1 bot in a weighted voting game scenario. Results show that the bot in the one-bot group performs better than the weight-proportional bot, with significant differences in winnings. The Shapley-proportional bot also outperforms the weight-proportional bot. The study shows that RL agents outperform handcrafted bots in negotiations, even with variations in hyper-parameters. The focus is on whether RL agents negotiate in line with cooperative game theory concepts, particularly the Shapley value. The experiment evaluates the negotiation policies of RL agents in two environments. The study evaluates RL agents' negotiation policies in two environments using a set of 20 boards. The results show a strong correlation between Shapley value predictions and the empirical gains of the RL agents, indicating a good fit with game theoretic predictions. RL agents exhibit deviations from game theoretic predictions in boards with high inequality in Shapley values. Empirical rewards achieved by RL agents are more equal than predicted in rare boards. Lower weight variance in board distribution leads to better correspondence between Shapley value and RL agents' gains. In a spatial negotiation environment, the consistency of rewards with the Shapley value from cooperative game theory is explored. The impact of spatial aspects on negotiation outcomes is investigated by varying the starting position of agents with different weights. The starting position of agents with different weights affects negotiation outcomes in a spatial environment. Results show that the highest-weight agent negotiating closer to the patch can demand a higher reward, but as the agent moves further away, its share of the reward drops significantly. While RL agents generally converge to outcomes approximating the Shapley value, deviations occur in boards with players having strong or weak negotiation positions. Factors such as representational capacity of RL agents may contribute to these deviations. The representational capacity of RL agents may affect their ability to compute the Shapley value in weighted voting games. Neural networks may struggle to accurately induce this function, leading to optimization errors in negotiation protocols. Additionally, learning dynamics of independent RL agents in non-stationary environments can cause deviations from cooperative game theoretic predictions. A neural network can approximate the Shapley value under supervised learning conditions. The study trained a model using a weighted voting game's parameters to output Shapley values of each player. They used Gaussian weights to generate 3,000 unique boards, trained a 3-layer MLP, and found it generalized well. Another experiment involved RL agents observing their weights and Shapley values, showing a discrepancy. The study introduced a scalable method for team-formation negotiation using deep reinforcement learning, showing a discrepancy between Shapley values and outcomes reached by RL agents. This deviation is not due to agents' inability to determine their negotiation power but possibly because of a non-stationary environment or convergence on different outcomes. The study introduced a method using deep reinforcement learning for team-formation negotiation, demonstrating superior performance compared to hand-crafted bots. The method was applied to complex negotiation environments, showing accurate predictions on the impact of spatial changes. This research opens up new possibilities for studying how team dynamics influence emergent language in reinforcement learning agents. One might consider creating tasks that interpolate between fully cooperative and non-cooperative game settings, managed by dynamic institutions. Results compare agreements of RL agents to the Shapley value in a weighted voting game setting, illustrating the importance of power indices in various domains like travel agencies and airline carriers offering multiple routing options for clients. In cooperative game theory, different carrier combinations in travel agencies and airline carriers offering multiple routing options can be analyzed using solution concepts to predict how agents share joint gains. The Shapley value is a prominent example of formalizing power in negotiation scenarios, such as in political decision-making bodies. The Shapley value formalizes power in negotiation scenarios, like forming coalition governments after elections. It depends on possible teams that can form, rather than direct game parameters. Parties must negotiate to form a ruling government when no single party wins the majority of votes. This is common in many countries but rare in the UK and has never happened in the USA. The Shapley value formalizes power in negotiation scenarios for forming coalition governments after elections. In this context, any team of at least two out of three parties is sufficient to form a government, giving them equal power. The small party with only 2 seats has a strong negotiation position and could demand a disproportionately high share of the budget. The Shapley value allocates shares based on fairness axioms, with equivalent agents receiving the same share. It does not consider weights or thresholds, only the characteristic function of the game. The Shapley value is characterized by fairness axioms, including giving null players no reward and the additivity axiom. A null player adds no value to any coalition and receives no reward. The additivity axiom states that the value allocated to any player in a sum-game is the sum of values in individual games. For more details on these axioms, refer to textbooks and papers. The essence of negotiation agents facing decisions is similar across environments. Agents who rarely reach agreements are considered bad negotiators, while easily reaching agreements does not necessarily make an agent strong. Using RL to train negotiation agents eliminates the need for hand-crafting specific rules for each negotiation environment. In the negotiation process, a hand-crafted bot for team patches is more complex than the propose-accept environment. Different baseline bots are used, such as a random bot, weight-proportional bot, and Shapley-proportional bot, each with varying decision-making strategies. The weight-proportional bot randomly chooses a viable team containing itself and proposes an allocation of the reward proportional to each team agent's weight. The Shapley-proportional bot computes Shapley values for all agents, which is feasible for games with few agents. The weight-proportional bot calculates its share of the target allocation in the proposed team and compares it with the offer it receives. The probability of the bot accepting the offer is determined by a logistic function, with a constant correcting the range of values. Acceptance probability increases as the offer exceeds the bot's expectations. The Shapley-proportional bot sets the target allocation based on Shapley values rather than weights. Performance comparison between the bot and RL trained agents is done by creating pairs of groups and evaluating them on Gaussian weigh boards. Each group is co-trained for a set number of episodes. During training, each group is co-trained for 500,000 episodes using different boards. In the evaluation step, agents play 5,000 games on each evaluation board. The performance of RL agents and bots is compared by averaging the total fixed reward achieved over group pairs and episodes. Positive values indicate RL-agent outperforms the bot. The RL-agent outperforms the weight-proportional bot by achieving 10% more reward, as shown in a significant Mann-WhitneyWilcoxon test. The performance gap is smaller with the Shapley-proportional bot, but the RL agent still outperforms it at the same statistical significance level. The random bot's performance is also analyzed, showing similar results. The random bot, which selects actions randomly, showed significant results (p < 0.001). An experiment was conducted where RL agents were trained with a weight-proportional bot but evaluated with a Shapley-proportional bot, resulting in a slight decrease in average reward share for RL agents. Despite this hindrance, RL agents still outperformed the bot (p < 0.005). The study suggests that RL agents can outperform simple heuristic hand-crafted bots, although well-crafted bots tailored to specific negotiation protocols may still outperform RL agents. The methodology proposed offers a way to automatically construct reasonable negotiators without fitting a bot to a specific negotiation protocol. Using the Shapley value from cooperative game theory, likely outcomes in the weighted voting game were examined. This analysis indicates that RL agents can make sensible negotiation decisions and improve performance by tuning distribution parameters. The technique is general and can handle new negotiation protocols without relying on hand-crafted solutions. The methodology proposed involves using the Shapley value from cooperative game theory to analyze likely outcomes in a negotiation setting. The environment is a 15x15 grid-world where agents observe the world from their own perspective. The Markov game is not computationally tractable to solve for the Nash equilibrium, so cooperative game theoretic solutions, like the Shapley value, are applied. In Experiment 3, the world is changed by setting the total number of patches to 2 (red and blue) and modifying the starting position of the agent with the highest weight to investigate the spatial perturbation's effect on their reward share. The agents observe the world from their own perspective and also observe weights, demands, and indices of other agents. Visualizations show the white agent being moved between squares 0 and 10, affecting their proximity to patches. The agent receives its index as a one-hot vector, weights of all agents, and their demands. Spatial perturbations are visualized with the agent with the highest weight being moved 0 to N squares from the nearest patch. The impact of weight and power inequality on the correspondence with the Shapley value is discussed, showing deviations in boards where some agents have weak or strong negotiation positions. The distribution used ruled-out boards with identical weights to investigate this impact. The study examined an alternative board distribution D with reduced variance, resulting in a stronger correspondence between the outcome obtained by RL agents and the Shapley value. This was shown in FIG8, where the mass is concentrated around points with equal Shapley values, indicating a closer outcome achieved by RL agents. High inequality in agent power leads to a discrepancy between the Shapley value and outcomes of RL agents. The analysis suggests that RL agents face challenges in estimating their negotiating power due to protocol details and weak supervision. Despite this, a small neural network can approximate the Shapley value well. The study concludes that basic supervised learning tasks can be achieved with the agents' network capacity. The study shows that RL agents may deviate from Shapley values in negotiations due to their focus on maximizing their own gains rather than achieving a fair share. This deviation is not due to the agents' inability to identify their negotiating positions but rather the RL procedure applied. The study highlights the importance of strong policies considering the negotiation protocol in a non-stationary environment where peers constantly adapt their behavior."
}