{
    "title": "SJxTk3vB3m",
    "content": "Neural machine translation (NMT) systems have achieved state-of-the-art performance but are susceptible to producing pathological translations called hallucinations. These translations are untethered from the source material, affecting user trust. Various NMT architectures are vulnerable to hallucinations, but data augmentation can reduce their frequency. Hallucinations can be identified in the attention matrix and network signatures. NMT systems can produce hallucinations, affecting user trust. They exhibit strengths and weaknesses of deep learning, with competitive performance but poor understanding. Strange translations may occur with repeated words in commercial systems. Recent work shows that NMT systems are sensitive to noise in input tokens and susceptible to adversarial inputs. Inserting a single additional token can completely alter the translation, leading to errors that are challenging to understand or fix. Researchers have been working to understand NMT better, moving from Seq2Seq to models with attention mechanisms for improved translation quality and interpretability. Studies have identified critical components of LSTM and its role in language modeling. Recent work focuses on robust NMT, studying the effects of input noise to reduce variation from typos and synonym choices. Adversarial training is used to stabilize NMT systems sensitive to input noise. Recent research has focused on understanding RNNs in language tasks, including handling pathologies in language modeling and translation. Techniques such as scheduled sampling and adversarial training have been used to stabilize NMT systems sensitive to input noise. Additionally, RNNs have been studied through the framework of dynamical systems, with findings showing that continuous time vanilla RNNs can exhibit high-dimensional chaos that can be beneficial if controlled properly. Efforts have been made to control chaos in RNNs and improve optimization in NMT systems. There are attempts to remove recurrence in NMT models to address complexities and slow compute times. The specific pathology of hallucinations is being studied in NMT models from a dynamical systems perspective, using a standard RNN-based encoder-decoder model known as GNMT. The NMT model encodes input sequences into vectors using an encoder, while the decoder generates translations one symbol at a time. The decoder utilizes an attention mechanism to query the encoder for each output symbol. The conditional probability of the target sequence is maximized during training. The model is smaller and less complex compared to traditional models. We use a smaller and less complex model for research tractability, consisting of a single layer bidirectional LSTM in the encoder and a two-layered unidirectional LSTM in the decoder with additive attention. The word embedding dimensions and LSTM hidden cell sizes are set to 256. The model, referred to as the canonical model, was trained 10 times with different random seeds to observe the impact of parameter initialization variability on results. The study focused on how different modeling choices influenced the occurrence of hallucinations. Model variants were trained multiple times with various random seeds and hyper-parameter searches were conducted. All models had a minimum BLEU score of 20.0 on the test set. In NMT, approximate decoding techniques like greedy decoding and beam search are used to generate translations with high BLEU scores. Greedy decoding selects the most likely symbol at each time step, while beam search keeps track of multiple hypotheses to improve accuracy. Our models achieved an average BLEU score of 25.66 with beam search decoding. In NMT, beam search is often used for more accurate translations compared to greedy decoding. Experiments were conducted using the German to English WMT dataset, with models trained and validated on specific sets. Sub-word tokens were generated using Byte-Pair Encoding, resulting in a vocabulary of 12,564 unique tokens. The experiment involved generating sub-word tokens using Byte-Pair Encoding, resulting in vocabularies with 12,564, 19,708, and 36,548 unique tokens. The study focused on the percentage of hallucinations in a NMT model by selecting common, mid-frequency, rare, and punctuation tokens and comparing translated sentences with reference sentences. The study involved generating sub-word tokens using Byte-Pair Encoding to analyze hallucinations in a NMT model. By perturbing input sequences with added tokens and comparing translations, the researchers identified instances of hallucinations. The study analyzed hallucinations in a NMT model by splitting tokens into common, rare, and mid-frequency types. They perturbed sentences by inserting tokens at different positions and modified the BLEU score to define a quantitative threshold for hallucinations. The study analyzed hallucinations in a NMT model by modifying the BLEU score to define a threshold for hallucinations based on n-grams. Hallucination patterns include grammatically correct output unrelated to the input text. The study analyzed hallucinations in a NMT model by modifying the BLEU score to define a threshold for hallucinations based on n-grams. Hallucination patterns include grammatically correct output unrelated to the input text. The study found that inserting tokens in the source sequence easily evoked hallucinations. Algorithm 1 was used to quantify model susceptibility to hallucination, showing that 73% of sentences in the WMT De\u2192En test set can be perturbed to hallucination in the canonical model. Beam search, hidden units, vocabulary size, and decoding were also studied for their impact on inducing hallucinations. In the study, the impact of beam search, hidden units, vocabulary size, and decoding scheme on hallucination percentages in a NMT model was analyzed. Changing hidden units and vocabulary size did not significantly decrease hallucination percentage, but increasing vocabulary size to 32K BPE codes lowered hallucination rates. Perturbing the input sequence with a token at the beginning induced the most hallucinations. Surprisingly, BLEU scores were not predictive of hallucination percentage. The study analyzed the impact of various methodologies on reducing hallucinations in NMT models. Surprisingly, hallucination percentage did not decrease as BLEU score increased. Three methodologies were investigated: simple regularizations, data augmentation, and regularizations on state space dynamics. Simple regularizations included dropout and L2 regularization techniques. Data augmentation was implemented by perturbing training sentences with random tokens, doubling the training set. A model with dropout in all feedforward layers and L2 regularization was trained, showing a decrease in hallucination percentages. Dynamical regularizations involved training models with the decoder's initial state tied to the encoder's last step to reduce hallucinations. In experiments, Chaos-free network (CFN) BID21 was used as a regularization method to replace the LSTM cell, resulting in decreased hallucination percentages. Data augmentation was effective in reducing hallucinations but required specific perturbations. Training models without certain types of data augmentation showed varying results. The study compared models trained with different data augmentation methods to reduce hallucination percentages. Results showed that excluding common and beginning perturbation types led to higher hallucination rates, but still provided some protection. This suggests that data augmentation can help mitigate hallucinations even without exact perturbations. The study compared models trained with different data augmentation methods to reduce hallucination percentages. Results showed that excluding common and beginning perturbation types led to higher hallucination rates, but still provided some protection. Additionally, the study examined whether a DA model would be less prone to hallucinate when perturbed with types of tokens or positions it had not been trained against. The Transformer model was also analyzed, showing significantly less hallucinations compared to other models. The study compared models trained with different data augmentation methods to reduce hallucination percentages. Results showed that excluding common and beginning perturbation types led to higher hallucination rates, but still provided some protection. The transformer model hallucinates significantly less than the canonical model, but can still be perturbed to hallucinate on average 15% of the time. The model is trained with various types of regularization and a longer input sequence length. Attention networks in normal translations tend to study the entire input sequence throughout decoding. The attention matrix in normal translations tends to have a strong diagonal pattern, but when translating hallucinations, the model focuses on only a few tokens. Information entropy was used to quantify the difference in attention matrices during decoding, with hallucinations showing a different distribution of attention compared to normal translations. The attention network gives a distribution over input tokens, which is averaged across all decoded output tokens to compute the entropy of average attention weight. A significant difference in entropy values was observed between hallucination and correct translation sequences. Control experiments showed no significant difference between original and perturbed input sequences for sentences that cannot be perturbed to hallucination. This measure of entropy can be useful in scenarios where a ground truth translation is not available. The breakdown of the attention module may indicate hallucinations in translation. Hallucinations in the decoder could be caused by broken attention modules or further breakdown in dynamics. Various model variants, like L2 regularization, aim to reduce hallucinations by regulating the model's dynamics. Differences in the decoder during translation may be linked to the occurrence of hallucinations. In this section, the study focuses on differences in the decoder during translation, specifically between hallucinating and non-hallucinating sentences. Perturbations were made to source sentences to analyze changes in the decoding pipeline. The research examines the early stages of decoding, particularly the first timestep when the decoder receives context from the encoder. The study analyzed differences in decoder behavior during translation, focusing on perturbations to source sentences. Despite attempts to reduce perturbations, there was no decrease in hallucination percentage. The research explored the possibility of hallucinations resulting from exciting unstable modes of the decoder early in the decoding process. The study analyzed differences in decoder behavior during translation, focusing on perturbations to source sentences. The normalized hidden state of the perturbed input was projected onto a subspace, revealing that during hallucinations, the attention network tends to focus on only a few input tokens. An entropy measure was used to quantify the distribution of attention over the input source sequence, showing statistically significant differences between normal translations and hallucinations. In the study, the researchers analyzed decoder behavior during translation by perturbing source sentences. They found that adding a single token can lead to complete mistranslation, resulting in hallucinations. These hallucinations were common in the NMT architecture and its variants, with a focus on the stability exponents of the decoder. The phenomenon of hallucinations appears to be model-specific. The study analyzed decoder behavior during translation, finding that adding a single token can cause complete mistranslation, leading to hallucinations. Hallucinations were common in NMT architecture and its variants, with attention matrices associated with hallucinations being statistically different. The model showed more hallucinations due to differences in size and training data, but news reports of strange translations in popular systems suggest a dynamic nature of the phenomenon. Effective techniques for preventing hallucinations require knowledge of the phenomenon. The study found that hallucinations were common in NMT architecture, with perturbing input tokens causing complete mistranslations. The phenomenon was likened to a dynamical problem, with hypotheses suggesting decoder issues or poorly trained states leading to erratic dynamics before stabilizing. Results indicated that NMT networks using CFN recurrent modules were highly susceptible to perturbations. In experiments, networks with CFN recurrent modules were sensitive to perturbations, indicating challenges in understanding and fixing recurrent network issues. The CFN's chaos-free property may not transfer to the larger graph due to its embedded auto-regressive loop. Techniques to reduce hallucinations act as dynamical regularization, such as L2 weight decay conditioning RNNs for stability. Data augmentation also contributes to stability effects. Production NMT systems should be tested for hallucinations, with attention matrices and hidden states of the decoder monitored. Regularization techniques like Dropout and L2 weight decay on embeddings are crucial for reducing hallucinations. Data augmentation, including randomly inserting perturbative tokens in input sentences, is recommended for stability, although knowledge of specific pathological phenomena is required. The NMT decoder uses an attention network to compute weighted encodings for translation. Adjusted BLEU scores indicate similarity between sentences. Stability exponents are defined for the decoder. The NMT decoder utilizes an attention network to calculate weighted encodings for translation, with stability exponents defined for the decoder to study its properties. The spectrum of stability exponents is compared between normal translations and hallucinations, focusing on \u2202h0 for insights into stability. The stability exponents of the NMT decoder are defined using a finite-time version of Oseledets matrix, with the i th stability exponent calculated as \u03bb i (x) = 1/2T log (\u03b1 i (x)). The distribution of stability exponents is compared between input sequences that can hallucinate and those that cannot, showing differences between the canonical model and the model trained with data augmentation. The stability exponents of the NMT decoder are compared between the canonical model and the model trained with data augmentation. Differences in means and distributions are observed, with reduced exponents in the augmented model."
}