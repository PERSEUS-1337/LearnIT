{
    "title": "B1fPYj0qt7",
    "content": "The Tensor-Train factorization (TTF) compresses weight matrices in RNNs efficiently. This work uses Riemannian stochastic gradient descent (RSGD) to train core tensors before reducing Tensor-Train ranks. The RSGD algorithm is tested on advanced Tensor-Train RNNs like bi-directional GRU/LSTM and Encoder-Decoder RNNs with a Tensor-Train attention model. Experiments show the effectiveness of RSGD for Tensor-Train RNNs in tasks like digit recognition and machine translation. Neural Networks (RNNs) require large weight matrices for fully-connected and recurrent layers, leading to the need for extensive training data and computational resources. The Tensor-Train factorization (TTF) reduces redundancy in RNN parameters by reshaping weight matrices into high-dimensional tensors before factorizing them in a Tensor-Train format. Compared to other tensor decomposition techniques, Tensor-Train can be scaled to high dimensions and offers computational tractability for large weight matrices. The Tensor-Train factorization (TTF) technique leverages computational tractability for large weight matrices by decomposing them into core tensors. This process involves reshaping the weight matrix into a high-dimensional tensor and then decomposing it into core tensors according to a specific formula. The TTF technique is particularly useful for fully-connected layers in neural networks, where the weight matrix is converted and decomposed to improve efficiency. The Tensor-Train factorization technique reduces model parameters significantly, especially in fully-connected layers of neural networks. This method decomposes weight matrices into core tensors, improving computational efficiency. The paper introduces the application of RSGD to find vectors of lower Tensor-Train ranks in training processes. It also presents the design of Bi-directional Tensor-Train GRU/LSTM and Encoder-Decoder Tensor-Train RNNs with a Tensor-Train Attention mechanism. The RSGD algorithm is applied to these models for tasks like digit recognition and machine translation, making it the first work to do so. This work also constructs Tensor-Train RNNs with complex architectures for NLP tasks, addressing the optimization problem through Riemannian optimization. The optimization problem of Tensor-Train RNNs is formulated as a Riemannian optimization problem, where W represents parameters in a d-dimensional Riemannian Manifold with Tensor-Train ranks constrained. The Riemannian measure induces inner product structure in tangent spaces, and RSGD algorithm is applied for training tasks like digit recognition and machine translation. The RSGD Algorithm updates parameters in the tangent space and conducts exponential mapping, while also rounding to lower Tensor-Train ranks. It involves obtaining gradients on the tangent space and performing gradient descent, followed by projecting back to the core tensor and transforming to lower ranks in a submanifold. The RSGD Algorithm updates parameters in the tangent space using exponential mapping and rounding to lower Tensor-Train ranks in submanifolds. The retraction algorithm orthogonalizes core tensors in a left-to-right order. The algorithm orthogonalizes core tensors in a left-to-right order using the QR decomposition. The rounding procedure updates the Tensor-Train rank before conducting an SVD computation. The rounding algorithm does not change the objective function values for Tensor-Train RNNs. The section analyzes the convergence of the RSGD algorithm for non-convex cases. A function is geodesically L-smooth if its first-order gradient is geodesically L-Lipchitz continuous. The RSGD algorithm orthogonalizes core tensors using the QR decomposition and updates the Tensor-Train rank before SVD computation. The RSGD algorithm ensures convergence for geodesically L-smooth functions. The algorithm finds sub-gradient g x = \u2207f (x) and Exp \u22121x (y) = \u2212\u03b7\u2207 x f (x). Theorem 3 guarantees convergence to the optimal point x * after T iterations with a learning rate \u03b7. Theorem 3 guarantees convergence to the optimal point x * after T iterations with a learning rate \u03b7. A Bi-directional Tensor-Train LSTM is designed with more operational gates than the Bi-directional Tensor-Train GRU, improving performance in deep learning applications. The model is built on Tensor-Train layers, including a Tensor-Train Attention model to enhance Encoder-Decoder RNNs. To build a Tensor-Train Attention model, add the Tensor-Train layer to generate an Attention vector. The implementation of Tensor-Tensor RNNs is introduced, along with applications tested using RSGD algorithms for digit recognition and machine translation tasks. PyTorch was used to implement Tensor-Train RNNs, leveraging dynamic graph advantages over Tensorflow. In digit recognition tasks on the MNIST dataset, PyTorch was used for dynamic graph generation. The dataset includes 60000 images with 28x28 pixels each. Instead of vectorizing pixels, they were treated as a data sequence with time steps and input dimensions. Training and testing sets had 50000 and 10000 data, with 2000 for validation. Bi-directional Tensor-Train GRU and LSTM were applied with specific settings for the Manifold dimension and Tensor-Train ranks. The Tensor-Train ranks were set to high values for the layers, and RSGD algorithm with a learning rate of 0.01 was applied to Bi-directional Tensor-Train GRU/LSTM. Results in FIG2 show comparison with traditional Bi-directional GRU/LSTM in terms of recognition error rates and parameters. Bi-directional Tensor-Train GRU performs similarly to traditional GRU/LSTM, while Bidirectional Tensor-Train LSTM shows slightly worse performance. The Bi-directional Tensor-Train GRU/LSTM can significantly reduce the number of parameters compared to traditional Bi-directional GRU/LSTM. The RSGD algorithm further decreases the parameters by lowering the Tensor-Train ranks. In a machine translation task from Dutch to English, the Bi-directional Tensor-Train GRU was used in an Encoder-Decoder architecture with a Tensor-Train Attention model. The RSGD algorithm with a learning rate of 0.01 was applied to the model. The Tensor-Train Attention model with initial ranks (1, 6, 6, 6, 1) and weight matrix shape (4 \u00d7 4 \u00d7 4 \u00d7 4) was compared to traditional Bi-directional GRU/LSTM with Attention model. Experimental results in FIG3 show Tensor-Train RNN performs similarly to Encoder-Decoder RNN. RSGD algorithm reduces parameters further, as shown in TAB1. This paper introduces RSGD for training Tensor-Train RNNs. The RSGD algorithm is introduced for training Tensor-Train RNNs, showing effectiveness in performance and model complexity. Future work includes applying RSGD to more models and larger datasets, as well as generalizing Riemannian optimization for faster convergence rates."
}