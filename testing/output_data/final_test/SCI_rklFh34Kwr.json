{
    "title": "rklFh34Kwr",
    "content": "Bayesian inference enhances deep neural network performance by providing robustness to overfitting, simplifying training, and offering a calibrated measure of uncertainty. Markov Chain Monte Carlo (MCMC) methods enable Bayesian inference by sampling from the posterior distribution of model parameters. Despite theoretical advantages, sampling methods like MCMC have lagged behind optimization methods for large-scale deep learning tasks. ATMC is introduced as an adaptive noise MCMC algorithm to sample from the posterior of a neural network. ATMC dynamically adjusts momentum and noise for parameter updates to compensate for stochastic gradients. Bayesian inference in machine learning uses a posterior distribution over the weights of a neural network, allowing for sampling model instances and robust uncertainty estimates. ATMC outperforms optimization baselines in accuracy and uncertainty calibration, showing intrinsic robustness to overfitting. Bayesian inference in machine learning involves sampling a distribution for uncertainty estimates, which is crucial in domains like medical diagnosis and autonomous driving. Traditional Markov Chain Monte Carlo methods are used for generating samples from the posterior distribution over model parameters. These methods are not commonly applied in deep learning due to scalability issues with large datasets. Bayesian inference in machine learning involves sampling a distribution for uncertainty estimates, crucial in domains like medical diagnosis and autonomous driving. Traditional Markov Chain Monte Carlo methods are not commonly applied in deep learning due to scalability issues with large datasets. Stochastic Gradient MCMC methods have fared better in scaling to large datasets, with the objective of making Bayesian inference practical for deep learning by scaling SG-MCMC methods to large models and datasets. The contributions in this work include the proposal of the Adaptive Thermostat Monte Carlo (ATMC) sampler for improved convergence and stability. Additionally, improvements were made to a second-order numerical integration method required for the ATMC sampler. To address compatibility issues with stochastic regularization methods, the ResNet++ network was constructed by modifying the original ResNet architecture. The ATMC sampler outperforms optimization methods in accuracy, log-likelihood, and uncertainty calibration. It shows superior performance with ResNet++ on Cifar-10 and ImageNet, and even with multiple samples on ImageNet. The sampler reduces the need for hyper-parameter tuning and avoids overfitting and carefully tuned learning rate schedules. The ATMC sampler outperforms optimization methods in accuracy, log-likelihood, and uncertainty calibration by defining a Stochastic Differential Equation (SDE) that converges to a target distribution. This framework allows for sampling from the posterior distribution by evaluating the energy function gradient. The ATMC sampler proposed in this study defines a Stochastic Differential Equation (SDE) to sample from the posterior distribution by evaluating the energy function gradient. The energy function is defined by the loss function, and a Gaussian distribution is assumed for the stochastic gradient to reduce bias. The ATMC sampler defines an energy function with momentum p, control variate \u03be, and hyper-parameter m. The dynamics Q(z) and D(z) are defined to simulate the resulting SDE without evaluating covariance B. The variance of momentum Var(p) = m controls the coupling strength between \u03be and p. The coupling strength between momentum p and control variate \u03be is controlled by the variance of momentum Var(p) = m. Unlike other thermostat MCMC methods, the friction term in the temperature control is proportional to \u03be, not \u03be/m. The dynamics and energy function are substituted into the equation, with the gradient of the loss replaced by a minibatch estimate. The momentum is dampened by a friction term \u03b2(\u03be) dependent on \u03b1(\u03be), and the stochastic gradient noise does not appear due to the specific energy function and dynamics chosen. The temperature variable correction for diagonal covariance may lead to bias in samples. Annealing the step size can reduce error from mini-batching and other sources of discretization error. Choosing a function \u03b1(\u03be) controls noise and momentum damping \u03b2(\u03be). The Nos\u00e9-Hoover thermostat corrects stochastic gradient noise but slows convergence and causes negative friction in certain cases. The ATMC sampler is defined by \u03b1(\u03be) = max(D \u2212 \u03be, 0) to prevent negative friction and maintain convergence speed. It operates in various temperature stages where noise is added to momentum based on \u03be values. The momentum experiences minimum friction when \u03be < 0, with noise proportional to D c \u2212 \u03be. The momentum update noise is adjusted based on stochastic gradients. The momentum energy function is generalized to a symmetric hyperbolic distribution with hyper-parameters m and c. The hyperbolic distribution results in relativistic momentum dynamics with parameter updates bounded by c. The average update magnitude is approximately 1/ \u221a m for c m. The pre-conditioner M(p) depends on p. The parameters m and c are interpretable hyper-parameters controlling the average and maximum parameter update per step. Deriving ATMC with a different momentum distribution like the hyperbolic distribution involves substituting equations and the alternative momentum distribution. The numerical integrator for the ATMC sampler is constructed by splitting the SDE into two terms. The ATMC sampler splits the SDE into two terms for constructing a numerical integrator. The dynamics are updated using linear ODE and SDE in parts (A) and (B) respectively. Operators \u03c6 h A and \u03c6 h B simulate these dynamics for a time step h. The second order method is obtained using the Strang splitting scheme.\u03c6 h A and \u03c6 h B update parameters \u03b8 and \u03be, and momentum p respectively.\u03c6 B involves the Ornstein-Uhlenbeck process with isotropic Gaussian noise \u03b7 t. Integrating the gradient step and noise with the friction term provides robustness to large updates. The ATMC sampler utilizes a two-way split integrator for updating dynamics with linear ODE and SDE components separately. This approach enhances robustness to large gradients by adjusting temperature to compensate for momentum updates. Methods like Dropout and BatchNorm introduce noise into model parameters, improving training and generalization for large neural nets. These techniques can be seen as approximations of Bayesian Inference. ResNet++ is a BatchNorm-free version of ResNet that incorporates SELUs, Fixup initialization, and weight normalization. ATMC is used to improve performance in ResNet++ by filling the gap left by the absence of BatchNorm. SELU activation is found to work well in BatchNorm-free networks, forcing activations towards zero mean and unit variance. The text discusses the use of Fixup initialization and weight normalization in ResNet models to address the issue of exploding residual branches. Weight normalization helps separate the direction and scale of linear feature vectors, improving model performance without relying on batch statistics. The ATMC sampler is competitive with optimization baselines for large-scale datasets and models. Experiments use ResNet-56 and ResNet-50 on Cifar10 and ImageNet. Comparison is made with and without BatchNorm, showing performance differences. The study compares the performance of a single sample and estimated posterior predictive using a cyclic step size with cycle length n. A group Laplace prior is applied to regularize ResNet++ features, with specific momentum noise and hyper-parameters chosen for Cifar10. The number of convolution filters is doubled compared to the original ResNet-56 implementation, using a single V100 GPU with a batch size of 128. The study compares the performance of a single sample and estimated posterior predictive using a cyclic step size with cycle length n. A group Laplace prior is applied to regularize ResNet++ features, with specific momentum noise and hyper-parameters chosen for Cifar10. The number of convolution filters is doubled compared to the original ResNet-56 implementation. Sampling with a Nos\u00e9-Hoover thermostat sampler is also reported. The uncertainty estimates are analyzed by grouping predictions into 8 bins based on confidence levels. The calibration of uncertainty estimates for the posterior predictive and optimization baselines is shown in Figure 2. For ImageNet experiments, a cyclic step size with a cycle length of 20 epochs is used. A single Google Cloud TPUv3 with a batch size of 1024 is employed. Training steps using ResNet+BatchNorm model take 20% longer in wall clock time compared to ResNet++. Results show that a single sample from the posterior outperforms the optimization baseline without BatchNorm. The sampler runs for 1000 epochs, with samples collected after 150 epochs. The posterior predictive based on ATMC outperforms the optimizer with BatchNorm in accuracy and test log-likelihood. Despite longer runtime, ATMC matches the accuracy of BatchNorm after 240 epochs. ATMC produces better calibrated predictions and less bias towards overconfidence compared to the optimization baseline. Sampling the posterior distribution of neural networks on large scale image classification problems like ImageNet is feasible. Using recent advances in initialization and the SELU activation function, we can stabilize and speed up training of ResNets without BatchNorm. However, BatchNorm still offers unique advantages in generalization performance. Multiple posterior samples provide more accurate estimates for inference, but using a large ensemble of models sampled from the posterior can be costly. Variational Inference methods can quickly characterize a local mode. Methods like variational inference and distillation can be used to approximate a mode of the posterior and compress a high-quality ensemble into a single network with limited computational resources. Alternative methods for dealing with stochastic gradients have also been proposed in the literature. Stochastic optimization methods can be seen as biased sampling. Multiple samples from SGD trajectories are used for uncertainty estimates in Deep Learning, requiring hyperparameter tuning for noise insertion."
}