{
    "title": "rJlo4UIt_E",
    "content": "In this paper, the authors demonstrate that normalizing flows can be extended to discrete events without the need for log-determinant-Jacobian computations. Two flow architectures are explored: discrete autoregressive flows for bidirectionality in language models, and discrete bipartite flows for parallel generation in text modeling. These advancements open up new possibilities for modeling discrete distributions. Normalizing flows can be extended to discrete distributions without log-determinant-Jacobian computations. This allows for flexible modeling of discrete sequences, including bidirectional language models and parallel text generation. This advancement opens up new possibilities for modeling discrete distributions. Discrete flows extend normalizing flows to model discrete sequences, offering flexibility in modeling. Two architectures are demonstrated: discrete autoregressive flows enable bidirectional language models with exact likelihood and sampling, while discrete bipartite flows allow for nonautoregressive text models with exact likelihood for training and evaluation. Autoregressive models require specific ordering, unlike bidirectional models that need approximate inference. In this work, discrete autoregressive flows enable bidirectionality while maintaining the benefits of a generative model. Nonautoregressive models have also been explored for flexible modeling with dependencies, particularly for discrete distributions. The prior is autoregressive and the decoder is conditionally independent. BID13 introduces an iterative refinement stage for initial parallel generations using discrete bipartite flows. Normalizing flows transform probability distributions with invertible functions, allowing for nonautoregressive generation while maintaining an exact density. Composing multiple flows can create complex distributions, with the determinant of the Jacobian incurring a complexity of O(D^3). Normalizing flows aim to efficiently compute the determinant of the Jacobian by using autoregressive transformations. Autoregressive flows, such as recurrent neural networks and Transformers, model data across modalities by computing a location-scale transform. The flow's Jacobian is lower-triangular, simplifying the determinant calculation. This allows for fast computation and parallel processing of the inverse transformation. Normalizing flows, like autoregressive flows, aim to efficiently compute the determinant of the Jacobian for training and evaluation. RealNVP coupling flows follow a bipartite factorization, allowing for flexible distribution learning. While not as expressive as autoregressive flows, RealNVP flows have fast computations for both forward and inverse transformations, making them suitable for generative modeling. Normalizing flows extend to discrete random variables by summing over the pre-image of a function f. The log-determinant-Jacobian correction is not needed for discrete distributions as volume adjustments are not applicable. This simplifies computations compared to the continuous case. Discrete invertible functions, like XOR bitwise operator, capture correlations in data that cannot be captured by the base distribution alone. Discrete flows perform multi-dimensional relabeling of data to make it easier to model with the base distribution. The curr_chunk discusses the extension of XOR to the categorical setting through location-scale transformations on a D-dimensional vector. This transformation is made invertible by ensuring coprimality between \u03c3 and K, with various methods suggested to achieve this. The modulo location-scale transform generalizes XOR and can be applied to the bipartite flow setting as well. The curr_chunk extends the idea of XOR to the bipartite flow setting by using functions (\u00b5, \u03c3) set to (0, 1) for a subset of data dimensions. An example is provided using flows to model correlated categorical data drawn from a mixture of Gaussians. A factorized base distribution fails to capture data correlations, while a single discrete flow can. The maximum likelihood objective per datapoint with discrete flow models is discussed. The curr_chunk discusses the negative log-likelihoods for autoregressive and bipartite flows, showing improvements over their base distributions. Gradient descent is used to optimize parameters. Autoregressive flows outperform their base on the Ising model. To optimize parameters for autoregressive and bipartite flows, backpropagation through the discrete-output function is used with the straight-through gradient estimator BID1. The network outputs two vectors of logits for each dimension, and the argmax operation is replaced with the softmax-temperature function on the backward pass. The Gumbel-softmax distribution approximation works well when the number of classes is less than 200, with a fixed temperature of 0.1. The study explores the utility of discrete autoregressive flows and discrete bipartite flows in various experiments using different base distributions and network architectures. The experiments aim to analyze the expressivity of the flows for arbitrary discrete distributions, utilizing a Transformer with 64 hidden units for both the base and flows. The study compares autoregressive flows and bipartite flows using a Transformer with 64 hidden units for both base and flows. Autoregressive flows outperform the base distribution, while bipartite flows compete with the autoregressive base. An addition task is examined following a right-to-left ordering, contrasting with the left-to-right ordering of the autoregressive base. The study compares autoregressive flows and bipartite flows using an LSTM with different hidden units. Autoregressive flows achieve lower nats than the base distribution, while bipartite flows show competitive results. The use of bidirectional generative models for learning undirected models is also explored. The study compares autoregressive flows and bipartite flows using an LSTM with different hidden units, achieving lower nats than the base distribution. Bidirectional generative models are used for learning undirected models, with experiments on Ising models showing promising results. The use of discrete flows for modeling discrete data is also discussed, with a focus on applying these ideas to larger-scale text data. One challenge in scaling discrete flows to large numbers of classes is the limitation of the straight-through gradient estimator, which is effective for small class sizes but may not work well for (sub)word-level modeling with vocabularies exceeding 5,000."
}