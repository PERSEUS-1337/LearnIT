{
    "title": "H1gW93NKvH",
    "content": "Inferring temporally coherent data features is essential for various learning tasks. A network architecture with temporal recurrent connections in residual blocks improves the stability of temporal states in convolutional neural networks. This approach shows promise in generating high-quality super-resolution images from low-resolution images, particularly challenging due to strongly aliased signals. Adjustments to generative adversarial architectures for video super-resolution have led to a promising initial model. Innovative approach proposed for video super-resolution using generative adversarial architectures to create detailed and coherent images from real-time renderer inputs. Introducing a new connection type in ResNet architectures to simplify learning tasks and stabilize inference results. Adversarially trained CNNs can generate detailed images from low-resolution inputs produced by a real-time renderer, benefiting applications like mobile device content generation and game streaming services. Our architecture introduces a recurrent connection for generating high-quality images in real-time rendering, differentiating from previous works by maintaining a stable latent-space representation without compromising the ResNet architecture's residual flow. Temporal connections in deeper layers are crucial for successful learning, with potential applications beyond real-time rendering. Modifications to existing architectures are outlined to achieve this goal. The curr_chunk outlines modifications to existing architectures crucial for high-quality image generation in real-time rendering, specifically addressing issues with strongly aliased input images. The proposed network with DRR connections infers more consistent spatio-temporal features compared to existing methods, reducing flickering artifacts and producing clearer results. This application is challenging for CNNs due to the need for high frame rates and severe aliasing. The proposed depth-recurrent connections in the network address issues with strongly aliased input images, enabling the synthesis of high-quality images with consistent spatio-temporal features. This is crucial for real-time rendering at high frame rates, reducing flickering artifacts and improving visual detail and temporal behavior. Additional results and comparisons are provided in the paper and supplemental material. Deep learning techniques, including Generative Adversarial Networks (GAN), have been used for image super-resolution tasks. GAN architectures involve a discriminator network to distinguish between real and generated samples, guiding the generator network. Modifications to GAN approaches have improved results by adjusting residual blocks and loss functions. Using techniques such as modifying residual blocks, employing Earth Movers distance, and considering the existence of fake samples, along with utilizing feature-space differences in image classification networks, has shown to be effective in improving image super-resolution tasks. For video super-resolution, aligning frames via warping and utilizing optical flow estimation networks are common practices. Flow estimation networks are commonly used for video super-resolution tasks. Employing adversarial loss in the temporal domain improves temporal coherence. Spatio-temporal GAN architecture is also utilized. Recurrent neural networks have been proposed for deep learning methods and shown to be useful for image generation tasks. The SR task is similar to image enhancement techniques like inpainting. In the context of video super-resolution tasks, spatio-temporal self-supervision is used to enhance path-traced images in real-time renderers. Existing work focuses on image denoising for ray-tracing applications, while computer games utilize rasterization-based rendering pipelines for shading geometry with complex lighting and texturing. In real-time rendering pipelines, shading geometry with complex lighting and texturing computations often use image-space methods to reduce aliasing artifacts. Super-Sampling and Multi-Sampling are anti-aliasing solutions, while Temporal Anti-Aliasing smooths and stabilizes edges using previous samples. Deep learning models are also employed for real-time image enhancement. The use of deep learning models for real-time image enhancement is limited, with only a closed-source solution provided by Nvidia. The goal is to provide an open solution and improve GAN-based SR by learning a stable internal network representation for persistent and stable predictions. This helps in processing strongly aliased input data streams and improves temporal coherence and small-scale details in SR rendering. The proposed architecture improves temporal coherence and generates stable small-scale details in super-resolution rendering. The generator network is augmented with aligned, temporal connections inside residual blocks to learn this internal representation. The network processes LR input and warped previous output through ResBlocks to produce detailed and temporally coherent output. Additionally, a spatio-temporal discriminator sees 3-frame sequences of the output and LR input, along with employing a perceptual feature loss via the discriminator network. The generator in the proposed architecture is trained with adversarial and feature loss, along with Ping-Pong term and L2 loss for stability. The Depth-Recurrent Residuals setup connects residual blocks of consecutive frames through latent space, improving temporal coherence in video data processing. Our model overcomes spatial and temporal aliasing challenges by using a \"feed-forward\" design with a warping step. The proposed variant focuses on temporal stability by allowing comparisons between activations from previous and current frames. The operations of a regular ResBlock involve convolutional layers followed by addition for the final output. The depth-recurrent residual blocks in our model use convolution and ReLU activation, with a modified second convolutional layer operation. Warping is applied based on externally computed velocities, and feature concatenation is denoted by \u2295. To reduce the number of features, we use 75% fewer for DRR blocks while maintaining the same number of weights per ResBlock. Curriculum learning steps are applied during experiments to stabilize training runs, with depth recurrent connections linearly faded in staggered fashion. The model uses depth-recurrent residual blocks with convolution and ReLU activation, incorporating a modified second convolutional layer operation. Warping is applied based on externally computed velocities, and feature concatenation is denoted by \u2295. The discriminator network is faded in after pre-training the generator. The proposed DRR connection connects the center latent space of ResBlock n at time t \u2212 1 to the same ResBlock n at time t. A variant, DRR-in, includes recurrent connections similar to previous work. Alternative connectivities are evaluated to support learning stable features. The improved architecture and warping operations in this variant of the model result in slightly worse spatial results but improved temporal stability. The recurrent ResNet architecture includes in-place connections feeding outputs from previous ResBlocks of both current and previous frames. This variant shows improved image quality and stability compared to other models. Our DRR-in variant of the model utilizes in-place connections feeding outputs from previous ResBlocks, resulting in improved stability by focusing on learning temporal features rather than a combination of spatial and temporal information. This approach preserves the structure that led to the success of the original model. Applying the W function with screen-space motion vectors improves results in all three variants of the model. Depth recurrent connections pass on learned latent-space features within the network, reducing the need to reanalyze image information for new tasks. However, encoding coherent small-scale features across frames remains challenging. Using only depth recurrent connections leads to temporally coherent results but causes a significant loss of spatial information. Depth recurrent connections work best in conjunction with a recurrent input for improved results in image rendering. Training data consists of real-time rendered images with a rasterization-based pipeline. The network is trained on 15-frame sequences of HR-LR image pairs using Unity's HDRP. Super-resolution models for videos are trained on down-sampled targets to enhance detail generation. Synthetic data exhibit aliasing artifacts, leading to larger changes between adjacent pixels. The data exhibits strong aliasing artifacts, making analysis difficult due to larger changes between pixels and temporal aliasing. Sub-pixel features are rare, making it challenging to correlate image content in subsequent frames. To address these challenges, DRR connections are introduced to provide more temporal context. Additionally, the model incorporates motion vectors from the renderer for warping. Incorporating motion vectors from the renderer for warping operations is crucial to reduce checkerboard artifacts and improve accuracy. Using resize-convolutions with bilinear interpolation instead of deconvolutions is also important. Providing depth as input enhances edges and reduces aliasing artifacts, especially near the camera. However, adding additional data fields like surface normals did not result in improvements. For strongly aliased data, using bilinear interpolation for up-scaling inputs is more beneficial than bicubic interpolation. The network needs to detect structures based on single samples in the low-resolution input to perform anti-aliasing and apply the necessary correction via residual addition to the high-resolution version. This approach is effective for smoother data in natural videos. Replacing the final addition of the residual with 2 additional convolutional layers improves image quality by allowing the network to choose parts of the up-sampled LR color. This modification adds 1% additional weights but enhances image quality despite increased computational cost. To maintain consistency in weight amounts, the number of base channels in ResBlocks is reduced when using recurrent connections. The discriminator plays a crucial role in generating sharp images, with activating the spatio-temporal discriminator leading to significant improvements. It is important to use a discriminator without conditional input to balance the networks, and increasing its complexity can further enhance results. Increasing the depth of the discriminator allows for detection of more complex features, providing gradients towards complex image content. A larger discriminator with 14 layers and 3.7M weights, paired with a generator with 26 layers and 769k weights, yields high-quality results. Depth-Recurrent Residuals in the generator also contribute to good results, especially when trained with a large discriminator. The generator network benefits from detailed gradient feedback from a large discriminator, improving image quality. Activating DRR connections later in training enhances results. Balancing the learning setup is crucial despite stabilization via DRR connections. The larger discriminator provides additional details but increases small-scale differences to the target, requiring L2 content loss for training stability. A parameter is introduced for choosing temporal stability. In practice, a parameter is introduced to balance temporal stability and image-space detail. The discriminator feature loss contributes to important details, while a strong feature loss reduces smoothing caused by content loss for optimal results. Using a temporal L1 loss without warping leads to streak-like artifacts, and additional edge loss terms do not improve this. Perceptual losses with pre-trained networks yield different outputs but not necessarily better ones. The different nature of images produced by rasterization pipelines may not be well captured by VGG features. Comparing our model with and without DRR connections to previous state-of-the-art methods, including supervised approaches like DUF, Enet, FRVSR, and TecoGAN, all existing methods struggle to stabilize input data and produce strong artifacts. Re-training a TecoGAN model with our dataset also results in undesirable outcomes. Standard image quality metrics often do not reflect human perception accurately. In practice, PSNR values fluctuate during training and have limited significance. Perceptual metrics like LPIPS are used for measuring similarity to target images. Temporal coherence is evaluated using tLP, with blurry data showing better coherence. Results show that the method yields the best temporal coherence and excellent LPIPS scores. The DRR connections are crucial for good temporal changes and details in real-time rendering scenarios. The pre-trained model takes 113ms per frame on average for a resolution of 1920\u00d71080, which can be improved with network compression and dedicated hardware evaluation. Residual connections in conditional generator architectures, specifically DRR connections, are effective for stable internal latent-space representations in iterative models with aliased data. The network architecture adjustments lead to high-quality synthesis in real-time rendering. DRRs have potential benefits for tasks like object tracking and physics predictions. Data from \"FPS Sample\" and \"Book of the Dead: Environment\" projects in Unity engine were used for training, validation, and testing with 57 sequences split accordingly. Each frame includes lit color, unlit diffuse color, surface normals, and roughness. The data used for training includes lit color, unlit diffuse color, view-space surface normals, roughness, screen-space motion, and depth for both high-resolution (HR) and low-resolution (LR) images. The network architecture is based on a modified TecoGAN generator and a larger version of TecoGAN's discriminator. Training involves 400,000 iterations with various losses gradually introduced over the first 40k iterations. During training, various losses such as L2, feature loss, and ping-pong loss are gradually introduced over 40k iterations. Temporal connections are faded in later during training in a staggered fashion, with exponential learning rate decay in the last 150k iterations. The discriminator is balanced with the feature loss to prevent overwhelming the generator, and is only trained if below a threshold. The final DRR version uses specific weights for the loss function terms. Our modified generator includes LR color, frame-recurrent input, and LR depth. It consists of sequential ResBlocks for processing, followed by resize convolutions and fine-tuning. The model can produce a range of content in low-resolution sequences. The trained model can produce additional sequences with low-resolution input and inferred output. For a full assessment of quality, refer to the supplemental material document for animated sequences."
}