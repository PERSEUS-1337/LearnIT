{
    "title": "SJtfOEn6-",
    "content": "Recent efforts in training light-weight binary neural networks have shown promising efficiency in execution and memory usage. ResBinNet, a new approach, combines residual binarization and temperature adjustment methods to improve the convergence speed and accuracy of binary convolutional neural networks. These methods jointly learn soft-binarized parameters, enhancing the performance of binary neural networks. A prototype hardware accelerator implementing ResBinNet demonstrates its applicability and scalability, offering a trade-off between runtime and inference accuracy through reconfigurable numerical precision of binarized features. Convolutional Neural Networks (CNNs) are over-parameterized for training efficiency. Optimization methods like tensor decomposition, parameter quantization, sparse convolutions, and binary networks aim to reduce network complexity. Binary networks offer benefits of reduced memory footprint and energy consumption by replacing multiplications with XNOR operations. In this paper, ResBinNet is proposed as a novel solution to increase the convergence rate and final accuracy of binary neural networks. The challenges of training binary CNNs include slower convergence rates and the need for high dimensional feature maps in wide CNN topologies to maintain classification accuracy. ResBinNet introduces Soft Binarization methods to address these challenges. The Soft Binarization phase introduces Residual Binarization and Temperature Adjustment methods to improve training of binary CNNs. These methods enhance convergence rate and final accuracy of ResBinNet. After soft training, weights are converted to binary values for fine-tuning in the Hard Binarization phase. Fine-tuning of the model is performed in the Hard Binarization phase using existing training algorithms like BinaryNets in few epochs. ResBinNet aims to enable reconfigurability for binary neural networks, improve convergence rate, and be compatible with XNOR multiplication approach. The contributions include proposing residual binarization for learning multi-level representations in binary CNNs and introducing temperature adjustment. The paper introduces ResBinNet, a method for gradual binarization of CNN weights using temperature adjustment. It discusses the trade-off between accuracy and execution time, evaluates ResBinNet on MNIST, SVHN, and CIFAR-10 datasets, and provides an open-source API for ResBinNet. The paper is organized into sections describing residual binarization for activations, temperature adjustment for weights, efficient implementation of ResBinNet operations on hardware accelerators, experiments, related work, and conclusion. The paper introduces ResBinNet, a method for gradual binarization of CNN weights using temperature adjustment. A binarization scheme converts values to binarized estimations, reducing memory footprint. Binarized weights and input features allow for efficient computation using XNOR-popcount operations. The binary representations are computed by encoding sign vectors to binary vectors. The paper introduces ResBinNet, a method for gradual binarization of CNN weights using temperature adjustment. It proposes a multi-level binarization scheme to increase numerical precision by sequentially binarizing residual errors. This approach limits the model's ability to provide accurate inference compared to floating-point counterparts. ResBinNet introduces a multi-level binarization scheme to improve numerical precision in CNN weights. The method computes estimates and residual errors at each level, using a parameter \u03b3 i to adjust the estimate. Each level has a separate full-precision representative \u03b3 i, learned during training. The approach uses residual binarization as the activation function, differentiating it from single-bit approaches. In ResBinNet, multi-level XNOR-popcount is used for dot product computation between residual-binarized feature vectors and binary weights. The method employs separate full-precision representatives for each level, adjusting estimates with parameter \u03b3 i. Activation functions are based on residual binarization, enhancing numerical precision in CNN weights. The dot product between feature vectors and binary weights is computed using multi-level XNOR-popcount in ResBinNet. Separate full-precision representatives are used for each level, adjusted with parameter \u03b3 i. Residual Encoding converts matrix-vector multiplications into XNOR-popcount operations by encoding features into binary values. In this section, the methodology to minimize approximation error during training for neural network weights is explained. A change of variable \u03b8 = \u03b3 H(\u03b1W) is used instead of directly using parameter set W to compute the layer's output. Parameters \u03b3 and \u03b1 control the maximum and minimum values of \u03b8 and the slope of function H(.), respectively. This approach aims to reduce binarization errors in the trained weights. The Temperature parameter, denoted as \u03b1, influences the slope of function H(.). Parameters \u03b1 and \u03b3 impact the nonlinear change of variable \u03b8 = \u03b3 T anh(\u03b1W). \u03b8 serves as a semi-binarized parameter during soft training. Increasing the temperature parameter makes H(.) closer to a binary sign function, reducing errors in approximating \u03b8 with \u00b1\u03b3. W and \u03b3 are trainable parameters, with \u03b8 used in forward propagation and W updated in backward propagation. The gradient flow through W is controlled by \u2202\u03b8 \u2202W; if \u03b8 is close to \u00b1\u03b3, the gradient is filtered out. Increasing the temperature parameter \u03b1 affects the gradient filtering term \u2202\u03b8 \u2202W during training. Higher temperatures amplify gradients for elements of W closer to 0, while damping gradients for elements closer to the binary regime (\u03b8 \u2248 \u00b1\u03b3). This pushes weights towards the binary regime more forcefully, reducing binarization error. To prevent loss optimization issues, start with a low temperature and gradually increase it during soft binarization. The soft binarization phase gradually adapts weights to binary values during training, increasing convergence rate of binary CNNs. ResBinNet offers a balance between accuracy and execution time with minimal modifications needed for hardware accelerators. ResBinNet provides a balance between accuracy and execution time for binary CNNs, with modifications made to hardware accelerators. The modified accelerator will be available on Github for public use. The matrix-vector multiplication unit is the most computationally intensive operation in the accelerator design. To accommodate residual binarization, the XNOR-popcount operation is sequentially performed on binary vectors, resulting in different vectors that are then computed to obtain the output. The runtime of multilevel XNOR-popcount with residual representations is approximately l times the runtime of conventional XNOR-popcount. Batch-Normalization in the inference phase involves multiplying a vector by a constant vector and subtracting another vector to obtain a normalized vector. In ResBinNet, features are represented with l-bit residual representations, making Boolean OR no longer equivalent to max-pooling. The pooling operation can still be performed over the features. In ResBinNet, features are encoded with binary values and max-pooling is performed directly over the encoded values using Keras BID1 library with a Tensorflow backend. The FPGA accelerator's resource utilization and latency are evaluated using Vivado Design Suite BID15. Temperature adjustment is done using a \"hard tanh\" nonlinearity by incrementing \u03b1 at the end of each epoch. ResBinNet is evaluated on MNIST, CIFAR-10, and SVHN datasets, comparing accuracy, training epochs, network size, and execution time. Architecture of the trained neural networks is presented in Table 1. The architecture of the trained neural networks for MNIST, CIFAR-10, and SVHN datasets is presented in Table 1. The accuracy of binary CNN models is influenced by the network size, with variations in accuracy reported for different architectures. For example, the accuracy of MNIST ranges from 95.83% to 98.4% when the number of neurons in hidden layers is varied. Similarly, using a smaller architecture for CIFAR-10 results in a drop in accuracy from 88.6% to 80.1%. ResBinNet can reduce accuracy drop by using more residual binary levels for activations in smaller models. Binarized CNNs typically require more training epochs compared to full-precision networks. Soft binarization in ResBinNet can significantly reduce the number of training epochs for binary CNNs. ResBinNet outperforms Binarynet and FINN in accuracy rates due to its network architecture. ResBinNet achieves higher accuracy rates by using more residual binary levels in smaller models compared to FINN. The training process includes a soft binarization phase and a single fine-tuning epoch. The comparison between ResBinNet and FINN shows improved accuracy for CIFAR-10 with more than 1 level of residual binarization. Additionally, ResBinNet's convergence rate improves as the number of residual levels increases for SVHN and MNIST datasets. ResBinNet achieves higher accuracy rates with more residual binary levels in smaller models compared to FINN. The accuracy and convergence speed improve as the number of residual levels increases for SVHN and MNIST datasets. Evaluation of area overhead and execution time for ResBinNet on modified hardware architecture is discussed, comparing resource utilization for different FPGA resources. ResBinNet shows increased accuracy with more residual binary levels in smaller models compared to FINN. The method offers a scalable design for real-world systems, with modest increases in resource utilization. ResBinNet enables higher accuracy by tolerating higher latency, which scales almost linearly with the number of residual levels. Training CNNs with binary weights and activations has been a recent focus in research. Recent works have proposed optimization solutions for using binarized values in CNNs, enabling simple and efficient hardware accelerators. These works suggest methods such as leveraging full-precision weights to generate binary representatives and replacing dot products with XNOR-popcount operations. However, they lack reconfigurability in their designs beyond changing the CNN architecture. Another line of research focuses on investigating the reconfigurability of CNN accelerators. The research focuses on reconfigurability of CNN accelerators by using adaptive low bit-width representations to compress parameters and simplify arithmetic operations. ResBinNet is the first solution to offer both reconfigurability and benefits of binarized CNNs, compatible with existing optimization solutions. The proposed novel reconfigurable binarization scheme aims to enhance the convergence rate and accuracy of binary CNNs. It involves two phases: soft binarization, which includes residual binarization and temperature adjustment, and hard binarization for fine-tuning the model. The joint use of residual binarization and temperature adjustment improves the performance of binarized CNNs. The proposed reconfigurable binarization scheme can be easily integrated into existing CNN hardware accelerators with minimal modifications. It offers a balance between application latency and inference accuracy for deep learning systems."
}