{
    "title": "SyxrxR4KPS",
    "content": "In this work, a virtual rodent is developed as a platform to study motor activity in artificial models of embodied control. The model is trained to solve four complex tasks, revealing two classes of representations encoding task-specific strategies and task-invariant kinematics. This neuroethological approach characterizes motor activity relative to the rodent's behavior and goals. The virtual rodent facilitates collaborations between deep reinforcement learning and motor neuroscience by studying flexible behavior in mammals to inspire algorithms for artificial systems. Recent research at the intersection of neuroscience and machine learning aims to use artificial models to understand biological intelligence, particularly in vision, audition, and navigation. Models of biological locomotion systems have also contributed to our knowledge of body mechanisms and behavior. The development of models of embodied control is valuable for motor neuroscience and AI research. A virtual model of a rodent is introduced to investigate embodied motor systems in a grounded manner. A virtual rodent model is created to study embodied motor systems, allowing for comparison between artificial control principles and biological data from real rodents. Inspired by deep reinforcement learning algorithms, the virtual rodent can perform complex movements in a physical environment with sensory inputs similar to animals. It has actuators for coordination and a sensory system for visual and proprioceptive input. The virtual rodent platform allows for studying embodied control across multiple tasks in a physical environment. The platform enables analysis of how movements and sequences change based on goals and task contexts. The virtual rodent was trained to solve four complex tasks requiring coordinated body control, posing the question \"Can a neuroscientist understand a virtual rodent?\" The virtual rodent platform enables analysis of movements and sequences in a physical environment. Using neuroscience analysis approaches, researchers explore the representations and dynamics of the virtual rodent's neural network in solving motor and cognitive tasks. The virtual rodent body implemented in MuJoCo has 38 controllable degrees of freedom, with tail, spine, and neck segments controlled by tendons. It has access to proprioceptive information and egocentric RGB-camera input. The virtual rodent in MuJoCo has 38 controllable degrees of freedom and is trained to perform tasks like jumping over gaps, foraging in a maze, escaping from a hilly region, and touching a ball with precise timing. These tasks encourage diverse motor behaviors in the rodent. The virtual rodent agent in MuJoCo performs tasks in diverse terrains, such as jumping over gaps, foraging in a maze, and touching orbs with precise timing. The agent infers tasks from visual input and is trained using a combination of visual image inputs, proprioceptive state observations, and a recurrent LSTM module. The core module is trained by backpropagation during training of the value function. Outputs of the core are passed as features to the policy module along with shortcut paths from proprioceptive observations and encoded features. The policy module consists of stacked LSTMs producing actions via a stochastic policy. A single architecture was trained on multiple motor-control-reliant tasks using an IMPALA-style setup for actor-critic DeepRL. The value-function critic was trained using off-policy correction via V-trace. To update the actor, a variant of MPO was used where the E-step is performed using advantages determined from empirical returns and the value-function. Training a singletask expert on the escape task and multi-task policies using kickstarting with a weak coefficient facilitated comparison of different architectures. The procedure involves training a neural network with 1, 2, or 3 layers, incorporating visual inputs to solve tasks like gaps, forage, escape, and two-tap. Analysis of the network's activity and behavior was done to understand how it solves multiple tasks, using techniques adapted from neuroscience. The study focused on understanding how different network layers encode and generate movement in a virtual rodent, aiming to provide insights into real rat behavior. Data on kinematics, joint angles, forces, sensory inputs, and cell unit activity were collected from various network architectures during behavioral tasks. The study aimed to understand how neural networks encode and generate movement in a virtual rodent by describing its behavioral repertoire. Behavior can be described at different timescales, from joint-specific patterns of forces and kinematics to coordinated movements like running and jumping. Analyzing neural representations of motor behaviors requires methods that span multiple timescales of behavioral description. Sets of behavioral features were developed to describe the kinematics of the animal on fast, intermediate, or slow timescales. The study developed sets of behavioral features to describe the kinematics of a virtual rodent on different timescales. These features were validated by producing a behavioral map that segregated virtual rodent behaviors into different regions. The virtual rodent's behavioral repertoire included actions like rearing, jumping, running, climbing, and spinning, with some unexpected features. The stride frequency during galloping matched that of real rats. The study also investigated how these behaviors were utilized by the virtual rodent across tasks. The virtual rodent exhibited behavioral flexibility across tasks, adapting movements selectively. Neural activity patterns showed distinct timescales for core and policy units. Core units fluctuated over 1-10 seconds, likely representing variables associated with behaviors. The neural activity patterns in the core and policy layers of the virtual rodent exhibited distinct timescales, with core units fluctuating over 1-10 seconds and policy units being more active over subsecond timescales. Representational similarity analysis (RSA) was used to quantify how different features were encoded in the neural network layers. Neural populations encode stimulus features, and their similarity matrices can be compared across network layers using metrics like matrix correlation or dot product. The linear centered kernel alignment (CKA) index is used for invariance to rotations of population activity. RSA can test how well stimulus features are encoded, with correlation strength reflecting the ability of a linear decoder trained on the neuronal population. Behavioral analysis using RSA involved partitioning time into 50 clusters to reveal distinct encoding of behavioral features in core and policy layers across different architectures. Policy layers encoded fast timescale kinematics more prominently, while core layers showed stronger encoding of slow behavioral features. This difference in encoding was consistent across all tested architectures. The neural networks' feature encoding in policy layers showed a hierarchy of behavioral abstraction, with the last layer emphasizing fast features and the first layer focusing on slow features. Core neuron representations were distinct across tasks, while policy layers had more overlap in representations. The neural networks' policy layers showed overlap in behavioral representations, especially for fast features, across tasks. Core layer representations varied more, suggesting encoding of behavioral sequences or task variables. One layer networks exhibited increased similarity in behavioral encoding across tasks compared to other architectures. Neural activity in networks with lower computational capacity relies on a shared behavioral representation across tasks. Sequential activation patterns were observed in core and policy units during the production of stereotyped behaviors, showing consistency across tasks and behaviors. Policy networks encode short-timescale kinematic features in a task-invariant manner, with largely conserved neural activity sequences. Neural activity sequences were consistent across tasks and behaviors, showing reduced variability during specific timepoints. Sequential activity suggests a mechanism for behavioral production. Principal components analysis was used to visualize population dynamics during task performance. During a two-tap task, rotational dynamics were observed in neural activity across core and final policy layers. jPCA was used to extract latent rotational patterns, revealing lower characteristic frequencies in core layers compared to policy layers. This suggests a mechanism for behavioral production during task performance. The rotational dynamics in the core and policy layers were associated with behaviors and task rewards. In the two-tap task, rotations in the core jPC planes were linked to reward approach, while rotations in policy layers were correlated with running and the two-tap sequence. This trend of core reflecting behavioral features and policy reflecting task-related features was consistent across different tasks. In the maze forage task, the core jPC planes correlated with reaching the target orb and discovering new orbs, while the policy jPC planes were associated with locomotor features like running phase. These findings support a model where the core layer transforms sensory information into a contextual signal for task-specific modulation of behavior. Silencing and activating different neuronal subsets in the two-tap task demonstrated the distinct roles of core and policy units in encoding task-relevant features and movement. In the two-tap task, different neuronal subsets were manipulated to observe their effects on trial success and behavior. Inactivation of policy units had a stronger impact on motor behavior compared to core units. Ablation of neurons in the final policy layer disrupted specific behaviors, while ablation of core units mildly affected behavior production. The ablation of core units mildly affected behavior production, altering the direction of behavior towards objects. Complementary perturbations aimed to elicit behaviors by overwriting cell states with average neural activity trajectories were successful in some cases, such as eliciting spinning movements during a two-tap sequence. The efficacy of activation in layers closer to motor output was reliable. Core units rarely elicited spins but rather sporadic dashes resembling a searching strategy. Computational neuroscientists aim to reverse-engineer the nervous system with embodied models of animals equipped with artificial nervous systems. A virtual rodent capable of complex locomotor behaviors was introduced to study neural control of movement. The neural control of movement was studied using a virtual rodent with artificial nervous systems. This approach allows for a comprehensive observation of sensory inputs, neural activity, and behavior, aiding in testing theories on behavior generation. The model provides complete knowledge of connectivity, sources of variance, and training objectives, offering a ground truth for neural analyses. The analyses aim to describe algorithms and representations used by the virtual rodent and replicate known functional objectives without prior knowledge. The analysis of neural dynamics in a virtual rodent model reveals insights into behavior generation and motor control. The core and policy layers represent value and motor production, with behavioral representations influencing moment-to-moment motor actions. Task-specific strategies are engaged through shared motor activity in the policy layer. Our analysis of neural dynamics in a virtual rodent model suggests that reused motor activity patterns are organized as sequences, such as running, jumping, spinning, and the two-tap sequence. Sequential neural activity is linked to stereotyped behaviors in rodents and singing birds. Different behaviors are associated with distinct rotations in neural activity space that evolve at different timescales, indicating hierarchical control. The virtual rodent model shows that motor activity patterns are organized as sequences, linked to stereotyped behaviors. Different behaviors are associated with distinct rotations in neural activity space that evolve at different timescales, indicating hierarchical control. The study aims to understand the neural mechanisms behind behavior generation by incrementally increasing the model's realism and diversity of tasks. The virtual rodent model aims to understand cognitive behaviors through decision making, memory-based navigation, and working memory. Biologically-inspired neural architectures are designed for comparisons to real neural recordings. The model is constructed by obtaining mass and lengths of body segments from dissected cadavers of rats. The study involved measuring body segments on anesthetized animals to approximate bone lengths. Length measurements of limb segments from female rats were used to create a virtual rodent model. Features describing the whole-body pose and kinematics were generated on different temporal scales. The study created a virtual rodent model by measuring body segments of anesthetized animals. Postural and kinematic features were computed on different temporal scales using wavelet transforms and principal components analysis. This allowed for a detailed representation of the animal's behavioral kinematics for comparison with neural representations. In separate work, postural and dynamical features were combined to create a 60-dimensional set of 'behavioral features' for mapping animal behavior using tSNE. Power spectral density estimates of animal behavior and network activity were computed, showing differences in spectral density across features and network layers. The power spectral density of network activity was analyzed using Welch's method, showing policy layers have more power in high frequency bands. Representational similarity analysis was used to compare population representations across different network layers and compute the encoding strength of features describing animal behavior. In scaling neural analysis to behavior, the challenge lies in discretizing continuous behavior into discrete chunks for comparison. Eight sets of features were defined to describe animal behavior on different timescales, including joint angles, eigenposture coefficients, and actuator forces. Each feature set was discretized using k-means clustering to yield a partition of timepoints in the experiment. In scaling neural analysis to behavior, discretizing continuous behavior into discrete chunks is a challenge. Clustering with k = 50 partitions timepoints in the experiment. Representational similarity analysis compares population responses across neural network layers or between network layers and behavioral features. Matrices X and Y represent population responses and behavioral categories. The linear Centered Kernel Alignment index is used to compare the representational structure of the matrices XX T and Y Y T. The linear Centered Kernel Alignment index is used to compare neural representations across network layers and behavioral descriptors. It involves comparing the similarity of network activity to behavioral clusters observed in different tasks. The linear Centered Kernel Alignment index compares neural representations across network layers and behavioral descriptors. A restricted partition of timepoints for each task is defined based on specific behaviors. Representational similarity analysis is performed across continuous time domains, focusing on population-level responses. Different subspaces of the population may encode information about distinct behavioral features at different timescales. This is an emerging domain in representational similarity analysis techniques. During running cycles, neural activity in policy 2 was analyzed, showing reduced variability during stereotyped behaviors. Neurons exhibit motor noise through sensory reafference, impacting their activity patterns. The neural variability reduces with stimulus or task onset, possibly due to moments in a task that benefit from increased behavioral precision. This effect emerges from training and may partly arise from variance modulation."
}