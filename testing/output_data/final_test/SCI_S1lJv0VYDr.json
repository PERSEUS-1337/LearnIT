{
    "title": "S1lJv0VYDr",
    "content": "Model-based reinforcement learning (MBRL) aims to reduce interactions with real-world environments by learning a dynamic model. However, estimation errors in the learned model lead to mismatches with real-world environments, impacting sample complexity. To address this, a new approach using WGAN to match distributions of multi-step rollouts is proposed, showing that it can minimize differences in cumulative rewards between real and learned transitions. Experiments demonstrate the effectiveness of this model imitation method. The proposed model imitation method in model-based reinforcement learning outperforms state-of-the-art in sample complexity and average return. Reinforcement learning is crucial for sequential decision-making, but model-free RL may not fully exploit past interactions in costly environments like autonomous driving. Model-based RL learns agent policies and environment dynamics from past queries, improving task performance. Model-based reinforcement learning (MBRL) reduces the number of samples needed to learn optimal policies by allowing agents to interact with simulated environments. Previous works in MBRL use supervised learning to obtain environment models, but optimizing policies on synthesized environments remains challenging due to the planning horizon dilemma. The planning horizon dilemma in MBRL requires meticulous design to ensure accurate transition model learning. While some propose multi-step training, experiments show limited benefits from this approach. The essence of supervised learning may not guarantee resemblance in trajectories, impacting policy gradient estimation. In this work, the authors propose learning the transition model via distribution matching using WGAN to ensure similarity between real and learned trajectories, addressing the limitations of supervised learning in preserving trajectory resemblance. Our method aims to generate similar trajectories by learning the transition model through distribution matching using WGAN. This approach overcomes the limitations of supervised learning in imitating multi-step rollouts, allowing for generalization to unseen transitions with only one dynamic model. The synthesized model acts as a generator in the WGAN framework, with a critic distinguishing between real and fake transitions. The generator and critic are updated alternately until the synthesized data cannot be distinguished. Our method, model imitation (MI), enforces the learned transition model to generate similar rollouts to the real data, achieving T \u2192 T theoretically. We show that MI is more sample efficient than existing MBRL and MFRL methods, outperforming them on standard tasks. Additionally, we stabilize model learning by guaranteeing our sampling technique and exploring training across WGANs. Motivation from learning from demonstration (LfD) and a survey of model-based reinforcement learning (MBRL) methods. Behavior cloning (BC) simplifies LfD to supervised learning but struggles without enough demonstration. Generative Adversarial Imitation Learning (GAIL) matches distributions of agent-generated trajectories with demonstrations to learn an optimal policy efficiently. GAIL's advantage is its minimal requirement for learning. Transition learning (TL) is akin to learning from demonstration (LfD) but with the roles of transition and policy exchanged. In TL, trajectories from a fixed policy are given to imitate the underlying transition. Studying the counterpart of GAIL in TL involves learning the transition through distribution matching, maintaining GAIL's benefits in MBRL. Transition learning (TL) involves learning the transition through distribution matching, maintaining GAIL's benefits in model-based reinforcement learning (MBRL). GAIL learns a better policy than behavioral cloning (BC), suggesting that distribution matching can lead to improved transitions. Various approaches, such as using supervised learning with mean-squared error and ensembles of multiple transition models, have been explored to address model bias. Additionally, meta-learning is utilized to gather information from multiple models, and SLBO is a theoretical algorithm that has evolved from these concepts. SLBO is a theoretical algorithm that develops from solid theoretical properties for model-based deep RL via a joint model-policy optimization framework. It introduces Gaussian process and Gaussian process with model predictive control as uncertainty-aware versions of MBRL. An ensemble method for probabilistic networks is also studied to mitigate model bias and foster stability in training deterministic or stochastic transitions. The stochastic model in model-based deep RL can induce instability if the true transition is deterministic, leading to the adoption of an ensemble of models to reduce variance. The Markov Decision Process (MDP) is represented by state space S, action space A, transition density T, reward function r, and discount factor \u03b3. The performance is evaluated based on the expected cumulative rewards in a length-H trajectory generated by (\u03b1, \u03c0, T) in the \u03b3-discounted infinite horizon setting. Reinforcement learning algorithms aim to find a policy to maximize rewards in a trajectory. The normalized occupancy measure \u03c1 represents the distribution of states and actions in the trajectory. The relation between \u03c1 and the model is characterized by the Bellman flow constraint.\u03c0(a|s) and \u03c1 have a one-to-one correspondence with the initial state distribution and transition function fixed. The occupancy measure \u03c1 and policy \u03c0(a|s) have a one-to-one correspondence with the initial state distribution and transition function fixed. The goal of maximizing cumulative reward can be achieved by adjusting \u03c1, \u03b1, and T, motivating the use of distribution matching approaches like WGAN. Consistency results and error bounds for WGAN are presented in this section, with the training objective focused on finding the optimal value for the inner maximization through Kantorovich-Rubinstein duality. The support constraint is crucial in training WGAN for consistent estimation of transitions. Empirical evidence suggests initial performance boosts indicate sufficient support. However, the consistency result is limited to the optimal case, prompting analysis of non-optimal scenarios. Theorem 1 shows that proper training with small errors leads to accurate cumulative reward estimation. Theorem 1 highlights that proper training of WGAN with small errors results in accurate estimation of cumulative rewards on synthesized trajectories. This contrasts with traditional RL literature where error bounds typically have a quadratic dependency on trajectory length. WGAN's improved reward estimation leads to more accurate policy updates. Additionally, a practical MBRL method called model imitation (MI) is introduced in this section to incorporate transition learning. To address the challenge of training WGAN directly from long synthesized trajectories, a sampling technique using synthesized transitions is employed to generate shorter trajectories. By setting a discount factor \u03b2 < \u03b3 for the short trajectories, the expected length is bounded by the training error of WGAN on these easier-to-imitate trajectories. The goal is to optimize \u03b2 to balance between encouraging a large value and minimizing errors. Sampling a sufficient number of short trajectories can reduce errors in practice. The occupancy measure \u03c1 \u03b2 T is crucial for training, as shown in the proof of Theorem 1. To improve training efficiency, a transition learning scheme aligns the distribution of (s, a, s) between real and learned environments using rewards from a WGAN critic. This approach mirrors GAIL but focuses on training a synthesized transition rather than a policy. In transition learning, the objective is to train a synthesized transition to maximize cumulative pseudo rewards. Multiple WGANs are trained due to policy updates, but switching between them can lead to getting stuck at local optima. Unlike GANs, WGANs have unbounded inner maximization, making the critic too strong and hindering the generator. Modifying the WGAN objective is necessary to address this issue. To address the issue of unbounded inner maximization in WGANs, a modified WGAN objective with cut-offs is introduced. This truncated version of WGAN, equivalent to a hinge loss version, aims to maximize margin for robustness. Transition learning focuses on maximizing cumulative pseudo rewards and does not directly optimize the modified WGAN objective. The modified WGAN objective introduces cut-offs to address unbounded inner maximization, focusing on maximizing margin for robustness. Transition learning aims to optimize cumulative pseudo rewards without directly optimizing the WGAN objective. The synthesized transition is modeled by a Gaussian distribution, even for deterministic tasks like MuJoCo, without harming empirical transition learning. The transition learning process utilizes PPO and maximum likelihood optimization to improve stability and enhance the transition model for policy gradient. The overall loss function includes the loss of MLE to achieve better transition learning results. The proposed model imitation method, summarized in Algorithm 1, aims to improve sample complexity and average return compared to state-of-the-art methods. It benefits from distribution matching and is superior to model-free approaches. The training procedure is similar to SLBO, with multiple update pairs for each real environment sampling. The experiment section will address the performance of the proposed model imitation. The proposed model imitation method aims to improve sample complexity and average return compared to state-of-the-art methods. It benefits from distribution matching and is superior to model-free approaches. The evaluation is conducted on four continuous control tasks using open-sourced environments released along with a model-based benchmark paper. The detailed performance comparison can be found in Table 1 of the benchmark paper. The evaluation compares the proposed model imitation method to model-free and model-based algorithms like TRPO, PPO, SLBO, PETS, METRPO, and STEVE. These methods aim to improve deep reinforcement learning by addressing uncertainty, data instability, and model rollouts. The evaluation compares the proposed model imitation method to model-free and model-based algorithms like TRPO, PPO, SLBO, PETS, METRPO, and STEVE. Among model rollouts of various horizon lengths, the model imitation method favors those with lower error estimates. Figure 2 displays learning curves for all methods, showing that the model imitation method converges quickly and outperforms competitors in Hopper, HalfCheetah, and Ant environments. Even though the model imitation method does not significantly improve performance in Ant, it maintains an average return of around 1,000 with only 5,000 transition data, indicating better transition capture. The method's performance also demonstrates lower variance compared to ensemble methods like METRPO and PETS. TRPO with model imitation (MI) requires fewer interactions with the real world to learn a good policy. MI, based on SLBO, generates similar rollouts to real environments, leading to superior performance. Comparing MI with benchmarked RL algorithms, it significantly outperforms most MBRL and MFRL methods. MI significantly outperforms MBRL and MFRL methods with fewer samples by incorporating distribution matching. The proposed approach incorporates WGAN for occupancy measure matching between real transitions and synthesized models, ensuring accuracy in estimating long rollouts. Truncated WGAN is suggested for stable training and to prevent local optima. The empirical property of WGAN application, such as imitation learning, shows potential to learn transitions with fewer samples than supervised learning. MI converges faster and achieves better policy than state-of-the-art model-based and model-free algorithms. Proof for WGAN consistency is provided, showing that at its optimal point, the WGAN loss function equals the 1-Wasserstein distance. To enhance state exploration, real transitions are sampled according to a Gaussian parameterized policy. Matching \u03c1 \u03b1,\u03c0 \u03b8 T with \u03c1 \u03b1,\u03b2 T is found to be more stable and sample-efficient. Using the mean \u00b5 \u03c6 of the Gaussian-parameterized transition accelerates policy optimization. Gradient penalty is employed to enforce the Lipschitz constraint on the WGAN critic f. To enforce the Lipschitz constraint on the WGAN critic f, gradient penalty is employed."
}