{
    "title": "S1EwLkW0W",
    "content": "The ADAM optimizer is popular in deep learning but sometimes fails. It combines gradient sign and variance for updates. Analyzing these aspects sheds light on its workings. A new method, transferring variance adaptation to momentum-SGD, complements ADAM. Many machine learning models involve empirical risk minimization problems. The ADAM optimizer is widely used in deep learning, along with its variants like RMSPROP and ADADELTA. These methods compute stochastic gradients for efficient optimization. ADAM, RMSPROP, and ADADELTA are popular optimizers in deep learning. ADAM updates parameters using bias-corrected exponential moving averages of gradients. It ensures numerical stability with a small constant \u03b5. The text discusses two alternative methods for optimizing stochastic gradients: \"Stochastic Sign Descent\" (SSD) and \"Stochastic Variance-Adapted Gradient\" (SVAG). SSD involves taking the sign of the stochastic gradient, while SVAG applies element-wise variance adaptation factors directly on the gradient. Section 2 explores the sign aspect, while Section 3 focuses on variance adaptation. In the context of optimizing stochastic gradients, the text discusses the derivation of optimal element-wise variance adaptation factors for a stochastic gradient and its sign. It also mentions the incorporation of momentum and practical estimation of stochastic gradient variance. Experimental results are presented in Section 4. The idea of using the sign of the gradient for optimizer updates has been explored in previous literature, such as the RPROP algorithm and research on reducing communication costs in distributed training of neural networks. Schaul et al. (2013) have derived element-wise step sizes for stochastic gradient descent that depend on stochastic factors. The text discusses the importance of the sign of a stochastic gradient in estimating the true gradient. It explores the distribution of the stochastic gradient and provides insight on when the element-wise sign of a stochastic gradient is a better update direction. The focus is on optimizing stochastic gradients and incorporating momentum for practical estimation. The text focuses on investigating the effects of curvature properties and their interaction with stochastic noise in stochastic quadratic problems. It compares update directions in terms of expected decrease in function value from a single update step, considering optimal step sizes for improvement. The text discusses the improvement of SGD and SSD in stochastic quadratic problems by analyzing the eigenvalues of Q and their orientation. It highlights the dependency of I(s) on |q_ij| and the challenges with arbitrarily oriented eigenbases. The concept of \"diagonal dominance\" is introduced to explain the difficulties with sign updates in such cases. The text discusses the concept of \"diagonal dominance\" in optimization problems arising from neural networks. It mentions empirical findings of high percentages of mass on the diagonals and the obstructive nature of constant offsets in the denominator. Additionally, it contrasts the effects of noise in I(g) and I(s), highlighting the bounded effect of noise in the latter. The text discusses learning problems with ill-conditioned Hessians and the potential benefits of sign updates for noisy problems. It presents empirical evidence on the performance of SGD and SSD on stochastic quadratic problems with different noise levels and eigenspectrums. The text discusses controlling the eigenspectrum of matrices for learning problems. Different matrices are generated with varying noise levels and compared using SGD and SSD algorithms. Gradient descent outperforms the sign-based method on well-conditioned, noise-free problems. Adding noise can help even out the performance difference between gradient descent and the sign-based method. The orientation of the eigenbasis has little effect on SSD performance in well-conditioned cases, but axis-aligned eigenbasis greatly benefits SSD over SGD. The optimal variance adaptation factors for the sign of a stochastic gradient are found to be \u03b3 i = 2\u03c1 i \u2212 1. ADAM is an approximate realization of the optimal variance adaptation scheme for stochastic gradient descent. Variants with small differences were found to have identical effects, with the choice of (1 + \u03b7) for efficiency. The optimal variance adaptation factors for SGD are \u03b3 i = 2\u03c1 i \u2212 1. The method discussed involves curvature estimates in element-wise step sizes for SGD, referred to as \"Stochastic Variance-Adapted Gradient\" (SVAG). A momentum variant is derived, guaranteeing convergence without manually decreasing the global step size. The O(1/t) rate of SGD for smooth, strongly convex functions is achieved. Theoretical results are based on an idealized version of SVAG with exact parameters, motivating variance adaptation. In practice, estimating the relative variance is necessary, as seen in the introduction with ADAM obtaining estimates from moving averages. ADAM estimates stochastic gradient variance from moving averages, assuming function stability over time. Unlike ADAM, SVAG does not use different moving average constants for gradient and its square. Individual constants for mt and vt have minor impact on performance. Alternative variance estimate can be computed locally. An alternative variance estimate can be computed locally within a single mini-batch. Experimentation with different estimators showed equal performance, leading to the use of moving average-based estimates. Variance adaptation factors for momentum updates should be determined by the relative variance of the direction. An approximation that avoids additional memory usage is proposed for practicality. An approximation is proposed that avoids additional memory requirements for practicality. It compares momentum-SGD (M-SGD) and ADAM to two new methods: M-SSD and M-SVAG, which involve stochastic sign descent with a momentum term and SGD with momentum and variance adaptation, respectively. These methods are based on different recombinations of the sign aspect and variance adaptation aspect of ADAM. The text discusses a method for variance adaptation in neural networks, testing it on various datasets including MNIST, CIFAR-10, and CIFAR-100. The method does not use an \u03b5-parameter like ADAM and handles cases where division by zero occurs. Different network architectures are used for the datasets, with varying numbers of convolutional layers. The training process involves a fixed decreasing schedule for the global step size. The training process for MNIST, CIFAR-10, and CIFAR-100 involves using a constant global step size for MNIST and a fixed decreasing schedule for CIFAR-100. Different batch sizes and step sizes were tuned individually for each method. Results show that ADAM outperforms M-SGD on MNIST, while there is little difference between M-SSD and ADAM. The experiments were replicated ten times with different random seeds. On CIFAR-10, sign-based methods like M-SSD and ADAM show superior performance over M-SGD and M-SVAG. However, on CIFAR-100, M-SGD outperforms ADAM with lower loss values and faster convergence. M-SGD also achieves higher test accuracies compared to ADAM. Variance adaptation has a positive effect for sign-based methods and M-SGD on this problem. The transition from M-SGD to M-SVAG results in initial speed gains, later balanced out by manual learning rate adjustments. ADAM combines sign-taking and variance adaptation, with separate analysis shedding light on its workings. Sign-based methods are effective on some problems, dependent on stochasticity and problem conditioning. Variance adaptation benefits all cases but may have minimal impact. M-SVAG, a variant of momentum-SGD with variance adaptation, proves useful in experiments. M-SVAG, a variant of momentum-SGD, is a useful addition for problems where ADAM fails. It has two hyper-parameters, momentum constant \u00b5 and global step size \u03b1. Implementation in TensorFlow will be available. Training includes a fully-connected neural network on MNIST and a CNN on CIFAR-10 dataset. The CNN model for CIFAR-10 dataset includes three convolutional layers with different filter sizes, max-pooling, and fully-connected layers with ReLU activation. Data augmentation techniques are applied during training with a batch size of 256. The AllCNN architecture is used with seven convolutional layers and no pooling layers, replacing fully-connected layers with 1x1 convolutions for global spatial averaging. The AllCNN architecture for CIFAR-10 dataset replaces fully-connected layers with 1x1 convolutions for global spatial averaging. ReLU activation is used in all layers, with cross-entropy loss function and L2 regularization on weights. Data augmentation and a batch size of 256 are employed. The global learning rate decreases after specific steps, and learning rates for optimizers are tuned. Experiment results are evaluated on the test set to select the best-performing learning rate. Using the best learning rate, the experiment was replicated ten times with different random seeds. Expressions for I(s) and I(g) were derived in Eq. (12), dropping the fixed \u03b8 for readability. For SGD, DISPLAYFORM0 is a general fact for quadratic forms of random variables. The gradient covariance for stochastic QP is DISPLAYFORM1 i. Stochastic sign descent has E[s i ] = (2\u03c1 i \u2212 1) sign(\u2207L i ). Probability density functions of Gaussian distributions with different variances were shown in Figure 4. The probability of a sample having the opposite sign than its mean is uniquely determined by the fraction \u03c3/|\u00b5|, as shown in Lemma 2. Lemma 2 proves the success probabilities of the sign of a stochastic gradient under the assumption that the gradient follows a normal distribution. The optimal variance adaptation factor for stochastic gradient descent is discussed, with Figure 5 illustrating the variance adaptation factors. The Gaussian assumption is closely approximated by DISPLAYFORM7, the factor used by ADAM. Proof of Lemma 1 shows the optimal choice by setting derivatives to zero. Figure 5 displays variance adaptation factors. The convergence results for variance-adapted stochastic gradient descent are proven. Stochastic optimizers generate a stochastic process {\u03b8 t} t\u2208N0. Proof of Theorem 1 bounds \u2207f using Lipschitz continuity. Jensen's inequality is used to bound the gradient \u2207f in the context of variance-adapted stochastic gradient descent. The proof involves simplifying expressions and applying the inequality to show convergence results. In the context of variance-adapted stochastic gradient descent, the convex function \u03c6(x) = 1/x is applied with coefficients g and t. The assumption is made that all gradients in the effective time horizon of the moving average have the same mean and variance. The gradient variance is estimated from the mini-batch to obtain an unbiased estimate of var[r t]. Different variance-adapted methods are examined, including a variation of M-SVAG and a variant of ADAM that applies a correction factor to the estimate of the relative variance of the momentum term. In the context of variance-adapted stochastic gradient descent, a variant of ADAM called ADAM* is introduced with two variance estimates. The method involves updating moving averages and stochastic gradients. Variants of ADAM* are evaluated on CIFAR test problems, showing comparisons with the original ADAM and mini-batch variant of M-SVAG."
}