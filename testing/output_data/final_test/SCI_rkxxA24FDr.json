{
    "title": "rkxxA24FDr",
    "content": "Neural networks with external memory can learn algorithms and complex tasks by storing data for a neural controller. The Neural Stored-program Memory model enhances memory-augmented neural networks, allowing for program switching, adaptation to contexts, and resembling the Universal Turing Machine. Experimental results show excellence in algorithmic problems, potential for various learning tasks, and overcoming limitations of Recurrent Neural Networks. Memory Augmented Neural Networks (MANNs) improve sequential learning tasks by separating memorization from computation. However, current MANNs lack stored-program memory, a key concept in computer design inspired by the Universal Turing Machine and Von Neumann Architecture. Memory Augmented Neural Networks (MANNs) like Neural Turing Machine (NTM) and Differentiable Neural Computer (DNC) lack stored-program memory, hindering flexibility in computations. To address this, we aim to enhance MANNs by incorporating external program memory, enabling quicker retrieval of controller network weights for improved task complexity handling. Our model, Neural Stored-program Memory (NSM), utilizes a meta network to control program memory operations, allowing for adaptive switching of programs/weights in the controller network. Tested as Neural Universal Turing Machine (NUTM) on various tasks, including algorithmic and few-shot learning, NUTM shows clear improvements over NTM. Additionally, NUTM equipped with LRUA as the MANN achieves notably better results in few-shot learning and linguistic problems. Our study advances neural network simulation of Turing Machines to neural architecture for Universal Turing Machines by equipping NUTM with DNC core for competitive performances in linguistic problems. This new class of MANNs can store and query both weights and data, demonstrating computational universality through diverse experiments. The interface network and state network 1 execute functions at each timestep. The interface network, typically a Feedforward neural network, takes c t as input and is weighted by W c. The state update and memory control involve data from current input x t and previous memory read r t\u22121. The interface vector \u03be t is used for memory operations, allowing for multiple memory accesses per step. This setup supports a deterministic one-tape Turing Machine defined by a 4-tuple. Refer to additional resources for memory read/write examples. A deterministic one-tape Turing Machine is defined by a 4-tuple (Q, \u0393, \u03b4, q 0 ), where Q is a finite set of states, \u0393 is a finite set of symbols on the tape, and \u03b4 is the transition function. The machine performs actions like writing new values, moving the tape head, and changing states based on inputs. This setup can be related to current MANNs, with Q, \u0393, and \u03b4 corresponding to controller states, read values, and the controller network. The function \u03b4 can be split into two sub-functions for interface and state networks. By encoding a Turing Machine into the tape, a UTM can simulate the machine. A Neural Stored-program Memory (NSM) is a key-value memory used to store the controller network for building a Neural Universal Turing Machine. It consists of a memory M p with basis weights of programs, where P, K, and S represent the number of programs, key space dimension, and program size respectively. This concept allows for simulating any Turing Machine by encoding only its transition function \u03b4. A Neural Stored-program Memory (NSM) utilizes key-value memory to store the controller network for a Neural Universal Turing Machine. It involves a memory M p with basis weights of programs, where P, K, and S represent the number of programs, key space dimension, and program size. The keys and values in NSM are updated gradually by backpropagation, with values dynamically interpolated to produce the working weight on-the-fly during sequence processing. The key-value design is crucial for convenient memory access, especially when the program size stored in M p can be millions of dimensions. The Neural Stored-program Memory (NSM) uses key-value memory to store the controller network for a Neural Universal Turing Machine. The program stored in memory can be millions of dimensions, making direct content-based addressing infeasible. External control can be imposed on memory behavior by constraining the key space to prevent program collapse. The combination of Memory Augmented Neural Networks (MANN) and NSM approximates a Universal Turing Machine. The controller in MANN reads its state and memory to generate control signals via the interface network Wc and updates its state using the state network RNN. In this paper, the Neural Stored-program Memory (NSM) is used to store the controller network for a Neural Universal Turing Machine. The program stored in memory can be millions of dimensions, making direct content-based addressing infeasible. The meta-network PI generates an interface vector for the program memory, allowing for dynamic computation of Wc t from NSM for every timestep. This approach enables the memory keys to be learnable, providing flexibility in adapting to changes in memory content. The Neural Universal Turing Machine (NUTM) implements one Neural Stored-program Memory (NSM) per control head to ensure functionality separation amongst heads. Each control head reads from or writes to the data memory M via memory (\u03be t , M). Using multiple heads increases the number of accesses to the data memory at each timestep, improving capacity but not adaptability like using multiple controllers per head would. The Neural Universal Turing Machine (NUTM) utilizes Neural Stored-program Memory (NSM) to vary memory access across timesteps by switching controllers. Regularization loss is employed to prevent program collapse, with a final loss incorporating prediction loss and an annealing factor. NUTM operations are detailed in Algorithm 1, treating memory access as a multi-dimensional regression problem. The aim is to generate a correct interface vector \u03be t from input c t via optimizing the interface network, utilizing NSM to partition the space of c t into subspaces for multiple transformations. The Neural Universal Turing Machine (NUTM) utilizes Neural Stored-program Memory (NSM) to vary memory access across timesteps by switching controllers. The program interface network routes input to appropriate transformations, mapping it to a specific space. This approach is similar to multilevel regression in statistics and has shown practical advantages over ordinary regression. RNNs can learn finite state computations, enhancing the model's capacity. The NUTM uses NSM to improve memory access during computations. It clusters states to reflect a Turing Machine's states. NUTM's performance is tested on algorithmic tasks, doubling training sequence length in the Copy task. Two models, NTM 2 and NUTM, are compared in the experiment. In the experiment, two models, NTM 2 and NUTM, are compared using cross-entropy objective function. NUTM shows faster convergence speed in tasks compared to NTM, validating the benefit of using two memory manipulation schemes. NUTM shows faster convergence speed and better generalization compared to NTM, requiring fewer training samples. Test results confirm NUTM's outperformance for generalization. Program distributions for Repeat Copy and Priority Sort tasks are illustrated in Fig. 3, showing different program usage patterns for encoding and decoding phases. In this section, an ablation study on Associative Recall (AR) is conducted to validate the proposed components of NSM. The study includes three additional baselines: NUTM using direct attention (DA), NUTM using key-value without regularization (KV), NUTM using fixed, uniform program distribution (UP), and a vanilla NTM with 2 memory heads. The meta-network P I in DA generates attention weight directly, while KV employs key-value attention without regularization loss. Training curves over 5 runs are plotted in Fig. 2 (b). The results show that NUTM with key-value attention converges faster and more completely than DA, UP, and NTM with 2 heads. Using multiple heads in NTM does not outperform NUTM with 1 head. The study also discusses the impact of sequencing tasks on brain function. In a new set of experiments, tasks are generated by sequencing subtasks in NTM. Different combinations of subtasks like Copy, Repeat Copy, Associative Recall, and Priority Sort are tested. NTM struggles with tasks like Copy and Associative Recall when sequenced together due to memory access issues. NTM struggles with memory access issues when tasks like Copy and Associative Recall are sequenced together. NSM can help NTM mitigate catastrophic forgetting by adapting program distribution per subtask. An experiment similar to Split MNIST is designed to test NSM's impact on NTM's performance. In the experiment, NUTM outperforms NTM by 10-40% per task, showing some memory preservation of previous tasks. NUTM excels in tasks like Copy even after learning Repeat Copy, but struggles with dissimilar task transitions, requiring more effort for continual learning. Few-shot learning tests the ability to adapt rapidly within a task while capturing variations in task structure. In the experiment, NUTM outperforms NTM by 10-40% per task, showing memory preservation. NUTM excels in tasks like Copy after learning Repeat Copy but struggles with dissimilar task transitions. MANNs can classify new data with few samples and NSM enhances adaptability to changes, improving performance. NSM is applied to LRUA memory using the Omniglot dataset for few-shot classification accuracy testing. After 100,000 episodes of training, models are tested with unseen images from the testing set. NUTM outperforms NTM in tasks, showing memory preservation. Table 2 shows test-set classification accuracy on the Omniglot dataset. NUTM with different parameters achieves varying accuracies. More details on learning curves and results can be found in App. D. In the question answering domain, NUTM with DNC core outperforms NTM in tasks, showing memory preservation. Using the bAbI dataset, NUTM with DNC core (variants p = 1, p = 2, and p = 4) reads stories word by word to predict answers, testing natural language reasoning skills. Increasing the number of programs improves NUTM performance. Increasing the number of programs helps NUTM improve performance, achieving a mean test error rate of 3.3% and solving 19/20 tasks. This result is the best reported mean result on bAbI. Previous investigations into MANNs focus on memory access mechanisms, including content-based, location-based, and dynamic memory reading/writing. The current work discusses modifications to Memory Augmented Neural Networks (MANNs) with extra memory proposed by Le et al. in 2018. Other related works include DNC modifications by Csordas & Schmidhuber and Franke et al., as well as module networks for visual question answering. The NSM model aims to help MANNs achieve general-purpose computability through dynamic weight generation and soft-attention over programs. The NSM model utilizes dynamic weight generation to enable fast adaptation in Neural Networks. It differs from other methods like Tensor/Multiplicative RNN and Hypernetwork by interpolating slow weights instead of directly generating fast weights. These approaches aim to make RNNs adaptable over time but lack modularity compared to the NSM model. The NSM model differs from approaches like Tensor/Multiplicative RNN by interpolating slow weights instead of directly generating fast weights. Unlike these methods, the NSM model uses a meta-network to generate coefficients for modularity, allowing for the switching or combination of programs to perform tasks efficiently. The goal is to achieve a neural simulation of Universal Turing Machine, focusing on modularity and functionality rather than just improving RNN with fast-weight. The paper introduces the Neural Stored-program Memory (NSM), an external memory for neural networks inspired by computer architecture. NSM allows memory-augmented neural networks to change control programs while remaining differentiable, simulating modern computer behavior. Experiments show improved algorithm learning, faster adaptation to new tasks, and state-of-the-art performance in tasks like bAbI when coupled with NSM. The paper introduces the Neural Stored-program Memory (NSM), an external memory for neural networks inspired by computer architecture. NSM allows memory-augmented neural networks to change control programs while remaining differentiable, simulating modern computer behavior. The proposed model shows significant improvement in the bAbI task and future works will explore benefits for other neural networks. The model uses RMSprop optimizer with fixed learning rate and momentum, batch size of 32, layer normalization, and removes temporal linkage for faster training. Hyper-parameters details are listed in Table 12. Table 12 lists hyper-parameters details. Full NUTM results are reported in Table 13 for bAbI task. When p = 1, model converges to layer-normed DNC. Ablation study's learning losses are plotted in Fig. 23."
}