{
    "title": "rkevMnRqYQ",
    "content": "Reinforcement learning agents optimize features specified in a reward function and are indifferent to what is left out. When deployed in human environments, robots can use implicit preference information to make decisions. An algorithm based on Maximum Causal Entropy IRL is used to evaluate this concept in various environments. The initial state can be used to infer side effects to avoid and preferences for organizing the environment. Deep reinforcement learning (deep RL) has shown success in complex tasks with a specified reward function. However, specifying human preferences, like avoiding side effects, can be challenging. Recent work aims to learn task specifications from human actions, such as specified rewards, task performance, choices made, and option ratings. The key insight of this paper is that when a robot is deployed in an environment that humans have been acting in, the state of the environment is already optimized for what humans want. For example, consider an environment in which a household robot must navigate to a goal location without breaking any vases in its path. The robot can infer human preferences by observing the state of the environment prior to its deployment. The robot learns human preferences by observing the environment and inferring that keeping vases intact is important based on Alice's actions. The initial state contains information about tasks the robot should perform, such as seeing a basket of apples near an apple tree. The robot can infer human preferences by observing the environment and avoiding negative side effects while allowing positive ones. This approach highlights the importance of the initial state in optimizing for human preferences. Unrealistic assumptions are made, such as known dynamics and hand-coded features, but are necessary without dynamics to determine the origin of features in the initial state. Our contributions are threefold: identifying the state of the world at initialization as a source of information about human preferences, deriving an algorithm called Reward Learning by Simulating the Past (RLSP) to infer rewards from the initial state, and demonstrating the properties and limitations of RLSP in various environments. The robot in FIG5 moves to the purple door without breaking the vase, showcasing the ability to learn implicit preferences and avoid side effects. The text discusses preference learning from various data sources, including the state of the environment at robot deployment. It introduces a new approach called Reward Learning by Simulating the Past (RLSP) to infer rewards from the initial state, showcasing the ability to learn implicit preferences and avoid side effects. The text discusses the frame problem in AI and the need to specify what stays the same in addition to what changes. It also mentions the importance of inferring frame properties automatically and using impact penalties to mitigate reward specification problems. In contrast to penalizing all irreversible effects, a preference inference approach can determine which effects humans value. Desired behavior in RL is specified with a goal state in a finite-horizon Markov decision process (MDP) where rewards are linear in features and independent of actions. Inverse Reinforcement Learning (IRL) aims to infer the reward function in a Markov decision process (MDP) from expert demonstrations. Maximum Causal Entropy IRL models experts as Boltzmann-rational agents maximizing total reward and causal entropy of the policy. The expert policy is assumed to act close to randomly when the expected total reward across actions is similar. The soft Bellman backup for the state-action value function Q is the same as usual. MCEIRL finds the reward parameters \u03b8 * that maximize the log-likelihood of the demonstrations. The algorithm presented in Appendix C for sampling from p(\u03b8 | s 0 ) is noisy and slow, so instead, the MLE is found. Similarly to MCEIRL, a gradient ascent algorithm is used to solve the IRL from one state problem. The algorithm RLSP computes the gradient using dynamic programming to find the exact gradient for each trajectory. It combines gradients from past trajectories to maximize the log-likelihood of demonstrations. The algorithm can easily incorporate a prior on \u03b8 by adding the gradient of the log prior to the gradient. RLSP is used to infer a reward \u03b8 Alice from s 0, which is then combined with the specified reward to get a final reward \u03b8 final = \u03b8 Alice + \u03bb\u03b8 spec. The goal is to correct badly specified instructions or reward functions in a suite of environments with true and specified rewards. The inferred reward is qualitatively inspected and the expected true reward obtained when planning with \u03b8 final is measured as a fraction of the expected true reward from the optimal policy. The hyperparameter \u03bb is tuned to control the tradeoff between R spec and the human reward for all algorithms, including baselines. Different reward policies are considered, such as a specified reward policy, a policy that penalizes deviations, and a relative reachability policy that penalizes deviations from observed features. RLSP is compared against baselines using known dynamics, with results summarized in TAB1 and environments shown in Figure 2. The evaluation includes initial and final positions of objects or agents after actions are taken. The evaluation of RLSP compared to baselines using known dynamics is summarized in TAB1 and environments shown in Figure 2. The trajectories taken by the robot are shown when following different policies, indicating side effects, distinguishing environment effects, implicit rewards, and desirable side effects. The room with a vase tests the robot's ability to avoid breaking the vase as a side effect of going to the purple door. RLSP infers negative rewards for broken vases and positive rewards for standing on carpets. RLSP successfully avoids breaking the vase by inferring negative rewards for broken vases and positive rewards for standing on carpets. The penalties in the environment also help achieve the desired behavior by avoiding vase breakage. The addition of a toy train tests the algorithm's ability to distinguish between agent and environment effects, with penalties for breaking the train and vases. The environment tests whether the algorithms can learn tasks implicit in s 0, with three trees growing apples and a basket for collecting them. The robot must infer the task from the observed state as the specified reward is zero. The robot infers tasks from observed states based on features like the number of apples in baskets, on trees, and if carrying an apple. Different policies yield varying rewards, with RLSP successfully harvesting apples. A side effect of batteries is introduced to test algorithm adaptability. The robot can pick up batteries and put them into the toy train, which stops operating if it runs for 10 timesteps without a new battery. The task reward incentivizes an operational train and being at the purple door. Two variants are considered - an \"easy\" case where the task reward equals the true reward, and a \"hard\" case where the task reward only rewards being at the purple door. Different policies succeed or fail based on these variants. RLSP solves both the easy and hard cases by inferring Alice's preferences. It picks up the battery and delivers it to the train, staying at the purple door. A limitation is shown in a room with a faraway vase, where the algorithm cannot identify side effects that Alice would not trigger. Our method infers a near zero weight on the broken vase feature, as it is not present on any reasonable trajectory to the goal. When using a uniform prior, RLSP learns a smaller negative reward on broken vases in rooms with vases and toy trains. In rooms with vases, RLSP overfits to consistent trajectories over carpets, leading to a near-zero negative reward. In the toy train scenario, the negative reward on broken trains becomes more negative. A uniform prior results in a smaller positive weight on the number of apples in baskets compared to known scenarios. The study considers cases where the basket starts with one or two apples, indicating Alice's lower interest in collecting apples. States with three or more apples are not considered. Following the inferred reward leads to good apple harvesting behavior. In the toy train scenario, a uniform prior results in a smaller positive weight on the number of apples in baskets compared to known scenarios. RLSP successfully delivers the battery to the train in both easy and hard cases, with an \"unseen\" side effect of avoiding a vase. Uncertainty over the initial state can improve results by increasing trajectory diversity. RLSP is robust to using a uniform prior, suggesting careful design is not necessary. Investigating RLSP's performance with the wrong planning horizon T is also considered. When RLSP assumes the wrong planning horizon for Alice, its performance worsens. Assuming a smaller horizon leads to uncertainty in inferring Alice's reward, while assuming a larger horizon helps the robot avoid incorrect actions. The results are shown in FIG1. The planning horizon parameter T is crucial for RLSP performance. Misestimating T can lead to degraded performance in inferring implicit preferences, as seen in the Apples environment. In the Batteries environment, if Alice waits until the 98th timestep to act, it is inconsistent with any reward function and degrades performance. However, even with misspecified T, RLSP performance gracefully degrades without significant harm. Our key insight is that when a robot is deployed, the state it observes is optimized to satisfy human preferences. We developed RLSP, an algorithm that computes a MAP estimate of Alice's reward function, allowing the robot to act based on a tradeoff between Alice's reward function and the specified reward function. The evaluation showed successful inference of side effects to avoid and tasks to complete from the initial state, but there are still challenges in inferring relevant preferences. The development of RLSP algorithm allows robots to act based on a tradeoff between human preferences and specified reward function. Future work includes scaling to realistic environments by adapting existing IRL algorithms and addressing challenges of unknown dynamics. Hyperparameter choice is crucial for improving model performance. RLSP is robust to hyperparameter choice but may face challenges in real-world scenarios. Conflicts may arise between human preferences and specified reward functions, requiring a tradeoff in decision-making. The text discusses optimizing the sum of human preferences and specified reward functions, suggesting decomposing tasks into subgoals to avoid conflicts. It proposes using inferred rewards to inform humans of conflicts and actively seeking more information. The text discusses RLSP learning tasks in environments like apples and batteries, highlighting the potential issue of robots inferring tasks instead of following explicit instructions. It raises concerns about preferences not solely based on human optimization and the need for more advanced AI systems. The paper suggests improving the existing trajectory approximation for better gradient calculation in seeking expert guidance. The text discusses the gradient calculation in seeking expert guidance for RLSP learning tasks. It involves computing the gradient of V t (s) and expected feature counts under the policy implied by \u03b8 from s t onwards. The text discusses gradient calculation for expert guidance in RLSP learning tasks, focusing on computing the gradient of V t (s) and expected feature counts under the policy implied by \u03b8 from s t onwards. The gradient is compared with Ziebart et al. (2010), where the expert policy feature expectations minus the learned policy feature expectations are used, approximating with feature expectations from demonstrations. The third term in the gradient calculation for expert guidance in RLSP learning tasks corrects for bias in observed states, downweighting the future value of the observed state and upweighting the particular next state that was observed. The third term downweights the future value of the observed state and upweights the future value of all other states based on their prior probability. This gradient derivation is necessary for solving argmax with gradient ascent, where the gradient is computed for each trajectory and weighted by the probability of the trajectory given the evidence and current reward. The gradient derivation is crucial for solving argmax with gradient ascent, computed for each trajectory and weighted by the probability of the trajectory given the evidence and current reward. To address computational challenges, MCMC sampling can be used to sample from the posterior distribution of \u03b8. The algorithm presented in Algorithm 1 is similar to Bayesian IRL and involves sampling from the posterior distribution of \u03b8. Despite being less efficient and noisier than RLSP, it provides an estimate of the full posterior distribution. In experiments, the full distribution was collapsed into a point estimate by taking the mean. Future work could leverage the full distribution to create risk-averse policies or identify uncertain features."
}