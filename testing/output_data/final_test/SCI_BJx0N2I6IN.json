{
    "title": "BJx0N2I6IN",
    "content": "In this paper, a reproduction of Bertinetto et al.'s paper \"Meta-learning with differentiable closed-form solvers\" for the ICLR 2019 Reproducibility Challenge is presented. The reproduced results show comparable or superior performance on two benchmarks for various settings. New baseline results are evaluated using a new dataset. Recommendations on reproducibility and comparability are provided. The authors updated their original paper and released code after being informed of the reproducibility work. The main contributions include reproducing key results, offering insights on reproducibility, and providing an open-source implementation. Few-shot learning is an area of machine learning that focuses on adapting to new tasks with minimal data. While artificial learning methods can outperform humans in specific tasks, they still require significant training data and time to adapt. Few-shot learning expands the learning scope to multiple tasks with only a few examples each, compared to the traditional single-task approach. Few-shot learning involves adapting to new tasks with minimal data by focusing on multiple tasks with only a few examples each. Meta-learning, or learning-to-learn, utilizes cross-task information to perform well on unseen tasks. The paper discussed in the curr_chunk presents a new meta-learning method that combines a deep neural network feature extractor with differentiable learning algorithms, reducing complexity and advancing accuracy in few-shot benchmarks. The authors updated their paper with more details and released a PyTorch implementation for few-shot meta-learning, focusing on classification tasks. Meta-learning involves training a model to quickly adapt to new tasks with minimal data. The set of tasks is denoted as T, with each task corresponding to a classification problem with example inputs and labels. In meta-learning, tasks are denoted as T, with training samples ZT = {(xi, yi)} \u223c T and evaluation samples ZT = {(xi, yi)} \u223c T. The goal is to learn a new task Tj from only K examples for N classes, known as an N-way K-shot problem. Two types of learners are involved: a base-learner for single tasks and a meta-learner for fast adaptation to unseen tasks. The system includes a generic feature extractor \u03a6(x). The meta-learning system involves a generic feature extractor \u03a6(x) and a task-specific predictor f T (X) that adapts to each task T based on few shots available. The task-specific predictor is specific to a task T and can be seen as the last layer(s) of a deep neural network. The system learns a parametrization of \u03a6 and a closed-form learning process \u039b to compute the parameters w T of f T. The meta-learning phase aims to learn the meta-parameters \u03c9 and \u03c1 by minimizing expected loss on test sets. In meta-learning, the algorithm minimizes expected loss on test sets from unseen tasks in T with gradient descent to learn meta-parameters. Recent works use CNNs as feature extractors, with variability in base learner f T and training procedure \u039b. Examples include k-nearest-neighbour algorithm, CNN with SGD, and nested SGD. Systems like MATCHINGNET use neural networks with memory and recurrence for few-shot image recognition. Temporal convolutions are utilized in meta-learning to reuse information from past tasks. Matching-based methods, such as graph neural networks, learn correspondence between training and testing sets. Sequential learning algorithms like recurrent neural networks enable long-term dependencies between data and gradient updates. Model-agnostic meta-learning (MAML) involves backpropagating through fine-tuning stochastic gradient descent updates of model parameters. Algorithm 1 presents a new approach using fast base learners like Ridge Regression Differentiable Discriminator (R2D2) or Logistic Regression Differentiable Discriminator (LRD2). The focus is on R2D2 due to its closed-form solver. The algorithm requires task distribution, feature extractor, and finetuning predictor with specific parameters. Initialization is done with pre-trained or random parameters, and the algorithm iterates until completion. During base-learning with R2D2, the linear predictor fT is adapted for each training task T using the learning algorithm \u039b, while the meta-parameters \u03c9 and \u03c1 remain fixed. The meta-parameters are updated in the meta-training phase using ZT, leading to a ridge regression evaluation. The authors propose a ridge regression evaluation for few-shot learning tasks, simplifying matrix calculations using the Woodbury matrix identity. Regression outputs are transformed to work with the cross-entropy loss function for classification tasks. The authors propose a ridge regression evaluation for few-shot learning tasks, simplifying matrix calculations using the Woodbury matrix identity. When updating the meta-parameters (\u03b1, \u03b2) using ZT, they serve as a scale and bias for the cross-entropy loss function. The R2D2 system architecture includes [96, 192, 384, 512] filters in the feature extractor with 4 convolutional blocks for the CIFAR-FS dataset. Reproducing the results of the baseline algorithm on different datasets, they first consider the MAML algorithm from Finn et al. [2017]. The MAML and R2D2 feature extractors are different, with MAML using four convolutional blocks with an organization of [32, 32, 32, 32]. The R2D2 algorithm's feature extractor is more complex than MAML, with four blocks employing a [96, 192, 384, 512] scheme. To compare, both simple and complex feature extractors for R2D2 were implemented and evaluated. Assumptions were made for the complex architecture and feature extractor, including 3x3 convolution blocks and 2x2 maximum pooling with specific parameters. In implementing the R2D2 algorithm, a multinomial regression was used for ridge regression, but the exact number of features at the output of the feature extractor differed from the original paper's values. Despite using the same number of classes and shots for training and testing, the feature extractor yielded different numbers of features compared to the paper's results. The R2D2 algorithm uses a random number of shots during training, unlike most baselines. Keeping training and testing procedures similar is crucial for comparability. The stopping criterion is vaguely defined as \"the error on the meta-validation set does not decrease meaningfully for 20,000 episodes\". Meta-training is done using 60,000 iterations. The R2D2 algorithm uses 60,000 iterations for meta-training. Meta-parameters are updated using the Adam optimizer with a learning rate of 0.005, dampened by 0.5 every 2,000 episodes. Results of different architectures and algorithms are shown in Figures 4 and 5, with detailed results in Tables 1 and 2. Implementations were done in Python 3.6.2 and TensorFlow 1.8.0, with simulations run on a machine with 24 Xeon e5 2680s at 2.5 GHz, 252GB RAM, and a Titan X GPU with 12 GB. The R2D2 meta-learning method outperforms the MAML method in most simulations due to its more complex network architecture. Differences in results may be attributed to assumptions or training criteria. The classification accuracy is influenced by the complexity and amount of data, with accuracy decreasing as the number of ways increases and shots decrease. The MAML simulation on miniImageNet showed a 2-way 1-shot classification accuracy of 78.8 \u00b1 2.8%, outperforming the 74.9 \u00b1 3.0% reported in BID4. While some discrepancies were found in reproduced results compared to the original paper, the general observations regarding meta-learning with differentiable closed-form solvers remain valid. The assumptions made in the original paper could be a factor in the differences in results. The logistic regression based algorithm (LRD2) was not focused on in this reproducibility work due to the lack of a closed-form solution. The logistic regression solver lacks a closed-form solution. Contributions of the reproducibility project include algorithmic description of R2D2 meta-learning, evaluation of MAML pipeline on miniImageNet and CIFAR-FS datasets, and implementation of R2D2 in TensorFlow. The work also includes an evaluation of the reproducibility of the ICLR 2019 paper on meta-learning with differentiable closed-form solvers by BID1. The original paper lacks details on training methodologies for full reproducibility, but reasonable assumptions allowed for reproduction of key results. Different neural network architectures should be considered when comparing results. Table 1 shows classification accuracies on CIFAR-FS, supporting the conclusions of the original paper. Table 2 displays N-way K-shot classification accuracies on miniImageNet with confidence intervals for various methods, including MAML, BID4, R2D2, and our proposed approach. The results show varying accuracies for different shot and way combinations."
}