{
    "title": "rkg8FJBYDS",
    "content": "Variational inference methods like variational autoencoders provide scalable generative models with a connection to manifold learning. However, these approximations can become degenerate in training if the subjective prior is not carefully chosen. On the other hand, diffusion maps automatically infer data topology but do not scale easily or provide the inverse homeomorphism. The paper introduces a measure for recognizing data-latent distribution mismatch and a method, VDAE, that combines variational inference and diffusion maps for generative modeling. The locally bi-Lipschitz property is used as a condition for a homeomorphism, and VDAE infers data distribution topology and models a diffusion random walk. Stochastic variational inference and manifold learning optimization are used for efficient computation in VDAEs, with approximation theoretic results provided for dimension dependence. The paper introduces VDAE, a method combining variational inference and diffusion maps for generative modeling. It demonstrates superior performance on real and synthetic datasets compared to other generative models. Generative models like VAEs and GANs enable sampling realistic points from high dimensional distributions at low computational cost. These models involve learning smooth mappings from a user-defined prior distribution to the modeled distribution, closely tied to manifold learning. The connection to manifold learning may explain the failure of generative models like VAEs and GANs. Posterior collapse in VAEs and mode collapse in GANs occur when mappings collapse large parts of the input to a single output, leading to degenerate latent spaces and poor generative performance. The variational diffusion autoencoder (VDAE) models the diffusion process using neural networks to approximate the covariance of the random walk. Prior mismatch and spectral learning methods are explored to improve VAEs, focusing on diffusion maps and the heat kernel over the manifold. In this paper, the authors address issues in variational inference and manifold learning by combining ideas from both fields. Diffusion maps have strong theoretical guarantees but are not easily scalable for large-scale generative modeling. The authors propose a method that combines theory in manifold learning with variational inference to improve recognition of prior mismatch and learn difficult-to-approximate features. The authors introduce the locally bi-Lipschitz property to measure stability in mapping between latent and data distributions. They propose VDAEs, a class of variational autoencoders that approximate diffusion processes on data manifolds. Deep neural networks can learn these processes, producing random walks with desirable properties. The VDAE framework shows superior performance compared to GANs and VAEs. Variational inference (VI) combines Bayesian statistics and latent variable models to approximate probability density. It assumes a latent variable structure in data generation, where observations are conditionally distributed given unobserved latent variables. Maximizing the evidence lower bound (ELBO) speeds up optimization by using an approximation of the latent variables. Diffusion maps (DMs) are a class of kernel methods that perform non-linear dimensionality reduction on a set of observations X. The induced random walk on the graph of X considers transition probabilities that are row normalized versions of the kernel. The diffusion map embeds the data into Euclidean space, approximating diffusion distance by Euclidean distance. This property transforms a complex random walk on the data manifold into an isotropic Gaussian random walk. SpectralNet is an algorithm introduced to speed up diffusion map computations by approximating the function \u03c8 for symmetric kernels. It leverages bi-lipschitz coordinates by kernel eigenfunctions for local embedding of Riemannian manifolds. The algorithm SpectralNet uses bi-Lipschitz coordinates based on kernel eigenfunctions for local embedding of Riemannian manifolds, allowing for a mapping from a neighborhood to spectral coordinates. The Lipschitz constants depend on the inradius at x and global constants, with the manifold covered by geodesic balls where the bi-Lipschitz property holds. Our proposed measure and model address degenerate latent spaces and poor generative performance in variational inference due to prior mismatch in data and prior distribution topologies. This mismatch occurs when dimensionalities and geometries do not align, leading to the need for homeomorphisms between distributions. The geometries do not match, preventing homeomorphisms between distributions and leading to poor generative performance. The default Gaussian distribution for p(z) is not suitable for many datasets, while spherical distributions are superior for directional data in various fields. The literature is sparse on studying data distributed on complex manifolds due to the difficult nature of such study. Previous research has focused on alleviating mismatch by using VAEs with geometrically hyperspherical priors or modeling complex manifolds as priors. However, methods for choosing the prior still need to be prescribed. Our method automatically chooses the best prior by using data to inform the prior, ensuring a homeomorphism between data and prior distributions. We propose VDAEs, a variational inference method that models the geometry of data by approximating a random walk over a latent diffusion manifold. The model is trained to maximize the local evidence of each point given its random walk neighborhood, generating points by sampling from the stationary distribution of the random walk. The random walk is described as a composition of three functions: diffusion map\u03c8 \u0398, sampling from diffusion process z \u223c q \u03c6 (z |x), and inverse diffusion map\u03c8. The latent space\u03c8 \u0398 (X) has advantages in being well-defined, well-approximated, and approximating Euclidean distances. Transition probabilities induced by k can be approximated by Gaussian kernels in M Z. To model a diffusion random walk over M Z, functions\u03c8 \u0398,\u03c8 \u22121 \u03b8, and C \u03c6 must be learned to approximate the diffusion map, its inverse, and the covariance. SpectralNet provides the approximate diffusion map\u03c8 \u0398. Variational inference is used to learn\u03c8 \u22121 \u03b8 andC \u03c6. The local evidence lower bound is defined for each x \u2208 X. The empirical loss function is generated by Eq. 4, using a deterministic function depending on\u03c8 \u0398 andC \u03c6. Algorithm 1 outlines VDAE training to initialize parameters and obtain parameters \u0398 for the approximate diffusion map\u03c8 \u0398. The algorithm for generating data points from p(x) involves obtaining parameters \u0398 for the approximate diffusion map\u03c8 \u0398, computing gradients of the loss, updating \u03c6 and \u03b8, and using a random walk sampling procedure. The diffusion random walk converges on a stationary distribution \u03c0, guaranteed to be the uniform distribution on the data manifold. VDAEs are introduced as a practical implementation using neural networks for \u03c8 \u0398 (x), q \u03c6 (z |x), and p \u03b8 (x |z). The VDAEs utilize neural network functions for \u03c8 \u0398 (x), q \u03c6 (z |x), and p \u03b8 (x |z). The neighborhood reconstruction error is modeled by sampling q \u03c6 to obtain z, allowing for an approximation of x \u2208 M X. The divergence of random walk distributions can be simplified as the divergence of two Gaussian kernels on M Z. The diffusion map \u03c8 provides the diffusion embedding Z. The diffusion map \u03c8 provides the diffusion embedding Z, approximating the distribution of p \u03b8 (z |x) in a neighborhood. The Mahalanobis distance is used to estimate moments in R D, and the KL divergence is minimized between q \u03c6 (z |x) and the implied distribution. The sampling procedure is highly parallelizable due to neural network approximations, with rapid mixing properties observed in the random walk. The diffusion map \u03c8 provides rapid mixing properties in the random walk, allowing for efficient sampling. Algorithm 2 VDAE sampling involves one step of the diffusion random walk and mapping back into input space. The inverse map from spectral coordinate codes to the manifold is approximated by a decoder network with bounded complexity. The encoder's capacity to map M to the diffusion map space has been previously discussed, while the focus here is on the decoder, which is treated differently. Theorem 1 proves the existence of a sparsely-connected ReLU network for locally bi-Lipschitz coordinates on a manifold. It complements a previous theorem by providing guarantees for the decoder, based on properties of ReLU neural networks. The ReLU neural networks have the ability to split curved domains into small patches and build differences of bump functions on each patch. This property allows for borrowing approximation results from wavelet theory on spaces of homogeneous type. The bi-Lipschitz property of the diffusion embedding is crucial in proving this. The coordinates of the manifold in the ambient space can be seen as functions of the diffusion coordinates, with each coordinate function being Lipschitz. This leads to ReLU wavelet coefficients being 1, enabling the use of existing guarantees. The ReLU neural networks can split curved domains into small patches and build differences of bump functions on each patch, borrowing approximation results from wavelet theory. The bi-Lipschitz property of diffusion embedding is crucial. The coordinates of the manifold in ambient space are functions of diffusion coordinates, with each coordinate function being Lipschitz. This enables the use of existing guarantees for generating new frames. In generating new frames from a video of rigid movement, a low-dimensional circular manifold is created due to the spinning figure and fixed background. A comparison is made with VAE, Wasserstein GAN, and hyperspherical VAE using different priors and latent dimensions. The benefit of VAE is highlighted, even with a dimension mismatch issue. In a series of experiments, the results of the sampling procedure in Algorithm 2 on three synthetic manifolds are visualized. An initial seed point is randomly selected, and points are generated on the manifold through a random walk process. After numerous resampling iterations, the algorithm consistently generates points on the manifold. The algorithm generates points on the manifold through resampling iterations, converging to a uniform distribution quickly. It covers a large part of the latent space with one step. The architecture includes a hidden layer of 512 neurons with tanh activations. The method can generate samples from data with multiple clusters in an unsupervised manner. The method can generate new points from a specific cluster in an unsupervised manner, demonstrated on MNIST. The DVAE architecture allows for cluster conditional sampling without the need for training labels. The generated points remain within the cluster of the seed point, showcasing the ability to address differences in latent space topologies. In recent works on rejection sampling, the issue of topology differences between generative model latent space and output data has been addressed. A comparison experiment was conducted using a 5x5 grid of bounded spherical densities, where standard GANs struggle to avoid generating points in the gaps. Our method creates a latent space that separates clusters into their own features, generating points only in the neighborhood of training data. This results in significantly fewer points in the gaps between clusters, eliminating the need for additional points. The VDAE architecture uses one hidden layer of 512 neurons and tanh activations. The local bi-Lipschitz property is computed practically to evaluate methods on the MNIST dataset. Our method analyzes the local bi-Lipschitz measure in the latent space to evaluate the behavior of the homeomorphism function. Comparisons are made between different methods trained on the MNIST dataset, including WGAN, VAE, SVAE, and our method using a 500 unit hidden layer network architecture. Our method utilizes a 500 unit hidden layer network with ReLU nonlinearities for both the encoder and decoder. By constraining the latent space to be the diffusion embedding of the data, our method achieves low values of the local bi-Lipschitz constant. In contrast, other methods like VAE and SVAE do not consider the data's topology in the prior distribution, leading to mode collapse. Our method can reconstruct the entirety of the input distribution X while maintaining a low local bi-Lipschitz constant. Our method reconstructs X while maintaining a low local bi-Lipschitz constant by taking the log of the random walk transition likelihood. The setup involves a d-dimensional smooth compact manifold M with a Riemannian metric g. The Laplacian-Beltrami operator on M with Neumann boundary condition is self-adjoint on L 2 (M, \u00b5), where \u00b5 is the Riemannian volume. The Laplacian-Beltrami operator on a d-dimensional smooth compact manifold M with a Riemannian metric g satisfies certain conditions: M is re-scaled to have volume 1, has discrete spectrum with eigenvalues satisfying Weyl's estimate, and the heat kernel has a spectral representation. The proof of Theorem 1 is an extension of Theorem 4, which needs to be proved for each individual extrinsic coordinate X k. The Laplacian-Beltrami operator on a d-dimensional smooth compact manifold M with a Riemannian metric g satisfies certain conditions. The proof of Theorem 1 is an extension of Theorem 4, which needs to be proved for each individual extrinsic coordinate X k. Theorem 4 states the existence of a sparsely-connected ReLU network on a smooth d-dimensional manifold M, with specific node configurations depending on the manifold's curvature and dimension. The first layer of a neural network can select coordinates using 4D units and ReLU bump functions. The theorem from Shaham et al. (2018a) requires X Ui : \u03c8(U i ) \u2192 R to be efficiently written in terms of ReLU functions. Invertibility of \u03c8 on \u03c8(U i ) allows X(\u03c8(x)) = X(x) to be defined as the extrinsic coordinate of the manifold at x corresponding to \u03c8(x). The extrinsic coordinate of the manifold at point x corresponding to \u03c8(x) is Lipschitz continuous, allowing it to be approximated by step functions on a ball of radius 2. This approximation rate is 1 \u221a N for N ReLU wavelet terms on a local patch expressible in terms of ReLU wavelet coefficients."
}