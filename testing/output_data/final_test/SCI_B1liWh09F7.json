{
    "title": "B1liWh09F7",
    "content": "In this paper, novel self attention-based architectures are proposed to enhance adversarial latent code-based text generation, inspired by the success of self attention mechanism in sequence transduction and image generation. The study benchmarks two latent code-based methods (AAE and ARAE) using the Google sentence compression dataset, showing that the proposed models outperform the state-of-the-art in adversarial text generation. Recent deep learning-based approaches to text generation can be categorized into three classes: auto-regressive, generative adversarial network (GAN), and reinforcement learning (RL) based methods. Auto-regressive models like BID26 use RNNs to predict the next token based on previously generated ones, but face challenges like exposure bias. Recent deep learning-based approaches to text generation have utilized generative adversarial networks (GANs) as a powerful model for text generation. GANs have been successful in addressing exposure bias issues that arise from using maximum likelihood estimation (MLE). The discriminator in GANs guides the text generator during training to produce samples similar to real data. However, there are still challenges in GAN-based text generation that need to be addressed. In GAN-based text generation, challenges like mode collapse and training instability exist. Techniques like feature matching and entropy regularization can help mitigate these issues. Another challenge is the non-differentiability of generator sampling due to the discrete nature of text. Transformer self-attention mechanism is incorporated in AAE and ARAE setups for text generation to address these challenges. The proposed architectures for AAE and ARAE setups in text generation incorporate Transformer's encoder layers with self-attention and fully-connected layers in a residual configuration. Spectral normalization is used to create a novel GAN structure for these setups, leading to performance improvements demonstrated through objective and subjective measures in experiments. In text generation setups, network-wide spectral regularization stabilizes GAN training, leading to diverse outputs and higher inception scores. Attention mechanisms were introduced to address performance bottlenecks in sequence modeling, allowing decoders to focus on specific tokens in the input sequence. Self-attention, also known as intra-attention, has shown to be an effective alternative to LSTMs in natural language inference tasks, achieving state-of-the-art performance with fewer parameters. Self-attention structures have been used to achieve state-of-the-art performance in various tasks by reducing path length between sequence inputs, making learning of long-term dependencies easier. Recently, self-attention and spectral normalization were applied to image generation with GANs, allowing the generator to attend to far neighborhoods and improving training dynamics. Transformer BID27 utilizes self-attention and spectral normalization in its architecture, eliminating convolutional and recurrent layers for sequence transduction tasks like machine translation. Language generation faces challenges due to the discrete nature of text, with solutions including continuous approximation of discrete sampling and using policy gradient from reinforcement learning. Autoencoders with continuous latent spaces have been successful in addressing this issue. One successful solution for language generation involves autoencoders with continuous latent spaces. Different training setups, such as adversarial and variational methods, have been proposed. A recent paper reviews various latent code-based text generation methods and evaluates them using a rigorous protocol. Inspired by this, we demonstrate the effectiveness of our self attention-based approach. The evaluation includes measures like forward and reverse perplexity, BLEU, and fluency. Additionally, two baseline methods using adversarial latent code-based generation techniques are briefly explained. In Section 3, the authors discuss adversarial latent code-based generation techniques, focusing on the Adversarial Autoencoder (AAE) and the Adversarially Regularized Autoencoder (ARAE). AAE matches encoder output to an arbitrary distribution for various applications, while ARAE learns an autoencoder with continuous contracted codes. The authors tailor AAE for text generation, incorporating self attention and Transformer in the model. The ARAE model learns an autoencoder with continuous codes that correlate with discrete inputs, aiming to exploit GAN's ability for text generation. Self attention-based models following ARAE and AAE setups are used, showing comparable results in text generation. Transformer BID27 is utilized in designing all autoencoders with three blocks in both encoder and decoder. The autoencoders designed use three Transformer blocks in both encoder and decoder, with layer normalization applied on every layer. Multi-head attentions have eight heads and embedding layers are of size 304. Positional encoding is used at the first layer of the encoder and decoder. For GAN structures, modified Transformer encoder layers combined with spectral normalization are used. All connections are residual, inspired by spectral normalization successes. The SALSA-TEXT generator and discriminator architecture use spectral normalization in the weights, inspired by successes in GAN-based image generation. Layer normalization was not found to be useful in conjunction with spectral normalization. Self attention-based structures are used in adversarial setups BID17 and BID14, with a two-phase training approach alternating between minimizing reconstruction and adversarial costs. The SALSA-TEXT generator and discriminator architecture utilize spectral normalization in weights, inspired by GAN-based image generation success. Self attention-based structures are employed in adversarial setups BID17 and BID14, with a two-phase training approach alternating between minimizing reconstruction and adversarial costs. The trade-off factor (\u03bb) between reconstruction and adversarial costs is 20. Input and attention heads are dropped with a probability of 0.1. Word and attention head dropout is performed with a probability of 0.1 in the encoder and decoder. The performance of self attentive (SALSA) architectures is compared with code-based setups studied in BID6 in sentence generation evaluation. The SALSA-TEXT generator and discriminator architecture utilize spectral normalization in weights for sentence generation. Training on the Google Sentence Compression dataset is challenging due to long and diverse sentences. Models use Google's SentencePiece tokenizer with byte-pair encoding. A man has been found guilty of driving under the influence and a deadly attack in the Gaza Strip. Another man has been arrested for sexual assault and assault charges. Additionally, a man is facing charges for stealing a child and killing a two-year-old girl. Former PSV Ethanolve mentioned his contract and title. Former PSV Ethanolve mentioned his contract and title, which will be released on Monday. The Queen will present a \"Curprone of the Donegal Plan\" in a new biopic of the forthcoming musical. The Dallas Morning News reported that a Houston man is leaving the Houston Rockets. The Dallas Morning News Corp. is opening a subsidiary in Houston. The training set has an average of 23.1 words per sentence and 183739 total sentences. The test dataset has 9254 lines with an average of 22.7 words per sentence. Generated sentences from all models are provided in Section 4.3. The input noise to the generator is of size 100, which is upsampled to the embedding size of 304 using a fully connected layer. The noise is copied multiple times equal to the maximum number of steps in the sentence. Positional encodings are added to each step in the generator, and also at the start of each transformer encoder block. The attention depth is fixed due to the use of fixed size sequences. The attention depth in the transformer encoder block is fixed at T bpe. Positional encodings are added to the input of each transformer encoder block. Various objective and subjective measures are used to evaluate the models, including BLEU BID20, Self-BLEU BID32, forward and reverse perplexity. Lower Self-BLEU scores indicate more diversity in generated texts. The evaluation process includes measuring BLEU scores for similarity, forward perplexity for sentence quality, and reverse perplexity for sentence variety. LSTM language models are trained on WMT News 2017 Dataset and GSC test dataset, with subjective evaluation done using Amazon Mechanical Turk. The evaluation process involves sampling 18 sentences from each model for a total of 162 sentences. These are evaluated by 50 native English speakers on a 5-point Likert scale for grammaticality, semantic consistency, and overall fluency. AAE generates short sentences, while SALSA-AAE improves this issue. ARAE experiences extreme mode collapse compared to its SALSA version. The proposed self attention-based (SALSA) architectures consistently outperform non-attention-based benchmarks in terms of diversity and output quality on the GSC dataset. ARAE suffers from extreme mode collapse and performs poorly on long sentences. Human evaluations do not account for lack of diversity in the models. In experiments with the original ARAE model, it shows high human evaluation scores but poor diversity metrics due to mode collapse. The sentences generated by ARAE are repetitive and lack diversity, with all starting similarly and mentioning arrests or crimes. This is attributed to the complex structure of the GSC dataset. In contrast, SALSA-ARAE consistently produces high-quality sentences. SALSA-AAE, a strong adversarial text model, outperforms the original AAE in quality and diversity metrics. SALSA-TEXT, a Transformer-based architecture, shows better performance in adversarial code-based text generation compared to state-of-the-art models. It incorporates self-attention mechanism for diverse, long, and high-quality output sentences. Future research directions include studying the performance of self-attention in various text generation methods and experimenting with deeper Transformer-based autoencoders for improved language model capture and unsupervised pre-training."
}