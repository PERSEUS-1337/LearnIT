{
    "title": "rkgqm0VKwB",
    "content": "Named entity recognition (NER) and relation extraction (RE) are important tasks in information extraction and retrieval (IE & IR). Joint models for these tasks have shown improved performance by avoiding error propagation in pipeline-based systems. A neural, end-to-end model is proposed in this paper for extracting entities and relations without relying on external NLP tools. The model integrates a pre-trained language model and is fast to train due to the use of self-attention instead of recurrence. In the biomedical and clinical domains, named entity recognition (NER) and relation extraction (RE) play crucial roles in tasks such as network biology, gene prioritization, and drug repositioning. These tasks are essential for large-scale biomedical data analysis and the creation of curated databases. In the clinical domain, NER and RE are crucial for tasks like disease prediction, readmission prediction, and patient cohort identification. Traditional pipeline systems for NER and RE have drawbacks like error propagation and lack of information exchange. Joint models that extract entities and relations simultaneously have been proposed to address these issues and achieve state-of-the-art performance. Many joint models for entity and relation extraction rely on external NLP tools like dependency parsers. For example, a neural, end-to-end system proposed by Bekoulis et al. (2018a) learns to extract entities and relations without relying on external tools, addressing limitations in domains where these tools may not perform well. In the proposed neural models for entity and relation extraction, researchers have explored different approaches such as adversarial training and deep biaffine attention. Li et al. (2019) achieved state-of-the-art results by using multi-turn question answering with a BERT-based QA model. However, end-to-end systems like those proposed by Bekoulis et al. (2018a; b) and Nguyen & Verspoor (2019) face challenges with long training times for large datasets and limited performance on small datasets, especially in biomedical domains. The proposed neural models for entity and relation extraction have faced challenges with long training times and limited performance on small datasets, especially in biomedical domains. Li et al. (2019) introduced a multi-pass QA model using BERT to address these issues, but it relies on handcrafted question templates for optimal performance. This may be a limitation in domains requiring specific expertise for crafting questions. In this study, an end-to-end model for joint NER and RE is proposed, incorporating a pre-trained BERT model for state-of-the-art performance. The model is trained without handcrafted features or external NLP tools, is fast to train, and achieves or surpasses state-of-the-art results on 5 datasets. The proposed model for joint NER and RE utilizes a pre-trained BERT model and achieves state-of-the-art performance on 5 datasets. The model consists of an NER module and an RE module, with the NER module following the BIOES tag scheme for entity labeling. The proposed model for joint NER and RE utilizes a pre-trained BERT model and achieves state-of-the-art performance on 5 datasets. In the RE module, predicted entity labels are obtained by taking the argmax of each score vector and then embedded to produce fixed-length vectors. Relation candidates are constructed using all possible combinations of the last word tokens of predicted entities. The model utilizes a deep bilinear attention mechanism for relation classification, projecting words into head and tail vectors. A biaffine classifier is used with tensors and matrices to predict relation classes. Cross-entropy loss is computed during training for relation extraction objectives. The model is trained in an end-to-end fashion to minimize the sum of the NER and RE losses. Entity pretraining is implemented by weighting the contribution of RE loss during the first epoch of training. This approach differs from delaying training of the RE module and helps achieve good performance quickly for all datasets. The model is implemented in PyTorch using the BERT BASE model and NVIDIAs AMP library Apex for faster training and reduced memory usage. It is evaluated on 5 benchmark corpora in English, including ACE04 and ACE05 with entity and relation types. For ACE04 and ACE05, the model splits the Physical relation into two classes, merges Employment-Membership-Subsidiary and Person-Organization-Affiliation, and removes the Discourse relation class. Evaluation is done using 5-fold cross-validation on specific subsets, with hyperparameters chosen through cross-validation. The model is trained on combined data from all folds and evaluated on the test set, reporting the micro-averaged F1 score. Pre-processing scripts are obtained from previous work, and the CoNLL04 corpus is used for training. The adverse drug event corpus, introduced by Gurulingappa et al. (2012), serves as a benchmark for identifying adverse drug events from medical case reports. It includes abstracts from PubMed, with two entity types (Drug and Adverse effect) and one relation type (Adverse drug event). Relations with overlapping entities are removed, and the model is evaluated using 10-fold cross-validation. The 2010 i2b2/VA dataset introduced by Uzuner et al. (2011) includes an NER task for medical entity types and an RE task for relation types. The dataset was split into train and test sets, with 5-fold cross-validation performed on combined data. The model was evaluated using macro F1 score averaged across folds. The 2010 i2b2/VA dataset includes NER and RE tasks. The data was split into train and test sets, with 5-fold cross-validation. The model was evaluated using micro F1 score. The joint NER and RE model was compared to independent systems, noting differences in scoring due to the nature of RE as a sentence-level classification problem. Examples were pre-processed to predict relation types, making the task easier. In the TrAP task, treatment is administered for a medical problem. The model makes relation predictions on ground-truth entities, simplifying the task compared to the joint setup. Different settings were used to evaluate the model, with hyperparameters kept consistent across experiments. Learning rate and batch size were selected through minimal grid search. For the TrAP task, relation predictions are made on ground-truth entities with consistent hyperparameters. Pre-trained weights for BERT BASE model were chosen manually, with different weights used for general, biomedical, and clinical corpora. Results show improved NER performance on most datasets, particularly on ACE04 and ACE05. Our joint model shows significant improvements in NER and relation extraction tasks on various datasets, outperforming previous methods on some datasets. It achieves state-of-the-art performance on ACE04, ACE05, and ADE datasets, with minor differences on CoNLL04. Ablation analysis on the CoNLL04 corpus helps identify key components contributing to our model's performance. Ablation analysis on the CoNLL04 corpus reveals that removing FFNN head and FFNN tail has the largest negative impact on model performance. Replacing them with a single FFNN has a smaller negative impact. Entity pre-training and entity embeddings also significantly affect model performance. Including a transformer-based language model in our architecture allows for easy visualization of attention weights, aiding in detecting model bias and identifying relevant attention heads. Previous research shows that specific attention heads mark syntactic dependency relations, with lower layers focusing more on syntax and higher layers on semantics. Our model, fine-tuned on the CoNLL04 corpus, displays attention weight patterns. Additionally, our model is compared to others on the i2b2 2010 corpus, with no published joint NER and RE models for evaluation. Our model, fine-tuned on the CoNLL04 corpus, displays attention weight patterns. We compare our model to the state-of-the-art for each individual task and provide access to attention weights for further analysis on our GitHub repository. In this paper, an end-to-end model for entity and relation extraction is introduced, with key contributions including no reliance on hand-crafted features or external NLP tools, integration of a pre-trained language model, and state-of-the-art performance on multiple datasets. The model is modular, allowing for easy initialization with different language models and fast training times. It outperformed previous state-of-the-art performance on ADE by a significant margin. The model achieved state-of-the-art performance on ADE with a large margin. The corpus was easy to learn due to the majority of sentences being annotated for two entities and one relation. However, high performance on this corpus may not transfer to real-world scenarios. The model reached almost perfect performance for RE on the test set when using ground-truth entities. The model currently only considers intra-sentence relations, while multiple entities within a document may exhibit complex inter-sentence relations. The model achieved state-of-the-art performance on ADE with a large margin, but it currently only considers intra-sentence relations. The model's restriction to intra-sentence relations may limit its usefulness for certain downstream tasks, such as knowledge base creation. Future work aims to extend the model to handle nested entities and inter-sentence relations. Additionally, the model's performance across multiple languages is expected to hold with the use of multilingual, pre-trained weights for BERT."
}