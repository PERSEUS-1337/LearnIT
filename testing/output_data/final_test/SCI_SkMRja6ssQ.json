{
    "title": "SkMRja6ssQ",
    "content": "Keyword spotting, also known as wakeword detection, is crucial for hands-free operation of voice-controlled devices. DONUT is a CTC-based algorithm for custom wakeword detection, allowing users to personalize their wakeword. It records training examples from the user, generates label sequence hypotheses, and detects the wakeword by aggregating scores from new audio recordings. DONUT combines CTC-based keyword spotting with user-adaptation, is computationally efficient, and suitable for embedded systems without uploading private user data to the cloud. In a query-by-example system, users teach the desired wakeword by recording training examples, and the keyword spotter uses template matching to detect the wakeword. Dynamic time warping (DTW)-based keyword spotting compares feature vectors from query and test audios using DTW alignment score for detection. Other approaches use fixed-length feature vectors like final hidden states of a pre-trained RNN or output of a Siamese network for template matching. The wakeword model using a CTC-based approach is easy to interpret compared to systems using template matching. Interpreting hidden states of RNNs can be challenging, while CTC-based models provide a human-readable string for inference. The neural network outputs are peaky and sparse, making it easy to determine what the network \"hears\" for any given audio. The proposed method \"DONUT\" combines query-by-example methods with CTC-based keyword spotting for custom wakeword detection. It allows system designers to identify and correct recognition issues by augmenting training data with examples of poorly recognized labels. The method involves recording training examples, using beam search to estimate keyword labels, and maintaining an N-best list of label sequence hypotheses. The proposed method \"DONUT\" combines query-by-example methods with CTC-based keyword spotting for custom wakeword detection. It maintains an N-best list of label sequence hypotheses to minimize errors in estimating labels. The method uses a model composed of a wakeword model and a label model, scoring hypotheses with the forward algorithm to obtain a single detection score. This approach achieves good performance, generates easily interpretable models, and matches user pronunciation better than when label sequences are supplied through text. The proposed method \"DONUT\" combines query-by-example methods with CTC-based keyword spotting for custom wakeword detection. It maintains an N-best list of label sequence hypotheses to minimize errors in estimating labels. The method uses a model composed of a wakeword model and a label model, scoring hypotheses with the forward algorithm to obtain a single detection score. This approach achieves good performance and matches user pronunciation better than when label sequences are supplied through text. The wakeword model consists of a set of label sequences and confidences, with the label model being a neural network trained using CTC on a speech corpus. The method \"DONUT\" combines query-by-example with CTC-based keyword spotting for custom wakeword detection. It uses a model with a wakeword and label model to score hypotheses and obtain a detection score. The user records wakephrases, computes label posteriors, and runs a beam search to find probable label sequences. The top N hypotheses are kept, and their log probabilities are converted to \"confidences\". The method \"DONUT\" combines query-by-example with CTC-based keyword spotting for custom wakeword detection. It uses an \"acoustic-only\" approach without language models or pronunciation dictionaries. A voice activity detector (VAD) determines frames with speech audio, reducing power consumption. The log probability of each hypothesis in the wakeword model is computed using the CTCForward function. DONUT is a fast and efficient method for custom wakeword detection using an acoustic-only approach. It combines query-by-example with CTC-based keyword spotting, where log probabilities are weighted by confidences to detect the wakeword. The algorithm can update hidden states as speech frames become available, reducing latency for online processing on embedded devices. DONUT is a fast and efficient method for custom wakeword detection using an acoustic-only approach. It combines query-by-example with CTC-based keyword spotting, where log probabilities are weighted by confidences to detect the wakeword. The algorithm can update hidden states as speech frames become available, reducing latency for online processing on embedded devices. The memory, storage, and computational requirements of running DONUT online can be broken down into two parts: running the label model and running the wakeword model. The runtime requirements are dominated by the label model (the neural network) which uses an RNN with frame stacking to reduce the number of operations. The wakeword model requires little storage and the CTC forward algorithm requires a certain number of operations to process a single label sequence. The algorithm for wakeword detection requires O(NUT) operations for N hypotheses with average length U. Shared terms can be identified to reduce operations, but optimization is not crucial due to small N and U values. Memory required is O(NU) for storing forward probabilities per timestep. Audio data is sampled at 16,000 Hz and converted to 41-dimensional Mel filterbank sequences. The algorithm for wakeword detection requires O(NUT) operations for N hypotheses with average length U. Memory required is O(NU) for storing forward probabilities per timestep. Two types of datasets were used: one to train label models using LibriSpeech BID19, and another to train and test wakeword detectors using LibriSpeech-Fewshot and EnglishFewshot datasets. The LibriSpeech-Fewshot dataset was created by splitting LibriSpeech recordings into short phrases for few-shot learning episodes. Each episode includes support and test examples, with positive and negative examples. The test set contains eight positive examples and 24 negative examples, with 20 phonetically similar and four phonetically dissimilar negative examples. The English-Fewshot dataset was created using crowdsourcing to record speakers saying phrases like \"Hello Computer\". It includes positive and negative examples from the same speaker, as well as negative examples from different speakers to avoid speaker verification bias. Due to data constraints, \"imposter\" examples from different speakers could not be obtained. In the study, the performance of a wakeword detector was measured using the ROC curve, EER, and AUC metrics. The experiment compared DONUT with other keyword spotting methods using DTW based on raw FBANK input and posteriorgram features. The 2 norm was used to compare FBANK features, and a distance-like metric was used for posteriorgram features. The study compared DONUT with other keyword spotting methods using DTW based on raw FBANK input and posteriorgram features. Different modifications were tried, but the best result was achieved with the CTC model. DONUT outperformed DTW methods in all three cases of query-by-example methods on English-Fewshot. In an experiment comparing DONUT with conventional CTC keyword spotting, DONUT outperformed the text-based approach for phonetically confusing examples. For non-confusing examples, both approaches performed similarly, with the text-based approach slightly better. This suggests that DONUT offers a more user-friendly interface while maintaining or improving performance. DONUT is interpretable, allowing system designers to identify and improve model problems. For instance, a wakeword model learned for the phrase \"of dress\" showed issues with phonemes. This information can be used to enhance the model by retraining with more examples. Debugging during the inference phase is also simplified with the use of CTC, allowing for easier identification of false accepts. DONUT explores hyperparameters impact on performance using the English-Fewshot dataset. Increasing the number of hypotheses generally improves performance, but may yield diminishing returns. Even a simple greedy search works well for the system. The choice of label model also affects performance. The choice of label model impacts performance in wakeword detection. Improving the label model can decrease error rate/increase AUC. Aggregating hypotheses' scores with weighted sum improves performance by considering the log probability of each hypothesis. In this paper, DONUT, an efficient algorithm for online query-by-example keyword spotting using CTC, is proposed. The algorithm learns hypothetical label sequences from user speech during enrollment and uses them to score audios at test time. The model is interpretable, easy to inspect, debug, and tweak, while maintaining high accuracy in wakeword detection. Training a wakeword model on the user's device without uploading private voice data to the cloud is possible through a simple beam search technique. This technique can be applied to various domains where users want to teach a system to recognize sequences of events like melodies or gestures. The transferability of this technique to other domains is worth exploring."
}