{
    "title": "H1lZJpVFvr",
    "content": "Adversarial training is effective for training robust models against adversarial examples, but lacks generalization on unseen data. This work explores the relationship between generalization and robust local features. A Random Block Shuffle transformation is used to break up global structure features, leading to the development of Robust Local Features for Adversarial Training (RLFAT). The study introduces Robust Local Features for Adversarial Training (RLFAT) by training on RBS-transformed adversarial examples to improve adversarial and standard generalization in deep learning models. Extensive experiments on various datasets show significant enhancements in model performance and alignment with human perception. Deep neural network models are highly vulnerable to adversarial examples, where small changes in input can deceive the model's predictions. These perturbations are hard for humans to detect and can transfer across different models for black-box attacks. Despite various defense methods, many only hide attack gradients. Adversarial training is currently one of the most effective techniques to combat these attacks. Adversarial training improves robustness by injecting adversarial examples into training data, but there is still a gap in generalization to unseen testing data. Recent studies suggest that adversarially trained models focus more on global structure features, while normally trained models are biased towards local features. Global features are robust against perturbations but struggle with unseen shape variations, while local features generalize well to shape variations but struggle with adversarial perturbations. The relationship between adversarial training and robust local features is investigated in this work. A Random Block Shuffle (RBS) transformation is used to study this relationship by breaking up global structure features on normal adversarial examples. A novel method called Robust Local Features for Adversarial Training (RLFAT) is proposed to learn robust local features for better adversarial and standard generalization. Features for Adversarial Training (RLFAT) learns robust local features and transfers them to training on normal adversarial examples. Implemented in PGD Adversarial Training and TRADES frameworks, RLFAT shows consistent improvements in adversarial robustness and standard accuracy on various datasets. Salience maps align better with human perception.notations and brief description of advanced methods for adversarial attacks and training are provided. In this paper, the focus is on adversarial attacks using Projected Gradient Descent (PGD) and Carlini-Wagner attack (CW). PGD is an iterative method for finding adversarial examples within bounded perturbations, while CW directly solves for adversarial examples using an auxiliary variable. The objective functions for optimization in both methods are defined, with CW utilizing a constant k to control the process. The function to optimize the auxiliary variable w is defined with a constant k controlling the confidence gap between adversarial and true classes. N attack (Li et al., 2019) is a derivative-free black-box adversarial attack breaking many defense methods. Adversarial training remains effective despite various defense methods being broken. The min-max optimization problem is solved in adversarial training. PGD Adversarial Training (PGDAT) and TRADES are two state-of-the-art adversarial training frameworks. PGDAT uses the PGD attack to generate adversarial examples for training, while TRADES maximizes the trade-off between adversarial robustness and standard accuracy. Unlike normally trained models, adversarially trained models are biased towards local features but more robust against adversarial examples. In contrast to global structural features, local features are more well-generalized but less robust against adversarial perturbation. This work focuses on learning robust local features through RLFAT, a novel form of adversarial training. By transferring these features into normal adversarial examples, the models exhibit strong robustness and generalization on unseen data. Adversarial training typically emphasizes global structure features for increased invariance against perturbations. To promote robust local feature learning, a simple image transformation called Random Block Shuffle (RBS) is proposed. The Random Block Shuffle (RBS) transformation is used to break up global structure features of images while retaining local features. It involves splitting and shuffling image blocks horizontally and vertically. RBS-transformed adversarial examples are used in training to encourage learning of robust local features. The Random Block Shuffle (RBS) transformation is utilized during adversarial training to learn robust local features. This approach, known as RBS Adversarial Training (RBSAT), does not use RBS transformation during the inference phase. Two state-of-the-art adversarial training frameworks, PGD Adversarial Training (PGDAT) and TRADES, are considered to showcase the effectiveness of robust local features. Different loss functions are used as alternatives to the objective functions of PGDAT and TRADES, taking into account the difference in input images between training and inference phases. The Robust Local Feature Transfer (RLFT) aims to transfer the knowledge of robust local features learned by RBSAT to normal adversarial examples. It focuses on minimizing the feature shift between normal and RBS-transformed adversarial examples, particularly on the logit layer for high-level feature alignment. RLFT is integrated with RBSAT into an end-to-end training framework called RLFAT. The RLFAT (Robust Local Features for Adversarial Training) framework integrates RLFT to transfer robust local features to normal adversarial examples. The training process of RLFAT involves initializing the network, updating parameters through backpropagation, and implementing new objective functions for feature representation learning. The RLFAT framework introduces RLFAT P and RLFAT T, with \u03b7 as a balancing hyper-parameter. Empirical evaluation shows significant improvements in robust and standard accuracy on benchmark datasets. Comparison with existing defenses like PGDAT and TRADES is done in white-box and black-box attack settings. Strongest white-box attacks like Projected Gradient Descent (PGD) are considered. In the attack setting, the strongest white-box attacks considered are Projected Gradient Descent (PGD) and CarliniWagner attack (CW). For the black-box attack setting, the powerful black-box attack N attack is performed on a sample of 1,500 test inputs. The proposed methods are compared with baselines on CIFAR-10 and CIFAR-100 datasets. Hyper-parameters are optimized with specific values for each dataset. The proposed methods, RLFAT P and RLFAT T, demonstrate significant improvements in adversarial robustness and standard accuracy over baseline models on various datasets. They outperform existing frameworks like TRADES and PGDAT, showing better adversarially robust generalization and standard generalization. The results highlight the effectiveness of robust local features in enhancing both adversarial and standard generalization in adversarial training. The study demonstrates improvements in adversarial robustness and standard accuracy with the proposed methods RLFAT P and RLFAT T. They outperform existing frameworks like TRADES and PGDAT, showing better generalization. The effectiveness of robust local features in enhancing generalization in adversarial training is highlighted. The study shows enhancements in adversarial robustness and standard accuracy using RLFAT P and RLFAT T methods, outperforming existing frameworks like TRADES and PGDAT. The effectiveness of robust local features in improving generalization in adversarial training is emphasized. Perturbation of testing data is done by adjusting brightness in the HSV color space, resulting in smoother loss functions. Loss sensitivity under gamma mapping is also approximated to analyze model performance. Sensitivity analysis results are reported in Table 2a and 2b for brightness adjustments on testing data. The study evaluates the effectiveness of RLFAT P and RLFAT T in improving adversarial robustness and standard accuracy compared to existing frameworks like TRADES and PGDAT. Results show that RLFAT T provides the smoothest model under distribution shifts on various datasets. Ablation studies dissect the impact of robust local feature learning and transfer on model performance. Additional experiments on STL-10, CIFAR-10, and CIFAR-100 demonstrate standard accuracy over clean data and average robust accuracy against attacks for each model. The study evaluates the effectiveness of RLFAT P and RLFAT T in improving adversarial robustness and standard accuracy compared to existing frameworks like TRADES and PGDAT. Results show that RLFAT T provides the smoothest model under distribution shifts on various datasets. Ablation studies dissect the impact of robust local feature learning and transfer on model performance. Additional experiments on STL-10, CIFAR-10, and CIFAR-100 demonstrate standard accuracy over clean data and average robust accuracy against attacks for each model. Robust Local Features Learning (RLFL) shows stable improvements on both standard accuracy and robust accuracy for RLFAT P and RLFAT T, supporting the hypothesis. The addition of Robust Local Feature Transfer (RLFT) further increases robust accuracy on all datasets for RLFAT P and RLFAT T, with standard accuracy also improving, except for RLFAT P on CIFAR-100. Transferring robust local features into the training of normal adversarial examples is beneficial. The study investigates how transferring robust local features improves standard and robust accuracy in adversarial examples. Salience maps generated using SmoothGrad show that RLFAT P and RLFAT T capture more local feature information compared to other models. These models align better with human perception and correctly classify images. In this work, a new adversarial training approach called Robust Local Features for Adversarial Training (RLFAT) is proposed to improve generalization. The method is implemented in state-of-the-art frameworks, PGDAT and TRADES, showing better standard and adversarially robust generalization. Salience maps of the models align better with human perception, indicating the benefits of robust local features for adversarial training. The study introduces a new approach called Robust Local Features for Adversarial Training (RLFAT) to enhance generalization in adversarial training. It raises questions about disentangling robust local features and leveraging them effectively. The findings suggest that considering both robust local features and global information can inspire new adversarial defenses. The study also provides details on training and attack hyper-parameters used in the experiments. In all training jobs, Adam optimizer with a learning rate of 0.001 and batch size of 32 is used. For CIFAR-10 and CIFAR-100, 79,800 training steps are run, while for STL-10, 29,700 steps are used. Adversarial examples are generated with specific parameters for each dataset. Different attack hyper-parameters are set for PGD, CW, and N attacks. More salience maps of adversarially trained models are provided in Figure 4. Salience maps of adversarially trained models on sampled images are shown in Figure 4. The images display the original image and salience maps of the four models sequentially."
}