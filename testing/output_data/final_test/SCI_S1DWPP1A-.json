{
    "title": "S1DWPP1A-",
    "content": "Intrinsically motivated goal exploration algorithms enable machines to discover diverse policies in complex environments. A 2-stage approach using deep representation learning algorithms is proposed to learn an adequate goal space for autonomous exploration. Experiments with a simulated robot arm show the effectiveness of this approach. Intrinsically motivated exploration is crucial for knowledge and skill development in both humans and machines. Children explore their bodies and objects independently, leading to significant discoveries. Equipping machines with similar exploration capabilities is essential for lifelong learning and artificial intelligence advancement. In the last two decades, computational models have contributed to understanding exploration processes in infants and applying them for autonomous lifelong machine learning. Research groups have modeled children as intrinsically motivated to explore the world like scientists, focusing on embodiment, intrinsic motivation, and social guidance mechanisms for efficient exploration and skill discovery. This article addresses challenges in learning goal representations for intrinsically motivated exploration. The article discusses learning goal representations for intrinsically motivated exploration, leveraging models of embodiment and social guidance. Computational models frame intrinsic motivation as mechanisms that self-organize agents' exploration curriculum by maximizing measures such as novelty, predictive information gain, learning progress, and competence progress in the Reinforcement Learning framework. In the Reinforcement Learning framework, intrinsic rewards like novelty and learning progress guide exploration, enabling artificial agents to make discoveries. These measures have been effective in solving problems with sparse rewards. However, current approaches focus on micro-actions and states, while children's exploration leverages abstractions of environments. The computational framework IMGEPs leverages abstractions of environments and self-generated goals for exploration. Agents sample parameterized goals and dedicate time to improve solutions, using lower-level optimization or RL methods. This approach allows agents to discover various outcomes during exploration. Agents leverage information gathered during exploration to improve solutions to other goals efficiently, even when goals are randomly sampled. Random goal exploration often leads to discovering novel outcomes and policies, pushing the boundaries of known solutions. This intrinsic motivation for exploration can be enhanced by using meta-learning algorithms to monitor competence evolution across goal spaces. Meta-learning algorithms automate curriculum sequences of goals by monitoring competence evolution and selecting the next goal to try based on expected progress. This allows robots to efficiently acquire locomotion skills or object manipulation techniques. Existing algorithms have limitations in Intrinsically Motivated Goal Exploration Processes. The limitations of existing algorithms in Intrinsically Motivated Goal Exploration Processes include the challenge of designing architectures that sample goals from a learned representation, particularly in unsupervised learning scenarios. In this paper, a 2-stage developmental approach named IMGEP-UGL is presented to address challenges in unsupervised goal representation learning and goal exploration processes within an Intrinsically Motivated Goal Exploration Process architecture. The IMGEP-UGL approach involves a two-stage process for unsupervised goal representation learning and exploration. In the first stage, a learner observes world changes and learns a latent space representation. In the second stage, this representation is used for goal sampling in an Intrinsically Motivated Goal Exploration Process. The current knowledge allows for guessing policy parameters to optimize outcomes for various goals. Executing policies improves knowledge for all goals in the embedding, leading to a diverse set of outcomes. However, the approach freezes learned representations in the second stage, limiting evolution. This decomposition mirrors infant motor exploration development. In the early stages of infant development, motor exploration is limited while they focus on observing the outside world with their eyes. Perceptual learning occurs during this phase, which is later utilized for motor learning. Infants learn passively by observing visual effects in their environment, which serves as a model for their learning process. This decomposition of learning into two phases helps in analyzing the complexity of the underlying algorithmic components. The main contribution of this article is the development of an algorithmic architecture that allows for representation learning algorithms to discover goal spaces for exploration, rather than using hand-designed goal spaces. The architecture is tested in environments where a simulated robot learns to move and rotate objects, with the diversity of outcomes measured using KL-coverage. The IMGEP-UGL architecture utilizes various unsupervised learning algorithms for goal space learning, showing that exploration dynamics can be matched using engineered representations. Different representation learning algorithms such as VAEs, AEs, Isomap, and PCA are compared for exploration efficiency within the architecture. The IMGEP-UGL architecture incorporates unsupervised learning algorithms for goal space learning, demonstrating that exploration dynamics can be improved with engineered representations. Various representation learning algorithms like VAEs, AEs, Isomap, and PCA are evaluated for exploration efficiency within the architecture. In the first phase, IMGEPs are algorithmic architectures used to explore high-dimensional continuous action spaces in robotic problems. The robotic agent acts as an experimenter seeking information through sequential experiments, with a context representing initial experimental factors. The curr_chunk discusses the parameterization space \u0398, outcome space O, and phenomenon dynamics D in the context of robotic experiments. It highlights the challenge of learning goal representations and the experimental factors controlled by the robotic agent. Developmental roboticists aim to create autonomous agents that learn forward and inverse models to predict outcomes and generate desired results. A strategy called Random Parameterization Exploration involves gathering tuples {c, \u03b8, o} to train these models by sampling random parameterizations and conducting experiments. However, in robotics, only a small subset of parameter space is likely to yield interesting outcomes. In robotics, a small subset of parameter space is likely to produce interesting outcomes. Random sampling in this space may not yield informative samples for learning forward and inverse models. Intrinsically Motivated Goal Exploration Strategies offer a solution by providing a Goal Space T with parameterized goals for the autonomous agent to target. In robotics, Intrinsic Motivated Goal Exploration Strategies provide a Goal Space with parameterized goals for the autonomous agent to target. A Goal Policy is a probability distribution over the Goal Space used for sampling goals, which can be updated over time. Goal-parameterized Cost Functions map outcomes to real numbers representing the goodness-of-fit regarding the goal. In robotics, Intrinsic Motivated Goal Exploration Strategies involve a Meta-Policy \u03a0 to solve minimization problems by approximating outcomes. The Outcome Space T can be set as an Euclidean space, allowing for Goal-parameterized cost functions to measure similarity. For example, in the Arm-Ball problem, the final ball position can be used as the Outcome Space, with the Euclidean distance serving as the Goal-parameterized cost function. Algorithmic architecture 2 outlines the Intrinsically Motivated Goal Exploration Processes, involving bootstrapping and goal exploration phases. The process includes sampling policy parameters, observing outcomes, initializing memory of experiments, and mixing random policy exploration with goal exploration. The goal policy can be a random stationary distribution or a contextual multi-armed bandit maximizing information gain or competence progress. The algorithm involves using a meta-policy algorithm to search for parameterization that minimizes the Goal-parameterized cost function. The process includes initializing with a parameter in the neighborhood of the goal, improving it using an optimization algorithm, executing the resulting policy, observing the outcome, and updating the memory and regressor. This approach enables sample efficient exploration in high-dimensional continuous action robotic setups. The algorithm involves using tools BID14, BID15, or soft deformable objects BID28 for object manipulations. However, using these algorithms in real-life setups poses challenges within a fully autonomous learning approach. Providing an Outcome Space to the agent can be difficult in many real-world cases, as the designer may not fully understand the space the robot is learning about. Previous approaches involved creating an external program to extract information from images, presenting it to the agent as a point in [0, 1] n. In complex environments, the designer may not know what is feasible for the robot, leading to many unfeasible goals in the Outcome Space. In complex environments, designing a system where the engineer creates the representation of an Outcome Space can limit autonomy. Can a mechanism be designed for the agent to construct an Outcome Space efficiently through examples? Deep Learning algorithms, like Autoencoders and Variational models, are explored for goal space representation learning within the IMGEP architecture. The IMGEP framework proposes adding unsupervised perceptual learning (UGL) before goal exploration to enable goal space representation learning. In the passive perceptual learning stage, the learner observes raw sensor values as the environment changes and uses them to train an unsupervised learning algorithm to map high-dimensional observations to lower-dimensional representations. The IMGEP framework introduces unsupervised perceptual learning (UGL) before goal exploration to learn a lower-dimensional representation of the environment. The architecture can be implemented with various algorithmic variants depending on the unsupervised learning algorithm used in the UGL phase. The outcome distribution is estimated using a kernel density estimator (KDE) in the goal exploration stage. The UGL phase in the IMGEP framework involves experimenting with different deep and classical Representation Learning algorithms to learn a lower-dimensional representation of the environment. Auto-Encoders (AEs) are a type of Feed-Forward Neural Networks trained in an unsupervised manner to model statistical regularities in data. Auto-Encoders (AEs) learn statistical regularities in data during training by encoding information in a compact representation. Variational Auto-Encoders (VAEs) are a recent alternative that extends this concept with a stochastic encoding approach. Variational Auto-Encoders (VAEs) differ from AEs as the encoder outputs parameters of a Gaussian distribution for sampling the representation z. An extra term conditions the distribution of z in the representation space, making VAEs faster to converge than AEs. However, the derivation of the cost function relies on the assumption of a factorial Gaussian distribution, which may not hold true for factors like periodic or discrete data. Normalizing Flow proposes a way to overcome restrictions on distribution by allowing more expressive ones through invertible transformations. Radial Flow, a transformation within this framework, provides flexibility to encode periodic factors. Isomap is a classical approach of Multi-Dimensional Scaling allowing embedding of N-dimensional points in an n-dimensional space, minimizing distortion in pairwise distances. It assumes data lies in a lower dimensional manifold and computes geodesic distances using Dijkstra's Shortest Path algorithm. Principal Component Analysis finds orthogonal transformation for linearly uncorrelated data. Principal Component Analysis finds the principal axis of the covariance matrix to yield linearly uncorrelated data, allowing for dimensionality reduction by selecting the first n dimensions. Gaussian Kernel Density Estimation is used to estimate the sampling distribution in the Outcome Space O learned by the agent. In our experiments, we used a Gaussian Kernel with a bandwidth matrix equaling the covariance matrix of the points, rescaled by a factor based on the number of samples. We investigated the efficiency of exploration dynamics in IMGEP-UGL implementation compared to IMGEP implementation using engineered goal space representations. The impact of target embedding dimensionality on these algorithms was also studied in two simulated environments derived from the Arm-Ball benchmark. In experiments on two Simulated Environments from the Arm-Ball benchmark, a 7-joint arm controlled by a 21-dimensional DMP BID20 controller interacts with objects. The Arm-Ball and Arm-Arrow environments involve moving objects within specific ranges, generating observable images for IMGEP-UGL learners. The physically reachable space for object movements is limited to a disk of radius 1 centered at 0. The learner observes object movements with a radius slightly larger than their own feasible space. Various Representation Learning Algorithms were tested for the UGL component. Classical IMGEP components were also considered. The classical IMGEP components were considered, with context space not observed by the agent and parameterization space used for DMP controllers. The Outcome Space is defined by embedding representations of images observed in the learning phase. RGE-EFR algorithm has l=2 in ArmBall and l=3 in ArmArrow, while IMGEP-UGL algorithms require a parameter for maximum dimensionality. In experiments, two cases were considered for the algorithms used in the UGL stage: 1) l = 10 for ArmBall and 3.3 times larger for ArmArrow, and 2) l = 2 for ArmBall and l = 3 for ArmArrow. The Goal Space was equated to the Outcome Space, and a Goal-Parameterized Cost function was used. The IMGEP architecture with Random Goal Exploration (RGE) was implemented. The strategy of Random Goal Exploration (RGE) involves sampling random goals from a distribution over the Outcome Space. A k-neighbors regressor is used for the forward modelD, and the Meta-Policy mechanism returns the nearest achieved outcome with parameterization perturbed by exploration noise. The focus is on exploring the outcome space and characterizing the diversity of effects discovered by the learner. The exploration dynamics in Random Goal Exploration (RGE) are quantitatively characterized using the Kullback-Leibler Coverage (KLC) measure. KLC computes the KL-divergence between the distribution of outcomes produced and a uniform distribution of physically possible outcomes. This measure helps define the diversity of effects discovered by the learner. The study evaluates exploration dynamics using the Kullback-Leibler Coverage measure. IMGEP-UGL algorithmic implementations are compared to baseline algorithms using a uniform distribution for exploration. The IMGEP implementation with handcrafted features is known for efficient exploration. The Random Parameterization Exploration (RPE) approach samples random parameterizations without using Outcome Space or Goal Policy. The study compares exploration dynamics of IMGEP-UGL algorithms and evaluates the impact of target embedding dimensions. Specifically, RGE-VAE is studied using Gaussian prior to replace KDE. In the study, RGE-VAE replaces the KDE estimator of p(O) in the UGL part. Performance comparison shows that IMGEP-UGL algorithms outperform RGE-EFR and RPE algorithms in terms of KLC evolution and speed. Details of outcomes in ArmBall and KLC measures are also presented. In the study, RGE-VAE outperforms RGE-EFR and RPE algorithms in terms of KLC evolution and speed. Both RGE-EFR and RGE-VAE perform well in discovering policies that move the ball, with RGE-VAE having a higher KLC due to discovering more policies. Providing the true target embedding dimension to IMGEP-UGL implementations improves RGE-Isomap on the ArmBall problem. Increasing the target embedding dimension in IMGEP-UGL algorithms improves performance, with RGE-Isomap showing slight improvement and RGE-AE remaining unchanged. RGE-PCA and RGE-VAE, however, experience degraded performance. ArmArrow also benefits from a larger target embedding dimension, with most algorithms outperforming RGE-EFR. Increasing the dimension enables learners to discover more policies, concentrating outcomes towards the external boundary of physically possible outcomes. Increasing the target embedding dimension in IMGEP-UGL algorithms improves performance by biasing exploration towards policies that produce outcomes beyond the feasible outcome space boundary. This is due to the increased probability of sampling goals outside the feasible space as the embedding space dimensionality increases. Increasing the target embedding dimension in IMGEP-UGL algorithms improves performance by biasing exploration towards novel outcomes and boundaries. The distribution used as stationary Goal Policy, such as isotropic Gaussian, impacts exploration, with Kernel Density Estimation showing similar performance. In this paper, a new Intrinsically Motivated Goal Exploration architecture with Unsupervised Learning of Goal spaces (IMGEP-UGL) is proposed. The Outcome Space representation is learned through passive observations of world changes using low-level raw sensors. The convergence on the KL term of the loss can vary depending on initialization, leading to a sampling bias when using the isotropic Gaussian Goal Policy. The new architecture for Intrinsically Motivated Goal Exploration with Unsupervised Learning of Goal spaces (IMGEP-UGL) is a milestone in AI research. It is the first architecture where the goal space representation is learned, rather than hand-crafted. The flexibility of this architecture allows for implementation using various unsupervised learning algorithms, including advanced deep neural network algorithms like Variational Auto-Encoders. This opens up the possibility to benefit from future advances in unsupervised representation learning research. Experiments have shown that most algorithms, except RGE-RFVAE, can compete with IMGEP using engineered features. Providing a larger target embedding dimension to IMGEP-UGL algorithms can be beneficial for exploration dynamics. Future work includes testing in a wider range of environments to improve understanding of IMGEP-UGL algorithms. In environments with multiple controllable objects/entities, modular or active Goal Policies are more effective than random ones. Using Independently Controllable Factors for modular representations of Goal Spaces could be beneficial. The study focused on passive perceptual learning followed by autonomous goal exploration, inspired by infant development. The ability to incrementally learn an outcome space representation and explore the world is a stimulating topic for future work. The Cost Function to minimize involves encoder and decoder parts of the architecture, which can be optimized using various methods like Stochastic Gradient Descent, Adagrad, or Adam. Training deep networks with encoding-decoding layers can be challenging, but a successful approach is to greedily train each pair of layers. Variational Auto-Encoders (VAEs) propose learning a model by searching for parameters that condition observed data on random factors. The likelihood function is computed using maximum likelihood principles, but the integral is often intractable for Monte-Carlo sampling. Variational Auto-Encoders (VAEs) address the issue of intractable likelihood functions by introducing an arbitrary distribution q(z|x, \u03c7) to maximize the Evidence Lower Bound (ELBO). By minimizing the KL-Divergence between q(z|x, \u03c7) and the true posterior p(z|x, \u03c8), VAEs learn the parameters of conditional distributions as non-linear functions. This approach allows for training a neural network under certain conditions. Variational Auto-Encoders (VAEs) use a cost function to train a neural network by assuming Gaussian distributions for q(z|x, \u03c7) and p(z|\u03c8). The Path-wise Derivative estimator is employed for the ELBO. The cost function involves encoding and decoding parts of the architecture, with a penalty term for the divergence between q(z|x, \u03c7) and the assumed prior. Normalizing Flow addresses previous issues in the process. Normalizing Flow allows for more expressive prior distributions by chaining invertible transformations, enabling computation of expectations without precise knowledge of the distribution. This results in an additional term in the ELBO related to the log-determinant of the transformations. The Householder flow is a volume-preserving transformation with a log determinant of 1, allowing it to be used without modifying the loss function. Other types of parameterized transformations, such as radial flow, have also been proposed for encoding and decoding in architectures. The Inverse Autoregressive Flow was proposed as a more complex transformation method. Two environments were considered: Arm-Ball, where a 7-joint arm interacts with a ball in a visual environment, and Arm-Arrow, where the arm manipulates an arrow with single symmetry. The environment in the Arm-Arrow scenario is parameterized by two bounded continuous factors for arrow coordinates and one periodic continuous factor for orientation. Physical situations are represented by small images similar to the dSprites dataset. A robotic arm with 7 joints, controlled by DMPs with 21 continuous parameters, interacts in the environment for 50 time-steps. The learner observes changes in the environment caused by another agent, modeled by sampling a random state and generating an image observed by the learner. The Algorithmic Architecture 1 was instantiated into Algorithm 3 (RGE-), denoted for representation learning algorithms like (RGE-AE) for Auto-Encoders, (RGE-VAE) for Variational Auto-Encoders, and (RGE-RFVAE) Image."
}