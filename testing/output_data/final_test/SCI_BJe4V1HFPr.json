{
    "title": "BJe4V1HFPr",
    "content": "Existing methods for AI-generated artworks struggle with generating high-quality stylized content and separating fine-grained styles from various artists. A novel Generative Adversarial Disentanglement Network is proposed to disentangle two factors of variations and decompose complex anime illustrations into style and content. The model has two stages, one encoding input images into style-independent content and another based on a dual-conditional generator. It can generate high-fidelity anime portraits with various styles from over a thousand artists and vice versa, using a single end-to-end network for style transfer applications. Existing methods for AI-generated artworks struggle with generating high-quality stylized content and separating fine-grained styles from various artists. A novel Generative Adversarial Disentanglement Network is proposed to disentangle two factors of variations and decompose complex anime illustrations into style and content. The model can generate high-fidelity anime portraits with various styles from over a thousand artists using a single end-to-end network for style transfer applications, showing superior output compared to current state-of-the-art techniques. Style transfer can be seen as a general problem where two factors of variation are present, with one factor labeled. The goal is to disentangle and control these factors independently. A Generative Adversarial Disentangling Network is proposed to address this, utilizing adversarial training techniques. The approach involves training a style-independent content encoder and introducing a dual-conditional generator. This method outperforms existing techniques in disentangling artist-specific styles from anime portraits. Our method excels in generating artist-specific styles with detailed facial features and overall visual quality. It can disentangle between writer identity and digit class in the NIST handwritten digit dataset. Neural Style Transfer, as proposed by Gatys et al., decomposes images into content and style using deep neural networks. In style transfer research, various methods have been developed to combine content and style from different images. Approaches include using masks for guidance, finding dense correspondences between images, and representing style with affine transformation parameters. These methods aim to go beyond transferring texture statistics to capture high-level style semantics. The concept of \"style\" in style transfer is domain-dependent, and texture statistics alone are not sufficient. A better approach is to treat style transfer as an image-to-image translation problem. Isola et al. introduced image-to-image translation, with extensions like CycleGAN and DualGAN removing the need for supervised training. These methods have shown the ability to translate between photorealistic images and styles of artists like Van Gogh and Monet. To improve style transfer, a single set of networks can be used for multiple domains by encoding each domain into a common code space and generating images based on domain labels. This approach aims to disentangle content and style factors in images, treating them as a single large domain. In StarGAN, domain information is used for generating images in different domains, but lacks explicit content space for disentangling style and content. DC-IGN achieves clean disentanglement of factors in images but requires well-structured data, making it challenging for style transfer. The method of Chen et al. (2016) is unsupervised and can discover disentangled factors of variations from unorganized data. The problem faced is to enforce the meaning of disentangled factors (style and content) with only one factor controlled in the training data. Techniques in audio processing, like voice conversion, have similarities to this problem. Our approach, similar to (Chou et al., 2018), uses artists' identities as proxies for style due to the difficulty in obtaining fine-grained style labels. Divided into two stages, Stage 1 focuses on training an encoder to encode image content without style information using per pixel L2 distance for reconstruction loss. The approach uses artists' identities as proxies for style. Stage 1 trains an encoder to encode image content without style information using per pixel L2 distance for reconstruction loss. To prevent encoding style information, an adversarial classifier is added to classify the encoder's output by artist. A \"style code\" vector is learned for each artist and provided to the decoder for reconstruction. The approach uses artists' identities as proxies for style. The style function maps an artist to their style vector. The objective is to classify the generator's output based on the combination of content and style. Parameters are constrained using KL-divergence loss. The optimization objectives in Stage 2 involve a Dual-Conditional Generator that addresses the issue of blurry outputs in autoencoders by incorporating texture information. The approach is based on auxiliary classifier GANs, using least squares loss instead of binary cross entropy for the discriminator. The encoder, generator, and style function from Stage 1 are utilized, with a joint sampling of a and x while ensuring a's independence. In Stage 2, a new classifier C2 is trained to classify images to their correct authors, different from the Stage 1 classifier C1. The generator aims to generate samples classified as the artist it is conditioned on. Unlike previous works, the classifier in this approach does not optimize any loss function on generated images. In Stage 2, a new classifier C2 is trained to classify images to their correct authors. The classifier is trained to be uncertain about the class of generated samples by maximizing entropy. The classifier's loss on generated samples is defined as the \"negative log-unlikelihood\". Separate networks are used for C2 and the discriminator. The generated samples are required to be encoded back to their content input. E(\u00b7) is fixed in stage 2. In Stage 2, a new classifier C2 is trained to classify images to their correct authors. The classifier is trained to be uncertain about the class of generated samples by maximizing entropy. The classifier's loss on generated samples is defined as the \"negative log-unlikelihood\". Separate networks are used for C2 and the discriminator. The generated samples are required to be encoded back to their content input. E(\u00b7) is fixed in stage 2. C2 is pre-trained on real samples before stage 2 using anime illustrations from Danbooru 1 dataset. The NIST handwritten digit dataset is also used for experiments in appendix B. The training set includes 106,814 images from 1139 artists, with each artist having at least 50 images for training. The network architecture consists of residue blocks with one convolution per block and ReLU activations. The structure is almost identical across all networks, with minor differences in input/output layers. The sequence of blocks, output channels, and spatial size of output feature maps are detailed in table 1. The generator's sequence runs from right to left, while other networks run from left to right. Fully connected input/output layers are added on top of the common part for each network. The network architecture consists of residue blocks with one convolution per block and ReLU activations. Fully connected input/output layers are added for each network, with specific numbers of input/output features. Training involves weighting loss terms with hyperparameters and using RMSprop for weight updating in stage 2 to disentangle style and content in the generator's input code. The network architecture consists of residue blocks with one convolution per block and ReLU activations. Training involves weighting loss terms with hyperparameters and using RMSprop for weight updating in stage 2 to disentangle style and content in the generator's input code. The style code and content code control different aspects of the generated image, allowing for the generation of the same content in different styles. Additional results and discussions can be found in Appendix A. In Figure 4, style transfer results are compared to existing methods such as neural style transfer and StarGAN. While neural style transfer mainly applies the color of the style image to the content image, StarGAN captures some features but fails to transfer intricate style elements. The proposed method transfers the style of the target artist more faithfully. The classification accuracy on generated samples is not considered adequate as a quality measure. The generator can classify samples by style with 86.65% accuracy. Limitations include testing only on portraits and anime illustrations, with inconsistencies in small features. Discussions and ablation study details can be found in the appendices. The Generative Adversarial Disentangling Network enables semanticlevel artwork synthesis using a single generator. Style and content are effectively disentangled through a two-stage framework. The technique has been validated on various anime illustration styles, showing significant improvements in artistic semantics and visual quality compared to existing methods for style transfer. The method aims to improve artistic semantics and visual quality in style transfer beyond anime artworks. It also explores modeling entire character bodies and scenes. The results show comparisons between different methods, including the proposed method, StarGAN, and neural style. The justification for using Gram matrices of neural network features as a representation of style is that it captures statistical texture information. Style transfer is limited to texture statistics transfer, but there is more to style than just feature statistics, such as exaggerations in caricatures that cannot be captured by local texture statistics alone. Domain dependency is also a problem when transferring or preserving color in style. The problem of transferring or preserving color in style transfer is domain dependent. Color can be considered part of the style to transfer, but distinguishing between style and content can be ambiguous. Existing methods like luminance-only style transfer and color histogram matching offer options to keep or transfer color, but for more complex aspects, this choice may not be straightforward. In style transfer, the concept of \"style\" is domain-dependent and involves different ways of presenting subjects. Successful methods must adapt to the domain and use labeled style information, as simple feature-based approaches are not effective. Previous methods claiming to separate style and content have limitations. Previous methods in style transfer have made questionable claims about separating style and content, particularly when tested on collections of real photographs. The distinction between style and content may not make sense in datasets with only one style, like photorealistic images. Avoiding established GAN datasets like CelebA or LSUN, which consist of real photos, is necessary as the differences in style can be subtle and may not be recognizable to all readers. The reader is advised to pay attention to various style elements in images, such as saturation, contrast, shading, eye size, chin shape, hair details, and more. The method discussed in the text has shown improvement in style transfer results, although fine content details may not be preserved well. The method discussed in the text focuses on style elements in images like saturation, contrast, shading, eye size, chin shape, and hair details. It mentions that fine content details are not well preserved due to the architecture choice of using an encoder-decoder with a fixed-length content code. In contrast, StarGAN uses a fully convolutional generator with 2 down-sampling layers, allowing for richer information preservation. Previous works did not demonstrate random sampling of content, which this method does. However, fully convolutional networks tend to preserve image structures down to the pixel level. The text discusses the limitations of a fully convolutional network in preserving fine details like facial features and expressions. It suggests that non-rigid spatial transformation may be necessary for better performance. The choice of reconstruction loss function in stage 1 is also highlighted as a factor affecting the preservation of features like eye color and facial expression. The text discusses the limitations of a fully convolutional network in preserving fine details like facial features and expressions. It suggests that non-rigid spatial transformation may be necessary for better performance. The network prioritizes encoding large features due to limited code length, resulting in more consistent large color regions in the background. Additional tags for eye color and facial expressions could be utilized for training the encoder and conditioning the generator. Loss functions aligned with human perception of visual importance may be needed for general problems. The dataset contains images with heterochromia, which is more prevalent in anime characters than in real life, potentially forcing the encoder to use two different sets of features. The network can disentangle between digit class and writer identity using the NIST handwritten digit dataset, showing variation even when the same person writes the same digit. The NIST handwritten digit dataset shows variation in the appearance of digits written by the same person. The network aims to disentangle between digit class (D), writer identity (W), and other factors (R). Two experiments are conducted: disentangling D and W + R with only digit labels, and disentangling W and D + R with only writer labels. The network has 3 levels with 64, 128, and 256 channels, and features of 16, 128, and 32 dimensions for digit class, writer identity, and other factors respectively. In the experiment, the labelled feature dimensions for W vs. D + R were 128 and 48 respectively, while for D vs. W + R they were 16 and 160. Samples of digits and writers are shown, demonstrating control over style and content independently. In the experiment, the output distribution of E(\u00b7) was visualized to disentangle W from D + R using Nested Dropout for automatic code dimension learning. The distribution of E(x) over the dataset was projected to the first two dimensions, showing digit class representation. A comparison was made with a vanilla VAE trained on the same dataset. Our E(\u00b7) produced a distribution where images of the same digit are closely clustered while different digits are separated. With only a two-dimensional feature space, we achieved 10 distinct clusters compared to a vanilla VAE where three digits were confused. By removing writer information, variation within the same digit class was reduced, leading to a clustering effect without knowing digit labels during training. This method is not limited to style and content separation. The text discusses disentangling style and content features in generated samples of digits and writers. It compares the variation in samples based on style and content, showing that the combination of style and writer information leads to more dramatic variation. In figure 7, variation in each row is due to combined effect of W and R, showing significant variation in multiple images of the same digit by the same writer. Figure 8 compares samples by the same writer from training dataset with samples generated by the generator. By fixing W and D and changing R, similar variations can be achieved. The distribution of E(x) is analyzed to purge digit class as a labeled feature from the encoder. The distribution of each digit should be indistinguishable in the encoder after purging the labeled feature. The encoder aims to encode unlabelled features while avoiding encoding the labeled feature, ensuring different distributions for each unlabelled class and similar distributions for each labeled class in the code space. Two metrics are used for evaluation: average Euclidean distance to the class mean and performance of a naive Bayesian classifier using the code. The average distance metric is used to evaluate the performance of a disentangling encoder trained on writer and digit labels. The encoder should cluster samples of the same digit more tightly together than the whole dataset, while samples by the same writer should not be noticeably more tightly clustered. Different results are expected for encoders trained on writer (E W) and digit (E D) labels. The evaluation includes computing the average Euclidean distance of a sample to the center of its class. The encoder's output is truncated to two or eight dimensions, with the most distinctive information encoded in the first few dimensions. The results show that E W causes samples to cluster tightly by digit, while E D has the opposite effect. E D causes samples to cluster somewhat more tightly by writer. In the encoder's output space, classification performance is studied using a naive Bayesian classifier with each class modeled by a multivariate normal distribution. Samples are classified by writer and by digit using different dimensions of the code space. Results are presented in tables showing average probability, rank, and classification accuracy. The encoder's output space is analyzed for classification performance using a naive Bayesian classifier. Results in tables show average probability, rank, and accuracy. Different features produced by E W and E D have varying top-1 accuracy rates. The method in (Chou et al., 2018) did not work well in experiments, leading to further discussion in Section 3. The method in (Chou et al., 2018) performed poorly in experiments due to the unconstrained distribution of E(x), allowing the encoder-decoder to encode style information while tricking the classifier. By replacing certain weights in the autoencoder, the code distribution can be linearly transformed without changing the function computed. This coordination between E(\u00b7) and G(\u00b7, \u00b7) parameters can alter the code distribution while maintaining reconstruction accuracy. The encoder-decoder can transform the code distribution constantly during training, making it difficult for the classifier to infer the artist from style information. Constraining the code distribution of the whole training set does not help, as it would still encode style information. This problem can be alleviated by linearly transforming the code distribution without changing the function computed. The problem of conflicting goals between reconstruction and style information in the encoder-decoder model can be alleviated by classifying the reconstruction result instead of the code distribution. The proposed method allows the classifier to see the image generated by the encoder's output and the style of a different artist, ensuring that style information is not encoded. The encoder-decoder model addresses conflicting goals by classifying the reconstruction output instead of the code distribution. By constraining the output distribution of the encoder with a KL-divergence loss, unnecessary style information is discouraged. An alternative approach involves using a multi-layer perceptron as a classifier, comparing images generated with different inputs. The encoder-decoder model focuses on reconstructing images with the correct style code while generating style-neutral images with a zero style code. Using an MLP for classifying the input image, the method successfully preserves key features like eye size and facial contour, outperforming conventional VAE reconstruction. The encoder-decoder model successfully reconstructed images with the correct style code using an MLP classifier, outperforming conventional VAE reconstruction. The performance of the MLP classifier was hindered by instabilities in the encoder's output distribution, as evidenced by tracking changes in the distribution during training. The encoder's distribution was visualized from iteration 200,000 to iteration 300,000 at intervals of 20,000 iterations. The output distribution of the vanilla VAE encoder remained stable, while the encoder trained with an MLP classifier showed instability introduced by adversarial training. The stage 1 encoder had a more stable output distribution, with digit classes fluctuating within a small range. The encoder trained with an MLP classifier clustered the same digit better than VAE but did not separate between different digits clearly. In style-conditional GAN training, the discriminator is adversarial to ensure the classifier learns all aspects of an artist's style, preventing it from focusing on specific features. This approach encourages the classifier to distinguish between real and generated samples based on various style elements, promoting a more comprehensive understanding of the artist's style. The effect of making the classifier adversarial in style-conditional GAN training is studied by comparing it to a non-adversarial classifier. Removing a specific loss component from the classifier's training process results in generated samples that may lack likeness. The choice of using \"negative log-unlikelihood\" as the classifier's loss is explained, highlighting concerns about its lack of upper-bound. The log-likelihood is not upper-bounded in training C2(\u00b7) using a specific objective. This leads to the classifier ignoring real samples and focusing on increasing negative log-likelihood on generated samples, causing issues with learning correct styles. In contrast to stage 1, where the encoder E(\u00b7) maximizes negative log-likelihood against C1(\u00b7), avoiding similar problems. The log-likelihood is not upper-bounded in training C2(\u00b7) using a specific objective, leading to issues with learning correct styles. The accuracy of classification by style on generated samples cannot properly assess the success of style transfer. The accuracy of classification on generated samples does not guarantee successful style transfer. The method has not yet achieved high accuracy against an adversarial classifier. Training data was used efficiently without a separate test set, generating samples randomly from learned content distribution. During stage 2 training, content codes are drawn randomly from a learned distribution modeled by a multivariate normal distribution. The network only ever sees a fixed set of content codes. Generated samples from two classifiers show different classification accuracies. Randomly drawn content codes serve as a separate test set, ensuring unseen content. During stage 2 training, content codes are used to condition image generation. The approach differs from previous methods by explicitly using content loss to guide image creation. The generated samples are classified differently by various classifiers, with caution needed when interpreting results. The content codes are randomly drawn from a learned distribution, ensuring unseen content during testing. During stage 2 training, content codes are used to condition image generation. The explicit condition on content by content loss is shown to be necessary. When \u03bb cont = 0, style and content control can no longer be independent. The content code loses its ability to control the content of the generated image, while the style code controls both style and content. The content code still influences head pose, indicating a form of \"partial mode collapse.\" During stage 2 training, the content code initially controls the content of generated images, but this ability is later lost, leading to partial mode collapse. Despite being initialized with weights from stage 1, where content control is possible, some input variables have little effect on the output. Therefore, explicit content conditioning through content loss is deemed necessary in our approach."
}