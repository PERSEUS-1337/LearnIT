{
    "title": "B1grSREtDH",
    "content": "Informed decision making for robots in uncertain environments is crucial. A scalable solution, Bayesian Residual Policy Optimization (BRPO), combines clairvoyant experts' advice with a Bayesian residual policy to improve decision-making and reduce uncertainty. This approach enhances the ensemble of algorithms for physical tasks alongside people. Bayesian Residual Policy Optimization (BRPO) significantly improves decision-making for robots in uncertain environments by combining clairvoyant experts' advice with a Bayesian residual policy. This approach outperforms existing adaptive RL methods and enhances the ensemble of algorithms for physical tasks alongside people. Bayesian RL focuses on agents being optimal with their current uncertainty over latent MDPs. While solving the belief MDP is challenging, solving individual latent MDPs is more manageable. This approach simplifies decision-making for robots in uncertain environments. Learning Bayesian corrections on top of clairvoyant experts is a scalable strategy for solving complex reinforcement learning problems by weighing policy proposals based on belief and exploring effectively to collapse uncertainty. Bayesian Residual Policy Optimization (BRPO) is a scalable Bayesian RL algorithm that outperforms ensemble of experts and existing adaptive RL algorithms by learning to take uncertainty-reducing actions based on expert recommendations and belief over latent MDPs. Belief-Space RL methods involve Bayesian reinforcement learning with a prior distribution over possible MDPs. Approximations are necessary for the Bayes-optimal policy, with approaches like SARSOP and PBVI for discrete state actions, and sampling methods like BAMCP and POMCP for continuous state actions. Online approaches aim to eventually act optimally by efficiently gaining information about the test time MDP. Belief-Space RL methods involve Bayesian reinforcement learning with a prior distribution over possible MDPs. Approaches like BEB and POMDP-lite introduce auxiliary rewards for exploration and PAC optimality. Various non-Bayesian curiosity-based heuristics for reward gathering have been explored. Online exploration techniques, such as posterior sampling and UP-OSI, aim to minimize regret. Methods like LSTM encoding and expert value function gating are used to compress belief MDP problems. Meta-reinforcement learning methods like MAML and RL2 train sample-efficient learners by exploiting common structure in a distribution of MDPs. BPO and BRPO utilize belief distributions to learn policies, with BRPO using an expert to handle complex tasks. MRL approaches have ties to Bayesian Reinforcement Learning, addressing pertinent problems in reinforcement learning. Our work is closely related to Bayesian MRL approaches such as MAML-HB, BMAML, PLATIPUS, and PEARL. Unlike other approaches, we utilize experts at test time to optimally correct them. Residual learning, rooted in boosting, is also employed in our methodology. Boosting in reinforcement learning involves combining weak learners to create a strong learner, leveraging known models by learning residuals, and improving policies over hand-defined ones. Our approach utilizes Bayesian reinforcement learning with a latent variable \u03c6 \u2208 \u03a6 to improve agent performance without prior knowledge of reward and transition functions. Bayesian RL with a latent variable \u03c6 \u2208 \u03a6 considers uncertainty over \u03c6 in the MDPs. The Bayes-optimal action value function and Bayesian reward function are defined based on belief distribution b \u2208 B. Posterior updates are computed recursively starting from initial belief b0. Our algorithm utilizes a black-box Bayes filter to produce a posterior distribution over the latent states, which can also be seen as compressing the history of states and actions. Recent work suggests that LSTM cells can be trained to compress history and predict future states. Posterior Sampling Reinforcement Learning (PSRL) is an online RL algorithm that maintains a posterior over latent MDP parameters, but our focus is on zero-shot scenarios where latent parameters are resampled for each episode. Posterior Sampling Reinforcement Learning (PSRL) samples an MDP before each episode, updates its posterior, and focuses on learning the Bayes-optimal policy for exploration and exploitation in new environments. This algorithm is crucial for Bayesian RL problems where sensing is essential for performance. In Bayesian Residual Policy Optimization, an ensemble of clairvoyant experts is used to make recommendations for exploration and exploitation in new environments. A Bayesian residual policy is trained to correct these recommendations, balancing exploration and exploitation. The agent adjusts its actions based on the effectiveness of the recommendation and learns to explore effectively when uncertainty is high. The problem involves k underlying latent MDPs, with clairvoyant experts computed for each using single-MDP RL methods. In Bayesian Residual Policy Optimization, an ensemble of clairvoyant experts is used to make recommendations for exploration and exploitation in new environments. The ensemble recommendation combines experts' beliefs to map state and belief to a distribution over actions. The recommendation can be computed by maximizing a lower bound using Jensen's inequality, especially when dealing with multimodal distributions. The resulting action is reliable as entropy reduces, with alternatives to consider such as Gaussian distributions from TRPO. The ensemble recommendation in Bayesian Residual Policy Optimization is reliable but not Bayes-optimal. Clairvoyant experts navigate optimally without sensing, while a Bayes-optimal agent would take sensing actions. BRPO collects trajectories by simulating the current policy on MDPs sampled from the prior distribution, querying the ensemble for action recommendations. The Bayes filter updates the posterior after executing the recommendation, which is combined with the correction from the residual policy network. The residual policy operates in a residual belief MDP by shifting actions based on expert recommendations, defining a new transition dynamics. The experts are fixed, so the residual belief-MDP remains constant. The residual belief-MDP is fixed during training and testing, inheriting mathematical guarantees from the policy optimization algorithm. BRPO improves sensing strategies in robotics domains, learning to sense cost-effectively. In robotics domains, BRPO can develop more effective sensing strategies for identifying latent goals in Maze environments. The agent must minimize sensing costs by obtaining better measurements when closer to goals. The agent observes its position, velocity, and distance to goals, with noisy measurements of the goal distance. In Maze environments, the agent observes noisy distance to goals, categorical belief distribution, and expert recommendations. Experts propose actions to reach goals without knowing penalties for incorrect goals. Maze4 rewards reaching the active goal with 500 points and penalizes reaching incorrect goals with -500 points. Maze10 penalizes incorrect goals with -50 points and allows continued exploration. Trained BRPO agents demonstrate rollouts in Maze4 and Maze10, where close goals have different paths. In Maze environments, the agent navigates through Latent goal mazes with four (Maze4) and ten (Maze10) possible goals. The agent reroutes itself while invoking sensing to improve belief accuracy. Sensing returns a noisy binary vector for all four doors in a classical POMDP problem. The agent can sense or crash into doors to check for the goal, with accuracy decreasing with distance. The agent changes direction as goals become less likely, marked with a true goal in red for clarity. The agent in Maze environments navigates through Latent goal mazes with four possible goals. It senses near doors to improve belief accuracy, with the red bars indicating the probability of a blocked door. The agent observes its position, velocity, distance to goal, and door interactions. BRPO's learned policy differs from experts by getting close to walls to minimize sensor noise. BRPO navigates through maze environments by getting close to walls to minimize sensor noise and identifying open doors to navigate through. It is compared to adaptive RL algorithms like BPO and UP-MLE, as well as an ensemble of experts baseline. The ensemble baseline strengthens by sensing with a probability of 0.5 at each timestep. More sophisticated sampling strategies are discussed in the appendix. Training performance of all algorithms is compared across three environments in Figure 4a. The training performance of algorithms in different environments is compared in Figure 4a. BRPO outperforms BPO and UP-MLE agents, even with additional bonuses for information-gathering. However, BPO and UP-MLE only partially solve the task in Maze4 and Maze10, and struggle in Door4. BRPO learns to sense when goals need to be distinguished and reroutes itself accordingly. The BRPO agent outperforms BPO and UP-MLE in different environments, including Maze4 and Maze10. It avoids crashing into doors, unlike UP-MLE, which relies on crashing to reduce uncertainty. The BRPO policy incorporates belief distribution, state, and ensemble recommendation as inputs. Providing both belief and recommendation to the policy is important for faster learning, especially on Door4. BRPO maximizes the Bayesian Bellman equation, incorporating exploration into its long-term objective without needing auxiliary rewards. The BRPO agent incorporates belief distribution, state, and ensemble recommendation as inputs, maximizing the Bayesian Bellman equation for faster learning. Adding auxiliary rewards for exploration, like intrinsic or surprisal rewards, can lead to over-exploration and underperformance, as seen in Door4 with a high reward bonus. BPO and UP-MLE struggle to learn without an exploration bonus, as observed in Maze4. Bayesian Residual Policy Optimization (BRPO) is a new algorithm that focuses on learning to explore complex latent MDPs by operating on the residual belief-MDP space. It outperforms existing BRL algorithms and POMDP solvers, which struggle with problems involving uncertainty and a large set of MDPs. BRPO is a new algorithm that excels in solving complex problems and outperforms existing BRL algorithms. Key challenges include efficiently constructing an ensemble of experts and developing an efficient Bayes filter. Future directions involve subdividing the latent space and computing a diverse set of policies. BRPO is a new algorithm that excels in solving complex problems by efficiently computing a set of experts. Bayesian reinforcement learning and posterior sampling address different problems, highlighted through a toy problem scenario. The algorithms are compared in a multi-episode setting to contrast performance. In a multi-episode setting, the agent interacts with the same MDP repeatedly using posterior sampling. PSRL samples an MDP before each episode, computes the optimal policy, and updates the posterior after each episode. PSRL avoids sensing actions to reduce uncertainty since the sampled MDP has no uncertainty. PSRL makes N-1/2 mistakes before finding the correct gold location, resulting in cumulative rewards over T episodes. In a multi-episode setting, the agent uses posterior sampling to interact with the same MDP repeatedly. PSRL avoids sensing actions to reduce uncertainty and makes mistakes before finding the correct gold location. The Bayes-optimal policy outperforms naive algorithms like BPO by efficiently balancing sensing and navigation tasks. BRPO leverages a set of experts to simplify learning, focusing on balancing sensing and navigation during training. The performance gap between Bayes-optimal policy and posterior sampling grows exponentially with tree depth. PSRL is an online learning algorithm designed for domains where the posterior updates with multiple interactions. It focuses on improving performance over episodes, unlike average or zero-shot performance. A more effective sensing ensemble baseline policy can be manually designed and used to enhance the BRPO agent. Offline tuning on the Maze10 environment showed that a better ensemble baseline agent only senses for the first 150 timesteps out of 750, with an average return of 416.3 \u00b1 9.4. The trained BRPO agent outperforms the original ensemble baseline with an average return of 416.3 \u00b1 9.4, but falls short of the BRPO agent starting with the original ensemble, which achieved an average return of 465.7 \u00b1 4.7 and a 100% task completion rate. The suboptimality of the ensemble recommendation is attributed to experts being unaware of penalties for incorrect goals. PSRL was evaluated on the Maze10 environment with modifications to handle the zero-shot scenario, sampling from the posterior at each timestep. Augmenting the PSRL agent by sensing with probability 0.5 results in an average return of 464.1 \u00b1 5.5 and a task completion rate of 94%. Failures occur when the posterior does not collapse to a single target for the posterior-sampled experts to navigate toward."
}