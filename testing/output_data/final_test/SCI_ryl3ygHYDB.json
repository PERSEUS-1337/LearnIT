{
    "title": "ryl3ygHYDB",
    "content": "Magnitude-based pruning is a simple method for reducing neural networks. Lookahead pruning, an extension of this method, outperforms magnitude pruning in high-sparsity scenarios across various networks like VGG and ResNet. The \"magnitude-equals-saliency\" approach, often overlooked, is effective in eliminating unnecessary weights from neural networks. Various methods have been proposed for pruning neural networks, including magnitude-based pruning (MP) and more sophisticated alternatives like Bayesian approaches, pregularization, sharing redundant channels, and reinforcement learning. Recent studies suggest that MP, combined with iterative pruning and dynamic reestablishment of connections, can achieve state-of-the-art results. Recent studies by Gale et al. (2019) and Frankle & Carbin (2019) highlight the effectiveness of magnitude-based pruning (MP) in achieving a balance between sparsity and accuracy in neural networks. They suggest that MP can automatically discover efficient subnetworks within overparametrized networks, calling for a reevaluation of the \"magnitude-equals-saliency\" approach. To better understand MP methods, a study explores a generalization of magnitude scores through a functional approximation framework, viewing MP as a distortion minimization process by zeroing out parameters in layerwise operators. In a novel pruning strategy called lookahead pruning (LAP), a distortion minimization problem is extended to multiple layers by considering the impact of pruning on neighboring layers. LAP consistently outperforms magnitude-based pruning (MP) in various network setups, achieving over \u00d72 compression rate gains with increasing benefits in high-sparsity scenarios. LAP is easy-to-use and agnostic to model and data, offering additional attractive properties. The proposed LAP is a simple score-based approach for pruning that does not rely on training data and has no hyper-parameters to tune. It can be used in conjunction with algorithmic tweaks developed for magnitude-based pruning, offering versatility in network compression. The LAP algorithm is a generalization of magnitude-based pruning (MP) for multiple layer setups. MP removes connections with smallest absolute weights until desired sparsity is achieved. LAP aims to minimize Frobenius distortion and has no hyper-parameters to tune. The MP algorithm can be interpreted as a worst-case distortion minimization approach, inspired by the work of Dong et al. (2017). For fully-connected layers, the output distortion caused by pruning weights can be bounded by the Frobenius distortion. Similarly, for convolutional layers, the linear operator can be represented as a doubly block circulant matrix constructed from the kernel tensor. The Frobenius distortion of these matrices can be controlled by the weight distortion. The Frobenius distortion of weight tensors in convolutional layers can be controlled by using block circulant matrices. The myopic optimization approach falls short in minimizing distortion in linear neural networks. A block approximation of magnitude-based pruning objective is considered for L-layer neural networks with weight tensors W1, ..., WL. For pruning an L-layer neural network, a score-based pruning algorithm called lookahead pruning (LAP) is proposed to minimize the Frobenius distortion of weight tensors. This approach considers weight tensors of neighboring layers in addition to the original weight tensor for each layer. The explicit minimization of block distortion is computationally intractable, leading to the use of LAP as an approximation method. The lookahead pruning algorithm (LAP) minimizes distortion of weight tensors in an L-layer neural network by considering weight tensors of neighboring layers. Lookahead distortion measures the distortion induced by pruning a weight while keeping other weights intact. Formula for three-layer blocks simplifies to a compact form for fully-connected and convolutional layers. LAP computes lookahead distortion for all weights. The lookahead pruning algorithm (LAP) minimizes distortion of weight tensors in an L-layer neural network by considering weight tensors of neighboring layers. LAP computes lookahead distortion for all weights and outperforms the Minimum Permutation (MP) method in terms of performance before retraining, especially on linear fully-connected networks. Remarkably, LAP pruned models show increasing test accuracy from 91.2% to 92.3% as sparsity level rises, outperforming MP which only achieves 71.9% accuracy. LAP maintains superiority even after retraining, especially at higher sparsity levels. LAP can be applied to nonlinear networks with activations like ReLU, proving effective in over-parametrized scenarios. In over-parametrized scenarios, LAP can prune connections in a two-layer fully-connected network with ReLU activations. It approximates the distortion introduced by pruning a weight by decoupling the probability of neuron activation from connection pruning. LAP assumes i.i.d. neuron activations due to a lack of additional training data access. The LAP-act variant of pruning is described in Appendix F, with comparisons to the OBD baseline. The lookahead distortion for neural networks with nonlinear activation functions is justified by recent discoveries on implicit bias from stochastic gradient descent. LAP is shown to be effective for sigmoids and tanh activations in Section 3.1. Batch normalization aims to normalize layer outputs per batch. Based on our functional approximation perspective, batch normalization in a neural network does not affect magnitude-based pruning. However, when considering lookahead distortion, batch normalization must be taken into account for assessing connection importance. Lookahead pruning can be adjusted to account for batch normalization by pruning connections with a minimum scaling factor value. The LAP algorithm considers different variants in terms of lookahead direction, order of pruning, and sequential pruning methods. Along with the \"vanilla\" LAP, six variants are considered, including mono-directional LAPs that prune layers by considering both preceding and succeeding layers. Looking forward only considers the succeeding layer, while looking backward only takes into account the expected structure of input coming into the present layer. The LAP algorithm explores variants in lookahead direction, order of pruning, and sequential pruning methods. Variants include LFP and LBP, LAP-forward, LAP-backward, and sequential LAP methods. The order of pruning is tested, with options for forward or backward pruning. Sequential pruning divides the pruning budget into steps for gradual weight reduction. In this section, LAP variants are compared with MP, validating LAP's applicability to nonlinear activation functions and testing LAP on various neural network architectures including VGG, ResNet, and Wide ResNet. Five neural network architectures are considered, including a fully-connected network with four hidden layers and a convolutional network with six convolutional layers. In this section, various neural network architectures including VGG, ResNet, and Wide ResNet are used. The networks have different configurations such as VGG-19 with batch normalization layers, ResNets of depths {18, 50}, and WRN-16-8. The models are trained on different datasets for image classification tasks. The focus is on one-shot pruning of MP and LAP, with results averaged over five trials. More details on setups are provided in Appendix A. The performance of LAP is compared with MP on FCN using different activation functions. LAP consistently outperforms MP, even with sigmoidal activation functions. LAP shows better test accuracy before retraining with nonlinear activation functions. The test accuracy of pruned FCN using ReLU on MNIST dataset before retraining shows that LAP outperforms MP. LAP retains original test accuracy until only 38% of weights survive, with less than 1% performance drop at 20% weights remaining. MP requires 54% and 30% to achieve similar performance. LAP and its variants are evaluated on FCN and Conv-6 trained on MNIST and CIFAR-10. Experimental results are summarized in Table 1 for FCN and Table 2 for Conv-6, compared with MP and random pruning (RP). In comparison to random pruning (RP) and MP, LAP consistently outperforms both methods with similar or lower variance. LAP shows significant performance gains in extreme sparsity scenarios, with over 75% gain on FCN and 14% on Conv-6. The performance improvement is attributed to better training accuracy rather than generalization. Among mono-directional lookahead variants, LFP performs better in low-sparsity regimes while LBP excels in high-sparsity regimes; however, LAP outperforms both methods. In ordered pruning, LAP-forward is akin to LBP as they prioritize layers closer to the input, with LAP consistently performing better. In the high-sparsity regime, LAP-forward outperforms LAP-backward, while in the low-sparsity regime, LAP-backward performs better. The importance of preserving the input signal is higher in high sparsity levels due to signal scarcity. Employing forward/backward ordering and sequential methods leads to better performance, especially in high-sparsity scenarios. No clear benefit is seen in adopting directional methods in low-sparsity situations. In the low-sparsity regime, LAP shows marginal or unreliable performance gains compared to other methods. Experimental results on various models show LAP consistently achieves higher accuracy levels than MP at all sparsity levels, with test accuracies decaying at a slower rate. Models pruned by LAP retain test accuracies of 70-80% even with less than 2% of weights. In this study, models pruned by LAP maintain test accuracies of 70-80% with less than 2% of weights, while MP pruned models drop below 30% accuracy. LAP-forward shows limited advantages over LAP, with marginal gains in high sparsity but consistently worse in low sparsity. The approach interprets magnitude-based pruning to minimize Frobenius distortion and introduces a novel lookahead pruning (LAP) scheme for efficient multi-layer operation. The LAP scheme is proposed as an efficient algorithm for optimization in neural networks, extending to nonlinear networks and minimizing distortion. Different sparsity ranges are used for various network architectures, and the optimization problem is shown to be NP-hard. The optimization problem in Eq. (3) is proven to be NP-hard by reducing it from a binary quadratic programming problem. By reformulating the problem and introducing certain equalities, it is shown to be a special case of Eq. (11). This completes the reduction from the initial problem. In this section, the derivation of Eq. (5) for fully-connected layers is provided. The Jacobian matrix of the linear operator for a fully-connected layer is the weight matrix itself. The matrix product is decomposed in terms of entries of the weight matrix, leading to the conclusion of Eq. (5) for fully-connected layers. In Section 2.1, a two-layer fully connected network with ReLU activation is discussed. The lack of knowledge about training data is reflected in the design of LAP. Lookahead pruning with activation (LAP-act) extends LAP by pruning weights based on activation probabilities. The derivation of LAP-act is provided in Appendix F.1, with preliminary empirical validations in Appendix F.2. In Section 2.1, LAP-act is discussed as an extension of LAP for pruning weights based on activation probabilities. Empirical validations are performed in Appendix F.2, showing LAP-act outperforms OBD in small networks. The observation allows for decoupling the probability of activation of each neuron from pruning connections. A random distortion is constructed following the philosophy of linear lookahead distortion. The text discusses approximating the root mean-squared lookahead distortion by applying the mean-field approximation to neuron activation probabilities. This allows for easy computation of lookahead distortion with ReLU nonlinearity or three-layer blocks using rescaled weight matrices. The derivation of Eq. (18) is completed by comparing the performance of three algorithms during the pruning phase: optimal brain damage (OBD), LAP using OBD (OBD+LAP), and LAP-act. Hessian diagonal computation for OBD and OBD+LAP is done using the \"BackPACK\" software package. These algorithms are also evaluated for global pruning experiments. The experimental results for FCN and Conv-6 show that OBD performs better than algorithms relying solely on model parameters for pruning. Applying lookahead criterion to OBD significantly enhances performance in the high sparsity regime. LAP-act consistently exhibits better performance compared to OBD, utilizing information about activation probabilities of each neuron. The average running time of LAP-act is significantly less than OBD/OBD+LAP, with the gap widening as the dataset dimensionality increases. MP involves computing the absolute value of the tensor, sorting the values, and selecting a cut-off threshold to zero out weights. LAP follows the same steps as MP, with Step (1) replaced by computing activation probabilities. The squared lookahead distortion can be computed efficiently using tensor operations for fully-connected and convolutional layers. The implementation was done in PyTorch with a fixed layerwise pruning rate of 90%, using 40 CPUs for computations. The time required for LAP did not exceed 150% of the time required for MP, confirming LAP's computational benefits. Most of the added computation in LAP comes from batch normalization factors, without which the added load is \u22485%. LAP is compared to MP in unstructured pruning, while MP in channel pruning removes channels with the smallest aggregated weight magnitudes. The comparison between LAP-based and MP-based channel pruning methods on Conv-6 and VGG-19 networks using CIFAR-10 dataset shows LAP consistently outperforming MP. LAP-2 performs better than LAP-1 by a small margin. No similar dominance is observed among MP-based methods. Among MP-based methods, pruning a fraction of weights with smallest scores is done, and the normalized versions outperform the unnormalized ones. LAP and its variants perform better than global pruning baselines like MP-normalize and OBD. LAP-normalize also outperforms MP with pre-specified layerwise pruning rates. In comparison to MP with pre-specified layerwise pruning rates, LAP-all shows similar performance to LAP but underperforms in high-sparsity scenarios. The shortfall may be due to ignoring the impact of activation functions, leading to a loss of benefits. Further analysis on the optimal \"sight range\" of LAP is suggested for future research. Additionally, the performance of large neural networks pruned using MP and LAP is compared to that of a small network like VGG-16, VGG-19, and ResNet-18 trained on CIFAR-10 dataset. In this section, the benefits of sub-networks discovered by LAP are discussed, focusing on generalizability and expressibility. A plot of test accuracies and generalization gap for FCN trained with MNIST dataset is presented. The plot of generalization gap for FCN trained with MNIST dataset suggests that LAP may not always result in a smaller generalizability. However, LAP-pruned models show similar benefits to MP-pruned models in terms of sparsity and generalizability. The observation of weight flow in neurons hints at the importance of connections and the abstract notion of neuronal importance. The multiplicative factors in Eq. (5) consider the neuronal importance score, assigning significance to connections with more gradient signals. LAP reduces to magnitude-based pruning without these factors."
}