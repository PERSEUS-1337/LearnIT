{
    "title": "S1m6h21Cb",
    "content": "The Wasserstein probability metric, unlike the Kullback-Leibler divergence, captures underlying geometry between outcomes. It has been valuable in ordinal regression, generative modeling, and reinforcement learning. Three key properties of probability divergences for machine learning are discussed: sum invariance, scale sensitivity, and unbiased sample gradients. While the Wasserstein metric has the first two properties, it lacks the third. Empirical evidence suggests this is a significant issue. An alternative proposed is the Cram\u00e9r distance. The Cram\u00e9r distance is proposed as an alternative to the Wasserstein metric, combining the best of Wasserstein and Kullback-Leibler divergences. Empirical results comparing these divergences are provided, and a new algorithm called the Cram\u00e9r Generative Adversarial Network (GAN) is introduced, showing desirable properties over the Wasserstein GAN. The KL divergence is commonly used in machine learning to assess probabilistic models, but it has limitations as it only considers relative probabilities, not closeness of outcomes. To address this, the Wasserstein metric is used, which incorporates the underlying geometry between outcomes and can be applied to distributions with non-overlapping supports. In this paper, the authors highlight issues with using the Wasserstein distance in deep learning due to biased gradients, proposing the Cram\u00e9r distance as an unbiased alternative. They demonstrate the quantitative difference between the two metrics in machine learning scenarios. The section discusses the use of the energy distance as a multivariate generalization of the Cram\u00e9r distance in machine learning scenarios, distinguishing it from other probability distances like the Kullback-Leibler divergence. It introduces notation for probability distributions and expectations with respect to continuous and discrete distributions. The text discusses probability distributions, divergence measures like the Kullback-Leibler divergence, and probability metrics. It introduces the concept of probability distance as a symmetric divergence. The text introduces the concept of probability distance as a symmetric divergence satisfying the relaxed triangle inequality. It focuses on the p-Wasserstein metrics, particularly the 1-Wasserstein metric, which is commonly used in practice and has a dual form known as the integral probability metric (IPM). The text discusses the difference between the KL divergence and the Wasserstein metric, highlighting that the latter is sensitive to both probability changes and the geometry of outcomes. It introduces the concept of an ideal divergence that is scale-sensitive and sum-invariant, as defined by Zolotarev (1976). Ideal divergences, as defined by Zolotarev (1976), possess sensitivity and invariance properties. They are illustrated using Dirac functions at different values of x. Scale-sensitive divergences exhibit specific relationships between outcomes. BID2 highlighted their importance in reinforcement learning, particularly in the contraction property of the distributional Bellman operator. In machine learning, divergences are often viewed as loss functions. In machine learning, divergences are seen as loss functions. The concept of unbiased sample gradients is crucial for convergence in stochastic gradient descent. The text discusses the properties of the KL divergence and Wasserstein metric in terms of unbiased sample gradients and scale sensitivity. The KL divergence has unbiased sample gradients but is not scale sensitive, while the Wasserstein metric is ideal in terms of scale sensitivity but lacks unbiased sample gradients. The text highlights issues with gradients of the sample Wasserstein loss and provides theoretical evidence. The text discusses biases in estimating a Bernoulli distribution parameter using stochastic gradient descent with the sample Wasserstein loss. It shows that the estimate is biased even in simple settings and provides a lower bound on the bias. Additionally, it reveals that the minimum of the expected empirical Wasserstein loss is not the same as the minimum of the Wasserstein loss, indicating potential issues with minimizing the sample Wasserstein loss using stochastic gradient descent. Minimizing the sample Wasserstein loss by stochastic gradient descent may fail to converge to the true loss minimum. Theorem 1 highlights non-vanishing minimax bias, wrong minimum of sample Wasserstein loss, and deterministic solutions to stochastic problems. It suggests challenges in minimizing Wasserstein loss using stochastic methods. Theorem 1 reveals challenges in minimizing the Wasserstein loss using stochastic gradient descent methods. The existence of a biased optimization procedure for this loss remains an open question, despite its prevalence in empirical studies. This bias may contribute to learning instability and poor convergence, as noted in previous research. The authors propose a two time-scale adversarial procedure for the dual loss, maximizing it with m samples and taking a stochastic gradient step w.r.t. \u03b8. This approach provides unbiased gradients as m \u2192 \u221e but at a higher cost. They introduce the Cram\u00e9r distance as an alternative to the Wasserstein metric, offering unbiased sample gradients and potential for machine learning applications. The Cram\u00e9r distance is proposed as an alternative to the Wasserstein metric for machine learning applications. It emphasizes the importance of certain distributions and possesses unique properties compared to other metrics. The Cram\u00e9r distance is introduced as a substitute for the Wasserstein metric in machine learning. It has unbiased sample gradients and combines benefits of the Wasserstein metric and SGD-friendliness of the KL divergence. The Cram\u00e9r distance is unique in having this property among all l p distances. The Cram\u00e9r distance is a substitute for the Wasserstein metric in machine learning, with unbiased sample gradients and benefits of the Wasserstein metric and SGD-friendliness of the KL divergence. The underparametrized discrete distribution Q \u03b8 assigns the same probability to x = 1 and x = 10, with distributions minimizing various divergences. The Cram\u00e9r solution is close to the 1-Wasserstein solution, while the minimizer of the sample Wasserstein loss provides a bad solution. Gradient descent is used to minimize true or sample losses with a fixed step-size in the stochastic setting, where small sample sizes cause stochastic gradient descent to fail. In an ordinal regression task using different divergences, minimizing the sample Wasserstein loss leads to worse performance compared to minimizing the Cram\u00e9r distance. This was demonstrated in a neural network trained on the Year Prediction MSD dataset, where predicting the year a song was written was the task. The results showed that increasing sample size may not guarantee good behavior in minimizing the sample Wasserstein loss. Minimizing the Cram\u00e9r distance yields the lowest RMSE and Wasserstein loss, showing the importance of unbiased sample gradients in ordinal regression. The Cram\u00e9r distance is preferred over KL and Wasserstein metrics for outcome similarity. The energy distance extends the Cram\u00e9r distance to the multivariate case, with Sz\u00e9kely demonstrating its application. Sz\u00e9kely demonstrated the energy distance, closely related to maximum mean discrepancies, as equivalent to squared MMD. The energy distance can be written in terms of a difference of expectations and possesses properties (I), (S), and (U). The Cram\u00e9r GAN Losses algorithm involves interpolating real and generated samples to calculate surrogate generator and critic losses. In the context of GAN frameworks, the Cram\u00e9r GAN proposes a better alternative to the Wasserstein GAN, focusing on the Cram\u00e9r distance. The GAN consists of a generator, a target source, and a discriminator. The Wasserstein GAN algorithm requires a powerful network for critic training, which is often challenging to achieve. Our proposed loss function combines the energy distance with a transformation function to allow for learning with imperfect critics in GAN frameworks. The generator minimizes the energy distance of transformed variables, while the critic maximizes this distance by adjusting the transformation parameters. The Cram\u00e9r GAN losses are summarized in Algorithm 1, with additional design choices detailed in Appendix C. The Cram\u00e9r GAN presented in this study combines the energy distance with a transformation function for training GANs. The squared MMD is used in Generative Moment Matching Networks, and direct minimization of the energy distance over raw images is found to be ineffective. The Cram\u00e9r GAN complements the comparison of Wasserstein and Cram\u00e9r distance, and offers novel GAN-related contributions such as conditional modeling capabilities. The Cram\u00e9r GAN introduces novel GAN-related contributions, including conditional modeling capabilities and increased diversity in generated samples compared to WGAN-GP. The experiment involves training generative models to predict the right half of an image given the left half using the CelebA 64x64 dataset. The completed faces show improved stability and diversity in the samples. The completions produced by WGAN-GP are almost deterministic, while those by Cram\u00e9r GAN are diverse with different hairstyles, accessories, and backgrounds. The lack of diversity in WGAN-GP is seen as undesirable for a generative model. Minimizing the sample Wasserstein loss may lead to a deterministic distribution, which is not ideal for a good generative model of images. The Cram\u00e9r GAN generates diverse images by learning almost deterministic predictions, leading to increased diversity compared to WGAN-GP. Increasing the number of critic updates improves the performance of Cram\u00e9r GAN, as shown by the independent Wasserstein critic distance during training. The Cram\u00e9r GAN performs poorly and benefits from additional critic updates to improve. It is important to adapt the h(x) transformation continuously. KL divergence may not always be suitable as a loss function in machine learning. Alternative divergences allow for unbiased estimators, incorporate geometric information, and possess the convergence properties needed for efficient learning. Open questions include deriving an unbiased estimator for minimizing the Wasserstein distance and analyzing/reducing the variance of the Cram\u00e9r distance gradient estimate. The KL divergence is a key component in stochastic gradient algorithms for classification. BID5 showed that the total variation does not have certain properties, leading to the same conclusion for the KL divergence. The Wasserstein metric has specific properties proven by BID3. Minimax bias is considered with Bernoulli distributions, showing biased sample gradients in p-Wasserstein metrics. The p-Wasserstein metrics have biased sample gradients, with the gradient of the loss and sample loss being biased for any m \u2265 1. The bias does not vanish with the number of samples m, and a similar argument holds for \u03b8 * and \u03b8 being close to 0 or 1. The bias in p-Wasserstein metrics does not vanish with the number of samples m. For \u03b8 * close to 0 or 1, the bias is non-vanishing when |\u03b8 * \u2212 \u03b8| is of order 1 m. Even worse when \u03b8 * is away from the boundaries, such as \u03b8 * = 1 2, the bias remains non-vanishing. Using the anti-concentration result of Veraar (2010), a lower bound on the bias is established for a specific range of \u03b8 values. The stochastic gradient descent algorithm based on the sample Wasserstein gradient converges to a median \u03b8 that may differ from the mean \u03b8*. The minimum of the expected sample Wasserstein loss is not the same as the minimum of the true Wasserstein loss, indicating that minimizing the sample Wasserstein loss using finite samples may not lead to the correct solution. In the specific case where (1/2) 1/n < \u03b8 * < 1, the gradient descent algorithm converges to 1 instead of \u03b8 * due to degenerate solutions when target distributions have low entropy. The sample 1-Wasserstein gradient converges to the true gradient as m \u2192 \u221e for probability distributions P and Q \u03b8. The gradient descent algorithm converges to 1 instead of \u03b8 * when target distributions have low entropy. The sample 1-Wasserstein gradient converges to the true gradient as m \u2192 \u221e for probability distributions P and Q \u03b8. The bounded derivative is derived from m independent samples drawn from P, and the Wasserstein distance measures the area between the curves of the distribution functions. The map \u03b8 \u2192 F Q \u03b8 (x) is differentiable in a neighborhood V(\u03b8) with uniformly bounded derivative. The dominated convergence theorem applies, and the set of x such that F P (x) = F Q \u03b8 (x) has measure zero. The empirical cumulative distribution function converges to the true distribution almost surely. The set where the distributions are equal has measure zero, leading to convergence of the set to zero. The bounded derivative allows for the use of the dominated convergence theorem, proving the Cram\u00e9r distance property. The l p metrics have dual forms as integral probability metrics, where F q is the set of absolutely continuous functions with bounded L q norm. The l p metric possesses properties (I) and (S) for p \u2208 [1, \u221e), with the case p = \u221e following a similar argument. The IPM formulation is used to prove property (I) by independence of A and X, Y, and Jensen's inequality. The set F q consists of absolutely continuous functions with bounded L q norm. Translations of functions in F q are also in F q. The requirement for finite expectations under consideration ensures well-defined and finite means. This condition guarantees light tails of the distribution function to avoid infinite Cram\u00e9r distances. The Cram\u00e9r distance is the only l 2 2 distance among all l p distances that satisfies certain conditions. The Cram\u00e9r distance, l 2 2, is the only l p distance with the (U) property. Assuming P is not a Dirac, for any X m \u223c P, DISPLAYFORM2 everywhere. When Q \u03b8 has bounded support, P can be a translated version of Q \u03b8 with non-overlapping supports. The 1-Wasserstein distance does not have the (U) property. The gradient for l p p (P, Q \u03b8) is discussed using \u03c6 p (z) = z p\u22121.\u03c6 p is convex for p \u2265 2 and concave for 1 < p < 2. Jensen's inequality is applied for convex and concave functions. The Cram\u00e9r distance is the only l p distance with unbiased sample gradients. The energy distance E(P, Q) has properties (I), (S), and (U), proven using characteristic functions of random variables X and Y. The Cram\u00e9r distance is the only l p distance with unbiased sample gradients. The energy distance E(P, Q) has properties (I), (S), and (U), proven using characteristic functions of random variables X and Y. When considering a real value c > 0, it is shown that the gradient of the true loss w.r.t. \u03b8 is equal to the gradient of the sample loss w.r.t. \u03b8, in expectation. The energy distance has property (U) and is compared with the Wasserstein distance in a toy experiment using the Year Prediction MSD dataset. A network with a single hidden layer is trained for ordinal regression task on song year prediction. Training models on the Year Prediction MSD dataset involves using different loss functions and minibatch sizes to compare performance. Results are presented in a graph, with the Cram\u00e9r loss showing the lowest root mean squared error. Training models on the Year Prediction MSD dataset involves using different loss functions and minibatch sizes to compare performance. Results are shown in FIG2, with the Cram\u00e9r loss resulting in the lowest RMSE. Training with Cram\u00e9r loss is faster and more robust, while Wasserstein loss leads to higher KL loss. Larger minibatch sizes improve Wasserstein method performance. Experiments on a generative model using 1-Wasserstein, Cram\u00e9r, or KL loss on CelebA 32x32 dataset show promising results. The CelebA 32x32 dataset contains 202,599 images of celebrity faces. PixelCNN models joint probability autoregressively by predicting each pixel using a histogram distribution. Wasserstein-type losses are suitable for this task due to the natural ordering on pixel intensities. Sample Wasserstein loss is reported as an upper bound on the true loss, as true losses are not available. For KL divergence, cross-entropy loss is reported. In the context of learning an autoregressive image model using the CelebA 32x32 dataset, the Cram\u00e9r distance is preferred over the Wasserstein metric. The Cram\u00e9r loss achieves lower Wasserstein and Cram\u00e9r loss compared to minimizing the Wasserstein distance. The critic in the Cram\u00e9r GAN has a special form with trainable parameters only inside the deep network used for the transformation. The generator loss in the Cram\u00e9r GAN is defined without the max f operator, allowing for unbiased sample gradients. The critic maximizes the generator loss while minimizing a gradient penalty to bound its outputs. The training process is similar to Wasserstein GAN, with \u03bb = 10 chosen for the penalty. The next sections explain how to compute gradients for the generator and transformation parameters. The energy distance is computed using the reparametrization trick for the generator parameters. A generator loss can be defined with only one real sample. Gradient estimates for the transformation can be obtained from three samples: two from the generator and one from the target distribution. The Cram\u00e9r GAN loss gradient estimation requires four independent samples: two from the generator and two from the target distribution. In cases where two independent target samples are not available, a surrogate critic with a defined form is used. The surrogate loss emulates an integral probability metric (IPM) and is utilized to train the critic. The training procedure involves training the critic to maximize a loss that enforces informative distributions, while the generator minimizes the energy distance of transformed variables. Two possibilities for training the generator are recommended, resulting in stable and diverse samples. The U-Net architecture is used for the generator, with no batch normalization. A surrogate loss is constructed to reduce variance, using two losses from the generator. The training procedure involves using the U-Net architecture for the generator, without batch normalization. The network is conditioned on the left half of the image and extra 12 channels with Gaussian noise. Two independent samples are generated for each image to compute the Cram\u00e9r GAN loss. The evaluation measures Inception score and Fr\u00e9chet Inception Distance are reported for GANs. The proposed evaluation for conditional GANs, called Inception Energy Distance (IED), aims to detect overfitting by comparing features of generated images with real images using a pretrained Inception network. The IED provides a more accurate assessment compared to traditional evaluation measures like Inception score and FID. The Inception Energy Distance (IED) is used to compare GAN performance, detecting underfitting and overfitting. WGAN-GP shows deterministic completions, while Cram\u00e9r GAN overfits the training set. Optimization successfully trains the generator, with potential for improvement through more data and regularization methods. Future work may involve training on large video datasets to minimize IED directly."
}