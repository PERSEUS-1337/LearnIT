{
    "title": "R40514",
    "content": "Federal education legislation, including Title I-A of the ESEA and IDEA, emphasizes assessment in schools. States receiving Title I-A funds must test students annually in reading and math from grades 3 to 8, and once in high school. These assessments are used to measure school progress in student achievement. States must also participate in the NAEP test at grades 4 and 8. IDEA requires assessments to identify and track progress of students with disabilities. This report provides a framework for understanding various types of assessments administered in elementary and secondary schools, including state assessments, NAEP, and state exit exams. It discusses purposes of educational assessment, comprehensive assessment systems, and technical considerations like validity, reliability, and fairness in drawing conclusions from assessment results. This report summarizes major assessment provisions in federal education laws like IDEA and NCLB, discussing implications for curriculum, students, and testing. It also mentions the development of alternative educational accountability systems in response to NCLB waivers offered by the Secretary of Education in September 2011. The report discusses the development of alternative accountability systems in response to NCLB waivers, exempting states from certain accountability requirements. Educational assessment involves gathering and analyzing data for decision-making, with achievement testing being the most common type used in current education policy. The report provides a framework for discussing assessments in education, including purposes, comprehensive systems, and scoring. It also covers current assessments, technical considerations, innovation, and accountability systems. A Glossary at the end offers definitions of assessment terms. Educational assessment is designed with specific purposes in mind, such as instructional, predictive, diagnostic, and evaluative. Test results should be used for their intended purpose to avoid undermining test validity. Instructional assessments help modify instruction to meet students' needs. Instructional assessments involve modifying and adapting instruction to meet students' needs. These assessments can be informal or formal, taking place in a classroom setting. Informal assessments include teacher questioning strategies and reviewing classroom work, while formal assessments may involve written pretests to analyze students' knowledge before teaching. Progress monitoring, through short assessments during an academic unit, helps determine if students are learning the content. Predictive assessments are used to gauge the likelihood of meeting predetermined goals. Predictive assessments like benchmark assessments are used to determine if students or schools are on track to meet end-of-year goals. Diagnostic assessments identify students' strengths and weaknesses beyond academic achievement, helping to provide a comprehensive picture of their overall functioning. These assessments can lead to interventions or programmatic changes to improve the likelihood of achieving goals. Diagnostic assessments are used to identify students for additional school services, including special education or English language services. These assessments can test cognitive functioning, behavior, social competence, language ability, and academic achievement. Evaluative assessments determine the outcome of a curriculum, program, or policy by comparing results to predetermined goals. Unlike other assessments, evaluative assessments focus on what students have learned rather than providing actionable information for instruction, prediction, or diagnosis. Assessments in accountability systems are conducted for an evaluative purpose, such as determining the outcome of a policy objective. State assessments under NCLB have been used to assess whether schools have made Adequate Yearly Progress. A comprehensive assessment system combining formative and summative assessments is necessary to cover all educational assessment purposes. Formative assessments are used during the learning process to improve curriculum and instruction. Formative assessments are used during the learning process to improve curriculum and instruction, while summative assessments are used at the end to evaluate the outcome of curriculum, instruction, or policy. The distinction lies in how the results are used, with formative assessments informing future decisions and summative assessments evaluating the effects. The line between the two types of assessments can blur depending on their intended use. Formative assessment is used to improve curriculum and instruction during the learning process, while summative assessment evaluates the outcome of curriculum, instruction, or policy. There is a lack of universal agreement on what constitutes formative assessment in education, with various stakeholders using the term to cover a wide range of assessments. This confusion has led some in the testing industry to avoid the term altogether or use alternative names for certain types of formative assessment. Formative assessments, such as classroom, interim, and benchmark assessments, are used by teachers for instructional and predictive purposes. These assessments help identify student knowledge deficits and guide adjustments in instruction. Teachers may change the pace or method of delivery, repeat content, and re-administer assessments to ensure expected learning outcomes. Some testing experts believe that referring to interim and benchmark assessments as \"formative\" is inaccurate, but others believe that these assessments can be used in a formative way to determine how school or district practices need to change to meet policy goals. This approach considers using interim or benchmark assessments as formative assessments at the school or district level, requiring adjustments in practices to increase student success. Interim and benchmark assessments track student progress towards policy goals. Interim assessments inform decisions at various levels and can report on individual or aggregate achievement. They are determined by the school or district, not the teacher, making them less flexible in the classroom. Interim assessments are less flexible in the classroom as they are determined by the school or district, not the teacher. They are used for predictive purposes and to evaluate short-term instructional programs or aspects of the curriculum. Benchmark assessments, a type of interim assessment, are widely used to predict success on later summative assessments. Summative assessments are tests given at the end of a lesson, semester, or school year to determine what has been learned. They are used for diagnostic or evaluative purposes and are often high-stakes with rewards or consequences attached to performance. These assessments help identify students who may need more intensive instruction and determine if schools or districts are on track to meet policy objectives like AYP. Under NCLB, states used high-stakes assessments to determine AYP, with consequences for schools and districts but not individual students. Not all summative assessments have high-stakes consequences, such as end-of-unit tests for grades. NAEP and international assessments provide a broader view of achievement without major consequences. Formative and summative assessments should be part of a comprehensive system to improve student achievement. The REL Mid-Atlantic analyzed the predictive validity of benchmark assessments in Delaware, Maryland, New Jersey, Pennsylvania, and Washington, D.C. Only one benchmark assessment showed strong predictive validity with state assessments, while none showed predictive validity in Maryland and New Jersey. Benchmark assessments should align closely with summative assessments to predict outcomes accurately. The predictive validity of benchmark assessments in Maryland and New Jersey varies depending on the assessment used. Without a strong predictive relationship between benchmark and state assessments, the formative function of benchmarks in the assessment system is questioned. Test scores can be reported in various ways, such as comparing to peers or indicating proficiency levels. Scores on tests can be reported in different ways, such as percentiles or proficiency levels. Misinterpreting test scores can lead to unintended consequences. Common methods of score reporting include norm-referenced tests (NRTs) and criterion-referenced tests (CRTs). NRTs compare individual scores to a normative sample. Norm-referenced tests (NRTs) compare individual scores to a normative sample with known demographic characteristics. Standard scores are used to report how a student performed relative to peers, often reported as percentiles for easy interpretation. Commercially available cognitive and achievement tests, like the Stanford Achievement Test Series (SAT10), are often norm-referenced. The Reading First program utilizes Language proficiency tests like the IPT Family of Tests for Limited English Proficiency (LEP) students, which are Norm-referenced tests (NRTs). NRTs, such as the Wechsler Intelligence Scale for Children (WISC), are also used to measure cognitive ability in students with disabilities. While NRTs are cost-effective and efficient for large-scale administration, they have been criticized for focusing on superficial learning rather than higher-level skills like problem-solving and critical thinking. Norm-referenced tests (NRTs) like the Wechsler Intelligence Scale for Children (WISC) are criticized for lacking instructional utility and not being linked to the curriculum. In contrast, Criterion-referenced tests (CRTs) compare performance against a predetermined standard, focusing on mastery of specific curriculum and content skills determined through professional judgment. Mastery can be defined in various ways. Criterion-referenced tests (CRTs) focus on mastery of specific curriculum and content skills determined through professional judgment. Unlike NRTs, CRTs do not compare students to a normative group but instead report scale scores or raw scores. CRT results can be reported in various formats such as grades, pass/fail, number correct, percentage correct, or performance standards. These tests can be designed using multiple formats and are versatile for instructional purposes. Criterion-referenced tests (CRTs) are versatile for instructional purposes and can be linked directly to the curriculum. They are cost-effective and time-efficient, but do not allow for easy comparisons across schools, districts, and states. Interest in CRTs grew in the 1990s due to standards-based reform in education. Performance standards are a type of score reporting that evolved from CRTs and standards-based reform. They define levels of performance in a content area using cut scores, with varying levels of proficiency. NAEP and state assessments use performance standards to determine achievement levels. Definitions are provided for each standard, describing associated competencies and abilities. Performance standards, evolved from CRTs and standards-based reform, define performance levels in a content area using cut scores. They provide context to assessment results by attaching a proficiency label to scores, making them meaningful for educators and parents. However, criticism exists for the somewhat arbitrary nature of the cut scores. Performance standards are criticized for their arbitrary cut scores, which may not reflect meaningful differences in student abilities. For example, a student scoring 238 may not be considered proficient, while a student scoring 242 may be. This can lead to inappropriate distinctions between students with similar abilities. Additionally, performance standards may be insensitive to student growth, as a student making significant progress from 242 to 299 within a year may still not meet the \"advanced\" level cut score of 300. Professional judgment plays a crucial role in assessing students' performance, especially when determining if a student meets the proficiency standard. Teachers use their judgment to evaluate students' knowledge, refer them for special evaluations, or assess their performance against set standards using rubrics. This approach allows for a more personalized assessment that considers individual student performance within the curriculum. Professional judgment is crucial in educational assessment, involving subjective decisions tied to the curriculum and individual student performance. The National Assessment Governing Board sets performance standards through a consensus process involving teachers, specialists, and the public. Various types of assessments are used in elementary and secondary schools within this framework. Assessments in elementary and secondary schools are discussed within the framework set by the National Assessment Governing Board. Federal law requires assessments like state assessments for AYP, NAEP, and special services identification. State exit exams, benchmark assessments, and international assessments are also widely used. Since the reauthorization of the ESEA by the NCLB, there has been a focus on state assessments for calculating required annual progress. States participating in the Title I-A program must administer standards-based assessments in reading, mathematics, and science to students in specific grade levels. The practice of linking assessments is also discussed, which involves comparing scores across different tests. NCLB provisions mandate 95% student participation in state assessments for schools to make AYP. States can create their own standards and assessments, but they must meet specific requirements for validity, reliability, and alignment with academic standards. Under NCLB, achievement standards must include three levels: basic, proficient, and advanced. The state educational agency must ensure assessments meet technical quality standards. Students with disabilities must participate in state assessments, with some taking alternate assessments. Regulations outline the administration and use of alternate assessments in the NCLB accountability system. In the NCLB accountability system, regulations outline the use of alternate assessments. In the 2007-2008 school year, states used NRTs and CRTs for assessments. Some states used NRTs only, CRTs only, or a combination of both at different grade levels. Examples of NRTs include the Iowa Test of Basic Skills and the SAT 10. State assessments include ITBS, SAT 10, TAKS, and NECAP. 42 states used a variety of test formats, with 7 using multiple choice only, 1 using extended response only, and 34 using a combination. 44 states and DC are developing new assessments for grades 3-8 and high school. Common assessments aligned with common core standards for reading and math are being developed by 44 states and DC for grades 3-8 and high school. The goal is to achieve 100% proficiency by 2013-2014, but comparison of proficiency percentages across states is not possible due to varying assessment methods and cut scores. The NAEP is a series of assessments administered since 1969 to students in grades 4, 8, and 12, covering various subjects. Proficiency levels vary between states due to different cut scores, making it inappropriate to compare overall student achievement levels. NAGB, an independent bipartisan group, establishes NAEP policies. The National Assessment of Educational Progress (NAEP) is administered to students in grades 4, 8, and 12 across the nation. It serves as a common metric for understanding student achievement. NAEP assessments cover various subjects and are administered by the Department of Education with assistance from contractors. The state assessment, which began in 1990, is administered every other year to students in grades 4 and 8 in reading and mathematics. States receiving Title I-A funding under NCLB are required to participate. The NAEP long-term trend assessments track trends in reading and math achievement every four years since the 1970s. It is given to a nationally representative sample of students ages 9, 13, and 17, using identical questions for consistency in tracking progress over time. NAEP is a summative assessment with performance standards (basic, proficient, advanced) consistent across states for comparison. NAEP reports aggregated achievement results for groups and subgroups of students by grade and content area. The meaning of \"proficiency\" varies between state assessments and NAEP, as states have the discretion to set their own proficiency standards. State assessments allow for discretion in designing assessments and setting proficiency cut scores. NAEP scores can be compared across states. Schools must provide special services to eligible students with disabilities and LEP students, with IDEA requiring special education services and ESEA requiring supplemental English language instruction. IDEA and ESEA provide guidelines for determining eligibility for special education services. States have flexibility in interpreting assessments for eligibility. Students must meet criteria for disability and require special services to benefit from public education under IDEA. States develop their own definitions of disability categories. Assessment methods vary by state and suspected disability. IDEA specifies requirements for special education evaluations, with assessments varying by state and disability. Local educational agencies must use various tools to gather relevant information, including parent input. Assessments cover functional, developmental, and academic domains, measuring skills like language, behavior, cognition, and motor skills for students with disabilities. LEAs interpret assessment scores to determine eligibility for special education services. LEAs interpret assessment scores to determine eligibility for special education services for students with disabilities. ESEA defines an LEP student as someone whose native language is not English and who may struggle with English proficiency. Each state sets its own criteria for LEP student eligibility, which includes a home language survey and an English language assessment. LEAs use home language surveys and English language assessments to determine LEP eligibility based on state criteria. Title I-A of NCLB mandates annual assessments in English language skills for LEP students. These assessments, whether formative or summative, provide diagnostic information on student strengths and weaknesses. NRTs and CRTs are used based on student needs and state criteria. An increasing number of states require students to pass exit exams to graduate from high school, which typically include tests in language arts, mathematics, science, and social studies. These exams can take various forms such as minimum competency exams, comprehensive exams, or end-of-course exams, assessing knowledge in different subject areas. The trend is moving towards comprehensive exams aligned with state standards and away from minimum competency exams. The Center on Education Policy (CEP) publishes information on state policies regarding high school exit exams annually. A national study found that high school exit exams do not improve reading and math achievement and may lower graduation rates. Students in states with exit exams are not more successful in the labor market compared to those in states without exit exams. Other state-level studies have reported similar findings. State exit exams are summative assessments with high stakes for individual students. Most are CRTs aligned with state standards. They can be taken multiple times, potentially serving as formative assessments. Benchmark assessments are mid-year evaluations to track progress towards end-of-year goals. The use of exit exams as formative assessments has not been studied. Benchmark assessments are used by schools to track progress towards end-of-year goals. They are aligned with state content standards and can be conducted by original test publishers or independent contractors. Examples include 4Sight Math and Reading, STAR Math and Reading, Study Island Math and Reading, and TerraNova Math and Reading. These assessments are considered formative and report scores as performance standards. Benchmark assessments report scores as performance standards and can be used for instructional purposes to identify deficits in students' knowledge. They are also used for predictive purposes and are closely aligned with state assessments. International assessments like PISA, PIRLS, and TIMSS provide a comparison of student performance across countries. Participation in these assessments is voluntary. International assessments like PISA, PIRLS, and TIMSS are voluntary and provide a comparison of student performance across countries. PISA assesses 15-year-olds in reading, math, and science literacy every three years since 2000. PIRLS assesses 4th-grade reading achievement, behavior, and attitudes, first administered in 2001 to students in 35 countries. TIMSS is an assessment of science and mathematics achievement of students in grades 4 and 8 in the United States and other countries. It reports national averages for comparison but does not provide individual student results. Some states use TIMSS for international benchmarking studies to compare student performance within their state to other countries. The TIMSS assessment evaluates science and math achievement in students from various countries. Results are often ranked based on performance compared to an international mean, which can vary between administrations. Some believe that students participate in too many assessments, potentially affecting instructional time in schools. There is interest in linking assessments to compare results across different tests. This process involves statistical analyses to connect scores from one test to another, regardless of scale equivalence. An example is the Fahrenheit and Celsius temperature scales, easily linked with a simple equation. This process aims to streamline assessments and reduce the time spent on administering tests in schools. Linking educational assessments is complex due to varying definitions of achievement. Achieving statistical linkages would allow for comparisons across state assessments using a common metric. This could lead to easier comparisons between states and even international assessments, potentially reducing the number of assessments administered to students. The National Research Council (NRC) studied the feasibility of creating a common metric to link scores from different assessments. They found that comparing state assessments using a common metric was not feasible and linking state assessments to NAEP could lead to misleading inferences due to differences in content and format. The feasibility of linking assessments can be affected by differences in content, format, and use of the tests. Progress has been made in linking NAEP results to TIMSS for grade 8 mathematics, allowing for comparisons between states and other countries. This linkage is particularly useful for international benchmarks. This section discusses technical considerations in assessment, including validity, reliability, and fairness. Test developers are responsible for investigating these characteristics and reporting statistical information to users in testing manuals. Users must administer the test correctly and use the information on validity, reliability, and fairness to interpret results accurately. Understanding how to evaluate these aspects allows for appropriate inferences to be made from test results. The section discusses the importance of validity in educational assessments and how to avoid inappropriate inferences. Validity is crucial in making instructional or policy decisions based on test results. It is about the appropriateness and meaningfulness of the inferences drawn from the test. Teachers, administrators, and policymakers often want to support multiple conclusions from test results. The SAT Reasoning Test is used to measure critical thinking skills for college success. High scores from School A seniors may suggest they are more likely to succeed in college, but other inferences like better curriculum or teachers may not be valid. The validity of an inference from SAT scores is tied to the test's purpose of predicting college success, not evaluating teachers or curriculum. Validation involves collecting evidence to support test score interpretation based on the test construct, investigating construct underrepresentation and irrelevance. Construct underrepresentation and irrelevance can impact the validity of test score inferences. Underrepresentation occurs when an assessment does not measure all skills within a defined construct, while irrelevance refers to test items that are not part of the intended construct. This can lead to inaccurate interpretations of a student's knowledge. The assessment's validity can be impacted by construct underrepresentation and irrelevance, which can lead to inaccurate interpretations of a student's knowledge. Statistical procedures are used to investigate if the assessment adequately covers all skills in the construct and if any skills are outside the realm of the construct. It is important to determine if the degree of underrepresentation or irrelevance affects different subgroups of the population differently. Construct irrelevance can lead to invalid inferences about student performance, especially between advantaged and disadvantaged subgroups. Validation methods include comparing scores with existing measures and predicting future outcomes. Validation is a thoughtful investigation of assessments, while reliability refers to the consistency of measurement. It presumes each student has a true score, with the observed score differing due to measurement error. The reliability of a measurement is inversely related to the measurement error. Higher reliability increases the likelihood that a student's observed score is close to their true score. Reliability can be expressed through the reliability coefficient, range of uncertainty, and consistency of classification. The reliability coefficient ranges from 0 to 1, with 0 indicating complete measurement error and 1 indicating no error. There is no set rule for what a high reliability coefficient should be. The reliability coefficient is crucial in educational assessments, with most assessments having coefficients above 0.8 or even 0.9. Different types of reliability coefficients include alternate-form, test-retest, inter-scorer agreement, and internal consistency coefficients. Alternate-form coefficients ensure consistency between scores from different forms of the same assessment, like the SAT. Test-retest coefficients measure score stability over time. The reliability coefficients in educational assessments, such as test-retest and inter-scorer agreement coefficients, measure score stability and agreement between scorers. Internal consistency coefficients assess the correlation of items within the same assessment. Internal consistency coefficients in educational assessments measure the correlation of related items within the same assessment, ensuring that they are measuring the same construct. The decision on which reliability coefficients to report depends on the assessment's purpose and format. For example, alternate-form coefficients may not be necessary for assessments that do not use alternate forms. In educational assessments, the reliability of test results is crucial for accuracy. Test-retest reliability may not be applicable for short-term student growth assessments. The format of the test, such as multiple-choice or constructed responses, influences the need for inter-scorer agreement. Reliability ensures the precision and certainty of assessment results, often reported with a range of uncertainty. In educational assessments, the reliability of test results is crucial for accuracy, often reported with a range of uncertainty known as a confidence interval. This interval estimates the likelihood that a student's true score falls within a range of scores, calculated using an estimated true score, standard error of measurement (SEM), and desired level of confidence. The confidence interval is reported as a range of scores with a lower and upper limit, commonly seen at 90%, 95%, or 99% confidence levels. The size of the confidence interval changes with the degree of confidence. For example, a 90% confidence interval for a student with an estimated true score of 100 and SEM of 10 would be 84 to 116. This means that about 90% of the time, the true score will fall within this range. A 95% confidence interval would be 80 to 120, and a 99% confidence interval would be 74 to 126. The confidence interval ranges from 74 to 126, with a 99% chance of the true score falling within this range. Consistency of classification is important for high-stakes decisions based on educational assessments. Consistency of classification is crucial in educational assessments for making high-stakes decisions, such as placing students in achievement levels or awarding diplomas. Without consistency, the accountability system and diploma awarding process may become unreliable. Statistical modeling shows that fluctuations in classification can occur depending on reliability. In educational assessments, classification consistency is crucial for making decisions on student eligibility for services like special education. Diagnostic assessments are used to determine disabilities based on state definitions, leading to special education services for eligible students. Research shows fluctuations in disability classifications over time, with some students no longer receiving services after a period. The reliability of assessments used to determine initial eligibility for special education services can impact the rate of students being \"declassified\". Fairness in educational assessments is a key issue, with different forms including lack of bias, equitable treatment, and equality for all populations. Bias is a common criticism in educational assessment, with concerns about systematic differences in observed scores based on subgroup membership. It can arise from cultural or linguistic factors influencing test scores, or disabilities hindering a student's ability to demonstrate their skills. Addressing bias in testing is challenging, with no professional consensus on mitigation strategies. Statistical procedures like differential item functioning may help detect bias in specific test items. Bias in educational assessment is a common concern, with issues of systematic score differences based on subgroup membership. Detecting bias in specific test items is possible, but it does not directly address bias in interpreting assessment results. It is crucial to note that score differences between subgroups do not always indicate bias. Fairness in testing, ensuring equitable treatment, is less controversial than addressing bias. Equitable treatment in testing is crucial, ensuring all students have a fair opportunity to demonstrate their knowledge. This includes providing a comfortable testing environment, equal time to respond, and accommodations for students with disabilities. It also involves giving students equal opportunity to prepare for the test, which can be challenging to monitor and enforce. Some schools provide sample test questions for practice, while others may not. The inconsistency in test preparation services from private companies like Kaplan, Inc. or Sylvan Learning can undermine the validity of assessments. Fairness in testing is crucial, especially for high-stakes decisions like state exit exams for high school graduation, where equality in outcomes is questioned. \"Equality in outcomes is a concern with high-stakes assessments like state exit exams, impacting graduation rates and future opportunities for disadvantaged students. Fairness in educational assessment is crucial, aligning with school curriculum to measure student knowledge accurately.\" The assessment of all students against the same standards for Adequate Yearly Progress (AYP) raises questions about fairness and equal opportunity to learn. The challenge lies in defining what constitutes an \"opportunity to learn\" and how factors like curriculum, school environment, and teacher quality impact student learning. Test users have a responsibility to consider these factors when evaluating student performance. Test users must carefully analyze the validity, reliability, and fairness of assessments to make appropriate inferences about student achievement. There is no simple checklist for this task, so considerations such as the assessment's construct, purpose, scores, evidence of validity, reliability, fairness, and context must be taken into account. Failure to do so can result in unintended consequences. Sample questions are provided to guide test users in making informed inferences about test scores. Understanding the construct of an assessment is crucial for drawing appropriate conclusions from test scores. Questions about the content area being assessed and the specific construct measured within that area are important. For example, international assessments like PISA and TIMSS measure different mathematical constructs, with PISA focusing on \"mathematical literacy\" and TIMSS being curriculum-based. Comparing test results requires a clear understanding of the assessment's construct. Results from the 2006 PISA administration showed lower U.S. math scores compared to the international average, while the 2007 TIMSS results indicated higher U.S. math scores. However, it is important to note that PISA and TIMSS measure different constructs, with PISA focusing on \"mathematical literacy\" and TIMSS being curriculum-based. Understanding the original purpose of assessments helps determine how results may be interpreted and used. State assessments are typically summative and may not be suitable for modifying instruction. Benchmark assessments, designed for predictive purposes, provide timely results for targeted instruction. Benchmark scores should not be considered definitive. Benchmark assessments provide timely results for targeted instruction, but scores should not be seen as definitive indicators of state assessment performance. Questions about how scores compare to others, criteria, proficiency levels, and growth should be considered to avoid misinterpretation. Understanding the scale and reporting of scores is crucial to making appropriate inferences. NRT scores can be compared to a normative sample, but cannot determine proficiency. Performance standards in state assessments are important for interpreting student scores relative to predetermined criteria. They provide a description of what students know but can be challenging to interpret. Unlike norm-referenced tests, criterion-referenced tests are designed to measure proficiency within a specific content area. Performance standards in state assessments categorize students into basic, proficient, or advanced levels based on their performance. However, interpreting these standards can be challenging as students within the same category may have varying scores and growth trajectories. Test users should be cautious about equating student performance within categories and making assumptions about growth based on category movement. Questions about technical quality in assessments include whether test developers provided statistical information on validity and reliability, addressed fairness and bias, collected appropriate evidence, and reported reliability for all subgroups. Commercial assessments come with a manual detailing validity and reliability, while smaller assessments may provide evidence upon request. Evaluating the quality of evidence is a complex task. When evaluating the technical quality of assessments, it is important to consider validity and reliability for all subgroups to avoid bias. Specific reliability evidence should match the assessment type, especially for subjective scoring methods like essay tests. In assessing technical quality, it is crucial to ensure inter-scorer reliability, especially for assessments with multiple forms like the SAT Reasoning Test. Without high alternate-form reliability, bias can be introduced, leading to unequal scoring. All assessments have some degree of measurement error and potential bias, depending on the stakes involved. In high-stakes assessments, the context and validity of the evidence are crucial for making important decisions. Low-stakes assessments, like classroom formative assessments, may not require exhaustive review of reliability and validity. In high-stakes assessments, such as state exit exams for graduation, it is crucial to ensure the validity and reliability of the assessment to defend inferences made. Poor validity can lead to testing on irrelevant content, affecting students' performance and diploma eligibility. Similarly, poor reliability can result in students passing or failing due to measurement errors. Protections are often implemented in high-stakes testing processes to safeguard against such issues. Some states allow multiple attempts for high school exit exams to reduce measurement errors. Additional data like student portfolios may be considered for diploma eligibility. Confidence intervals are used in high-stakes assessments to report student achievement, with NCLB emphasizing student assessment for school accountability. Test-based accountability under NCLB has led to increased focus on student achievement in reading, math, and science, particularly for disadvantaged and special needs students. However, critics argue that it narrows the curriculum, sets low proficiency expectations, and neglects students at different achievement levels. One potential positive outcome of test-based accountability under NCLB is the increased focus on state-level content standards and teaching to those standards. This has led to the development of rigorous content standards for reading and mathematics, which many believe will improve learning outcomes. Using content standards and assessments can help teachers focus their instruction and receive feedback on its effectiveness. High-performing schools align state content standards with their curriculum, involving teachers in the process for better buy-in. However, test-based accountability may narrow the curriculum by increasing testing burden and potentially affecting classroom instruction. Test-based accountability systems may lead to a testing burden that detracts from classroom instruction. Teachers feel pressure to teach to the test, engaging in test preparation activities at the expense of instruction. Instructional time is often reallocated towards tested subjects like reading and mathematics, neglecting non-tested subjects. The No Child Left Behind Act (NCLB) emphasizes reading and mathematics instruction over other subjects like history, foreign language, and arts. Survey results show a consistent focus on tested subjects, but data collection relies on self-reports from teachers and administrators. NCLB requires schools to disaggregate student assessment data for subgroups, holding them accountable for reaching proficiency in reading and math by 2014. This accountability system has increased attention on specific subgroups' achievement. Disaggregating data by subgroups under NCLB has increased attention on specific student groups, aiming to improve equity in education by closing achievement gaps between white and minority students, and economically advantaged and disadvantaged students. This focus on subgroups allows for a more accurate measurement of student performance and access to rigorous academic curriculum for all students. Under NCLB, assessment results are disaggregated to measure the achievement gap and track progress over time. However, a consequence of this accountability system is the potential focus on students near proficiency, neglecting those below or above that level. This could lead to a lack of resources for students not targeted for improvement. This has prompted the exploration of alternative methods for measuring achievement, including growth. The use of growth models in accountability systems may credit teachers and schools for student growth, even below proficiency levels. High-stakes assessments under NCLB hold schools accountable for student achievement, leading to efforts to prepare students for these tests, which can result in score inflation. Test preparation can impact the validity of test scores, with some forms increasing validity by familiarizing students with standardized testing. However, excessive alignment between test items and curriculum, coaching on specific test items, or cheating can negatively affect validity. Studying score inflation prevalence in school districts is challenging. Several studies have shown discrepancies in student achievement levels and gains between high-stakes state assessments and low-stakes NAEP tests, indicating possible score inflation in state assessments. This inflation raises questions about the validity of the inferences made from state assessments. Using a low-stakes \"audit\" assessment like NAEP consistently could help reduce the problem of score inflation. Using a low-stakes \"audit\" assessment like NAEP consistently can help policymakers differentiate between true student achievement gains and score inflation on state assessments."
}