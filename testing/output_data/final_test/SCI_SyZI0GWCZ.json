{
    "title": "SyZI0GWCZ",
    "content": "Many machine learning algorithms are vulnerable to imperceptible perturbations in their inputs. Adversarial attacks that rely solely on the final model decision are important for real-world applications like autonomous cars, as they require less knowledge and are more robust to defenses compared to other attack methods. The Boundary Attack is a decision-based attack that is more robust to defenses than other attack methods. It starts with a large perturbation and reduces it while remaining adversarial. It requires minimal hyperparameter tuning, does not rely on substitute models, and is competitive with gradient-based attacks in tasks like ImageNet. This attack opens new avenues for studying machine learning model robustness and raises questions about the safety of deployed systems. The Boundary Attack, available in Foolbox, is a decision-based attack applicable to real-world ML algorithms. It does not require model information like gradients or confidence scores, making it robust to defenses. This attack highlights the susceptibility of high-performance ML algorithms to minimal input changes, as demonstrated with a VGG-19 network recognizing a tiger cat but changing to a bus with slight perturbations. Adversarial perturbations can drastically alter predictions in machine learning models, posing security risks for systems like autonomous cars. These perturbations, known as adversarial attacks, highlight the gap between human and machine sensory processing, offering insights for more robust AI architectures. Adversarial attacks in machine learning models can be categorized into gradient-based, score-based, and transfer-based attacks. Defending against gradient-based attacks involves masking the gradients to protect against attacks like FGSM, BIM, DeepFool, JSMA, Houdini, and Carlini & Wagner attack. Defending against gradient-based attacks involves masking gradients by adding non-differentiable elements or using non-differentiable classifiers. Score-based attacks rely on predicted scores to estimate gradients numerically. Adding stochastic elements like dropout or using robust training methods can impede the numerical gradient estimate. Transfer-based attacks do not rely on model information but require knowledge of the training data to train a substitute model for generating adversarial perturbations. A defense method against transfer attacks involves robust training on a dataset augmented with adversarial examples from substitute models, proving effective in thwarting various attacks. Decision-based attacks focus on direct attacks that rely solely on the final decision of the model, making them more relevant in real-world machine learning applications where confidence scores are not always accessible. These attacks have the potential to be more robust to standard defenses like gradient masking and robust training compared to other types of attacks. Decision-based attacks are more relevant in real-world machine learning applications as they rely solely on the final decision of the model. They are potentially more robust to defenses like gradient masking and robust training compared to other attack types. These attacks require less information about the model and are simpler to apply. However, there is currently no effective decision-based attack that scales to natural datasets like ImageNet for deep neural networks. Previous work has focused on transfer attacks using synthetic datasets, but it has not been proven to scale to more complex datasets like CIFAR or ImageNet. The work by BID0 falls between transfer attacks and decision-based attacks, requiring knowledge of the data distribution on which the black-box model was trained. Naive decision-based attacks induce large perturbations, unlike gradient-based or transfer-based attacks. The paper introduces a decision-based attack called the Boundary Attack, which is effective for complex machine learning models and natural datasets. This attack is simple, flexible, requires minimal tuning, and competes with gradient-based attacks in computer vision. The Boundary Attack is competitive in computer vision scenarios, breaking defense mechanisms like defensive distillation. It is applied to black-box machine learning models for brand and celebrity recognition. The algorithm starts from an adversarial point and performs a random walk. The Boundary Attack algorithm starts from an adversarial point and performs a random walk along the boundary between adversarial and non-adversarial regions to reduce the distance towards the target image. It uses rejection sampling with a proposal distribution to find smaller adversarial perturbations based on a given criterion. The algorithm needs to be initialized with a sample that is already adversarial. The Boundary Attack algorithm starts with an adversarial point and walks along the boundary to reduce the distance to the target image. It uses rejection sampling with a proposal distribution to find smaller perturbations. The efficiency depends on the proposal distribution, which should meet certain constraints. The Boundary Attack algorithm performs rejection sampling along the boundary between adversarial and non-adversarial images. It draws a new random direction by projecting on a sphere and making a small move towards the target image. The step-sizes are dynamically adjusted according to the local geometry of the boundary. The algorithm uses rejection sampling along the boundary between adversarial and non-adversarial images. It involves sampling from a Gaussian distribution, projecting onto a sphere, and making small movements towards the original image to create perturbations for hyperparameter tuning. The goal is to classify inputs as adversarial based on misclassification or targeted misclassification. The Boundary Attack is a flexible method for generating adversarial inputs, allowing for various criteria to be used. It only requires two parameters: the total perturbation length and the step size towards the original input, which are adjusted dynamically based on the local geometry of the boundary. This adjustment is inspired by Trust Region methods. The Boundary Attack method for generating adversarial inputs adjusts perturbation length and step size dynamically based on local geometry. It tests orthogonal perturbations for adversarial nature, adjusting step size accordingly. Success rate determines step size adjustments. The Boundary Attack method adjusts perturbation length and step size dynamically based on local geometry to generate adversarial inputs. It is evaluated on three standard datasets: MNIST, CIFAR-10, and ImageNet-1000 using pretrained networks like VGG-19, ResNet-50, and Inception-v3. The attack is converged when the perturbation converges to zero. The Boundary Attack is evaluated in untargeted and targeted settings, compared against FGSM and DeepFool. FGSM computes the gradient to maximize loss, while DeepFool approximates the model classifier with a linear classifier. The attack by Carlini & Wagner is an iterative gradient attack using the Adam optimizer, multiple starting points, and a tanh-nonlinearity. The success of each attack is evaluated using the median squared L2-distance across all samples. For MNIST and CIFAR, 1000 randomly drawn samples are evaluated, while for ImageNet, 250 images are used. The Boundary Attack is competitive with gradient-based attacks in generating adversarial samples, despite being restricted to the final class prediction. It requires more iterations to converge but is stable against the choice of the initial point. The Boundary Attack is competitive with gradient-based attacks in generating adversarial samples, requiring more iterations but being stable against the initial point choice. It needs 1.200.000 forward passes and zero backward passes, making it more expensive to run, but fewer iterations are needed for imperceptible perturbations. Targeted setting can be applied, initializing the attack from a correctly identified sample of the target class. Many attack methods are straightforward to defend against, such as gradient masking. Examples include the saturated sigmoid network BID18 and defensive distillation BID21, which uses a temperature-augmented softmax. Defensive distillation involves training a teacher network with temperature T and then training a distilled network with the same architecture on the softmax outputs of the teacher. The distilled network, trained with a temperature-augmented softmax, initially showed promising results against gradient-based attacks. However, it was later discovered that the network appeared robust due to masking gradients of the cross-entropy loss. Attacks on the logits instead of the softmax led to a recovery in success rate. Decision-based attacks, like the Boundary Attack, are immune to these defenses. The Boundary Attack was applied to two distilled networks trained on MNIST and CIFAR, using the same architecture as in section 3. The attack was able to break defenses based on gradient masking, showing that defensive distillation does not significantly increase network robustness. In real-world machine learning applications, attackers may only have access to the final decision, making decision-based attacks like the Boundary Attack effective. In this section, the Boundary Attack is applied to two models of the cloud-based computer vision API by Clarifai. The first model identifies brand names in natural images, recognizing over 500 brands, while the second model identifies celebrities, recognizing over 10,000 individuals. The attack only receives the name of the identified object, not the confidence score provided by Clarifai. Samples of natural images with brand names or celebrity portraits are selected for the attack. The Boundary Attack is applied to Clarifai models that identify brand names and celebrities in images. Adversarial perturbations were more difficult to achieve on Clarifai models compared to ImageNet models. Most perturbations resulted in slightly noticeable noise, but the original and adversarial images were often perceptually indistinguishable. The importance of decision-based adversarial attacks is emphasized in this study. The importance of decision-based adversarial attacks is highlighted, focusing on attacks that can find adversarial examples in models where only the final decision is observable. These attacks are crucial for real-world machine learning systems like autonomous cars and are more robust against common deceptions. The Boundary Attack is introduced as an effective method applicable to general machine learning algorithms and complex natural datasets. The Boundary Attack is a simple yet flexible method that follows the decision boundary between adversarial and non-adversarial samples using rejection sampling and a proposal distribution. It performs on par with gradient-based attacks on computer vision tasks and highlights the brittle information processing of current systems. The Boundary attack exposes the fragility of current computer vision architectures. Improvements can be made by optimizing the proposal distribution and considering the history of successful and unsuccessful proposals. Decision-based attacks are crucial for evaluating machine learning model robustness, especially for closed-source systems like autonomous cars. This attack is expected to drive future research in this field."
}