{
    "title": "BJ_QxP1AZ",
    "content": "Convolutional neural networks (CNNs) are crucial for computer vision advancement but face challenges in complexity and data requirements. To address these limitations, novel models for few-shot learning based on visual concepts within CNNs are developed, aiming for simplicity and interpretability. Using visual concepts in feature encoding, two intuitive models for few-shot learning are presented, achieving competitive performances and being more flexible and interpretable than alternative methods. CNNs have shown success in computer vision, with some claiming they surpass human-level performance, although other work suggests otherwise. Despite vulnerabilities to adversarial attacks, CNN successes have inspired the development of more sophisticated models in the computer vision community. The computer vision community aims to develop more sophisticated models, but understanding the effectiveness of CNNs remains limited due to their complex structures and non-linear nature. CNNs typically require large annotated datasets, posing challenges for real-world applications. Few-shot learning, inspired by human intelligence, is desirable for machine learning systems. This paper focuses on developing a simple and interpretable approach to few-shot learning, building on the successes of CNNs. The text discusses the learning of object parts from CNNs and the use of Visual Concepts (VCs) to detect semantic parts and represent objects. VCs are learned unsupervised and correspond to semantic parts of objects like cushions, windows, and wheels. Visual Concepts (VCs) are learned unsupervised to represent objects by binary codes, enabling few-shot learning. However, applying VCs to few-shot learning is challenging due to the need to learn VCs from a smaller number of examples compared to the large numbers used in training. Additionally, VCs were learned independently for each object category using deep network features from CNNs trained on datasets containing those categories. In this paper, VCs are used to learn models of new object categories from existing models of other categories. VCs are extracted by clustering intermediate-level raw features of CNNs, dividing them into a discrete dictionary. They can be learned in the few-shot learning setting and have desirable properties for image encoding. In few-shot learning, VCs are used for image encoding with category sensitivity and spatial patterns. A dictionary of VCs is learned to represent novel objects, enabling two models: nearest neighbor and factorizable likelihood model based on VC-Encoding. These models are flexible and can be applied directly to any few-shot learning scenarios. The flexibility of VCs in few-shot learning scenarios using CNNs is highlighted, showing potential for novel category learning. The models achieve comparable results to state-of-the-art methods, emphasizing the need to study CNN internal structures for better understanding. Our work focuses on using VCs for few-shot learning with CNNs, showcasing category sensitivity and spatial patterns. We introduce interpretable models that outperform existing methods on specific tasks and can be applied directly to other few-shot scenarios without additional training. This research intersects with understanding neural network representations and few-shot learning literature, exploring internal CNN structures for insights. The study explores internal representations in CNNs through visual cues, showing relationships between visual cues and semantic parts. It also discusses few-shot learning and the limitations of conventional data-demanding learning. The study discusses few-shot learning and the limitations of conventional data-demanding learning. Recent efforts in few-shot learning include embedding inputs into a feature space and meta-learning to efficiently train models with few examples. An alternative approach involves estimating parameters of the prediction layer using regression from previously learned objects. In BID23, visual cues (VCs) were discovered as internal representations in deep networks, playing a key role in understanding CNN properties and developing interpretable few-shot learning models. VCs are formalized within a hierarchy of lattices in CNNs, with modifications made for few-shot learning. Visual cues (VCs) are extracted from CNN feature vectors using an unsupervised clustering algorithm based on a mixture of von Mises-Fisher distributions. This approach removes spatial and image identity information, allowing for a population code representation of VCs. The VCs are generated by a mixture of von Mises-Fisher distributions and learned using the EM algorithm to maximize the likelihood function. Parameters like mixing proportion, mean direction, and concentration values are defined. The VCs are represented by mean directions of the learned distribution. Cosine distances are computed to understand the VCs. The VCs are computed by cosine distances from original feature vectors. Feature vectors with smallest distances to each VC are traced back to the input image, yielding visualization patches of VCs. These patches correspond to semantic parts of objects, supporting the assertion that VCs are semantic visual cues. Previous studies used CNNs trained for object classification tasks to extract VCs from hundreds of images within specific object categories, resulting in category-specific visual cues for interpreting CNN behaviors. In recent work, Visual Cues (VCs) were used to encode semantic parts and objects for detection tasks in the presence of occlusion. This paper introduces how VCs are learned in few-shot settings and proposes interpretable models for few-shot learning based on VC-Encoding. This approach is different from prior work on VCs, which focused on situations with many training examples. In few-shot learning tasks, Visual Cues (VCs) are learned from a small number of examples of novel object categories using features from CNNs trained on other categories. This approach ensures that the CNN used for feature extraction has never seen the novel categories before. By pooling feature vectors from different categories and performing clustering, VCs are extracted for novel categories with few examples, encouraging VC sharing between categories. By making modifications to encourage VC sharing between different categories and extracting few-shot VCs suitable for few-shot learning, this work differentiates itself from previous studies. Surprisingly, only a few images per category are needed to extract high-quality VCs, which possess similar properties as traditional VCs for few-shot object classification tasks. This process involves decomposing objects into semantic parts and establishing explicit relationships between features and VCs. The text discusses the process of establishing explicit relationships between features and visual concepts (VCs) for few-shot learning. The encoding threshold is determined through a grid-search to ensure sufficient coverage and a firing rate close to one. The final VC-Encoding has desirable properties such as category sensitivity. The VCs tend to fire intensively for specific object categories, providing useful information for object classification. The spatial patterns of VC firings also indicate the object category, despite spatial information being ignored during feature clustering. The VCs give binary maps with spatial patterns for images of the same category. Few-shot learning models are built based on VC-Encoding from few examples, using a fuzzy similarity metric for template matching. Images are classified during testing to the category of the training example. During testing, images are classified to the category of the training example with the largest similarity based on VC-Encoding. Convolutional operations in a neural network embed input images into feature spaces, decomposing inputs at different levels. VC-Encoding yields explicit semantic decomposition through matching templates and non-linear activation. Another method models the likelihood of VC-Encoding using a distribution. Few-shot learning is a challenging task where humans outperform current algorithms, requiring the ability to learn generalizable knowledge from limited examples. A distribution over VC-Encoding is specified using a Bernoulli distribution with probability \u03b8, assuming independence of elements. The likelihood of VC-Encoding is expressed for each object category y, with predictions made based on probabilistic distributions \u03b8. Implementing a discriminative model from a generative distribution, distributions \u03b8 are smoothed using a Gaussian filter to account for unlikely events. Our experiments show that VC-Encodings can enable few-shot learning models to perform competitively with existing methods like BID17. These models can be applied to a wide range of few-shot scenarios without additional training, showcasing the potential of trained CNNs to recognize novel objects from limited examples. The capability of our few-shot methods is evaluated on the Mini-ImageNet dataset, demonstrating their effectiveness in this common few-shot learning benchmark. The Mini-ImageNet dataset, proposed by BID22, is used as a benchmark for few-shot learning. It consists of 100 categories with 600 examples each. Trials of few-shot learning are conducted with 5 unseen categories sampled during testing. Performance is compared against baselines and state-of-the-art methods. The curr_chunk discusses the average classification accuracies on Mini-ImageNet using different methods, including nearest neighbor and factorizable likelihood. It also mentions the best published results and the extended setting for variance in the number of categories. The methods evaluated include Matching Network, Meta-Learner, and MAML. The curr_chunk discusses training a VGG-13 network with cross entropy objective on Mini-ImageNet. 10% of images are preserved for validation. 200 VCs are extracted from Pool-3 layer. Gaussian filter with \u03c3 of 1.2 is used for smoothing factorizable likelihood model. Nearest neighbor matching with raw features from Pool-3 layer is included. Few-shot learning ability is evaluated with variance in number of categories. The results show that VCs-based methods perform well in few-shot learning compared to current methods. They outperform Meta-Learner and matching network in 1-shot and 5-shot setups, slightly behind MAML. Low level visual cues in trained CNNs can naturally support few-shot learning. The likelihood model is significantly better than the nearest neighbor model in the 5-shot scenario. The likelihood model outperforms the nearest neighbor model in the 5-shot scenario. The inference procedure involves transforming input to Pool-4 Raw Features, calculating Pixel-wise Likelihood, aggregating into Class-wise Likelihood, and choosing the class with the highest likelihood. Visualizing the process using VC-Encoding in a factorizable likelihood model. The likelihood model combines training examples into distributions, outperforming nearest neighbor models in the 5-shot scenario. It involves transforming input to Pool-4 Raw Features, calculating Pixel-wise Likelihood, aggregating into Class-wise Likelihood, and choosing the class with the highest likelihood. The variance of aggregated likelihood is rescaled to 1 for better visualization. This approach allows for combining different features from training examples into distributions, unlike nearest neighbor models which can only match individual examples. Our few-shot learning methods based on VC-Encoding can easily handle changes in the number of shots and categories, unlike previous methods like Meta-Learner LSTM. We apply our methods to the PASCAL3D+ dataset, originally tailored for 3D object detection and pose estimation, to visualize the inference process. Our few-shot recognition approach involves visualizing every step of the inference process using large VC-Encoding distribution maps. By analyzing VCs on the PASCAL3D+ dataset, we can interpret the visualizations easily. For example, we identify a VC related to the corner of a TV Monitor by analyzing the distribution maps. This VC mainly responds to the upper right corner of TV Monitors, which is reflected in the VC-Encoding. The VC-Encoding indicates that a specific VC fires on the upper right corner of the input. By calculating pixel-wise likelihoods from image distributions, it is evident that most categories have low likelihood in this corner area. Evaluation is done on PASCAL3D+ dataset, with different models compared for classification accuracy. For testing on PASCAL3D+, objects are cropped using annotated bounding boxes and resized to 224 \u00d7 224. Pool-4 features from VGG-16 are utilized for few-shot methods due to the larger input image size. Two baseline models are proposed for comparison. During evaluation on PASCAL3D+, VCs significantly outperform baseline models using VGG-16 features. VC-Encoding proves to be a useful semantic decomposition of images, showing superior performance in nearest neighbor methods by transferring raw feature vectors into VC distances. In this paper, the challenge of developing simple interpretable models for few-shot learning using CNN internal representations is addressed. The use of Visual Concepts (VCs) improves interpretability and performance, with VCs extracted from a small set of images of novel object categories. Two novel methods for few-shot learning are proposed based on category sensitivity and spatial patterns of VC-Encoding. Our methods for few-shot learning using Visual Concepts (VCs) and VC-Encodings show comparable performances to specialized state-of-the-art methods. The flexibility of our models allows them to be applied to different few-shot scenarios with minimal re-training. Future work includes improving VCs quality and extending the approach to few-shot detection. In Section 4.2, real-valued distances from VCs are transformed into binary VC-Encoding by a dynamically determined threshold for coverage and error rate. This encoding benefits learning with limited budgets, enabling the implementation of the Factorizable Likelihood Model and filtering out noise from model estimations based on few training examples. In Table 1, models based on VC-Encoding are compared with a model using real-valued distances (VC-Distance) for Mini-ImageNet. Results show that binary coded models outperform the distance-based model, including the original CNN features. VCs are obtained through unsupervised clustering of CNN-extracted features. In our proposed models, VCs are obtained through unsupervised clustering using a mixture of von Mises-Fisher distribution. Results show that VC-based models are robust to different clustering methods, with von Mises-Fisher slightly outperforming K-Means on a 5-shot 5-category learning task. In our proposed models, VCs are obtained through unsupervised clustering using a mixture of von Mises-Fisher distribution. Results show that VC-based models are robust to different clustering methods, with von Mises-Fisher slightly outperforming K-Means on a 5-shot 5-category learning task. Different scales of semantic parts were tested, with a single scale chosen for simplicity. Pool-3 features of VGG-13 were used in the CNN. The proposed scale of VCs was found to be the most effective, outperforming other scales in the few-shot learning setting. The performance of VC-based models remained stable while the baseline method using original features for nearest neighbor saw drastic drops in performance as scale changed. The performance of VC-based models, particularly the Likelihood model, shows more gradual decay compared to the baseline method when scale changes. This highlights the robustness of VC-based models. Additional VCs are illustrated in FIG0 and their distributions over object categories in TAB8."
}