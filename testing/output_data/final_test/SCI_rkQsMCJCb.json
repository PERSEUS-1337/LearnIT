{
    "title": "rkQsMCJCb",
    "content": "Most existing GANs architectures use fixed upsampling algorithms, but a novel adaptive convolution method is proposed to address this limitation. By learning the upsampling algorithm based on local context, the modified models show significant improvement on CIFAR-10 dataset and achieve state-of-the-art performance on CIFAR-10 and STL-10 datasets in unsupervised settings. Generative Adversarial Networks (GANs) generate realistic images from noise through a minimax game where a generator creates data from noise and a discriminator distinguishes real from generated data. GANs are challenging to train effectively, requiring careful selection of hyper-parameters and network architectures. While successful on visually similar datasets like bedroom scenes and faces, GANs struggle with diverse visual categories like ImageNet, only learning basic image statistics and shapes. Recent works have addressed the limitations of GANs trained on ImageNet by incorporating high-level information to guide the generator. One approach is training the discriminator in a semi-supervised manner or adding a second training objective to direct the generator towards similar samples from the training set. Another method involves using artificial class labels through clustering in the representation space. The rigidity of the normal convolution operator is believed to contribute to the difficulty of GANs in learning on diverse visual datasets. Many GANs generators use fixed convolutions or resize-convolution to upsample low-resolution feature maps, which can be unintuitive due to varying pixel locations. The proposed Adaptive Convolution Block (AdaConvBlock) addresses the limitations of GANs generators by learning to generate convolution weights and biases adaptively based on local feature maps at each pixel location. This helps the generator to learn different upsampling schemes for diverse object categories, especially in early upsampling layers where higher level information is crucial. AdaConvBlock enables the generator to learn upsampling algorithms based on local context, mimicking the intuitive process of human drawing. Experiments show that replacing convolutions with AdaConvBlocks in GANs improves qualitative and quantitative performance on CIFAR-10 dataset. Just replacing convolution in upsampling can have a significant impact. The lowest resolution feature map with adaptive convolution significantly impacts the baseline model. Best models achieve state-of-the-art unsupervised performance on CIFAR-10 and STL-10 datasets. Generative Adversarial Networks (GANs) involve a generator G mimicking real data against a discriminator D distinguishing generated samples. Training alternates in a minimax game formulation. In Generative Adversarial Networks (GANs), the generator function G maps random noise to real data space, while the discriminator function D determines the probability of data belonging to the real distribution. Training alternates between updating the discriminator and generator. G and D can be any differentiable functions, often implemented as convolutional neural networks for image generation. The discriminator is trained to convergence before updating the generator to prevent overfitting on finite datasets. The discriminator in GANs is usually constructed as the reverse of the generator, using strided convolutions to downsample feature maps. Training GANs is challenging due to the balance between the strength of the generator and discriminator, mode collapse issues, and sensitivity to hyperparameters. Various approaches, like WGAN BID1, have been developed to address these challenges. Spectral Normalization GANs control the Lipschitz constant of the discriminator by dividing the weight matrix of each layer by its spectral norm. This method has been shown to make GANs robust to hyperparameter choices without significant overhead. In this paper, a novel Adaptive Convolution Block (AdaConvBlock) is proposed to enhance GANs' performance on visually diverse datasets. AdaConvBlock increases the generator's capacity to learn complex multimodal distributions. The network architectures utilizing AdaConvBlocks are detailed in section 5.2, focusing on replacing normal convolutions. The paper introduces the AdaConvBlock to improve GANs on diverse datasets by enhancing the generator's ability to learn complex distributions. It focuses on replacing normal convolutions with rearranged input volumes and applying AdaConvBlock for better performance. The implementation in Tensorflow limits experimentation to Adaptive Convolution Blocks. The AdaConvBlock introduces adaptive convolutions where each spatial location has unique weight and bias variables generated based on local information. This approach improves GANs by enhancing the generator's ability to learn complex distributions. The AdaConvBlock utilizes four matrices to regress adaptive weights and biases for each pixel location, with K adaptive controlling the amount of local information used. This approach enhances GANs by improving the generator's ability to learn complex distributions. The AdaConvBlock uses adaptive weights and biases for each pixel location, enhancing GANs by improving the generator's ability to learn complex distributions. The operation of computing adaptive convolution weights is costly, so depthwise separable convolution is used to reduce computation cost and memory usage. Depthwise separable convolution replaces normal convolution with two convolutions: depthwise convolution acts on each channel, followed by pointwise convolution. The first convolution has costs proportional to K^2 adaptive x C_in x C_depthwise, while the second has costs proportional to DISPLAYFORM4 x C_out. Choosing C_depthwise = 1 reduces memory and computation costs by roughly K^2 adaptive. The AdaConvBlock reduces memory and computation costs by roughly K^2 adaptive times. It includes adaptive convolution weights and biases, with the option to apply additional transformations to the input volume. However, some transformations may decrease network performance. BID20 can exponentially increase the receptive field for regression of weights and biases, making object shapes more coherent. Using multiple 3x3 dilated convolutions made training unstable, but increasing K adaptive of the adaptive convolution achieved the same effect without drawbacks. Adding 1x1 convolutions before regression was detrimental to the generator's performance. Batch normalization was not applied in AdaConvBlock to regress weights and biases, with a non-linearity applied after convolution. The AdaConvBlock utilizes ReLu activation for regression of weights, avoiding activation for biases to prevent output limitations. Depthwise separable convolutions with a depth multiplier of one are used to reduce memory and computation costs while regressing adaptive convolution weights, increasing the receptive field without additional costs. The choice of depth multiplier was determined through experiments. The depth multiplier choice was determined through experiments, increasing it led to higher costs and slower training progress without improving model performance. Previous works have focused on improving GAN performance on datasets with high visual variability, such as BID17 proposing a semi-supervised training method that turns the discriminator into a K + 1-way classifier. Warde-Farley & Bengio (2016) trained a denoising auto-encoder on the discriminator's penultimate layer features. The authors trained a denoising auto-encoder on the discriminator's penultimate layer features to guide generated samples towards higher probability configurations. BID7 used k-means clustering on the last hidden layer to create artificial classes for GAN training, known as denoising feature matching. Our approach aims to improve the generator output by utilizing artificial class labels and clustering techniques during GAN training. The discriminator constructs probability distributions over the artificial class labels, with clusters splitting when reaching a sample threshold. This method differs from others by focusing on enhancing sample generation without relying on additional high-level information. Our method aims to enhance generator output by improving the architecture, inspired by adaptive convolution in video frame interpolation. The encoder-decoder network extracts features for interpolation using 1D kernels estimated by four subnetworks. The baseline generator architecture includes convolutional layers with different output channels and activation functions. Our method improves generator output by utilizing adaptive convolution techniques for interpolation. The design differences between our work and previous methods stem from the scalability of our model and the range of convolution filters used. Experiments on CIFAR-10 and STL-10 datasets are conducted in an unsupervised setting without additional training objectives. Spectral Normalization is applied to enhance model performance. The training process utilizes training objectives, Spectral Normalization for stabilizing training, zero padding for convolutions, and Adam optimizer with specific parameters. Inception score metrics are computed over randomly generated images, and the model with the highest score is selected. The baseline architecture is based on the Spectral Norm GAN. Transposed convolutions in the generator network are replaced. The generator architecture in the Norm GAN BID11 model is modified by replacing transposed convolutions with resize-convolution for upsampling. The generator has six layers, with the first layer generating Gaussian noise followed by a fully connected layer. The base channels are reduced from 512 to 128 to fit into GPU memory. The discriminator remains unchanged from the BID11 model, leading to a low Inception score. The baseline generator model is enhanced by replacing 3x3 convolutions with AdaConvBlocks in the last layers. This modification improves model performance slightly. AdaConvBlocks maintain filter size and output channels, with M g = 4 for CIFAR-10 and M g = 6 for STL-10. Specific details like K adaptive for each AdaConvBlock are not provided. The architecture uses AdaConvBlocks to replace normal convolutions in the baseline generator model. Due to memory constraints, the number of base channels had to be reduced from 512 to 128. The models are named based on the number of AdaConvBlocks used, such as AdaGAN-1 and AdaGAN-2. The memory and computation cost of AdaConvBlocks grow cubically with the number of input channels. AdaGAN-2 and AdaGAN-3 models replace convolutions with AdaConvBlocks. AdaGAN has all 3x3 convolutions replaced. The choice of K adaptive is crucial for performance. K adaptive is fixed for all AdaConvBlocks in an architecture. Architecture names include K adaptive value. Performance is compared on CIFAR-10 dataset using K adaptive = 3 for all AdaConvBlocks. In this experiment on the CIFAR-10 dataset, AdaConvBlocks are used to replace normal convolutions in the generator architecture. The Inception score increases with the number of AdaConvBlocks used, with the biggest impact seen when replacing the convolution in the first upsampling layer. The benefits of AdaConvBlocks diminish in subsequent layers but still outperform models using normal convolutions. Using AdaConvBlocks in the generator architecture significantly improves performance compared to normal convolutions, as seen in the AdaGAN models on the CIFAR-10 dataset. Increasing the number of AdaConvBlocks leads to higher Inception scores, with the biggest impact in the first upsampling layer. The flexibility of AdaConvBlocks plays a crucial role in enhancing the generator's performance, confirming the limitations of normal convolutions in upsampling layers. Additionally, a model with K adaptive = 5 shows a slight improvement over K adaptive = 3, with both AdaGAN models achieving state-of-the-art performance. The AdaGAN models show improved performance on CIFAR-10 compared to other methods, with unsupervised Inception scores presented in Table 4. Samples generated by AdaGAN models are shown in Figure 2 and 3. For STL-10 experiments, AdaGAN models are trained on the unlabeled set and downsampled images. A larger K adaptive is beneficial for STL-10 due to its larger image size. AdaGAN architectures converge slower on STL-10, requiring training for 400000 iterations. The AdaGAN models achieved state-of-the-art performance on visually diverse datasets like CIFAR-10 without using augmented training objectives. They were able to learn global context well and generate reasonable sample quality. The adaptive convolutions in the GANs generator improved the performance significantly, with AdaGAN-5x5 and AdaGAN-7x7 models outperforming others. Samples generated by AdaGAN models demonstrate their ability to learn object shapes effectively. The success of AdaGAN models on CIFAR-10 dataset indicates potential for performance improvement by modifying GAN architectures. While our approach differs from others injecting high-level information into the discriminator, it can complement existing methods. However, the high memory and computation costs remain a challenge, which could be alleviated with additional tricks. The AdaConvBlock in our model uses 1-D convolutions to reduce costs by over 50%. We aim to exploit locality by interpolating adaptive convolution weights and biases of neighboring pixel locations. Future work will address this issue."
}