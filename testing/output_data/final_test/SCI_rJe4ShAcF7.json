{
    "title": "rJe4ShAcF7",
    "content": "Music relies heavily on repetition to build structure and meaning. The Transformer, a sequence model based on self-attention, has shown promising results in tasks requiring long-range coherence. Existing approaches for representing relative positional information in the Transformer are impractical for long sequences like musical compositions. A new algorithm is proposed to reduce memory requirements, enabling better modeling of music. Our modified relative attention mechanism in the Transformer reduces memory requirements for long musical compositions, enabling the generation of minute-long pieces with compelling structure. State-of-the-art results were achieved on the Piano-e-competition dataset. The model references past elements to create coherent and varied compositions. The Transformer model with self-attention is well-suited for tasks like music generation. By using self-attention in multiple layers, the model can capture different levels of self-referential phenomena in music. Unlike recurrent neural networks, which struggle with storing elements for reference, the Transformer's approach allows for easier training. The model uses absolute position representations, unlike RNNs and CNNs which use relative position modeling. The Transformer model with self-attention is effective for music generation, capturing various self-referential phenomena. Unlike RNNs and CNNs, it uses absolute position representations. BID13 introduced a relation-aware self-attention to capture pairwise relations between representations, improving sample quality and perplexity for the JSB Chorales dataset. This approach maintains the regular timing grid in the dataset and captures global timing. The model introduces a more memory-efficient way to apply relative attention to long sequences in the Maestro dataset of piano music. It adopts a sparse, event-based representation to capture expressive dynamics and timing with high granularity. The model introduces a sparse, event-based representation for music with high granularity. Relative attention improves perplexity and sample quality by capturing idiomatic piano gestures and recurring motifs. The Transformer with relative attention can generalize beyond trained lengths, as confirmed in experiments. It is the first successful use of Transformers in generating music with long-term structure, surpassing LSTMs in scale and quality. Relative self-attention is crucial for model quality, leading to more coherent samples in listening tests. Transformers with relative self-attention can generate accompaniments conditioned on melodies, allowing interaction with the model. An algorithmic improvement reduces memory requirements significantly, enabling training on longer sequences while using GPUs efficiently. The use of relative self-attention Transformers with 64 heads allows for training on long sequences using GPUs. Sequential models have been traditionally used for music modeling, such as Hidden Markov Models, RNNs, and LSTMs. Pianorolls can be represented as multi-hot pitch vectors and modeled using CNNs or other methods like Restricted Boltzmann Machines. Self-attention is a key mechanism in Transformer models, allowing for the mapping of input through different projections to queries and keys. Recent works have focused on applying and improving self-attention for tasks like image generation, speech, and summarization. The challenge lies in scaling attention computationally to long sequences due to the growing time and space complexity. The time and space complexity of self-attention grows quadratically in long sequences, especially problematic for relative self-attention. Music is represented as discrete tokens in a language-modeling approach, with different genres requiring unique serialization methods. The JSB Chorale dataset represents choral music as a matrix of voices and time discretized to sixteenth notes, serialized in raster-scan fashion. The Maestro dataset includes expressive timing information at finer granularity and more voices than the JSB Chorale dataset. It uses a performance encoding with a vocabulary of NOTE_ON events, NOTE_OFFs, TIME_SHIFTs, and VELOCITY bins for dynamics. The Transformer decoder is an autoregressive generative model with self-attention mechanisms and position information. The Transformer model utilizes attention mechanisms and position information to process the Maestro dataset, incorporating relative positional self-attention for timing information representation. The Transformer model incorporates relative positional self-attention by using a separate relative position embedding for each possible pairwise distance between query and key positions. This approach improves memory efficiency while infusing relative distance information in the attention computation. The implementation of relative attention in the Transformer model improves memory efficiency by reducing intermediate memory requirements. By directly multiplying Q with the relative position embedding Er, the necessary terms from QR are obtained. Skewing the QE r matrix ensures that the relative logits are correctly positioned for matching with QK indexing. This procedure is illustrated in detail in the next section. The \"skewing\" procedure, detailed in the next section, transforms a matrix from absolute-by-relative (i q , r) indexing to absolute-by-absolute (i q , j k ) indexing. This method is 6x faster at length 650 compared to prior work. The steps involve padding a dummy column vector, reshaping the matrix, and slicing to retain specific rows and columns. The local attention method reduces memory requirements for long sequences by chunking the input sequence into blocks that attend to themselves and the one before. Relative attention is extended to the local case by using smaller blocks with unmasked relative indices. The learned E r for the local case has a specific shape. The learned E r for the local case in the context of reducing memory requirements for long sequences by chunking the input sequence into blocks with specific shapes. The J.S. Bach Chorales dataset is used for evaluating generative models for music. Position representations improve grammar learning, enhancing relative attention and sample quality. The model captures timing on a global level, leading to regular phrasing. Absolute timing is also enhanced by concatenating sinusoids to input embeddings. Performance is improved for both baseline and relative transformer models. Comparison is made against COCONET, a top-performing model on the 16-note grid dataset split. The text discusses the evaluation of models on a 16-note grid dataset split. Transformer models with attention mechanisms are implemented, with parameters such as number of layers, attention hidden size, and feedforward hidden size being tuned. To capture relational information, relative attention is extended to include pairwise distances on attributes like timing and pitch. Separate relative embeddings are learned for timing and pitch intervals. The approach involves embedding pairwise pitch interval and relative information for timing and pitch intervals. Memory complexity is O(L^2 D) due to explicit gathering of relative embeddings. Training involves random crops of 2048-token sequences with data augmentation for pitch transpositions and time stretches. Evaluation includes segmenting sequences into 2048 length subsequences. The evaluation process involves segmenting sequences into 2048 length subsequences, with 1128 and 1183 subsequences in the validation and test set respectively. Each subsequence is evaluated by running the model forward once with teaching forcing, and the overall negative loglikelihood (NLL) is averaged on the token level. The results are compared to PerformanceRNN and LookBack RNN models. LookBack RNN requires monophonic music with barlines, which is not present in the dataset. The evaluation process involves segmenting sequences into 2048 length subsequences, with 1128 and 1183 subsequences in the validation and test set respectively. Each subsequence is evaluated by running the model forward once with teaching forcing, and the overall negative loglikelihood (NLL) is averaged on the token level. The results are compared to PerformanceRNN and LookBack RNN models. LookBack RNN requires monophonic music with barlines, which is not present in the dataset. Orderings are assumed to be uniformly distributed, and the loss is computed by averaging losses over multiple random orderings. Coconet is evaluated as an autoregressive model under the chronological ordering, showing that sample quality is better with Gibbs sampling compared to autoregressive generation. Transformer-based architectures fit the dataset better than LSTM-based models, with attention mechanisms implemented in the Tensor2Tensor framework using default hyperparameters for training. Four architectures are compared, varying on global versus local, and regular versus relative attention. Reducing query and key channel size to three forth of hidden size works well for the dataset. Block size 512 is used for local attention, while relative global attention considers half the training sequence length. Relative local attention considers the full memory length. Memory-efficient relative attention outperforms regular attention in both global and local cases. Global attention outperforms local attention in both relative and regular cases, with the advantage of directly looking back for repeating motifs. Relative local attention allows for deeper models and longer sequences, benefiting text and image generation work. When primed with an initial motif, models perform differently: Transformer with relative attention elaborates the motif, creating varied phrases with clear contour, while LSTM drifts off to other material. Relative attention generalizes to longer lengths than trained, outperforming baseline Transformer. See Appendix C for visualizations. The study experimented with a sequence-to-sequence setup of Transformers for a conditioned generation task involving melodies and accompaniment. Relative attention was used on the decoder side, improving performance. A listening test compared models trained on the Maestro dataset, showing the effectiveness of the Transformer with relative attention compared to other models like LSTM. The study compared the performance of models using relative attention, including PerformanceRNN (LSTM), in a listening test. Participants rated musical excerpts from different models on a Likert scale. Results showed that using relative attention improved sample quality over the baseline Transformer model. LSTM performed better overall but not in direct comparisons with the Transformer. The study demonstrated that the Transformer with relative attention is well-suited for generative music modeling, showing significant improvement in sample quality compared to the baseline Transformer. The model's ability to capture long-term structure and expand upon a prime suggests potential creative applications. The findings highlight a limitation of the original Transformer in capturing periodicity at different time scales. Improving the Transformer's ability to capture periodicity at various time scales and relations between scalar features could enhance time-series models. The memory-efficient implementation allows for relative attention on longer sequences like texts or audio waveforms, expanding its applicability. Adapting sequence models for music involves decisions on serialization of polyphonic textures, with different data types requiring specific representations. The J.S. Bach Chorales dataset uses a serialized grid-like representation for four-part score-based choral music with a time resolution of sixteenth notes. The pianoroll representation of a four-part score-based choral music piece is visualized as a grid, with each voice represented by rows and time steps by columns. The grid is serialized into a sequence by interleaving the voices at each time step. Each token in the sequence represents a pitch, with the most common sequence length being 1024. The opening measure of BWV 428 is visualized as a pianoroll with different voices represented by pitches. The second dataset, Maestro BID7, consists of polyphonic piano performances with expressive timing and dynamics, serialized into a sequence of one hot encoded events. The sustain pedal control events in MIDI are used to extend the duration of notes based on when the pedal is pressed or released. This involves converting MIDI note events into a sequence of NOTE_ON, NOTE_OFF, TIME_SHIFT, and SET_VELOCITY events. The velocity for future NOTE_ON events is represented by events quantized into 32 bins. A piano performance example shows a C Major chord arpeggiated with sustain pedal control. Participants heard two musical excerpts with a common priming sequence, followed by a continuation sampled from models or extracted from the study. Participants were asked to rate musical excerpts on a Likert scale of 1 to 5 based on their perceived musicality. Each pair of excerpts, generated from different models, was compared in 60 pairwise evaluations. The study involved participants rating musical excerpts from different models in 180 pairwise comparisons. Statistical analysis revealed significant differences between the models. Post-hoc analyses were conducted using various tests to compare the models' performances. The relative Transformer model was consistently rated as more musical than the baseline Transformer. The study found that the Relative Transformer model was perceived as more musical than the baseline Transformer and LSTM in aggregate comparisons. However, there were no consistent statistically significant differences within pairs. Real music from the validation set outperformed both LSTM and baseline Transformer in aggregate comparisons. In comparisons, real music outperformed LSTM and baseline Transformer, with no significant difference from Relative Transformer. Attention-based models allow visualization of attention distribution, showing how recurring structures are built. Visualizations of pianorolls from Transformer with relative attention highlight query and previous memories attended to, with different heads represented by color and weight by line width. Figure 8 shows a piece with a recurring triangular contour, where the query attends to previous high notes. Figures 10 and 11 illustrate steps for \"skewing\" matrices into different indexed forms for global and local attention."
}