{
    "title": "ryxeB30cYX",
    "content": "Existing neural networks are vulnerable to \"adversarial examples\" created by adding small perturbations to inputs to induce misclassification. Adversarial training, the most investigated defense strategy, augments training data with adversarial examples. Single-step adversaries in training can lead to overfitting, while multi-step training achieves state-of-the-art performance but requires significant time. To address overfitting in single-step training, we propose Stochastic Quantized Activation (SQA), which provides random selectivity to activation functions and enables robustness comparable to multi-step training in a faster manner. Our method, Stochastic Quantized Activation (SQA), enhances neural network robustness against adversarial attacks with single-step training. It outperforms PGD training in robustness with lower computational cost. CNNs are crucial for various computer vision tasks, but their vulnerability to adversarial attacks poses a threat to security-sensitive systems. SQA offers a unique approach to handling strong adversaries, different from existing methods. Adversarial attacks are a serious problem in neural networks, as they can transfer across different models, enabling black-box attacks. Adversarial training is a common defense strategy, but it requires extra time and may still leave the model vulnerable to new attacks. Recent research has shown that adversarial training using Projected Gradient Descent (PGD) can create models that are resistant to attacks on MNIST and CIFAR-10 datasets. However, this method is time-consuming as it requires iterative steps, with PGD training taking significantly longer than FGSM training. The goal is to find a universal method that is resistant to all attacks while minimizing computational costs. The proposed Stochastic Quantized Activation (SQA) reduces the complexity of neural network boundaries, making it harder for adversaries to attack. SQA, when combined with fast adversarial training, provides robustness comparable to PGD training but with lower computational cost. It can be easily integrated into deep learning models and offers a certain level of defense against adversarial attacks. In this paper, the authors propose Stochastic Quantized Activation (SQA) as a defense strategy against adversarial attacks. They categorize existing defense methods, introduce the SQA procedure, present experimental results on MNIST and CIFAR-10 datasets, and compare SQA with fast adversarial training. The study concludes that SQA combined with FGSM training achieves robustness comparable to state-of-the-art PGD adversarial training at a lower computational cost. The proposed method, Stochastic Quantized Activation (SQA), is efficient and flexible, making it applicable to existing deep neural networks and can be combined with other defense strategies. SQA is shown to make the model robust against adversaries at high and low levels using t-SNE and activation maps. Adversarial examples are defined as input x with added noise \u03b7, causing a different prediction from the true label. Fast Gradient Sign Method (FGSM) is a quick single-step method for crafting adversarial examples. The Fast Gradient Sign Method (FGSM) is a fast single-step method for creating adversarial examples proposed by BID10. It is based on the effects of linear summation in DNNs and involves perturbing the input to increase loss and move the prediction away from the extrema. Projected Gradient Descent (PGD) is a multi-step variant of FGSM and is one of the strongest known white box attacks. The DISPLAYFORM1 attack is a strong optimization-based iterative attack that uses Adam to optimize over the adversarial perturbation \u03b7 n. Adversarial training increases robustness by augmenting training data with adversarial examples, improving classification accuracy. The high cost of generating adversarial examples and patching them into training data is a challenge for practical adversarial training on large datasets like ImageNet. Fast-generated adversarial examples using FGSM are used for training, but they may not increase robustness, leading to overfitting. To address this, training with multi-step FGSM k, PGD adversaries has shown state-of-the-art performance on MNIST and CIFAR-10. Obfuscated Gradients, such as Shattered Gradients, Stochastic Gradients, and Exploding & Vanishing Gradients, make it difficult to generate adversaries by lacking useful gradients. The method introduced in BID1 successfully circumvents defenses with 0% accuracy on 6 out of 7 defenses at ICLR2018. SQA is a combination of shattered gradients and stochastic gradients, showing robustness against various attacks, including those breaking obfuscated gradients. The approach focuses on quantizing activations without affecting weight vectors, maintaining full-precision weight representation without significant training slowdown in PyTorch. SQA is a stochastic activation function that introduces quantized threshold effects into vanilla CNNs, improving robustness against adversarial attacks. It maintains full-precision weight representation without significant training slowdown in PyTorch. The algorithm in Algorithm 1 involves Min-Max normalization and scaling of latent space outputs from convolutional layers. The process includes normalization of values to a range of 0 to 1, scaling to a specified level using a factor \u03bb, and stochastic quantization to converge values to the closest or second closest integers. This method aims to rescale the quantized values back to the original output range. The text chunk discusses the rescaling of values within the original range using a straight through estimator in the context of normalization and scaling of latent space outputs from convolutional layers. The feasibility of this approach is demonstrated in experiments on MNIST and CIFAR-10 datasets using PyTorch BID24. The text chunk discusses using two Vanilla CNNs with different channel sizes and increasing channels by a factor of 2. They are denoted as SMALL and LARGE with (16, 32, 64) and (64, 128, 256) filters. Stochastic Gradient Descent (SGD) is used with specific parameters, and ResNet models are used as baselines for CIFAR-10 dataset. Stochastic quantization shows higher accuracy than deterministic quantization during training. Stochastic quantization demonstrates higher accuracy compared to deterministic quantization. SQA is applied to ResNet's first layer and bottleneck module with \u03bb values of 1 and 2. MNIST training parameters are used with 350 total epochs and a learning rate decrease of 0.1 every 150 steps. Various l \u221e intensity levels are tested for attacks on MNIST and CIFAR-10 datasets. Different attack parameters such as step sizes and number of steps are specified for FGSM, C&W, and PGD attacks. The study investigates the impact of layer-wise quantization on network robustness. Results show that applying quantization on earlier layers increases robustness, with binary quantization on the first layer and ternary quantization on the second layer providing less accuracy degradation. This supports the argument that small perturbations in lower-level representations lead to larger perturbations in higher-level representations. Layer-wise quantization improves network robustness, with binary quantization on the first layer and ternary quantization on the second layer showing less accuracy degradation. SQA models do not overfit to adversaries, exhibiting high accuracy on unseen adversarial examples. There is a correlation between robustness and model capacity. The experiment confirms that increasing model capacity improves robustness against strong adversaries. LARGE SQA outperforms SMALL SQA against FGSM attacks and is more than ten times robust against PGD attacks. Model capacity not only enhances robustness but also prevents overfitting. Experiments on CIFAR-10 using ResNet BID11 show the effectiveness of SQA on RGB image datasets. The SQA module helps prevent overfitting to FGSM adversaries, with larger capacity leading to higher robustness against attacks. Comparison with other defenses like BID22, SAP, PixelDefend, and Thermometer BID6 BID25 BID3 shows performance against PGD and C&W attacks. Architecture differences make exact comparisons challenging. Our method is more robust against gradient-based PGD attacks compared to optimization-based C&W attacks, achieving a state-of-the-art accuracy of 52% against PGD. It also shows good accuracy against C&W attacks similar to Adv. Training. Other methods based on obfuscated gradients struggle to defend against these strong adversaries. Time complexity of adversarial training is explored, considering single-steps and multi-steps with parameters \u03c4, \u03ba, and \u03c5 representing processing times. In this subsection, the time complexity of adversarial training is defined, comparing SQA + FGSM training with PGD training. SQA + FGSM training is found to be 18 times faster than PGD training. The penultimate layers of the network trained with this method are analyzed, showing comparisons with full-precision networks using C&W attacks. Visualization of the penultimate layers is done using t-SNE and activation maps with clean images and adversarial examples. Our method demonstrates that the full-precision network trained with FGSM struggles against adversarial attacks, while our model shows less broken clusters. The robust classifier requires a complex decision boundary, which our model seems to have learned by generating adversarial examples. Additionally, analyzing the penultimate layer reveals that the highest activation values remain consistent under adversarial conditions. Our method, utilizing SQA, provides stable activation frequencies against adversarial attacks, preventing overfitting to FGSM training. It enhances robustness and reduces computational costs, showing improvements comparable to PGD training. The network learns strong adversaries without overfitting, demonstrating a robust decision boundary. In future work, the SQA method will be tested on large image datasets to further enhance its efficiency and flexibility."
}