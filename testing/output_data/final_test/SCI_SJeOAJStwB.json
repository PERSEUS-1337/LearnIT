{
    "title": "SJeOAJStwB",
    "content": "Federated learning is a promising approach for distributed training of deep networks, offering high communication-efficiency and privacy-preservability. However, challenges arise with non-IID training data on local devices. A study on hyperparametric conditions under non-IID environments reveals insights on parameter divergence and performance degradation. Additionally, the effects of optimizers, network depth/width, and regularization techniques are revisited. The study explores the impact of hyperparameter optimization strategies in non-IID data settings, noting diminishing returns. It categorizes failure cases based on parameter divergence metrics. Federated learning has been successful in reducing communication overhead in distributed training, allowing for local updates over multiple iterations for improved efficiency compared to traditional approaches. Federated learning reduces data privacy risks by concealing on-device data from the server or other learners. It is effective for environments with highly private data and is emerging as a promising methodology for privacy-preserving distributed learning. The approach involves iterative parameter averaging of local updates from each learner's dataset, enabling efficient shared model learning without centralizing training data. Handling the decentralized non-IID data in federated learning remains a statistical challenge due to the heterogeneity of training data distributions across learners. Severe performance degradation has been observed under highly skewed non-IID data, with diminishing returns as the probabilistic distance of learners' local data from the population distribution increases. To address this issue, various contributions have been made in the field. In this paper, the authors explore the effects of hyperparameters on addressing the non-IID issue in federated learning. They investigate parameter divergence of local updates as a root cause of performance degradation from non-IID data. The excessive magnitude of parameter divergence can disrupt the performance of the consequent parameter. The authors discuss the effects of optimizers, network depth/width, and regularization techniques for the first time. The study investigates the origin of parameter divergence in non-IID data environments and analyzes the internal reasons for failures using parameter divergence metrics. The findings conflict with hyperparameter optimization methods' positive outcomes under \"vanilla\" training or IID data settings. The excessive magnitude of parameter divergence, steep fall phenomenon, and high training loss of local updates are identified as reasons for performance degradation. The study considers Algorithm 1 as a federated learning method based on FedAvg. In this study, a federated learning method based on FedAvg is discussed, which has been widely debated in the literature. Experiments were conducted using Tensorflow on CIFAR-10 and SVHN datasets for multi-class classification tasks. The term hyperparameter optimization methods is used interchangeably with hyperparametric methods. Google currently employs this algorithm in their Gboard mobile keyboard application. In this study, the baseline network model for image classification is a CNN with three convolutional layers and three fully-connected layers. Weight decay regularization is applied using decoupled weight decay method. The source code for the study is available on GitHub. The study uses a CNN model for image classification with weight decay regularization. The configuration includes 10 learners with 5000 nonoverlapping training examples each, focusing on data balanced cases. Non-IID(N) data setting allows each learner to have training examples for N class(es). Parameter divergence is a significant issue in federated learning, leading to diminishing returns from decentralized non-IID data. It is often referred to as gradient/loss divergence in the literature. Divergence metrics measure the difference between each learner's local model parameters and the population distribution, causing parameter diversity among local updates due to heterogeneous data distributions. Maintaining a balanced level of parameter divergence is crucial for effective exploitation of data. Parameter divergence in federated learning can lead to bad solutions when local datasets are far from the population distribution. The divergence is directly related to the distance of local datasets from the population distribution, especially in non-IID cases. Maintaining a balanced level of parameter divergence is crucial for effective data exploitation. In federated learning, parameter divergence can occur when local datasets are skewed and heterogeneous, leading to significant differences in gradients across learners. This divergence is especially pronounced in non-IID data settings, where weight values in the output layer are distributed based on each learner's data distribution. Two metrics are defined to capture parameter divergence, crucial for effective data exploitation. Parameter divergence in federated learning can be significant due to variations in data distribution and model parameters across learners. To address this, cosine distance is used to measure divergence, ensuring normalized comparisons. The proposed metrics offer insights into the effectiveness of iterative parameter averaging, crucial for optimizing data exploitation in non-IID settings. In federated learning, parameter divergence is significant due to data distribution and model variations. Cosine distance is used to measure divergence, ensuring normalized comparisons. Earth mover's distance is used to measure probabilistic distance in Non-IID settings, showing a positive correlation between parameter divergence and learning performance. The parameter divergence metrics can reveal the link between data non-IIDness and learning performance in federated learning. Various hyperparameter optimization methods may have negative effects under non-IID data settings, unlike in deep learning. Further experimental results and discussion are provided in the next subsection and appendix. Effects of optimizers like Adam and momentum SGD are discussed in relation to non-IID data. Parameter averaging for all model variables is highlighted as important for performance. Additionally, the impact of network depth/width on performance is mentioned, emphasizing the importance of techniques like information highways and shortcut connections. Widening networks can improve outcomes in non-IID data environments, unlike global average pooling which may reduce performance. Batch Normalization's dependence on minibatch activations can be a drawback, mitigated by Batch Renormalization but not completely resolved. Regularization techniques like weight decay and data augmentation are also important in non-IID data scenarios. Regularization techniques such as weight decay and data augmentation can lead to high training loss in non-IID data, offsetting generalization gains. The causes of failures in non-IID data were classified into three categories based on experimental results. Parameter divergence is a significant issue in federated learning with non-IID data. In federated learning with non-IID data, some hyperparametric methods lead to greater parameter divergence as network depth increases. This results in varying final test accuracies, with deeper models having more parameters and higher parameter divergence. The study shows that deeper models exhibit larger parameter divergence, even at a layer level. Results using modern network architecture like ResNet are provided. In non-IID data settings, lower weight decay levels achieve higher test accuracy, suggesting smaller weight decay is needed for federated learning with non-IID data. In non-IID data settings, a high number of local iterations per round led to divergence values affecting accuracy gap. Even with weight decay factor of 0.0005, parameter divergence values were similar to smaller factors in early rounds. Dropout also increased parameter divergence under non-IID data setting. Test accuracy showed diminishing returns with Nesterov momentum SGD, achieving +2.85% under IID but only +1.69% under non-IID. In non-IID data settings, the Dropout effect on generalization is still valid in test accuracy for pure SGD and Adam. A phenomenon called the steep fall phenomenon was observed where parameter divergence values decrease rapidly over rounds, indicating poor local minima or saddles. The Adam optimizer's performance is sensitive to the range of model variables in non-IID data environments. The Adam-WB under the Non-IID(2) setting shows poor performance due to twice as many momentum variables compared to momentum SGD, leading to increased sensitivity to non-IIDness. The Adam-WB optimizer shows similar or smaller parameter divergence values compared to Adam-A, with a steep fall phenomenon observed in the last fully-connected layer. This phenomenon is also seen in relation to network width and the use of Batch Normalization. NetC-Widest uses different pooling techniques after the last convolutional layer, resulting in varying numbers of neurons in the output layer. Test accuracy under the Non-IID(2) setting increases with the degree of wideness. Batch Normalization 9 shows parameter divergence and low test accuracy. Batch Renormalization is suggested as a better alternative. Batch Renormalization is proposed as a better alternative to Batch Normalization due to parameter divergence and low test accuracy. The steep fall phenomenon in test accuracy is explained by sharper minima in loss landscapes, leading to poorer generalization. The failure cases are attributed to excessively high training loss of local updates. The failure cases are due to excessively high training loss of local updates, with NetB-Baseline showing higher loss compared to other models. NetB-Baseline uses global average pooling after the last convolutional layer, resulting in lower test accuracy. In contrast, NetB-Wider and NetB-Widest use max pooling with larger strides, leading to better performance. The local updates in non-IID data environments can easily overfit, leading to high training losses. Excessive training loss is observed under non-IID settings with weight decay and data augmentation. Weight decay levels above 0.0001 result in severe performance degradation. Data augmentation has diminishing returns on test accuracy with Nesterov momentum SGD and Adam, especially under non-IID conditions. In this study, the effects of various hyperparameter optimization strategies for optimizers, network depth/width, and regularization on federated learning of deep networks were explored. The performance degradation in data augmentation cases was attributed to high training loss. Data augmentation showed a generalization effect on test accuracy in experiments on SVHN. The primary focus was on non-IID data environments leading to overfitting in local updates. In this study, the focus was on federated learning of deep networks, particularly on non-IID data settings. The research highlighted the differences in behaviors under non-IID data compared to IID settings and vanilla training. The concept of parameter divergence was used to explain these differences, with empirical and theoretical support. The study addressed the lack of research on non-IID data in federated learning, emphasizing the importance of understanding essential factors in training under decentralized data environments. The study focuses on federated training under non-IID data environments, using CNN architectures with specific network configurations. It includes details on convolutional layers, max pooling, fully-connected layers, and activation functions like ReLU and softmax. In the experiments, network models are initialized following a truncated normal distribution with a mean of 0. Different optimization methods such as pure SGD, Nesterov momentum SGD, and Adam are used with varying initial learning rates. Learning rates are decreased by 0.1 at specific training iterations. Learners' local training datasets are predetermined in the environmental configurations. In the experiments, network models are initialized with different optimization methods. Learners' local training datasets are predetermined in environmental configurations. The study focuses on how non-IID data affects model parameter divergence in deep network training for multi-class classification. The text discusses the SGD update of learner k in a multi-class classification model, emphasizing the importance of model parameters and local iterations. It also mentions the impact of non-IID data on model parameter divergence in deep network training. In this section, the complete experimental results are provided, including the impact of optimizers like momentum SGD and Adam on deep network training. The experimental setup and notation are also explained. The experimental results show a significant performance gap between Adam-A and Adam-WB under non-IID data setting, unlike momentum SGD trials. The poor performance of Adam-WB was initially thought to be due to parameter divergence, but the values were similar or even smaller than Adam-A. Adam-WB, compared to Adam-A, shows a steep fall phenomenon in parameter divergence in the early rounds but quickly reduces. It leads to higher training loss and severe degradation in test accuracy. Deepening networks in non-IID data settings result in bigger parameter divergence and diminishing returns as expected in test accuracy. The test accuracy results under the vanilla training and IID data setting show as expected. Parameter divergence increases qualitatively with the number of convolutional layers under the non-IID data setting. The parameter averaging algorithm did not work properly with NetA-Deepest, leading to little difference in test accuracy values in the early rounds. The effect of network width is investigated next. The effect of network width on federated learning algorithm performance is explored. Widening networks shows positive effects under the non-IID data setting. Global average pooling performs better in vanilla training but worse in non-IID settings. Parameter divergence increases as network width decreases in the non-IID data setting. The parameter size varies for NetC-Baseline, NetC-Wider, and NetC-Widest. Global average pooling leads to high training loss. NetC-Baseline results in high loss values under IID data setting. Diminishing returns are observed for pure SGD and NMom-A cases. The degradation rate is higher under the non-IID data setting due to overfitting. Weight decay should be smaller in non-IID settings for federated learning. Excessive parameter divergence occurs with weight decay set to 0.0005 in non-IID settings. In experiments with non-IID data settings, a high number of local iterations per round (100) led to significant parameter divergence and accuracy gaps. Even with weight decay at 0.0005, test accuracy improved similarly at early rounds. Additional experiments with FedProx showed the use of a surrogate loss function to prevent excessive deviation from global model parameters. In experiments with non-IID data settings, a high number of local iterations per round (100) led to significant parameter divergence and accuracy gaps. FedProx, a surrogate loss function, was used to prevent excessive deviation from global model parameters. The implementation of Batch Normalization in the federated learning algorithm helped in obtaining proper moving variance at each round, especially crucial under non-IID data settings. In experiments with non-IID data settings, Batch Renormalization with specific hyperparameters showed improved performance compared to Batch Normalization. Batch Renormalization reduced parameter divergence and overfitting issues, leading to better outcomes despite some layers showing greater parameter divergence than Batch Normalization. However, Batch Renormalization still did not surpass the baseline performance. The implementation of data augmentation, including random horizontal flipping, brightness & contrast adjustment, and 24\u00d724 cropping & resizing, did not improve performance compared to the baseline due to significant parameter divergence. Data augmentation yielded diminishing returns for PMom-A, NMom-A, and Adam-A cases on CIFAR-10 under the non-IID data setting, with even worse outcomes for Adam-A. The parameter divergence remained similar with or without data augmentation, and the diminishing outcomes were rooted in high training losses from local updates. In the pure SGD case, high training loss values were observed under the IID data setting with data augmentation, leading to lower test accuracy compared to the baseline. Data augmentation still had a generalization effect on test accuracy in experiments on SVHN. Dropout was used with rates 0.2 and 0.5 for convolutional and fully-connected layers, respectively, showing greater parameter divergence under non-IID data settings. This resulted in diminishing returns for PMom-A and NMom-A cases on CIFAR-10. The effect of Dropout remains positive in federated learning, even with unbalanced data settings. Experimental results show that findings from previous sections are still valid under unbalanced data settings, with performance being worse in Non-IID(2) setting. The negative impact of data unbalancedness is not as great as that of non-IIDness in federated learning, but it becomes much bigger when the two are combined."
}