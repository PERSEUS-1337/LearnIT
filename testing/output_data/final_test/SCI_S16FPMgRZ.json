{
    "title": "S16FPMgRZ",
    "content": "Convolutional neural networks consist of convolutional layers followed by fully-connected layers. Convolutional layers map high-order activation tensors, while fully-connected layers operate on flattened activation vectors. However, flattening removes the multi-dimensional structure of activations and requires a large number of parameters. The new techniques introduced include tensor contraction layers to replace fully-connected layers in neural networks and tensor regression layers to express the output as a low-rank multi-linear mapping. These methods significantly reduce the number of parameters while maintaining accuracy, particularly beneficial for datasets with multi-modal structures like audio spectrograms and images represented as tensors. Tensor methods extend linear algebra to higher order tensors, offering promising tools for manipulating and analyzing multi-modal data. Tensor decompositions are broadly useful techniques within tensor methods, gaining prominence in machine learning for datasets represented as tensors. Tensor decompositions are important techniques in tensor methods for learning latent variables. Deep Neural Networks often work with high-order tensors, such as in convolutional Neural Networks for image recognition. However, flattening tensors before connecting to output neurons can lead to loss of information and require a large number of parameters. To address this, Tensor Contraction Layers (TCLs) and Tensor Regression Layers (TRLs) are proposed as trainable components in neural networks, leveraging multilinear structure while maintaining the power of deep learning. By replacing fully-connected layers with tensor contractions, the proposed TRL in neural networks preserves multi-modal structure and aggregates long-range spatial information efficiently. Enforcing low rank reduces parameters needed without sacrificing accuracy. The TRL represents regression weights through a low-rank tensor decomposition, eliminating the need for flattening in output generation. Combining tensor regression with tensor contraction enhances efficiency, leading to improved performance on the ImageNet dataset while reducing parameters by almost 65%. This architecture retains the multi-dimensional tensor structure throughout the network, a novel approach in deep learning. Several recent papers have applied tensor decomposition to deep learning, with various approaches such as CP decomposition for speeding up convolutional layers and Tucker decomposition for fine-tuning pre-trained networks. Other methods like weight sharing in multi-task learning and Tensor-Train format for imposing low-rank tensor structure on weights have also been proposed. Despite the success of deep neural networks, questions remain about their efficiency and parameter usage, leading to the emergence of tensor methods for analysis and optimization. Tensor methods are being used to analyze deep neural networks, with papers exploring their expressive power, global optimality, and optimization of non-convex factorization problems. These methods are also being investigated for devising neural network learning algorithms with theoretical guarantees of convergence. Prior research has shown the effectiveness of tensor regression in preserving natural multi-modal structure and learning compact predictive models, although they often require analytical solutions and manipulation of large datasets. No prior work combines tensor contraction or tensor regression with deep learning in an end-to-end trainable fashion. Tensors are defined as multidimensional arrays, with vectors denoted as v and matrices denoted as M. Tensor unfolding is the process of converting a tensor into a matrix. Tensors are multidimensional arrays, with unfolding converting them into matrices. Tensor vectorization flattens a tensor into a vector. The n-mode product of a tensor with a matrix is defined. Generalized inner product and Tucker decomposition for tensors are also discussed. Tensor decompositions can be achieved by projecting along each mode with projection factors to obtain a low-rank core. The factors and core are typically obtained by solving a least squares problem. Tensor contractions and regressions can be incorporated into neural networks as differentiable layers by applying tensor operations to activation tensors. The Tensor Contraction layer (TCL) in neural networks applies tensor contraction to activation tensors to obtain a low-dimensional representation. TCLs require fewer parameters and less computation compared to fully-connected layers. The TCL produces a compact core tensor from an activation tensor, with projections learned end-to-end with the network. In standard CNNs, the input is flattened and passed to a fully-connected layer, while in TCL, gradients are taken with respect to the factors V(k) for each k. The Tensor Contraction layer (TCL) in neural networks applies tensor contraction to activation tensors to obtain a low-dimensional representation. By leveraging the spatial structure in the activation tensor, the output is formulated as lying in a low-rank subspace through a low-rank tensor regression. This approach enforces a low multilinear rank of the regression weight tensor, providing a more efficient alternative to standard CNNs. In this work, tensor regression is incorporated as trainable layers in neural networks to estimate the regression weight tensor directly from high-order input data, avoiding the need for hand-crafted feature extraction. Low rank constraints are enforced on the weights to improve efficiency for large datasets. The Tensor Regression Layer (TRL) is applied to high-order input data, enforcing low rank constraints on regression weights. By leveraging the multi-modal structure in the data, the solution is expressed on a low rank manifold. Dimensionality reduction is done before tensor regression, replacing fully connected layers with TRL. For binary classification, the output is a product of activation tensor and low-rank weight tensor. For multi-class, regression weights become a 4th order tensor. Gradients can be obtained by unfolding the tensors. The effectiveness of preserving the tensor structure through tensor contraction and regression is empirically demonstrated in state-of-the-art architectures, showing similar performance on the ImageNet dataset. The effectiveness of low-rank tensor regression is demonstrated on VGG-19 BID22 and ResNet-50/101 BID6. Synthetic data is used to show its superiority over fully-connected regression in capturing weight structure and robustness to noise. The data onX +\u1ebc, where\u1ebc is added Gaussian noise sampled from N (0, 3), is compared between a TRL with squared loss and a fully-connected layer with squared loss. The TRL is easier to train on small datasets and less prone to over-fitting due to its low rank structure of regression weights. Experiments were conducted on the ImageNet-1K dataset using various network architectures, reporting results in terms of accuracy across all 1000 classes. When experimenting with the tensor regression layer, the network was not retrained each time but started from a pre-trained ResNet. Two settings were explored: replacing the last average pooling, flattening, and fully-connected layer with a TRL or a combination of TCL + TRL, and investigating replacing the pooling and fully-connected layers with a TRL that jointly learns spatial pooling as part of the tensor regression. All models were implemented using the MXNet library BID1 and trained with data parallelism across multiple devices. When experimenting with the tensor regression layer, the network was not retrained each time but started from a pre-trained ResNet. Models were implemented using the MXNet library BID1 and trained with data parallelism across multiple devices, utilizing 4 NVIDIA k80 GPUs on Amazon Web Services. Batch normalization layers were added before and after the TCL/TRL to prevent vanishing or exploding gradients. We also applied weight normalization to the factors of the Tucker decomposition. The effectiveness of the TCL was investigated using a VGG-19 network architecture with 138,357,544 parameters. By adding Tensor Contraction Layer (TCL) to reduce the size of activation tensor before fully-connected layers, significant space saving can be achieved in models. The space saving of a model with n total parameters in fully-connected layers compared to a reference model can be expressed as 1 \u2212 nM/nR. Different combinations of TCL show varying impacts on performance and space saving, with a TCL that preserves input size slightly improving performance with minimal space loss, while reducing TCL size results in over 65% space saving with little performance impact. Testing TCL with ResNet-50 and ResNet-101 architectures on ImageNet showed promising results. By testing ResNet-50 and ResNet-101 architectures on ImageNet, the study removed the average pooling layer to maintain spatial information in the tensor. The full activation tensor is directly passed to a Tensor Regression Layer (TRL) for final predictions after applying softmax. To reduce computational burden while preserving multi-dimensional information, a Tensor Contraction Layer (TCL) can be inserted before the TRL. Results on ImageNet for various network configurations are presented in TAB1, showing the size of the TCL and the rank of the TRL. Additionally, joint spatial pooling and low-rank regression can be achieved by learning spatial pooling as part of the tensor regression. The study explores tensor regression by removing the average pooling layer and feeding the tensor to the Tensor Regression Layer (TRL) with a rank of 1 on spatial dimensions. Weight initialization for the TRL involves a partial tucker decomposition of a pre-trained model's fully connected layer. The Tucker decomposition was used to initialize the Tensor Regression Layer (TRL) with reduced parameters, maintaining performance. TRL eliminates the need to flatten input tensors and learns a low-rank manifold for compact networks with similar accuracies. Future plans include applying TRL to more networks. Future plans include applying the Tucker decomposition and Tensor Regression Layer (TRL) to more network architectures, leveraging recent work on extending BLAS primitives to optimize tensor contractions."
}