{
    "title": "HJMCcjAcYX",
    "content": "The Permutation-Optimisation module proposed aims to learn how to permute sets end-to-end, enabling the creation of permutation-invariant set representations. This approach avoids bottlenecks in traditional set models and achieves state-of-the-art results on tasks such as number sorting, image mosaics, classification from image mosaics, and visual question answering. The Permutation-Optimisation module enables learning to permute sets end-to-end, creating permutation-invariant set representations. This approach overcomes bottlenecks in traditional set models and has shown superior performance in tasks like number sorting, image mosaics, classification from image mosaics, and visual question answering. The Permutation-Optimisation module allows for end-to-end learning to permute sets, creating permutation-invariant set representations. This approach improves traditional set models and has demonstrated better performance in tasks such as number sorting, image mosaics, classification from image mosaics, and visual question answering. The Permutation-Optimisation module optimizes permutations in neural networks using pairwise comparisons, allowing for flexible, permutation-invariant set representations. It outperforms existing methods in tasks like number sorting and image mosaics, demonstrating its effectiveness in representing permutations. The Permutation-Optimisation module improves performance in tasks like number sorting and image mosaics by optimizing permutations in neural networks through pairwise comparisons. It allows for flexible, permutation-invariant set representations and outperforms existing methods. The Permutation-Optimisation module enhances performance in tasks such as number sorting and image mosaics by optimizing permutations in neural networks through pairwise comparisons. It enables flexible, permutation-invariant set representations and surpasses current methods. The algorithm aims to learn a permutation matrix to order feature vectors correctly without relying on their arbitrary order. The Permutation-Optimisation module improves performance by optimizing permutations in neural networks through pairwise comparisons. It aims to learn a total cost function to generate a desired permutation, ensuring the ordering cost is learnable through differentiable optimization processes. Gradient descent is used to minimize the total cost for a fixed number of steps, allowing the module to improve the quality of permutations based on current ordering costs. The Permutation-Optimisation module uses gradient descent to optimize permutations in neural networks by learning a total cost function for generating desired permutations. This ensures that the model respects set symmetries while producing sequence outputs. The total cost function measures the quality of a permutation and is optimized to improve performance. The total cost function for generating permutations in neural networks is optimized using gradient descent. It computes pairwise ordering costs to create a cost matrix C, ensuring that C ij = \u2212C ji. The total cost function is defined as the sum of ordering costs over all pairs of elements i and j. The total cost function for generating permutations in neural networks is optimized using gradient descent. It computes pairwise ordering costs to create a cost matrix C, ensuring that C ij = \u2212C ji. The total cost function is defined as the sum of ordering costs over all pairs of elements i and j, where the cost is based on the permutation mapping i before or after j in the output sequence. Optimizing the total cost of a permutation involves enforcing constraints on a valid permutation matrix. To make optimization feasible, a common relaxation is to allow soft assignments of elements to positions in the set of doubly stochastic matrices. This change simplifies the optimization process by treating permutations as soft assignments, similar to fitting a mixture of Gaussians model using Expectation-Maximization. The optimization of permutation cost involves reparametrizing the matrix P using the Sinkhorn operator to ensure constraints are met. This method leads to better solutions than projected gradient descent. The Sinkhorn operator normalizes rows and columns of the matrix to converge to a doubly-stochastic matrix, ensuring P is always approximately doubly-stochastic. By reparametrizing the matrix P using the Sinkhorn operator, the optimization of permutation cost ensures constraints are met. This simplifies the optimization problem to minimize P without constraints, making it easier to optimize with standard gradient descent. The gradient can be computed with automatic differentiation, and defining B jq allows for efficient matrix multiplication. The optimization of permutation cost is simplified by reparametrizing the matrix P using the Sinkhorn operator. The algorithm initializes P in a permutation-invariant way and performs gradient descent to converge to the optimized permutation P(T). The gradient descent is done on the normalized permutation P, with a specific update rule. The algorithm for optimizing permutation cost involves setting specific values in equation 5, resulting in better permutations. The runtime is dominated by computing gradients of c(P), with a time complexity of \u0398(N^3). Small values for T, such as 4, are sufficient for good permutations. The ordering cost Cij determines the pairwise cost for placing i before j, with the key property being that the function F producing the entries of C is anti-symmetric. The function F producing the entries of C is anti-symmetric, achieved by defining F as a small neural network. C is normalized to have a unit Frobenius norm, decoupling the scale of F from the step size parameter \u03b7 for stable optimization. In some tasks, the set can be permuted into a lattice structure instead of a sequence, adapting the model accordingly. The total cost of an assignment to a grid is the sum of costs over all rows and columns. The model considers row-wise and column-wise relations when permuting inputs into a grid or lattice structure. Inspired by Mena et al. (2018), the model uses a reparametrisation to learn permutations implicitly. Our model differs from previous work by considering pairwise relations when creating permutations, allowing for local relations in the output to be taken into account. This approach is important for learning from permutations implicitly and enables processing variable-sized sets. The model discussed in the current chunk focuses on invariance to the ordering of sets and utilizes an RNN to encode a richer class of functions, including learning complex functions based on the order between elements. This approach contrasts with relation networks and concurrent work that approximates averaging the output of a neural network over all possible permutations. The model discussed in the current chunk focuses on invariance to the ordering of sets and utilizes an RNN to encode a richer class of functions. It falls under the categorization of a learned canonical input ordering and is relevant to neural networks operating on graphs. The model combines with an RNN as an alternative set function for state updates in graph-convolutional networks. Permutation learning for representation learning in a self-supervised setting is possible, as shown by Noroozi & Favaro and BID7. The model in BID7 is similar to Mena et al., but focuses on using the permuted set itself for learning good representations. The current chunk discusses the potential of optimization for processing sets in a supervised setting. It mentions the challenges of finding optimal solutions for non-convex quadratic programs and resorts to gradient descent for a reasonable solution. This work differs from learning to rank approaches. The current chunk discusses the limitations of existing approaches in learning permutations, emphasizing the need for soft assignments for differentiability. It suggests exploring variational approximation for obtaining a differentiable permutation with hard assignments in the future. The text discusses the challenges in using existing methods for solving minimum feedback arc set problems in set representation learning. Different models are introduced, and a qualitative analysis is performed. Experimental details and implementation information are provided for reproducibility. The text also mentions additional results and starts with a toy task of sorting random numbers. Our PO-U model outperforms existing end-to-end learning-based approaches in sorting sets of numbers, showing perfect generalization across different evaluation intervals without mistakes. This contrasts with previous models that start making errors at specific number ranges. The PO-U model shows significant improvement in learning permutations, especially for sorting images into grids. The model performs well on tasks involving images from MNIST, CIFAR10, or ImageNet resized to 64x64 pixels, using a unique cost function to arrange tiles. The mean squared error is minimized to achieve the correct permutation, with evaluation done using the Hungarian algorithm. Our own implementation of the model closely reproduces MNIST results, with PO-LA performing best in most cases. PO-U follows on CIFAR10 and ImageNet datasets, then LinAssign. MNIST shows LinAssign outperforming PO-U on higher tile counts due to object centering. The LinAssign model performs better on higher tile counts due to object centering, especially on MNIST. However, the PO-U model is more suitable for complex images like those in ImageNet. The issue with background tiles is more pronounced on ImageNet compared to CIFAR10. The PO-U model is more suitable for complex images where relative positioning matters, while PO-LA combines the best of both methods. In tasks where the goal is not producing the permutation itself, a suitable permutation is learned implicitly while solving another task, such as the image mosaic task modified for classification using ResNet-18. The network learns a permutation of image tiles for accurate classification. The CNN is pre-trained on the original dataset without permuting image tiles to prevent it from ignoring artefacts on tile boundaries. Once fully trained, only the permutation mechanism is trained. Results show that the PO-LA model usually performs best, especially for complex images like ImageNet. In complex images like ImageNet, the benefits of linear assignment decrease, and the optimization process becomes more important. PO-LA outperforms LinAssign on MNIST with a higher number of tiles. For visual question answering, the VQA v2 dataset is used with bottom-up attention features for object representation. Our model incorporates a modification to the state-of-the-art BAN model for object proposal processing. It learns to permute the set of object proposals into a sequence using an LSTM, feeding the representation back into the baseline model. Results on the VQA v2 validation set are presented. Our model significantly improves the overall performance of the state-of-the-art model on the VQA v2 validation set by 0.37%, with 0.27% of the improvement attributed to the learned permutation. This demonstrates the benefit of learning an appropriate permutation for better set representations. The model also outperforms in the number category, showing that the learned representation through permutation is non-trivial. The improvement is not solely due to increased model size and computation, as increasing the BAN model size did not yield further gains. In this paper, the Permutation-Optimisation module is discussed for learning permutations of sets using an optimisation-based approach. The approach is validated through experiments, showing the merit for learning permutations and set representations. The optimisation-based approach for processing sets is highlighted as underappreciated, with potential for inspiring new algorithms. However, the cubic time complexity in set size compared to quadratic complexity of Mena et al. limits the model to tasks with relatively small elements. Our model is limited to tasks with small element sizes, such as VQA with up to 100 object proposals per image. However, it does not scale well to larger set sizes like point cloud classification. Improvements in the optimization algorithm may help, possibly through a divide-and-conquer approach. Sets are seen as a step forward from tensors for modeling unordered collections, allowing for greater abstraction with permutation invariance. The algorithm optimizes neural networks by computing ordering costs, normalizing assignments, and using gradient descent steps. The Sinkhorn operator is applied to permute rows of the input matrix X to obtain the output Y. The algorithm optimizes neural networks using the Sinkhorn operator with a modified formulation that includes a temperature parameter. The comparison function learned for the number sorting task outputs negative numbers for lower inputs and positive numbers for higher inputs. The algorithm optimizes neural networks using the Sinkhorn operator with a modified formulation that includes a temperature parameter. The learned function for the number sorting task depends on the second argument and is a scaled and shifted version of it. The output of F in the optimisation process is easier to interpret than f. Investigating the behaviour of F on the image mosaic task reveals interesting patterns. The algorithm optimizes neural networks using the Sinkhorn operator with a modified formulation that includes a temperature parameter. It successfully learned the order of tiles in the number sorting task, with clear entries showing high confidence when tiles fit together correctly. The function F also shows patterns in per-row and per-column comparisons, indicating which tiles should be placed together. The algorithm optimizes neural networks using the Sinkhorn operator with a modified formulation that includes a temperature parameter. It successfully learned the order of tiles in the number sorting task, with clear entries showing high confidence when tiles fit together correctly. F 1 and F 2 show patterns in per-row and per-column comparisons, indicating which tiles should be placed together based on their positions. The algorithm optimizes neural networks using the Sinkhorn operator with a modified formulation that includes a temperature parameter. It successfully learned the order of tiles in the number sorting task, with clear entries showing high confidence when tiles fit together correctly. F 1 and F 2 show patterns in per-row and per-column comparisons, indicating which tiles should be placed together based on their positions. Within the tiles, F 1 and F 2 are most sensitive to, illustrating important areas for comparisons. Gradients of F with respect to input tiles show different spatial patterns for MNIST and CIFAR10 datasets. MNIST focuses more on edges and corners with increasing tile numbers, while CIFAR10 shows distinct left-right and top-bottom comparisons. The algorithm learned the order of tiles in number sorting tasks using F 1 and F 2, which focus on different areas within the tiles for comparisons. Gradients of F show distinct spatial patterns for MNIST and CIFAR10 datasets, with MNIST focusing more on edges and corners, while CIFAR10 emphasizes left-right and top-bottom comparisons. The algorithm learned the order of tiles in number sorting tasks using F 1 and F 2, which focus on different areas within the tiles for comparisons. Gradients of F show distinct spatial patterns for MNIST and CIFAR10 datasets, with MNIST focusing more on edges and corners, while CIFAR10 emphasizes left-right and top-bottom comparisons. The gradients of F with respect to input tiles for specific pairs of tiles provide insight into the changes that would affect the comparison cost the most. Brightening pixels in the gradient maps orders the corresponding tile more strongly towards the left for F 1 and towards the top for F 2, while darkening pixels with blue entries orders the tile more strongly towards the right for F 1 and the bottom for F 2. Saturated colors in the gradient maps correspond to greater effects on the cost when changing those pixels. The gradients of the second tile show that to encourage the permutation to place it to the right of the first tile, it is best to increase the brightness of the curve in tile 2 that is already white (red entries in tile 2 gradient map) and decrease the black pixels around it (blue entries). This means that it recognized that this type of curve is important in determining that it should be placed to the right, perhaps because it matches up with the start of the curve from tile 1. The gradient map of tile 2 suggests increasing blue entries to form a curve that complements tile 1, creating a loop. The gradient of tile 1 changes significantly when paired with different tiles, indicating specific comparisons are learned rather than a general trend. The linear assignment model is limited to modeling pairwise interactions. Observations show that matching edge colors between tiles is crucial for ordering them correctly. For example, increasing blue entries in tile 2 complements tile 1, creating a loop. The importance of matching edge colors between tiles for correct ordering is highlighted. Changes in brightness and color are necessary for tiles to complement each other. The gradient analysis helps determine how tiles should be arranged to minimize entropy in permutation matrices. The gradient analysis in permutation matrices focuses on the vanishing gradient problem, which can be avoided by using an alternative update method. This helps reach a good solution efficiently by preventing the gradient from vanishing towards the optimum permutation matrix. To efficiently reach a good solution in our algorithm, we ignore the gradient step size and move in a similar direction to saturate the Sinkhorn normalization, obtaining a doubly stochastic matrix closer to a proper permutation matrix. The total cost function can be written as a quadratic program with linear constraints, defined by matrices O and Q. The cost function involves a bijection between indices and a flattened version of P, with Q being indefinite due to potential negative costs. The cost function for the algorithm involves a bijection between indices and a flattened version of P, with better permutations having negative cost. The problem is non-convex and possibly NP-hard, with a quadratic number of optimization variables. Experimental results can be reproduced using the provided implementation in PyTorch. The inner gradient descent initializes weights with Xavier initialization and uses a small MLP for the ordering cost function. Inputs are concatenated pairs of elements, linearly projected to hidden units with ReLU activation, and then projected to 1 or 2 dimensions for sorting numbers and assembling image mosaics. The ordering cost matrix is created using the outputs of the MLP. Evaluation involves switching to double precision floating point numbers. When evaluating the model, we switch to double precision floating point numbers to ensure unique floats in the interval [1000, 1001]. Single precision floats are sufficient for other intervals and smaller set sizes. Inputs from MNIST, CIFAR10, and ImageNet datasets are normalized to zero mean and standard deviation one. For ImageNet, rectangular images are cropped to be square by reducing the longer side to match the shorter side. Images not divisible by the number of tiles are resized to the nearest bigger size. When processing images, tiles are rescaled to the nearest bigger size divisible by the image size. Each tile undergoes convolution, max pooling, and activation before being flattened into a feature vector. Unlike previous studies, upscaling MNIST images did not significantly improve performance. Good hyperparameters improved results for various models, including PO-U and PO-LA. This task is also referred to as a jigsaw puzzle. The task, known as image mosaics, involves using square tiles that allow for multiple solutions, unlike traditional jigsaw puzzles. A ResNet-18 model is utilized, with the first convolutional layer modified for MNIST and CIFAR10 datasets. The model is trained on the original dataset first, then the permutation method is applied to prevent the model from focusing on individual tiles. This process is essential to avoid artefacts from permuted tiles. The permutation method is used to prevent artefacts from permuted tiles in image mosaics. The LinAssign model resulted in NaN values, which were avoided by clipping outputs into a specific interval. The PO-U model did not have this issue. The BAN model, used as a baseline, generates attention weights between object proposals and question words. The attention weight for each object proposal is the maximum over all words in the question. Each object proposal includes attention logit, bounding box coordinates, and a feature vector projected to 8 dimensions. The batch size was reduced from 256 to 112 due to GPU memory constraints. The object proposals in the BAN model are permuted with T = 3 and a 2-layer MLP to produce ordering costs. The sequence is weighted based on relevance and fed into an LSTM. The LSTM's last cell state is the set representation, which is added back into the BAN model. 8 attention glimpses are processed with a PO-U module and shared LSTM parameters. During training, the step size \u03b7 in the PO-U module decreases initially, reducing the influence of the permutation mechanism. As training progresses, \u03b7 starts to rise above 1, indicating improved accuracy of the ordering cost. This allows the PO module to have a greater impact on the permutation. Results in TAB4 show accuracy with explicit supervision, while FIG0 displays example reconstructions learned by the PO-U model. The PO-U model reconstructs images with varying quality, showing reconstructions on 3x3 and 2x2 datasets. Mean squared error reconstruction loss is presented in Table 5, indicating similar trends as before. Example reconstructions learned by the PO-U model are shown on different dataset versions. The reconstructions are noisier due to supervision. The reconstructions in the current chunk show noise due to implicit supervision, indicating room for improvement despite superior performance in reconstruction error and classification accuracy compared to existing methods. The CNN can recognize implicitly learned permutations even if they do not exactly match the original image. Our models naturally produce reconstructions closer to the original image than the LinAssign model, with examples that have not been cherry-picked."
}