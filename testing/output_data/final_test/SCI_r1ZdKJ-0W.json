{
    "title": "r1ZdKJ-0W",
    "content": "Graph2Gauss is a method for learning node embeddings on large-scale graphs, outperforming in tasks like link prediction and node classification. It represents nodes as Gaussian distributions to capture uncertainty and can handle various types of graphs without additional training. The approach leverages network structure and node attributes for generalization to unseen nodes. Our approach in learning node embeddings utilizes a personalized ranking formulation based on node distances within the network structure. Experiments show superior performance compared to other methods on various tasks. Modeling uncertainty helps estimate neighborhood diversity and latent dimensionality of graphs, which are natural representations of real-life data. Node embeddings offer a powerful way to analyze such data by leveraging proven learning techniques and simplifying complex node interactions. Incorporating complex node interactions in tasks like link prediction, node classification, community detection, and visualization benefits from latent node representations. Leveraging network structure and attributes in attributed graphs provides more useful representations. Existing graph embedding approaches represent nodes as single points in a low-dimensional vector space, lacking information about uncertainty in representations. Representing nodes as points limits understanding of conflicting information sources within a node in a complex graph. The novel embedding approach represents nodes as Gaussian distributions to capture uncertainty in their representations. An unsupervised personalized ranking formulation is proposed to learn the embeddings, considering the complex interactions between nodes. The network structure imposes an ordering between nodes in the embedding space based on their distances, leading to the ranking. Graph2Gauss is an inductive method that leverages node attributes to generate embeddings for unseen nodes. It embeds nodes as Gaussian distributions to capture uncertainty and incorporates network structure for more powerful embeddings. The approach is able to rank nodes based on their distances in the embedding space, providing a significant benefit over transductive methods. The focus of this paper is on unsupervised learning of node embeddings for different types of graphs. Various approaches like DeepWalk, node2vec, LINE, SDNE, and GraRep have been proposed to learn embeddings based on network structures and proximity. Various methods for learning node embeddings in graphs include GraRep BID1, Tri-Party Deep Network Representation (TRIDNR) BID24, CENE BID28, Text-Associated DeepWalk (TADW) BID33, GraphSAGE, and graph convolutional networks BID16 BID3 BID13 BID22 BID23 BID26. These methods consider local and global structural information, node attributes, network structure, text features, and graph Laplacian. Graph data BID16 BID3 BID13 BID22 BID23 BID26 utilizes graph Laplacian and spectral convolution for aggregation over neighbors, implicitly learning embeddings. Most methods are (semi-)supervised, except for GAE BID17 which learns node embeddings unsupervised. BID31 learns Gaussian word embeddings, while BID12 and BID4 focus on knowledge and heterogeneous graphs respectively, not applicable for unsupervised learning of graphs. Graph2Gauss (G2G) is a method introduced in this section that considers both node attributes and network structure in learning node representations. Unlike previous methods like BID12 and BID4, which do not consider node attributes and are not suitable for unsupervised learning, G2G learns embeddings for each component of triplets in the knowledge graph. Graph2Gauss (G2G) introduces a method for learning node representations that considers both node attributes and network structure. The embedding process involves passing node attributes through a deep neural network and formulating an unsupervised loss function based on the natural ranking of nodes. The goal is to find a lower-dimensional Gaussian distribution embedding where nodes similar in attributes and network structure are also similar in the embedding space. In Graph2Gauss, node representations are learned considering attributes and network structure. The method involves passing attributes through a neural network and creating a Gaussian distribution embedding where nodes with similar attributes and structure are close in the space. The personalized ranking approach captures structural information by ranking nodes based on their distance to a given node in the embedding space using k-hop neighborhoods. Graph2Gauss learns node representations by incorporating attributes and network structure. It uses a personalized ranking approach based on k-hop neighborhoods to ensure nodes in closer proximity to a given node in the embedding space. The method employs the asymmetric KL divergence as a dissimilarity measure for latent representations, allowing for capturing network structure at multiple scales. Graph2Gauss learns node representations by incorporating attributes and network structure using deep neural networks parametrized by \u03b8. The method employs asymmetric KL divergence for latent representations, capturing network structure at multiple scales. The parameters are shared across instances for statistical strength benefits. Graph2Gauss utilizes a deep encoder to process node attributes and generate hidden representations, enabling inductive learning. Due to the complexity of pairwise constraints, an energy-based learning approach is adopted, penalizing ranking errors based on KL divergence. The objective function aims to optimize the loss by considering valid triplets and positive examples. The deep encoder in Graph2Gauss processes node attributes to generate hidden representations for inductive learning. An energy-based learning approach is used to penalize ranking errors based on KL divergence, optimizing the loss by considering valid triplets and positive examples. The square-exponential loss function is employed to push the energy of negative examples to infinity with exponentially decreasing force, ensuring pairwise rankings are satisfied. Parameters are shared across all instances for easier learning compared to treating distribution parameters independently. The distribution parameters (e.g. \u00b5 i , \u03a3 i ) are optimized independently using Adam BID15 with a fixed learning rate of 0.001. To address the intractability of computing the complete loss for large graphs, a stochastic sampling strategy is proposed. Instead of naively sampling triplets uniformly from D t, a node-anchored sampling strategy is suggested to ensure that low-degree nodes are updated more frequently. Theorem 1 presents a new sampling strategy to obtain unbiased gradient estimates for stochastic optimization. By selecting anchor nodes and subsampling mini-batches, the reformulated loss is equal in expectation to the original loss. The effect of this sampling strategy on convergence and the quality of the stochastic variant is analyzed in experimental studies. The sampling strategy in Theorem 1 ensures unbiased gradient estimates for stochastic optimization. The final layer outputs \u03c3 id \u2208 R to ensure positive definiteness. Inductive learning allows for obtaining node representations based solely on attributes post-learning, enabling handling of new nodes easily. Unlike SDNE and GraphSAGE, our method can handle nodes without existing connections. Our method can handle nodes without existing connections by relying only on attribute information. It can also analyze plain graphs using one-hot encoding when attributes are not available, outperforming some attributed approaches. The encoder architecture can utilize CNNs/RNNs depending on the type of node attributes. In practice, a simple feed-forward architecture with rectifier units is sufficient for processing node attributes. Graph2Gauss is not sensitive to hyperparameter choices. The time complexity for computing the loss is O(N^3), but with node-anchored sampling, it becomes O(K^2N), which is linear in the number of nodes. Graph2Gauss has a time complexity of O(N) and requires a small number of epochs for convergence. It outperforms competitors in terms of wall-clock time. Competitors include TRIDNR, TADW, GAE, and node2vec. TRIDNR is trained unsupervised but may not always be applicable due to its attribute processing limitations. Graph2Gauss outperforms competitors like TRIDNR, TADW, and GAE in terms of wall-clock time. It requires symmetrizing the graph before use, giving it an advantage in link prediction tasks. The dataset description includes CORA, CORA-ML, CITESEER, and DBLP datasets with varying sizes and attributes. Link prediction is a common task to evaluate embeddings' performance. Candidate edges are ranked using the area under the ROC curve (AUC) and average precision (AP) scores. Validation/test sets are created with randomly selected edges/non-edges for hyper-parameter tuning and reporting performance. Additional datasets like CITESEER, DBLP, and PUBMBED are also commonly used for citation analysis. Source code for G2G and supplementary material can be found at https://www.kdd.in.tum.de/g2g. Our method outperforms competitors in link prediction task on various datasets, even the constrained version without considering attributes. GAE achieves comparable performance on some datasets but doesn't scale well to large graphs. Logistic Regression baseline also shows strong performance. The Logistic Regression baseline performed strongly, even surpassing more complex methods, on the GPU memory-limited dataset. Graph2Gauss excelled in both original and simplified datasets, while node2vec struggled with the former but improved in the latter. Sensitivity analysis showed G2G's ability to generate effective embeddings even with small sizes. Graph2Gauss outperforms competitors in embedding sizes, especially for small number of training edges. Performance is evaluated with training edges varying from 15% to 85%, showing strong results. Node classification is used to evaluate learned embeddings' strength. The strength of learned embeddings is evaluated through node classification on three datasets (Cora-ML, Citeseer, DBLP) with ground-truth classes. Embeddings are trained unsupervised, then used with labels for logistic regression. Results show our method outperforms competitors in classification performance. Our method outperforms competitors in node classification performance, even the constrained version without attributes. It shows stable performance regardless of the percentage of labeled nodes, making it efficient for training on a small percentage. Node-anchored sampling strategy converges faster than naive random sampling in link prediction tasks. Our node-anchored sampling strategy achieves lower loss and variance of gradient updates, leading to faster convergence. Learning an embedding as a distribution captures uncertainty, with experiments showing correlation between uncertainty and neighborhood diversity. The study demonstrates that nodes with less diverse neighborhoods have lower variance in embeddings compared to nodes with more diverse neighbors. The unsupervised learning approach effectively captures uncertainty in embedding, reflecting the diversity of neighboring nodes' class labels. The learned uncertainty helps detect the latent dimensionality of the graph. The study shows that by monitoring the variance change over time, relevant latent dimensions can be automatically detected. This detection is effective for multiple datasets, as demonstrated with Cora-ML. Removing uncertain dimensions has minimal impact on performance until true latent dimensions are removed, leading to noticeable degradation. The study demonstrates that detecting relevant latent dimensions by monitoring variance change over time is effective for various datasets, such as Cora-ML. Removing uncertain dimensions has minimal impact on performance until true latent dimensions are removed, leading to noticeable degradation. In a use case analysis, nodes with high uncertainty reveal interesting patterns, such as the surprising citation patterns of a highly uncertain paper in the Cora dataset. G2G can learn embeddings for nodes not part of the network during training, supporting both transductive and inductive learning. An experiment is conducted to evaluate the approach's generalization to unseen nodes by hiding a percentage of them. The experiment involves hiding a portion of nodes in the network, learning node embeddings for the remaining nodes, and evaluating link prediction performance for unseen nodes. Utilizing rich attribute information allows for strong performance even when a quarter of nodes are missing, making the method applicable for large graphs where training on the entire network is not feasible. Other methods like SDNE and GraphSAGE require edges for unseen nodes, while Graph2Gauss is the only inductive method that can obtain embeddings. Graph2Gauss is an inductive method that produces node embeddings based solely on node attributes. It creates 2D visualizations of networks for data exploration. By learning lower-dimensional embeddings and mapping them with TSNE, nodes are represented as 2D Gaussians, visualizing uncertainty. The method is effective even in low dimensions, as shown in the visualization of the Cora-ML dataset. Graph2Gauss is an unsupervised approach that represents nodes in attributed graphs as Gaussian distributions, capturing uncertainty and revealing latent dimensionality. It leverages attribute information for generalization to unseen nodes and achieves high link prediction performance. Future work includes studying personalized rankings beyond shortest path distance. Graph2Gauss is an unsupervised approach that represents nodes in attributed graphs as Gaussian distributions, capturing uncertainty and revealing latent dimensionality. It leverages attribute information for generalization to unseen nodes and achieves high link prediction performance. The method involves applying the expectation operator to obtain the original loss, showing that taking the gradient with respect to a set of samples gives an unbiased estimate of the gradient. Graph2Gauss is not sensitive to hyperparameters or embedding size. A single hidden layer of size 512 is recommended for the encoder. Node embeddings are obtained using node attributes and activation functions like relu and elu. Graph2Gauss utilizes activation functions like softplus, ELU, and ReLU for ensuring positive values. Xavier initialization is used for weight matrices, and multiple hidden layers or CNNs/RNNs can be employed. Unlike other methods, Graph2Gauss does not explicitly regularize means or clip covariance matrices due to the self-regularizing nature of KL divergence. Parameters are optimized using Adam with a fixed learning rate. Some methods like node2vec may struggle to produce node embeddings. Graph2Gauss does not require every node to be in the train set, unlike other methods like node2vec. To ensure every node appears at least once in the train set, an edge cover is needed, which is a set of edges where every node is incident to at least one edge. The minimum edge cover problem involves finding the smallest edge cover size."
}