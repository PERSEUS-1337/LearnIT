{
    "title": "Byxn9CNYPr",
    "content": "The rate of medical questions online exceeds the capacity to answer them, leading to many unanswered or inadequately answered questions. Identifying similar questions efficiently is crucial. Research has struggled with general question similarity, especially in the medical domain. A semi-supervised approach using neural networks pre-trained on medical question-answer pairs shows promise in determining medical question similarity, achieving an accuracy of 82.6%. The emergence of medical question-answering websites has led to a surge in online medical inquiries. Building a system to match unanswered questions with similar answered ones can optimize doctor time and reduce the number of unanswered questions. The rise of online medical inquiries has led to the need for accurate systems to provide medical advice. Patients seek online help due to cost, convenience, or embarrassment, making it crucial for the system to be reliable. However, finding similar medical questions accurately is challenging, and simple heuristics may not be effective. Patients who use online advice in addition to in-person care may face consequences if the information does not align with their doctors' advice. Machine learning is a potential solution for finding similar medical questions accurately, but it requires labeled training data. To address the lack of available data, a dataset of medical question pairs has been generated and released. Previous research in medical NLP has focused on applying general models, but these are not specifically trained on medical data. In this work, researchers enhance general language models with medical knowledge from question-answer pairs. Models pre-trained on this task outperform those trained on out-of-domain question similarity. The task of question-answer matching shows promise for generalizing to other domains. Performance gains are specific to this task and not seen in other in-domain tasks like question categorization and answer completion. The paper introduces a dataset of medical question pairs labeled by doctors and highlights the importance of domain-specific pre-training for medical NLP tasks. It compares the performance of models pre-trained on medical question-answer matching with those trained on out-of-domain tasks. The study utilizes BERT and XLNet models for experiments. BERT and XLNet are models trained on semi-supervised tasks like predicting masked words in a sentence and sentence sequence prediction. BERT was trained on a large corpus from BooksCorpus and Wikipedia, showing better generalization to Wikipedia datasets. Researchers have re-trained BERT on specific domains like scientific papers, medical notes, and biomedical articles. Retraining BERT on domain-specific data is crucial for better performance. Fine-tuning BERT on in-domain tasks can provide benefits similar to retraining on new domains. Previous studies have explored question similarity using various methods such as CNNs and cosine similarity metrics. Gupta (2019) focuses on relative 'best' match instead of absolute similarity. Abacha & Demner-Fushman (2016) emphasize the importance of medical question similarity for training models to determine if existing FAQs can answer new questions. Labeled training data is a challenge in the medical field, leading to the use of augmentation rules to generate similar question pairs automatically (Li et al., 2018). In the medical field, generating labeled training data for question similarity is challenging. One approach is weak supervision, but it is difficult due to the nuances of medical similarity. Transfer learning from related tasks is another option. Large datasets like Quora Question Pairs (QQP) and HealthTap provide labeled question pairs, although they cover various topics beyond medicine. The dataset contains labeled pairs of similar and dissimilar medical questions from HealthTap and WebMD. HealthTap is a medical question-answering website, while WebMD is an online publisher of medical information. The dataset is reduced to match the size of QQP for performance comparisons. Pre-training tasks involve restructuring the HealthTap and WebMD data. The pre-training tasks involve restructuring HealthTap and WebMD data for comparisons. Question Answer Pairs (QA) task focuses on determining semantic similarity between questions. Positive examples are true question-answer pairs, while negative examples pair questions with random answers from the same category. A classifier is trained to label pairs as positive or negative. Answer Completion (AA) task involves mimicking next-sentence prediction by splitting answers from HealthTap. The Answer Completion task involves splitting HealthTap answers into two parts and labeling them as positives or negatives based on whether they match. Question Categorization pairs HealthTap questions with main-category labels for classification. The paper introduces a dataset for medical question similarity, different from question entailment datasets. The questions are patient-asked, using less technical language and covering a wider range of topics. Doctors hand-generate 3,000 medical question pairs for a specific dataset. They rewrite original questions while maintaining intent and create related but dissimilar questions. This process ensures a dataset tailored to medical needs. Doctors hand-generate 3,000 medical question pairs for a specific dataset by rewriting original questions while maintaining intent. The first instruction creates positive question pairs, and the second creates negative question pairs to ensure the task is not trivial. Each doctor interprets the instructions slightly differently to reduce bias. An oracle score is obtained by having doctors hand-label question pairs generated by a different doctor. The accuracy of the second doctor in labeling question pairs intended by the first is 87.6% in a test set of 836 pairs. The goal is to determine if two medical questions have the same meaning by embedding medical knowledge into a generic language model using transfer learning from a bi-directional transformer network like BERT. This involves a double finetune process on an intermediate task and then on the final task of medical question pairs. The study involves finetuning on intermediate tasks such as quora question similarity, medical question answering, answer completion, and question classification before training BERT on a small medical question-pairs dataset. Each intermediate task is trained for 5 epochs with 364 thousand examples, and models are finetuned on labeled medical question pairs. The models are trained with specific parameters and on NVIDIA Tesla V100 GPUs, with experiments conducted on different train/validation splits for error analysis. The study involves finetuning BERT models on intermediate tasks and a small medical question-pairs dataset. Different BERT models are compared against previous state-of-the-art models in the medical field, and an ablation study is conducted starting with the XLNet model. Error analysis is performed to understand model performance qualitatively. In this study, 5 models were trained on different train/validation splits to investigate the impact of the training corpus domain on the medical question similarity task. By analyzing consistently wrong question pairs, hypotheses were formed and validated by adjusting inputs until correct labels were achieved. Finetuning BERT on intermediate tasks like Quora question pairs and HealthTap question answer pairs before the final task showed that the QA model performed well. The QA model outperforms the QQP model by 2.4% to 4.5% on the final task, with statistically significant results. Models trained on medical question-answering perform better than those trained on question-similarity tasks from an out-of-domain corpus. Performance reaches 84.5% when using the full HealthTap question-answer pairs corpus. Results are consistent across different models and datasets. The study compared QA models with QQP models using datasets from WebMD, HealthTap, and QQP. Results showed that the QA model consistently outperformed the QQP model, even surpassing the HealthTap model with the same data size. Different tasks were created using HealthTap data to test performance, showing that in-domain tasks improved model performance. The study compared QA models with QQP models using datasets from WebMD, HealthTap, and QQP. Results showed that the QA model consistently outperformed the QQP model, even surpassing the HealthTap model with the same data size. Different tasks were created using HealthTap data to test performance, showing that in-domain tasks improved model performance. In contrast, finetuned BERT models on medical tasks performed worse than the baseline BERT model, indicating that domain relevance is crucial but not all tasks are suited for encoding proper domain information. Comparisons were made with BioBERT, SciBERT, and ClinicalBERT models. The study compared QA models with QQP models using datasets from WebMD, HealthTap, and QQP. Results showed that the QA model consistently outperformed the QQP model, even surpassing the HealthTap model with the same data size. Different tasks were created using HealthTap data to test performance, showing that in-domain tasks improved model performance. In contrast, finetuned BERT models on medical tasks performed worse than the baseline BERT model, indicating that domain relevance is crucial but not all tasks are suited for encoding proper domain information. Comparisons were made with BioBERT, SciBERT, and ClinicalBERT models. Only BioBERT outperforms the original BERT model, and the differences in performance are not statistically significant. The augmented questions are not added to our test set and do not contribute to our quantitative performance metrics; they are only created to form hypotheses about mislabeled examples. The study compared QA models with QQP models using datasets from WebMD, HealthTap, and QQP. Results showed that the QA model consistently outperformed the QQP model, even surpassing the HealthTap model with the same data size. Different tasks were created using HealthTap data to test performance, showing that in-domain tasks improved model performance. In contrast, finetuned BERT models on medical tasks performed worse than the baseline BERT model, indicating that domain relevance is crucial but not all tasks are suited for encoding proper domain information. Comparisons were made with BioBERT, SciBERT, and ClinicalBERT models. Only BioBERT outperforms the original BERT model, and the differences in performance are not statistically significant. The augmented questions are not added to our test set and do not contribute to our quantitative performance metrics; they are only created for the sake of probing and understanding the network. Consider the example in Table 4. In order to label this example correctly as it is written in row 1, the model has to understand the syntax of the question and know that 4'8\" in this context represents poor growth. Changing the second question to what is written in row 2 prompts the QA model to label it correctly, indicating that one thing the QA model was misunderstanding was the question's word order. Additionally, changing the phrase I am 4'8\" with I have not grown as shown in row 3 is enough to help the out-of-domain models label it correctly. So, while numerical reasoning was the difficult part of that question pair for the other models, the question answer model was actually able to identify 4'8\" as a short height. This supports the claim that pre-training on the medical task of Misspellings, capitalization. The study compared QA models with QQP models using datasets from various sources. Pre-training on the medical task of Misspellings and capitalization did not significantly impact model performance. When applied to non-medical data, pre-training on QQP led to a decrease in accuracy on question similarity tasks. The study found that pre-training on an out-of-domain task can decrease performance accuracy, but pre-training on related tasks within the same domain can improve results. The low accuracy and challenges with question length truncation were noted, with a suggestion to explore ways to reduce question length. A medical question-pairs dataset was released, showing the effectiveness of semi-supervised pre-training on in-domain question-answer matching for duplicate question recognition. The QA model outperforms the out-of-domain same-task QQP model, but there are instances where the QQP model has learned additional information. Future exploration includes combining features from both models through multitask learning. Error analysis helps identify mistakes for better weak supervision and data augmentation, aiming to increase accuracy on the task."
}