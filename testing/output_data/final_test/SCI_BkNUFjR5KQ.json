{
    "title": "BkNUFjR5KQ",
    "content": "In this paper, a new network structure is proposed that combines the benefits of dense and sparse connections in deep convolutional neural networks. The structure uses dense modules as building blocks and connects them sparsely during training. Experimental results show competitive performance on benchmark tasks while keeping the network slim. This approach aims to bridge the trends of increasing density for accuracy and creating sparsity for lower computational costs. Our paper explores methods to introduce sparsity directly into network structures, avoiding post-training pruning. Research on brain structure reveals locally dense but externally sparse neuron connections. The visual cortex processes sensory information through hierarchy regions, with densely packed pyramidal cells forming locally dense structures. Modular structures play a vital role in the brain's information transfer. Modular structures in the brain are important for information processing and supporting neural dynamics. Instead of pruning redundancy in trained models, we introduce local density by adding untrained dense modules. Using a structure similar to DenseNet with narrow channels, we evolve weights and connections through training. The growth rate parameter in Densely Connected structures determines how the input feature map scales as the network deepens. Previous methods for neural module construction with structural sparsity are mostly empirical. Our genetic training strategy optimizes connection matrices for neural modules with structural sparsity, creating sparse connections and enhancing model robustness. It introduces long-distance connections and maintains competitive performance on image datasets. The paper enhances hierarchical structure by utilizing locally dense but externally sparse connections. An evolving training algorithm is used to optimize connection matrices instead of empirically constructing module connections. Modules choose output flow globally, allowing features to flow through various depths. Detailed analysis is provided on how different sparse connections and module properties contribute to final performance. The paper explores network architectures in deep learning, emphasizing the importance of sparse connections between dense modules. It discusses the evolution from simpler structures like LeNet to more complex ones like DenseNet, highlighting the challenges of over-fitting and gradient-vanishing. Dense connections within each dense block are crucial for increasing network capacity. In this paper, the focus is on constructing internal densely connected modules within dense blocks to achieve competitive results on benchmark datasets using a slim network structure. Deep neural network compression methods aim to reduce network redundancy through numerical approximation of kernels and sparse regularization on kernels. Our paper combines global sparsity and locally dense features to maintain high capacity while slimming down the network structure. Evolutionary algorithms have been used in neural network research, with recent focus on reinforcement learning and potential for image classification. Google's NasNet is a state-of-the-art deep neural network structure. Google has proposed the NasNet BID42 deep neural network structure, achieving the best performance by searching for the best architecture on a large scale. The paper focuses on structural density and sparsity, using an evolving algorithm to search for sparse connections during training. The convolution operation is explained as a projection between different channels of feature maps, with connections represented by channel-wise mapping. The importance of each channel pair is represented using Frobenius norm in filter equations. The importance of channels in a convolutional neural network is represented using Frobenius norm. By analyzing the connections between input and output channels, it is possible to identify areas of redundancy. Local dense connections in convolution kernels can help save parameters, as shown in the decomposition method. The decomposition method sacrifices connections in each layer to maintain high model capacity. Sparse connections are created between modules to save parameters. Local density is achieved by regulating weights distribution and exploring sparse connections during the training process. In order to achieve locally dense connections, the model stacks narrow convolution kernels into dense modules with a bottleneck layer. The connectivity between bottleneck layers is densely connected, and the feature map size remains constant within each dense module while channels grow rapidly with depth and growth rate. A transit layer is used to reduce the channels of the output feature map. In DenseNet BID16, a transit layer is introduced to reduce the channels of the output feature map by half. The dense module consists of densely connected bottleneck layers, with a transition layer controlling the feature map size. The dense blocks in the experiment have depths ranging from 6-20 layers. To address sparse connections between modules, methods for both short and long distance connections are explored. In DenseNet BID16, a transit layer reduces output feature map channels by half. Dense modules have densely connected bottleneck layers, with a transition layer controlling feature map size. Methods for short and long distance connections are explored. Experiment results show that the addition method is preferred for sparse connections over concatenation. Long-distance connections are facilitated by a transfer layer with {1*1conv -average pooling} structure. In DenseNet BID16, a transfer layer with {1*1conv -average pooling} structure is used to fit feature maps into dense modules. Sparse connections are represented using an adjacent matrix, with density defined as sum(Cmax). Directed graphs and down sampling connections are utilized, ensuring the lower left of the matrix is always zero. In creating sparse topology connections, a genetic algorithm is used to search for important connections in neural networks. The connection matrix is encoded as genes for the algorithm to optimize sparse connections. The algorithm generates new individuals with genes from the best performer in the previous iteration, evolving by selecting the best performing individual. The genetic algorithm evolves by selecting the best performing individual with a good encoding to describe object features using the adjacent matrix. The initial state involves randomly initializing module weights and setting the population between 2 to 3 individuals. The connection matrix in the initial state only has parallel direct connections. Evolution Strategy involves generating individuals based on the best performer from the previous iteration. The connection matrix is defined as P init, P best, and P i at the beginning of each iteration. The mutation function G uses P best to generate new individuals for the next iteration, ensuring the best performer continues to evolve. The mutation function G randomly changes connections in the input connection matrix, making it denser if density is less than 0.5 and sparser if density is greater than 0.5. Each individual in the population is separately trained for a complete epoch to ensure fair comparison. Checkpoints are set before training to ensure all individuals start from the same point, and only the best individual is retained after the training process. After setting checkpoints, the best performing individual is retained for the next iteration. The process is outlined in Algorithm 1 and Fig P init \u2190 Initial Connection Matrix. An experiment comparing Concatenation vs. Addition methods is conducted to determine the connection method. The network is trained on the CIFAR10 dataset using sparse connections. The test results are shown in FIG5. The test results in FIG5 compare the addition and concatenation methods for connection in a network trained on the CIFAR10 dataset. The addition method shows negligible differences with the concatenation method, despite having more fluctuations in the curve. The addition method is chosen for later experiments due to its speed and convenience for changing feature map sizes. Step jumps in accuracy are caused by learning rate changes in all experiments. In experiments with prefixed dense modules, step jumps occur at the same position. Modules have varying depths and layers, with a total of 12 modules and a growth rate of 32. Sparse connection evolving algorithms were tested on the CI-FAR10 dataset using specific parameters. The learning rate strategy involved three phases, affecting accuracy 'step jumps' as shown in FIG5. The experiments showed that the training curve of P best converged despite initial randomness in population generation. The optimized connection matrix varied but still achieved high accuracy. Modules with shallow depth tended to form long-distance connections, aligning with trends in other papers. The experiments demonstrated that modules with smaller growth rates had higher test accuracy and easier training, but lower final accuracy compared to modules with larger growth rates. This suggests that neural network redundancy is essential for high performance, but not necessarily better with larger growth rates. After reaching a growth rate larger than 32, test accuracy does not improve further. Large module capacities can hinder training with unstable input features. Increasing growth rates increase model scale and the risk of overfitting. The paper focuses on sparse connections' impact on model performance, with performance scores on benchmark datasets. While test accuracy on ImageNet and CIFAR is not as high as state-of-the-art models, competitive results are still achieved. Optimal sparse connections are obtained through an evolving training algorithm. The study focuses on the importance of sparse connections in neural networks. By testing the accuracy loss when cutting off individual connections, it was found that local and direct connections play a vital role in feature propagation. The experiment results show that connections with shallow depth are more crucial in conducting features/patterns. The study emphasizes the significance of shallow connections in neural networks. Shallow connections play a more important role in conducting features/patterns than deeper connections. Experiment results suggest that local connections contribute to base accuracy, while long-distance connections contribute to incremental accuracy improvements. Modules with more connections as input are more robust when some connections are cut off. The study explores the importance of sparse connections in neural networks, showing that evolving sparse connections can achieve competitive results on benchmark datasets. By conducting contrast experiments, it is revealed that a combination of dense and sparse structures can lead to better performance. The study also highlights that redundancy in dense modules does not always lead to improved test accuracy. The internal dense and externally sparse structure aligns with the modularity seen in the human brain. The study focuses on the feasibility of sparse connections in neural networks and the relationship between different connection matrices for optimal performance. It aims to achieve state-of-the-art results on various datasets and tasks in the future. Additionally, the potential of separating a network into smaller networks without losing accuracy is also discussed."
}