{
    "title": "HyfHgI6aW",
    "content": "Memory Augmented Control Network (MACN) is proposed to address planning problems in partially observable environments that cannot be solved directly with convolutional networks. MACN splits planning into a hierarchical process, learning to plan in locally observed spaces at a lower level and using policies computed on these spaces to create an optimal plan in the global environment at a higher level. The network's performance is evaluated on path planning tasks with simple and complex obstacles, as well as its ability to generalize to new environments not seen during training. Planning tasks in partially observable environments involve inferring the environment structure from local observations and acting based on the current estimate. Previous approaches have used supervised learning and deep reinforcement learning, with end-to-end methods being popular for solving complex problems. However, purely convolutional architectures perform poorly in planning problems due to their reactive nature. In planning problems, convolutional neural networks (CNNs) struggle due to their reactive nature, especially in partially observable environments. To address this, recurrent neural networks or memory networks are needed to retain past information for optimal control. Recent advances in memory augmented networks have shown the benefits of using external memory with read and write operators that can be learned by a neural network. The Differentiable Neural Computer (DNC) is a notable example that uses an external memory and network controller to handle read, write, and access operations. This approach separates computation and memory operations, allowing for end-to-end training and integration with convolutional architectures. The Memory Augmented Control Network (MACN) is a novel architecture designed for planning in partially observable environments with sparse rewards. It splits the planning problem into two levels of hierarchy, with a planning module computing optimal policies using a feature-rich representation of the locally observed environment. The approach involves using a planning module to output a local policy that augments neural memory for optimal policy generation in the global environment. The model is evaluated on its ability to learn policies in environments with obstacles and generalize to unseen situations. Key contributions include a network architecture with differentiable memory for environment geometry estimation and hierarchical planning for path planning to the goal. The methodology involves using environment geometry and hierarchical planning to learn path planning. It includes notation, problem statement, theory on value iteration networks and memory augmented networks, and practical implementation details for a model operating in an unknown environment with hidden state labeling. The agent's task is to reach a goal region in an obstacle-free environment, using observations from a sensor that reveals nearby states. The agent's available information includes sequences of observations, states, and actions to compute its next move. Problem 1 involves finding a function \u00b5 that maps sensor observations directly to actions for an agent to reach a goal region in an obstacle-free environment. The task is to learn a policy considering partial observability and memory in a Markov Decision Process (MDP) defined by M(H, A, T, r, \u03b3). The reward function in an MDP is defined based on observations z 0:t to formulate a locally valid problem. Value Iteration (VI) is a typical algorithm used to solve an MDP by iteratively calculating action values for each state to determine an optimal policy. The Value Iteration algorithm can be approximated using a convolutional network with max pooling, as shown in recent works on deep learning. This approach involves using a VI module in Value Iteration Networks (VIN) to represent the algorithm over K iterations. The recent works focus on learning to read and write to external memories in the DNC architecture. The external memory uses differentiable attention mechanisms, and a controller like LSTM is used to learn read and write operations. The DNC architecture focuses on learning to read and write to external memories using differentiable attention mechanisms. Memory addressing is defined separately for writing and reading, with content-based addressing and dynamic memory allocation for write locations, and content-based addressing and temporal memory linkage for read locations. The agent in a 2D grid world aims to navigate to a goal region in an environment represented by a Markov Decision Process (MDP). The architecture focuses on planning in a locally observed space within a Markov Decision Process (MDP). Planning is done at a lower level in this local space, independent of previous observations, to calculate the optimal policy and value function. However, this approach fails in environments with local minima, such as long narrow tunnels in a 2D/3D world. In environments with local minima like long narrow tunnels, the agent must explore all the way to the end due to partial observability. Differentiable memory is proposed to estimate the environment map, allowing the controller to selectively read and write information for successful backtracking. The network learns to read and write information selectively in environments with local minima. A differentiable memory scheme keeps track of important events and landmarks, discarding redundant information. A value iteration module approximates the value iteration algorithm within a neural network to learn value maps from local information. The architecture shown in Figure 3 utilizes value maps in a differential memory scheme for better planning compared to using only CNN features. The VI module learns to plan based on local observations, with local value maps concatenated with environment features and sent to a controller network. The controller network interacts with memory through an access module, generating read, write, and access heads. It also performs planning computations, with the output concatenated and passed through a linear layer to produce an action. This two-level problem decomposition is trained end-to-end. The planning problem is solved by decomposing it into a two-level problem. A feature-rich representation of the environment is used to generate local policies at a lower level, while a learned history representation and sparse feature representation of the current environment are used to generate a global optimal policy. The computation graph involves a 2D grid world with obstacles, start and goal regions, where the network observes only a small part of the image at a time. The agent observes only a small part of the image at a time within the range of its sensor. The input image is stacked with a reward map, and convolved with kernels to estimate the action value function and state value. The MACN architecture uses convolutional layers to extract features from the environment and generate value maps. The model calculates initial state values by taking the max over action values. In a for loop, R and V are concatenated, convolved with a filter, and updated to get the action value of the state. The controller network emits read and write heads based on the value maps and low-level features. The MACN architecture uses convolutional layers to extract features from the environment and generate value maps. The DNC controller, with access to external memory, uses LSTM with varying hidden units. The output from the controller and memory is concatenated for action prediction. RMSProp optimizer with a learning rate of 0.0001 is used. The formulation can be extended to larger state and action spaces, as demonstrated in experiments. The experiments aim to answer key questions about MACN's planning capabilities. MACN experiments aim to answer key questions about planning capabilities in partially observable environments with sparse rewards. The network can learn to plan in 2D grid world environments and generalize to new unknown environments. It can be extended to other domains, with the ability to navigate obstacles in a grid world with random start and goal positions. The network is tested on a 2D partially observable environment with random start and goal positions. Different models are evaluated on maps not present in the training set, with episodes terminated after a certain number of time steps or if the agent collides with an obstacle. The CNN architecture processes sensor images with reward maps through convolutional layers to extract features. The CNN architecture processes sensor images with reward maps through convolutional layers to extract features. The Memory-Augmented Convolutional Network (MACN) is tested on a 2D partially observable environment to navigate unknown environments. Results show that MACN can learn to navigate in such environments, while a CNN + Memory architecture similar to previous work performs poorly due to sparse rewards. In experiments, MACN outperforms other architectures due to sparse rewards. Testing accuracy drop is less as grid world scales. Next, the experiment will determine if MACN learns to plan or overfits. Previous results show MACN can plan in 2D environments, but generalizability is weak. Test environments have same dimensions as training set, with limited random obstacles, allowing wrong actions to still reach the goal. In the following experiment, the network's ability to generalize to new environments is tested by training on smaller maps and then testing on larger maps with tunnels. The network's capacity to generalize to longer tunnels in bigger maps not seen in the training set is evaluated. The network's memory plays a crucial role in planning tasks, allowing it to remember past information and policies to output the right actions. Traditional deep reinforcement learning struggles with sparse reward structures, as seen in experiments with DQN and A3C architectures. In contrast, the CNN + memory scheme learns to navigate tunnels effectively, showcasing better performance in turning around at fixed lengths. The CNN + memory scheme shows better performance in navigating tunnels effectively, with the ability to generalize to new environments. The network's memory plays a crucial role in planning tasks, showing a big shift in memory states when the agent reaches the end of the wall or exits the tunnel. The network's memory plays a crucial role in planning tasks, showing a big shift in memory states when the agent reaches the end of the wall or exits the tunnel. The proposed model is generalizable to new environments with similar structures, even when state and action spaces are scaled up. In planning tasks, the environment is defined as an undirected graph where nodes represent possible states. The agent can only observe edges connected to its current node, making it partially observable. The action space consists of possible nodes to visit next, with rewards given upon reaching the goal. Random start and goal positions are added, along with a transition probability of 0.8. The results show that the MACN model outperforms other models in planning tasks on graphs with small nodes. However, performance starts to degrade on larger map sizes. Despite occasional wrong actions, the agent still reaches the goal efficiently. MACN demonstrates the ability to plan in more complex problems with non-limited state and action spaces. In robotics, navigating in unknown environments with limited visibility is a common challenge. Traditional methods involve storing a representation of the entire environment, which can be memory-intensive. An experiment extended MACN to a SE2 robot capable of moving in the x-y plane with orientation. The robot uses a laser scanner to perceive the environment, which is converted into a 2D framework for the MACN. The environment is represented as a m \u00d7 n grid fed into the MACN for processing. The experiment extended MACN to a SE2 robot navigating in unknown environments with limited visibility. The robot uses a laser scanner to perceive the environment, which is converted into a matrix fed into the MACN. The network output generates way points for the controller, with training and testing done on randomized spawn and goal locations. Performance results show successful navigation to the goal with close trajectory to ground truth, despite the added complexity of orientation. The experiment extended MACN to a SE2 robot navigating in unknown environments with limited visibility using a laser scanner. MACN shows faster convergence to the goal compared to other baselines and can scale up in larger environments. Increasing memory size improves success percentage in reaching the goal. In the robot world, MACN's success percentage increases as the distance to the goal grows, showing its ability to scale well in larger environments by adjusting memory size. The network outperforms traditional motion planning baselines in computing trajectories to the goal. The proposed network outperforms traditional motion planning baselines in grid world environments and tunnel tasks. Expert trajectories are obtained using A* for grid worlds and Human Friendly Navigation (HFN) for continuous control domains. Results show that the VIN struggles to reach the goal in a fixed number of steps and gets stuck in local minima in tunnels. The CNN+Memory struggles to navigate in tunnels, turning around before reaching the end. The MACN performs well in small grid worlds but struggles as the world size increases. The MACN with DNC outperforms MACN with LSTM in larger grid worlds. Both network architectures successfully navigate tunnel environments like A* but with more compact map representations. Our model-free approach allows for adaptation to different environments with previously trained policies. The curr_chunk discusses the use of value iteration networks augmented with memory for navigation in partially observable environments. It contrasts with previous work that used a planning module and map representation for robot navigation. The model learns to project image scans into a 2D grid world and adapt to new features by fine-tuning the trained policy. In contrast to previous work, the model in the curr_chunk focuses on designing a general memory-based network for partially observed planning problems without building a 2D map of the environment. This approach involves learning a belief over the environment and storing it in a differentiable memory, different from other works that project the environment into a 2D grid world. The curr_chunk discusses the use of a modified memory scheme with a differentiable planner for optimal path learning and efficient exploration in partially observable environments with sparse rewards. Deep RL policies' generalization to new environments is also highlighted as an area that has not received much attention. In this work, the focus is on designing architectures for computing optimal policies in environments with sparse rewards. The generalization power of the learned policy is thoroughly investigated, showing scalability to large dimensional spaces. Experiments in grid worlds demonstrate the network's ability to plan effectively. The model is tested in environments beyond 2D grids, including graphs with no constraints on state or action space, and under continuous control. Future plans involve transferring policies to real-world platforms like robots in partially observable environments. The work focuses on designing architectures for optimal policies in environments with sparse rewards. The use of simple perfect sensors without considering sensor effects is a limitation. Future exploration includes learning sensor models and planning. The sensor in the grid world is a 7x7 patch with the agent at the center. The input image to the VI module is a [m x n x 2] image, with the sensor input in I[:, :, 0] and the reward map in I[:, :, 1]. The rewards are sparse, with the reward map only showing the goal position. The reward image R of dimension [n \u00d7 m \u00d7 u] is convolved and sent to the VI module. The value maps from the VI module are fed into the memory network controller, which includes a LSTM with 256 hidden units. During training and testing, sequences are rolled out state by state based on ground truth or network output. Transitions from start to goal are considered episodes, with the internal state of the controller and memory cleared at the end of each episode. The external memory size is 32 \u00d7 8 in the grid world task, with a parameter controlling read and write heads frequency. In the grid world task, the number of read heads is fixed at 4 and the number of write heads at 1. MACN performs better with curriculum training, as shown in the original VIN and DNC papers. VIN and CNN+Memory models are also trained with curriculum learning for baseline comparison. Tasks in the grid world environment can be made harder by increasing obstacles and their sizes gradually. In the grid world experiments, the optimal action is generated using A star BID13 with increasing obstacles and sizes. Error curves on the test set for MACN with LSTM and addressable memory scheme are shown in FIG2. The network is set up similarly to the grid world experiments with blob shaped obstacles. Training with curriculum is not necessary due to the simple environment structure. The VI module performs well with tunnel shaped obstacles when fed with partial maps instead of sensor input. The network in the grid world experiments can generalize to new maps with longer tunnels without needing memory. The DQN performs poorly with sparse rewards, never reaching the goal even after 1 million iterations. The agent converges to a bad policy under a random initial policy. The agent in the grid world experiments fails to reach the goal under random initial policy and A3C. Even when partial map is used instead of sensor input in DQN, the agent does not converge. Memory states change only during important events, such as entering/exiting the tunnel and turning towards the goal. MACN is prone to overfitting in this task. In the graph experiment, a random connected undirected graph with N nodes is generated. The agent moves between nodes via edges and can take actions to move to specific nodes. Transition probabilities are determined at each node, with access to unique node numbers and adjacency matrix rows. In a graph experiment, a random connected undirected graph with N nodes is generated. The agent moves between nodes via edges and can take actions to move to specific nodes. Transition probabilities are determined at each node, with access to unique node numbers and adjacency matrix rows. To train the network to navigate this graph, supervised training with an expert demonstrating breadth-first search behavior was used. Training samples were generated by running breadth-first search and connecting nodes explored by previously traveled nodes. The adjacency matrix shows connections with white and no connections with black. The goal is represented by a green shaded row, and the current state by a blue shaded row. The row vector of the matrix was reshaped into a matrix for use in the network and 2D convolutions. In a graph experiment, the network is trained to navigate a random connected undirected graph with N nodes using supervised training with an expert demonstrating breadth-first search behavior. The row vector of the matrix is reshaped into a matrix for use in the network and 2D convolutions. The reward prior is reshaped and stacked with the observation, and example paths between pairs of nodes are used for training. The task is more complex than grid world navigation due to increased action and state space. MACN and baselines are trained with curriculum training, and the number of hops between start and goal state defines complexity in the graph task. In robotics, Simultaneous Localization and Mapping (SLAM) is used to localize the robot and create a map of the unknown environment. A differential drive robot with LIDAR is used for continuous control experiments. Ground truth maps are generated using Human Friendly Navigation (HFN) for reactive collision avoidance paths. The experiment focuses on using LIDAR data, with potential future use of camera data. In robotics, Simultaneous Localization and Mapping (SLAM) is utilized for robot localization and mapping of unknown environments. The experiment involves generating way points for the controller using a tuple of (x, y, \u03b8) for each observation. A matrix is initialized for the environment and a reward array is concatenated with it. Laser scan observations are converted to a matrix, updating values in the environment array. The environment matrix is reset at each iteration to provide a partially observable environment for the MACN. The experiment involves using SLAM for robot localization and mapping in unknown environments. The input image I to the VI module is [m \u00d7 n \u00d7 2] where m = 200, n = 200. The reward image R is obtained by convolving I. The network controller is a LSTM with 512 hidden units and external memory with 1024 rows. The access module has 16 write heads and 4 read heads. Output from the access module is combined with LSTM output and passed through a linear layer for probability distributions for (x, y, \u03b8). The robot controller uses probability distributions for (x, y, \u03b8) to determine waypoints, which are then clipped for incremental steps. Performance improves with curriculum training, training first on close goals and then on further goals. Training and testing occur on the same map with random start and goal regions."
}