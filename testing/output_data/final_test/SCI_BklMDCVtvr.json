{
    "title": "BklMDCVtvr",
    "content": "Neural networks can perform tasks that rely on compositional structure without obvious mechanisms for representation. ROLE is a technique that detects if representations encode symbolic structure by approximating a target encoder's representations. It defines symbolic constituent structure through roles that can be filled by symbols. ROLE successfully extracts ground-truth roles when the encoder is explicitly structured. It was also tested on a seq2seq network trained for a complex compositional task (SCAN) without a predefined role scheme. ROLE successfully discovers an interpretable symbolic structure in a complex compositional task (SCAN) without a ground truth role scheme. The discovered symbolic structure is causally important, as manipulating hidden embeddings based on it changes the model's output as predicted. Popular sentence embedding models are found to not capture compositional structure, highlighting the potential for ROLE to improve model abilities. Certain AI tasks involve computing a function \u03d5 governed by strict compositional rules. Symbolic AI systems can assign symbolic structures to inputs and apply compositional rules to achieve systematic generalization in processing novel combinations of familiar constituents. Symbolic AI systems excel at tasks involving strict compositional rules, such as assigning meanings to English adjectives. However, natural language tasks are only partially compositional, leading to exceptions in the rules. Deep learning research has shown that Neural Networks can outperform symbolic AI systems on partially-compositional tasks, but still fall short on fully-compositional tasks. Neural Networks (NNs) have shown success on partially-compositional tasks, raising questions on how they achieve strong generalization. Research indicates that NNs trained on highly compositional tasks learn representations similar to symbolic structures, enabling them to process inputs and generate outputs based on compositional rules. These NNs, referred to as target NNs, will be analyzed in this paper. The paper introduces a new Role Learner (ROLE) neural network model to analyze target networks without relying on hand-specified hypotheses. ROLE automatically learns symbolic structures to approximate the internal representation of the target network, simplifying the analysis process and allowing for successful uncovering of symbolic structures in various tasks. In this section, the paper discusses fully-compositional tasks such as string-manipulation and the SCAN task, as well as partially-compositional tasks in NLP. It also explores how insights from standard neural networks can inform new inductive biases to enhance compositionality in neural network learning. The analysis task DISCOVER is introduced to dissect compositionality in vector representations within neural networks. McCoy et al. (2019a) demonstrated that in GRU encoder-decoder networks, the medial encoding can be well approximated by Tensor Product Representations (TPRs). TPRs are vector embeddings of symbolic structures that represent strings of symbols by parsing them into constituents based on their positions. The embedding of symbolic structures in neural networks can be represented by Tensor Product Representations (TPRs), where roles are filled by symbols and the whole string embedding is the sum of its constituents. This work aligns with the analysis techniques used to interpret NNs, focusing on compositional structure. Popular techniques include behavioral and probing approaches, evaluating models on linguistic phenomena examples. The probing approach in neural network analysis involves training an auxiliary classifier to classify internal representations based on linguistic distinctions. This method tests for the presence of specific information in the model's encodings but does not determine if this information is utilized by the model. DISCOVER bridges the gap between representation and behavior by revealing encoded information and its causal implications in the model's behavior. It provides a more comprehensive view of the representation compared to the probing approach, exhaustively decomposing the model's representational space. This method is closely related to other approaches that aim to discover a complete symbolic characterization of vector representations. ROLE is a method for producing a vector-space embedding of input strings by minimizing mean-squared error between its output and a pre-trained target string-encoder. The role attention vector is encouraged to be one-hot through regularization to correspond directly to defined roles in the role matrix. ROLE is an extension of the Tensor-Product Encoder (TPE) that learns to compute role labels for input symbols by using role-embedding vectors and symbol-embedding filler vectors. ROLE is an extension of the Tensor-Product Encoder (TPE) that learns to compute role labels for input symbols using role-embedding vectors and symbol-embedding filler vectors. It generates a TPR by combining symbol embeddings with role vectors, and learns a linear transformation to map this TPR into a specific dimension.ROLE uses an LSTM to compute role-assigning attention vectors for each symbol, aiming to bias learning towards one-hot vectors through regularization. ROLE is an extension of the Tensor-Product Encoder (TPE) that uses regularization to bias learning towards one-hot vectors. The performance can vary across runs due to the discrete symbolic landscape. There is no explicit bias favoring discrete representations in the target encoder E. ROLE is applied to two fully compositional Tensor Product Encoder (TPE) models for autoencoding sequences of digits. The models use different role schemes: one with a simple left-to-right scheme and the other with a complex tree position scheme. Each model is paired with a specific decoder, achieving near-perfect performance. The tree-RNN decoder used by McCoy et al. (2019a) achieved near-perfect performance on the autoencoding task. The encoding for each sequence is extracted after training the encoders to train ROLE. The ROLE approximation showed perfect performance on the left-to-right TPE, with 100% substitution accuracy and a V-Measure of 1.0. ROLE achieved similar accuracy to the target encoder E and successfully identified the compositional structure of models designed to be compositional. However, the question remains on how ROLE can uncover structure in models without explicit compositional design. Our hypothesis is that models like standard RNNs can implicitly learn compositional structure despite lacking explicit constraints. To test this, we apply ROLE to a standard RNN-based seq2seq model trained on a fully compositional task, specifically the SCAN task designed for compositional generalization and systematicity testing. The curr_chunk discusses a sequence-to-sequence mapping task where an input action plan is mapped to primitive actions using a GRU encoder achieving high accuracy. The encoder's final hidden embeddings are extracted after training. After training, the final hidden embeddings from the encoder are extracted for each example in the training and test sets. ROLE is provided with 50 roles to use, and the substitution accuracy of the learned role scheme is evaluated in three ways: continuous method, snapped method, and another discrete method. The final evaluation method uses discrete roles without a train/test discrepancy. Various hand-crafted role schemes are compared for substitution accuracy. The predefined role schemes provide poor approximations, with none surpassing 44.12% accuracy. In contrast, the role scheme learned by ROLE achieves 94.12% accuracy, demonstrating the effectiveness of compositional internal representations. ROLE serves as a valuable analysis tool in complex scenarios, showcasing its utility beyond autoencoding tasks. The symbolic algorithm created for predicting roles in the SCAN training set achieved 98.7% accuracy in predicting roles for test sequences. The algorithm reveals how the filler-role scheme encodes task-relevant information, such as determining the structure of the output based on the commands in the sequence. Role 30 is exclusively used for the filler, while role 17 is only used in sequences connected by \"and\". The decoder uses role 30 for the filler and role 17 for sequences with \"after\". It determines the basic structure of the output and fills in actions using other roles based on absolute position within a command. The decoder uses roles 8 and 46 to identify the subcommands surrounding \"after\" and determine if a cardinality is present. The decoding process may involve parallel steps, and further research is needed to understand the operations performed in an RNN. The decoder intervenes on internal representations by replacing constituents to observe changes in output. The encoding is modified based on the roles determined by the algorithm. This surgical approach extends the analogy method used for word analysis. In this section, the text explores whether neural networks trained on partially-compositional tasks can capture compositional structure. Four sentence encoding models are tested: InferSent, Skipthought, and Stanford Sentiment Model. The text explores neural networks trained on compositional tasks using various models like InferSent, Skipthought, and Stanford Sentiment Model. Encodings are extracted for SNLI premise sentences to train ROLE with 50 roles. The challenge of explaining compositionality is discussed, and syntactic categories are extracted from the SCAN grammar. Based on the analysis in Appendix A.8, occurrences of \"and\" and \"after\" are not replaced due to significant role changes. TPEs are trained with pre-defined role schemes, with ROLE and continuous attention showing the lowest mean squared error for most sentence embedding models. The BOW role scheme assigns the same role to every filler, performing similarly to structure-free BOW for most models except SST. Structure is not essential for tasks like Natural Language Inference, as shown by the success of a bag-of-words model. The results suggest that sentence embedding models may not rely on compositional representations. TPRs provide NNs with systematicity by separating fillers and roles. ROLE is used to interpret the workings of the encoder E, with plans to train it in an end-to-end manner to encourage learning compositional encodings. This explicit bias for compositionality will be tested to see if it improves network performance. In this work, the structured representations provided by TPRs allow for the processing of fillers and roles to be encoded independently, showing how a hidden embedding can be factored into fillers and roles. The results suggest that ROLE can disentangle syntax and semantics, potentially improving network performance in compositional tasks. In this work, the neural network ROLE learns to approximate representations of a target network using symbolic structure. It successfully discovers symbolic structure in models with and without explicit structure, showing potential for improving systematic generalization in compositional tasks. The sentence encodings represent compositional structure, uncovering the latent symbolic structure of neural network representations. The Tensor Product Encoder architecture combines vector embeddings for fillers and roles to produce a matrix representing the TPR of the constituent. This approach aims to improve compositional generalization for partially-compositional tasks. The Tensor Product Encoder architecture combines vector embeddings for fillers and roles to produce role schemes for target networks on digit sequence tasks. Two TPEs were trained end-to-end with an RNN decoder, one using a left-to-right role scheme and the other using tree positions. The left-to-right TPE achieved 100% accuracy on the test set, while the tree TPE achieved 98.62%. The regularization term in ROLE training encourages one-hot vectors for each position in a discrete symbolic structure. The factors in the regularization term are non-negative, favoring one-hot vectors, and using both terms helps the training process. The final term in the regularizer encourages each position to hold at most one symbol. In ROLE training, the regularization term promotes one-hot vectors for each position in a symbolic structure. The regularization term includes non-negative factors to favor one-hot vectors and ensure each position holds at most one symbol. R is designed to encourage this by minimizing R3 when all elements of s are 0 or 1. Normalizing each role embedding in the role matrix R ensures small attention weights have small impacts on the weighted-sum role embedding. The hidden embeddings for each item in the training, dev, and test sets were extracted after training the TPEs in Sec. A.3.ROLE models for the digit sequence task used a bidirectional 2-layer LSTM with filler dimension of 20 and regularization constant \u03bb = 1, trained using ADAM. The ROLE model trained on the LTR TPE had 20 roles of dimension 20, while the Tree TPE model had 120 roles of dimension 120. The standard RNN on SCAN used a fixed GRU architecture with a single hidden layer and no attention mechanism. Hyperparameter search included hidden dimension sizes of 50, 100, 200, and 400, as well as dropout values of 0, .1, and .5. The best performing network in the study had a hidden dimension of 100 and dropout of .1. For the ROLE models, a filler dimension of 100 and a role dimension of 50 with 50 roles available were used. The role assignment module utilized a bidirectional 2-layer LSTM. A hyperparameter search was conducted over the regularization coefficient \u03bb, with the best performing value being .02. The Role Learner algorithm accurately assigns roles to input elements in the SCAN model, matching 98.7% of sequences. Input sequences are categorized into types based on the presence of \"and\" or \"after\" connectors. Breaking down commands with these connectors helps determine role assignments effectively. The Role Learner algorithm assigns roles to input elements in the SCAN model, matching 98.7% of sequences by categorizing input sequences based on \"and\" or \"after\" connectors. Breaking down commands with these connectors aids in effective role assignments. Key details include components like last word, first word, opposite, direction word, and sequence patterns with \"and\" or \"after\". The Role Learner algorithm in the SCAN model assigns roles to input elements by categorizing sequences based on connectors like \"and\" or \"after\". Details include analyzing components like last word, first word, opposite, direction word, and sequence patterns. The Role Learner algorithm in the SCAN model categorizes input elements based on connectors like \"and\" or \"after\". It assigns roles to components such as last word, first word, opposite, direction word, and sequence patterns. For example, in a sequence like \"jump around left after walk thrice\", roles are assigned to words based on their position and function in the sequence. The Role Learner algorithm in the SCAN model assigns roles to input elements based on connectors like \"and\" or \"after\". It categorizes components such as last word, first word, opposite, direction word, and sequence patterns. The algorithm determines the roles based on the presence of certain words in the sequence, allowing the decoder to distinguish between one or two basic commands in the output. The Role Learner algorithm in the SCAN model assigns roles to input elements based on connectors like \"and\" or \"after\". It categorizes components such as last word, first word, opposite, direction word, and sequence patterns. The algorithm determines the roles based on the presence of certain words in the sequence, allowing the decoder to distinguish between one or two basic commands in the output. The decoder can then decode specific commands by checking which fillers are bound to relevant roles for that type of command. Each command consists of a single mandatory action word followed by optional modifiers like rotation words, direction words, and cardinalities. The SCAN encoder implicitly learns different roles based on the last element of a subcommand, providing functionally-relevant information. The algorithm in the SCAN model assigns roles to input elements using connectors like \"and\" or \"after\", allowing the decoder to decode specific commands by checking bound fillers. The algorithm in the ROLE model describes representations produced by the original network. It is interpretable locally but not globally transparent. ROLE can identify complex role schemes using gradient descent, enabling analysis of networks beyond human intuition. The ROLE model can identify complex role schemes in network representations, potentially explaining limitations in the original model's systematic generalization. Future work can use ROLE to test the hypothesis that greater systematicity requires simpler and more compositional role schemes implicitly learned by the encoder. The algorithm of A.8.1 simplifies role assignment by using only 16 out of 50 roles, reducing the number of conditionals needed to assign roles to words in different contexts to just 47, improving performance on the test set without overfitting. The algorithm in A.8.1 simplifies role assignment by using only 16 out of 50 roles, reducing the number of conditionals needed to assign roles to words in different contexts to just 47, improving performance on the test set without overfitting. The 47-conditional algorithm found abstracts over many details of the context to generalize effectively. The algorithm in A.8.1 simplifies role assignment by using only 16 out of 50 roles, reducing the number of conditionals needed to assign roles to words in different contexts to just 47. Training involved three randomly initialized TPEs for each role scheme, selecting the best performing one based on MSE. Filler embedding dimensions varied for different models, with a linear transformation applied to the pre-trained filler embedding. The filler vector in the filler-role binding in the TPE used a role dimension of 50. To generate tree roles from English sentences, a role dimension of 50 was used for each TPE. Training was conducted with a batch size of 32 using the ADAM optimizer with a learning rate of .001. Three randomly initialized ROLE models were trained for each sentence embedding model, with the best performing one selected based on the lowest MSE. A linear transformation was applied to the pre-trained filler embedding for the filler vector in the filler-role binding in the TPE. The role dimension for each TPE was set at 50, and a linear transformation was applied to the pre-trained filler embedding. Training was done with a batch size of 32 using the ADAM optimizer with a learning rate of .001. A hyperparameter search was conducted over the regularization coefficient \u03bb, with the best performing networks using values such as \u03bb = 0.001, 0.01, 0.001, 0.1 for different models. The structured representation bias provided by TPRs allows for biases to structure processing over representations, enabling independent encoding of fillers and roles. In this work, a hidden embedding can be factored into fillers and roles, as well as a weight matrix into weights processing roles and fillers. Using the example of SCAN, the mapping rule \u03d5(x twice) = \u03d5(x) \u03d5(x) is illustrated. The TPR encoding of jump twice is represented as e(jump twice) = e F (jump) \u2297 e R (r 2 ce -arg). The output string is encoded with positional roles R i, showing filler:role bindings for WALK LOOK. The text discusses how a system can learn to generalize correctly to input \"jump twice\" to produce output \"JUMP JUMP\" by mapping the filler and role using separate mappings \u03d5 F and \u03d5 R computed through weight matrices in a neural network. This can be achieved by using the weight tensor W = W F \u2297 W R to map the embedding of \"jump twice\" accurately. Future models may achieve greater compositional generalization by utilizing TPRs as internal representations and factoring processing weights into fillers and roles separately, as demonstrated in the example with W = W F \u2297 W R in a neural network."
}