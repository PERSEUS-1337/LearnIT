{
    "title": "BJeguTEKDB",
    "content": "Loss functions are essential in deep metric learning, with various types proposed. Instance cross entropy (ICE) is introduced in this work, measuring the difference between estimated and ground-truth matching distributions. ICE offers clear probabilistic interpretation, scalability to infinite training data, and seamless sample reweighting. In deep metric learning, Instance Cross Entropy (ICE) is introduced as a loss function that measures the difference between estimated and ground-truth matching distributions. ICE incorporates seamless sample reweighting to control the differentiation degree over training examples, demonstrating superiority in real-world benchmarks. In deep metric learning, Instance Cross Entropy (ICE) is introduced as a loss function that incorporates sample reweighting for better performance. Efforts have been made to develop efficient loss functions for supervising the learning process, including strategies for informative sample mining and exploiting semantic relations among multiple examples. These structured losses have shown improved performance compared to pairwise and triplet-wise approaches. In deep metric learning, a novel loss function called Instance Cross Entropy (ICE) is proposed to address the limitations of traditional approaches like CCE. ICE aims to improve performance by learning an embedding function through minimizing cross entropy between predicted instance-level matching distribution and ground-truth. ICE is a structured loss function in deep metric learning that focuses on maximizing the matching probability between a query and similar instances, rather than class-level context vectors like CCE. It is scalable to infinite training classes and integrates sample reweighting to prevent trivial training examples as the model improves. The reweighting scheme in deep metric learning aims to differentiate and emphasize informative training examples without explicit data truncation or mining. It addresses the challenge of bounded relative weights between samples by rescaling samples' gradient to control their importance. ICE is a novel approach in deep metric learning that focuses on instance-level matching probability, rather than distance thresholds, for intraclass compactness and interclass separability. It is scalable to an infinite number of training classes and does not assume boundaries between different classes, making it easier to apply in scenarios with unknown intraclass variances. ICE is a novel approach in deep metric learning that focuses on instance-level matching probability and exploits structured information for learning supervision. A seamless sample reweighting scheme is derived for ICE to address the challenge of learning an embedding subspace by projecting all features to a unit hypersphere surface. The superiority of ICE is demonstrated by comparing it with state-of-the-art methods on three real-world datasets. ICE is a novel approach in deep metric learning that focuses on instance-level matching probability and exploits structured information for learning supervision. It compares with NCA and S-NCA, but ICE's optimization task is harder, leading to better generalization. ICE's optimization task is more challenging, resulting in improved generalization compared to NCA and S-NCA. N-pair-mc aims to identify one positive example from N-1 negative examples of N-1 classes, with different approaches such as query versus parametric class centroids, non-parametric class means, and one instance per class. Various prior works include CCE, Heated-up, NormFace, TADAM, DRPR, Prototypical Networks, NCA, and S-NCA. ICE and related losses are compared to prior work in the context of a query versus instances. Different losses have varying cross entropy computations and interpretations. ICE utilizes all negative examples for richer information, unlike N-pair-mc which requires expensive offline class mining. N-pair-mc is considered superior to NCA and Hyperbolic in preserving similarity structures among instances. Unlike Hyperbolic, which learns a hyperbolic embedding space, we focus on learning an angular space where similarity depends on the angle between embeddings. Mining informative examples during training is crucial for faster convergence and better performance, with strategies like emphasizing data pairs with higher losses during gradient backpropagation. In training models like N-pair-mc, emphasis is placed on mining informative examples for faster convergence and better performance. Various strategies, such as assigning higher weights to harder examples and mining hard negative classes, have been proposed to address sample mining and weighting challenges. The proposed ICE method includes a similarity scaling factor to prioritize informative examples, while also considering the importance of mining semi-hard negative pairs. Other novel approaches include hardness-aware examples generation and divide-and-conquer of the embedding space. In ICE, a flexible similarity scaling factor helps prioritize informative examples and alleviate outliers. Unlike other methods, ICE does not rely on heuristic mining or weighting schemes. Matching Networks, NCA, and other scalable methods are also discussed. ICE optimizes retrieval performance by processing one positive at a time, unlike FastAP and TADAM which optimize ranked-based average precision and formulate instances versus class centers, respectively. ICE adjusts the influence of other instances and uses a flexible similarity scaling factor to prioritize informative examples. During training, a weighting scheme is applied to input mini-batches containing images and labels. The goal is to learn an embedding function that captures semantic similarities among samples. Deep embeddings are represented as f(X), with 'positives' and 'negatives' referring to samples of the same class and different classes. CCE is commonly used in classification tasks, with a deep classifier consisting of feature learning and linear classifier learning components. The feature learning module consists of convolutional and non-linear activation layers, while the classifier learning module focuses on learning class-level context vectors for correct classification. CCE aims to maximize the joint probability of correct classification, while ICE measures instance matching quality without the need for class-level context vectors. An anchor may have multiple positives, each isolated in separate matching distributions. The feature learning module includes convolutional and non-linear activation layers, while the classifier learning module focuses on class-level context vectors for correct classification. An anchor may have multiple positives in separate matching distributions. The probability of a query matching a positive is computed using dot product, aiming to minimize cross-entropy between predicted and ground-truth distributions. The feature learning module includes convolutional and non-linear activation layers, while the classifier learning module focuses on class-level context vectors for correct classification. When N c > 2, multiple matching points exist, and the goal is to maximize the joint probability of all positive instances being correctly identified. The proposed ICE on X involves L2-normalization of feature embeddings before the inner product to determine cosine similarity between feature vectors. The feature norm can be large without L2 normalisation, leading to unstable model training. L2 normalisation projects features to a unit hypersphere, determining semantic similarity based on learned representations' direction. It acts as a regulariser during training, different from hyperspherical learning methods. Feature normalisation is output regularisation and invariant to neural network parametrisation. The learning objective includes implementing a differentiable L2-normalisation layer at the neural net output. Intrinsic sample weighting technique ICE emphasizes harder samples by analyzing gradient magnitudes. It calculates partial derivatives for positive and negative examples to assign higher weights to harder negative samples. Relative weight analysis shows notable differences between positive and negative points within the same anchor. The text discusses the introduction of a scaling parameter to modify the absolute weight non-linearly in a model inspired by (Hinton et al., 2015). It also mentions the batch setting with C classes and N c images from the c-th class, as well as the hyper-setting with the scaling parameter s and the number of iterations \u03c4. The process involves feeding data into a function to obtain feature representations and computing similarities between an anchor. The text introduces a scaling parameter to modify absolute weight non-linearly in a model inspired by (Hinton et al., 2015). It discusses the batch setting with C classes and N c images from the c-th class, as well as the hyper-setting with the scaling parameter s and the number of iterations \u03c4. The process involves feeding data into a function to obtain feature representations and computing similarities between an anchor and the remaining instances. Gradient back-propagation is used to update the parameters of f, aiming to maximize an anchor's matching probability with its positive instance against its negative set. The weights are normalized based on each anchor, with equal contribution from the negative and positive sets. ICE addresses the sample imbalance problem by treating the positive set. It simplifies the weighting schemes for positives and negatives in a naive case scenario where there are two samples per class in every mini-batch. Each anchor has only one positive and many negatives, leading to a need for balancing the recognition of positive examples. ICE addresses the sample imbalance problem by treating the positive set equally with the negative set. It focuses on informative negative instances with higher matching probabilities with a given anchor, using a non-linear transformation to control the relative weight between negative points. The weighting scheme is similar to temperature-based categorical cross entropy. Algorithm 1 summarizes the learning process with ICE, which has the same input data format as CCE. ICE is a flexible method for image matching that maximizes matching probabilities between anchor images and positive samples while considering negative examples. It has a computational complexity of O(N^2) and avoids expensive sample mining and class mining. Data augmentation includes resizing and cropping images, as well as random cropping and mirroring during training. In the test phase, centre cropping is used without horizontal flipping. The embedding size is set to 512 on all datasets. GoogLeNet V2 with batch normalisation is chosen as the backbone architecture, with 512-neuron fully connected layers followed by ICE. The new layers are randomly initialised and optimised with a larger learning rate for faster convergence. The algorithm is implemented in the Caffe framework. The study evaluates a proposed method on fine-grained datasets using the Caffe framework. The evaluation protocol involves testing the generalization and transfer capability of the learned deep metric on image retrieval tasks. The experiments are conducted with specific training settings and GoogLeNet V2 as the backbone architecture. The experiments are conducted on a single PC with specific training settings using SGD optimization. The comparison excludes certain methods and ensemble models for fairness. HTL utilizes GoogLeNet V2 but is deemed unscalable and expensive due to its approach. HTL achieves better performance on small datasets but performs worse on large dataset SOP. Other research topics worth studying include robust distance metric and metric learning with continuous labels. GoogLeNet V2 has three fully connected layers: L for low-level, M for mid-level, and H for high-level. Results are reported for their combination (L, M, H) following RLL. Competitors include Triplet Semihard, Lifted Struct, N-pair-mc, and Struct Clust. Several baseline models, including Lifted Struct, N-pair-mc, Struct Clust, Spectral Clust, Proxy-NCA, RLL, and ICE, are trained and evaluated using the same settings with GoogLeNet V2 as the backbone network. Results from previous studies are re-implemented for fair comparison, and Proxy NCA is noted as not scalable due to learning class-level proxies during training. ICE achieves the best Recall@1 performance compared to state-of-the-art DML losses, with RLL being the closest competitor in some aspects. However, RLL is more complex with three hyperparameters and a different perspective in processing the positive set. ICE achieves a Recall@1 performance of 77.3% on SOP, only 0.9% lower than the best-performing method. ICE offers a clear probability interpretation and aims to maximize joint instance-level matching probability, unlike other approaches. ICE is a successful exploration of softmax regression for learning deep representations in DML. The weight scaling parameter s in ICE impacts the emphasis on difficult examples, with larger s assigning higher weights to them. Experiments on the large dataset SOP show that when s is too small, training does not converge, highlighting the importance of weighting/mining samples. Focusing on difficult samples is beneficial, but the emphasis degree should be properly controlled. The proposed ICE framework for deep metric learning emphasizes the importance of properly controlling the degree of emphasis on difficult samples. It offers a clear probability interpretation, exploits structured semantic similarity information, and is scalable to infinitely many classes. ICE has only one weight scaling hyper-parameter for mining informative examples and does not rely on distance thresholds for achieving intraclass compactness and interclass separability. The ICE framework for deep metric learning does not assume intraclass variances and has general applicability. Batch content evaluation on the SOP dataset shows that increasing the number of classes in a mini-batch improves performance, suggesting that more classes make training more difficult and help the model generalize better. Increasing the number of classes in the batch size also leads to performance growth. Increasing the number of classes in a mini-batch improves performance in deep metric learning. Performance grows as the number of classes increases from 50 to 90, reaching 77.3%. The difficulty of mini-batch training helps the model generalize better. Experimenting with different embedding sizes on SOP dataset shows that smaller sizes result in worse performance, gradually improving as the size increases. Visualizations of learned embeddings are available in Figures 2, 3, 4."
}