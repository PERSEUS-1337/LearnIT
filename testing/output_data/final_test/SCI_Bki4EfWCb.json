{
    "title": "Bki4EfWCb",
    "content": "Amortized inference in variational autoencoders is crucial for efficient approximate inference on large datasets. The quality of posterior inference depends on the variational distribution's ability to model the true posterior and the recognition network's capacity to generalize over all datapoints. Suboptimal inference is often due to amortizing inference rather than the complexity of the approximating distribution. The generator learns to accommodate the choice of approximation, and parameters that increase expressiveness play a role in generalizing inference. Significant work has been done to improve inference in variational autoencoders. In variational autoencoders (VAEs), the quality of posterior inference relies on the variational distribution's ability to model the true posterior and the recognition network's capacity to generalize over all datapoints. Suboptimal inference is often due to amortizing inference rather than the complexity of the approximating distribution. The inference gap in VAEs is analyzed, breaking it down into the approximation gap and the amortization gap. The approximation gap arises from the approximate distribution family's inability to match the true posterior exactly, while the amortization gap is caused by amortizing the variational parameters over the entire training set instead of optimizing for each datapoint independently. Our experiments investigate the impact of encoder, posterior approximation, decoder, and model optimization on the approximation and amortization gaps in VAE inference. We train VAE models on various datasets and demonstrate the importance of using parameterized functions to enhance the expressiveness of the approximation. Using parameterized functions to enhance the expressiveness of the approximation reduces error from amortization. Table 1 summarizes Gap Terms, with the middle column representing the general case and the right column specific to VAEs. VAEs optimize the ELBO of the marginal log-likelihood, which is computationally intractable due to integration over the latent variable z. VAEs use a factorized Gaussian distribution for efficient inference. Amortized inference is achieved through a recognition network. The model is trained by optimizing the ELBO using the reparametrization trick. Strategies like normalizing flows and auxiliary variables enhance the expressiveness of approximate posteriors. Normalizing flow involves transforming probability densities through invertible mappings. The text discusses transforming random variables with distributions to build complex distributions using tractable transformations. The law of the unconscious statistician allows for taking expectations without knowing the formula. The main constraint is the computability of the determinant of the Jacobian. Deep generative models can be enhanced with auxiliary variables for more expressive variational distributions. Hierarchical variational models introduce dependencies between latent variables and data, utilizing auxiliary variables to enhance the variational distribution's expressiveness. The addition of auxiliary variables can lead to a higher lower bound in the model's marginal log-likelihood estimation. This concept has been applied in various works such as ADGM, HVM, and HVI. The IWAE bound provides a tighter lower bound compared to the VAE bound, offering increased flexibility in modeling complex distributions. The IWAE bound, introduced with the Importance Weighted Autoencoder, offers a tighter lower bound on the marginal log-likelihood compared to the VAE bound. It serves as an evaluation metric for generative models and utilizes an importance weighted q distribution. The inference gap decomposes into approximation and amortization gaps, with the distribution maximizing the bound being q*(z|x). The experimentation compares two families of approximate posteriors: fully-factorized Gaussian (FFG) and flexible flow (Flow). The flow model involves a combination of Real NVP and auxiliary variables, resembling leap-frog dynamics in Hamiltonian Monte Carlo. The flow step includes differentiable mappings parameterized by neural nets, allowing for joint training of the generative and flow-based inference model. The experimentation involves jointly training the generative and flow-based inference model using different variational distributions. Multiple transformations can be stacked to enhance expressiveness. Various bounds are used to compute the inference gaps, including IWAE and AIS bounds. The AIS bound is computed using 100 chains with 500 intermediate distributions, each transition consisting of one HMC trajectory with 10 leapfrog steps. The initial distribution for AIS is encoder-independent. When computing DISPLAYFORM0, 5000 samples are used. L VAE [q * ] is computed by optimizing variational distribution parameters for each datapoint. Previous work focused on local optimization of variational parameters for each datapoint. Recent work has applied this idea to improve approximate inference in directed Belief networks. Recent work has also addressed errors in variational learning with inference networks by optimizing approximate inference locally from an initialization output. The researchers propose optimizing approximate inference locally from an initialization output by the inference network to improve training on high-dimensional, sparse data. They focus on reducing the amortization gap and analyzing errors from limited approximating distributions. BID2 visually demonstrate that training with an importance-weighted approximate posterior results in a more complex true posterior compared to fully-factorized Gaussian approximations. This observation is extended quantitatively in the setting of flow-based approximate inference. The researchers visually demonstrate the properties of inference in VAEs by training a VAE with a two-dimensional latent space on MNIST. Contour plots of various distributions in the latent space are shown, including true posteriors, amortized FFG, optimal FFG, and optimal Flow. Posterior A is highlighted as an example where FFG fits well. The researchers visually demonstrate the properties of inference in VAEs by training a VAE with a two-dimensional latent space on MNIST. Contour plots of various distributions in the latent space are shown, including true posteriors, amortized FFG, optimal FFG, and optimal Flow. Posterior A is highlighted as an example where FFG fits well, while Posterior B shows dependence between dimensions, highlighting limitations of factorized approximation. Posterior C demonstrates a shortcoming of amortization with a limited-capacity recognition network, and Posterior D showcases the ability of flexible approximation to fit complex distributions. The comparison raises questions about the leading cause of distribution mismatch in typical VAEs. The study compares the impact of approximation and amortization errors on the total inference gap in VAEs trained on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Results show that for MNIST, both errors contribute equally to the gap, while Fashion-MNIST and CIFAR-10 exhibit larger amortization gaps. The evaluation is based on subsets of datapoints due to computational constraints. The study found that the amortization gap is the main cause of inference suboptimality in VAEs, especially with more challenging datasets. Increasing the encoder capacity reduced the amortization gap, leading to improved inference results. The study focused on reducing the amortization gap in VAEs by increasing the encoder capacity. Different variational distributions were used to train the model, aiming to minimize the approximation gap. Results showed that varying the encoder size and distribution had an impact on the amortization error. The study aimed to reduce the amortization gap in VAEs by increasing encoder capacity and using different variational distributions. Results showed that adjusting the encoder size and distribution impacted the amortization error significantly. The use of q Flow as an approximate distribution led to a decrease in both the approximation and amortization gaps, indicating the importance of parameters in improving the flexibility of the variational approximation. The flow transformations employed were expected to enhance expressiveness and encoder capacity. Increasing the flexibility of variational approximation in models may have improved results by reducing amortization error. It is challenging to visually assess the true posterior distribution, but quantitative methods can estimate the closeness to a fully factorized Gaussian distribution. The Optimal Flow improves upon the Optimal FFG for the FFG trained model by 0.4 nats on MNIST, and by 12.5 nats on the Flow trained model. This suggests that the true posterior of a FFG-trained model is closer to FFG than the true posterior of the Flow-trained model. The same trend is observed on the Fashion-MNIST dataset, indicating that the decoder can learn to have a true posterior that fits better to the approximation. Using a flexible approximate without constraints results in better generative models, supporting the justification of approximation and amortization gap results. The amortization error is a main cause of inference suboptimality, as the generator adjusts to the choice of approximation, reducing the error. Training VAEs with larger decoder capacities decreases the approximation gap, indicating that a more flexible generator requires less flexible approximation. Increased decoder capacity reduces the approximation gap, as shown in Table 5. Training VAEs with larger decoder capacities reduces the approximation gap by adjusting to the choice of approximation. Annealing the entropy of the approximate distribution q during training helps prevent the latent variable from degrading to the prior. This technique, known as warm-up, allows the true posterior to be more complex. Comparing results from models trained with and without entropy annealing schedule shows the importance of warm-up techniques. In this study, the impact of entropy annealing on VAE training was explored. Results show that without entropy annealing, the true posterior is more Gaussian, indicating better utilization of the expressive approximation. The amortization gap was identified as a significant source of inference suboptimality. The study found that the amortization gap is a leading source of inference suboptimality in VAEs. Increasing the capacity of the encoder reduces the amortization error, and optimization techniques like entropy annealing help the generative model utilize the variational distribution better. Future work includes evaluating other expressive approximations and complex likelihood functions. The VAE model uses a decoder with architecture 2-100-784 and an encoder with architecture 784-100-4. It is trained for 3000 epochs with a learning rate of 10^-4 using the ADAM optimizer. MNIST and Fashion-MNIST datasets are used with 60k and 10k datapoints respectively. Images are rescaled to have pixel values in the range [0, 1]. The VAE models for both datasets have the same architecture and use a Bernoulli likelihood for the generator. In the large encoder setting, the number of hidden units for the inference network is changed to 500. The warm-up models are trained with a linear schedule over the first 400 epochs using the exponential linear unit (ELU) activation function. Models are trained with a batch size of 100 using ADAM on the CIFAR-10 dataset with rescaled pixel values. The discretized logistic likelihood model is followed, with a 32-dimensional factorized Gaussian for the latent variable. In the experiment, a 32-dimensional factorized Gaussian is used for q(z|x) following BID14. ELU is chosen as the activation function for neural networks. The network architecture is detailed in Table 9. A decreasing learning rate is adopted with a warm-up applied over the first 20 epochs. Models are trained with a batch size of 100 using ADAM, with early-stopping based on held-out set performance. For increased expressiveness, four flow steps are used. The experiment aims to demonstrate that parameters enhancing approximation expressiveness also reduce amortization error. In the experiment, a factorized Gaussian is used for q(z|x) with ELU activation function. The network architecture details are in Table 9. A decreasing learning rate is applied with a warm-up for the first 20 epochs. Models are trained with a batch size of 100 using ADAM. Four flow steps are used to increase expressiveness. The experiment aims to show that enhancing approximation expressiveness reduces amortization error. Two approximate distributions q(z|x) are compared: fully factorized Gaussian (FFG) and a flow distribution using BID5 transformations. An auxiliary variable is included to avoid selecting how to divide the latent space. The network architecture for CIFAR-10 experiments includes a factorized Gaussian distribution for q(z|x) with ELU activation function. The generator uses MLPs to output channel-wise scales for the discretized logistic likelihood model. Batch-normalization is applied. The overall mapping f consists of two sheer mappings f1 and f2. The experiment aims to demonstrate that increasing approximation expressiveness reduces amortization error. An auxiliary variable is used to avoid dividing the latent space. For local optimization, the mean and variance are initialized as N(0, I) and optimized using Adam with a learning rate of 10^-3. Convergence is determined by comparing the average ELBO values every 100 steps. The Flow model parameters are optimized similarly. Neural nets for the flow are initialized with Xavier initialization. 100 Monte Carlo samples are used to compute the ELBO. Annealed importance sampling is used to compute a lower bound to the marginal log-likelihood. Annealed Importance Sampling (AIS) is a method that computes a lower bound to the marginal log-likelihood by sampling from a proposal distribution and transforming the samples through reversible transitions. This process involves updating weights at each annealing step to estimate the marginal likelihood when integrating with respect to the target distribution. The intermediate distributions are typically defined as geometric averages. Model evaluation with AIS is used in deep belief networks and decoder-based models. The approach was validated using Bidirectional Monte Carlo and showed advantages over the IWAE bound when the inference network overfits. The inference gap in VAEs during training is quantitatively measured to assess how well inference is performed. The true marginal log-likelihood is compared to the lower bound to evaluate the inference process. The training curves for a FFG and Flow inference network are compared using VAE, IWAE, and AIS bounds. The Flow model achieves a higher AIS bound on the test set due to a lower inference gap during training compared to the FFG model. A small inference gap can be achieved even with a limited approximation like a factorized Gaussian, as shown in an experiment with a small dataset. The training curves for a FFG and Flow inference model on a small MNIST dataset are compared, showing that the AIS bound provides the tightest lower bound and is independent of encoder overfitting. The model is overfitting as seen by the decreasing test set bounds, with little difference between FFG and Flow models trained on 1000 datapoints. Separating encoder from decoder overfitting is explained, with the AIS bound used to observe decoder overfitting independently from encoder overfitting. The AIS bound is encoder-independent, allowing observation of decoder overfitting. Encoder overfitting is measured by the difference between AIS and IWAE bounds on the test set. Significant encoder and decoder overfitting is evident in a small dataset, indicating a need for regularization. The Flow model does not significantly impact encoder or decoder overfitting compared to the FFG model. The Gaussian distribution's flexibility lies between FFG and Flow models, with the ability to model interactions. The Gaussian distribution, positioned between FFG and Flow models, can model interactions with covariance. However, its expressiveness is limited due to its unimodal nature and inability to capture higher order interactions. The reparameterization trick involves Cholesky decomposition on the covariance matrix to sample from N(\u00b5, \u03a3). Training VAEs on MNIST and Fashion-MNIST with a Gaussian approximate posterior reveals challenges in local optimization with full covariance inference. Training VAEs with full covariance inference using FFG results in a lower bound, indicating the approximation significantly affects the true posterior. Comparing results on MNIST and Fashion-MNIST datasets, the full-covariance VAE performs similarly to Flow on MNIST and outperforms on Fashion-MNIST in terms of log-likelihood estimation."
}