{
    "title": "r1xYr3C5t7",
    "content": "Multi-label classification (MLC) involves assigning multiple target labels to a sample. Recurrent neural network (RNN) models have been effective for MLC but have limitations in parallel computation and interpretability. A new approach, Message Passing Encoder-Decoder (MPED) Networks, replaces RNNs with message passing mechanisms for faster, accurate, and interpretable MLC. The models are simple, fast, accurate, interpretable, and can be used on structured or unstructured data. Experiments on real-world datasets show promising results. The proposed MPED Networks outperform autoregressive RNN models in multi-label classification tasks, showing significant speedup in training and testing. MLC is crucial for tasks like text categorization and image classification, where accurate and scalable methods are needed. The binary relevance approach is common but ignores label dependencies, which probabilistic classifier chain models aim to address. One notable work in the PCC category implemented a classifier chain using an RNN-based Seq2Seq architecture, Seq2Seq MLC. The main drawback of classifier chain models is their sequential nature, hindering parallelization during training and inference. This can be problematic with a large number of positive labels, requiring beam search for optimal results. PCC methods have time-cost disadvantages and other drawbacks. PCC methods have drawbacks such as requiring a defined label ordering for sequential prediction, struggling with long-range dependencies in dense label cases, and lacking interpretability. Message Passing Neural Networks (MPNNs) BID3 offer an alternative approach. Message Passing Neural Networks (MPNNs) BID3 introduce a method for modeling joint dependencies of variables using neural message passing. They provide a flexible approach for modeling multiple variables jointly without explicit ordering. To address the limitations of BR and PCC methods, a modified version of MPNNs called Message Passing Encoder-Decoder (MPED) Networks is proposed for multi-label classification. The key idea is to use neural message passing to improve speed, accuracy, and interpretability in multi-label predictions. The proposed MPED networks aim to replace RNNs with neural message passing for multi-label classification, allowing for more parallelization in training and testing. The main contributions include a novel approach for MLC, achieving accurate results across various metrics, and demonstrating faster speeds compared to previous models. The attention-based MPED models offer a transparent way to explain label dependencies, input relationships, and feature connections. Message Passing Neural Networks (MPNNs) generalize graph neural networks (GNNs) by using message passing for efficient inference, updating node states based on messages from neighboring nodes over multiple time steps. Neural message passing with attention allows nodes in a graph to learn different weights for their neighborhoods without prior knowledge of the graph structure. This method involves passing messages using neural attention, where nodes can attend over their neighborhoods differentially. The weights for neighboring nodes are obtained through attention, enabling a weighted sum calculation for message passing. This approach, also known as graph attention, is used interchangeably with neural message passing in the paper. Neural message passing with attention involves using attention coefficients to determine the importance of neighboring nodes in a graph. Messages are passed between nodes using learned attention weights and transformation matrices, with weight sharing across nodes to learn node dependencies in an order-invariant manner. In MPNNs, node dependencies are learned in an order-invariant manner. Notations define data samples, inputs x, outputs y, input features as embedded vectors, and labels as embedded vectors in MLC. The goal is to predict binary labels based on input features and joint probabilities of labels. The Message Passing Encoder-Decoder (MPED) networks aim to achieve the performance of explicit joint probability methods like PCCs, while maintaining the test speed of BR methods. MPED networks use three MPNN modules with attention to pass messages within encoder and decoder graphs for joint label prediction. The Message Passing Encoder-Decoder (MPED) networks utilize three MPNN modules to pass messages within encoder and decoder graphs for joint label prediction. The MPNN modules update input and label nodes by passing messages, followed by a readout function to make binary classification predictions for each label. The Message Passing Encoder-Decoder (MPED) networks use MPNN modules to pass messages within encoder and decoder graphs for joint label prediction. Nodes on G ENC are represented as embedding vectors, and nodes on G DEC are updated by passing messages from the encoder to the decoder. The Message Passing Encoder-Decoder (MPED) networks utilize MPNN modules to pass messages within encoder and decoder graphs for joint label prediction. MPNN xy is used to pass messages from input embeddings to label embeddings, while MPNN yy is used to pass messages between label embeddings. The decoder updates label nodes using MPNN xy, passing messages from input to labels. Messages are directed from encoder nodes to decoder nodes, with weights learned via attention for importance of input components to label nodes. The key advantage of input-to-label message passing with attention is that each label node can attend to different input nodes. The decoder can make independent predictions for each label conditioned on x. Interactions between label nodes are modeled using message passing with a third module, MPNN yy. Label embeddings are updated by a weighted combination through attention of neighbor label nodes. In MPED networks, label embeddings are updated using MPNN for T time steps to predict each label's output vector. The model uses binary mean cross entropy for training and approximates label predictions through message passing. Multi-head Attention is employed to enable specific interactions between label nodes. MPED utilizes multiple attention heads for each matrix during message computation to allow nodes to attend to multiple others simultaneously. T time steps of embedding updates are computed to learn complex relations among nodes, resulting in a stack of MPNN layers. The joint probability of labels is not explicitly estimated, enabling parallel predictions and reducing test time significantly. The MPED model enables parallel predictions by implicitly modeling joint label probabilities using the MPED decoder, leading to a substantial speedup in test time. It removes dependencies on label ordering and beam search, making it beneficial for dense label predictions. MPED networks predict all labels at once, maintaining label dependencies through label to label attention. Additionally, 'soft' predictions for each label in Multi-Label Classification are supported, addressing a major drawback of other models. MPED models offer flexibility in handling various input and output structures, including unknown relational structures like text corpora or multi-label classification labels. They provide interpretability through neural attention, allowing visualization of input dependencies, input/label dependencies, and label dependencies. The use of graph attention in MPED models is linked to structured output predictions. The use of graph attention in MPED models is closely connected to structured output prediction for MLC. Previous research utilized conditional random fields to model dependencies among labels and features for MLC. Our method simplifies predictions in one step using feed-forward blocks with attention mechanisms on embeddings. We plan to enhance MPED models by incorporating a structured loss formulation. The proposed models in the current text chunk focus on using graph neural networks and embedding models for structured components. These models perform nonlinear function mappings to learn feature representations and pass embeddings from node to neighbor nodes. Various works have extended the basic GNN framework with different message passing, update, and readout functions. However, none of these have applied GNNs to multi-label classification (MLC). In section 5, the text discusses comparing MPED variations to baseline models in multi-label classification. MPED outperforms or achieves similar results as baseline models across seven real-world datasets with varying characteristics. Autoregressive models are shown to not be crucial for most metrics in MLC. In multi-label classification, autoregressive models are not essential for most metrics. MPED outperforms autoregressive models in miF1 and maF1 across various datasets, especially in predicting rare labels. Autoregressive models with beam search often make incorrect predictions on rare labels. MPED is a reliable choice across all metrics. MPED is a solid choice for multi-label classification, outperforming autoregressive models in miF1 and maF1 metrics. It does not explicitly model label dependencies but shows indications of learning them through attention weights. MPED is faster than autoregressive models during testing due to parallelization capabilities. MPED is faster than autoregressive models during training and testing, with speedups of 1.7x and 5.0x, respectively. It produces similar or better metric scores compared to baseline models across 7 datasets. Visualizations of attention weights show positive results for MPED in multi-label classification. The state-of-the-art probabilistic MLC method, RNN Seq2Seq, achieves speedups over the baseline model. The attention weights in the MPED model demonstrate relationships between input-label pairs and label-label pairs. Future work includes adding a structured prediction loss function to improve attention mechanisms. MPED Networks use neural message passing to model label dependencies in MLC tasks and handle various input data types effectively. Our method demonstrates effectiveness in handling various input data types and output label structures in multi-label sequence classification tasks. Future extensions include adapting the model to predict more dynamic outputs. The method is tested against baseline methods on seven different datasets with varying characteristics. For some datasets, a portion of the training set is used for validation. Additionally, for the TFBS dataset, a convolutional layer is used to extract \"words\" from DNA characters. In deep learning models for DNA, positional encoding is added to word embeddings for datasets with sequential ordering. Different encoding methods are used based on the input data type, such as bag-of-words or graph structures. The model is validated on seven MLC datasets with varying input data types. The model parameters differ for the SIDER dataset due to its smaller size. Training is done on an NVIDIA platform. The model parameters for training on DNA datasets include T=1 time step, d=64, and K=4 attention heads. Training is conducted on an NVIDIA TITAN X Pascal with a batch size of 32 using Adam BID31 optimizer. Dropout of p=0.2 is applied, and layer normalization is used. Autoregressive models predict positive labels before outputting a stop signal, following the PCC+ approach. Non-autoregressive models use beam search with a beam size of 5 at inference time. The best threshold for converting labels to {0, 1} is chosen based on validation set performance. Various measures like Subset Accuracy (ACC), Hamming Accuracy (HA), and Example-based F1 (ebF1) are used to evaluate the prediction performance. Label-based measures treat each label as a separate prediction problem. MLC methods in various domains like text, images, and bioinformatics can be categorized into different groups. One such model is the Label Powerset (LP) model, which classifies inputs into one label combination from all possible combinations. LP explicitly predicts the subset of positive labels, with the label set growing exponentially in the total number of labels. High maF1 scores indicate good performance on less frequent labels, while high miF1 scores indicate good performance on more frequent labels. The Label Powerset (LP) model classifies inputs into one label combination from all possible combinations, but faces challenges with subset scarcity and bad generalization. Binary relevance (BR) methods predict each label independently, while Probabilistic classifier chain (PCC) methods estimate the true joint probability of output labels using the chain rule, predicting one label at a time. PCC models have issues with slow inference for large L and errors. To address issues with PCC models, a solution is to predict only positive labels in the LP subset, known as PCC+. This approach reduces prediction steps when the number of possible labels is large. In both PCC and PCC+, inference is done using beam search, a costly dynamic programming step. In machine translation, Seq2Seq models use an encoder RNN to read the source language sentence and a decoder RNN to predict the target sentence. BID7 improved this model with \"neural attention\" allowing the decoder to focus on every encoder word during translation. Recent studies have shown that using a recurrent neural network (RNN) based encoder-to-decoder can achieve state-of-the-art results in machine translation. Using a Seq2Seq RNN model, MLC results were achieved by predicting positive labels sequentially until a 'stop' signal. The Transformer in BID8 eliminated the need for recurrent networks in machine translation by explicitly modeling pairwise dependencies among features using attention. Autoregressive models, effective for MLC and machine translation, require sequential predictions, limiting parallelization. Beam search is commonly used for optimal predictions in autoregressive models, but it is limited by large beam sizes leading to time constraints. Initial wrong predictions can propagate with modest beam sizes, affecting performance, especially with a high number of positive labels. Autoregressive models are suitable for machine translation due to their sequential decoding process, but for MLC, output labels lack intrinsic properties. The decoding process in machine translation involves sequential decoding, but for Multi-Label Classification (MLC), output labels do not have a natural order. Autoregressive models are commonly used in machine translation, but a non-autoregressive approach like the Non-Autoregressive Transformer can provide stable predictions by making proxy predictions called \"fertilities\". The model uses 3 encoder and decoder time steps with node-to-node attention. The Non-Autoregressive Transformer model uses 3 encoder and decoder time steps with node-to-node attention and K=4 attention heads."
}