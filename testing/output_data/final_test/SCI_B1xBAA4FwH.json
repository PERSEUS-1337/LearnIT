{
    "title": "B1xBAA4FwH",
    "content": "A suite of multifaceted metrics is proposed by the Explainable Artificial Intelligence (XAI) community to objectively compare explainers based on correctness, consistency, and confidence of generated explanations. These metrics are computationally inexpensive, applicable across different data modalities, and evaluated on common explainers like Grad-CAM, SmoothGrad, LIME, and Integrated Gradients. The experiments demonstrate that the metrics align with qualitative observations from previous works in the field of deep learning. The XAI community has proposed algorithms to explain predictions of state-of-the-art models used in various tasks, highlighting their fragility, susceptibility to small perturbations, and biases. This has led to a need for interpretable black-box models. In response to the need for interpretable black-box models, a suite of metrics is proposed to evaluate various interpretability methods. These metrics aim to address drawbacks such as computational cost, domain limitations, and narrow focus on specific attributes of explainers. Unlike previous work, the proposed metrics consider correctness, consistency, and confidence across different data modalities. The proposed metrics evaluate and compare popular explainers like LIME, Grad-CAM, SmoothGrad, and Integrated Gradients on an Inception-V3 model without human intervention. The metrics focus on correctness, consistency, and confidence, offering a cost-effective way to assess explainers. Grad-CAM performs well but has some drawbacks not previously noted. The field of eXplainable AI (XAI) is actively researched with efforts to explain AI models using local or global explanations. Research is also focused on evaluating and comparing explainers, with frameworks introduced to assess predictive accuracy, descriptive accuracy, and relevancy. Notably, different studies have outlined desired characteristics for explainers. Recent efforts in the field of eXplainable AI (XAI) have focused on evaluating and comparing explanation techniques. Hall et al. (2019) identified desired characteristics for explainers, emphasizing effectiveness, versatility, constraints, and objective explanations. However, some techniques like DeConvNet, Guided BackProp, and LRP have been found to produce incorrect explanations for linear models. To address this, PatternNet and PatternAttribution were proposed as theoretically sound methods for linear models. Other studies have evaluated saliency methods and found them unreliable for tasks sensitive to data or model variations. Recent studies have evaluated various interpretability methods, including SmoothGrad, to determine the accuracy of feature attributions. Yang & Kim (2019) introduced three metrics to assess explainers, focusing on false-positives. Alvarez-Melis & Jaakkola (2018) proposed an alternative set of metrics for evaluation. In evaluating interpretability methods, Alvarez-Melis & Jaakkola (2018) introduced metrics focusing on explicitness, faithfulness, and stability of explanations. Yeh et al. (2019) defined fidelity of explanations in capturing model changes. Objective metrics are used to assess explainers, considering correctness, consistency, and confidence. The text discusses the importance of transformations that preserve semantic information in input data, the use of explainers to generate explanations for classifier predictions, and the function that computes the difference between generated explanations. It also emphasizes the need for classifiers with acceptable test-set performance and minimal degradation under transformations. The text elaborates on the components of a proposed evaluation framework for explainers in image classification scenarios. It mentions the importance of reliable explanations from well-performing classifiers and the ability to combine components for a comprehensive evaluation. The weight for each component depends on the use case and end-user preference, which is not discussed further in the current work. The text proposes a novel method for evaluating explainers in image classification scenarios. It aims to address the limitations of existing metrics by considering both the identification of relevant components and the avoidance of selecting non-important components. The text discusses the challenges of using simple masking to improve accuracy in image classification explainers. It highlights issues such as empty pixels in masked images and the importance of context in visual tasks. The study empirically shows that vanilla masking leads to performance deterioration. The study emphasizes the importance of realistic backgrounds for extracted patches in visual tasks to properly evaluate them. A proposed procedure involves pairing top and bottom images from each class in the dataset and overlaying important regions from high confidence images onto low confidence images. The study pairs top and bottom images from each class, overlaying important regions from high confidence images onto low confidence images to create a dataset of masked images. The masking operation involves a threshold function and element-wise multiplication. Accuracy is measured on this masked dataset to evaluate if the explainer captures important pixels. To ensure non-important pixels are not selected, the process is repeated using inverted saliency maps. The study evaluates the explainer's ability to capture important pixels by overlaying regions from high confidence images onto low confidence images. Accuracy is measured on masked images, with a focus on excluding non-important pixels using inverted saliency maps. The consistency of the explainer in capturing relevant components under various transformations is also assessed. In this work, the focus is on Semantically Invariant Transforms, which are a subset of transformations that do not alter the semantic information in the input. These transforms include not only simple perturbations in a small neighborhood but also more complex changes. The goal is to assess the consistency of the explainer in capturing important components under different transformations. The focus is on Semantically Invariant Transforms, which include various transformations like affine transformations, flips, and scaling. Evaluating the explainer's performance across larger input spaces is crucial. The Euclidean distance metric may not be robust for images, as it only considers pixel-wise intensity differences. Dynamic Time Warping (DTW) is preferred over Euclidean distance for images due to its ability to compute distances between misaligned or out-of-phase time series. FastDTW, an approximation of DTW with linear complexity, is used to calculate distances between pairs of explanations. Confidence in generated explanations and masked inputs leading to high confidence predictions is crucial for downstream use. Generating high-confidence predictions is crucial for downstream use, as it builds trust with human users and enables explanations to be useful for individual instances. This concept is extended from max-margin classifiers to explainers, emphasizing the importance of explanations that result in high confidence predictions. To ensure high-confidence predictions, the difference in probability and entropy of softmax distributions is computed using normal and inverted saliency maps. Coarse explainers may skew results, so the average number of pixels in the generated explanations is also reported. The study compares different explanation methods like LIME, Grad-CAM, SmoothGrad, and Integrated Gradients using an Inception v3 architecture. Sparsity and confidence metrics are evaluated, with considerations for different preferences in computational biology domains. The experiments were implemented in PyTorch and metrics were computed over 5 runs. The study compares various explanation methods using an Inception v3 architecture. Correctness and confidence metrics are computed over 5 runs, with results reported in tables 6 and 7. Semantically invariant transforms like translations, rotations, and flips are considered, with no significant drops in classifier accuracy observed. Noise in the image domain is not included in the experiments. The study compares different explanation methods using an Inception v3 model. Baseline accuracy results are provided, with Grad-CAM showing the highest increase. However, results from LIME and Grad-CAM with inverse masking contradict the hypothesis. Grad-CAM generates larger explanations, potentially misidentifying non-important pixels. This leads to confusion for the classifier when computing inverse masks. LIME generates small explanations with the smallest accuracy gain compared to other explainers. The study evaluates the sensitivity of correctness to the number of top and bottom images used for masking. Changes in accuracy for different values of k are shown in Fig. 2 and detailed in Tables 4 and 5. For both normal masking and inverse masking, LIME shows smaller decreases in accuracy compared to other methods as k increases. This is because LIME does not capture all important pixels, leading to less informative replacements. Grad-CAM, on the other hand, shows a stable and minimal drop in accuracy. Grad-CAM produces small inverse maps, leading to stable accuracy drops. FastDTW distances increase with more drastic transformations, showing Grad-CAM outperforms other methods. The study compares different explainers for image classification, with Grad-CAM performing the best. Confidence is measured using changes in probability and entropy. Trends in confidence align with performance rankings. The study compared different explainers for image classification, with Grad-CAM outperforming others in accuracy and confidence. LIME showed unexpected behaviors in inverse masking due to the patchiness of its explanations. The paper introduced metrics to evaluate explainers objectively and compared well-known explainers like LIME, Grad-CAM, Integrated Gradients, and SmoothGrad. Our experiments compared well-known explainers like LIME, Grad-CAM, Integrated Gradients, and SmoothGrad on an Inception-V3 model. Grad-CAM performed better overall but struggled with inverse masking. LIME performed poorly in all situations. Combining metrics can hide anomalies, so users should check individual metrics before making a decision. The XAI community is advised against using one-size-fits-all metrics as they can hide important trade-offs. Researchers are encouraged to test the proposed metrics on different explainers, datasets, classifiers, and data types. There is potential for developing explainers that optimize for these metrics and integrating them into self-explaining models."
}