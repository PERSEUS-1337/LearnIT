{
    "title": "SkgHtkrYPH",
    "content": "In recent years, new deep learning architectures and building blocks have been developed to improve inference efficiency. Examples include the squeeze-and-excitation module, depthwise separable convolutions, and the inverted bottleneck. These building blocks have not only increased efficiency but also accuracy, leading to widespread adoption. This work introduces sparse counterparts to dense primitives in neural network architectures to further enhance efficiency by reducing parameter count. While the concept of sparsity for decreasing parameters is not new, the focus here is on replacing dense primitives with sparse ones. In recent years, new deep learning architectures and building blocks have been developed to improve inference efficiency. Examples include the squeeze-and-excitation module, depthwise separable convolutions, and the inverted bottleneck. This work introduces efficient sparse kernels for various hardware platforms, showing that sparse versions of MobileNet v1 and MobileNet v2 outperform dense baselines on the efficiency-accuracy curve. On Snapdragon 835, sparse networks outperform dense equivalents by 1.1\u22122.2x, demonstrating significant improvement in efficiency. Efficient architectures like MobileNets have been designed to run on mobile devices while maintaining high accuracy. Weight sparsity can lead to improved efficiency in these architectures, with 1x1 convolutions playing a key role in computational complexity. In this work, fast kernels for Sparse Matrix-Dense Matrix Multiplication (SpMM) are introduced to accelerate sparse neural networks with a focus on the sparsity range of 70-95%. The SpMM kernel outperforms both the Intel MKL and the TACO compiler. Using fast kernels for Sparse Matrix-Dense Matrix Multiplication (SpMM), the study demonstrates the effectiveness of weight sparsity in MobileNet architectures, leading to significant improvements in efficiency. These new efficient CNN models reduce inference times, parameter counts, and FLOPs compared to previous generations. Recent advancements in CNN efficiency have been driven by the need to reduce inference costs, with a focus on improving accuracy while minimizing model parameter counts, FLOPs, and memory requirements. The MobileNet family of architectures has been successful in enhancing efficiency through depthwise separable convolutions, leading to significant improvements in inference speed and model size reduction. The MobileNet family of architectures has focused on improving efficiency through depthwise separable convolutions, which are responsible for the parameter efficiency of these models. MobileNet v1 used 1 \u00d7 1 convolutions followed by depthwise convolutions, while MobileNet v2 introduced the inverted residual block. Depthwise convolutions account for a small fraction of the total FLOPs, parameters, and inference time in these models. A different line of work focused on making CNNs more efficient by directly pruning weights of full convolutional filters, but struggled to accelerate 1 \u00d7 1 convolutions. Channel pruning approaches have been preferred due to the difficulty of accelerating sparse computation. These approaches prune entire filters, leaving the final model dense, and function more as an architecture search over channel counts. Neural Architecture Search has also been applied to architectures like MBv2, resulting in MobileNet v3, FBNet, and EfficientNet. In MobileNet v3, FBNet, and EfficientNet, factorizations of 1\u00d71 convolutions have been explored. ShuffleNet and Learnable Butterfly Factorizations also utilize factorizations in their weight matrices. Work in Text-to-Speech (TTS) showed that increasing sparsity in RNN models can improve model quality. Additionally, fast block-sparse matrix-vector (SpMV) multiplication routines were demonstrated for RNN inference. In investigating efficient convolutional models like MBv1, MBv2, and EfficientNet, the performance of SpMM kernels is analyzed. Sparse networks are trained on ImageNet using gradual magnitude pruning. The first dense convolution in the networks is not pruned, as its impact on parameters, FLOPs, and runtime is minimal. Instead, a dense convolutional kernel is implemented to convert input from HWC to CHW layout for sparse operators. In investigating efficient convolutional models like MBv1, MBv2, and EfficientNet, the performance of SpMM kernels is analyzed. The values for different channels in HWC layout are adjacent in memory, while in CHW layout, values of all spatial locations for one channel are adjacent. Pruning the squeeze-excitation blocks in EfficientNet contributes <1% of total FLOPs. The last fully-connected layer contributes insignificantly to FLOPs but a significant fraction of total parameters. Visualization of memory reads and writes in the algorithm shows loading spatial locations simultaneously for non-zero weights and performing calculations for output channels. After analyzing efficient convolutional models like MBv1, MBv2, and EfficientNet, it is found that introducing sparsity benefits performance. In sparse EfficientNet models, the number of filters in the last convolution is doubled from 1280 to 2560. The fully-connected layer can also be made sparse without loss of accuracy. Activation tensors must be stored in CHW format for the scheme to work effectively. The SpMM convolution is illustrated in figure 2. The high performance of kernels is enabled by sparse weight matrix and dense activation matrix. Processing matrix in the right order keeps values in L1 cache for fast access. Utilizing structure in weight matrix improves data reuse. Constraining sparsity pattern creates 'blocks' in weight matrix for performance boosts. The weight matrix pattern creates 'blocks' for data reuse in output channel dimension. ARM kernels implemented in C with NEON intrinsics for efficiency. Library available for running sparse models trained with TensorFlow model pruning. The text discusses the conversion of dense representation to a BCSR-like representation for inference, providing various kernels for high performance, including depthwise convolutions and global average pooling. Results for MBv1 and MBv2 models are presented, with EfficientNets following similar trends. Performance results for SpMM kernels are revealed, along with how networks respond to sparsity to find models with the lowest inference time. Ruy is used as the current TensorFlow Lite ARM64 backend. The text discusses the conversion of dense representation to a BCSR-like representation for inference, providing various kernels for high performance. Results for MBv1 and MBv2 models are presented, with EfficientNets following similar trends. Performance results for SpMM kernels are revealed, along with how networks respond to sparsity to find models with the lowest inference time. Ruy is used as the current TensorFlow Lite ARM64 backend. The FLOPs achieved at each layer in the model are plotted in figure 4, showing that sparse kernels perform 40-90% of dense kernels' raw performance. The effective FLOPs range from 2-7\u00d7 due to sparsity in the layer. In MBv1, performance drops in the last layers due to exceeding L1 cache size. MBv2 shows a sawtooth pattern with higher performance in expand kernels. An AVX-512 version outperforms Intel MKL in most layers with a speedup of 1.20. Hyper-parameters for MBv1 and MBv2 are listed in table 2. The dense models were found using a grid search with specific hyper-parameters to match or exceed reported accuracies with fewer training iterations. Sparsity was induced in MBv1 and MBv2 starting at iteration 7,000 and stopping at 28,000, while for EfficientNet it started at iteration 23,000 and ended at 105,000. Training was done on the ImageNet dataset. The study induced sparsity in MBv1 and MBv2 models with a pruning frequency of 2,000 during training on the ImageNet dataset. The effect of block size on accuracy was analyzed, showing a tradeoff between the number of elements in each block and not their configuration. Larger width models exhibited a slight decrease in accuracy loss due to blocking. The efficiency of changing layers to block size 4 over unstructured models was also evaluated. The study analyzed the efficiency of changing layers to block size 4 over unstructured models in MBv1 and MBv2 models. Results showed that increasing sparsity levels improved efficiency, with EfficientNet exceeding all other models in both FLOP and parameter efficiency. The study focused on designing models with the best accuracy vs. inference time trade-offs by making specific assumptions to reduce the search space. By considering only block size 1 and block size 4 variants and inducing the same level of sparsity in all 1 \u00d7 1 convolutions, the researchers conducted a search at width multiplier 1.4 over N models. The model with the highest ratio of inference time reduction relative to a fully unstructured model and top-1 accuracy loss was chosen for further training at all widths. The study focused on designing efficient models by selecting the model with the highest ratio of inference time reduction and top-1 accuracy loss. Sparse convolutional networks were found to be more accurate than dense ones for a constant computational budget. Sparse RNNs are more accurate than dense RNNs for floating-point operations. Weight sparsity accelerates convolutional networks on ARM processors, outperforming dense networks by 1.1 \u2212 2.2\u00d7 on Snapdragon 835. The code for the 4x1 kernel is provided, along with hyper-parameters for MBv1 and MBv2 training. Learning rates are specified in a reduced space and then multiplied by a factor of 16 due to batch size. Plots for EfficientNet scaling with sparsity are presented. EfficientNet shows less improvement as sparsity increases, with block size being more crucial than block configuration."
}