{
    "title": "ByMVTsR5KQ",
    "content": "WaveGAN is a new approach using GANs for unsupervised synthesis of raw-waveform audio, capable of generating coherent one-second audio slices. It can produce intelligible words without labels and synthesize audio from various domains like drums, bird vocalizations, and piano. Comparisons with other methods show WaveGAN to be promising for audio generation. Generative Adversarial Networks (GANs) are a promising approach for synthesizing audio for specific domains, aiding in creative sound design for music and film. Musicians and Foley artists currently rely on large sound effect databases, but a more efficient method would involve exploring a compact latent space of audio to find desired sounds and making small adjustments to finetune them. However, generating audio signals with high temporal resolution requires strategies that can effectively operate in high dimensions. Generative Adversarial Networks (GANs) are a potential tool for audio synthesis, offering advantages such as data augmentation for speech recognition and rapid sampling of audio. While GANs have shown success in synthesizing images, their capability in generating audio remains unproven. Operating GANs on spectrograms is a possible solution for audio synthesis. In the generative setting, operating image recognition algorithms on audio spectrograms is common. However, this approach is problematic as the most informed spectrograms are non-invertible. Recent work has shown that neural networks can be trained with autoregression to operate on raw audio, eliminating the need for engineered feature representations. In this work, the authors investigate waveform and spectrogram strategies for generating one-second audio slices with GANs. They introduce SpecGAN for spectrogram representation and WaveGAN for waveform generation, providing a practical approach for audio synthesis with GANs. The method is envisioned for creating short sound effects for music and film, such as training a WaveGAN on drums to create a procedural drum machine. In this study, WaveGAN and SpecGAN were trained on drums to create a procedural drum machine for electronic musicians. Human evaluation for domain-specific tasks would require expert listeners, so a speech benchmark was also considered for easier assessment. The focus was on generating spoken digits \"zero\" through \"nine\" to test unsupervised learning strategies on speech data. Both WaveGAN and SpecGAN were able to generate intelligible spoken digits, showing success in learning global structure in audio signals without conditioning. WaveGAN outperforms SpecGAN in generating audio with better sound quality and speaker diversity. GANs learn mappings from latent vectors to natural data points in a minimax game between a generator and discriminator. The training algorithm aims to minimize the Jensen-Shannon divergence between the data distribution and the generator's implicit distribution. The original formulation of GANs involves the data distribution P_X and the generator's implicit distribution P_G. GANs are difficult to train and prone to failure cases. BID1 proposes minimizing the Wasserstein-1 distance instead of Jensen-Shannon divergence. They suggest using a GAN training algorithm (WGAN) to minimize this distance. The first eight principal components for 5x5 patches from natural images and audio slices show different patterns. The transposed convolution operation is depicted for DCGAN and WaveGAN generators. The DCGAN and WaveGAN generators use different filter sizes and upsampling factors. WaveGAN is motivated by the unique structure of audio data compared to images, as shown by principal component analysis. Audio signals have periodic components that decompose the signal into frequency bands. The WaveNet architecture uses dilated convolutions to increase the model's receptive field for processing raw audio signals. WaveGAN, based on DCGAN, modifies the transposed convolution operation to upsample low-resolution feature maps into high-resolution audio signals. WaveGAN modifies the transposed convolution operation to widen its receptive field by using longer one-dimensional filters and upsampling by a factor of 4 at each layer. The discriminator is also modified with length-25 filters and increased stride. These changes result in WaveGAN having the same parameters and output dimensionality as DCGAN, but with the ability to generate audio samples equivalent to slightly more than one second of audio at 16 kHz. WaveGAN modifies the DCGAN method by flattening 2D convolutions into 1D, increasing stride factor, removing batch normalization, and training using WGAN-GP strategy. This results in WaveGAN generating audio samples equivalent to slightly more than a minute at 16 kHz. Phase shuffle operation in Wave-GAN discriminator perturbs phase of feature maps by Uniform \u223c [\u2212n, n] samples, filling in missing samples by reflection. Checkerboard artifacts in images are less common in audio, but discriminator can learn to reject them. Trivial policy can be learned to reject generated examples with artifact frequencies occurring at a particular phase. Phase shuffle operation with hyperparameter n is proposed to prevent the discriminator from learning a trivial policy to reject generated examples. It randomly perturbs the phase of each layer's activations before input to the next layer, making the discriminator's job more challenging. This approach is applied only to the discriminator, as the latent vector already allows the generator to manipulate the phase of the waveform. SpecGAN is a frequency-domain audio generation model that uses a spectrogram representation suitable for GANs. It processes audio with short-time Fourier transform, scales amplitude logarithmically, and normalizes frequency bins. This preprocessing is common in audio processing. The SpecGAN model preprocesses audio data by normalizing frequency bins and clipping spectra to 3 standard deviations before using the DCGAN algorithm to generate spectrograms. The spectrograms are then converted back to waveforms using the Griffin-Lim algorithm for human evaluation on the Speech Commands Dataset. The Speech Commands Zero Through Nine (SC09) dataset contains recordings of spoken digits zero through nine, with each word having 1850 utterances in the training set. The dataset is challenging for generative modeling due to its high dimensionality and diverse recording conditions. The dataset for generative modeling is challenging. Different configurations of WaveGAN are compared, including one with phase shuffle and another using nearest-neighbor upsampling. WGAN-GP algorithm is used for experiments, showing reasonable results. Performance is also compared to SpecGAN and tested on other datasets like drum sound effects and bird vocalizations. The dataset for generative modeling is challenging. WaveGAN networks converge within four days for SC09 dataset and produce speech-like audio within the first hour of training. SpecGAN networks converge more quickly, within two days. Training WaveGAN for 200k iterations on other datasets takes nearly 1500 epochs. Generation with WaveGAN is fully parallel. Generation with WaveGAN is fully parallel and can produce an hour of audio in less than two seconds. Evaluation of generative models is complex, with quantitative measures of sample quality showing poor correlation with human judgement. Various evaluation metrics are used for validation, including the inception score which correlates well with human judgement. The inception score is a measure used for evaluating generative models, calculated by training an audio classifier on SC09. The classifier processes input audio using a short-time Fourier transform and convolution layers, projecting the result to a softmax layer with 10 classes. The score ranges from 1 to n for n classes, maximizing when the model is confident and predicts each label equally. It is used as a primary quantitative evaluation method and early stopping criteria. The classifier achieves 93% accuracy on the test set by projecting input audio to a softmax layer with 10 classes. Inception score can have two failure cases, one being a generative model outputting examples with uniform probability, and the other overfitting the training data. Two indicators are used to detect these cases. The average Euclidean distance of examples to their nearest neighbor is measured to assess diversity. Distances are evaluated in the frequency-domain representation. Another indicator measures distance to the nearest neighbor in the real training data. Human annotators on Amazon Mechanical Turk are used to label the generated audio. Amazon Mechanical Turk is used to label the generated audio. The best WaveGAN and SpecGAN models are used to generate 300 examples for each digit, totaling 3000. Annotators are asked to label the perceived digit in batches of ten examples, with accuracy compared to classifier labels. Annotators also assign subjective values for sound quality, intelligibility, and speaker diversity. Results are reported in TAB0, including accuracy and mean opinion scores. Evaluation is also done on real training data, real test data, and SC09 generated by a speech synthesizer. Comparison is made with SampleRNN and WaveNet implementations. The autoregressive models WaveNet and WaveGAN were tested for small-vocabulary speech data. The best WaveGAN model achieved an inception score of 4.7 using phase shuffle with n = 2. Comparisons with other regularizers like dropout were made, showing phase shuffle's impact on the inception score. SpecGAN achieved a higher inception score (6.0 vs. 4.7) and better human labeling accuracy compared to WaveGAN. However, WaveGAN was preferred by humans for sound quality and speaker diversity. SpecGAN captures variance in the data but deviates from real data statistics. SpecGAN shows potential in capturing data variance but faces sound quality issues when inverting spectrograms to audio. The poor ratings may be due to the Griffin-Lim inversion process. Both WaveGAN and SpecGAN have promise in audio generation, with WaveGAN producing visually consistent frequency-domain spectra. Further research is recommended for spectrogram generation methods. In the context of audio generation, text-to-speech systems can be either concatenative or parametric. Concatenative systems sequence small portions of pre-recorded speech, while parametric systems map text to speech parameters synthesized by a vocoder. Recent research explores end-to-end neural network approaches for parametric speech synthesis. Neural network approaches learn to produce vocoder features directly from text or phonetic embeddings. These features are synthesized to raw audio using methods like WORLD and Griffin-Lim, or trained neural vocoders. Some methods explore unsupervised generation of raw audio, such as WaveNet by van den Oord et al. GANs are also used in combination with these approaches. WaveGAN is the first application of GANs to unsupervised audio generation, allowing for the creation of hours of audio in seconds. It can be used for sound design in multimedia production and future work aims to expand its capabilities to handle variable-length audio and explore different label conditioning strategies. Post-processing filters are used to eliminate noise frequencies, while boosting signal in speech. The generative procedure creates filters for speech and bird vocalizations in WaveGAN. While image-generating GANs can reject artifacts, audio generation faces challenges with checkerboard artifacts that can be devastating to results. In WaveGAN, periodic distortions are perceived as an abrasive tone. The impulse response is measured by initializing it randomly 1000 times and passing unit impulses to the first convolutional layer. The frequency domain plot shows sharp peaks at linear multiples of sample rates. Strategies to mitigate these artifacts include adding a post-processing filter with a long window to filter out undesirable frequencies. In WaveGAN, a post-processing filter with a long window is used to boost signal in specific frequency regions and introduce notches to remove artifacts. Transposed convolution upsamples signals by inserting zeros and applying a learned filterbank, introducing aliased frequencies that can be crucial for producing fine-grained details in the output. In WaveGAN, different upsampling strategies were experimented with, including nearest-neighbor, linear, and cubic interpolation. Nearest neighbor upsampling produced similar audio output to transposed convolution, while linear and cubic interpolation resulted in poor audio quality. The aliased frequencies generated by upsampling convolutions are believed to be more important for audio generation than image generation. WaveGAN and SpecGAN models were developed for sound effect generation, which is different from text to speech tasks. Autoregressive models like WaveNet and SampleRNN were considered for waveform generation but failed to produce cohesive words and had weak inception scores. The models took longer to generate waveforms and did not learn a compact latent space, limiting tasks like continuous exploration and interpolation. Autoregressive models like WaveNet and SampleRNN failed to synthesize coherent words and had weak inception scores. These models took over 24 hours to generate even a thousand examples, while our methods produce 50k examples in seconds. Autoregressive waveform methods have not shown the ability to generate words without rich linguistic features. The study excluded poor scores from the results table to avoid sending the wrong message. Implementations like WaveGAN produced synthetic bird vocalizations that intrigued the researchers, suggesting the method could potentially convince non-human animals. In TAB2 and TAB4, the architectures for WaveGAN generator and discriminator, as well as SpecGAN, are listed. The hyperparameters for the experiments, including batch size and model size modifications, are detailed in TAB5. Adam optimizer with specific parameters is used in all experiments."
}