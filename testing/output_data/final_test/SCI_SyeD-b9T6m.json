{
    "title": "SyeD-b9T6m",
    "content": "A measure of consistency in tagging scientific papers is proposed, where tags are analyzed for their predictiveness in citation graph links. An algorithm is presented to calculate consistency using human- and machine-generated tags. Augmentation of manual tags with machine-generated ones improves consistency. Cross-consistency is introduced to predict citation links between papers tagged by different methods. Cross-consistency can evaluate tagging quality between different taggers, such as manual and machine methods. The limited amount of labeled data hinders the construction of knowledge graphs, which involves analyzing publications and adding tags using natural language processing or machine learning methods. Evaluating tagging quality involves comparing machine-produced tags to human-produced tags in a \"golden set\" of papers. However, human tagging is expensive and inconsistent, posing challenges for this approach. The challenges of tagging papers with a large tagging dictionary like MeSH and UMLS are exacerbated by the need for domain experts to choose the right tags. This presents difficulties for multidisciplinary research papers that may require the efforts of multiple qualified taggers. Additionally, evaluating tag augmentation poses challenges when using human-tagged papers as a benchmark. The evaluation of tag augmentation involves adding machine-generated tags to papers tagged by humans to improve search functions. A measure of tagging quality, inspired by graph embeddings, is proposed to assess tagging engines continuously ingesting and tagging fresh publications. The tags can predict graph edges in a citation graph, where papers are represented as nodes. Good tags should be able to predict links on the graph, showing consistency in tagging similar papers. To improve tagging accuracy, cross-consistency with a known \"good\" tagger can be calculated by manually tagging some papers and using machine-generated tags to predict citation links. This method expands the labeled papers for evaluating machine-based taggers and increases the data available for training and testing. A consistent tagging system should predict citation links based on the publication time of papers. The paper discusses an algorithm to calculate consistency and presents experimental results. The algorithm involves selecting seed papers and labeling random papers from their reference list. The algorithm selects seed papers and labels random papers from their reference list with positive or negative samples based on publication dates. It calculates tags for each sample and measures consistency through ROC curve analysis. The algorithm calculates consistency of tagging by measuring average AUC and its variation. It uses Algorithm 1 for cross-consistency of taggers, selecting different sources of tags for seed papers and samples from PubMed database. Gene names and diseases are identified using GNAT BID4 and DNORM BID6, respectively. The algorithm calculates consistency of tagging by measuring average AUC and its variation. It selects random papers from the reference list of seed papers and labels them as positive or negative samples. AUC is calculated for the classification problem, and the coverage of papers by different tagging sources is shown. The number of seed papers chosen was 100, and specific hyperparameters were selected for good convergence of the measure. The hyperparameters chosen for good convergence included k=10 and m=2. Date granularity was set to year-month. A baseline was created by randomizing tag sets. AUC was calculated for each seed paper, with results shown in Tukey plots. Consistency of tagging sources was displayed in Figure 2. Figure 2 displays consistency measures for tagging sources including DNORM, GNAT, MANUAL, NEJI, and combined automatic taggers. Figure 3 explores adding machine-generated tags to manual ones, while Figure 4 shows cross-consistency between manual tags and NEJI-generated ones. The experiment used different sources for tagging seed papers and samples, revealing a clear difference in consistency between random and real tags. At AUC = 0.5, outliers exist above and below this value. Real tags consistently perform above AUC = 0.5, except for low coverage sources like GNAT. Adding machine-generated tags improves consistency, as shown in Figures 3 and 4. Cross-consistency between manual and machine-generated tags is explored. Cross-consistency between different taggers can be used to estimate their similarity, especially when some taggers (e.g. manual tagging) are too expensive to run on a large set of papers. A simple measure of consistency of tagging can be informative about the tagging process and can be used to assess and evaluate it. The cross-consistency between manual tags and NEJI generated ones is discussed."
}