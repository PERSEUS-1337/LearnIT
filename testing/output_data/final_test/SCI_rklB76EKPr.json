{
    "title": "rklB76EKPr",
    "content": "Gradient clipping is a technique used in deep network training to control the dynamics of iterates, enhancing convergence to a local minimum. Recent studies show that suitable clipping can lead to faster convergence. This paper introduces a new perspective on gradient clipping, focusing on robustness to noise. Standard gradient clipping may not provide robustness to label noise in classification, but a simple variant can offer provable robustness by modifying the loss function. In this paper, a new perspective on gradient clipping is introduced, emphasizing robustness to noise in deep network training. The study investigates whether gradient clipping can address label noise in classification, but findings show that it alone does not provide robustness to even simple models. The study introduces a new perspective on gradient clipping for deep network training, focusing on robustness to label noise in classification. It is shown that gradient clipping alone does not provide robustness to simple models. A composite loss-based gradient clipping approach is proposed, which preserves classification calibration and is robust to label noise. Empirical verification on synthetic and real-world datasets demonstrates the effectiveness of partially Huberised versions of standard losses in the presence of label noise. The study introduces composite loss-based gradient clipping for deep network training, focusing on robustness to label noise in classification. It compares standard and composite loss-based gradient clipping using the logistic loss for binary classification. Huberised losses are found to be not robust to label noise, while partially Huberised losses are. The study introduces composite loss-based gradient clipping for deep network training, focusing on robustness to label noise in classification. It compares standard and composite loss-based gradient clipping using the logistic loss for binary classification. Gradient clipping is discussed in the context of supervised learning tasks, where the quality of a model is measured by a loss function. The use of clipped gradient descent is related to normalised gradient descent (NGD). The study discusses gradient clipping in deep network training for robustness to label noise in classification. It compares standard and composite loss-based gradient clipping using logistic loss for binary classification. Gradient clipping is related to normalised gradient descent (NGD) and has been shown to accelerate convergence over gradient descent. The misclassification risk R(f) is minimized using a margin loss function \u03c6(y \u00b7 f), which is classification calibrated if it drives excess risk to zero. The hinge loss \u03c6(z) = [1 \u2212 z]+ is a canonical example. A loss function \u03c6 is admissible if it is bounded, strictly convex, continuously differentiable, non-increasing, and classification calibrated. A proper composite loss function can be interpreted as probabilities, composed of a base loss \u03d5 and an invertible link function F. The curr_chunk discusses losses composed of a base loss \u03d5 and an invertible link function F, such as the logistic loss and softmax cross-entropy loss. It also touches on classification under label noise, where labels are corrupted with a fixed probability. This problem has a history in statistics and is of recent interest in machine learning. The curr_chunk discusses the problem of learning under symmetric label noise and the ineffectiveness of gradient clipping in providing robustness to label noise. It introduces a framework for analyzing class-conditional label noise and highlights the equivalence of stochastic gradient clipping with linear models to modifying the underlying loss. This modified loss is related to a Huberised loss and loss-based gradient clipping. The curr_chunk discusses the limitations of L-gradient clipping in the presence of label noise in binary classification problems. It introduces a modified loss function that replaces the original loss with a linear function when the derivative exceeds a certain threshold. This modification aims to improve robustness to label noise in stochastic gradient descent with linear scorers. The curr_chunk discusses gradient clipping in the context of robust regression, specifically focusing on the instance-dependent margin gradient and its relation to the Huber loss. It highlights the use of Huber-style losses through a variant of gradient clipping and introduces loss-based gradient clipping (L-gradient clipping) as a method to clip only the contribution arising from the loss. The curr_chunk discusses L-gradient clipping, focusing on bounding the loss derivative and using a Huberised version of the loss function. It introduces a method to clip only the contribution arising from the loss, emphasizing the use of Huber-style losses in gradient clipping. The curr_chunk discusses the use of Huberised losses in classification and their label noise robustness. It explores whether these losses maintain classification calibration and preserve class-probabilities, even in the absence of noise. The curr_chunk discusses the Huberised loss in classification, its composite nature, and the impact of L-gradient clipping on label noise robustness. It highlights the classification calibration of the Huberised loss and its non-invertible link function for small \u03c4 values. The study also questions whether gradient clipping enhances robustness to label noise. The Huberised loss in classification is not robust to label noise, even with convex losses like softmax cross-entropy. Proposition 4 shows that for any margin loss \u03c6 and \u03c4 > 0, there exists a distribution where the optimal linear classifier under \u03c6 \u03c4 is equivalent to random guessing under symmetric noise. This highlights the susceptibility of Huber loss to outliers in regression problems. The text discusses the vulnerability of the Huber loss to outliers in regression problems and label noise in classification. It suggests using CL-gradient clipping as a method to achieve label noise robustness by partially Huberising the base loss for composite losses. In the context of label noise robustness, CL-gradient clipping partially Huberises the base loss for composite losses, ensuring Lipschitz continuity. The partially Huberised loss method optimizes a new loss by linearizing the base loss while retaining the link function. It behaves like the link beyond a certain threshold and is bounded. This approach can be applied to a multi-class setting and is beneficial for classification tasks. The partially Huberised loss method optimizes a new loss by linearizing the base loss while retaining the link function. It behaves like the link beyond a certain threshold and is bounded. This approach can be applied to a multi-class setting and is beneficial for classification tasks. Clipping is always benign from a classification perspective, and provided \u03c4 is sufficiently large, from a probability estimation perspective as well. By exploiting the equivalence of CL-gradient clipping to a partially Huberised loss, it is shown that partially Huberised losses have an advantage over Huberised losses under symmetric label noise. This implies that label noise cannot have an excessively deleterious influence on the loss. The partially Huberised log loss method provides robustness to label corruption and outliers in feature space. It is related to a family of losses studied in various works and interpolates between log and linear losses. This method offers advantages over Huberised losses under symmetric label noise. The partially Huberised log loss method interpolates between log and linear losses, allowing the choice of a different link function. It guarantees Lipschitz continuity and considers sample informativeness in loss gradients. The partially Huberised loss method softens the influence of label noise by enforcing a hard cap on its impact, contrasting with a truncated loss that discards poorly-predicted instances. The analysis assumes symmetric label noise, with potential challenges in dealing with asymmetric or instance-dependent noise. Proposition 4 presents a distribution that challenges the Huberised loss under linear models, but in practice, more benign distributions and nonlinear models may not succumb as severely to label noise. The aim of the paper is to show that a simple modification of clipping can avoid worst-case degradation without adding complexity. The types of gradient clipping considered in the study are summarized in Table 1 for binary classification problems involving a parametrised scoring function and differentiable composite margin loss. The text discusses different types of gradient clipping for binary classification problems, focusing on how clipping affects the loss gradients for each sample in a minibatch. It compares L-gradient clipping, CL-gradient clipping, and robustness guarantees under symmetric label noise. The study aims to show that a simple modification of clipping can prevent worst-case degradation without complexity. The text discusses using partially Huberised loss with minibatch gradient clipping to achieve both robustness and optimization benefits. Setting a hyperparameter \u03c4 is necessary for partially Huberised losses, balancing noise-robustness and gradient informativeness. Experiments show that partially Huberised losses perform well on real-world datasets with label noise. The experiments involve synthetic datasets with label noise. A linear classifier is trained using different losses, including logistic, Huberised, and partially Huberised versions. Results show that partially Huberised loss performs well under noise, while logistic and Huberised losses struggle. This confirms the effectiveness of CL-gradient clipping in noisy scenarios. In a 1D setting with synthetic datasets and label noise, different losses are compared for training a linear classifier. Partial Huberisation loss performs well under noise, while logistic and Huberised losses struggle. CL-gradient clipping is effective in noisy scenarios. The study compares different losses for training a linear classifier in a 1D setting with synthetic datasets and label noise. Partial Huberisation loss performs well under noise, while logistic and Huberised losses struggle. Real-world datasets show that partially Huberised losses perform well with deep neural networks trained on MNIST, CIFAR-10, and CIFAR-100. In 2016, SGD with momentum, weight decay, batch normalization, and different batch sizes were used for training. Training labels were corrupted with noise at varying probabilities. Test set accuracy of different losses combined with softmax link was compared, including cross-entropy, linear loss, and generalised cross-entropy. Global gradient clipping and partial Huberisation were also assessed. For each dataset, tuning parameter \u03c4 is chosen to maximize accuracy on noisy samples with noise rate \u03c1 = 0.6. Tuning \u03c4 separately for each noise rate setting can improve performance. Partial Huberisation balances noise robustness and gradient informativeness. In noise-free cases, all methods perform similarly, but with noise injection, differences arise. When noise is introduced, the accuracy of the CE method significantly decreases. Gradient clipping can offer some improvements under high noise levels, but it is not as effective as other losses with robustness guarantees. The linear loss, known for its robustness to symmetric noise, performs well even at \u03c1 = 0.6, but optimization is more challenging due to the lack of accounting for instance importance. The GCE and partially Huberised losses show better performance, especially on the CIFAR-100 dataset, where the linear loss struggles even without noise. Partially Huberised losses are competitive and often outperform their counterparts, with the partially Huberised CE showing significant improvement under high noise levels. The partially Huberised loss performs better than the CE under high noise, and the partially Huberised GCE improves GCE numbers on CIFAR-100. It can be combined with other base losses to handle noise effectively. Gradient clipping alone is not sufficient for label noise robustness, but a composite loss-based gradient clipping method shows promising results on noisy datasets. Further exploration is needed to analyze the behavior of gradient-clipping inspired losses for more general problems. The behavior of gradient-clipping inspired losses for distributionally robust learning is analyzed. The normalised gradient is computed using an admissible and decreasing function. The clipped gradient corresponds to the gradient under a \"Huberised\" loss function. Proof of Lemma 2 shows the relationship between the Huberised loss and the clipped function. The Huberised loss \u03c6 \u03c4 is convex, differentiable, and decreasing. It must be classification calibrated. The minimizer of the conditional risk is shown in Figure 3. The Huberised logistic loss is strictly monotone and continuous.\u03c6 is strictly convex. The quantity of interest is \u03c6 \u03c4, which is invertible when \u03c4 \u2265 \u2212\u03c6 (0). The loss function \u03c6 \u03c4 must be convex, non-increasing, continuously differentiable, and asymptote to zero. This is satisfied by the assumption of \u03c6 being admissible. The proof of Lemma 5 shows that CL-gradient clipping is equivalent to using the loss \u03c6 \u03c4. Lemma 6 establishes that \u03c6 is invertible under certain conditions, ensuring the properness of the loss function. The loss function \u03c6 \u03c4 is proper composite, with a unique minimiser z * for the conditional risk. The minimising score's sign indicates the dominance of the positive class-probability, making the loss classification calibrated. The risk on the noisy distribution is a scaled version of the risk on the clean distribution, with an additional term that can be bounded. The symmetry condition must be satisfied for the additional term to be constant. The loss function is proper composite, with a unique minimiser for the conditional risk, indicating the dominance of the positive class-probability. The minimizers of the clean and noisy risks can be found with constants C1, C2. Proper losses are essential for class-probability estimation tasks, where it is optimal to predict the positive class-probability. Real-valued scores are preferred for losses, such as those from a neural network's final layer pre-activation. Proper composite losses with symmetric links can be extended to multiclass settings. The Huberised, partially Huberised, and generalised cross-entropy losses are illustrated with varying tuning parameters. The link functions in each loss may be non-invertible if the tuning parameter is too large. The Huberised and partially Huberised losses with varying tuning parameters have link functions that may become non-invertible if the tuning parameter \u03c4 is too large. The Huberised loss saturates more slowly as \u03c4 is decreased, while the partially Huberised loss has an implicit link function. The linear loss for \u03c4 = 1 is not suitable for class-probability estimation. The link function becomes non-invertible for \u03c4 \u2265 2 in the partially Huberised loss, requiring rescaling for interpreting output probabilities. The base \u03d5 \u03b1 loss composition with a sigmoid link function is illustrated in Figure 8. The partially Huberised loss ensures that the model is not influenced by arbitrarily large values of x. Proposition 8 states that for any convex, differentiable margin loss function, the derivative of the loss approaches a finite value as x approaches infinity. This is achieved by the saturating behavior of the Huberised loss. The synthetic data used in the study includes a distribution concentrated on six atoms in a two-dimensional dataset. Instances are labeled based on specific criteria, and the distribution is modified to include isotropic Gaussians. Additionally, outliers in the feature space are considered in the experiment. The data in feature space consists of points on a line, with positively labeled samples from a Gaussian centered at (1, 1) and outliers at (\u2212200, 1). Negatively labeled samples are the negation of all points. An unregularised linear classifier is learned from this data, represented by a single scalar \u03b8. A 2D dataset with 500 points falling into two bands is used to illustrate differences among methods. Decision boundaries for various losses are shown when trained with a linear model using explicit quadratic features. Logistic and generalised cross-entropy losses show changes in decision boundaries with 45% symmetric label noise, while the partially Huberised loss maintains the correct classification boundary. The data comprises six points, with blue points as positive and red points as negative. The two \"fat\" points have twice the probability mass of their \"thin\" counterparts. Minimizing convex losses with a linear model under label noise results in random guessing. Adding 45% symmetric label noise changes the logistic loss boundary, but the partially Huberised loss maintains the correct classification boundary."
}