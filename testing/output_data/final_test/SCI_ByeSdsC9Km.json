{
    "title": "ByeSdsC9Km",
    "content": "The paper introduces APL, an algorithm that approximates probability distributions by recalling past surprising observations from an external memory module. It can generalize beyond direct recall and perform well on few-shot classification benchmarks with a smaller memory footprint. APL's memory compression allows it to scale to thousands of unknown labels and excel in a more challenging meta-learning reasoning task. The algorithm APL can learn and generalize behaviors quickly with few context observations, memorize past experiences for adaptation, and display cognitive abilities in a sequential decision problem involving digits and unknown symbols. The algorithm APL can quickly adapt and recall past experiences for reasoning tasks, representing a new approach to statistical inference. Unlike traditional neural networks, meta-learning systems learn an algorithm that infers functions directly from observations at test time, allowing for better generalization to unseen distributions. The curr_chunk discusses the challenges of storing information in models beyond small toy examples, especially in tasks with higher dimensional data or real-world problems. Current methods involve summarizing past experiences in lower-dimensional representations or using memory modules. However, these approaches have limitations in the amount of information that can be encoded. Neural memory modules face challenges in storing and retaining relevant experiences. To achieve successful task completion, a model must capture information about numerous symbols without redundancy. Reasoning involves processing recalled experiences to apply information to current data points. While simple cases like classification can use recalled memories for interpolation, more complex reasoning is needed for human-level generalization. The paper introduces Approximate Posterior Learning (APL) as a solution. In this paper, Approximate Posterior Learning (APL) is introduced as a model that addresses challenges in few-shot approximation of new probability distributions. APL learns to store minimal context points for tasks and processes recalled experiences for varying complexities. Inspired by Bayesian posterior updating, APL shows comparable accuracy to state-of-the-art algorithms in few-shot classification benchmarks with higher data efficiency. It can scale to a larger number of classes while maintaining good performance and is applied to reasoning tasks successfully. The main contributions of this paper include a simple memory controller design using a surprise-based signal, an integrated external and working memory architecture, and a training setup for learning algorithms efficiently. The proposed model aims to approximate the posterior without backpropagating through the entire data sequence in an episode. Our proposed model consists of an encoder, external memory store, and decoder for generating a probability distribution over targets. The encoder converts incoming query data to a lower-dimensional representation, typically using a convolutional network architecture. The model aims to approximate the posterior without backpropagating through the entire data sequence in an episode. The external memory module in our model stores experiences and is queried to find the k-nearest neighbors for classification. The memory contains columns for attributes like embedding and true label, with rows representing individual data points. Euclidean distance is used to calculate proximity between points. The model does not backpropagate through the memory module. In the model, euclidean distance is used to find neighbors in the memory module for classification. The encoder aims to cluster class-discriminative embeddings together. A simple memory controller minimizes data points stored. Surprise, defined as -ln(y t), helps in storing minimal data by predicting true class probabilities. The memory controller in the model decides to store data points based on their level of surprise, with a threshold determined by a hyperparameter \u03c3. If a data point is 'surprising' (i.e., prediction confidence is lower than a uniform prediction), it is stored in memory. The model's performance remains robust to variations in \u03c3 after training. The model's memory controller stores data points based on surprise levels determined by a hyperparameter \u03c3. The decoder utilizes a relational feed-forward module with self-attention, external memory architecture, and two other decoder architectures for classification problems. The decoder architectures under consideration include a relational self-attention feed-forward decoder, a relational working memory decoder, and an LSTM decoder. Each architecture has specific modules for processing neighbor embeddings and label embeddings. The system updates its beliefs online through training episodes consisting of pairs (x t , y t) presented in a shuffled manner. The model's memory is empty at the start of each episode, with predictions made sequentially and losses minimized through gradient updates. Losses are used to determine memory writes, with cross entropy loss used for classification tasks. APL learns a sequential update algorithm to minimize cross-entropy loss independently at each time step. The model is encouraged to read information from queried neighbors rather than just fitting the current episode's label mapping. Meta-learning covers various areas and is not tied to a specific task. Meta-learning algorithms, such as those based on autoregressive predictions or nearest neighbors approach, have been successfully applied to various challenges like few-shot classification and scene understanding. Autoregressive models have shown state-of-the-art results in tasks like supervised learning, but their reliance on full context history hinders parallelization. Nearest neighbors methods use an encoder to find suitable embeddings for memory lookups, resulting in a weighted average of representations. Nearest neighbors methods use an encoder to find embeddings for memory lookups, resulting in a weighted average of labels. Meta Networks utilize external memory for learning from previous examples, achieving state-of-the-art performance. Conditional neural processes summarize data into a fixed representation for efficient computation but may not scale to large problems. Memory augmented neural networks (MANN) like BID20 use a controller to write into a neural dictionary, but backpropagating through the entire sequence for learning is computationally expensive. Our approach introduces a simpler writing mechanism and a relational decoder to exploit nearest neighbor structure. The Omniglot dataset consists of 1623 characters with 20 examples each, split into train and test sets. The Omniglot dataset consists of 1623 characters split into train and test sets. Examples are presented to the model in batches of 16, with N classes chosen for each episode. The model's accuracy and number of memories written to memory are time-dependent. The architecture includes four convolutional blocks with 3x3 convolutions, relu, and batch normalization. Classes in the training set are augmented by rotating symbols. All three decoder architectures perform similarly for this task. In Figure 4a, the algorithm's performance saturates after seeing more examples in a single episode. Fewer than 2 examples per class are enough for 5-way Omniglot classification. Figure 4b shows the evolution of the posterior distribution in 20-way classification. Initially, a uniform distribution is output, which refines as more examples are added to memory. Figure 4c illustrates different numbers of... In Figure 4c, different numbers of examples are stored in memory for each class, optimizing memory usage based on classification difficulty. The model stops writing to memory after 2 examples per class for 5-way classification. The evolution of the posterior distribution in 20-way classification shows a convergence to the correct class as more items are added to memory. In 20-way classification, the model converges as more items are stored in memory. The number of labels stored per class is varied, with APL achieving high accuracy by storing 44 items. For 1000-way classification, accuracy improves with more items written to memory. Comparing model accuracy to baselines with fixed context sizes shows comparable performance for 1 and 5-shot classification. In experiments testing the model's performance, it achieved comparable results to state-of-the-art models without extensive hyperparameter tuning. The model was tested on 423-way and 1000-way classifications, as well as on the MNIST dataset, showing promising accuracy levels. Additionally, when applied to the full-scale ImageNet dataset, the model demonstrated strong performance without held-out classes for testing. The dataset used for testing did not have held out classes, so labels were shuffled among 1000 Imagenet classes. The pretrained Inception-ResNet-v2 BID25 was used as an encoder due to computational constraints. The model achieved 80.4% accuracy in the fixed label case and 86.7% accuracy in a 20-way classification challenge. Performance remained high with 100-way classification at 72.9% accuracy. Further investigation into training the encoder end-to-end is left for future work. The model's performance remains high for 100-way classification (72.9% top-1 accuracy) but degrades for 1000-way classification (52.6% top-1 accuracy) with shuffled classes. Large scale meta-learning on real world datasets is challenging even when all classes are observed. The number analogy task challenges the model to generalize with logic reasoning using minimal examples. Two levels of difficulty are tested: fixed number values with 10 different symbols, and shuffled values for both digits and symbols. When both numbers and symbols are shuffled, the model requires logical deduction to infer the correct symbols. The model can generalize using 50 examples written to memory, which is fewer than seeing one of all 100 possible combinations. In this complex case, APL reaches 97.6% accuracy on the test set. The model can generalize using 50 examples written to memory, which is fewer than seeing one of all 100 possible combinations. Once a symbol's meaning is figured out, it is no longer necessary to solve the system of equations for that unknown. The decoder can infer symbol and number meanings for better performance in 1-shot classifications. The relational self-attention feed-forward module can generalize better from few examples than other decoder architectures. The model introduced a self-contained system that can learn to approximate a probability distribution with minimal data and quickly. It includes a training setup that encourages adaptation, an external memory for recalling past events, a writing system for uncertain situations, and a working memory architecture for efficient comparison. The model achieves state-of-the-art accuracy with a smaller memory footprint, scales to large problems using an external memory module, and performs well in 1-shot generalization through relational reasoning. In all experiments, the same training setup is used where elements are sampled from N classes and shuffled to create training batches. Gradient descent with Adam optimizer is applied for each batch, with the learning rate annealed from 10^-4 to 10^-5 over 1000 steps. The query data is passed through an embedding network and used to query an external memory module with multiple columns of data. The memory size ensures no overwriting in a single episode. The memory contents and query are fed to a decoder architecture for processing. The query data is processed by a decoder architecture after being encoded with a convolutional network for omniglot images. The encoder used is a pretrained Inception-ResNet-v2 network with standard preprocessing. The decoders have a hidden dimensionality of 512 and generate final logits for classification using Cross Entropy loss. The encoder for MNIST uses the same convolutional network as described in the Omniglot section. Symbols are one-hot vectors for the experiments. The memory is queried for neighbors of digit embeddings and symbols, concatenated, and fed to the decoder. The classification target is the one-shot encoded version of X + S, where X is the digit value and S is the symbol value. The tensor is processed through a relational self-attentional block with multihead attention and a shared nonlinear layer. This process is repeated 8 times in a residual manner. Distances between neighbors and query are passed through a softmax layer to generate an attention vector, which is multiplied with the activations tensor. The final representation is obtained by summing over the first axis. A Relational working memory core is used for initialization. The memory core is initialized with concatenated vectors and the query is fed multiple times to the relational memory core. A standard LSTM module is used with the neighbor embedding and label concatenated at each time step. The final output state is used as input to the logits linear layer. All three decoder architectures perform equally well for classification. The Relational self-attention feed forward module works best for the analogy task. The performance of APL is affected by the choice of parameter \u03c3. The choice of parameter \u03c3 affects the performance of APL, with memory size and accuracy remaining largely unchanged for a large range of values. The memory storage mechanism is self-regulating, leading to a flat number of elements in memory. In a 200-way classification scenario, a \u03c3 value around 5.2 seems close to optimal for accuracy vs. elements in memory. Increasing the value too much leads to performance degradation, while a value of 0 results in the model being unable to write to memory. In the few-shot learning setting, APL algorithm can also be used in continual learning setups. Each task involves learning 10 new classes with 200 unique examples, drawn from the test set. A new logits layer is added for each task on top of a convolutional encoder pretrained on the omniglot training set. APL performance is compared to progressive networks in figure 12, showing comparable or better results. APL can achieve similar or better performance than progressive networks in lifelong learning tasks without using gradient information. The memory store of APL provides the necessary information to update predictions for new tasks, as shown in Figure 12 on a task involving learning 10 new classes in Omniglot."
}