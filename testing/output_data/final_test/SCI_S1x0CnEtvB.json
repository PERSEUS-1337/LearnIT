{
    "title": "S1x0CnEtvB",
    "content": "Depth is a crucial factor in Deep Neural Networks (DNNs) design, but it is challenging and requires human effort. AutoGrow is proposed to automate depth discovery in DNNs by growing new layers if it improves accuracy, otherwise stops growing to determine the optimal depth. The method is efficient and generalizes well across different network architectures and datasets, consistently discovering near-optimal depth on various datasets. The success of Deep Neural Networks (DNNs) relies heavily on the depth of the network models. While shallow networks lack accuracy, overly deep networks can face over-fitting and training convergence issues. Determining the optimal depth for a DNN remains a challenge, often requiring heuristic trials. For example, ResNet-152 uses varying numbers of residual blocks at different output sizes without a clear quantitative relation. Researchers typically design a network with a specific depth, train and evaluate it on a dataset, then adjust the depth through iterative testing to find the best configuration. AutoGrow is proposed to automate depth discovery in DNNs, eliminating the need for manual trial and error iterations. Previous works focused on adding or morphing layers to increase depth, while AutoGrow aims to find the optimal depth automatically. AutoGrow automates depth discovery in DNNs by dynamically adding new layers without predefined limits. It starts from a shallow backbone network and gradually grows sub-modules until a stopping policy is met. Surprisingly, a random initializer is as effective as Network Morphism, and growing before a shallow net converges is more efficient. The AutoGrow algorithm dynamically adds new layers to DNNs without predefined limits. It avoids full convergence during growth by using random initialization, a large learning rate, and a short growing interval. The algorithm fine-tunes the network for a specified number of epochs to achieve a trained neural network with learned depth. Figure 1 provides an overview of AutoGrow, which describes the architecture hierarchy using networks, sub-networks, sub-modules, and layers. A network consists of a cascade of sub-networks, each composed of sub-modules with the same output size. Sub-modules, like a residual block, are elementary growing blocks made up of one or a few layers. The generic version of AutoGrow is rigorously formulated in this section, with a deep convolutional network being a cascade of sub-networks. Each sub-network operates on an input image or feature tensor, with parameters for each sub-module within the sub-network. The AutoGrow algorithm supports various popular networks like VggNet, GoogLeNet, ResNets, and DenseNets. It starts with a shallow net and grows by adding sub-modules for spatial dimension reduction. The algorithm is described in Algorithm 1, looping over all growing steps. AutoGrow algorithm adds sub-modules for spatial dimension reduction to popular networks like VggNet, GoogLeNet, ResNets, and DenseNets. It loops over growing sub-networks, stacking new sub-modules until accuracy no longer improves. Our method details will be explained in the following subsections, focusing on growing depth for four types of DNNs. AutoGrow algorithm enhances popular networks like VggNet, GoogLeNet, ResNets, and DenseNets by adding sub-modules for spatial dimension reduction. It automatically learns the number of sub-modules starting from a seed architecture with only one sub-module per output spatial size. The method involves stacking new sub-modules in sub-networks until accuracy plateaus. Network Morphism changes DNN architecture while preserving the loss function through special initialization of new layers. Residual sub-modules have a unique property where initializing the last Batch Normalization layer as zeros preserves the function of the shallower net but morphs the DNN into a deeper net. This zero initialization (ZeroInit) allows for easy implementation of Network Morphism. Additionally, a new AdamInit method is proposed for Network Morphism initialization. In AdamInit, parameters are frozen except for the last Batch Normalization layer in W, and Adam optimizer is used to optimize it for up to 10 epochs. This process, viewed as Network Morphism, results in similar training loss. AdamInit typically converges in less than 3 epochs. Random initialization of the last Batch Normalization layer using Gaussian noise (GauInit) yields the best results, challenging the concept of Network Morphism. The growing policy in Algorithm 1 determines when a network should expand a sub-module. The growing policy in AutoGrow determines when a network should expand a sub-module. Two growing policies are studied: Convergent Growth and Periodic Growth. Convergent Growth only grows when the network has converged, while Periodic Growth always grows every K epochs. Experiments show that Periodic Growth outperforms Convergent Growth, suggesting that a fully converged shallower net may not be an adequate initialization for training a deeper net. The stopping policy in AutoGrow determines when to expand a sub-module based on validation accuracy improvement. Hyper-parameters \u03c4, J, and K control the operation and can be easily set up. \u03c4 represents the significance of accuracy improvement, J determines how many epochs to wait for improvement, and K sets the period for growth. In AutoGrow, hyper-parameters \u03c4, J, and K control the stopping policy for sub-module expansion based on validation accuracy improvement. J represents epochs to wait for improvement, K sets growth period. Model architecture denoted as Basic3ResNet-2-3-2 contains sub-modules in each network. AutoGrow starts from the shallowest. AutoGrow uses a 2-3-2 structure and maximizes validation accuracy to guide layer growth. DNN baselines are trained with SGD and different learning rates on ImageNet and smaller datasets. Early experiments with Network Morphism failed to find optimal depth. AutoGrow uses a 2-3-2 structure to guide layer growth based on validation accuracy. Network Morphism experiments failed to find optimal depth, leading to the hypothesis that a converged shallow net gives a bad initialization for training a deeper neural network. The effectiveness of AutoGrow is demonstrated through experiments in Sections 3.1, 3.2, 3.3, and 3.4, showcasing the integration of Network Morphism under Convergent Growth. The process involves training a shallower ResNet to converge, morphing to a deeper net by stacking residual blocks, initializing new layers with ZeroInit or AdamInit, and training the deeper net in a standard way. The accuracy difference between Network Morphism and training the deeper net from scratch is compared in Table 2. Network Morphism experiments showed lower accuracy compared to training a deeper net from scratch, confirming the hypothesis that a converged shallow network provides a poor initialization for deeper net training. Integration of Network Morphism in c-AutoGrow with Convergent Growth policy involves training a shallower net to convergence, initializing new layers with Network Morphism, and repeating the process for improved accuracy. In every interval of K training epochs, \"staircase\" learning rate is used with c-AutoGrow. The learning rate is reset to 0.1 at the first epoch and decayed by 0.1\u00d7 at epoch Table 3. However, there are issues with c-AutoGrow: final accuracy is lower than training from scratch, and depth learning stops too early. This suggests that a converged shallow net with Network Morphism gives a bad initialization, hindering further growth signals. After proposing three solutions to enhance AutoGrow in Section 3.1, p-AutoGrow was introduced with a large constant learning rate, random initialization, and Periodic Growth. These modifications aim to prevent full convergence into a bad initialization and allow for rapid growth before a shallower net converges. p-AutoGrow is the final iteration of AutoGrow, as discussed in the following section through an ablation study. In an ablation study, c-AutoGrow is incrementally enhanced with a constant learning rate and random initializer, resulting in p-AutoGrow with improved accuracy. The combination of constant learning rate and GauInit performs the best, and these settings are adopted for future experiments. The focus is on automating depth discovery to achieve a high accuracy with minimal depth in the final DNN. AutoGrow aims to find a \"found net\" with minimum depth for high accuracy. The metric \"\u2206\" indicates how well shallower nets initialize deeper nets. The final solution, p-AutoGrow, incorporates Periodic Growth. Results show a preference for faster growth with a smaller period K. Our results validate the preference for faster growth with a smaller period K. Accuracy plateaus at K = 3 on CIFAR10/CIFAR100, and further reducing K only marginally improves accuracy. p-AutoGrow outperforms c-AutoGrow by finding deeper nets, overcoming early stop issues. Ablation study on initializers in p-AutoGrow confirms the effectiveness of GauInit. Network Morphism aims to start deeper nets from well-optimized shallower nets to avoid restarting training from scratch. Our results challenge the necessity of Network Morphism when growing neural networks. Ablation study shows that a shallowest DNN works as well as a deeper seed, implying AutoGrow can appropriately stop regardless of the seed network's depth. The focus of this work is on depth automation. AutoGrow focuses on depth automation by starting with the shallowest seed network to avoid manual search. It verifies adaptability using an identical configuration and tests over multiple datasets and seed architectures. AutoGrow discovers layer depth across scenarios without tuning, achieving the main goal. For ResNets, it identifies a near-optimal depth where accuracy saturates, with final accuracy comparable to training the discovered network from scratch. AutoGrow enables the training of very deep plain networks, surpassing baselines even with Batch Normalization. By tuning K, AutoGrow discovers smaller DNNs with improved accuracy, showing potential for better accuracy-depth trade-offs. AutoGrow can find a better accuracy-depth trade-off by tuning K, but for generalizability, K = 3 is used. AutoGrow adapts to different depths based on sub-module design and dataset size, showing potential for improved accuracy with better architectures. Experiments confirm that AutoGrow can discover shallower networks for smaller datasets. AutoGrow explores different values of K to find an optimal accuracy-depth trade-off. Results show that K = 5 leads to a smaller DNN with balanced accuracy and model size, while K = 2 achieves higher accuracy. AutoGrow outperforms manual depth design in terms of accuracy and computation efficiency. The Periodic Growth feature helps in training deeper networks. AutoGrow is efficient in terms of wall-clock time, with quick growth and immediate stopping if no improvement is sensed. It outperforms Neural Architecture Search (NAS) by learning the depth of DNNs instead of fixing it in the search space. NAS methods may find DNNs with different depths, but AutoGrow learns the depth in a more flexible manner. AutoGrow is a more efficient alternative to Neural Architecture Search (NAS) as it learns the depth of DNNs instead of fixing it in the search space. NAS methods are computationally intensive and time-consuming, while AutoGrow can scale up to ImageNet with short depth learning time, comparable to training a single DNN. AutoGrow is a more efficient alternative to Neural Architecture Search (NAS) as it learns the depth of DNNs instead of fixing it in the search space. Structure pruning and growing were proposed for different goals, such as efficient inference, lifelong learning, and model adaptation. AutoGrow performs in a scenario where the DNN depth is unknown, hence seeking for the optimal depth. The visualization method in the study extends from Li et al. (2018) and uses PCA to project trajectories from high dimensional space to 2D space. Zeros are padded when projecting a shallower net to a deeper net space, and the loss increase along the trajectory may not accurately reflect the situation in high dimensional space. The loss decreases in high dimensions but increases in 2D space. Figure 5 shows that shallower nets converge differently than deeper nets. Figure 6 visualizes trajectories of c-AutoGrow, with detours in shallower nets. Adaptability of AutoGrow to dataset sizes is summarized in Tables 10 and 11. The convergence curves and growing process on ImageNet for different network architectures are compared in Table 10, showing varying training times on different GPUs."
}