{
    "title": "B1gd0nEFwS",
    "content": "There is a need for versatile learning techniques to transfer class-separability knowledge from a labeled source domain to an unlabeled target domain in the presence of a domain-shift. Existing domain adaptation approaches are not practical due to their reliance on source-target label-set relationship. A novel two-stage learning process is proposed to equip the model for future source-free deployment, enhancing its ability to reject out-of-source distribution samples. In a novel generative classifier framework, the model's ability to reject out-of-source distribution samples is enhanced by leveraging available source data. A unified adaptation algorithm is designed for deployment across category-gaps without access to previous source samples. A simple yet effective source-free adaptation objective is defined using a Source Similarity Metric (SSM) for instance-level weighing. The proposed learning framework shows superior domain adaptation performance compared to state-of-the-art approaches, demonstrating practical usability. Domain adaptation algorithms aim to minimize discrepancies in input distribution between source and target domains without target label information. Existing approaches assume a common label-set shared between the domains. In domain adaptation, existing approaches assume a common label-set shared between the source and target domains. However, in real-world scenarios, this assumption rarely holds true. Researchers have explored Partial DA, where the target label space is a subset of the source label space, and Open-set DA, where the target label space is not restricted to the source label space. In Open-set Domain Adaptation (DA), the target label space is a superset of the source label space. Challenges include detecting target samples from unobserved categories in a fully-unsupervised scenario. Some approaches consider a partly mixed scenario with private label sets for both domains, requiring extra supervision like few-shot labeled data or knowledge of common categories. Prior works tackle each scenario independently, necessitating knowledge of label-set relationships to choose a suitable DA algorithm. The proposed DA framework addresses source-free DA for all label-set relationships, without prior knowledge of category-gap. It aims to tackle complications by adopting unique learning techniques different from existing DA literature. The proposed domain adaptation framework aims to address source-free domain adaptation for all label-set relationships by utilizing unique learning techniques different from existing literature. It achieves superior performance compared to prior source-dependent approaches and reviews domain adaptation methods categorized under closed-set DA. The curr_chunk discusses different approaches to domain adaptation, including pixel-level adaptation using GAN framework, partial domain adaptation focusing on class-level matching, and open-set domain adaptation. Various techniques such as adversarial class-level matching, importance weights for source samples, and addressing negative-transfer are highlighted in the text. The curr_chunk discusses open-set domain adaptation, universal domain adaptation, and source-free domain adaptation. It includes stages like procurement stage and equipping the model for adaptation. The curr_chunk discusses equipping the model for the Deployment stage in open-set domain adaptation. It involves using an artificially generated negative dataset and adapting the model to unlabeled target domain samples without access to the source dataset. The challenges in domain adaptation techniques heavily rely on adversarial discriminative strategies, requiring access to the source samples for reliable characterization of the source domain distribution. Generative models can be used for source-free adaptation but are not scalable for large datasets like ImageNet due to additional parameters and training difficulties. In domain adaptation, challenges arise from the need for source samples to characterize the domain distribution. Generative models offer source-free adaptation but face scalability issues with large datasets like ImageNet. Additional parameters and training difficulties call for a fresh analysis of requirements beyond existing literature solutions. In a general domain adaptation scenario, learning domain invariant features is a common approach, where source category clusters adjust in the presence of unlabeled target samples to maintain separation between source and target clusters. Source clusters may need to disperse in Open-set DA and rearrange in Partial DA to accommodate target clusters. In a complete source-free framework, there is no flexibility. In a complete source-free framework, the objective is to learn a placement of source clusters suitable for various category-gap scenarios and to develop the ability to reject out-of-distribution samples for unsupervised adaptation. In the presence of source data, two regularization strategies are adopted to restrain the model's bias: regularization via generative modeling and utilization of a labeled simulated negative source dataset. The negative source dataset must meet key properties to enable intra-class compactness and interclass separability. The proposed method involves simulating negative source samples by compositing local regions between images from the positive source dataset, aiming to introduce new class labels in the latent space. This approach is particularly useful for image-based object recognition tasks. The method involves sampling virtual negative instances in the latent space away from high confidence regions of positive clusters. The generative source classifier consists of a backbone-model, feature extractor, and classifier. Positive source classes are defined with priors, while negative samples are distributed in the latent space. The training algorithm in the Procurement stage involves initializing parameters and updating them alternately to minimize different losses. It includes re-computing sample mean and covariance for positive classes and generating fresh negative samples in the latent space. In the Procurement stage, the training algorithm involves updating parameters to minimize losses. The algorithm computes parameters of normal distributions and defines a cross-entropy loss to enforce intra-class compactness and inter-class separability. A feature decoder is introduced to minimize cyclic reconstruction loss for positive source categories and randomly drawn samples. Challenges are addressed by biasing towards positive source samples and considering the unreliability of the negative dataset. The challenges in adapting target samples to positive source categories may lead to misclassification of target private samples. Unlike domain agnostic architectures, a domain-specific approach is preferred to maintain the placement of source clusters. To maintain the placement of source clusters, a domain-specific feature extractor F t is introduced, initialized from F s. The generative classifier from the Procurement stage is utilized to complement separate ad-hoc networks. A weighting factor (SSM) is defined for each target sample x t, indicating its similarity towards positive or negative source categories. The SSM criterion defines the similarity of target samples to positive or negative source categories. It relies on class probabilities from the source model for positive class labels. The SSM and its complement are defined accordingly to satisfy the generative learning strategy. In the Procurement stage, a generative learning strategy is used with an exponent in Eq. 2 to amplify separation between target samples from shared and private label sets. Source-free domain adaptation in the Deployment stage involves moving target samples towards positive source clusters using only the F t network parameters. The decision on weighting the loss is computed using the source feature extractor F s. The deployment model h = D \u2022 F t \u2022 M (x t ) uses the target feature extractor for softmax predictions over K categories. The primary loss function for adaptation is defined accordingly. In the absence of label information, uncertainty in predictions leads to higher entropy for samples. Entropy minimization is adopted to move target samples close to confident regions of the classifier's feature space. Separate class probability vectors are defined for positive and negative source classes to effectively distinguish target-private set from full target dataset. The final loss function for adapting the parameters of F t is presented with a hyper-parameter \u03b2 controlling the importance of entropy minimization. A source-free, universal domain adaptation framework is evaluated against prior models on multiple datasets, with an ablation study to establish generalizability. The Office-Home dataset includes images from 4 different domains. The dataset includes images from various domains such as Artistic, Clip-art, Product, Real-world, Amazon, DSLR, Webcam, and ImageNet-Caltech. Different categories are selected as C, C s, and C t for each dataset. The shared label-set C is constructed using classes from Office-31 and Caltech-256. Negative samples are simulated for evaluation. In You et al. (2019), negative labeled samples are simulated for training in the Procurement stage by sampling pairs of images from different categories to create unique negative classes. A random mask splits the images into complementary regions, and the negative image is created by merging alternate mask regions. For the ImageNet-Caltech task, a large number of possible negative classes are addressed by randomly selecting 600 negative classes for ImageNet(I) and 200 negative classes for Caltech(C) in the task C\u2192I. In the study, negative labeled samples are generated for training by sampling pairs of images from different categories to create unique negative classes. Two models are compared in the Procurement stage training: USFDA-a uses image-composition as negative dataset, while USFDA-b uses latent-simulated negative samples. Evaluation is done using the VisDA2018 Open-Set Classification challenge protocol, where target samples classified into negative classes are marked as \"unknown\". The method discussed in the curr_chunk focuses on classifying samples into negative classes without relying on sensitive hyperparameters. It is highlighted that the method is source-free during deployment, unlike other methods. Evaluation includes measuring target unknown accuracy, which is crucial for assessing model vulnerability post-deployment. This metric is not reported by a previous method, UAN. The UAN training algorithm is unstable with decreasing T unk and T avg values. The network is implemented in PyTorch with ResNet-50 as the backbone model, pre-trained on ImageNet. Sensitivity analysis of hyper-parameters is provided in figures. In the proposed framework, hyperparameters are fixed at \u03b1 = 0.2 and \u03b2 = 0.1. Adam optimizer with a learning rate of 0.0001 is used for training. Comparison with prior methods shows state-of-the-art results, even in a source-free setting. Target-unknown accuracy is presented in Table 2. The proposed framework achieves higher accuracy than UAN* on various datasets, with superior performance due to a novel learning approach. USFDA-a and USFDA-b show similar performance on standard benchmarks. The framework offers a simpler adaptation algorithm compared to UAN, without the use of ad-hoc networks or additional finetuning. The effectiveness of the learning algorithm in source-free deployment relies on the formulation of SSM. The proposed learning algorithm for source-free deployment relies on the formulation of SSM, which distinguishes between target-shared and target-private label spaces. Sensitivity to hyperparameters is low, with certain hyperparameters fixed across datasets. Variations in one hyperparameter can be made by fixing the others. The model shows low sensitivity to hyperparameters, with variations in one affecting regularization in the Procurement stage. A comparison of label-set relationships in the Office-31 dataset is illustrated in Fig. 6, highlighting the complexity of Open-set and Partial DA scenarios. The proposed tabular form in Fig. 6A demonstrates the variation of private classes for target and source. The proposed framework shows superiority in practical scenarios compared to closed-set settings. In universal adaptation, knowledge transfer of class-separability criteria is emphasized. An extreme case is considered where there are no shared categories between the source and target domains. The proposed framework demonstrates superior knowledge transfer capability in practical scenarios compared to closed-set settings. One-shot recognition accuracy is achieved with 64.72% accuracy, outperforming UAN* (You et al., 2019) at 13.43%. The scalability of the approach is evaluated by varying the number of negative classes in the Procurement stage. The study evaluates the scalability of the approach by varying the number of negative classes in the Procurement stage. Results show a drop in accuracy with fewer negative classes, validating scalability for large-scale datasets like ImageNet. Additionally, combining three or more images to form negative classes leads to under-fitting on positive source categories. In a two-stage framework, learning in the Procurement stage is crucial for exploiting class-separability knowledge with robustness to out-of-distribution samples. Success in the Deployment stage is attributed to well-designed learning objectives utilizing the source similarity criterion. This work serves as a pilot study for efficient inheritable models in the future. In the Procurement stage, a classifier D is designed to allow for dynamic modification in the number of negative classes post-procurement. Two separate classifiers, D src and D neg, operate on positive and negative source classes respectively. The final classification score is obtained by computing softmax over the concatenation of logit vectors produced by D src and D neg, enabling retraining on a different number of negative classes post deployment. The Procurement stage involves designing a classifier D that can be retrained on a different number of negative classes post deployment using another negative class classifier D neg. Two methods are proposed to generate negative samples for the Procurement stage, resulting in models named USFDA-a and USFDA-b. The negative dataset D n is generated by compositing images from different classes, creating 4 negative images for each pair of images to cover the inter-class negative region effectively. To cover the inter-class negative region effectively, image pairs are randomly sampled from different classes without constraints on class selection. 5000 pairs are chosen for tasks on various datasets, and 12000 for ImageNet-Caltech. A negative dataset is synthesized offline to ensure finiteness of the training set. The training algorithm for USFDA-a involves splicing and eigenvalue calculations. The algorithm performs rejection sampling to obtain a sample from the global source prior, ensuring it lies in an intermediate region between source class priors. The two most confident class predictions are used to assign a unique negative class label to the sample. The algorithm uses rejection sampling to obtain a sample from the global source prior, ensuring it lies between source class priors. The feature extractor F s is trained using negative samples to learn the arrangement of clusters. The cross-entropy loss (L p) enforces intra-class compactness and inter-class separability in the embedding space. Negative samples are not included in this loss to bias towards confident positive classes. The algorithm uses rejection sampling to obtain a sample from the global source prior, ensuring it lies between source class priors. The feature extractor F s is trained using negative samples to learn the arrangement of clusters. Multiple Adam optimizers are used for training to handle multiple loss terms efficiently. The algorithm uses rejection sampling to sample from the global source prior and train the feature extractor F s with negative samples. Multiple Adam optimizers are used to efficiently handle multiple loss terms during training. Different label-set relationships are defined for different datasets, with specific classes assigned as shared, source-private, and target-private classes. During the Deployment stage, the Feature Decoder G is not available, limiting access to source data. Training involves the Feature Extractor Ft initialized from Fs. The SSM is calculated by passing target images through the source model network, determining confidence based on softmax values. The study evaluates the performance of a method on the adaptation task A\u2192D in the Office-31 dataset by pretraining the ResNet-50 backbone on the Places dataset. Results show that the method outperforms source-dependent methods, even those initialized with a ResNet-50 backbone pretrained on Places dataset. Comparisons are made with a variant of another algorithm involving ResNet-50 finetuning, showing the effectiveness of the proposed method. The proposed method outperforms a variant of UAN with fewer trainable parameters and a simpler adaptation pipeline. It has significantly lower total training time and space complexity, achieving state-of-the-art domain adaptation results. Our method demonstrates superior domain adaptation performance without accessing labeled source data, achieving statistically significant improvements in outlier class detection over UAN. The results are presented in Figure 7, showcasing the efficiency of our algorithm in real-time deployment scenarios. In experiments across datasets and label-set relationships, hyperparameters are fixed as \u03b1 = 0.2, \u03b2 = 0.1, |C n | = |Cs| C 2, and b +ve /b \u2212ve = 1. Sensitivity of the model to these hyperparameters is demonstrated in Figures 8 and 5. The model shows low sensitivity to hyperparameters, even in challenging scenarios. Evaluation is also done in unsupervised closed set adaptation scenario, comparing with closed set domain in Table 4. In Table 4, comparison is made with closed set domain adaptation methods DAN, ADDA, CDAN, and universal domain adaptation method UAN. DAN, ADDA, and CDAN rely on shared label space and require retraining on source data during adaptation. The method being discussed shows superiority in source-free setting, maintaining accuracy on source samples. Increasing negative classes does not significantly affect classification accuracy on the source validation set. The method discussed in the current chunk focuses on transferring knowledge of \"class separability\" from the source domain to the target environment by segregating data samples based on expected characteristics. This is achieved by carefully choosing a bias towards positive source samples to maintain model discriminative power even with class imbalance. The proposed framework achieves 64.72% accuracy in one-shot Nearest-Neighbour based classification, outperforming UAN* (You et al., 2019) which achieved 13.43% accuracy. The samples are clustered in the intermediate feature level, validating efficient transfer of \"class separability\" in an unsupervised manner. A t-SNE plot at the intermediate feature level shows the embedding of target and source samples. The framework achieves high accuracy in one-shot classification, outperforming previous methods. It clusters samples at an intermediate feature level, showing efficient transfer of class separability. The architecture is developed and trained using Python 2.7 with PyTorch 1.0.0."
}