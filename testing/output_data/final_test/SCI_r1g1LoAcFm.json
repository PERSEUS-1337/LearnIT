{
    "title": "r1g1LoAcFm",
    "content": "In environments like health-care or biology, massively multi-label prediction/classification problems require precise predictions. A challenge is the long-tailed label distribution, leading to few positive examples for rare labels. A solution involves modifying the neural network's output layer to create a Bayesian network of sigmoids, leveraging ontology relationships to share information between rare and common labels. This method significantly improves predictive performance in disease and protein function prediction tasks. In massively multilabel classification, there is a need for precise categorization in fields like biology and healthcare. The long-tailed label distribution poses a challenge with few positive examples for rare labels. Current multi-task learning approaches help address this issue by sharing information between labels. In this paper, a new method for massively multi-label prediction, a Bayesian network of sigmoids, is introduced to improve performance on rare classes by utilizing ontological information to share information between rare and common labels. The focus is on enhancing multi-label prediction with complex DAG structures, in contrast to prior work on hierarchical softmax which aims to improve runtime performance with simpler tree structures for mutually exclusive labels. The new method introduced in this paper, a Bayesian network of sigmoids, aims to improve predictive performance on rare labels by utilizing ontological information to share information between rare and common labels. The method is tested on two different massively multi-label tasks: disease prediction using ICD-9 hierarchy and protein function prediction using Gene Ontology DAG. Results show better performance on rare labels while maintaining similar performance on common labels. The goal of multi-label prediction is to learn the distribution P (L|X) for instances X having labels L from a dictionary of N labels, particularly when there are superclass relationships provided by an ontology. The ontology used in the study consists of a DAG where labels have superclass relationships. The traditional approach involves learning separate functions for each label, leading to less sample efficiency. A more advanced method involves multi-task learning to share information between labels. The study proposes using multi-task learning techniques with neural networks to share information between label-specific binary classifiers. By introducing shared layers and constructing a Bayesian network of sigmoids based on the ontology, additional information can be shared in a guided way among labels. The study proposes using multi-task learning techniques with neural networks to share information between label-specific binary classifiers. By constructing a Bayesian network of sigmoids based on the ontology, the probability of a label can be factored into conditional probabilities, enabling the transfer of knowledge from more common labels. The factoring of probabilities in a Bayesian network allows for knowledge transfer from common labels to rare labels. This enables better learning for rare labels with limited training data by leveraging classifiers from higher-level ancestors. This approach can improve the accuracy of predicting rare labels, as demonstrated in the example of learning about lung cancer from a broader category of cancer patients. In experiments, the Bayesian network assumption allows for incorporating a better trained Cancer classifier to improve LungCancer prediction performance on rare labels. The assumption requires subgraphs to correctly represent a Bayesian network for label probability distribution, with conditional independence of labels given parent labels and X. The assumption that subgraphs need to represent a Bayesian network for label probability distribution is weaker than requiring the entire graph to follow a Bayesian network. This assumption allows every tree ontology to meet the criteria, as every {L} \u222a ancestors(L) subgraph of a tree is a simple chain, which cannot violate the conditional independence assumption behind Bayesian networks. The assumption of conditional independence in Bayesian networks is weaker than it seems because it only requires independence given a specific instance X. For example, male and female breast cancer are not conditionally independent, but become so when the gender of the patient is considered. However, there may be cases where this assumption is violated, rendering the computed product invalid. In Bayesian networks, the assumption of conditional independence may be violated, leading to invalid computed products. Despite this, the resulting scores can still be useful, as demonstrated in experiments on protein function prediction tasks. The conditional probabilities P (L|X, parents(L)) can be modeled using a sigmoid function computed on logits from neural networks. An encoder neural network is defined for each task to generate a fixed-length representation of the input X, along with a fixed-length embedding for each label L. This allows for the modeling of P (L|X, parents(L)). The encoder and label embedding in Bayesian networks model conditional probabilities using a sigmoid function on logits. Training is done with cross entropy loss on patients with parent labels, reducing negative examples for lower level classifiers. In the ICD-9 subgraph, three conditional probabilities need to be learned for Cancer, LungCancer, and BreastCancer. Different models are trained with a flat sigmoid output layer to directly learn the probabilities. The method's predictive performance was evaluated on predicting future diseases for patients based on medical history. In disease prediction, the task involves predicting future diseases for patients using ICD-9 codes from medical history data. Two years of data are used to predict which codes will appear in the following nine months. The experiment setup closely resembles that of BID17, using a large insurance claims dataset for modeling. The data includes diagnoses, medications, procedures, and metadata like age, gender, and occupation. The dataset for disease prediction includes patient information such as age, gender, and occupation. It consists of 15.7 million patients, with 5% used for validation and testing. Two cases are considered: a \"high data case\" using 14.1 million patients for training, and a \"low data case\" with a 2% sample of 281,874 patients. The target label dictionary includes leaf ICD-9 billing codes that appear at least 5 times in the training data, resulting in 6,902 codes for prediction. The dataset for disease prediction includes patient information like age, gender, and occupation. It consists of 15.7 million patients, with 5% used for validation and testing. Two cases are considered: a \"high data case\" using 14.1 million patients for training, and a \"low data case\" with a 2% sample of 281,874 patients. The target label dictionary includes 6,902 codes for the small disease prediction task and 12,533 codes for the large disease prediction task. The patient timelines are partitioned into input history and output prediction labels, with the input history further divided into time-bins. A feed-forward architecture inspired by BID1 is used for the encoder. The encoder architecture for disease prediction involves splitting data into time-sliced bins and extracting codes experienced by patients in each time slice. Higher-level code features are added for ICD-9, ATC, and CPT ontologies. Input embeddings are tied to output embeddings, and mean pooling is used to summarize embeddings for each time bin. Metadata entries are also processed through an embedding matrix for mean pooling. The resulting means are concatenated for further processing. The patient embeddings are computed using mean pooling from each timeslice and metadata, then fed into a feedforward neural network. Hyperparameters are optimized using a grid search on the validation set. Logistic regression models are also trained for rare ICD-9 codes. The logistic regression models are regularized with L2 and optimized using cross-validation. To address the imbalance in the dataset, negative examples are subsampled. Another experiment involves predicting protein functions from sequence data, focusing on human proteins with Gene Ontology annotations. Amino acid sequences are used as features, and human GO labels are used as labels. In 2018, 15,497 human protein sequences were annotated and split for training, validation, and testing. Predictions were made for GO terms appearing at least 5 times in the training data. Relationships between labels were constructed using the GO basic ontology. A 1-D CNN encoder was used to encode protein sequence information, with each letter represented as a word and embedded with a size of 26. Max-over-time pooling was applied to obtain a fixed-length representation, followed by ReLU and a fully connected layer. The protein sequence information is encoded using a 1-D CNN encoder with ReLU and a fully connected layer. Dropout is added for regularization. Features are generated using sequence alignment with the BLAST tool. CNN models are trained with Adam, and hyperparameters are optimized using grid search on the validation set. See Appendix A.1 for details on hyperparameters. The study explores hyperparameters for flat sigmoid and Bayesian network of sigmoids models, along with using BLAST features alone for protein function prediction. A weighting scheme based on label frequency did not perform well on rare words. Results are shown in Appendix A.3, with Figure 3 displaying AUROC and AP for less frequent labels. The study compared the performance of Bayesian network of sigmoid models with flat sigmoid models for rare labels in three tasks. Bayesian network of sigmoid models showed better AUROC and average precision for rare labels, with significant improvements in average precision for the rarest code bins. The Bayesian network of sigmoids output layer provides better AUROC and AP for disease prediction but worse performance for protein function prediction. The Bayesian network assumption may be correct for disease prediction due to the tree structure of the ontology, but not for protein function prediction with a more complicated DAG structure. Minor violations of the assumption in the protein function task could lead to worse performance compared to a flat sigmoid decoder. Previous work has focused on improved softmax variants for multi-class problems, such as language modeling, but primarily with a tree structure connecting labels. The focus is on improving training time for the expensive softmax operation in multi-class problems like large-vocabulary language modeling. Variants like hierarchical softmax use a tree structure to decompose the probability distribution. Disease prediction, specifically predicting ICD-9 codes from medical records, has also been explored. GRAM utilizes the CCS hierarchy to enhance the encoder for disease prediction tasks. Our work focuses on improving predictions for rare codes by enhancing the output layer, differentiating from GRAM which enhances the encoder. Protein function prediction, specifically Gene Ontology term prediction, has been explored by previous works like DeepGO, which uses a CNN on sequence data. In contrast, our approach targets rarer terms and only modifies the output layer. Additionally, Phrank constructs Bayesian networks for computing similarity scores between sets of phenotypes for diagnosing genetic disorders. This paper introduces a new method for improving the performance of rare labels in massively multi-label problems with ontologically structured labels. It uses Bayesian networks to model the probability of rare labels as a product of conditional probabilities of more common higher-level labels, achieving better performance in AUROC and average precision. The paper introduces a new method using Bayesian networks to improve performance for rare labels in massively multi-label problems with ontologically structured labels. This method outperforms flat sigmoid baselines in protein function prediction and disease prediction experiments, enabling more precise predictions for smaller label categories."
}