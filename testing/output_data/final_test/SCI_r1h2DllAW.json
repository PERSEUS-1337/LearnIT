{
    "title": "r1h2DllAW",
    "content": "The research focuses on training low precision neural networks using a Bayesian approach to infer weight distributions for hardware-friendly implementations. The proposed model achieves state-of-the-art performance on real-world datasets and exhibits significant sparsity for reduced computational costs during inference. Deep neural networks have shown impressive performances in various applications, but improvements are mainly due to hardware advancements. There is a growing interest in developing NN architectures for embedded devices with limited resources, focusing on fast and energy-efficient inference with reduced memory requirements. This paper specifically looks at reduced precision methods to decrease the memory footprint while maintaining network size. Reduced precision methods aim to decrease memory footprint in neural networks by using binary weights and logical XNOR operations for faster computation. However, training such networks is challenging as gradient-based methods cannot be directly applied. The full computational benefits of binary weights are seen with sign activation functions, but these are not suitable for backpropagation. Training methods typically involve quantizing pre-trained full precision weights. Most methods for training reduced precision NNs involve quantizing weights of pre-trained full precision NNs or maintaining a set of full precision weights that are quantized during propagation. The straight through gradient estimator (STE) is a promising approach that allows information to flow backwards for parameter updates. In this paper, a Bayesian approach is proposed to support discrete weights in neural networks. The aim is to develop principled methods for obtaining a single discrete-valued neural network with good performance by training a distribution over discrete-valued neural networks and deriving a single network from that distribution. The proposed method involves inferring a distribution over a discrete weight space to optimize over. The paper proposes a Bayesian approach to support discrete weights in neural networks by training a distribution over discrete-valued neural networks. This allows optimization over real-valued distribution parameters instead of the intractable combinatorial space of discrete weights. The distribution q(W) represents an ensemble of NNs weighted by their probabilities, maintaining a whole distribution for each connection. Variational inference is used to approximate the true posterior p(W|D) by minimizing the variational objective KL(q(W)||p(W|D)), with recent attention due to the reparameterization trick for real-valued NNs. The reparameterization trick is limited to real-valued distributions, but the Gumbel softmax distribution allows for the application of this trick to one-hot encoded discrete distributions. However, for the sign activation function, alternative methods like STE are still needed. The log-derivative trick provides another option for expressing gradients of expectations with expectations of gradients for discrete distributions, but it is known to have high variance in gradient samples. In this work, the intractable variational objective is approximated with a probabilistic forward pass (PFP) by propagating probabilities through the network using Gaussian approximations. This method allows for the computation of gradients with respect to variational parameters analytically, even for discrete weight distributions and the sign activation function. The flexibility of using different weight distributions in different layers is also highlighted. In this work, the flexibility of using different weight distributions in different layers is utilized. The first layer is represented with 3 bits, while ternary weights are used in the remaining layers. The performance of the model is evaluated by reporting errors of the approximate posterior and expected predictions using the PFP. Averaging over small ensembles of NNs sampled from the approximate posterior improves performance without increasing computational overhead. The method also exhibits sparsity, reducing computational costs compared to BID8. The paper introduces the PFP method for feed-forward neural networks with multiple layers. It defines the structure of a NN with L layers and explains the function y = x L = f (x 0 ) using linear transformations and non-linear functions. The softmax activation function is used in the final layer, but it is not computed at test time to reduce computational costs. The paper introduces the PFP method for feed-forward neural networks with multiple layers. It defines the structure of a NN with L layers and explains the function y = x L = f (x 0 ) using linear transformations and non-linear functions. The softmax activation function is used in the final layer, but it is not computed at test time to reduce computational costs. For a Bayesian treatment of NNs, a prior distribution p(W ) over the discrete weights is assumed, and the output of the NN after the softmax activation is interpreted as likelihood p(D|W ). To approximate the intractable likelihood for NNs of any decent size, variational inference is employed by minimizing KL(q(W |\u03bd)||p(W |D)) with respect to the variational parameters \u03bd. The variational objective is transformed and minimized with respect to \u03bd without involving the intractable posterior p(W |D). The KL term acts as a regularizer pulling the approximate posterior towards the prior distribution, while the expected log-likelihood captures the data. The expected log-likelihood is typically intractable due to a sum over exponentially many terms, so a PFP is proposed as a closed-form approximation. The goal is to eliminate weights in each layer to reduce the number of terms to sum over, starting with approximating the activation distribution with Gaussians. The Gaussian distributions are propagated through the sign activation function to obtain Bernoulli distributions. Iterations continue until a Gaussian approximation of the output activations is achieved. Neuron activations are computed as weighted sums over previous layer outputs, treated as random variables. Activation distributions are approximated with Gaussians for computational ease, assuming independence within layers. After propagating Gaussian distributions through the sign activation function to obtain Bernoulli distributions, iterations continue until a Gaussian approximation of the output activations is achieved. The expectation of the resulting Bernoulli distribution with values x \u2208 {\u22121, 1} of the sign activation function is calculated using a second-order Taylor approximation. The log-softmax is approximated by its second-order Taylor approximation around the mean \u00b5 a L with a diagonal covariance approximation. The second term in the Taylor approximation penalizes large output variances when the output activation means are large and close to each other. Details of the finite weight sets and their corresponding distributions are provided, with a focus on using higher precision for weights in the first layer. Three bits are used for the first layer weights and ternary weights for the remaining layers. We use three bits for the weights in the first layer and ternary weights in the remaining layers. The values in the first layer can be represented as fixed point numbers using three bits, with a range of -0.75 to 0.75. Additional values are not included for symmetry around zero. The range of values performs well for inputs x \u2208 [-1, 1]. The values can be scaled at training time without affecting the output of the neural network. Two different variational distributions for weights are investigated. The text discusses two variational distributions for weights: log-probabilities for discrete values and a discretized Gaussian distribution. The former allows for expressing tendencies towards specific values, while the latter eliminates parameter dependency on the size of D1. The discrete distribution N D1 (m w , v w ) is parameterized with two parameters for an arbitrary size of D 1. The distribution is unimodal with neighboring values having similar probabilities. The mean \u00b5 w and variance \u03c3 w require a weighted sum computation. The prior distribution p(W 1 ) uses a discretized Gaussian N D1 (0, \u03b3) with \u03b3 as a hyperparameter. Ternary weights are used for remaining layers. For the remaining layers, ternary weights are used with a shifted binomial distribution. This distribution simplifies computations and requires only one parameter per weight. The mean and variance are efficiently calculated, making the Bernoulli distribution a suitable choice. A binomial prior distribution is selected for its favorable properties, centered at zero. The activations of layer l are normalized by d l\u22121. Activation normalization in layer l by d l\u22121 scales activation means towards zero and keeps variances independent of incoming neurons. This helps gradients flow backwards in the computation graph. It only affects the PFP and not the classification result of individual NNs. The variational inference objective struggles to balance expected log-likelihood and the KL term due to the disparity between NN weights and data samples. The optimization procedure focuses on balancing the expected log-likelihood and the KL term by using a convex combination with a tunable hyperparameter \u03bb. This approach helps counteract the issue of the KL term dominating the optimization process, ensuring that the data influence is not overshadowed. The performance of the NN VI model was evaluated on MNIST, variants of MNIST, and a TIMIT dataset. A three-layer structure with 1200 hidden units was used for all experiments, with different variational distributions for each layer. Optimization was done using ADAM without KL reweighting and rmsprop with KL reweighting, with the latter showing better classification performance. For optimization, an exponential decay of the learning rate is employed with dropout rates for input and hidden layers. Hyperparameters are tuned using Bayesian optimization. Results are reported for the most probable model from the approximate posterior, which is a low precision network suitable for hardware implementation. Predictions are computed using PFP to approximate expected predictions in Bayesian inference. The model is compared with real-valued NNs trained with batch. Our model, trained with Bayesian inference, performs similarly to real-valued NNs with batch normalization, dropout, and ReLU activation function. It outperforms NN STE on TIMIT dataset and challenging variants of MNIST. Particularly excelling on MNIST Background and MNIST Background Random, possibly due to its Bayesian nature. The PFP model outperforms the single most probable model by utilizing all available information from the approximate posterior. The general variational distribution shows slightly better performance than the discretized Gaussian but with higher computational overhead. Training time on MNIST dataset varies between models, with the computational bottleneck in the first layer. Sampling multiple models from the posterior distribution is used to approximate expected predictions on the MNIST Rotated Background dataset. The study demonstrates the performance of NN VI on the MNIST Rotated Background dataset using various model configurations. Bayesian averaging over 1000 NNs sampled from the model with the best PFP performance shows that the PFP is a good approximation to the true expected predictions. However, a large ensemble is needed to approach the PFP, leading to longer computation times. Using a greedy forward selection strategy, 100 NNs are sampled and only the one with the lowest error is included in the ensemble, resulting in slightly better performance than Bayesian averaging. Averaging only a few models leads to a performance increase while allowing for faster inference. NNs obtained from q(W) can be efficiently implemented in hardware with 3 bit fixed point values, resulting in faster computations compared to NN (real) and NN STE. In the input layer, utilizing zero weights can reduce computational costs significantly. For example, on the MNIST Background Random dataset, a single NN requires only 23000 integer multiplications and 1434000 XNOR operations instead of 2393000 floating point multiplications. The model's behavior during training heavily depends on tunable parameters, such as variational distributions for individual layers. The binomial distribution is a natural choice for evenly spaced values with desirable properties, but not suited for the first layer where setting weights to zero is crucial. Setting weights to 0.5 results in the largest variance, which may not be an issue if predictions are computed as true expectations. In experiments, using the binomial distribution in deeper layers favors weights -1 and 1 over 0. However, binary weights from a Bernoulli distribution perform worse due to the importance of zero weight and larger variance. Sign activation functions pose issues when activations are close to zero, causing neurons to take on different values with small input changes. Dropout is a helpful tool to counteract issues with activations in neural networks. It introduces a larger spread of activations, making them more stable and robust. Regularization techniques are crucial when using the sign activation function. A method to infer neural networks with low precision weights was introduced. Our method uses variational inference to derive a discrete-valued neural network or ensemble without relying on heuristics. It models weights with fixed point values and allows flexible bit-widths for each layer, reducing computational costs. Our model, utilizing variational inference, achieves performance comparable to higher precision methods while exhibiting significant sparsity. The MNIST dataset consists of grayscale images of handwritten digits, normalized to [-1, 1]. The task is to classify these images without prior knowledge of image structure, such as convolutional neural networks. The MNIST dataset includes images of handwritten digits transformed by various operations to create more challenging datasets. Variants include Basic, Background, Background Random, Rotated, and Rotated Background. The pixel intensities are normalized to [-1, 1]. The TIMIT dataset BID27 contains 92 features representing phonetic segments, with the task of classifying them into 39 phonemes. The data is split into 140173 training samples, 50735 validation samples, and 7211 test samples. Features are normalized to have zero mean and unit variance."
}