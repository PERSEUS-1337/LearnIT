{
    "title": "Byl8hhNYPS",
    "content": "In this paper, a universal visual representation is introduced for neural machine translation (NMT) using monolingual corpora with image annotations. The method retrieves images related to the source sentence, encodes them with a pre-trained ResNet, and fuses visual and text information with an attention layer for predicting target translations. This approach allows visual information to be integrated into large-scale text-only NMT. The proposed method integrates visual information into text-only NMT models, showing significant improvements over strong baselines in translation tasks. Previous studies have introduced visual information for NMT, but the contribution of images remains an open question. Bilingual parallel sentence pairs are manually annotated with images to train a multimodal NMT model, with results reported on the Multi30K dataset. The multimodel NMT model utilizes visual information to enhance translation quality, but its effectiveness depends on the availability of bilingual parallel sentence pairs with manual image annotations. This limits the application of visual information to a small dataset like Multi30K, creating a bottleneck in multimodal NMT. The paper introduces a universal visual representation method for NMT, using image-monolingual annotations to break the bottleneck of incorporating visual information. It transforms sentence-image pairs into a topic-image lookup table from the Multi30K dataset, retrieving similar topic images during training and decoding. An attention layer fuses image and source sentence representations for input. The proposed approach integrates image representations with source sentence representations for NMT decoding without the need for large-scale bilingual parallel corpora. It was evaluated on various translation datasets, showing effectiveness in utilizing visual information in text-only NMT models. The proposed approach integrates visual information for NMT decoding without large-scale bilingual data. Experiments on translation tasks confirmed its effectiveness. Studies on image description generation led to a new shared translation task for multimodal machine translation. The dataset Multi30K includes multilingual parallel sentence pairs with image annotations. Recent studies have focused on incorporating visual features into neural machine translation models. While some have shown improvements in translation performance by including spatial visual features and global image features, others have suggested that the visual modality may not always be necessary. However, these approaches have been limited to a small dataset, hindering the broader applicability of image information in NMT. Incorporating visual features into neural machine translation models has shown improvements in translation performance. However, the high cost of image annotations hinders the applicability of image information in NMT. The capacity of multimodal machine translation has not been fully explored yet. This work aims to enable visual information to benefit NMT, especially text-only NMT, by introducing a universal visual representation method. The proposed universal visual representation method aims to transform sentence-image pairs into a topic-image lookup table to link topic words in a sentence with relevant images. A filtering method using TF-IDF is applied to extract key \"topic\" words from the sentence, focusing on important information while reducing noise like stopwords and low-frequency words. The proposed method transforms sentence-image pairs into a topic-image lookup table by extracting key topic words using TF-IDF. Each word is paired with an image, forming a lookup table where topic words are linked to relevant images for retrieval. The method transforms sentence-image pairs into a topic-image lookup table by extracting key topic words using TF-IDF. Images associated with each topic word are grouped together to form an image list. Images are sorted based on frequency of occurrences to maintain a total number of images for each sentence. Multiple topic words can be associated with an image, resulting in multiple occurrences in the list. Examples of sentence-image pairs with topic words highlighted are shown. The corpus is processed using the topic-image transformation. The corpus is processed to obtain topic words using TF-IDF. These topic words are used to retrieve images from a lookup table. The images are sorted based on frequency of occurrences to select the top images that cover the most topics of the sentence. Manual alignment of words and images is not strictly required, relying on co-occurrence instead. The proposed universal visual representation method for NMT eliminates the need for manual alignment of words and images by relying on the co-occurrence of topic words and images. The framework includes a Transformer-based NMT encoder with multiple layers, each consisting of self-attention and feed-forward network sub-layers. Residual connections are applied between these sub-layers for improved performance. The proposed model utilizes a lookup table for NLP tasks without paired images, allowing for generalization. The source sentence is encoded and paired with retrieved images, which are processed through a pre-trained ResNet for learning the source image representation. The model uses a pre-trained ResNet to learn image representation and applies an attention mechanism to combine it with text representation for translation prediction. A weight parameter is computed to determine the importance of image information for each source word, and the fused representation is fed to the decoder for target translation prediction. The proposed method uses a single aggregation layer to fuse image and text information for translation prediction. It was evaluated on four translation datasets: WMT'16 EN-RO, WMT'14 EN-DE, WMT'14 EN-FR, and Multi30K. Different training data and test sets were used for each translation task. The Multi30K dataset contains English\u2192{German, French} parallel sentence pairs with visual annotations. The dataset was used for image retrieval implementation, where 29,000 sentence-image pairs were used to build a topic-image lookup table. Top-8 high TF-IDF words were selected for retrieval, resulting in 3K topic words associated with 10K images. The Multi30K dataset was used for image retrieval with 10K images. Features were extracted using a pre-trained ResNet50 CNN. A text-only Transformer baseline was used with specific settings for encoder and decoder layers. A multimodal baseline (MMT) was also evaluated with paired source sentences and images. Byte pair encoding with a vocabulary size of 40,000 was adopted. Training batches contained approximately 4096\u00d74 tokens for both source and target sentences. During training, the model used label smoothing of 0.1, with attention and residual dropout set to p = 0.1. Adam optimizer was used with 1,000 batches on the dev set. The model was trained up to 10,000 steps for Multi30K dataset and early-stopped if dev set BLEU score didn't improve for ten epochs. For EN-DE, EN-RO, and EN-FR tasks, the model with the highest BLEU score after 200,000 batches was selected for test set evaluation. Beam size was set to five during decoding. Models were trained and evaluated on a single V100 GPU using Multi-bleu.perl 8 for computing BLEU scores. Signtest was used for statistical significance testing. Model configurations from Vaswani et al. (2017) were followed to train Big models for WMT. Our implemented Transformer models, based on Vaswani et al. (2017), were trained for WMT translation tasks using fairseq 9. Results in Table 1 show similar BLEU scores to the original Transformer, validating our method against strong baseline NMT systems. The proposed +VR model outperformed the baseline, showcasing the effectiveness of incorporating visual information for text-only NMT. This approach proved universally beneficial across different language pairs with varying training data scales, enhancing translation performance. The proposed method introduced a universal approach to improve translation performance by incorporating visual information. It added a small number of parameters to base and big transformers, utilizing fixed image embeddings from a pre-trained ResNet feature extractor. The training time remained similar to the baseline model. Evaluation on the Multi30K dataset showed that the model outperformed the transformer baseline, with marginal contribution from image presentation due to the simplicity and repetitiveness of the source text. The current bottleneck of Multimodal Machine Translation (MMT) is due to the limitations of Multi30K dataset. Results from test2016 and test2017 for the MMT task are presented, showing the necessity of transferring multimodality into text-only NMT tasks. The lookup table contributes to content connection between sentences and images, as well as topic-aware co-occurrence of similar images and sentences. In the context of Multimodal Machine Translation (MMT), the idea of using consistent images as topic hints for similar sentence modeling is explored. By setting a threshold for TF-IDF retrieval, improper images can be filtered out. The concept of Distributional Hypothesis is extended to the multimodal world, suggesting that sentences with similar meanings are likely to pair with similar images. This approach is akin to word embedding, where each image is treated as a \"word\" in the model. In Multimodal Machine Translation (MMT), images are used as topic hints for sentence modeling. The image content is represented as a 2400d vector, affecting neural network capacity. Mapping text words to word embeddings and sentences to image embeddings is crucial. Different methods like Shuffle, Random Init, and Random Mapping were tested, with BLEU scores of 33.53, 33.28, 32.14. The results support the proposed VR approach. The results of experiments on EN-RO show that the proposed VR approach outperforms the baseline model. Increasing the number of paired images initially improves BLEU scores, but too many images can introduce noise. Setting the number of images to 5 is optimal for the models. The experiments on EN-RO show that the proposed VR approach outperforms the baseline model. Increasing the number of paired images initially improves BLEU scores, but too many images can introduce noise. Setting the number of images to 5 is optimal for the models. Additionally, adding external sentence-pairs from the training set of MS COCO image caption dataset improved BLEU scores, indicating the benefit of a modest number of pairs. The weight \u03bb of the gated aggregation method was learned automatically to measure the importance of visual information, with manually set values also showing effectiveness in experiments on the EN-RO test set. The proposed VR approach outperforms the baseline model on EN-RO experiments. The dependency of image information varies for each source sentence, highlighting the need to automatically learn gating weights for image representations. The extra computation cost of obtaining image data for sentences and learning image representations is minimal compared to training a NMT model. The lookup table maps token indices to image ids, and the retrieval method sorts image ids by frequency. Learning image representations takes about 2 minutes. The image ids are sorted by frequency, and image representations are extracted in about 2 minutes for all 29,000 images in Multi30K. The extracted features form the \"image embedding layer\" for quick access in neural networks. This work introduces a universal visual representation method for neural machine translation using monolingual image annotations, breaking the dependency on bilingual sentence-image pairs. Visual information can now be applied to large-scale text-only NMT through a topic-image lookup, offering insights for future MMT research. The elderly woman is cooking in a kitchen, a small boy is playing with a soccer ball on a field, and images are retrieved for sentences in the Multi30K dataset."
}