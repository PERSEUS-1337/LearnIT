{
    "title": "HJ_aoCyRZ",
    "content": "Spectral clustering is a popular technique in unsupervised data analysis with limitations in scalability and generalization. A deep learning approach called SpectralNet is introduced in this paper to address these issues. SpectralNet learns to embed data points into the eigenspace of their graph Laplacian matrix for clustering, using constrained stochastic optimization. This allows scalability to large datasets and generalization to unseen data points. Additionally, pairwise Gaussian affinities are replaced to improve clustering quality. To further enhance clustering quality, pairwise Gaussian affinities are replaced with affinities learned from unlabeled data using a Siamese network in SpectralNet. Applying the network to code representations from standard autoencoders can lead to additional improvements. The end-to-end learning process is fully unsupervised, and a lower bound on the size of SpectralNet is derived using VC dimension theory. State-of-the-art clustering results are achieved on the MNIST and Reuters datasets. Discovering clusters in unlabeled data is valuable due to the cost and expertise required for labeling. Spectral Clustering is a popular algorithm that embeds data in the eigenspace of the Laplacian matrix to reveal underlying structure. Its optimal embedding minimizes pairwise distances between similar data points, making it appealing for clustering tasks. Spectral clustering variants arise as relaxations of graph balanced-cut problems and outperform other popular clustering algorithms on simulated datasets in 2D and 3D by successfully finding non-convex clusters. The Euclidean distance in the embedding space equals a diffusion distance, measuring the time for probability mass to transfer between points. SpectralNet is a deep learning approach to spectral clustering that addresses scalability and out-of-sample-extension problems. It is trained in a stochastic fashion and provides a function that maps input data points to spectral embedding coordinates, allowing easy application to new test data. The model is trained using constrained optimization with orthogonality enforced by adding a linear layer set by QR decomposition. SpectralNet is a deep learning approach to spectral clustering that improves clustering quality by training Siamese networks on unlabeled data to learn informative pairwise distances. The network approximates Laplacian eigenvectors well, enabling clustering of challenging non-convex point sets. The model is trained using constrained optimization with orthogonality enforced by adding a linear layer set by QR decomposition. Recent deep learning methods for clustering aim to learn a code suitable for k-means or mixture of gaussians clustering models. DCN BID24 optimizes a loss with a reconstruction term, DEC BID23 updates a target distribution for cluster associations, and DEPICT adds a regularization term for balanced clusters. Other approaches use variational autoencoders for clustering tasks. Several recent clustering algorithms utilize variational autoencoders with Gaussian mixture priors, such as VaDE, GMVAE, and IMSAT. Different approaches include deep belief nets with non-parametric maximum margin clustering and a recurrent-agglomerative framework for image clustering. While these methods achieve accurate results on standard datasets, there is a bias towards forming clusters with convex shapes due to the use of k-means criterion and Gaussian mixture prior. In contrast to previous clustering algorithms biased towards convex shapes, SpectralNet shows less vulnerability to this bias. BID21 uses an autoencoder to map graph Laplacian matrix rows to spectral embeddings for clustering, while Tian et al.'s method takes entire Laplacian rows as input, making OOSE impractical. The kernel spectral method by BID0 allows for out of sample extension and handles large datasets through smart sampling. BID26 addresses 3D shape segmentation using graph convolutions and graph spectral embedding. Other works in deep learning apply a spectral approach in supervised learning, such as BID12 using supervised metric learning and BID13 training a network to compute graph Laplacian eigenvectors. In contrast to previous works, our approach uses a small submatrix of the affinity matrix in each iteration for spectral clustering. Future work will focus on adapting algorithms to improve convergence rates. The proposed network is described, along with its key components and connection to spectral clustering. SpectralNet is a neural network approach for spectral clustering that learns a similarity measure between data points to assign them to clusters. It can determine cluster assignments for new points without recalculating similarities or re-clustering. SpectralNet uses a neural network to compute a map and cluster assignments for spectral clustering. It involves unsupervised learning of affinity and map F \u03b8, and learning cluster assignments through k-means clustering in the embedded space. SpectralNet utilizes a neural network to generate a map and cluster assignments for spectral clustering. The loss function is defined to ensure that similar points are embedded close to each other, with orthonormal outputs expected with respect to the unknown distribution. Optimization is done stochastically by sampling mini-batches of data. SpectralNet uses a neural network for spectral clustering, minimizing loss to embed similar points close together. Optimization is done stochastically with mini-batches. The network enforces orthogonality in the last layer to orthogonalize the output matrix. The last layer of SpectralNet uses QR decomposition to orthogonalize the output matrix. Training involves alternating between orthogonalization and gradient steps with different minibatches. Once trained, all weights are frozen, including those of the last layer. The last layer of SpectralNet acts as a linear layer. Cluster assignments are obtained by propagating inputs through it to get embeddings and performing k-means. The loss function can be written in terms of the graph Laplacian matrix. The minimum loss is achieved when the column space of the embeddings corresponds to the smallest eigenvalues of the Laplacian matrix. SpectralNet approximates spectral clustering through stochastic training, trading accuracy for scalability and generalization. It provides a parametric function for clustering large datasets and embedding new test points. Experiments with the MNIST dataset show that SpectralNet's outputs closely approximate the true eigenvectors. SpectralNet approximates spectral clustering through stochastic training, providing a parametric function for clustering large datasets. Cluster assignments are determined by applying k-means to the embeddings, with the option to use other clustering algorithms. The loss function for training with normalized graph Laplacian is different from traditional classification or regression losses. SpectralNet loss is summed over pairs of points using the affinity matrix W full. Minibatches should be large and sampled randomly to capture data structure. Output layer orthogonalizes \u1ef8, but guarantees on other minibatches are not provided. Stabilization of weights observed with large enough m. To train SpectralNet, larger minibatches are used compared to common choices, with sizes of 1024 for MNIST and 2048 for Reuters. Choosing a good affinity measure is crucial for spectral clustering success, with Siamese nets trained to learn affinity relations between data points. In training SpectralNet, larger minibatches are used, with Siamese nets learning affinity relations between data points to improve clustering quality. The network is trained on positive and negative pairs of data points, with positive pairs constructed from nearest neighbors and negative pairs from points with larger distances. The Siamese network is trained to learn an adaptive nearest neighbor metric by mapping data points into embeddings. The network minimizes contrastive loss with a margin. Using Siamese distances in the affinity matrix for SpectralNet training improves clustering quality significantly. This shows that unsupervised training of Siamese nets can lead to learning useful affinity relations. The end-to-end training approach involves constructing a training set, training a Siamese network, and using an orthogonalization step to tune network weights for affinity matrix computation and loss optimization. Once SpectralNet is trained, new test points can easily be embedded and assigned to clusters by propagating them through the network and assigning them to the nearest centroid computed using k-means on the training data. SpectralNet can be applied in either the original input space or a lower-dimensional code space obtained from an autoencoder. SpectralNet performs best in code space, using the code directly as data representation. It determines cluster assignments in training and produces a map that can generalize to unseen data points at test time. The network size needed to represent the spectral map is a natural question to consider. The VC dimension theory is used to study the minimal size a neural network needs to compute spectral clustering for k = 2. The network should map training points to binary values based on thresholding the eigenvector of the graph Laplacian. The goal is to determine the minimal number of weights and neurons required for this task. The VC dimension theory is applied to study the minimal neural network size needed for spectral clustering on n points in Euclidean spaces R^d, with d \u2265 3. The main result proves a linear lower bound on the VC dimension of spectral clustering, showing its richness compared to k-means. This analysis is significant for computing Laplacian eigenvectors in neural networks. Spectral clustering has a linear lower bound on its VC dimension, making it almost as rich as arbitrary clustering of points in Euclidean spaces. The proof is deferred to the appendix, but it shows that spectral clustering can shatter a set of points in R^d. This contrasts with k-means clustering, which has a VC dimension of d + 1 and can only shatter d + 1 points in R^d. Spectral clustering has a linear lower bound on its VC dimension, allowing it to shatter a set of points in R^d. By constructing a Gaussian affinity W with a suitable value of \u03c3, the spectral clustering loss can separate points S from T, respecting the original dichotomy. This connection with neural nets helps bound the size of any neural net that computes spectral clustering. The VC dimension of neural nets with sigmoid nodes and weights is related to spectral clustering. To represent functions realizable by spectral clustering, the number of weights must be at least proportional to the number of points. To express functions computed using spectral clustering, the number of weights in neural nets must be proportional to the number of data points. The size of the network needs to grow with n in the general case, but can be smaller when data has geometric structure. Neural networks can learn eigenvectors of Laplacian matrices well, with network size depending on desired error and manifold parameters, not on n. The size of the network for spectral clustering depends on the desired error and manifold parameters, not on the number of data points. Two measures used for evaluating clustering accuracy are unsupervised clustering accuracy (ACC) and normalized mutual information (NMI). ACC is computed using permutations, and NMI measures the agreement between true labels and predicted clusters. SpectralNet is compared to various deep learning-based clustering approaches on real-world datasets, with ACC and NMI used to evaluate clustering accuracy. The number of clusters is assumed to be given, and results are compared to k-means and standard spectral clustering. Gaussian affinity functions using Euclidean and Siamese distances are considered. Technical details for k-means and spectral clustering are provided in the appendix. In experiments, two variants of Gaussian affinity functions were used: Euclidean distances and Siamese distances following Algorithm 1. Results of SpectralNet and Siamese net were reported in input and code space. MNIST dataset with 70,000 images of handwritten digits was used for training various clustering algorithms. Positive pairs for Siamese net were constructed by pairing each instance with its two nearest neighbors, while an equal number of negative pairs were randomly chosen. SpectralNet outperforms DEC, DCN, VaDE, DEPICT, and JULE on the MNIST dataset by using Siamese distance and code space representation. The Grassmann distance between SpectralNet outputs and true eigenvectors decreases rapidly at the beginning of training and stabilizes over time. The distance decreases rapidly at the beginning of training and stabilizes around 0.026. SpectralNet generalizes well to unseen test data with an accuracy of .970. K-means accuracy on the test set is .546 in the input space and .776 in the code space, both inferior to SpectralNet. The Reuters dataset consists of English news labeled by category. The dataset used for the study consists of 685,071 English news articles labeled by categories such as corporate/industrial, government/social, markets, and economics. A tfidf vector representation was created for each article using the 2000 most frequent words. To handle the large dataset, an Autoencoder (AE) was trained on a subset of 10,000 samples. Positive and negative pairs were constructed for the Siamese net using random sampling methods. The performance of various algorithms on the Reuters dataset is shown in Table 1, with results similar to those observed on MNIST. Our SpectralNet implementation on the Reuters dataset outperforms all other methods, particularly in code space and using Siamese affinity. It took less than 20 minutes to learn the spectral map, while computing the top four eigenvectors of the Laplacian matrix for spectral clustering took over 100 minutes using ARPACK. SpectralNet showed robustness compared to spectral clustering, which failed to produce reasonable clustering. Generalization ability was evaluated by dividing the data randomly into a 90%-10% split and re-training the models on the larger subset. SpectralNet, a deep learning approach for approximate spectral clustering, was introduced. The stochastic training of SpectralNet allows scaling to larger datasets, and the use of unsupervised Siamese networks for computing distances resulted in better performance. Applying the network to code representations from a stacked autoencoder further improved results. Additionally, a novel analysis of the VC dimension of spectral clustering was presented. SpectralNet integrates spectral clustering with deep learning, providing a useful tool for unsupervised deep learning. It outperforms existing methods on datasets with clusters that cannot be contained in non-overlapping convex shapes. The approach involves computing eigenvectors of the unnormalized graph Laplacian and applying k-means to the embeddings. The affinity matrix is computed using a standard practice in diffusion. SpectralNet integrates spectral clustering with deep learning, outperforming existing methods on datasets with complex cluster structures. The net outputs closely approximate true eigenvectors, with Grassmann distance approaching zero as loss decreases. In further experiments, various methods were tested on 2D datasets, using different network architectures and hyper-parameter settings. Unfortunately, various network architectures and hyper-parameter settings were tested, but no appropriate clustering was achieved for DCN, VaDE, and DEPICT on the datasets. IMSAT only worked on two out of five datasets, failing in simple cases. Plots of the methods' results on 2D datasets are shown in FIG4. Further experiments with nested 'C's data revealed that all methods failed to cluster points correctly when clusters couldn't be linearly separated, as depicted in FIG5. DEPICT's target distribution, initialized with successful agglomerative clustering, became corrupted during training despite its loss. The target distribution of DEPICT becomes corrupted during training, despite reduced loss. Lemma C.2 states that for any integer m, a set X can be constructed with n = 10m points, allowing for a balanced binary partition. Lemma C.2 states that a set X can be constructed with n = 10m points for a balanced binary partition, with points placed in a 2-dimensional unit grid inside a square of minimal diameter. Lemma C.2 describes the construction of a set X with n = 10m points for a balanced binary partition on a 2-dimensional unit grid inside a square of minimal diameter. Points are added to make sets balanced, copied at Z = 1 and Z = -1, midpoints are added, and more points are placed along the minimal length spanning tree. The red points form set S, blue points form set T, and X = S \u222a T. The construction of set X with n = 10m points for a balanced binary partition on a 2-dimensional unit grid involves adding points to ensure balance, copying points at Z = 1 and Z = -1, adding midpoints, and placing more points along the minimal length spanning tree. The points in set S and T are connected by spanning trees, and additional points are added along the edges. The resulting point set X satisfies the conditions of the lemma, with S \u2282 S and T \u2282 T, and the length of each spanning tree not exceeding 2m. The construction of set X with n = 10m points for a balanced binary partition on a 2-dimensional unit grid involves adding points to ensure balance, copying points at Z = 1 and Z = -1, adding midpoints, and placing more points along the minimal length spanning tree. The points in set S and T are connected by spanning trees, and additional points are added along the edges. The resulting point set X satisfies the conditions of the lemma, with S \u2282 S and T \u2282 T, and the length of each spanning tree not exceeding 2m. Every two points in S or T are connected by a path with distances less than 1, and grid points in X are at least distance 1 apart. In k-means clustering, Python's sklearn.cluster was used with default settings. Spectral clustering involved computing an affinity matrix and applying k-means to the eigenvectors. The loss was computed with a factor of 1/m for numerical stability. The architectures of Siamese net and SpectralNet are described in TAB2. Additional technical details are provided. The Siamese net and SpectralNet architectures are detailed in TAB2, with additional technical information in TAB3. Learning rate policy involved monitoring loss on a validation set, decreasing the rate by 10 when validation loss didn't improve for a specified number of epochs. Training stopped at a learning rate of 10^-8 after about 100 epochs for Siamese net and less than 20,000 parameter updates for SpectralNet on MNIST and Reuters datasets. Training set for Siamese net in MNIST experiments paired each data point with its two nearest neighbors. During spectral map training, the batch affinity matrix was constructed by connecting each point to its nearest two neighbors in Siamese distance. Scale \u03c3 in equation (6) was set to the median of distances from each point. In the Siamese net experiment, the scale \u03c3 was set globally to the median distance from any point to its 10th neighbor. Validation loss determined hyper-parameters, showing correlation to clustering accuracy in experiments with the MNIST dataset. The net architectures and learning rate policies were varied while keeping the Siamese net and Gaussian scale parameter \u03c3 constant. The correlation between validation loss and clustering accuracy in experiments was -0.771, indicating that spectral map learning hyper-parameters can be chosen based on validation loss. Learning rate schedule and stopping criterion were determined by the convergence of validation loss."
}