{
    "title": "S1xA7ILFOV",
    "content": "Optimal Transport (OT) is crucial in machine learning for handling cross-modality data, but its heavy computational burden limits its use. To address scalability, a framework called SPOT (Scalable Push-forward of Optimal Transport) is proposed, approximating the optimal transport plan using a reference distribution. The OT problem is then transformed into a minimax problem, allowing efficient solutions with primal dual stochastic gradient-type algorithms. SPOT also enables recovery of the optimal transport plan density using neural ordinary differential equations. Numerical experiments demonstrate SPOT's robustness, favorable convergence behavior, and efficient sampling capabilities for downstream applications. Efficiently sample from the optimal transport plan for downstream applications like domain adaptation, resource allocation, and economic benefit optimization. Optimal transport problems have a long history dating back to Monge (1781) and have gained increasing attention in machine learning applications. Optimal transport problems have a long history dating back to Monge (1781) and have gained increasing attention in various fields. The goal is to find an optimal joint distribution of two sets of data, X and Y, to minimize the expectation on a cost function. The optimal expected cost, known as Wasserstein distance, measures the discrepancy between the input data. The optimal transport plan aims to ensure identical marginal distributions of X and Y. In optimal transport problems, the Wasserstein distance measures the discrepancy between X and Y. To address scalability and efficiency, a new implicit generative learning-based framework is proposed to approximate the optimal transport plan \u03b3 * using a generative model mapping from latent variable Z to (X, Y). This approach estimates the mapping G between Z and (X, Y) by solving a specific equation. The proposed framework aims to approximate the optimal transport plan \u03b3 * by solving a minimax optimization problem using deep neural networks to approximate the Lagrangian multiplier. This approach allows for efficient solution of large minimax problems in machine learning and takes advantage of recent advances in deep learning. Deep neural networks can adapt to data with low dimensional structures efficiently. Generative models used in the framework allow for sampling from optimal transport plans, beneficial for domain adaptation. The framework can recover the density of entropy regularized optimal transport plans by modeling dynamics with neural Ordinary Differential Equations. The text discusses the evolution of density in optimal transport plans and the efficient recovery of density without extra parameters. It also provides background knowledge on optimal transport and implicit generative learning, including the concept of optimal transport and the Monge formulation. The Kantorovich formulation in optimal transport is a relaxation of the deterministic mapping, ensuring feasibility. Implicit generative learning addresses issues in direct estimation of probability density functions. Implicit generative learning is used to address issues in direct estimation of probability density functions. It involves transforming a latent random variable Z through an unknown mapping G to generate observed variable X. The generative model is trained by estimating G with a chosen loss function, making it easier than Maximum Likelihood Estimation (MLE). The distribution of G(Z) is referred to as the push-forward of a reference distribution \u03c1. This approach allows for efficient generation of new samples by choosing a convenient sampling distribution for \u03c1. Generative models like deep neural networks (DNNs) are used to represent the mapping G in implicit generative learning. Various DNN architectures have been successful in parameterizing G, leading to the development of complex generative models. While these methods may not directly estimate the density of the target distribution, they can recover the density of G(Z) in certain applications. Generative flow methods like NICE, Real NVP, and Glow impose sparsity constraints on weight matrices to compute densities layer by layer. NICE proposes a neural ODE approach to compute the transition from Z to G(Z) using DNNs with recursive structures. The text discusses using an ODE to describe the evolution of input Z towards output G(Z) in continuous time. A new framework called SPOT is proposed for solving the optimal transport problem efficiently. Notations and assumptions are introduced to find an optimal joint distribution \u03b3. The mapping G(\u00b7) is guaranteed to be reversible, and the density of G(Z) can be computed in O(d) time. The text introduces a new framework called SPOT for efficiently solving the optimal transport problem. It discusses the use of a mapping G from a latent variable Z with a distribution \u03c1 that is easy to sample. The method involves converting equations into a minmax optimization problem using Lagrangian multipliers. The approach aims to find the joint distribution \u03b3 with densities pX(x) and pY(y) for compact sets X and Y. The text introduces the SPOT framework for solving the optimal transport problem efficiently. It involves using a mapping G from a latent variable Z with a distribution \u03c1. The method converts equations into a minmax optimization problem using neural networks to parameterize G, \u03bbX, and \u03bbY. The equilibrium of the optimization problem may not exist due to unbounded Lagrangian multipliers. The text discusses the need for neural networks to be \u03b7-Lipschitz for parameterizing \u03bb X and \u03bb Y in the SPOT framework. It utilizes an alternating stochastic gradient algorithm to solve the optimization problem, with Spectral Normalization used to control the Lipschitz constant of \u03bb X and \u03bb Y. The algorithm in Algorithm 1 for the Mini-batch Primal Dual Stochastic Gradient Algorithm for SPOT efficiently approximates the spectral norm of W to avoid the computationally intensive SVD. It connects to Wasserstein Generative Adversarial Networks (WGANs) by viewing it as a multi-task learning version of WGANs, with G as a generator and \u03bb X and \u03bb Y as discriminators evaluating generated sample distributions. The text discusses a framework that approximates the Wasserstein distance between distributions using discriminators and a joint generator. It can be extended to multiple marginal distributions by solving a multi-marginal problem. The text introduces a parameterization method for solving the multi-marginal problem of equation 10, using neural networks. It highlights the scalability and efficiency of the framework, which only requires at most 2m neural networks. The entropy regularizer in optimal transportation can balance estimation bias and variance by controlling the smoothness of the density function. We efficiently recover the density p \u03b3 of the transport plan with entropy regularization using a neural ODE approach. The dynamics of the joint density p(t) are described in Proposition 1, where \u03be 1 and \u03be 2 are uniformly Lipschitz continuous in z and continuous in t. The log joint density satisfies an ODE where Jacobian matrices are involved. Proposition 1 directly follows Theorem 1. The joint density can be recovered efficiently, enabling computation of the entropy regularizer. The entropy regularized Wasserstein distance is the objective function. Updating the functional operator G involves the neural ODE and entropy regularizer. The training algorithm updates G by updating the parameter w using the gradient. The regularization coefficient is computed using the adjoint variable. The ODE method is used to efficiently compute equations. The SPOT framework is evaluated on tasks like Wasserstein distance approximation and domain adaptation using PyTorch on a GTX1080Ti GPU. The SPOT framework efficiently approximates the Wasserstein distance using PyTorch on a GTX1080Ti GPU. It demonstrates accurate approximation with Gaussian distributions and closed-form solutions, avoiding memory limitations of discretization-based algorithms. The SPOT framework approximates the Wasserstein distance using PyTorch on a GTX1080Ti GPU with 40 GB memory. G X, G Y, \u03bb X, and \u03bb Y are parameterized with neural networks. The latent variable Z follows a standard Gaussian distribution. SPOT and Regularized Optimal Transport (ROT) are compared with different regularization coefficients, showing SPOT converges to the true Wasserstein distance with only 0.6% error. SPOT converges to the true Wasserstein distance with minimal errors for different learning rates and network sizes, while ROT requires extensive tuning. The number of hidden units in the network varies from 2 to 210, affecting the parameters in G and \u03bb X, \u03bb Y. The tuning parameter \u03b7 is adjusted accordingly. SPOT is robust with respect to network size, as shown in FIG1. It effectively recovers joint density with entropy regularization using the neural ODE approach.\u03c6(a, b) is denoted as the density N(a, b). Marginal distributions \u00b5 are taken into account. In Section 4, \u03c6(a, b) represents the density N(a, b). Marginal distributions \u00b5 and \u03bd are Gaussian distributions. The ground cost is the Euclidean square function. The training algorithm runs for 6 \u00d7 10^5 iterations, generating 500 samples from \u00b5 and \u03bd in each iteration. \u03be is parameterized with a neural network, with a regularization coefficient affecting the smoothness of the joint density in the optimal transport plan. Entropy regularization in the optimal transport plan prevents degeneracy by enforcing smoothness of the density. SPOT can generate paired samples from unpaired data X and Y with different marginal distributions. Using the squared Euclidean cost, synthetic data is generated with specific parameters and sample sizes. FIG3 illustrates the input and generated samples with Gaussian distributions. The lower row in the generated samples shows X as a Gaussian distribution with specific mean and covariance, and Y as a combination of uniform and Gaussian distributions. The paired relationship is observed to be as expected, with upper mass transported to the upper region and lower mass to the lower region. SPOT is also capable of generating high-quality paired samples from unpaired real datasets like MNIST and MNISTM, leveraging semantic-aware cost functions. The text discusses the use of a semantic-aware cost function to extract handwritten letters' edges. Separate neural networks are used to parameterize G X and G Y, with detailed network settings provided. The framework is trained until generated samples become stable, as shown in FIG2. The results of CoGAN are also reproduced for comparison. SPOT outperforms CoGAN in generating paired images with better quality. SPOT's paired results have nearly identical contours, colorful foreground and background, while CoGAN's results lack clear paired relation and have limited colors. SPOT achieves this by leveraging ground cost c for paired relation, while CoGAN relies on sharing parameters. The framework is further tested on Photo-Monet and Edges-Shoes datasets with different cost functions. SPOT demonstrates superior performance in generating high-quality paired images compared to CoGAN. The generated images show a clear paired relation with identical contours and vibrant colors. SPOT leverages ground cost for paired relation, while CoGAN relies on parameter sharing. The framework is tested on Photo-Monet and Edges-Shoes datasets with different cost functions, showcasing its ability to tackle large-scale domain adaptation problems effectively. The proposed method, DASPOT, aims to identify coupling information between source and target data by training optimal transport plans and classifiers for X and Y. The classifiers consist of embedding and decision networks, denoted as D X = D e,X \u2022 D c,X and D Y = D e,Y \u2022 D c,Y. The proposed method, DASPOT, utilizes embedding and decision networks (D X = D e,X \u2022 D c,X and D Y = D e,Y \u2022 D c,Y) to extract high-level features from the source and target data. It then aligns X and Y using an optimal transport plan based on these features. The Wasserstein distance of the OT problem is calculated, and the networks are trained to minimize the empirical risk and cross-entropy loss functions. Joint training optimizes the objective function with tuning parameters \u03b7 s and \u03b7 da. DASPOT utilizes embedding and decision networks to align source and target data using optimal transport. It achieves equal or better performance compared to other methods on various datasets. DeepJDOT is less efficient than DASPOT due to solving optimal transport problems using Sinkhorn algorithm. DeepJDOT solves optimal transport problems using the Sinkhorn algorithm. Existing literature discusses stochastic algorithms for computing the Wasserstein distance between continuous distributions, but they only apply to the dual of the OT problem and cannot provide the optimal transport plan. BID19 expands dual variables in reproducing kernel Hilbert spaces and uses the Stochastic Averaged Gradient (SAG) algorithm for OT with continuous or semi-discrete marginal distributions. BID47 parameterizes dual variables with neural networks and uses Stochastic Gradient Descent (SGD) for better convergence. Our framework efficiently computes joint density from latent variable transformation. Architecture of discriminators \u03bb X , \u03bb Y is shown in TAB1. CNN architecture for USPS, MNIST, and MNISTM with PReLU activation. Generators G X and G Y architecture is shown in TAB2. Discriminators \u03bb X , \u03bb Y and classifiers D X , D Y architecture is shown in TAB3. TAB4 and TAB5 display the architectures of generators G X, G Y, discriminators \u03bb X, \u03bb Y, and classifiers D X, D Y. The last column in both tables indicates which groups share the same parameter. Residual block specifications are consistent with BID38."
}