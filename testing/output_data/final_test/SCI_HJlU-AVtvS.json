{
    "title": "HJlU-AVtvS",
    "content": "In this work, the spectra of the Conjugate Kernel (CK) and the Neural Tangent Kernel (NTK) are studied to understand neural networks' initial distribution and training properties. Eigenvalues analysis provides insights into network behavior at initialization and during training, addressing questions about network bias, feature complexity, and layer training effectiveness. Extensive experiments validate these insights. The computational tools developed for analyzing the spectra of CK and NTK lay a strong foundation for future studies of deep neural networks. The code for generating plots and analyzing neural network behavior has been open-sourced on github.com. Recent research suggests a \"simplicity bias\" in neural networks, where certain simple functions are sampled more frequently. This bias was observed in relu networks, raising questions about its universality across different architectures. The nonlinear nature of random networks poses challenges in reasoning about their distribution. However, treating networks with infinite width simplifies the analysis, as they are distributed as Gaussian processes. The Conjugate Kernels (CK) describe these networks well, with the simplicity bias of wide neural networks reflected in the spectrum of CK. This spectral perspective allows for probing the simplicity bias and more. The Neural Tangent Kernel is a recent advance that reveals how wide neural networks evolve like linear models under certain conditions. This kernel is different from the Conjugate Kernels (CK) associated with training only the last layer of a wide randomly initialized network. The CK reflects the simplicity bias of wide neural networks, providing insights into how hyperparameters affect generalization. The theory of the Neural Tangent Kernel was initially derived in the infinite width setting but has been confirmed to be predictive of finite width neural networks as well. The Neural Tangent Kernel theory explains how wide neural networks behave like linear models. Understanding how hyperparameters affect neural networks' performance is examined through the lens of NTK and its spectrum. Fractional variance is used to analyze the impact of minute changes in kernel on performance. Based on the Neural Tangent Kernel theory, deeper neural networks can learn more complex features but excessive depth can be detrimental. Training all layers is better for complex features, while training just the last layer is more suitable for simpler features. The spectral analysis shows that deeper networks increase fractional variance up to an optimal depth, beyond which it decreases. The fractional variances of \"complex\" eigenspaces in the NTK are larger than those in the CK. We use spectral theory to predict the maximal nondiverging learning rate of SGD. We verify our theory with experiments on various distributions and real data like MNIST and CIFAR10. The neural tangent kernel and conjugate kernel are briefly reviewed. The conjugate kernel in neural networks is associated with the embedding induced by random initialization. An MLP with specific widths, weight matrices, and biases is considered, with a focus on scalar output. The network is parametrized by the NTK parametrization, with recursive computation and hyperparameters \u03c3 w, \u03c3 b. The conjugate kernel in neural networks is associated with the embedding induced by random initialization. It can be shown that each hidden layer becomes a Gaussian process with zero mean and a specific kernel function in the limit of infinitely wide layers. The Neural Tangent Kernel corresponds to training the entire model, while the CK is the kernel of a linear model with features from the last layer. The Neural Tangent Kernel (NTK) corresponds to training the entire model, while the Conjugate Kernel (CK) is the kernel of a linear model with features from the last layer. In the limit of infinitely wide layers, each hidden layer becomes a Gaussian process with zero mean and a specific kernel function. The NTK can be computed using an inductive formula for the corresponding infinite-width kernel of the linear model. In specific cases like when \u03c6 = relu or erf, explicit formulas exist for evaluating CK and NTK efficiently. The CK governs the distribution of a randomly initialized neural network and the properties of training the last layer, while the NTK governs the dynamics of training the entire network. Studying their spectra reveals the implicit prior and bias of training neural networks. At initialization, a randomly initialized network follows a Gaussian process N(0, K) where K is the corresponding CK in the infinite-width limit. In the infinite-width limit, a randomly initialized network follows a Gaussian process N(0, K) with eigenvalues \u03bb i and eigenfunctions u i. Training the last layer via full batch gradient descent corresponds to Gaussian process inference. Similarly, training all parameters of the network yields the mean prediction of the GP N(0, NTK) in expectation. This reveals the implicit prior and bias of training neural networks. In the context of Gaussian process inference and training neural networks, the alignment between eigenvalues and coefficients of the ground truth function's expansion is crucial for generalization. Investigating settings where the ground truth function is an eigenfunction can help determine the effectiveness of different kernels. In this paper, the Neural Kernel of MLPs is explored, which takes the form of a kernel function acting as an integral operator on functions in various input spaces. The eigendecomposition of neural kernels as integral operators over different distributions is investigated, with a focus on the spectra of neural kernels on the boolean cube. The alignment between eigenvalues and coefficients of the ground truth function's expansion is crucial for generalization in Gaussian process inference and training neural networks. The neural kernel K on the boolean cube X = {\u00b11}^d is considered, where K is diagonalized by a Fourier basis {\u03c7_S} for any function f : d \u2192 R. The unique expansion of functions into multilinear polynomials on the boolean cube is discussed, with a focus on the spectra of neural kernels. The Fourier basis {\u03c7_S} diagonalizes the neural kernel K on the boolean cube X = {\u00b11}^d. The eigenfunctions of K are determined by dimensionality considerations. The shift operator T \u2206 sends functions over [\u22121, 1] to \u03a6(\u00b7 \u2212 \u2206). The eigenvalue can be expressed using \u00b5 k as in Theorem 3.1. The expression for \u00b5 k via Fourier series coefficients of \u03a6 is discussed in Appendix H.1. The case when X = \u221a dS d\u22121 is the radius-\u221a d sphere in R d with uniform measure is considered. The neural kernel K on the boolean cube X = {\u00b11}^d is diagonalized by the Fourier basis {\u03c7_S}. The eigenvalues of K are determined by dimensionality considerations. The eigenfunctions of K are determined by the shift operator T \u2206. The eigenvalues can be expressed using \u00b5 k as in Theorem 3.1. The isotropic Gaussian vector can be obtained by sampling its direction uniformly from the sphere and its magnitude from a chi distribution. The kernel K can be partially diagonalized into a sum of products between spherical harmonics and kernels on R equipped with standard isotropic Gaussian N (0, I). The neural kernel on the sphere and standard Gaussian can be partially diagonalized into a sum of products between spherical harmonics and kernels on R with a chi distribution. Complete eigendecompositions can be obtained in certain cases, such as when \u03a6 is positive homogeneous. The behavior of the kernel under these distributions is similar to that under a uniform distribution over the boolean cube in high dimensions. This similarity is expected due to the approximation of a high-dimensional standard Gaussian by a uniform distribution over a boolean cube. The eigenvalues of the integral operator on different distributions in high dimensions are approximately the same, as verified empirically and theoretically. Focusing on eigenvalues over the boolean cube is easier to compute compared to the sphere and standard Gaussian distributions. Computing boolean cube eigenvalues is more stable and efficient in high dimensions, while other methods may struggle with numerical stability. The eigenvalues of the integral operator on different distributions in high dimensions are approximately the same, as verified empirically and theoretically. Focusing on eigenvalues over the boolean cube is easier to compute compared to the sphere and standard Gaussian distributions. Computing boolean cube eigenvalues is more stable and efficient in high dimensions, while other methods may struggle with numerical stability. The procedure involves taking a linear combination of values of \u03a6 at grid points on [\u22121, 1], spaced apart by \u2206 = 2/d. The coefficients C d\u2212k,k r are efficient to compute, but the change in sign makes the procedure numerically unstable for large d. To address this, Eq. (5) is used to isolate the alternating part for evaluation in a stable manner. A network with 2 hidden layers and 40 neurons each is randomly initialized, and the function output is thresholded to obtain a boolean function sample. This process is repeated for 10^4 random seeds to obtain all samples, which are then sorted according to their empirical probability and plotted. The high values at the left of the relu curve indicate that a few functions are sampled repeatedly. The eigenvalues of the integral operator on different distributions in high dimensions are approximately the same, as verified empirically and theoretically. Focusing on eigenvalues over the boolean cube is easier to compute compared to the sphere and standard Gaussian distributions. When \u03a6 arises from the CK or the NTK of an MLP, all derivatives of \u03a6 at 0 are nonnegative. An improved algorithm for numerical stability involves computing the kth finite difference with a larger step size first, then computing the sum as in Eq. (8). The text discusses the bias of neural networks towards simple functions, showing that it depends on nonlinearity, sampling variances, and network depth. Experimental results with relu and erf networks are presented, revealing biases that disappear with increased sampling variances for erf networks. The eigenvalues of the Gaussian process kernel of random networks show that relu networks tend to be almost constant, while erf networks exhibit more variance with linear components. The variance in random networks comes mostly from degree 1 components, which decreases as the variance in weights increases. Eigenvalues become more evenly distributed as \u03c3 w increases, leading to a closer look at fractional variance of different degrees for relu and erf NTK. Bright regions in the heatmaps move up with increasing degree, except for an even/odd alternating pattern in erf NTK. The pattern for CKs is similar. The 32-layer erf network with \u03c3 2 w = 4 has all its nonzero eigenvalues equal, forming a \"white noise\" distribution on odd functions. Despite a weak simplicity bias in neural network-induced CKs and NTKs, eigenvalues show a nonincreasing pattern across even and odd degrees. This weak bias does not hold for the fraction of variance contributed by the degree k eigenspace. In this work, eigenvalues are computed over the 128-dimensional boolean cube for various hyperparameters, including degree, nonlinearity, depth, and variance. The impact of these hyperparameters on the kernels is analyzed, with trends summarized in the main text. The change in spectrum is primarily measured by the degree k fractional variance. Comprehensive contour plots are included in Appendix D. The text discusses the impact of hyperparameters on kernels, focusing on the degree k fractional variance. It shows that the best validation loss is inversely correlated with the fraction variance, but the relationship varies based on the degree and whether all layers are trained. Experimental details are provided in Appendix E. The text in curr_chunk discusses experimental details comparing training all layers vs training just the last layer, showing that in higher degrees, losses are high across all depths. The color of each dot indicates the degree of the ground truth polynomial, with training all layers being better in most cases. In higher degrees, training all layers is generally better than training just the last layer. However, this is not the case when learning constant functions. Experimental details for MNIST and CIFAR10 are provided in Appendices E and F, showing results over different input distributions. Choosing a kernel with maximized eigenvalues is important for polynomial learning. There is an inverse relationship between degree k fractional variance and validation loss, but no precise correlation. Future work aims to find a better measure for predicting generalization. The fractional variance of a fixed degree k converges to a fixed value as the input dimension d \u2192 \u221e. For CK or NTK of an MLP on a boolean cube d, K can be expressed as K(x, y) = \u03a6(x, y/d) for some analytic function \u03a6: R \u2192 R. As the input dimension d \u2192 \u221e, the fractional variance of degree k converges to a limit. For relu or erf MLP, the depth of the network helps increase the degree k fractional variance. Experimental focus is on the d = 128 case. In Fig. 2 (a) and (b), depth increases with degree k for relu and erf kernels. The depth effect is stronger for erf than relu. NTK tends to have higher variance for higher degrees than CK. Examples of fractional variance curves are shown for relu and erf CK and NTK. In Fig. 2 (a) and (b), depth increases with degree k for relu and erf kernels, with NTK showing higher variance for higher degrees than CK. The optimal depth for each degree k increases with k for relu NTK and odd degrees of erf NTK. The difference between maximal fractional variance and slightly suboptimal values decreases as k increases. The increase in optimal depth with degree is verified in Fig. 3 (b) by training relu networks of varying depth against a ground truth multilinear polynomial of varying degree. The optimal depth for neural networks increases with the degree of the ground truth multilinear polynomial. High depth can be detrimental beyond the optimal point, even in real distributions like MNIST and CIFAR10. This challenges the belief that increasing depth always improves expressivity. The fractional variance of NTK is higher than CK for large degrees, and vice versa for small degrees. Training only the last layer of a neural network (CK dynamics) is expected to learn simpler features better, while training all parameters (NTK dynamics) is expected to learn more complex features better. This is verified by training against ground truth functions of various homogeneous polynomials, showing that CK is more likely to be accurately approximated by low degree polynomials than NTK. In deep learning benchmarks, training the last layer of a neural network works better for simpler features, while training all layers is more effective for complex features. The fractional variance is not always indicative of performance, and learning rate tuning is crucial for pushing benchmarks. The spectral theory accurately predicts the maximal nondiverging learning rate over real datasets and toy input. The spectral theory accurately predicts the maximal nondiverging learning rate over real datasets and toy input distributions. It is important to set the correct upper limit for a learning rate search, with the maximum learning rate being determined by the eigenvalues of the kernel matrix. Training only the last layer or all layers of a neural network affects the complexity of features learned. In this work, the spectral theory accurately predicts the maximal learning rate for training on various datasets and input distributions. The study explores how hyperparameters influence the initial distribution and generalization properties of neural networks using neural kernels and their spectra. While the fractional variance heuristic provides rough predictions backed by experiments, there is room for refinement to improve test loss prediction accuracy. The spectral perspective is seen as a promising research direction to unravel mysteries in deep learning. The spectral theory accurately predicts the maximal learning rate for training on various datasets and input distributions, informing design choices in practice. Boolean cube theory predicts max learning rate for real datasets MNIST and CIFAR10, as well as over boolean cube 128. Theoretical predictions for max learning rate are highly accurate, with empirical validation for training only the last layer of an MLP. Theoretical predictions for max learning rate based on NTK of the MLP are highly accurate, with empirical validation showing correlation between theoretical and empirical max learning rates. Theoretical predictions underpredict, suggesting that half of the theoretical learning rate should lead to convergence with SGD. This discrepancy is expected due to the NTK limit being exact only in the large width limit. In deeper networks, higher learning rates are accepted than predicted by theoretical models. The Gaussian process behavior of neural networks has been studied extensively in various settings and architectures, leading to the development of new models. The Neural Tangent Kernel (NTK) is a recent discovery that has been applied in building new models or algorithms. It is closely related to the discussion of signal propagation in deep neural networks to prevent pathological behaviors. This line of work can be traced back to the Glorot and He initialization schemes for deep networks. The investigation of signal propagation in deep neural networks involves studying the infinite-depth limit of CK and NTK. Remarkable results include training a 10,000 layer CNN and the impact of batch normalization on gradient explosion. Random matrix theory is used to refine the signal propagation perspective by computing the singular value distribution of the input-output map in random neural networks. Other works explore neural network training and generalization from a random matrix perspective. Yang (2019) introduces Tensor Programs, a framework unifying various perspectives on neural networks such as GP, NTK, signal propagation, and random matrix views. It extends these perspectives to new scenarios like recurrent neural networks, enabling the computation of infinite-width limits through manipulation of computation graphs. Other works also adopt a spectral perspective on neural networks, with Rahaman et al. (2018) studying real Fourier frequencies of relu networks. The study focuses on the spectra of the CK and NTK, which inform the Fourier frequencies of neural networks. It complements previous research on relu networks learning low frequency components first. Karakida et al. (2018) analyze the spectrum of the Fisher information matrix, sharing nonzero eigenvalues with the NTK. Their work computes mean, variance, and maximum eigenvalues, while the current study provides all eigenvalues of the NTK. The study focuses on the spectra of the NTK and CK, providing all eigenvalues and eigenfunctions. Recent works have explored neural networks over the sphere, while this study focuses on the boolean cube to analyze hyperparameters' effects on spectra and generalization properties. The developed spectral theory led to observations that challenge the universality of simplicity bias and highlight the optimal depth for networks of fixed complexity. The study explores the effects of hyperparameters on spectra and generalization properties of neural networks over the boolean cube. It challenges the universality of simplicity bias and highlights the optimal depth for networks of fixed complexity. The observations are tested on distributions like the uniform distribution over the sphere, MNIST, and CIFAR10. The study examines the impact of hyperparameters on neural networks over different distributions. Optimal depths are observed for networks of varying complexity, with results shown in figures 6 and 7. The study does not test the hypothesis of increasing depth with degree for MNIST and CIFAR10 datasets. The study explores optimal depths for neural networks of varying complexity, showing in Fig. 7 that shallower or deeper networks perform worse. Training last layer vs all layers is compared for different polynomials in Fig. 8, revealing better performance for higher degree polynomials with all layers trained. In the study, the complexity of neural networks is explored through the concept of harmonic polynomial degree. While experiments were conducted on MNIST and CIFAR10 datasets, the idea of what constitutes a \"complex\" or \"simple\" feature is less clear. Preprocessing the data by centering and normalizing to the sphere resulted in accurate predictions of the max learning rate in practice. Different preprocessing methods such as PCA or ZCA showed varying levels of accuracy in predicting the max learning rate. An exception was observed with relu networks on PCA or ZCA preprocessed CIFAR10 data. The study explores neural network complexity using harmonic polynomial degree. Experiments on MNIST and CIFAR10 datasets show that validation loss is similar for degrees 0 to 2, but less precise for degree 3. Deeper or shallower networks than optimal depth result in increasing loss. SGD with learning rate 10 and batch size 256 was used for training. Best test error was recorded for each depth, with 10 random seeds used for variance estimation. Results show the best test error over training on CIFAR10 and MNIST datasets. The study explores neural network complexity using harmonic polynomial degree, showing that deeper or shallower networks than the optimal depth result in increasing loss. The best test error over training on CIFAR10 and MNIST datasets is demonstrated in the rows, with the best depth being around 5 when training all layers. Performance decreases for networks shallower or deeper than the optimal depth. Theoretical and empirical max learning rates are compared for training only the last layer and all layers, with different preprocessing procedures considered. The study compares theoretical and empirical max learning rates for different preprocessing procedures on neural networks trained on CIFAR10 and MNIST datasets. The results show that there is an optimal depth for learning features, with networks deeper or shallower than this depth resulting in increasing loss. The color contour plots indicate the optimal depth for learning features of a certain degree. The study involves training relu networks against a randomly generated ground truth multilinear polynomial using SGD with batch size 1000. The ground truth function is created by sampling monomials and coefficients, with hyperparameters including learning rate and ground truth degree. The study involves training relu networks against a randomly generated ground truth multilinear polynomial using SGD with batch size 1000. The hyperparameters include learning rate, ground truth degree, depth, activation function, and random seeds per hyperparameter combination. The maximum learning rate is half the theoretical maximum, and different setups are tested with varying parameters such as depth, activation function, and training of last layer weight and bias. The study involves training relu networks against a randomly generated ground truth multilinear polynomial using SGD with batch size 1000. The theoretical max learning rate is computed based on the last layer or all layers trained. For MNIST and CIFAR10, preprocessing involves centering and projecting images onto a sphere. In preprocessing, images are transformed using different schemes like \"no preprocessing,\" \"PCA128,\" and \"ZCA128.\" Target functions vary for boolean cube, sphere, Gaussian, MNIST, and CIFAR10 datasets. Parameters include depth, activation function, weight variance, bias variance, and width. Training can focus on the last layer (CK) or all layers (NTK). The curr_chunk discusses the behavior of units in a neural network as the width of the network approaches infinity. It mentions the application of the central limit theorem and Gaussian conditioning techniques to analyze the behavior of units in the network. The result is a generalization for almost any architecture. The NTK governs the evolution of neural network function under gradient descent in the infinite-width limit. It allows for quick evaluation of certain activation functions like relu or erf, enabling computation of CK and NTK in O(|X|2L) time. The parameters and neural network function evolve under continuous time gradient flow, visualized on a finite input space X = {x1, ..., xk}. The NTK governs the evolution of neural network function under gradient descent in the infinite-width limit, allowing for quick evaluation of activation functions like relu or erf. The parameters and neural network function evolve under continuous time gradient flow on a finite input space X = {x1, ..., xk}. The equations for kernel gradient descent with functional loss L(f) and kernel \u0398t are visualized, where the kernel \u0398t remains constant for infinitely wide MLPs. If training only the last layer, a similar equation holds for the CK \u03a3. The loss function is effectively with functional gradient when the training set Xtrain is much smaller than |X|. In the infinite width limit, the neural network function evolves linearly under gradient descent with the NTK or CK. The distribution of the function remains Gaussian for all t, with the posterior distribution described by specific formulas. Training only the last layer results in a Gaussian process with a certain mean and kernel variance, while training all layers leads to a different Gaussian process distribution. In the infinite width limit, the neural network function evolves linearly under gradient descent with the NTK or CK. The kernel of the Gaussian process posterior involves the NTK \u0398 and the CK \u03a3. Training the last layer or all layers of an MLP infinitely long yields the mean prediction of the GP inference. The theory of Hilbert-Schmidt kernels and eigenvalues/eigenfunctions are briefly reviewed. In the context of neural network function evolution under gradient descent with NTK or CK in the infinite width limit, the kernel of the Gaussian process posterior involves NTK \u0398 and CK \u03a3. The theory of Hilbert-Schmidt kernels states that if K is a symmetric positive semidefinite Hilbert-Schmidt kernel, there exists a sequence of eigenvalues and eigenfunctions important for training and generalization in machine learning. The eigenfunctions play a role in the shift operator T \u2206, with \u03a6 being an eigenfunction with eigenvalue e \u2212\u03ba\u2206. The text discusses the eigenfunction \u03a6 of the operator T \u2206 with eigenvalue e \u2212\u03ba\u2206, related to neural networks and Fourier series. It also touches on bounded variation functions and periodicity in Fourier analysis. The text discusses the eigenfunction \u03a6 of the operator T \u2206 with eigenvalue e \u2212\u03ba\u2206, related to neural networks and Fourier series. It also touches on bounded variation functions and periodicity in Fourier analysis. Function f : [\u22122, 2] \u2192 R that is periodic has a pointwise-convergent Fourier series. Theorem H.5 states that recovering the values of \u03a6 given the eigenvalues \u00b5 0 , . . . , \u00b5 d is possible. The matrix K can be recovered from eigenvalues \u00b5 0 , . . . , \u00b5 d. The operator T sends \u00b5 \u2022 \u2192 \u00b5 \u2022+1, and the matrix H.2 SPHERE is considered when X = \u221a dS d\u22121 is the radius-\u221a d sphere in R d. The text discusses spherical harmonics as eigenfunctions of the Laplace-Beltrami operator on the sphere. Zonal harmonics are a special class of spherical harmonics with a reproducing property. Zonal harmonics are a special class of spherical harmonics with a reproducing property. The polynomials used are Gegenbauer polynomials, and there is a unique zonal harmonic for each oriented axis and degree. Gegenbauer coefficients can be expressed via derivatives of \u03a6. The Gegenbauer coefficients and eigenvalues can be expressed via derivatives of \u03a6. As the dimension tends to infinity, the eigenvalues simplify to the derivatives of \u03a6. Theorem H.13 states that for an MLP on the sphere, the eigenvalues simplify to the derivatives of \u03a6. The isotropic Gaussian vector z \u223c N (0, I) can be sampled by independently sampling its direction v uniformly from the sphere S d\u22121 and sampling its magnitude r from a chi distribution \u03c7 d with d degrees of freedom. The spectral theorem states that a positive semidefinite Hilbert-Schmidt operator on L 2 (N (0, I)) exists if \u03a6 can be decomposed as specified. The positive definiteness of K depends on the positive semidefinite nature of all A l. Refer to Appendix I for a proof. The positive definiteness of K relies on all A l being positive semidefinite. K has an eigendecomposition under the standard Gaussian measure in d dimensions. For certain simple F, we can explicitly obtain {A l}. If K is degree-s positive-homogeneous, then \u03a6(t, q, q ) = (qq ) s\u03a6 (t) for some \u03a6 : [\u22121, 1] \u2192 R. Theorem H.16 states that for a function \u03a6 with Gegenbauer coefficients {a l}, the Gegenbauer expansion of \u03a6 can be defined. This expansion has an eigendecomposition under the standard Gaussian in R d. A common example where this theorem applies is when K is the CK of an MLP with relu activation functions. In general, K cannot be exactly diagonalized in a natural basis, but we can investigate the \"variance due to each degree of spherical harmonics\" by computing the coefficients of Gegenbauer polynomials. The text discusses the computation of coefficients of Gegenbauer polynomials to investigate the variance due to each degree of spherical harmonics. It also explores the convergence of random variables and the description of top eigenvalues of K over the standard Gaussian in the limit of large input dimension d. Additionally, conditions on the function \u03a6 are specified for the Taylor expansion. Theorem H.19 states that for an MLP with polynomially bounded activation function, the eigenvalue a l0 of K over N(0, Id) at spherical harmonics degree l is related to the operator A l. The proof involves showing that a l0 is bounded by a l and greater than b l. The eigenvalues of the kernel over different spaces are very close in high dimensions. The eigenvalues of the erf CK with specific parameters are plotted for various dimensions and degrees, showing convergence as dimension increases. In high dimensions, the eigenvalues of the kernel converge, with only the top eigenvalues being significant in practice due to limited training samples. In high dimensions, the eigenvalues of the kernel converge, with only the top eigenvalues being significant in practice due to limited training samples. The smaller eigenvalues of K are hardly detected in the training sample, and cannot affect the machine learning process much. For example, the boolean cube eigenvalues are very close to the sphere eigenvalues for k \u2264 5. It would require at least 275,584,033 samples to detect the eigenvalue \u00b5 6 and the possible difference between it and the sphere eigenvalue a 6. Comparatively, Imagenet has only about 15 million samples and an input dimension of 196608. In high dimensions, the eigenvalues of the kernel converge, with only the top eigenvalues being significant in practice due to limited training samples. The smaller eigenvalues of K are hardly detected in the training sample, and cannot affect the machine learning process much. It would require at least 275,584,033 samples to detect the eigenvalue \u00b5 6 and the possible difference between it and the sphere eigenvalue a 6. Comparatively, Imagenet has only about 15 million samples and an input dimension of 196608. The input dimension of d = 128 pales in comparison to Imagenet's 3 \u00d7 256 2 = 196608, CIFAR10 (d = 3 \u00d7 32 2 = 3072), and MNIST (d = 24 2 = 576). The effect of large d on eigenvalues is relevant, while keeping k small. The dimension of eigenspaces affected by limit theorems increases like \u0398(d k ), requiring an increasing number of training samples to see the difference in higher degrees k. Top k spectral closeness matters for fractional variance. In high dimensions, the eigenvalues of the kernel converge, with only the top eigenvalues being significant in practice due to limited training samples. The smaller eigenvalues of K are hardly detected in the training sample, and cannot affect the machine learning process much. It would require at least 275,584,033 samples to detect the eigenvalue \u00b5 6 and the possible difference between it and the sphere eigenvalue a 6. Comparatively, Imagenet has only about 15 million samples and an input dimension of 196608. The input dimension of d = 128 pales in comparison to Imagenet's 3 \u00d7 256 2 = 196608, CIFAR10 (d = 3 \u00d7 32 2 = 3072), and MNIST (d = 24 2 = 576). The effect of large d on eigenvalues is relevant, while keeping k small. The dimension of eigenspaces affected by limit theorems increases like \u0398(d k ), requiring an increasing number of training samples to see the difference in higher degrees k. Top k spectral closeness matters for fractional variance. Cor H.21 states that for any > 0, there is a k such that the total fractional variance of degree 0 to degree k sums up to more than 1 \u2212 , for the cube, the sphere, and the standard Gaussian simultaneously, when d is sufficiently large. The asymptotic fractional variance is completely determined by the derivatives of \u03a6 at t = 0. The eigenvalues of the kernel converge in high dimensions, with only the top eigenvalues being significant in practice. The smaller eigenvalues of K are hardly detected in the training sample. Valle-P\u00e9rez et al. (2018) conducted experiments on the {0, 1} d boolean cube, while the focus here is on the {\u00b11} d boolean cube. The NTK proof involves a product step where V \u03c6 (\u03a3 l\u22121 ) is multiplied with \u0398 l\u22121, resulting in a Taylor series with nonnegative coefficients. In contrast to experiments on the {0, 1} d boolean cube, this study focuses on the {\u00b11} d boolean cube. The relu network biases towards certain functions over both cubes, with erf and increasing \u03c3."
}