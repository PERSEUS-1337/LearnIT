{
    "title": "rJl-b3RcF7",
    "content": "Neural network pruning techniques can significantly reduce parameter counts, improving computational performance without sacrificing accuracy. The \"lottery ticket hypothesis\" suggests that randomly-initialized networks contain subnetworks capable of training effectively, leading to comparable test accuracy in fewer iterations. The algorithm identifies winning tickets in neural networks, supporting the lottery ticket hypothesis. These winning tickets are much smaller than the original networks but learn faster and achieve higher accuracy. Pruning techniques can reduce parameter counts by over 90%, improving efficiency in inference. Training smaller architectures uncovered by pruning can be harder and less accurate than the original networks. Retraining a pruned model performs better than training from scratch, indicating the challenge of training a network with reduced capacity. During retraining, it is more effective to keep the weights from the initial training phase for surviving connections than to re-initialize pruned layers. Gradient descent performs well when the network is initially trained but struggles after re-initializing and retraining some layers. The speed of network learning is measured by the iteration at which early-stopping would end training, based on the minimum validation loss. Training smaller pruned architectures can be challenging and less accurate than the original networks. The study explores architectures for CIFAR10, showing smaller subnetworks that train from the start and learn as fast as larger counterparts. The Lottery Ticket Hypothesis suggests that a randomly-initialized dense neural network contains a subnetwork that can match the test accuracy of the original network after training for the same number of iterations. The study investigates the Lottery Ticket Hypothesis, which proposes that a randomly-initialized neural network contains a subnetwork that can achieve the same test accuracy as the original network. This subnetwork, called a winning ticket, can be uncovered through standard pruning techniques in fully-connected and convolutional feed-forward networks. The study explores the Lottery Ticket Hypothesis, revealing that smaller networks do not train effectively unless appropriately initialized. Winning tickets are identified by pruning smallest-magnitude weights, resetting each unpruned connection's value to its original initialization. Central experiments involve various network architectures and pruning rates. The study investigates the Lottery Ticket Hypothesis, showing that smaller networks require proper initialization to train effectively. Winning tickets are identified by pruning smallest-magnitude weights and resetting unpruned connections to their original initialization values. Various network architectures and pruning rates are tested in the experiments. The study explores the Lottery Ticket Hypothesis, revealing that pruning uncovers trainable subnetworks that achieve comparable test accuracy to the original networks. Pruning also identifies winning tickets that learn faster and generalize better, supporting the new perspective on neural network composition. The study explores the Lottery Ticket Hypothesis, revealing that pruning uncovers trainable subnetworks that achieve comparable test accuracy to the original networks. Winning tickets can improve training performance, design better networks, and enhance theoretical understanding of neural networks. In this study, the lottery ticket hypothesis is assessed on fully-connected networks trained on MNIST using the Lenet-300-100 architecture. A layer-wise pruning heuristic is applied to remove weights with the lowest magnitudes, with connections to outputs pruned at a lower rate. Various hyperparameters are explored, including learning rates, optimization strategies, initialization schemes, and network sizes. Test accuracy on Lenet is shown in Figure 3, with each curve representing the average of five trials. Iterative pruning of winning tickets in fully-connected networks trained on MNIST using the Lenet-300-100 architecture shows that networks learn faster and reach higher test accuracy when more weights are pruned. A winning ticket comprising 51.3% of the original network's weights reaches higher test accuracy faster than the original network but slower than when only 21.1% of weights are pruned. Learning slows when less than 21.1% of weights are pruned, and a winning ticket with 3.6% of the weights regresses to the original network's performance. When iteratively pruning by 20% per iteration, the winning tickets learn faster as the percentage of weights remaining decreases. Early-stopping occurs 38% earlier than the original network when Pm is at 21%, with test accuracy improving by more than 0.3 percentage points at 13.5% pruning. However, further pruning beyond this point causes a decrease in accuracy. At early stopping, training accuracy increases with pruning, implying winning tickets optimize effectively but do not generalize better. Iteratively-pruned winning tickets show test accuracy improvement despite reaching 100% training accuracy. Random reinitialization tests winning ticket's initialization importance. The importance of initialization for winning tickets is highlighted in experiments where randomly reinitialized networks learn slower and lose test accuracy compared to winning tickets. The average reinitialized iterative winning ticket's test accuracy drops off faster than the original accuracy when pruning. When Pm = 21%, the winning ticket reaches minimum validation loss 2.51x faster and is half a percentage point more accurate. Networks reach 100% training accuracy for Pm \u2265 5%. Winning tickets generalize better than randomly reinitialized ones, supporting the lottery ticket hypothesis. Original initialization benefits from pruning, while random reinitialization's performance diminishes steadily. One-shot pruning can identify winning tickets without repeated training, showing improved results in test accuracy compared to the original network. However, iteratively pruned winning tickets learn faster and achieve higher test accuracy at smaller network sizes. The focus is on iterative pruning to find the smallest winning tickets in convolutional networks. Applying the lottery ticket hypothesis to convolutional networks on CIFAR10, the study explores Conv-2, Conv-4, and Conv-6 architectures with varying complexity and network sizes. The iterative pruning process reveals that smaller winning tickets learn faster and achieve higher test accuracy in convolutional networks. Lenet in Section 2 repeats that as the network is pruned, it learns faster and test accuracy rises compared to the original network. Results show that winning tickets reach minimum validation loss faster for Conv-2, Conv-4, and Conv-6 architectures. Test accuracy also improves significantly for these architectures. Test accuracy improves for Conv-2, Conv-4, and Conv-6 architectures when Pm is above 2%. Training accuracy reaches 100% at specific iterations for all networks with Pm \u2265 2%. Winning tickets show smaller gap between test and training accuracy, indicating better generalization. Random reinitialization experiment shows networks take longer to learn. The networks take longer to learn upon continued pruning. Test accuracy drops off more quickly for random reinitialization experiments. Dropout improves accuracy by randomly disabling units on each training iteration. The lottery ticket hypothesis suggests that dropout and finding winning tickets may interact. The results show that dropout and finding winning tickets interact positively. Training with dropout increases initial test accuracy and iterative pruning further improves it. Learning becomes faster with iterative pruning, especially for Conv-4 and Conv-6. The study explores the interaction between dropout and iterative pruning in deep convolutional networks like VGG-19 and Resnet-18. The results suggest that dropout-induced sparsity may prime a network for pruning, making winning tickets easier to find. Techniques targeting weights or per-weight dropout probabilities could further enhance this process. The networks are trained with batchnorm, weight decay, decreasing learning rate schedules, and augmented training data. In experiments with deep convolutional networks like VGG-19 and Resnet-18, the study explores the interaction between dropout and iterative pruning. The results suggest that dropout-induced sparsity may facilitate finding winning tickets. Techniques targeting weights or per-weight dropout probabilities could enhance this process. The networks are trained with batchnorm, weight decay, decreasing learning rate schedules, and augmented training data. Global pruning is used on different architectures, with slight modifications for deeper networks like Resnet-18 and VGG-19. Explanation for the behavior of deeper networks like VGG-19 is due to uneven distribution of parameters across layers. Global pruning is used to avoid bottlenecks in smaller layers. Results of iterative pruning and random reinitialization on VGG-19 are shown, with different learning rates impacting the discovery of winning tickets. At lower learning rates, subnetworks maintain accuracy close to the original but do not match it. When randomly reinitialized, subnetworks lose accuracy as they are pruned. Linear learning rate warmup is explored to bridge the gap between lottery ticket behavior and accuracy advantage in VGG-19 training. Training VGG-19 with warmup at a learning rate of 0.1 improves test accuracy by about one percentage point. Resnet-18, a 20 layer convolutional network, is trained with SGD with momentum and iterative pruning. Results show winning tickets at lower learning rates but not higher ones. Best winning tickets achieve 89.5% accuracy. The best winning tickets at lower learning rates achieve 89.5% accuracy, falling short of the original network's 90.5% accuracy at higher learning rates. Winning tickets trained with warmup close the accuracy gap with the unpruned network at higher learning rates. Existing work on neural network pruning shows that a neural network can often be represented with fewer parameters. Neural network pruning involves training the original network, removing connections, and fine-tuning to create a sparse network. The lottery ticket hypothesis suggests that trainable subnetworks exist within architectures. Winning tickets require proper initialization for successful learning, as reinitialization leads to slower learning and lower accuracy. The lottery ticket hypothesis suggests that winning ticket weights may move further than other weights, indicating the benefit of initialization is linked to optimization algorithms, datasets, and models. Contrary to conventional wisdom, pruned networks can be trainable when randomly reinitialized, as shown in experiments on VGG-19. Our experiments confirm that highly overparameterized networks can be pruned successfully up to a certain level of sparsity. However, beyond this point, extremely pruned networks only maintain accuracy with specific initialization. The structure of winning tickets encodes an inductive bias customized to the learning task. The inductive bias embedded in deep network structure determines data separation efficiency. Winning tickets generalize better, surpassing original network test accuracy. Pruning leads to an Occam's Hill effect, balancing complexity for improved generalization. Compact hypotheses are known to better generalize. The lottery ticket hypothesis suggests that larger neural networks may contain simpler representations, leading to better generalization. Winning tickets, which are pruned networks, can achieve high accuracy with fewer parameters. Overparameterized networks trained with SGD can converge to global optima. SGD can optimize neural networks to global optima. The presence of winning tickets may not be necessary for optimization. Overparameterized networks are easier to train due to more potential winning ticket subnetworks. Vision-centric tasks on smaller datasets are considered, larger datasets are not investigated due to computational intensity. Sparse pruning is the method used to find winning tickets. Future work aims to explore more efficient methods for studying the lottery ticket hypothesis in resource-intensive settings. In future work, the study aims to explore structured pruning and non-magnitude pruning methods to optimize network architectures for modern hardware. The winning tickets found have unique initializations that enable them to match the performance of unpruned networks at smaller sizes. Additionally, the properties of these initializations and the inductive biases of pruned network architectures are being studied for their impact on learning capabilities. In future work, the study plans to explore why warmup is necessary for iterative pruning to find winning tickets in deeper networks. The overparameterization of neural networks allows for distillation and pruning techniques to reduce parameters while maintaining accuracy. Dense networks contain sparse subnetworks capable of learning on their own from their original initializations. Sparse, trainable networks can exist within larger networks, as demonstrated by the research. Techniques like Distillation and pruning are used to train small networks efficiently. Researchers have successfully trained networks by restricting optimization to a small subspace of the parameter space. Winning tickets are found through a principled search process involving pruning. Recent research has focused on training small networks to mimic the behavior of larger networks through techniques like Distillation and pruning. Pruning, which reduces the size of image-recognition networks, is central to these experiments. Additionally, dropout techniques have been shown to approximate Bayesian inference in Gaussian processes. In 2016, techniques were developed to prune and sparsify networks during training by adjusting dropout probabilities for weights. The iterative pruning strategy involves training a neural network, pruning parameters, and resetting weights to find winning tickets. The iterative pruning strategy involves training a neural network, pruning parameters, and resetting weights to find winning tickets. Two strategies are discussed: one resets the weights back to their initial values before retraining, while the other retrains using the already-trained weights after each round of pruning. In the iterative pruning strategy, weights are reset to initializations after pruning. Strategy 1 maintains higher validation accuracy and faster early-stopping times. Validation and test loss decrease early, reach a minimum, then increase as the model overfits. The validation loss decreases early, reaches a minimum, and then starts increasing again in iterative pruning experiments using Lenet, Conv-2, Conv-4, and Conv-6 architectures. The early-stopping criterion identifies the point where the validation loss forms a clear bottom, indicating faster learning for networks that reach this moment sooner. In experiments with Lenet, Conv-2, Conv-4, and Conv-6 architectures, the early-stopping criterion identifies faster learning for networks that reach a clear bottom in validation loss sooner. This is supported by the order in which experiments meet the early-stopping criterion aligning with the order they reach a specific test accuracy threshold. The Appendix accompanies Figures 4 and 5 in the main paper, showing early-stopping iterations and test accuracy for different architectures. The Appendix includes additional graphs of training accuracy at the end of the training process, comparing winning tickets and randomly sparse networks. Training accuracy reaches 100% for most networks, with winning tickets maintaining 100% accuracy longer than randomly reinitialized networks. In the Appendix, a comparison is made between networks found via iterative pruning with original initializations, randomly reinitialized networks, and random sparse subnetworks. Results show that randomly reinitialized networks outperform random sparsity for fully-connected Lenet architecture for MNIST, but there is no significant difference for convolutional networks. The difference is attributed to the nature of the input data, where certain parts of the MNIST images contain more valuable information for classification. In this Appendix, the structure of winning tickets from the Lenet architecture trained on MNIST is examined to understand why they can learn effectively despite heavy pruning. The distributions of winning ticket initializations for different levels of pruning are shown in Figure 15, representing the weights of connections that survived the pruning process. The distributions for each layer closely mirror those aggregated from multiple trials of the lottery ticket experiment. The histograms in Figure 15 display the normalized initialization distributions for unpruned networks. As the network is pruned, the second hidden layer and output layer become increasingly bimodal, with asymmetric peaks on either side of 0. Surviving connections tend to have higher magnitude-initializations. The winning tickets are identified by pruning connections with the lowest magnitudes in each layer. The input layer maintains its distribution, while the bimodal distributions are present across all layers. Connections with higher-magnitude initializations are more likely to survive pruning, resulting in bimodal distributions with peaks on opposite sides of 0. The winning tickets are identified by pruning connections with the lowest magnitudes in each layer, with the first hidden layer favoring negative initializations and the second hidden layer and output layer favoring positive initializations. Randomly reinitializing winning tickets from different distributions does not improve performance. The graphs show that low-magnitude weights that are pruned from the beginning may not be important to the network. Pruning before training results in worse performance for winning tickets compared to iterative selection. Pruning is done before training the network, followed by training with adam. The experiment shows the distribution of weight magnitudes for winning tickets compared to non-winning tickets. Initial and final weights of the unpruned network are examined to identify properties of winning ticket weights. The study examines the difference in weight changes between winning tickets and the rest of the network. Winning ticket weights tend to change by a larger amount, suggesting they are not already close to the optimum. This distinction may be due to the placement of winning tickets in the optimization landscape. The study explores the difference in weight changes between winning tickets and the rest of the network, indicating that winning ticket weights tend to change more significantly. This suggests that winning tickets may not be close to the optimum and are biased towards weights with higher magnitudes. This could potentially allow for more efficient methods of finding winning tickets earlier in the training process. The study examines the connectivity of winning tickets in neural networks. It is found that each unit retains incoming connections proportional to the amount of pruning. The output layer retains more connectivity due to being pruned at a slower rate. Outgoing connections show differences between adam and SGD-trained networks. The study explores the connectivity of winning tickets in neural networks, revealing variations in outgoing connections between adam and SGD-trained networks. The input layer shows uneven distributions of outgoing connectivity, with certain features being more useful to the network than others. In this Subsection, the study investigates the robustness of winning tickets in neural networks to Gaussian noise added to their initializations. Adding noise reduces test accuracy and slows learning, emphasizing the importance of the original initialization. Despite decreased accuracy with more noise, winning tickets show surprising resilience. The study explores the robustness of winning tickets in neural networks to Gaussian noise added to their initializations. Even with noise up to 3\u03c3, winning tickets maintain their performance advantage over random reinitialization. The experiment considers different noise levels and hyperparameters for the Lenet architecture. The fully-connected Lenet architecture with two hidden layers and a ten unit output layer is evaluated on the MNIST dataset. A 5,000-example validation set is randomly sampled from the training set, with the remaining 55,000 examples used for training. Hyperparameter selection experiments are conducted using the validation set, while the accuracy of the networks in the main paper is evaluated on the test set. Training is done in mini-batches of 60 examples per epoch. The network is trained in mini-batches of 60 examples per epoch, with the entire training set shuffled at each epoch. Each graph line represents data from three experiments, showing average performance with error bars indicating minimum and maximum results. The lottery ticket experiment is conducted iteratively with a 20% pruning rate per iteration (10% for the output layer), and each network layer is pruned independently. Training consists of 50,000 iterations per iteration, with early-stopping determined retroactively based on validation performance. Validation and test data are not considered during training. The paper evaluates validation and test performance every 100 iterations using the Adam optimizer and Gaussian Glorot initialization. Hyperparameters are chosen to be generic to minimize dependence on hand-chosen values. The learning rate for Adam is selected in this Appendix, along with consideration of other hyperparameters like optimization algorithms and initialization strategies. In this appendix, the iterative lottery ticket experiment was conducted on the Lenet architecture trained with MNIST using stochastic gradient descent at various learning rates. The experiment involved different network sizes and pruning strategies, with the data collected from training variations of the Lenet architecture over 3,000 times. The learning rate for Adam was selected based on minimizing training iterations on the unpruned network. The learning rate should minimize training iterations and maximize validation accuracy on the unpruned network. It should also match the early-stopping iteration and accuracy of the original network with as few parameters as possible during the iterative lottery ticket experiment. The learning rate should be conservative to optimize heavily pruned networks effectively. The iterative lottery ticket experiment with the Lenet architecture trained with MNIST using stochastic gradient descent with momentum at various learning rates. Learning rates between 0.0002 and 0.002 achieve similar validation accuracy levels on the original network and maintain performance as the network is pruned. Among these rates, 0.0012 and 0.002 have the fastest early-stopping times and maintain them to smaller network sizes. 0.0012 is chosen for its higher validation accuracy on the unpruned network. The lottery ticket pattern persists across all learning rates. The lottery ticket experiment with the Lenet architecture trained with MNIST using stochastic gradient descent (SGD) at various learning rates shows that even rates that do not meet early-stopping criteria still improve accuracy with pruning. SGD rates of 0.4 and 0.8 reach early-stopping in a similar number of iterations as the best Adam rates but maintain performance when the network is pruned further. The lottery ticket experiment with Lenet architecture trained with MNIST using SGD at different learning rates shows that even non-early-stopping rates improve accuracy with pruning. Learning rates between 0.025 and 0.1 maintain high validation accuracy and faster learning for more pruning iterations. Learning rate 0.025 achieves the highest validation accuracy on the unpruned network but decreases gradually as it is pruned, while higher rates reach early-stopping faster. After training Lenet with iterative lottery ticket experiment, pruning each layer separately at varying rates, it was found that iterative pruning yields smaller winning tickets than one-shot pruning. Different values of pruning rate (k) were explored, showing that lower rates (0.1 and 0.2) lead to higher validation accuracy and faster learning compared to higher rates (0.4 and above). This difference is evident in early-stopping time and validation accuracy. Pruning rates of 0.2 lead to higher validation accuracy and faster learning in Lenet experiments. Output layer is pruned at half the rate of the rest of the network due to its smaller size. Gaussian Glorot BID13 initialization scheme is used for the network. The Lenet architecture was initialized from Gaussian distributions with various standard deviations and optimized with Adam. The lottery ticket pattern persisted across all standard deviations, with the network achieving high validation accuracy and low early-stopping times when initialized with a standard deviation of 0.1. The experiment on the Lenet architecture with different layer sizes showed consistent performance with Gaussian Glorot initialization and Adam optimization. The Lenet architecture with different layer sizes maintained a 3:1 ratio between units in the first and second hidden layers. The lottery ticket hypothesis raises questions about network size and the search for \"better\" winning tickets in larger networks. When evaluating winning tickets based on accuracy, larger networks tend to find better tickets as shown in Figure 31. Larger networks may have an advantage due to containing sparse subnetwork configurations that smaller networks cannot express. However, when considering the time needed to reach early-stopping, larger networks do not have a significant advantage. The left graph in Figure 31 shows that winning tickets from larger networks learn marginally faster than those from smaller networks, but differences are slight. Regardless of initial size, winning tickets return to original network accuracy when pruned to 9,000-15,000 weights. This Appendix complements Sections 3 of the main paper, exploring optimization algorithms and hyperparameters. The curr_chunk discusses the optimization algorithms and hyperparameters for the Conv-2, Conv-4, and Conv-6 architectures, which are variants of the VGG network architecture scaled down for the CIFAR10 dataset. The networks consist of modules with convolutional filters and fully-connected layers, similar to VGG. The first module has 64 convolutions in each layer, the second has 128, and the third has 256. The Conv-2, Conv-4, and Conv-6 architectures have 1, 2, and 3 modules, respectively. The CIFAR10 dataset includes 50,000 training examples and 10,000 test examples. A 5,000-example validation set is randomly sampled from the training set. The training set consists of 45,000 examples used for the rest of the paper. Hyperparameter selection experiments are evaluated on the validation set, while examples in the main body are evaluated on the test set. Training is done in mini-batches of 60 examples, with the entire training set shuffled at each epoch. The Conv-2, Conv-4, and Conv-6 networks are initialized with Gaussian Glorot initialization and trained for a specified number of iterations. The training iterations for BID13 networks are optimized with Adam, and the learning rate is selected for each network. Validation and test performance are measured every 100 training iterations, with each line in the graphs representing the average of three experiments. Error bars show the range of values. In the lottery ticket experiment, convolutional layers and fully-connected layers are pruned at different rates. The experiment is conducted on Conv-2, Conv-4, and Conv-6 architectures optimized with Adam at various learning rates to select the best rate for training iterations and accuracy. The lottery ticket experiment is conducted on Conv-2, Conv-4, and Conv-6 architectures with different pruning rates for convolutional and fully-connected layers. The learning rate of 0.0002 is selected for Conv-2, leading to high validation accuracy and fast early-stopping times. The learning rate of 0.0003 is selected for Conv-4, showing high initial validation accuracy, maintaining accuracy and fast early-stopping times when pruned, with improvements in both validation accuracy and early-stopping time. The learning rates for different Conv layers show varying effects on validation accuracy and early-stopping times. For Conv-6, a learning rate of 0.0003 is chosen for similar reasons as Conv-4, resulting in improved validation accuracy and faster early-stopping times. Learning rates of 0.0004 and 0.0002 also have their own trade-offs between accuracy and early-stopping times. The lottery ticket pattern holds for most learning rate combinations, except at very high rates where early-stopping times briefly decrease and accuracy briefly increases. This behavior is similar to the results observed in Section 4, where the iterative pruning algorithm fails to find winning tickets. The behavior of the lottery ticket experiment is explored for Conv-2, Conv-4, and Conv-6 networks optimized with stochastic gradient descent at various learning rates. The Conv-2, Conv-4, and Conv-6 networks were challenging to train with SGD and Glorot initialization. SGD learning rates did not match the validation accuracy of Adam-trained networks. Gradients tended to explode at higher learning rates, while networks failed to learn at lower rates. Winning tickets were found at all depicted learning rates. Pruning initially decreased early-stopping times before increasing again, similar to other lottery ticket experiments. Conv-6 network showed accuracy patterns consistent with other experiments. The Conv-2 and Conv-4 architectures showed a unique validation accuracy pattern in the iterative lottery ticket experiment, initially declining with pruning before eventually surpassing the accuracy of the unpruned network. The pruned networks reached early-stopping in about the same or fewer iterations than the original network, indicating a winning ticket. The lottery ticket experiment showed that pruning Conv-2 and Conv-4 networks led to early-stopping in fewer iterations, indicating a winning ticket. The pattern persisted with SGD momentum optimization, except at very low learning rates. Pruning Conv-2 and Conv-4 networks with different learning rates led to increased accuracy levels compared to unpruned networks. Pruning convolutional layers more slowly allowed for further model pruning while maintaining performance. The iterative lottery ticket experiment was performed on Conv-2, Conv-4, and Conv-6 with different pruning rates to find the lowest parameter-count models while maintaining validation accuracy. Pruning convolutional layers more slowly allowed for further model pruning while maintaining performance. The iterative lottery ticket experiment was conducted on Conv-2, Conv-4, and Conv-6 with various pruning rates. The selected rates were 10% for Conv-2, 10% for Conv-4, and 15% for Conv-6. Training with dropout required adjusting learning rates, and the networks were trained for longer iterations. The models were iteratively pruned at the determined rates. The iterative lottery ticket experiment was conducted on Conv-2, Conv-4, and Conv-6 architectures with various pruning rates. The networks were pruned iteratively at determined rates, and the experiment showed early-stopping iteration and validation accuracy for each architecture. Unstable lines with large error bars indicated failed experiments with low accuracy and high early-stopping times. The Conv-2 network had difficulty training consistently with dropout, showing wide error bars and low average accuracy in the lottery ticket experiments. Learning rates of 0.0001 and 0.0002 had trials that failed to learn until later pruning rounds, while at 0.0003 all networks learned productively at every pruning level. At learning rate 0.0004, one network occasionally failed to learn. We selected learning rate 0.0003, which allowed networks to learn most often while achieving high initial accuracy. Networks that initially couldn't learn at 0.0001 eventually did after multiple rounds of the lottery ticket experiment. Investigating whether pruning or training provided useful information for learning is important. A slower learning rate of 0.0002 led to the highest accuracy for Conv-4 and Conv-6 architectures. A slower learning rate of 0.0002 leads to the highest accuracy on unpruned networks in the lottery ticket experiment. With dropout, the unpruned Conv-4 architecture reaches 77.6% validation accuracy, a 2.7 percentage point improvement over the network trained without dropout. Dropout-trained winning tickets reach 82.6% accuracy when pruned to 7.6%. Early-stopping times improve by up to 1.58x with dropout. The unpruned Conv-6 architecture reaches 81.3% accuracy with dropout, a 2.2 percentage point improvement. The dropout-trained winning tickets in the lottery ticket experiment show an average validation accuracy of 84.8% when pruned to 10.5%, with a 1.5x average improvement in early-stopping times. The lottery ticket pattern generally holds for accuracy across different learning rates, with improvements as networks are pruned. However, not all learning rates exhibit decreases in early-stopping times. The effect of pruning convolutions and fully-connected layers on test accuracy and learning speed is shown in FIG3. Pruning convolutions alone leads to higher test accuracy and faster learning, while pruning fully-connected layers alone generally causes test accuracy to worsen and learning to slow. However, pruning convolutions alone has limited ability to reduce the overall parameter-count of the network. The iterative lottery ticket experiment was conducted on Conv-2, Conv-4, and Conv-6 architectures with dropout and Adam optimizer at various learning rates. Different pruning schemes were used for Lenet/Conv-2/4/6 and VGG-19/Resnet-18 networks. Comparison of global and layer-wise pruning methods was done for VGG-19 and Resnet-18 architectures. When training VGG-19 with learning rate 0.1 and warmup to iteration 10,000, winning tickets are found with P m \u2265 6.9% for layer-wise pruning compared to P m \u2265 1.5% for global pruning. Accuracy drops off sooner for layer-wise pruning than for global pruning. Global pruning discovers smaller winning tickets than layer-wise pruning for Resnet-18, but the difference is less extreme than for VGG-19. The rationale for the efficacy of global pruning on deeper networks is discussed in Section 4, where layers with vastly different numbers of parameters can become bottlenecks when pruning layer-wise. Regardless of the pruning method used, the patterns from Section 4 remain consistent at learning rate 0.1. When training VGG-19 with learning rate 0.1 and warmup to iteration 10,000, winning tickets are found with P m \u2265 6.9% for layer-wise pruning compared to P m \u2265 1.5% for global pruning. The VGG19 architecture was first designed by Simonyan & Zisserman (2014) for Imagenet and adapted by Liu et al. (2019) for CIFAR10. The network architecture includes convolutional layers followed by max-pooling and average pooling. Training procedures for resnet18 are followed, including data augmentation, batch normalization, weight decay, and three stages of training with decreasing learning rates. The network is pruned at a rate of 20% per iteration, with exceptions for the output layer. VGG19 is trained with different learning rates. The Resnet-18 architecture, introduced by He et al. (2016), consists of 20 layers including convolutional layers, average pooling, and a fully-connected output layer. The training process involves data augmentation, batch normalization, weight decay, and three stages of training with decreasing learning rates. The training set is divided into 45,000 training examples and 5,000 validation examples, with random flips and pads/crops used for data augmentation. A batch size of 128 is used with SGD and momentum (0.9), and weight decay of 0.0001 is applied. The Resnet-18 architecture, introduced by He et al. (2016), consists of 20 layers including convolutional layers, average pooling, and a fully-connected output layer. The training process involves data augmentation, batch normalization, weight decay, and three stages of training with decreasing learning rates. Our iterative pruning experiments involve stages lasting 20,000, 5,000, and 5,000 iterations each, with a global pruning rate of 20% per iteration. We use Gaussian Glorot initialization and observe that winning tickets are found at a lower learning rate of 0.01, but not at the typical rate of 0.1. Adding linear warmup to the initial learning rate enables finding winning tickets for VGG-19 and Resnet-18 at higher learning rates. Iterative pruning can identify winning tickets when the network is trained at the original learning rate of 0.1 for VGG-19 and 0.03 for Resnet-18. For VGG-19 and Resnet-18, using a learning rate of 0.1 and 0.03 respectively, the accuracy of winning tickets increases with higher values of k. The accuracy improves rapidly as k increases up to 5000, but shows diminishing returns beyond that. In Section 4 experiments, k = 20000 achieves the highest validation accuracy for VGG-19, while k = 10000 is selected for Resnet-18 as larger values do not provide significant benefits."
}