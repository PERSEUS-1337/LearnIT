{
    "title": "HJgCcCNtwH",
    "content": "The text discusses the challenges of long training times in deep neural networks and the potential solution of training sparse networks. It introduces a theoretical foundation for choosing the intra-layer topology in sparse networks and presents a new initialization scheme for exploring deep sparse networks. The text also evaluates different topologies and highlights the importance of selecting the right topology for achieving high accuracy. The text discusses the challenges of long training times in deep neural networks and the potential solution of training sparse networks. It introduces a data-free heuristic for evaluating network topology independently from the dataset. Research is focused on accelerating training to speed up research iteration and explore DNN architectures using evolutionary algorithms. Network training is also shifting towards edge devices. The memory requirements of dense, convolutional, and recurrent layers grow quadratically with layer information bandwidth, making DNN training impractical without batching. Batching can decrease model accuracy, especially when scaling training on large clusters. Larger models in off-chip memory become dominant energy costs, complicating online training on battery-powered devices. This work aims to decouple layer size and the number of layer inputs and outputs. This work aims to decouple layer size and the number of layer inputs and outputs to speed up training networks, remove memory bottlenecks, and reduce energy consumption in DNNs. Various methods like structured sparsity and weight sharing have been proposed for training simpler but 'wider' models. This paper proposes a sparse cascade architecture and neural network initialization scheme to optimize sparse neural network training. It evaluates different topologies for matrix reconstruction tasks to determine the best topology for efficient processing. The paper proposes a sparse cascade architecture and neural network initialization scheme to optimize training. Different topologies are evaluated for matrix reconstruction tasks, showing the impact on network accuracy. A data-free heuristic is developed to predict network expressiveness, leading to the identification of a single family of sparse networks. Methods for arriving at a sparse network are classified into those enforcing sparsity before, during, or after training. Post-training pruning methods involve zeroing out certain weights after training, allowing for evaluation of accuracy to size trade-off during inference. Using the dense model, post-training pruning methods like Optimal Brain Damage and Optimal Brain Surgeon remove weights based on the second derivative of the loss. DeepCompression achieves high sparsity by replacing the Hessian-based metric with weight magnitudes and finetuning after pruning. Alternatively, Liu et al. decompose convolutional layers into per-channel basis kernels and a sparse kernel matrix for input feature maps. Structured sparsity methods have been explored to improve execution efficiency on GPUs and CPUs. Authors have used particle filters to prune whole channels or kernels, while others have focused on structured intra-layer sparsity by pruning small blocks of weights. Pruning during training can reduce computational load, but the device must still store the whole dense model initially. L1 regularization promotes sparsity during training by incentivizing all weights to approach zero. However, it can lead to decreased accuracy and unstructured sparsity. Group Lasso regularization enforces sparsity on coarse-grained structures instead of individual weights. Model size can also be reduced before training through layer-level methods. Weight reuse techniques like HashedNets group weights using a hash function to share them. Several techniques have been proposed to promote sparsity in neural networks. HashedNets use a hash function to group weights, while CirCNN utilizes block-circulant matrices for weight storage. Deep Expander Networks (X-Nets) and RadiX-Nets replace dense layers with sparse layers, with X-Nets guaranteeing connectivity within a logarithmic number of layers. ClosNets replace dense layers with a cascade of sparse layers using the Clos topology for full connectivity. In this work, the focus is on ClosNets, which utilize a sparse topology for full connectivity. The goal is to determine the optimal topology for maximizing performance per weight in neural networks. The proposal is to sparsify layers to enable training of wider networks without a significant increase in network size. The approach aims to replace fully-connected layers with a cascade of sparsely-connected layers to reduce parameter count. Hidden layer neurons have linear activation, while output neurons retain the original activation. The cascade topology must ensure connectivity between input-output pairs, low parameter count, and hardware efficiency. A priori pruning can also be applied to convolutional networks to reduce parameter count. The curr_chunk discusses disentangling input/output features and convolutional filters in MobileNets by pruning pointwise convolutions. Proper initialization of deep neural networks is crucial for training accuracy. The curr_chunk discusses the challenges of training deep sparse networks with common initialization schemes like Xavier initialization, leading to the vanishing gradient problem. This problem is addressed by proposing a new initialization scheme for deep sparse networks. In this section, a new initialization scheme is developed for sparse networks to address the vanishing gradient problem. The scheme considers layer sparsity by using a mask matrix and Bernoulli distribution. The activation variance of output neurons in sparse networks is updated based on the number of connected input neurons. The Xavier initialization is updated to consider sparsity in neural networks. Testing on the MNIST dataset shows that deep sparse networks can be trained using this sparse initialization. The choice of network topology greatly impacts accuracy and parallelizability. A metric is developed to evaluate topologies independently of specific tasks. In order to evaluate network topologies independently of specific tasks, a matrix reconstruction problem is devised using sparse matrices and adjacency matrices. The goal is to find a topology that performs well regardless of the matrix structure, with the use of L2 loss for evaluation. The choice of network topology is crucial for overall accuracy and parallelizability. The text discusses the importance of choosing the best network topology for optimal performance, highlighting the challenges of evaluating topologies through evolution and the need for a heuristic approach to predict topology quality accurately. The text discusses the importance of developing a heuristic to predict the quality of network topologies accurately. It revisits equations to define sparse decomposition and paths between input and output neurons. Using L0 loss for analysis, the goal is to reconstruct elements of the network matrix for better performance. The text discusses the reconstruction of network elements using L1 loss and constraints. It introduces a bipartite maximal matching problem to find the minimal L0 reconstruction loss. The text introduces a method for evaluating the capability of a topology to reconstruct a graph, highlighting the limitations of constraint satisfaction counting heuristics. It proposes a continuous evaluation approach that is not influenced by random data generation and can assess topology quality in a single pass. The method evaluates network topologies using ReLU and linear activations, focusing on homogenous functions. Neurons learn input ratios instead of magnitudes, shifting the task to the next layer. L0 loss measures constraints solved in reconstructing a matrix W. The text discusses constraints in reconstructing a matrix W in neural networks, focusing on ratio constraints over magnitude constraints. It introduces controllability of neurons based on input ratios and provides a method to calculate control in neurons with varying inputs and outputs. The controllability of neurons in neural networks is defined based on input ratios. The controllability of a ratio at a neuron is bounded, and the total controllability of a neuron is limited. This concept is crucial in understanding the constraints in reconstructing a matrix W in neural networks. The controllability of a neuron in a neural network is limited by the number of ratios. Neuron n inherits controllabilities from input neurons i and j, and can control the ratio between them if at least one weight is tunable. This allows for setting additional ratios between inputs and satisfying multiple constraints in optimization. The controllability of a neuron in a neural network is limited by the number of ratios. Neuron n can partially satisfy multiple constraints by adjusting controllability \u2206C a/b within a range of [0, 1]. The optimizer can tune individual values in \u2206C, splitting control across output neurons x and y connected to neuron n. The optimizer can choose how to split this control without the need for fairness. Theorems and corollaries extend these concepts for neurons with multiple inputs and output neurons. The controllability of neurons in a neural network is influenced by trainable ratios. The sparse cascade's topology is decomposed to analyze controllability of output neurons with respect to input neurons. The controllability tensor C, added controllability tensor \u2206C, and ratio tensor R must adhere to specific constraints. The controllability loss is defined based on the number of ratios a neuron has control over. Neuron output control ratios can be minimized to determine cascade control. Analysis of controllability heuristic focuses on improving topologies for matrix reconstruction tasks and identifying the best topology. Skip connections, similar to ResNet networks, can enhance sparse cascade performance by shifting magnitude learning to the layer above. Homogenous activation functions allow neurons to learn input ratios, reducing the need for learning magnitudes. In butterfly networks, having one connection per neuron with a constant value of 1 improves performance significantly. Skip connections enhance sparse cascade performance and reduce the need for learning magnitudes in neural networks. The trained network produces the controllability tensor of the last layer, a 3D tensor with different outputs for various network topologies like hypercube, butterfly, and torus. The total controllability is determined by the sum of K and represents the 'brightness' of the network. Different network structures show varying controllability ratios, with hypercube and torus networks exhibiting more variance compared to Clos and butterfly networks. Small-world networks like torii and hypercubes find it easier to satisfy closer connections. The controllability tensor of the last layer in a trained network shows varying controllability ratios for different network topologies. Small-world networks like torii and hypercubes find it easier to satisfy closer connections. Torus and hypercube networks exhibit significant variance, which can impact reconstruction accuracy. Shallow linear networks have faster convergence compared to deep linear networks. When choosing between equivalent network topologies, it is advisable to opt for shallower ones due to slower convergence in deep linear networks. Sparse deep linear networks have not been extensively studied, but it is observed that butterfly, torus, and hypercube networks lose performance with increased depth. Sparse random initializations in deep networks may be more susceptible to noise. Constant-depth topologies like Clos and low rank eventually outperform variable-depth ones, suggesting that an ideal topology should have a constant depth. In Figure 4, Clos topology is outperformed by butterflies and hypercubes for small parameter counts due to limited information bandwidth. Evaluation of different topologies shows that skip connections are necessary, controllability matrix should have no variance, depth should remain constant, and high information bandwidth is crucial. Only Clos and butterfly meet all criteria. The Clos and butterfly topologies meet certain constraints, but not all. A new parallel butterfly topology is proposed, consisting of p butterfly networks with maximum depth d. It outperforms other topologies and is shown in Figure 4. In this work, the parallel butterfly topology with p = 2 is explored for accelerating DNN training by pruning networks ahead of time. A sparse neural network initialization scheme is proposed to train deep networks without the vanishing gradient problem. Topologies maximizing accuracy over any domain are investigated, and a data-free heuristic is developed to evaluate the expressiveness of a given topology. Requirements for a good topology include skip connections, information bandwidth, shallowness, and input-output pair equality. The parallel butterfly is proposed as the ideal training topology. The parallel butterfly topology is proposed as the ideal topology for training sparse networks, outperforming other topologies. The Xavier initialization is recommended for weight initialization. A 'diagonal layer' can be added to measure the number of ratios a network can learn accurately. The number of ratios a network can accurately express is measured by training with L1 loss until convergence and counting the constraints satisfied. The network can satisfy k \u2212 n ratios, with the last layer satisfying all magnitude constraints. Neuron n can only satisfy one ratio constraint, affecting neurons x and y. With L2 loss, the network settles for a compromise solution. Controllability C a/b (n) for neuron n with input neurons connected by trainable and constant connections is discussed. Graph decomposition is shown in Figure 7 to apply Theorems 6.4 and 6.5. Neurons x and y have identical controllability due to having only one input."
}