{
    "title": "r1lL4a4tDB",
    "content": "In partially observable environments, deep reinforcement learning agents face challenges in extracting information and improving policy. A proposed RL algorithm combines a variational recurrent model with an RL controller for better performance in PO robotic control tasks. The algorithm showed improved data efficiency and policy optimization compared to other approaches in tasks with unobservable states. Model-free deep reinforcement learning algorithms have been developed to solve control and decision-making tasks in partially observable environments, where incomplete information is common. These tasks often require considering the history of observations, such as in video games and robotic control using real-time images. Several methods have been proposed to solve partially observable tasks in deep reinforcement learning, including incorporating a history of raw observations in the current observation and using recurrent neural networks as function approximators. These tasks often involve control and decision-making in environments with incomplete information, such as video games and robotic control using real-time images. Recurrent neural networks (RNN) are used as function approximators in deep reinforcement learning to tackle learning representation of underlying states and maximizing returns. Training RNNs is challenging due to their difficulty compared to feedforward neural networks. Another approach involves learning a model of the environment and estimating a belief state from state-transitions. In this study, a variational recurrent model (VRM) is developed to estimate belief states in partially observable tasks. The VRM models sequential observations and rewards using a latent stochastic variable, extending the variational recurrent neural network (VRNN) model. This approach addresses the challenge of estimating belief states in reinforcement learning tasks. Our approach in this study involves using a variational recurrent model (VRM) to estimate belief states in partially observable tasks. An algorithm is proposed to solve partially observable tasks by training the VRM and a feed-forward RL controller network. The algorithm shows significant policy improvement in various robotic control tasks, outperforming alternative approaches in environments where only velocity information is observable or long-term memorization is required. Our method utilizes a variational recurrent model (VRM) to auto-encode state-transitions, distinguishing it from typical model-based RL methods. It is similar to stochastic latent actor-critic (SLAC) in training a latent variable model for robotic control tasks. In more general partially observable environments, the actor network of SLAC did not utilize latent variables, leading to issues with long-term memorization of reward-related state-transitions. The critic in SLAC also did not include raw observations in its input, complicating training before model convergence. The problems studied can be framed within the framework of partially observable Markov decision processes (POMDPs). A POMDP is defined by a 7-tuple (S, A, T, R, X, O, \u03b3) where states, actions, state-transitions, rewards, observations, and a discount factor are key components. The goal is to maximize future rewards by learning a policy function. An algorithm was developed to address POMDP problems by learning state representations through observation-transitions and reward functions. The VRNN model is used to model general state-transitions in partially observable tasks. It is a recurrent latent variable model that can encode and predict sequential observations with a stochastic latent variable. The model consists of a variational generation model and a variational inference model, which predict future observations and approximate the latent variable given the current observation and internal states. The inference model approximates the latent variable given sequential data, maximizing the evidence lower bound ELBO. A VRNN in a POMDP can model the environment and provide additional information to an RL agent. Soft actor-critic (SAC) is a model-free RL algorithm that maximizes reinforcement returns and policy entropy for improved performance. Soft actor-critic (SAC) aims to maximize reinforcement returns and policy entropy to enhance performance. SAC implementation involves parameterizing neural networks for value functions and policy, learning an entropy coefficient factor, and minimizing loss functions. The reparameterization trick is used for action gradients in SAC. In this work, SAC is applied in partially observable tasks by including the state variable of the VRNN in the input of function approximators. The proposed VRM model extends the original VRNN model by incorporating action feedback. The inference model \u03c6 and the generative model \u03b8 are included in the architecture, with the reward r t\u22121 added to the raw observation x t for convenience. The generative model (denoted by \u03b8) uses LSTM for recurrent connections and is trained by maximizing an evidence lower bound. The VRM is trained by sampling minibatches of truncated sequences for computation efficiency in experience replay. Training of the VRM is separate from training of the RL controllers. Training the VRM is separate from training the RL controllers. Strategies for conducting them in parallel include a smooth update strategy for the RL controller and pre-training the VRM before RL starts to avoid instability and representation shift issues. Another approach is to use two VRMs to resolve conflicts. To resolve conflicts in training VRMs and RL controllers, two VRMs are proposed: the first-impression model and the keep-learning model. The first-impression model is pre-trained and then frozen, while the keep-learning model continues to update. State variables from both VRMs, along with raw observations, are used as input for the RL controller, resulting in better performance compared to using a single VRM. The text chunk discusses the training process using multi-layer perceptrons (MLP) as function approximators for V and Q in a reinforcement learning (RL) setting. It mentions the use of two Q networks for better performance and stability, as well as a target value network for computing V. Additionally, it describes the policy function \u03c0 \u03b7 and its components \u00b5 \u03b7 and \u03c3 \u03b7 as MLPs. In the execution phase, observations and rewards are used as inputs to compute internal states. The agent selects actions based on a policy function and interacts with the environment. RL networks are trained by sampling sequences from a replay buffer and updating them with gradient descent. The training process does not involve updating the inference models. Experimental evaluations were conducted on various control tasks, comparing the algorithm to alternative methods. The overall procedure is outlined in Algorithm 1. The training procedure for RL controllers involved using hyperparameters from the original SAC implementation. The first-impression VRM was pre-trained for 5,000 epochs, while the keep-learning VRM was updated every 5 steps. Batches of size 4 with 64 steps each were used for training. Two variations of SAC were implemented: SAC-MLP used a 2-layer MLP for function approximation, while SAC-LSTM utilized LSTM followed by MLPs to consider the entire history of observations for decision-making. Our algorithm separates representation learning from dynamic programming and includes pre-training of the first-impression model. Comparison with SAC-LSTM is done using the Pendulum and CartPole tasks for evaluating RL algorithms. CartPole task involves preventing the pole from falling by applying force to the cart based on observable information. The agent learns to swing an inverse-pendulum up and maintain it at the highest position for more rewards in classic control tasks. Experiments compare fully observable tasks with partially observable versions where only velocities can be observed. SAC-MLP failed in PO tasks, while the algorithm succeeded in learning to solve all tasks, with SAC-LSTM showing poorer performance in some cases. In classic control tasks, SAC-LSTM performed poorly in some cases, such as the pendulum task with only angular velocity observable. SLAC did well in CartPole tasks but had less efficient performance in Pendulum tasks. The proposed algorithm was also evaluated in more challenging control tasks in OpenAI Roboschool environments, which include continuous robotic control tasks with higher degrees of freedom. Experimental results showed significant policy improvement in partially observable tasks using the proposed algorithm, with agents achieving comparable performance to fully observable cases. The algorithm outperformed SAC-LSTM in environments where only velocities can be observed, indicating its efficiency in encoding underlying states from velocity observations. Learning of a SLAC agent was found to be unstable in some cases. The learning of a SLAC agent was found to be unstable, with its policy sometimes converging to a poor one. The average performance of SLAC was less promising in most PO robotic control tasks compared to other algorithms. A long-term memorization task, the sequential target reaching task, was used to evaluate different algorithms, where a robot agent needs to reach 3 targets in a specific sequence to receive rewards. The robot needs to reach 3 targets in a specific sequence without receiving any signal indicating which target to reach. Agents using the algorithm achieved almost 100% success rate, while SAC-LSTM also succeeded but took more training steps. SLAC struggled due to limited observations for the actor. One concern is the representation change in the RL controllers' input. The input of RL controllers can undergo representation change, impacting convergence of keep-learning VRM. Loss functions were plotted for 3 tasks, showing policy improvement even without full VRM convergence. Results indicated policy enhancement with sufficient sample efficiency, despite incomplete VRM convergence. In this paper, a variational recurrent model is proposed for learning to represent underlying states of partially observable environments. The algorithm for solving POMDPs is also introduced, showing effectiveness in tasks where underlying states cannot be easily inferred from short observations. The study highlights the importance of stochastic Bayesian inference in RL, which is less explored in current research. The study discusses the use of stochastic models in RL, which outperformed deterministic ones. It mentions the potential replacement of VRNN with other alternatives for improved performance. Additionally, it references a novel way of inference using back-propagation of prediction errors and the idea of two distinct systems for model-based and model-free RL in the brain. In this section, the authors discuss learning a successor representation of the environment that benefits both model-free and model-based RL. They propose the idea of a learned model that is not used for planning or dreaming, blurring the distinction between model-based and model-free RL. The algorithm implementation details and hyperparameters summaries are provided, with the first-impression model and keep-learning model sharing the same architecture. The authors utilized 128 hidden neurons for the inference and generative models, with 2-layer MLPs for the decoder. Gaussian variables had linear mean output functions and softplus variance output functions. The VRMs used tanh activation functions and RL controllers similar to SAC-MLP. Training involved using entire episodes as mini-batches with zero initial states. When dealing with long episodes or infinite-horizon problems, computation consumption in back-propagation through time (BPTT) can be significant. To improve efficiency, 4 length-64 sequences were used for training RNNs, with the burn-in method applied to provide initial states. This approach is crucial for tasks requiring long-term memorization and helps reduce bias from incorrect initial states. The SAC-MLP implementation followed the original SAC model, with automatic learning of the entropy coefficient \u03b1. To enhance efficiency in dealing with long episodes, a LSTM network with size-256 was added to SAC's function approximators, receiving raw observations as input. The actor and critic function approximators remained the same as SAC, with the LSTM's output as input. Gradients could pass through the LSTM, synchronizing the training of the LSTM and MLPs. The implementation mostly followed SLAC, with the use of CNNs and transposed CNNs for input feature extraction and output decoding layers replaced in this case. The authors replaced CNN and transposed CNNs with 2-layer MLPs for feature extraction and output decoding. They set the output variance for each image pixel as 0.1 but found it better to use trainable parameters. A 2-layer MLP was used to approximate \u03c3 y. Activation functions were tanh except for outputs. Environments from OpenAI Gym and Danforth were used for robotic control tasks. The CartPole environment had a continuous action space, and codes for sequential target reaching tasks were provided by the authors. The authors conducted an ablation study comparing the performance of their algorithm with variations using different models: a single VRM, only the first-impression model, and only the keep-learning model. Performance curves were obtained in evaluation phases with agents using the same policy but not updating networks or recording state-transition data. Each experiment was repeated with 5 different random seeds. The study compared different models, including the keep-learning model and deterministic RNNs. The proposed algorithm consistently outperformed modified versions, as shown in learning curves. Trained robots successfully learned to hop or walk in PO robotic control tasks. The algorithm relies on encoding capacity of models without requiring accurate predictions. The study compared different models, including the keep-learning model and deterministic RNNs, showcasing open-loop and close-loop predictions of raw observations. Experiments were conducted to show how hyperparameters of VRMs affect RL performance, with learning rate and sequence length variations. The study compared different models, including the keep-learning model and deterministic RNNs, showcasing open-loop and close-loop predictions of raw observations. Experiments were conducted to show how hyperparameters of VRMs affect RL performance, with learning rate and sequence length variations. The sequence length was randomly selected from {16, 32, 64} to ensure a total of 256 samples in a batch. Results in Fig 10 show overall performance consistency with different hyperparameters, indicating low sensitivity. The representation learning part of the algorithm is not highly affected by hyperparameters due to the absence of a bootstrapping update rule. The study compared different models for training VRMs, including the keep-learning model and deterministic RNNs. Experiments were conducted to analyze the scalability of the algorithm, with results shown in Table 4. The working environment used an Intel i7-6850K CPU, and the task was \"Velocitiesonly RoboschoolHopper\". Wall-clock time included training the VRM or pretrainings."
}