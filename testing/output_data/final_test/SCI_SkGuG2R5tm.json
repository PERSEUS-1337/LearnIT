{
    "title": "SkGuG2R5tm",
    "content": "In this work, the authors propose a novel approach to adapt data to a quantizer by training a neural network with fixed parameter-free quantization layers. They introduce a regularizer based on the Kozachenko-Leonenko differential entropy estimator and a locality-aware triplet loss to promote uniformity in the spherical latent space while preserving neighborhood structure. Our end-to-end approach surpasses most quantization methods and is competitive with the state of the art on benchmarks. Training without quantization step shows minimal accuracy difference but provides a versatile catalyst for any subsequent quantization technique. Recent work BID27 proposed leveraging machine learning algorithms to enhance traditional index structures like B-trees or Bloom filters. They approximated the cumulative density function using a neural network for optimal B-Tree construction in one-dimensional cases. The goal is to extend this approach to multi-dimensional spaces by mapping real-valued vectors to a uniform distribution over a d-dimensional sphere for competitive coding performance in similarity search. Our approach aims to optimize a coding stage and a neural network for similarity search. The network learns to preserve neighborhood structure in the input space while covering the output space uniformly, controlled by a parameter \u03bb. The \"catalyzer\" function is optimized to improve the quality of the coding stage. The trade-off between preserving neighbor fidelity and covering the output space uniformly is controlled by a parameter \u03bb. Different values of \u03bb offer varying trade-offs, with \u03bb = 0 maintaining neighbor locality but not covering the output space, and \u03bb \u2192 \u221e leading to loss of neighbor fidelity. Optimizing through discretization functions is challenging, leading to a focus on networks producing binary codes. However, improving upon more powerful codes like product quantization remains difficult. The proposed technique simplifies learning algorithms for indexing by adapting data to the index instead of the other way around. It involves optimizing two criteria: preserving neighbors with a ranking loss and favoring a uniform output with a regularization similar to maximum entropy. The proposed technique involves casting an existing entropy estimator into a regularization term to improve similarity search methods for high-dimensional data. It aims to reduce the discrepancy between near and nearest neighbors, making indexing easier by learning a neural network adapter. The text discusses a neural network adapter that improves similarity search methods by favoring uniformity in the spherical output space. The learned mapping enables the use of spherical lattice quantizers efficiently. The network can be trained without the quantization layer and enhances performance significantly for quantization-based and binary methods. In this work, the focus is on combining a strategy with lattice assignment to create compact codes. The approach involves mapping an empirical input distribution to a uniform distribution on the spherical output space. Unlike GANs and VAEs, the emphasis is on learning the encoder for preprocessing input vectors for indexing. The study also delves into dimensionality reduction and representation learning, referencing relevant literature on the topic. The literature on dimensionality reduction includes self-organizing maps, stochastic neighbor embedding, and t-SNE for visualisation in low-dimensional spaces. Product compact codes for indexing, such as Locality Sensitive Hashing, have evolved to include data adaptation methods like Iterative quantization and Vector Quantization. Some recent works leverage semantic information with neural networks to estimate distances or similarities with compact codes inspired by Vector Quantization. Lattices, discrete subsets of Euclidean space, have excellent discretization properties and are used as hash functions in LSH, but may waste capacity for real-world data due to assumptions about space density. In this paper, the focus is on spherical lattices for their bounded support. Entropy regularization is used in various machine learning applications, such as penalizing confident output distributions and speeding up computation of optimal transport distances. Recent works on binary hashing also incorporate entropic regularization to increase marginal entropy. Our proposal is inspired by prior work on one-dimensional indexing, but with a different approach. Our strategy involves training a neural network to map vectors from a d-dimensional space to a hypersphere in a d-dimensional space. We introduce a regularizer to spread out points uniformly across the hypersphere. By maximizing the differential entropy as a proxy for the density of points, we aim to reduce the overlap between nearest neighbors in the original space. The differential entropy of the distribution can be estimated using a regularizer that pushes closest points away, ensuring diminishing returns. The neural network is trained to maintain the same neighborhood structure as in the input space using the triplet loss function. The regularizer used in training the neural network ensures diminishing returns by pushing closest points away, maintaining the same neighborhood structure as in the input space. The overall loss combines triplet loss and entropy regularizer, controlled by parameter \u03bb for trade-off between ranking quality and uniformity. The KoLeo regularizer results in more uniform marginal distributions in the high-dimensional latent space, leading to qualitative evaluation of uniformity. The regularizer used in training the neural network ensures diminishing returns by pushing closest points away, maintaining the same neighborhood structure as in the input space. The KoLeo regularizer results in more uniform marginal distributions in the high-dimensional latent space, leading to qualitative evaluation of uniformity. Higher uniformity in the high-dimensional latent space is achieved by reducing the overlap between distributions before and after applying the catalyzer. The catalyzer decreases the probability of the distance between a point and its nearest neighbor being larger than the distance between another point and its 100th nearest neighbor. Visualization of the output distribution aims to map input samples to a higher dimensional hyper-sphere. The visualization of high-dimensional density from a different viewpoint with the Deep1M dataset mapped in 8 dimensions shows angular histograms representing the dataset points. The regularizer used in training the neural network ensures diminishing returns by pushing closest points away, leading to more uniform marginal distributions in the high-dimensional latent space. The method described in this section focuses on the uniformizing effect on mapping, resembling a uniform distribution for \u03bb = 1. It discusses the interplay with discretization at training and search time, utilizing parameter-free coding methods like binarization and a fixed set of points on the unit sphere from a lattice spherical quantizer. The advantage of this fixed coding structure is that compressed-domain distance computations between codes do not rely on external meta-data, unlike quantization-based methods. Binary features are generated by applying the sign function to coordinates, with relaxation during training and binarization used for cross-validation of the regularization parameter. The curr_chunk discusses the use of lattices for quantization, focusing on achieving uniformity in the output space. It describes the computation of catalyzed features and finding the nearest lattice point efficiently. The method approximates similarity between vectors asymmetrically and does not quantize query vectors. The experimental results focus on similarity search methods using compressed representations for database vectors, enabling storage of large datasets in memory. The encoding phase transforms database vectors into a compressed representation, followed by a quantization or binarization stage. The search phase involves transforming query vectors and finding the corresponding codes. In the search phase, query vectors are transformed and compared with codes to find the top-k nearest vectors. Two benchmark datasets, Deep1M and BigAnn1M, are used for evaluation. Deep1M consists of the first million vectors from the Deep1B dataset, reduced to 96 dimensions and normalized. BigAnn1M contains SIFT descriptors. Different datasets are used for training and hyperparameter validation, including the full Deep1B and BigAnn datasets with 1 billion elements. Methods are evaluated based on recall at k performance measure. In training, a 3-layer perceptron model with ReLU non-linearity and hidden dimension 1024 is used. The final linear layer projects the dataset to the desired output dimension with 2-normalization. Batch normalization is applied, and the model is trained for 300 epochs with Stochastic Gradient Descent. The initial learning rate is 0.1 and is decayed to 0.05. The study evaluates lattice-based indexing compared to conventional quantization methods like PQ and OPQ. The lattice quantizer outperforms PQ and OPQ for most code sizes on both datasets. The method maps input vectors to a dimensional space, quantized with a lattice of radius r, showing superior performance in the comparison. The study compares lattice-based indexing with conventional quantization methods like PQ and OPQ, showing that the lattice quantizer outperforms PQ and OPQ. Varying hyperparameters like rank parameters did not significantly impact performance. The trade-off between a good representation and compressibility is influenced by the dimensionality of the output space. Lower dimensions perform better at low bitrates due to approximation quality, while higher dimensions are better at higher bitrates due to representation quality. The regularizer \u03bb needs to be set to a large value for small dimensions and low bitrates. The regularizer \u03bb needs to be set to a large value for small dimensions and low bitrates, but higher dimensions and higher bitrates require lower values of \u03bb. Large-scale experiments were conducted on the Deep1B dataset with 64-bit codes, showing a drop in recall performance compared to smaller datasets. Additive quantization variants BID1 BID32 BID34 are state-of-the-art encodings for vectors, but their iterative optimization process is slow for practical use cases. The curr_chunk discusses the performance comparison of different quantization methods, showing that the Catalyst + Lattice variant method is significantly faster than LSQ BID32. Despite a slightly slower search time, the method achieves competitive results for large-scale practical use. The curr_chunk discusses the impact of different training methods on the performance of the lattice quantizer, showing that end-to-end training has limited impact on overall performance for 64 bits. The KoLeo regularizer helps narrow the performance gap induced by discretization. The curr_chunk introduces a catalyzer layer that enhances the performance of quantization methods like OPQ and binary hashing. It shows improvements in recall@10 and compares the catalyzer to popular methods like LSH and ITQ. The catalyzer applies a simple sign function to the features, demonstrating its effectiveness in improving hashing techniques. The catalyzer layer improves quantization methods like OPQ and binary hashing by applying a sign function to features. It enhances performance by 2-9 percentage points across different settings. A neural network is trained to map input features to a uniform output distribution on a unit hypersphere, improving high-dimensional indexing accuracy. This approach competes with adapting data distribution to a rigid quantizer instead of vice versa, offering benefits like faster encoding and decoding without the need for codebooks. The code for the experiments is available at https://github.com/facebookresearch/spreadingvectors. The regularizer \u03bb decreases with dimension, optimal values for Deep1M are shown in TAB2. A \"normalization\" function N of vectors is defined by taking absolute values and sorting by decreasing coordinates. The process to solve Equation 5 involves normalizing y, finding the atom z that maximizes N(y), and encoding the vector z \u2208 Srd relative to N(z)'s range. The text discusses encoding permutations using combinatorial number systems and normalizing vectors for optimization. It also mentions encoding the sign of non-zero elements and the efficiency of the quantization process. Additionally, it highlights the improved performance of their method in range search and k-nearest neighbors search on Deep1M. After encoding permutations and normalizing vectors, the method improves performance in range search and k-nearest neighbors search on Deep1M. By mapping vectors, a unique threshold can be used to find most neighbors, reducing variance in results. In the transformed space, \u03b5 = 0.38 returns 200 results for 80% recall, compared to 700 in the original space with \u03b5 = 0.54. The latent spherical space shows much better agreement."
}