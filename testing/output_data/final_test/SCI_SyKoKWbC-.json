{
    "title": "SyKoKWbC-",
    "content": "In this work, the authors propose distributional adversaries that operate on sets of multiple points drawn from a distribution, rather than single observations, to address mode collapse in generative models. Experimental results show that generators trained with these distributional adversaries are more stable and less prone to mode collapse compared to traditional models. The framework also shows strong improvement in domain adaptation. The application of the framework to domain adaptation shows significant improvement over recent state-of-the-art. Generative Adversarial Networks (GANs) have been successful in tasks like image generation, text to image synthesis, and video prediction, but they are known to be difficult to train due to oscillations and mode collapse issues. Recent research aims to address instability and mode collapse in adversarially-trained models, with insights on training instability and saturation of the discriminator. An alternative training scheme, WGAN, estimates the Wasserstein distance to improve model performance. In this work, a new perspective on mode collapse in GANs is presented, focusing on the separability of the discriminator in GANs and WGANs over observations. The lack of information sharing between observations during training may lead to mode collapses. A framework is proposed to address this issue by making the discriminator a true distributional adversary. The text introduces a new distributional framework for adversarial training of neural networks, focusing on sharing global information between gradients to stabilize training. It presents two models that utilize nonlinearity for information-sharing and connect to popular ideas in deep learning and statistics. The framework operates on a genuine sample rather than an observation, making off-the-shelf discriminator networks distribution-aware through simple modifications to their architecture. Our distributional adversarial framework improves training stability and mode coverage compared to single-observation methods. It enhances domain adaptation and prevents mode collapse in the generator by utilizing nonlinearity and global information sharing. The framework modifies discriminator networks to be distribution-aware, leading to strong improvements over existing methods. The aim of the generator G is to make its outputs indistinguishable from the training distribution. The discriminator converges to a fixed limit in relation to G. The gradient of G's loss is weighted by terms representing the discriminator's confidence function. The discriminator's confidence function on D(x G) can result in vanishing gradients for points where the generator's distribution has low probability compared to the real distribution, preventing the generator from spreading mass to other modes. The discriminator's confidence function can lead to vanishing gradients for points with low probability, hindering the generator from spreading mass to distant modes and causing mode collapse. The mode-collapsing phenomenon in GANs is caused by a myopic discriminator that bases its predictions on a single observation, leading to gradients dominated by one mode. To address this, a discriminator that considers an entire sample instead of a single observation is proposed. The discriminator function M is constructed step by step, utilizing a Neural Mean Embedding (NME) for discerning between distributions. The NME is learned in a data-driven manner and forms a key component of an adversarial learning framework. Two alternative adversary models are proposed to leverage the NME for discriminating between samples and enhancing training signals. The Neural Mean Embedding (NME) is used to build a discriminator for adversarial training, combining it with a classifier to create a sample classifier. This classifier predicts whether a sample is from real data or a generated distribution, with a logistic loss value function for the adversarial game. The classifier predicts one label for the entire sample, differentiating it from the original GAN objective. The proposed method involves a two-sample discriminator that predicts whether two samples are drawn from the same or different distributions by using the absolute difference between Neural Mean Embeddings. This approach shifts from classification to a discrepancy objective, with a logistic loss function. The method involves a two-sample discriminator that evaluates the difference between halves of samples X and Z. Logistic loss is used as the objective function to highlight the distribution-based adversary's unique labeling approach. The Distributional Adversarial Network (DAN) introduces novel distributional adversaries to enhance existing models. DAN can be integrated with adversarial training algorithms, such as in generative adversarial settings and adversarial domain adaptation. By using distributional adversaries in GANs, DAN-\u03be tackles a saddle-point problem similar to the original GAN. The optimization process involves alternating updates on generators and discriminators, resulting in improved performance in various settings. The Distributional Adversarial Network (DAN) introduces distributional adversaries to enhance models like GANs. DAN proposes a regularized version of the original GAN objective, optimizing with a weaker discriminator and a parameter \u03bb. This approach provides stable training signals for the generator, improving performance in complex data distributions. In DAN-S and DAN-2S, data and noise distributions are approximated via finite sample averages. The training procedure for DAN-S is similar to GAN, while DAN-2S requires a modified scheme for balanced exposure to pairs of samples from the same and different distributions. Samples are drawn from data and noise distributions, split into two parts, and used for prediction by the discriminator D 2S. Target outputs are set for different pairs to train the discriminator effectively. The distributional adversaries proposed in DAN overcome the vanishing gradients issue by sharing information across observations when computing gradients. This allows DAN to recover all modes, unlike vanilla GAN which remains stuck in mode collapse. The gradient in DAN suggests that choosing a discriminator that enforces interaction between sample points is key. Sample-based aggregation is intuitive and confirmed by experimental results. The sample classifier defines a general discrepancy measure between distributions, including existing adversarial objectives as special cases. The discrepancy discussed in the curr_chunk generalizes existing adversarial objectives, such as the 1-Wasserstein distance used by WGAN. It includes measures like MMD, which compute distances between sample means using different kernels. MMD and its variants have been used in adversarial models either as direct objectives or as adversaries. Our distributional adversary improves on existing adversarial objectives by adapting to given distributions, evolving its witness function during training to better discriminate between generated and true data. Unlike previous methods, it uses a neural network instead of hand-picked kernels, offering better generalization and efficiency. Our model utilizes a neural network to compute the discrepancy of kernel MMD, resulting in linear time complexity and flexibility. It generalizes Integral Probability Metrics (IPMs) and is connected to GAN frameworks based on IPMs. Minibatch discrimination, including batch normalization, stabilizes training in generative models. Zhao et al. (2017) proposed a repelling regularizer for orthogonalizing minibatches. Our implementation generalizes minibatch discrimination by leveraging neural networks for different forms of discrimination without hand-crafted objectives. It adapts to data distribution and target models, utilizing a permutation invariant operator on unordered samples. Recent work explores neural architectures that operate on sets and are invariant to input permutations. BID31 proposes a content attention mechanism for unordered inputs of variable length, while BID35 embeds samples into a fixed-dimensional latent space. BID18 uses a similar network for image embedding but aggregates with a learned weighted sum. The NEM structure resembles these networks in permutation invariance but differs in motivation and usage within discriminators in adversarial training. Various other approaches have been proposed to address training instability and mode collapse in GANs. In generative models, Distributional Adversarial Networks (DAN) training improves mode recovery and domain adaptation by aligning latent spaces of source and target domains. DAN training shows better mode recovery than nondistributional models on synthetic and real datasets. In a simple generative setting, DAN is tested on a two-dimensional mixture of Gaussians with means equally spaced on a circle of radius 6. The comparison includes GAN, WGAN, and WGAN-GP using feed-forward networks with ReLU activations. DAN consistently outperforms other methods in mode recovery and is less sensitive to network architectures and hyperparameters. Our DANs consistently recover all modes of the true distribution and are stable across a reasonable range of \u03bb's. Mode recovery entails generating samples that lie in the corresponding mode and recovering the true probability mass of the mode. The model is evaluated on MNIST and Fashion-MNIST datasets with 10-class balanced distributions, and compared to other generative models like RegGAN, EBGAN, WGAN, WGAN-GP, and GMMN using a similar neural network architecture. Training models without Batch Normalization, except for RegGAN and EBGAN, which benefit from BN. DAN uses regularized objective with \u03bb > 0. Results show that vanilla GAN, RegGAN, and GMMN have issues with mode distribution. WGAN-GP and DAN perform well on MNIST, while WGAN-GP is more sensitive to hyperparameters. DAN achieves stable mode frequency recovery and generates diverse features in image generation tasks, outperforming DCGAN. It also shows promise in domain adaptation compared to DANN BID9. The DAN framework is used for domain adaptation with a domain-classifier adversary to enforce similar representations between source and target domains. It outperforms GAN-based DANN on Amazon reviews dataset, with an average accuracy improvement of 1.41% for DAN-S and 0.92% for DAN-2S. DAN is integrated without network structure tuning and is effective in achieving distributional similarities between different domains. The proposed distributional adversarial framework, DAN, improves domain adaptation for image label prediction by 5% compared to DANN. It does not rely on observation-wise functions but considers samples as a whole, leading to a stabilizing effect in generative adversarial networks and addressing mode collapse. The new approach in the distributional adversarial framework improves domain adaptation for image label prediction by 5% compared to DANN. It utilizes sample-based discriminators to share information across observations, offering advantages over separable alternatives. The framework is general, allowing for various extensions, and the impact of sample size on training stability and mode coverage warrants further investigation. To maintain discrimination power in complex distributions, sample sizes for discriminators should increase with the number of modes. The training procedure for DAN is outlined in Algorithm 1, with updates alternating between distributional and single-observation adversaries. Input parameters include total iterations, minibatch size, step number, and model mode. The training procedure for Discriminator-Generator Network (DAN) involves updating the discriminator and generator based on loss functions. The generator and discriminator architectures are specified, with latent vectors sampled uniformly. Different parameters are set for WGAN and WGAN-GP models. The weight for the gradient penalty in WGAN-GP is set to 0.1, while for DAN, distributional adversaries have two initial hidden layers of size 32. WGAN is optimized using RMSProp with a learning rate of 5 \u00d7 10 \u22125, while other models are optimized using Adam with a learning rate of 10 \u22124 and \u03b2 1 = 0.5. Minibatch size is fixed to 512. GAN training results in a single mode being recovered, confirmed by the concentrated distribution around a single mode in the generator. WGAN avoids getting stuck at a single mode but still experiences mode collapse. WGAN-GP and DAN can recover all 4 far-apart modes, avoiding mode collapse. Points close to the boundary of D's decision can cause G's mode to shift, concentrating mass towards another mode. This effect is commonly observed in distributions with close modes, where G's mass can traverse from mode to mode. The training for WGAN-GP is unstable due to slight changes in hyperparameters, leading to different generator distributions. The network architecture includes a fully connected generator with 3 hidden layers and a linear projection to 2 dimensions. The discriminator is also fully connected. Optimization is done using Adam with specific parameters, and training is conducted for 30,000 iterations. Despite separation, all models successfully capture all modes. The training for WGAN-GP is unstable due to slight changes in hyperparameters, while DAN remains stable across different settings. DAN shows stability across a range of trade-off parameters, with DAN-2S being stable in a larger range than DAN-S. In synthetic experiments, GAN suffers from mode collapse, WGAN(+GP) can fail in recovering modes, and DAN constantly recovers all modes and is stable for various hyperparameter settings. GAN may oscillate between modes, WGAN still experiences mode collapse, while WGAN-GP and DAN consistently recover all modes. The architecture for the generator in various models consists of fully-connected layers with ReLU activations, followed by a linear layer and Sigmoid activation. The discriminators in GAN, RegGAN, and DAN have a similar architecture with three fully-connected layers. DAN uses the same architecture for both adversaries, except for the averaging layer in the distributional adversary. This design choice helps limit the number of additional parameters in the regularizer for more efficient training. The decoder in EBGAN shares the same architecture as the generator, while the encoders in RegGAN and EBGAN have their own unique architectures. The decoder in EBGAN shares the same architecture as the generator, while the encoders in RegGAN and EBGAN have unique architectures. GMMN uses Gaussian kernels with different bandwidths. Hyperparameters include Adam with a learning rate of 0.0005, minibatch size of 256, and 100 epochs of training. DAN-S and DAN-2S show superior mode coverage and generation quality compared to other models. In additional experiments on the SVHN dataset, DAN-S and DAN-2S outperform GAN, WGAN, WGAN-GP, and GMMN in terms of mode coverage and generation quality. The batch-size in DAN's affects model performance, with DAN-S performing better with smaller batch-sizes and DAN-2S excelling with larger batch-sizes. The model goes through fully connected layers with hidden units 1,024 and 8,192, then reshaped to [8 \u00d7 8 \u00d7 128] and transposed. For GAN, WGAN, and DAN, the discriminators consist of convolutional layers and fully connected linear layers. DAN achieves stable mode frequency and good mode coverage on the SVHN dataset. TV distances are shown in FIG0. DAN achieves stable mode frequency recovery and good mode coverage. GMMN performs slightly better than DAN but suffers from poor generating quality. Generated samples from various models on CIFAR10 are compared, with DAN showing one of the best mode coverage and generation quality. DAN also performs well on the CELEBA dataset. The network architecture for the DCGAN implementation on the CELEBA dataset involves preprocessing image data by cropping to 160x160 and resizing to 64x64. The generator includes a linear layer mapping from a latent space of [-1, 1] 100 to dimension 8,192, followed by deconvolution layers with activations. The discriminator mirrors the generator with Leaky ReLU activations and a Sigmoid output layer. Both adversaries in DAN share the same architecture with differences in the averaging layer for the distributional adversary."
}