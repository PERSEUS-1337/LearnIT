{
    "title": "SkxKFJ2NtS",
    "content": "The Normalizing constant, essential in Bayesian inference, is typically challenging to compute accurately. A new method called Gaussianized Bridge Sampling (GBS) combines Normalizing Flow and Optimal Bridge Sampling to estimate it from posterior samples. GBS outperforms existing methods like Nested Sampling and Annealed Importance Sampling in speed and accuracy, with reliable error estimation. Bayesian evidence, or marginal likelihood, is a key component of Bayesian methodology. Existing methods like Annealed Importance Sampling (AIS) and Reverse AIS (RAIS) provide bounds to the normalizing constant, but can be inaccurate and slow. Nested Sampling (NS) is another alternative but can be costly and lead to inaccurate estimates. A new approach is developed to address these challenges. The new approach combines Normalizing Flow density estimators with Optimal Bridge Sampling for accurate estimation of normalizing constants in Bayesian inference. This method requires relatively few additional likelihood evaluations compared to standard MCMC methods. The calculations are performed on standard CPU platforms and will be available in the BayesFast Python package. Bridge Sampling is a method for estimating normalizing constants in Bayesian inference, with different estimators like Importance Sampling and Harmonic Mean being special cases. An optimal bridge function can be constructed for a given proposal function, minimizing the relative mean-square error. Choosing a suitable proposal for Bridge Sampling can be challenging, requiring a large overlap between the proposal and target distributions. One approach is Warp Bridge Sampling (WBS). Warp Bridge Sampling (WBS) transforms the target distribution p(x) to a Gaussian distribution with linear shifting, rescaling, and symmetrizing. However, this approach may be inaccurate for complex probability densities. An appropriate proposal q(x) with a large overlap with p(x) is crucial for accurate results in MCMC analysis. One method is to use a bijective Normalizing Flow (NF) to estimate q(x) from posterior samples, mapping p(x) to a zero mean unit variance Gaussian distribution. The estimated distribution q(x) is normalized, with a Jacobian of transformation taken into account. The Iterative Neural Transform (INT) is a new approach in machine learning that combines optimal transport and information theory. It focuses on transforming one-dimensional marginals that deviate the most between target and proposal distributions, using a bijective transformation to maximize entropy along non-Gaussian directions. This method aims to address the limitations and high computational costs associated with previous Normalizing Flow (NF) techniques. The Gaussianized Bridge Sampling (GBS) approach combines Optimal Transport with Normalizing Flow density estimation. It involves using a spline-based transformation to match data distribution to a Gaussian CDF, with KDE for smoothing. The method includes running No-U-Turn Sampler (NUTS) to obtain samples and dividing them into batches to avoid underestimation. The method involves fitting INT to obtain proposal q(x), drawing samples from q(x), solving for the normalizing constant ratio, and reporting the result as ln Zp with error approximation. Four test problems were used to compare estimator performance. Hierarchical models are recommended to be reparameterized to overcome pathology. The 32-d Banana example consists of 16 uncorrelated 2-d bananas with a random rotation. The 48-d Cauchy example features a mixture of two heavy-tailed Cauchy distributions along every dimension. The 64-d Ring example has strong non-linear correlation between parameters. The proposed GBS algorithm outperforms other estimators in accuracy and error estimation. Running GBS with fewer evaluations (GBSL) still provides unbiased results with larger error than GBS but smaller than other methods. GIS and GHM are better than NS or (R)AIS but worse than GBS(L). The importance of using a more expressive NF for q(x) is highlighted in comparison to WBS, which fails on several examples. GBS(L) requires most evaluation time for posterior samples with standard MCMC, while Thermodynamic Integration (TI) or (R)AIS is more expensive than posterior sampling. NS is also more costly than MCMC approaches, especially with non-informative priors. A new method is presented for estimating the normalizing constant in the context of Bayesian inference. In Bayesian analysis, a new method is introduced to estimate the normalizing constant using samples from the posterior distribution. The approach combines OBS with INT, a novel NF based density estimator, showing superior performance in accuracy and computational cost. The model likelihood is defined with flat priors, and a rotation matrix is generated from a random sample. The fiducial values for ln Z p are -127.364, -254.627, and -114.492. A rotation applied to the corner plot removes strong degeneracy, hindering estimators. Dynamic NS in dynesty is more efficient than static NS for high dimensions. The gradient of the likelihood affects sampling efficiency in high dimensions. dynesty uses Hamiltonian Slice Sampling for d > 20 and random walks for 10 \u2264 d \u2264 20. Error estimates for evidence are provided. For (R)AIS, warm-up iterations of NUTS are divided into two stages with a sigmoidal schedule for annealing. The logistic sigmoid function is denoted by \u03c3, with \u03b4 set to 4. 1,000 warm-up iterations are used for all examples, adjusting the number of states T to match the total evaluations of GBS. 16 chains are run for each case, with reported ln Z p averaged across chains for AIS and RAIS. Uncertainty is estimated from chain scatter, representing the error of the lower (upper) bound of ln Z p. Mass matrix and step size of NUTS at \u03b2 = 0.5, with the prior as base density, may explain why RAIS failed to provide an upper bound in the Banana example due to the broad density at high temperatures. In the Banana example, RAIS failed to give an upper bound due to the broad density at high temperatures. Using a better base density closer to the posterior would require delicate hand-tuning. The upper (lower) bounds of (R)AIS are valid with a large number of samples, but this can be costly in practice. Normalizing constant estimators require a sufficient number of samples from p(x), obtained with NUTS. Different chains and iterations were run for various examples, with warm-up iterations removed from the samples. Computation cost for INT using GBS, GIS, and GHM is minimal. The computation cost for GBS, GIS, and GHM is small relative to NUTS sampling. For GBSL, the total computation cost is reduced by half. The relative mean-square error of OBS is minimized. The uncertainty in Figure 1 is assumed to be Gaussian distributed. Different strategies can be used to allocate samples. The error in Figure 1 is assumed to be Gaussian distributed. Strategies for sample allocation can vary. It is recommended to draw samples from p(x) and q(x) based on equal-sample-size or equal-time allocation. NUTS based sampling is more expensive than NF based sampling. An adaptive strategy is used to determine the sample size for GBS(L). The error of OBS can be minimized by adjusting the number of q(x) samples, ensuring a specified fraction of contributions while reusing existing samples. Constraints on the fraction of p(x) evaluations used for q(x) samples are also considered. Default values for these fractions are set to 0.1 to balance cost and accuracy improvements. When focusing on posterior sampling, adjusting the error and evaluation fractions can enhance accuracy. Different numbers of samples can be used for proposal fitting and evidence evaluation. For GIS and WBS, the number of samples varies, with all p(x) samples used for INT in GIS and additional evaluations needed for evidence estimation in WBS. GHM does not require q(x) samples."
}