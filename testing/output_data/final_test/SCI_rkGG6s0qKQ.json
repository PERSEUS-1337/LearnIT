{
    "title": "rkGG6s0qKQ",
    "content": "Generative adversarial networks (GANs) are deep generative models used for unsupervised learning. Training GANs is challenging and requires hyperparameter tuning and neural architecture engineering. Various techniques have been proposed to improve GAN performance, but there is a lack of standardized evaluation metrics. This study aims to provide a practical perspective on the current state of GANs, exploring common pitfalls and reproducibility issues. Code and pre-trained models are available on GitHub for further research. Deep generative models, like GANs, are used in unsupervised machine learning for tasks such as image generation and domain transfer. GANs involve a two-player game where a generator transforms input distribution to images, while a discriminator distinguishes between real and synthesized samples. The goal is to reach a Nash equilibrium where both players minimize their loss. The text discusses the challenges of training GANs, which involve a minimax problem between the generator and discriminator. Various loss functions, regularization, and normalization schemes have been proposed to address these difficulties. The authors provide an empirical analysis of these approaches to help researchers and practitioners navigate the GAN landscape. In exploring the GAN landscape, the text delves into hyperparameter optimization, loss function impact, normalization, and regularization schemes, as well as different architectures. It highlights the stability of the non-saturating loss and the usefulness of gradient penalty and spectral normalization in high-capacity architectures. The discussion also covers common pitfalls, reproducibility issues, and practical considerations, offering reference implementations and pre-trained models on TensorFlow Hub. In exploring the GAN landscape, the text discusses the non-saturating loss function and the Wasserstein GAN (WGAN) approach, which considers the Wasserstein divergence instead of the Jensen-Shannon divergence. The WGAN aims to minimize the Wasserstein distance between the target and model distributions by ensuring a 1-Lipschitz discriminator. The loss functions in Wasserstein GANs include the least-squares loss and gradient norm penalty, which enforces 1-Lipschitzness. The gradient penalty encourages piece-wise linearity in the discriminator. The gradient norm penalty in Wasserstein GANs acts as a regularizer for the discriminator, improving performance for various losses. However, it can be affected by changes in the model distribution during training. The Dragan method's reliance on a Gaussian assumption for the manifold is uncertain, and computing gradient norms incurs a significant time penalty. Additionally, normalizing the discriminator can enhance optimization by improving gradient flow and stability. Normalization techniques like batch normalization (BN) and layer normalization (LN) have been suggested in the context of GANs to improve optimization by stabilizing gradient flow. Batch normalization normalizes pre-activations of nodes in a layer, while layer normalization normalizes hidden units in a layer with shared terms. These techniques enhance the spectral structure of weight matrices in neural networks, leading to more efficient optimization and richer representations. In the context of GANs, layer normalization with shared terms \u03b2 and \u03b3 is used to normalize different samples. Spectral properties of neural networks are analyzed by considering them as compositions of mappings. Spectral normalization is applied to weight matrices, including convolutional kernels, to control the maximum singular value. This approach results in discriminators of higher rank compared to other techniques. The study explores deep convolutional generative adversarial networks (DCGAN) and residual networks (ResNet) as architectures for GANs. Recently, BID19 introduced SNDCGAN, a variation of DCGAN with an eight-layer discriminator network. ResNet19 is another architecture with five ResNet blocks in the generator and six in the discriminator for 128x128 images. The setup reproduced state-of-the-art results, with detailed parameters in TAB4. An ablation study on ResNet modifications is available in the Appendix, focusing on image domain metrics. Inception Score (IS) is a metric for evaluating the quality of generated samples based on conditional label distribution and sample variability. It is correlated with human annotator scores but has drawbacks like insensitivity to label distribution. An alternative is the Frechet Inception Distance (FID), which measures the distance between embedded data following a multivariate Gaussian distribution. The Fr\u00e9chet Inception Distance (FID) measures the distance between two Gaussians estimated from embedded samples. It is argued to be consistent with human judgment, robust to noise, and sensitive to visual quality. FID can detect mode dropping and is compared to Kernel Inception distance (KID) in empirical studies. In this work, the focus is on FID as it correlates strongly with image quality measures like MS-SSIM. Mode collapse and mode-dropping are critical issues in GANs, affecting diversity of generated samples. The drawback of using MS-SSIM for diversity is not knowing the class of generated samples, limiting its application to one-class datasets like CELEBA-HQ-128. In this work, the focus is on evaluating diversity in generated samples using FID, MS-SSIM, and IS metrics. Three datasets, CIFAR10, CELEBA-HQ-128, and LSUN-BEDROOM, are considered. LSUN-BEDROOM dataset contains over 3 million images, while CELEBA-HQ dataset has 30k images. CIFAR10 dataset includes 70k images. Test sets are randomly partitioned for evaluation. The study evaluates diversity in generated samples using FID, MS-SSIM, and IS metrics on datasets like CIFAR10, CELEBA-HQ-128, and LSUN-BEDROOM. The baseline FID scores are 12.6 for CELEBA-HQ-128, 3.8 for LSUN-BEDROOM, and 5.19 for CIFAR10. The search space for GANs is costly, so the study focuses on analyzing different aspects like loss, regularization, normalization, and architecture separately for each dataset to understand their impact. In this study, the focus is on improving hyperparameter settings for better model performance. By combining the best settings from previous literature and using Gaussian Process regression, the study aims to uncover optimal hyperparameters. The fixed hyperparameter settings from recent publications are summarized in a table, and a total of 24 settings are considered to reduce bias. Gaussian Process is used for a fair comparison of models based on computational budget. In this study, hyperparameter settings are optimized using Gaussian Process regression for better model performance. The impact of loss functions, such as non-saturating (NS) loss, gradient penalty, and spectral normalization, on sample quality is analyzed. Spectral normalization and gradient penalty outperform the baseline in terms of computational efficiency. The number of discriminator updates per generator update is also explored, leading to additional hyperparameter settings. In this study, hyperparameter settings are optimized using Gaussian Process regression for better model performance. The impact of loss functions, such as non-saturating (NS) loss, gradient penalty, and spectral normalization, on sample quality is analyzed. Additional hyperparameter settings are explored by varying the number of discriminator update steps. The experiments focus on FID distribution, sample diversity score, and the tradeoff between computational budget and model quality. Each model is retrained 5 times with different random seeds. The study optimized hyperparameter settings using Gaussian Process regression for improved model performance. Different loss functions were analyzed, including non-saturating (NS) loss, gradient penalty, and spectral normalization, on sample quality. Results showed that spectral normalization and gradient penalty improved model quality on both data sets. The experiments focused on FID distribution, sample diversity score, and the tradeoff between computational budget and model quality. The gradient penalty can enhance model quality, but finding the right regularization balance is challenging and computationally intensive. Models using the GP penalty benefit from a 5:1 discriminator to generator update ratio. A study on hinge loss was also conducted. Both gradient penalty (GP) and spectral normalization (SN) outperform the baseline, with GP being more computationally expensive. The goal is to compare various regularization and normalization methods' performance, using ResNet19 architecture with fixed non-saturating loss. The ResNet19 architecture is used with various normalization and regularization methods like batch normalization, layer normalization, spectral normalization, gradient penalty, and L2 regularization. Results show that adding batch norm to the discriminator reduces performance, while spectral normalization improves model quality and is more computationally efficient than gradient penalty. Finding the right balance between loss and regularization strength is challenging, and models using gradient penalty benefit from a 5:1 discriminator to generator update ratio. In an ablation study, running the optimization procedure for an additional 100K steps can enhance model performance with gradient penalty. Simultaneous regularization and normalization, such as spectral normalization or layer normalization, significantly improve model quality over the baseline. The study explores the benefits of regularization and normalization techniques, specifically spectral normalization and layer normalization, in improving model quality. Computational effort is required for marginal gains in FID, but simultaneous regularization and normalization are recommended. The effects of regularization and normalization become more relevant for deeper architectures and optimization considerations. The study compares the benefits of spectral normalization and layer normalization in improving model quality, showing that both techniques achieve comparable results. There are divergences in how the FID score is computed, leading to potential mismatches in results. The FID score should be computed with respect to the test data set using 10k test samples and 10k generated samples on various datasets. Details of neural architectures, such as design decisions in popular architectures like ResNet, can impact results and should be considered for fair comparisons. The importance of using the architectures presented in the work as a baseline is emphasized, along with the availability of an ablation study on ResNet modifications in the Appendix. Issues related to data set processing, implementation details, and non-determinism in code are highlighted, including the challenge of removing randomness from the training process. In a recent study on GANs and Variational Autoencoders, researchers explored various loss functions and regularizers' effects on the FID score using low-to-medium complexity datasets and a specific architecture. They found no significant difference between new models and existing ones in this limited setting. In a recent study, researchers found no significant difference between new GAN models and the original non-saturating GAN. They explored the effects of gradient-norm regularization, spectral normalization, and more complex neural architectures, datasets, and regularization schemes in the GAN landscape. In this study, the GAN landscape was explored, focusing on losses, regularization, normalization schemes, and neural architectures. The impact on sample quality was assessed using quantitative metrics. Recommendations include using non-saturating GAN loss and spectral normalization as default choices. Additional gains can be achieved by adding gradient penalty and training until convergence. Combining normalization and regularization can further improve results. Different architectures proposed perform well, with marginal changes seen in quality from ResNet style tricks. In a large-scale study on GANs, common pitfalls hindering accurate comparisons were identified, including issues with metrics, data preprocessing, non-determinism, and missing implementation details. Concrete actions were proposed to address these challenges. An empirical study was conducted on SNDCGAN and ResNet CIFAR architectures, evaluating different loss functions on CIFAR10. Hinge loss was found to perform similarly to non-saturating loss. The study aims to provide a solid baseline for future GAN research. The study compares the KID metric to the FID metric using models from a Regularization and Normalization study. The Spearman rank-order correlation coefficient between KID and FID scores is high. The experiment shows a strong correlation between FID and KID metrics, suggesting either can be chosen for practical applications. The study compares KID and FID metrics using models from a Regularization and Normalization study. The architecture used is similar to BID19, with specific parameters and operations described in detail. ResNet19 architecture and ResBlock are also outlined, with explanations of the layers and operations involved. The ResNet architecture used in the study includes downscale and upscale layers marked in TAB5, with specific operations for convolution and pooling. The input and output shapes, as well as the input and output channels for ResNet blocks, are defined. The ResNet CIFAR architecture described in TAB6 is used for reproducing results, with minor differences noted. The ResBlock discriminator structure is outlined, with details on the layers and operations involved. The ResNet architecture used in the study has six minor differences compared to the implementation from a specific GitHub repository. An ablation study was conducted to assess the impact of these differences, with details provided in FIG7. The study includes variations such as using input as output for shortcut connections in the discriminator ResBlock, adjusting hidden layer output channels, and optimizing the setup for the first discriminator ResBlock. The study conducted an ablation study on the ResNet architecture, which included six minor differences compared to a specific GitHub repository. The impact of these differences was assessed, with details provided in FIG7. One key finding was that using CIN and OPT together improved results, as the first ResBlock was optimized while the remaining ResBlocks used ci for hidden output channels. Overall, the impact of these differences was minor according to the study on CIFAR10. To simplify future GAN training, a set of best parameters for three setups is proposed in the study on CIFAR10. The top 2 parameters for SNDCGAN, ResNet19, and ResNet CIFAR architectures are summarized based on FID scores. Gaussian Process optimization hyper-parameters are not included, and ResNet19 with at most two regularizers was run only once due to computational constraints. The model stability is shown by listing the best FID score out of five seeds for the same parameters. Spectral normalization outperforms other normalizers on SNDCGAN and ResNet CIFAR architectures, while layer normalization and spectral normalization work well on ResNet19. Figures 8, 9, and 10 display generated examples by GANs with the best FID score. Heatmaps in Figure 11 show the impact of hyperparameters on FID for each neural architecture and dataset. MS-SSIM scorer from TensorFlow was used with default settings. To adapt the edge to CELEBA-HQ-128 dataset, the filter size was adjusted to 11 to match the image size in the last scale layer. This modification allowed for computation following previous work."
}