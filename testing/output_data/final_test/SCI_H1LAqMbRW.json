{
    "title": "H1LAqMbRW",
    "content": "Model-free deep reinforcement learning approaches have demonstrated exceptional performance in simulated environments like Atari games and Go. This paper focuses on learning a forward model in the latent space constructed by existing actor-critic models for model-based planning in MiniRTS, a Real-time Strategy game with incomplete information. The forward model can predict future states effectively and improve win rates against rule-based agents using latent space Monte-Carlo Tree Search. Model-free deep reinforcement learning approaches like DDPG and A3C have shown superhuman performance in simple game environments like Atari games. However, for complex environments like Real-time Strategy Games such as StarCraft, planning ahead with a forward model is crucial. Constructing a forward model in partially observable environments is challenging due to the concealment of information and the need to capture the agent's belief of the unknown. In complex environments like Real-time Strategy Games, planning ahead with a forward model is crucial due to the concealment of information and the need to capture the agent's belief of the unknown. Model-free approaches use a shared network to extract features and predict reinforcement learning quantities for decision making. The learned latent space from these approaches could be used as a state representation for a forward model in this study using MiniRTS. In MiniRTS, a two-player Real-time Strategy game, agents use model-free approaches to construct forward models. The game involves building units, gathering resources, exploring hidden regions, defending against attacks, and invading enemy bases. The agent's actions are limited to its sight, with 9 discrete actions for overall strategy control. The study focuses on the relationship between the latent space learned by model-free approaches and the state representation of forward models, a topic rarely explored in model-free RL research. In MiniRTS, agents use model-free approaches to construct forward models for the two-player Real-time Strategy game. The study explores the latent space learned by model-free approaches and its relationship with state representation in forward models, a topic rarely explored in model-free RL research. Additionally, the study improves the performance of model-based agents in MiniRTS through input feature design and shows that actor-critic models can reconstruct critical game information. Novel algorithms are proposed to learn a forward model that maps a latent state to its future counterpart with reduced drifting, enabling the use of model-based planning like Monte-Carlo Tree Search in incomplete information games. Positive performance is demonstrated against rule-based agents, with win rates 8% higher than random planning. Forward modeling is a standard approach for modeling robots, where a low-dimensional state representation is used to learn a forward model. Learning a forward model from high-dimensional observations is challenging, but researchers aim to simplify the process by projecting inputs to a low-dimensional state. Some studies have successfully learned forward models directly from visual input for manipulation tasks. In robotics, forward modeling involves learning a low-dimensional state representation from visual input for manipulation tasks. Regularization is used to ensure the learned latent state can reconstruct the input effectively. Unlike model-free approaches, forward modeling can drift over many steps, affecting planning. Some approaches use forward model prediction as a feature or for guiding exploration in real-time strategy games. This paper focuses on stabilizing the forward model through multi-step long-term prediction. In real-time strategy games, micro-management and macro strategies are key components. Different approaches like model-free reinforcement learning and Monte-Carlo Tree Search have been used to address these tasks. While some focus on perfect information and forward models, others deal with partial observability. Grouping unit-based actions into strategic actions is a challenging topic, with various methods like deep models and replay analysis being employed. The goal is to maximize long-term rewards through learning and planning. In model-free reinforcement learning, an agent maximizes long-term rewards without knowledge of the environment. A model-based approach involves building a model to predict state changes. The paper uses Batch A3C to train a model-free agent with gradient directions of policy and value functions. Importance factor captures policy discrepancy. The importance factor \u03b7 t is used in Batch A3C to handle off-policy data collection. Shared parameters \u03b8 V and \u03b8 \u03c0 are often used in works with shared feature extraction. The common trunk h encodes information for both policy and value functions in a low-dimensional representation. In MiniRTS, a strong pre-trained model-free agent is used to build a forward model for the game, which involves building troops, gathering resources, exploring territory, defending against attacks, and timing attacks effectively. In MiniRTS, a model-free agent is used to play the game by defending against enemy attacks and launching counterattacks. The game ends when one player's base is destroyed. The game runs at 40K FPS on a laptop with complex dynamics. Different models with varying input features are used to strengthen the agent, including Vanilla, BuildHistory, and PrevSeen models. The agent in MiniRTS records build history and attaches related information in the feature plane. The game engine is modified to send the most recent information into the input for faster convergence and encourages aggressive strategies like rush. PrevSeen model shows the strongest performance and its latent representation is useful for forward models. The PrevSeen model's latent space is useful for forward modeling and understanding game statistics. An interpretation network is used to predict input from the hidden state, which can be used for decision making and model-based planning. However, simple predictions may suffer from drifting, leading to accumulation of prediction errors for long-term predictions. The PrevSeen model's latent space is utilized for forward modeling and understanding game statistics. To address the issue of prediction errors accumulating over time, a training procedure is implemented to ensure accurate long-term predictions. By training the forward model efficiently, existing planning methods can be applied to complex incomplete information games, resulting in improved performance over baselines. The trained AI uses T = 20 for experiments and frame skip of 50. Network structure includes 2 conv + 1 pooling + 2 conv + 1 pooling layers with 64 channels each. Input features are 20x20 images with 35 channels. The latent representation is 875 dimensional. After convolution, it predicts action distribution and value function. RNN computes the next latent state by concatenating previous state with a learnable embedding of the action. The latent state at time step t+1 is computed by concatenating the previous state with a learnable embedding of the action, then compressing them into a vector using an MLP. Frame-stacking and RNN are used to mimic short-term memory during training. Historic data can improve RL performance, with frame-stacking showing comparable results to RNN. PrevSeen outperforms both RNN and BuildHistory in encoding short-term information. In summary, short-term information can be encoded into the hidden space using RNN or frame-stacking, but long-term information needs to be manually encoded. This difference affects training curves and final performance of the agent, especially in terms of understanding the opponent's behavior. Incomplete information leads to sudden drops in the value function as the agent struggles to estimate the opponent's actions. A surprise metric is used to quantify this difference, showing that AI trained with complete information performs better in understanding different situations. The study shows that AI trained with complete information performs better in understanding different situations compared to those trained with incomplete information. The distribution of surprise differs between the two types of bots, with even the complete information bot experiencing sudden drops due to the game's complex dynamics. Factors like path planning and collision detection can lead to different game outcomes. Despite similar final performance, bots trained from PrevSeen and BuildHistory exhibit different behaviors against rule-based AI. For example, PrevSeen-trained AI learns to explore the map or rush the enemy with a few tanks. The AI trained from PrevSeen learns to explore the map and rush the enemy with tanks at the beginning of the game. The number of nonzero seen-then-hidden events per game from PrevSeen is twice that of BuildHistory. The model can reconstruct the original 20x20 channels and predict one channel at a time. The normalized reconstruction accuracy (NRA) is defined and the model correctly identifies relevant features from the input. The AI trained from PrevSeen learns to explore the map and rush the enemy with tanks at the beginning of the game. The model can partially recover the workers' location during training but ignores them at convergence, indicating the model learns to discard less relevant features. Predicting the 5x5 down-sampled version of the original 20x20 channels shows higher reconstruction accuracy, as the model only needs to reconstruct features in a rough region. Multiple relevant features such as the location of own RANGE ATTACKER and BASE, affiliation, and unit HP ratio emerge. The agent learns to pay attention to the opponent's WORKER and MELEE ATTACKER. Different forward models are trained to predict future states and actions. Win rate is determined by decisions made based on predicted latent states. The agent learns to predict future states and actions using different forward models. Evaluation is based on win rates determined by decisions made from predicted latent states. MatchPi is the most stable method with the highest win rate, outperforming the baseline approach. MatchA learns quickly but faces numerical instabilities, while PredN is stable but slower in learning speed. The performance of forward models is tested using a planning algorithm like Monte-Carlo Tree Search (MCTS) in RTS games. Shared tree is used for parallelization in MCTS to reduce the cost of expanding tree nodes. Virtual loss of size 5 is used to increase initial exploration diversity during MCTS execution. During MCTS execution, the network projects the input state to its latent representation, predicts the next state, and calculates the value for back-propagation. Models like MatchPi struggle in MCTS, while PredN consistently outperforms with a 25% win rate. A good forward model is essential for running reduced space MCTS. In a comparison against AI SIMPLE, PredN shows strong performance in latent space MCTS with 100 rollouts. The study explores using latent space MonteCarlo Tree Search in 1000 games with 100 rollouts. Despite efforts to improve performance, combining forward models with action probability from a model-free approach did not yield significant gains. The latent space learned by model-free reinforcement learning is crucial for making optimal decisions in complex environments. The paper validates the effectiveness of the latent space and proposes methods to learn forward models within this space. In a real-time strategy game with incomplete information, the study focuses on training a model-free agent and learning forward models in the latent space. Despite challenges, forward models enable the use of planning methods like Monte Carlo Tree Search, showing consistent improvements over baselines. Future work includes improving the performance of the learned forward model and automatically learning global actions from unit-based commands. Learning global actions from unit-based commands remains a challenging issue to solve, especially when dealing with exponentially large datasets."
}