{
    "title": "SyUkxxZ0b",
    "content": "State of the art computer vision models are vulnerable to small adversarial perturbations, leading to misclassification of visually similar images. Despite research interest, the cause of this phenomenon is poorly understood. A hypothesis suggests it is due to the high dimensional geometry of the data manifold. Studying a synthetic dataset of classifying between two concentric spheres reveals a tradeoff between test error and distance to nearest error. Models misclassifying a fraction of a sphere are susceptible to adversarial perturbations of size $O(1/\\sqrt{d})\". The vulnerability of neural networks to small adversarial perturbations is a logical consequence of the test error observed. The theory shows that models trained on a dataset of classifying between two concentric spheres approach a theoretical bound for error sets. This analysis may lead to understanding how the geometry of real-world data sets contributes to adversarial examples. Adversarial examples are errors that are robust and invariant to various factors. The cause of this phenomenon is still not well understood, with hypotheses suggesting linearity in neural network classifiers and being off the data manifold. Some propose defenses to increase robustness by changing non-linearities. Some methods aim to increase robustness to perturbations by changing non-linearities, distilling large networks, or using regularization. Adversarial training has shown to enhance robustness in many cases, but local errors can still occur beyond the trained distances. This phenomenon may be due to the high-dimensional nature of the data manifold. In a synthetic task of classifying between two concentric high dimensional spheres, adversarial examples are studied with a well-defined data manifold. The experiments show that most points from the data distribution are correctly classified but are close to an incorrectly classified input, even with a low test error rate. There is a tradeoff between generalization error and the average distance to the nearest point in this dataset. Neural networks trained on a dataset of two concentric spheres achieve an optimal tradeoff between error rate and distance to nearest error. Models can be highly accurate even when ignoring a large portion of the input. The data distribution is described as two concentric spheres in d dimensions. In a synthetic high dimensional dataset, two concentric spheres are generated in d dimensions with a target y associated with each x. The dataset offers advantages such as a well-defined probability density, a theoretical max margin boundary, and the ability to control the difficulty of the problem by varying d and R. The choice of R = 1.3 was arbitrary, and the relationship between adversarial examples was not explored in detail. The study focused on training a neural network on a dataset consisting of two concentric spheres in high dimensions. The dataset was designed with a target y for each x, offering advantages like a well-defined probability density and theoretical max margin boundary. The experiments involved training a 2 hidden layer ReLU network with 1000 hidden units on this dataset, exploring different training regimes and input dimensionality. Batch normalization was applied, and the data manifold was visualized in a 2D slice of the input space. The study trained a 2 hidden layer ReLU network on a dataset of two concentric spheres in high dimensions. The model achieved a low error rate of less than 1 in 10 million after 1 million training steps with Adam optimizer and sigmoid cross entropy loss. The final model was evaluated on 10 million samples with no errors observed. The study trained a 2 hidden layer ReLU network on a dataset of two concentric spheres in high dimensions. The model achieved a low error rate of less than 1 in 10 million after 1 million training steps with Adam optimizer and sigmoid cross entropy loss. The final model was evaluated on 10 million samples with no errors observed. Despite the unknown error rate, adversarial errors were found on the data manifold using gradient descent on the spheres. Two types of adversarial examples were generated: worst-case examples and nearest neighbor examples. The decision boundary was visualized by taking different 2d projections of the 500 dimensional space, showing the model's ability to interpolate between the two spheres. The study trained a neural network on two concentric spheres in high dimensions, achieving a low error rate. Adversarial errors were found using gradient descent, with worst-case and nearest neighbor examples generated. Visualizations showed the model's ability to interpolate between the spheres, with misclassifications appearing randomly on the sphere. The mean L2 distance to the nearest error on the data manifold was 0.18. In high dimensions, sampled points on inner sphere are typically 1.41 distance apart. Nearest point on outer sphere has average L2 distance of 0.0796 and norm of 1.07. Errors only occur in high dimensions, with ReLU net trained up to d=60 without errors. Network trained on two 500 dimensional spheres shows accuracy using theoretical decision boundary. The accuracy of the network is evaluated on the entire space using a theoretical decision boundary of 1.15. The accuracy increases as we move away from the margin, with errors observed at norms .6 and 2.4. A ReLU net trained on 100 samples in d=2 shows no errors on circles. Adversarial examples are hypothesized to be off the data manifold, leading to the design of a manifold attack to test this hypothesis. The manifold attack maximizes the probability of the target class given an input while ensuring the adversarial example stays on the data manifold. This is achieved through projected gradient descent and normalization steps. The attack only produces adversarial examples on the data manifold, maintaining the same probability under the data distribution as correctly classified points. The quadratic network is a simpler model with a single hidden layer and a quadratic non-linearity. It has d \u00d7 h + 2 learn-able parameters and can be rewritten in a specific form. Training it with h = 1000 resulted in no adversarial examples in an online learning setup. When training a quadratic network with 10^6 data points, most learned parameters are incorrect, leading to a low error rate. Using the Central Limit Theorem, the estimated error rate is approximately 10^-11, although the accuracy of this estimate is uncertain. The Central Limit Theorem closely approximates the error rate in large regimes. A \"perfect\" initialization of a quadratic network leads to rapid divergence of worst and average case loss. Adversarial examples can be found after 1000 training steps due to the training objective not directly tracking accuracy. The training objective does not directly track model accuracy and shows how worst and average case losses diverge in high dimensions. The ratio k/d needed for a certain error rate decreases as input dimension grows. Using the CLT, we can estimate accuracy for the quadratic network in terms of \u03b1 i. Proposition 4.1 estimates error rate on the inner sphere, suggesting settings of \u03b1 i with low error rates. The model's accuracy is highly dependent on the settings of \u03b1 i, with low error rates achieved when E[\u03b1 i] is close to (1 + R \u22122 )/2 and variance is not too high. Despite 80% of \u03b1 i being incorrect, the model can still produce accurate results by summing incorrect numbers. The flexibility in choosing \u03b1 i increases with input dimension, especially in a simplified quadratic network model with parameters k and b. The CLT approximation in Proposition 4.1 helps determine the necessary k value for achieving a desired error rate. In this work, the model's accuracy is influenced by the settings of \u03b1 i, with low error rates achieved when E[\u03b1 i] is close to (1 + R \u22122 )/2. The flexibility in choosing \u03b1 i increases with input dimension, especially in a simplified quadratic network model. The CLT approximation in Proposition 4.1 helps determine the necessary k value for achieving a desired error rate. The fraction of input dimensions needed for accuracy is plotted in FIG2, showing that even with 80% incorrect \u03b1 i, accurate results can still be obtained. Neural networks trained on the sphere dataset exhibit a similar phenomenon to image datasets, where most random samples are correctly classified and close to a misclassified point. The property occurring on the sphere dataset is explained by considering points on the inner sphere that are misclassified by some model. Theorem 5.1 states that for any model trained on the sphere dataset, the probability of error on the test set is directly linked to the average distance to the nearest error independently of the model. Models misclassifying a small fraction of the sphere must have errors close to most randomly sampled data points. The error set E maximizing the average distance to the nearest error on the sphere dataset is a \"cap\". The distribution of a single coordinate on the sphere illustrates the counter-intuitive property of high dimensional spheres. Theorem 5.1 provides an optimal trade-off between generalization error and average distance to nearest error, allowing comparison with actual trained neural networks. In Figure 5, actual trained neural networks are compared with an optimal bound. Three different architectures are trained on the sphere dataset with varying sample sizes. The error rate and average distance to nearest error are computed during training for different network configurations. During training, neural networks on the sphere dataset show a trade-off between average distance to nearest error and error rate. The decision boundaries of these networks appear well-behaved. The error set for the quadratic network is of the form E = ... The error set for the quadratic network on the sphere dataset is E = {x \u2208 R n : ||x|| 2 = 1, Geometrically this is the area of the sphere outside the ellipsoid decision boundary. If \u03b1 i > 1 for 2 or more i, then E is a connected region, suggesting the network approaches the optimal tradeoff between d(E) and \u00b5(E). The small and large ReLU networks have similar performance compared to the Quad-h1000 network. The error region of large neural networks demonstrates a tradeoff between error amount and average distance to nearest error, similar to smaller networks. Studying a synthetic dataset revealed that most random points are correctly classified and close to misclassified points, similar to image models. Theoretical tradeoff between error rate and distance to nearest error in neural network architectures, significant for understanding vulnerability to adversarial examples. Small classification errors may lead to many adversarial examples, explaining the difficulty in fixing the issue despite research interest. Future work should explore if similar principles apply to image manifolds. Despite substantial research interest, solving the issue of adversarial examples remains difficult. Recent works have attempted to increase robustness, but local errors persist both on and off the data manifold. This challenges the assumption that adversarial examples are off the data manifold. The test error rate of state-of-the-art image models is non-zero, indicating that a fraction of the data manifold is misclassified. The test error rate of image models suggests that a fraction of the data manifold is misclassified, implying the presence of local adversarial errors. The concentric spheres dataset, while simple, may not fully represent the complexities of real-world image manifolds. Insights from this simple case can guide the exploration of adversarial examples in more complex datasets."
}