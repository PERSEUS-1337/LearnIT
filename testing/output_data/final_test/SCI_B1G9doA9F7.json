{
    "title": "B1G9doA9F7",
    "content": "Training a model for a task usually needs a significant amount of data from relevant domains. Domain adaptation involves adapting a model trained in a data-rich domain to perform well in a data-poor domain. CycleGAN is a framework that maps inputs between domains using adversarial training and cycle-consistency. A new approach proposes using an external task-specific model to enforce cycle-consistency, preserving task-relevant content in low-resource settings. In low-resource settings, our approach improves digit classification performance by 14% and 4% when adapting SVHN to MNIST and vice versa, respectively. It outperforms unsupervised domain adaptation methods that require high-resource unlabeled data. Additionally, our model excels in speech recognition, improving performance by 2% for female speakers in the TIMIT dataset. Domain adaptation BID18 BID36 BID1 aims to generalize a model from a source domain with abundant training data to a target domain with limited data. This is achieved by learning mappings between domains using Generative Adversarial Networks (GANs) with cycle-consistency constraints. CycleGAN preserves the content from the source domain while transferring the style to match the target domain's distribution. Enforcing cycle-consistency in domain adaptation using Generative Adversarial Networks (GANs) helps preserve semantic information, but can be restrictive with imbalanced data across domains. Scarce data in the target domain makes it challenging to learn optimal mappings, leading to sub-optimal results. The proposed augmented cyclic adversarial learning model (ACAL) aims to address sub-optimal mappings in domain adaptation by replacing the reconstruction objective with a task-specific model. This model focuses on preserving semantic information by minimizing the loss of mapped samples for the task-specific model, which also enhances the discriminator's ability to model the distribution in the domain. Additionally, the task-specific model aids in disentangling essential task information. The model improves performance on digit domain adaptation and TIMIT dataset by 40% and \u223c5% respectively. It is related to domain adaptation using neural networks for supervised and unsupervised domain adaptation. Supervised domain adaptation involves utilizing label information in the target domain to minimize distribution differences between source and target domains. Our method for supervised domain adaptation involves using models from both domains to assist adversarial learning for better learning of the target domain distribution. Unlike other approaches that fine-tune source/target domain models, we train on data from both domains and transfer data from the source domain. Many recent works have utilized GAN framework for domain adaptation, focusing on high-resource unsupervised domain adaptation. BID2 and BID34 propose methods for domain adaptation using GAN and adversarial learning to match source and target domain representations. BID2 focuses on adapting data with limited target domain data, while BID34 employs pre-trained models and implicitly matches representation distributions. Cycle-consistent adversarial domain adaptation (CyCADA Hoffman et al., 2018) enforces cycle-consistency using semantic consistency. The approach includes an additional cycle starting from the target domain, crucial for low-resource target domains. The Relaxed cycle-consistent model (RCAL) enforces cycle-consistency through task-specific models, while the Augmented cycle-consistent model (ACAL) uses task-specific models to augment the discriminator for improved learning. The goal is to utilize these models for effective domain adaptation. Our approach aims to use mapped samples from the source to target domain to enhance the limited data in the target domain. We improve CycleGAN by enforcing content consistency and style adaptation through cyclic adversarial learning. Our model differs from recent approaches by implicitly learning content and style representation through an auxiliary task, making it more suitable for low resource domains. We extend the idea of using classification to assist GAN training, proposing task-specific models like CatGAN for preserving task-specific information in data. The text discusses the use of task-specific models like CatGAN to preserve information in data. It also introduces the concept of unsupervised domain adaptation using generative adversarial networks (GANs), specifically mentioning CycleGAN for multiple domains. CycleGAN extends the framework to multiple domains, learning to map samples between them with adversarial objectives and cycle-consistency. The model optimizes a combination of objectives to balance contrastive forces and avoid sub-optimal mapping functions. The dynamics of CycleGAN balance adversarial and reconstruction objectives to generate samples close to the true distribution. However, challenges arise in domain adaptation with sparse target domain data. Discriminator overfitting can occur, leading to difficulties in modeling the actual distribution. Limiting capacity or using regularization may induce over-smoothing and under-fitting. The limited data in the target domain can lead to over-smoothing and under-fitting, causing the discriminator to prioritize reconstruction over adversarial objectives. This results in a small distribution support and limited learning signal for meaningful cross-domain mappings in domain adaptation. The root issues lie in the strong emphasis on exact reconstruction and the reliance on the discriminator for mapping functions. To address issues in domain adaptation, a task-specific model is proposed to enforce cycle-consistency and train more meaningful cross-domain mappings. The cycle-consistent objective ensures that reverse mappings preserve semantic information without strict reconstruction requirements. This approach reduces reliance on the discriminator for mapping functions. The relaxed cycle-consistency objective in domain adaptation involves training models within specific domains. The adversarial objective is augmented to optimize mappings between domains, with assistance from the discriminator and task-specific model. The task-specific model captures conditional probability distributions to preserve information, distinct from the discriminator's role. The proposed model focuses on preserving useful information for predicting Y in domain adaptation. It utilizes a conditional model to mediate the influence of data not accessible to the discriminator. The approach can be extended to unsupervised domain adaptation and semi-supervised domain adaptation by modifying objectives for labeled and unlabeled target samples. The proposed model focuses on domain adaptation for visual and speech recognition, using unlabeled target samples interchangeably. Visual domain adaptation is evaluated on datasets like MNIST, Street View House Numbers, and USPS, while speech adaptation focuses on gender within the TIMIT dataset. Male speech is treated as the source domain, and female speech as the low resource target domain. In this section, ablations are performed to analyze the contribution of each component of the model using SVHN as the source domain and MNIST as the target domain. MNIST training data is downsampled to only 10 samples per class (MNIST-(10)), representing 0.17% of the full training data. Testing performance is evaluated on the full MNIST test set using a modified LeNet with specific architecture details provided. Various methods, including cycle-consistency and adversarial training, can be utilized for domain adaptation within the model components. The model described in BID2 uses adversarial training on the target domain and task-specific model on the source domain to preserve data content. The importance of the double cycle, proposed in BID39, is also examined. One cycle models (S\u2192T\u2192S)-One Cycle and (T\u2192S\u2192T)-One Cycle are tested, along with the effectiveness of relaxed cycle-consistency and augmented adversarial loss. The one cycle models tested in this study show similarities to the model proposed in BID12, with the simple conditional model performing surprisingly well compared to more complex cyclic models. The performance of the single cycle is poor when the target domain has limited data, but improves when the cycle is reversed. The adaptation mapping in the performance improves with cycles in both directions, benefiting from real examples. Relaxed cycle-consistency with task-specific losses reverses trends, preserving semantics when source domain dataset is large. Learned mapping functions transfer styles while preserving data content. Using task-specific loss with two cycles for low-resource unsupervised domain adaptation improves adaptation performance compared to CyCADA and its relaxed variation. The task-specific model enhances adaptation by transferring styles and preserving data content, outperforming models without adaptation. The CyCADA model and its relaxed variation struggle with semantic loss and instability in low-resource situations, while the ACAL model demonstrates stable performance by utilizing the source classifier for consistency. Experiments on digit recognition domain adaptation are conducted with different target domains and adaptation types. The ACAL model demonstrates stable performance in low-resource domain adaptation by utilizing the source classifier for consistency. Evaluation results show that ACAL outperforms other baselines in all adaptations, including high-resource unsupervised adaptation. The ACAL model shows strong performance in low-resource domain adaptation, outperforming other baselines in various scenarios. Additionally, improvements were made to the VADA adversarial model by incorporating natural gradient as teacher-student training. The model also excelled in domain adaptation for speech recognition using the TIMIT dataset. In the TIMIT dataset, male speakers are used as the source domain and female speakers as the target domain for speech recognition evaluation. The model utilizes spectrogram representation and a multi-discriminator architecture for adaptation performance. Results show significant improvements over baseline models and comparable performance to previous methods, particularly in male to female adaptation. The proposed model shows comparable performance to the baseline model trained on true female speech in male to female speech domain adaptation. The adapted distribution closely matches the true target distribution, leading to significant performance improvements. Additionally, with more data, the model outperforms the baseline by a noticeable margin. The approach involves using augmented cycle-consistency adversarial learning for domain adaptation and introducing a task-specific model to facilitate learning domain-related mappings. The proposed approach introduces a task-specific model for domain adaptation, enforcing cycle-consistency with a task-specific loss. It demonstrates significant performance improvement in two domain adaptation tasks compared to the baseline. The method can be applied to various unsupervised learning tasks, such as speech modeling or language modeling using different networks. The study introduces a task-specific model for domain adaptation, enforcing cycle-consistency with a task-specific loss. Results show significant performance improvement in two domain adaptation tasks compared to the baseline. Experimentation with different task-specific models and architectures is conducted, showing that using a more complicated deep model for the target domain weakens the performance improvement. Using a simpler LeNet architecture for domain adaptation from SVHN to MNIST improves test performance by 27% compared to CycleGAN. Results also show improved generalization performance with variable labeled target samples on MNIST and SVHN datasets. Evaluation of semi-supervised adaptation on MNIST and SVHN datasets is presented in Table 6, showing a qualitative comparison of domain adaptation models. The augmented cycle-consistent model preserves semantic information while matching the target distribution. Low-resource semi and unsupervised domain adaptation on MNIST, USPS, and SVHN datasets is also discussed in Table 6, with modifications mentioned for BID39 and BID13. The generators in CycleGAN are based on U-net BID28 architecture with 4 convolution layers of varying sizes, followed by deconvolution layers. The discriminator output is modified for stability in training, predicting a single scalar for real/fake probability. The ASR model, based on BID38, includes convolutional and residual blocks, followed by bidirectional GRU RNNs and a fully-connected hidden layer. The ASR model includes convolutional and residual blocks, bidirectional GRU RNNs, and a fully-connected hidden layer of size 1024 as the output layer. Qualitative results on transcriptions from different models are shown. The ASR model utilizes convolutional and residual blocks, bidirectional GRU RNNs, and a fully-connected hidden layer of size 1024 as the output layer. No Adaptation CycleGAN is used for image translation."
}