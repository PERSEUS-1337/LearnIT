{
    "title": "Hyg74h05tX",
    "content": "Flow-based generative models are efficient in sampling and inference, making them powerful exact likelihood models. Flow-based models are computationally efficient but have lower density modeling performance compared to autoregressive models. This paper proposes Flow++, a new state-of-the-art non-autoregressive model for unconditional density estimation on image benchmarks, addressing limitations in prior flow-based models. This work aims to narrow the performance gap between autoregressive and flow-based models. Recent advancements in modeling high dimensional raw observations from complex real-world datasets have been achieved by models such as GANs, PixelRNN/CNN, Image Transformer, PixelSNAIL, NICE, RealNVP, and Glow. Autoregressive models excel in density estimation but suffer from slow sampling times, while inverse autoregressive models offer quick sampling but are not efficiently trained by maximum likelihood. Non-autoregressive flow-based models like NICE, RealNVP, and Glow are efficient for sampling but lag behind autoregressive models in density estimation. Recent advancements in modeling high dimensional raw observations have been achieved by models like GANs, PixelRNN/CNN, Image Transformer, PixelSNAIL, NICE, RealNVP, and Glow. Autoregressive models excel in density estimation but have slow sampling times, while non-autoregressive flow-based models like NICE, RealNVP, and Glow are efficient for sampling but lag behind in density estimation benchmarks. To bridge this gap, a new flow model called Flow++ is introduced, aiming to combine fast sampling, fast inference, and strong density estimation performance. Flow++ is a new model that addresses inefficiencies in prior flow models by using variational flow-based dequantization, expressive affine coupling flows, and powerful convolutional layers in conditioning networks. Sampling and training are efficient due to invertible transformations and tractable Jacobian determinants. Flow++ introduces improved design choices such as variational flow-based dequantization, logistic mixture CDF coupling flows, and self-attention in conditioning networks. Dequantization converts discrete data into a continuous distribution before modeling with a continuous density model. The dequantized data y = x + u is obtained from discrete data x with values in {0, 1, 2, ..., 255}, where u is uniformly drawn. Training a continuous density model on uniformly dequantized data maximizes a lower bound on log-likelihood for a discrete model on the original data. Jensen's inequality ensures that maximizing log-likelihood on dequantized data prevents the continuous model from collapsing onto the discrete data. The new dequantization technique based on variational inference introduces a noise distribution to model discrete data using a continuous density model. This approach aims to prevent the collapse of the continuous model onto the discrete data, addressing the limitations of uniform dequantization. The variational lower bound is maximized jointly over the model and conditional flow-based generative model q. The objective is calculated using a stochastic gradient estimator, with a special case for uniform dequantization. The gap between the objective and true log-likelihood is addressed by using a noise distribution to model discrete data. Using an expressive flow-based q allows the p model to place density in each hypercube x + [0, 1) D according to a flexible distribution q(u|x), improving training and generalization loss. Flow models like invertible 1 \u00d7 1 convolution and affine coupling layers increase expressiveness while maintaining tractability for computations. The affine coupling layer BID6 splits x into two parts, computes y, and forms a data-parameterized family of invertible affine transformations. This layer allows for complex dependencies on the data while maintaining tractable inversion and log-likelihood computation. The splitting and merging operations are usually performed in a checkerboard-like pattern. The affine coupling layer BID6 splits x into two parts and applies nonlinear elementwise transformations using a neural network to produce transformation parameters for each component of x 2. This improves density modeling performance by incorporating mixture probabilities, means, and log scales in the transformation process. The coupling transformation involves computing y 2 elementwise, with an inverse sigmoid to ensure its existence within the range of (0, 1). The logistic mixture CDF can be efficiently inverted using bisection, and the Jacobian determinant involves calculating the probability density function of the mixtures. Improving the expressiveness of the conditioning on x 1 is crucial, achieved by enhancing the neural network responsible for producing transformation parameters. Our architecture involves stacking convolutions and multi-head self attention into a gated residual network, resembling the Transformer. Each block consists of two layers connected in a residual fashion, with layer normalization after each connection. The network outputs elementwise transformation parameters. Flow++ achieves state-of-the-art density modeling performance on CIFAR10 and ImageNet datasets. Ablation experiments quantify improvements proposed in section 3, and generative samples are compared against autoregressive models. Weight normalization, data-dependent initialization, and various flow techniques are employed in the experiments. Flow++ achieves state-of-the-art density modeling results on CIFAR10 and ImageNet datasets using various flow techniques. The model includes 4 coupling layers with checkerboard splits at 32x32 resolution, 2 coupling layers with channel splits at 16x16 resolution, and 3 coupling layers with checkerboard splits at 16x16 resolution. The performance of Flow++ is competitive with autoregressive models, such as PixelCNN, and shows potential for further improvement in future revisions. The ablations of the model on unconditional CIFAR10 density estimation were conducted, comparing variational dequantization vs. uniform dequantization, logistic mixture coupling vs. affine coupling, and stacked self-attention vs. convolutions only. The performance of these ablations relative to Flow++ at 400 epochs of training was analyzed, showing their relative performance differences. Switching from variational dequantization to uniform dequantization in the model costs approximately 0.127 bits/dim, while other changes such as switching to affine coupling layers and a pure convolutional residual architecture cost around 0.03 bits/dim each. Despite similar parameter counts, Flow++ outperforms due to improved inductive biases. The dequantization scheme significantly affects training and generalization loss, with a train-test gap of approximately 0.02 bits/dim after 400 epochs. Likelihood-based models, including variational and exact likelihood models, offer efficient approximate inference and exact log likelihood computation. Variational dequantization shows a train-test gap of 0.02 bits/dim, while uniform dequantization has a larger gap of 0.06 bits/dim. Training with variational dequantization is considered a more natural task for the model. More details can be found in the appendix. Prior work has focused on improving the sampling speed of deep autoregressive models. The Multiscale PixelCNN modifies the PixelCNN to allow for faster sampling but reduces the model's capacity for density estimation. WaveRNN enhances sampling speed for audio autoregressive models through sparsity and engineering considerations. Recent work aims to enhance the expressiveness of coupling layers in flow models, with BID15 demonstrating improved density estimation using an invertible 1x1 convolution flow. Flow++ is a new flow-based generative model that aims to bridge the performance gap between flow models and autoregressive models. It introduces specific design principles such as dequantization, flow design, and conditioning architecture design to guide future research in likelihood-based models."
}