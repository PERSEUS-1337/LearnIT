{
    "title": "SJlHwkBYDH",
    "content": "Deep learning models are vulnerable to adversarial examples created by making imperceptible changes to benign inputs. Two new methods, NI-FGSM and SIM, are proposed to enhance the transferability of adversarial examples. NI-FGSM incorporates Nesterov accelerated gradient to improve transferability, while SIM leverages the scale-invariant property of deep learning models to optimize adversarial perturbations. Our attack methods, NI-FGSM and SIM, enhance transferability of adversarial examples against defense models. Empirical results on ImageNet show higher success rates compared to state-of-the-art attacks. Adversarial examples expose model vulnerabilities and can fool other models, aiding in assessing model robustness. Generating robust adversarial examples against defense models is a challenging task, especially in the black-box setting. Existing attacks often struggle to succeed in creating effective adversarial examples under these conditions. The optimization process of generating adversarial examples has gained attention in the literature, with various gradient-based attacks proposed. While white-box attacks can achieve high success rates, black-box attacks face difficulties, particularly against models with defense mechanisms like adversarial training and input modification. In this work, two new methods are proposed to improve the transferability of adversarial examples: Nesterov Iterative Fast Gradient Sign Method (NI-FGSM) and Scale-Invariant attack Method (SIM). NI-FGSM adapts Nesterov accelerated gradient to enhance transferability, while SIM leverages the scale-invariant property of deep learning models to optimize adversarial perturbations. Our methods, NI-FGSM and SIM, improve transferability of adversarial examples by optimizing perturbations over input image copies. Combining NI-FGSM and SIM with existing attacks boosts success rates on ImageNet dataset. SI-NI-TI-DIM achieves 93.5% success rate against adversarially trained models in black-box setting. The attack methods NI-FGSM and SIM optimize perturbations on input image copies to improve transferability of adversarial examples. These methods achieve a success rate of 93.5% against adversarially trained models in a black-box setting, outperforming state-of-the-art gradient-based attacks. Adversarial attack methods aim to generate adversarial examples by maximizing the loss function with one-step or iterative updates. Examples include Iterative Fast Gradient Sign Method (I-FGSM), Projected Gradient Descent (PGD), and Momentum Iterative Fast Gradient Sign Method (MI-FGSM). These methods aim to generate perturbations within a specified norm bound to deceive machine learning models. The Iterative Fast Gradient Sign Method (MI-FGSM) integrates momentum for higher transferability of adversarial examples. The Diverse Input Method (DIM) optimizes perturbations over diverse transformations, while the Translation-Invariant Method (TIM) uses translated images for optimization. The Translation-Invariant Method (TIM) calculates gradients by convolving with a kernel matrix and can be integrated with other attack methods. The combination of TIM and DIM, TI-DIM, is a strong black-box attack method. Carlini & Wagner attack is an optimization-based method for finding adversarial examples, but lacks transferability for black-box attacks. Adversarial training is a defense method that augments training data with adversarial examples. Adversarial training methods, such as ensemble adversarial training, aim to improve robustness against black-box attacks by augmenting training data with perturbations from various models. Input modification techniques, like image transformations, can help remove adversarial perturbations while preserving data integrity. Various defense methods have been proposed to mitigate adversarial effects, including random transformations, high-level representation denoiser, JPEG-based compression, and end-to-end image compression. However, these methods may not guarantee true robustness. Cohen et al. (2019) use randomized smoothing to achieve certified adversarial robustness in an ImageNet classifier. Adversarial examples can be seen as an optimization problem similar to training neural networks. In the context of defending against adversarial attacks, adversarial examples can be seen as an optimization problem. The transferability of these examples is akin to the generalization ability of trained models. Methods used to improve model generalization can be applied to generating adversarial examples to enhance their transferability. Many techniques, such as better optimization algorithms like Adam optimizer, have been proposed to improve model generalization. To improve the transferability of adversarial examples, Nesterov Accelerated Gradient (NAG) is proposed as an optimization method, along with using scaled images for model augmentation. NAG is a variation of gradient descent that speeds up training and enhances convergence. NAG is an improved momentum method that stabilizes update directions, helps escape poor local maxima, and improves transferability in iterative attacks. It integrates NAG to leverage its looking ahead property for building a robust adversarial attack. NI-FGSM (Nesterov Iterative Fast Gradient Sign Method) is an adversarial attack that leverages the looking ahead property of NAG. It involves making a jump in the direction of previous accumulated gradients before computing gradients in each iteration. The update procedure starts with g0 = 0 and includes a decay factor \u00b5. Model augmentation can also improve the transferability of adversarial examples through loss-preserving transformations. Model augmentation involves deriving a new model from an original model by applying a loss-preserving transformation. This can enhance the transferability of adversarial examples by attacking an ensemble of models simultaneously. This approach avoids the computational cost of training multiple models for attack. In this work, an ensemble of models is derived from the original model through model augmentation, which involves a scale-invariant property of deep neural networks. A Scale-Invariant attack Method (SIM) is proposed to optimize adversarial perturbations over scaled copies of input images, reducing the need to train multiple models for attack. The Scale-Invariant Nesterov Iterative Fast Gradient Sign Method (SI-NI-FGSM) combines NI-FGSM and SIM to create a stronger ensemble attack by introducing model augmentation and a better optimization algorithm for crafting adversarial examples. This approach helps avoid overfitting on the white-box model and generates more transferable adversarial examples. SI-NI-FGSM can be combined with DIM, TIM, and TI-DIM as SI-NI-DIM, SI-NI-TIM, and SI-NI-TI-DIM to enhance the transferability of adversarial examples. Experimental evidence on the effectiveness of these methods is provided, including comparisons with baseline methods and advanced defense techniques. Additional discussions include comparisons with classic attacks. In Section 4.6, comparisons between NI-FGSM and MI-FGSM, as well as classic attacks, are discussed. Codes can be found at https://github.com/JHL-HUST/SI-NI-FGSM. The study involves using clean examples from the ILSVRC 2012 validation set and various models such as Inception-v3, Inception-v4, Inception-Resnet-v2, and Resnet-v2-101. Adversarially trained models like Inc-v3 ens3, Inc-v3 ens4, and IncRes-v2 ens are also considered, along with advanced defense models. The curr_chunk discusses advanced defense models such as HGD, R&P, FD, Comdefend, and RS, along with integrating methods with DIM, TIM, and TI-DIM to show performance improvement. Hyper-parameters include perturbation, iteration, step size, decay factor, transformation probability, and kernel size settings. Incorporating the scale-invariant property of deep neural networks, the SI-NI-FGSM method utilizes a Gaussian kernel with a size of 7x7 and 5 scale copies. Testing on ImageNet dataset images with scale sizes ranging from 0.1 to 2.0 shows smooth and stable loss curves within the range of 0.1 to 1.3. This property is leveraged to optimize adversarial perturbations over scale copies of input images, integrated with TIM for improved defense models. The SI-NI-FGSM method integrates with TIM, DIM, and TI-DIM to improve black-box attack success rates by 10% to 35%. It achieves nearly 100% success rates under the white-box setting, enhancing transferability of adversarial examples. Attacking an ensemble of models (Inc-v3, Inc-v4, IncRes-v2, Res-101) simultaneously shows improved success rates across all models. Our methods consistently outperform baseline attacks by 10% to 30% in black-box settings, with SI-NI-TI-DIM achieving a high success rate of 93.5%. The effectiveness of our methods is also tested on various advanced defense solutions, including HGD, R&P, NIPS-r3, FD, and others. Our SI-NI-TI-DIM method achieves a high attack success rate of 90.3%, surpassing state-of-the-art attacks by 14.7%. It can fool adversarially trained models and other advanced defenses by solely depending on the transferability of adversarial examples. SI-NI-TI-DIM can deceive adversarially trained models and advanced defense mechanisms, posing a new security challenge for developing more robust deep learning models. Adversarial examples crafted by SI-NI-TI-DIM show higher attack success rates compared to MI-FGSM, requiring fewer iterations for the same success rate. NI-FGSM demonstrates better transferability and can accelerate the generation of adversarial examples. Comparisons with classic attacks like FGSM and I-FGSM are also considered. In this work, new attack methods NI-FGSM and SIM are proposed to improve transferability of adversarial examples. These methods can be combined to create a robust attack, SI-NI-FGSM, which outperforms other classic attacks under the black-box setting. Integration of SI-NI-FGSM with baseline attacks further enhances transferability of adversarial examples. Our work introduces new attack methods NI-FGSM and SIM to enhance transferability of adversarial examples. These methods, combined in SI-NI-FGSM, outperform classic attacks in black-box scenarios. By integrating them with baseline attacks, we achieve higher success rates and break strong defense mechanisms. Our future work will explore the use of momentum methods like Adam to strengthen attacks, leveraging the scale-invariant property of deep neural networks. The algorithm for SI-NI-TI-DIM attack is outlined in Algorithm 2. Our future work will explore the use of momentum methods like Adam to strengthen attacks, leveraging the scale-invariant property of deep neural networks. The algorithm of SI-NI-TI-DIM attack is summarized in Algorithm 2, with variations for SI-NI-DIM and SI-NI-TIM attacks. Input includes a clean example x with ground-truth label y, a classifier f with loss function J, perturbation size, maximum iterations, number of scale copies, and decay factor. Gradients are computed and convolved with a pre-defined kernel, with updates visualized on randomly selected benign images. The proposed SI-NI-TI-DIM attack generates imperceptible adversarial perturbations on ensemble models like Inc-v3, Inc-v4, IncRes-v2, and Res-101."
}