{
    "title": "SJgNwi09Km",
    "content": "The latent tree variational autoencoder (LTVAE) is a variant of variational autoencoders with a superstructure of discrete latent variables. The superstructure is a tree structure of multiple super latent variables learned from data. LTVAE can produce multiple partitions of data, each given by one super latent variable, allowing for meaningful clustering of high dimensional data. Cluster analysis is fundamental in unsupervised machine learning and is central to many data-driven applications. Cluster analysis is essential in unsupervised machine learning and various data-driven applications. Different clustering methods like k-means, Gaussian mixture models, and spectral clustering have been proposed. Traditional clustering on high-dimensional and unstructured data, such as images, is challenging due to the complexity of the original data space. Deep learning-based clustering methods have emerged, utilizing deep neural networks to learn nonlinear embeddings for cluster analysis in the embedding space. Representation learning in deep neural networks aids cluster analysis by creating effective high-level representations from high-dimensional data. Unsupervised deep learning methods like RBM, autoencoders, and VAE are commonly used for this purpose. However, traditional deep learning clustering methods often assume a single partition over the data, which may not hold true in real-world scenarios where data can be partitioned in multiple ways based on different attributes. For example, student populations can be clustered based on course grades or extracurricular activities. The proposed unsupervised learning method, latent tree variational autoencoder (LTVAE), aims to discover multi-facet structures in high-dimensional data by learning latent superstructures through neural networks and tree-structured Bayesian networks. This method automatically selects subsets of latent features for each facet and learns the dependency structure among different facets. LTVAE aims to discover multi-facet structures in high-dimensional data through systematic structure learning. Efficient learning algorithms are proposed with gradient descent and Stepwise EM. The paper is organized into sections for related works, the proposed method, learning algorithms, empirical results, and conclusion. Recent advancements in deep learning based clustering methods have gained attention. Deep learning clustering methods like DEC and GMVAE improve cluster analysis by learning feature representations and cluster assignments simultaneously. DEC BID33 enhances clustering by driving the network to learn a better mapping, while Improved Deep Embedded Clustering BID10 adds reconstruction loss to the original clustering loss. GMVAE combines a Gaussian mixture model with a VAE for joint learning of representations and clustering. The method proposed in BID35 uses convolutional neural networks to jointly learn representations and clustering in a recurrent framework. It assumes flat partitions over the data, unlike hierarchical nonparametric variational autoencoders in BID9, which focus on learning a hierarchy of concepts with infinitely deep and branching tree structures. Our work focuses on multifacets of clustering, such as partitioning based on identity of subjects or pose. We present a latent tree variational autoencoder for joint representation and structure learning for multidimensional clustering using deep generative models. The latent tree variational autoencoder learns the deep representations of data by maximizing the marginal loglikelihood over parameters and latent variables. This process, known as representation learning, captures relevant information and forms a Bayesian network with a treestructured latent tree model. The latent tree model defines clusters based on discrete latent variables Y, each representing a facet partition over the data. Each observed variable z follows a conditional Gaussian distribution given a value y of Y. The model consists of Gaussian mixture models where each z and its parent form a GMM. The marginal distribution of z sums over all possible combinations of Y states, making it a Gaussian LTM with one latent variable connecting to all observed variables. The proposed LTVAE model aims to jointly learn data representations and the latent structure of Y, with the latent structure S automatically learned from data. Amortized variational inference is used for the latent variable z, with an inference network BID16 defining an approximate posterior q\u03c6(z|x). The evidence lower bound (ELBO) L ELBO of the marginal loglikelihood of the data given (S, \u0398) is calculated. The ELBO can be estimated using Monte Carlo sampling, with the marginal loglikelihood efficiently computed through message passing in Bayesian networks. Message passing involves building a clique tree and forming cliques with potential functions. In this section, efficient learning algorithms for LTVAE are proposed through gradient descent and stepwise EM with message passing. The parameters of neural networks can be optimized through stochastic gradient descent. The gradient of the marginal loglikelihood from the latent tree model is crucial for learning the model. The text proposes an efficient method to compute the gradient of the marginal loglikelihood in a latent tree model through message passing. This allows for the optimization of parameters in the inference and generation networks using stochastic gradient descent. The Stepwise EM algorithm is proposed to jointly learn the parameters of the latent tree \u0398. It involves maximizing the marginal loglikelihood of z under the latent tree and updating the parameter \u0398 using expected sufficient statistics. The update is performed iteratively with a learning rate \u03b7 and complete data loglikelihood l. The Stepwise EM algorithm involves updating the parameters of the latent tree using expected sufficient statistics with a learning rate \u03b7 and complete data loglikelihood l. Each iteration of LTVAE includes gradient descent for neural network parameters and Stepwise EM for latent tree model parameters. Determining the latent structure involves the number and cardinalities of latent variables, as well as connectivities among variables. The goal is to find the model that maximizes the BIC score, balancing model fit and complexity through likelihood and penalty terms. Systematic searching is used to find a structure with a high BIC score. The systematic search for a high BIC score structure involves using the hill-climbing algorithm with 7 search operators. These operators include node introduction, node deletion, state introduction, state deletion, node relocation, pouching, and unpouching. The operators produce candidates from the existing structure, with the best candidate chosen to improve the previous one. The search process involves expanding the structure with SI, NI, and PO, adjusting with NR, and simplifying with UP, ND, and SD. The algorithm for structure learning involves iterative adjustments and simplifications to find the best model. Acceleration techniques are used to make the process efficient. The learning process includes parameter learning of neural networks and improving the structure of the latent tree model. Starting from a pretrained model, the algorithm iteratively refines the structure and parameters while learning data representations through neural networks. The effectiveness of the method is demonstrated through experimentation. The proposed method demonstrates effectiveness through synthetic data with two facets controlling attributes in a latent representation. The generative model defines clusters in each facet and generates samples for recovering the structure and latent domain from observations. The LTVAE successfully discovers the true superstructure of Y1 and Y2, with separable latent space clusters matching ground-truth cluster assignments. The proposed LTVAE model is evaluated on various datasets and compared against other deep learning based clustering algorithms. The study explores different clustering methods, including AE+GMM, VAE+GMM, DEC, and DCN, using datasets like MNIST, STL-10, Reuters, and HHAR. Evaluation is done using specific network structures for encoder and decoder layers, with a pretraining procedure involving greedy layer-wise pretraining and fine-tuning of a deep autoencoder. The weights of the deep autoencoder are used to initialize the encoder and decoder weights. The weights of the deep autoencoder are used to initialize the encoder and decoder networks. Different hyperparameter settings are used for DEC, DCN, and LTVAE. LTVAE uses Adam optimizer with a learning rate of 0.001 and mini-batch size of 128. Stepwise EM has a learning rate of 0.01 and updates the latent tree model every 5 epochs. Candidate models are optimized with 10 random restarts and trained with EM for 200 iterations. The proposed LTVAE model outperforms conventional VAE and IWAE in fitting data. It achieves higher test data loglikelihood and ELBO, indicating better modeling of complex data distributions. The model features variable selection for clustering and uses standard unsupervised evaluation metrics for comparisons with other algorithms. The LTVAE model automatically determines the number of facets and latent superstructure through structure learning. It achieves high clustering accuracy on datasets like MNIST, STL-10, Reuters, and HHAR, outperforming other methods. The model provides multiple partitions over the data, explaining the underlying structure effectively. The proposed LTVAE model provides multiple partitions over the data, explaining it in multi-faceted ways. In the experiment, the z dimension is set to 20, resulting in clean clustering for MNIST digits in facet 1 and a more detailed partition based on shape and pose in facet 2. Similarly, four facets are discovered for STL-10 dataset, showing visible patterns like animals with clearly visible eyes in facet 2. In facet 2, animals like cats, monkeys, and birds have visible eyes, while deers show antlers/ears. Cars are seen in frontal view in facet 2, while facets 1 and 3 show side views. Facet 1 clusters have the same types of objects/animals, while facet 3/4 clusters have a similar feel but not necessarily the same type. The data structure in latent space allows for structured sampling through ancestral or Gaussian mixture component sampling for image generation. LTVAE allows for structured sampling by picking a component from the Gaussian mixture to sample z from, generating samples with clear semantic meaning. Conditional image generation can alter attributes of the same digit. LTVAE learns dependencies among latent variables, which are often correlated. Experiments show that removing these dependencies results in inferior data loglikelihood. The inference network uses mean-field inference network with the same structure as the generative network. In this paper, the authors propose LT-VAE, an unsupervised learning method that combines representation learning and multidimensional clustering. They suggest using a more expressive inference network structure, such as one parameterized with MADE model, to improve learning. The LT-VAE learns latent embeddings and discovers multi-facet clustering structure based on subsets of latent features. The proposed LT-VAE method achieves state-of-the-art clustering performance by discovering multi-facet structures of the data. For the MNIST dataset, the conditional probability between identity facet Y1 and pose facet Y2 is shown, revealing clusters in Y1 facet corresponding to multiple clusters in Y2 facet. LTVAE allows for conditional image generation by fixing variables in one facet and sampling in others. The proposed LT-VAE method allows for conditional image generation by fixing variables in one facet and sampling in others, as demonstrated in the MNIST dataset. The computational time comparison between LTVAE with structure learning and fixed structure shows changes in the pose of input digits in the generated samples."
}