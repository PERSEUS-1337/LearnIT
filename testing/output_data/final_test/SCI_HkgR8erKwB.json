{
    "title": "HkgR8erKwB",
    "content": "Bayesian neural networks are successful in various scientific fields due to their ability to extract desired representations from large datasets effortlessly. However, generalization bounds for this setting are still missing. In this paper, a new PAC-Bayesian generalization bound for the negative log-likelihood loss is presented, utilizing the Herbst Argument for the log-Sobolev inequality to bound the moment generating function of the learner's risk. Deep neural networks, known for achieving state-of-the-art results, do not tend to overfit in practice. Various works on generalization bounds have investigated this phenomenon, with many bounds applying to neural networks, assuming the loss function is bounded. In this work, a new PAC-Bayesian generalization bound for the negative log-likelihood loss of deep neural networks is introduced. The bound incorporates the norm of gradients with respect to the input and the KL-divergence between the learned posterior and the prior, capturing model expressivity and learning complexity. The PAC-Bayesian bound introduced in this work incorporates the norm of gradients, KL-divergence between learned posterior and prior, and model expressivity. It is shown to be tightest when learning mean and variance of each parameter separately. The optimization minimizes the gap between risk and empirical risk, outperforming standard Bernoulli dropout and other Bayesian inference approximations. Our work explores the use of Bayesian inference approximations, such as Monte Carlo dropout and Gaussian dropout, to improve uncertainty estimates compared to standard methods. Various works have applied different posteriors in Bayesian neural networks, with some focusing on learning the mean and variance of parameters separately. The connection between PAC-Bayesian bounds and these Bayesian inference techniques is highlighted. Our work utilizes the connection between PAC-Bayesian bounds and Bayesian inference to optimize prior parameters in a Bayesian setting for deep neural networks. Approximations are necessary due to the computational complexity of exact Bayesian inference. Blundell et al. (2015) used variational Bayesian practice to determine mean and standard deviation for model parameters. Experimental validation stressed the importance of learning both mean and variance. Generalization bounds provide statistical guarantees on learning algorithms by measuring performance on test data compared to training data. Risk of a learner is its average loss when data is sampled from an unknown distribution. PAC-Bayesian theory bounds learner risk when parameters are averaged. The PAC-Bayesian theory bounds the risk of a learner by averaging parameters over the learned posterior distribution q, which is obtained from training data S. The theory is connected to Bayesian inference, with a focus on the negative log-likelihood loss function. Germain et al. (2016) showed that the optimal posterior in this setting is q(w) = p(w|S). The PAC-Bayesian theory relies on an approximated posterior q to bound the risk of a learner by averaging parameters. This approximated posterior is crucial for the evidence lower bound (ELBO) used in Bayesian neural networks. While PAC-Bayesian and ELBO bounds share similarities in learning optimization, they serve different purposes - one for risk bounding and the other for marginal loglikelihood bounding. Despite this, both can be optimized using the same algorithms, influencing the practice of Bayesian neural networks. The practice of Bayesian neural networks involves deriving a PAC-Bayesian bound for the negative log-likelihood (NLL) loss, which is challenging due to the unbounded nature of the loss function. The log-partition function is bounded for uniformly bounded loss functions, but becomes unbounded for NLL loss. The main theorem shows that for smooth loss functions, the log-partition function is bounded by the expansion of the loss function's gradient norm. The log-partition function is bounded by the gradient norm of the loss function for smooth loss functions in Bayesian neural networks. This property leads to tighter generalization bounds for deep networks compared to shallow networks. The proof technique involves using the Log-Sobolev inequality for Gaussian distributions. The Gaussian assumption for the data generating distribution can be relaxed to any log-concave distribution. The proposed bound for generalization in deep networks consists of two terms: the log-partition function, which is influenced by the norm of gradients with respect to the input, and the KL-divergence between the learned posterior and the prior. The proof involves using the Herbst Argument and the Log-Sobolev inequality to bound the moment-generating function in three steps. The proof involves using the Herbst Argument and the Log-Sobolev inequality to bound the moment-generating function. The bound in Theorem 2 is favorable for deep networks due to rapid gradient decay. The technique can also be applied to shallow nets trained with NLL loss, obtaining PAC-Bayesian bounds for multi-class logistic regression. Logistic regression with Gaussian distribution for multi-class classification is analyzed using a PAC-Bayesian bound. The proof involves the negative log-likelihood loss and prior density function. The result shows that the bound can achieve a rate of \u03bb = m, extending the PAC-Bayesian framework for regression. The PAC-Bayesian bound for logistic regression is derived by analyzing the gradient of log p(y|x, w) with respect to x. The gradient norm is upper bounded, leading to a final bound. Empirical studies on the derived bound include an ablation study and results for multiclass classification tasks with different datasets and architectures. The section concludes with an analysis of models' uncertainty estimates. The study explores the effect of \u03c3 p on model performance and generalization bound using Bayesian Neural Networks. Models were trained on MNIST and Fashion-MNIST datasets with different \u03c3 p values. Results show the impact on train and test loss and accuracy. The study investigates the impact of different \u03c3 p values on model performance and generalization bound using Bayesian Neural Networks. Results show that \u03c3 p = 0.1 performs better in terms of average loss and accuracy on both MNIST and Fashion-MNIST datasets compared to \u03c3 p = 0.2. Additionally, the effect of \u03bb on constructing tighter generalization bounds is discussed. The study explores the impact of \u03bb on model generalization bounds, which depend on the model architecture's gradient norm. Five models with varying layers were trained for classification and regression tasks using different datasets. All models, except linear ones, had similar parameter numbers and used ReLU activation. The study investigates the effect of model depth on generalization bounds, showing that deeper models lead to tighter bounds across datasets. Deeper models also exhibit smaller gradients, potentially explaining their better performance in downstream tasks. This is supported by the rapid decay of gradients with increasing model depth, as shown in Figure 1a. The study explores the impact of model depth on generalization bounds, revealing tighter bounds with deeper models. Gradients decay rapidly with more layers, leading to improved performance. Visualization of weights in Bayesian Neural Networks shows average mean at zero and standard deviation approaching 0.1 for MNIST and Fashion-MNIST models. The study investigates the impact of model depth on generalization bounds, showing tighter bounds with deeper models. The standard deviation of model weights decreases on the last layer, possibly due to fast optimization. Bayesian Neural Networks are compared against softmax and Monte Carlo Dropout baselines in multi-class classification experiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Train set and test set loss, accuracy, and generalization gaps are reported. In multi-class classification experiments, results for BNN and MC-Dropout models are reported, showing comparable results to baselines with lower loss and accuracy generalization gaps. Dropout value of 0.3 is used for Softmax throughout the experiments. In the experiments, a dropout value of 0.3 is used for Softmax and MC-Dropout models, and \u03c3 p = 0.1 for BNN models after grid search. Uncertainty estimates of BNN models are evaluated against softmax and MCDropout models using in-distribution and out-of-distribution examples. The Bayesian approach with carefully chosen priors can lead to better uncertainty estimates. Results suggest that BNNs produce better calibrated outputs for all settings, except for ECE for MNIST and MCE for CIFAR10. Uncertainty estimates were evaluated using OOD examples from different datasets, showing that all models performed at chance level. Higher loss values were observed for softmax and MC-Dropout models. Further OOD experiments with different dropout and prior rates can be found in the Appendix. In the study, higher values were observed for the softmax and MC-Dropout models, indicating overconfidence in their predictions. Average entropy was measured across all models, with Bayesian Neural Networks (BNNs) showing higher entropy and better uncertainty estimates. A new PAC-Bayesian generalization bound for deep learning with NLL loss function was introduced, providing insights for model optimization and prior distribution search. Learning the mean and STD for all parameters along with optimizing the prior leads to improved uncertainty estimates and prevents overfitting. The proof involves decomposing a function using statistical independence, computing derivatives, and applying the log-Sobolev inequality for Gaussian distributions. The gradient of log p(y|x, w) with respect to x is then analyzed to apply Theorem 2. The architectures used in this section are for multi-class, binary, and uncertainty estimates experiments. Multilayer perceptrons are used for the MNIST dataset, while convolutional neural networks (CNNs) are used for Fashion-MNIST and CIFAR-10. In this section, CNNs are used for Fashion-MNIST and CIFAR-10 datasets. The NLL loss function is optimized using SGD with a learning rate of 0.01 and momentum of 0.9. Mini-batches of size 128 are used without any learning rate scheduling. For MC-Dropout models, a weight decay value of 1e-5 provides the best validation loss. Experiments on binary classification with MNIST dataset show consistency with previous studies, using multilayer perceptrons with one hidden layer of 300 neurons and ReLU activation function. The study compares BNN to softmax models with dropout rates. Both models achieve similar accuracy levels, but softmax models have larger generalization issues. Using higher Bernoulli-dropout rates helps mitigate generalization errors in softmax models compared to BNN. Additional experiments explore dropout and \u03c3 p values for different models. Results for multi-class classification are summarized in Table 7. The generalization bound is analyzed as a function of network depth using the Fashion-MNIST dataset."
}