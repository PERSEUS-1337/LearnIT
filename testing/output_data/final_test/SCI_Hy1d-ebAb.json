{
    "title": "Hy1d-ebAb",
    "content": "Graphs are essential data structures used to model real-world data. This paper focuses on learning generative models of graphs to generate samples with similar properties as the dataset. The unique challenges include handling symmetries and ordering of elements during generation. A generic graph neural net model is proposed for generating arbitrary graphs and its performance is compared to baselines on various tasks. Potential issues and open problems are also discussed. Graphs are natural representations of information in various domains, such as knowledge graphs and social networks. Capturing the distribution of graphs has many applications, like in drug discovery. Modeling distributions on graphs is essential for obtaining graph-structured semantic representations for natural language sentences and providing priors for Bayesian structure learning of graphical models. Distributions on graphs can provide priors for Bayesian structure learning of graphical models. Random graph models make strong independence assumptions and are effective for certain graph properties like degree distribution. However, more expressive models using graph grammars are needed for richly structured graphs encountered in chemistry or natural language processing. Graph grammars are systems of rewrite rules that derive an output graph via transformations of intermediate graphs. Inducing grammars from unannotated graphs is challenging, and traditional graph grammars make a hard distinction between included and excluded elements. A new model developed in this work can assign probabilities to any arbitrary graph without making assumptions on the graphs. Our model generates graphs using graph grammars, where new structure is added based on the graph derivation history. Graph nets are used to represent the graph during each step, providing a good match for our purposes. The model is evaluated by fitting graphs in three problem domains. Our proposed model outperforms random graph models and LSTM baselines in generating graphs with specific properties, molecule graphs, and conditional parse tree generation. Challenges include learning and optimization difficulties, with potential improvements discussed. The earliest probabilistic graph model assumed independent edge probabilities, leading to rich theory but limited complexity. Recent models incorporate preferential attachment, where nodes with more connections are more likely to form new edges. The Kronecker graphs model proposed by BID18 can capture multiple properties of graphs but has limited capacity for mathematical analysis. Other models focus on specific properties like small diameter and local clustering. Work in natural language processing and program synthesis communities has also explored tree generation models. Our graph generative model, based on neural net models called graph nets, is capable of generating more general graphs with complicated loopy structures. Various variants of these models have been developed and applied to graph problems, learning representations of graphs, nodes, and edges through a propagation process. Graph nets are used to learn representations for decision-making in the graph generation process. While our work is similar to BID12 in constructing graphs for reasoning problems, our goal is to learn unconditional or conditional densities on a space of graphs. BID12 focuses on using graphs as intermediate representations in reasoning tasks, with probabilistic semantics for their graphs. However, as a generative model, BID12 makes strong assumptions for the generation process. Our model differs from BID12 in that it does not make strong assumptions for the generation process. While BID12 uses soft node/edge connectivity for reasoning tasks, our model generates graphs with either existing or non-existing nodes/edges. The dense and soft representation in BID12 may be suitable for some applications, while our sparse discrete graph structures may be better for others. Our graph generative model can potentially be used in end-to-end pipelines for prediction tasks like BID12. Our model generates graphs in a sequential process, adding nodes one by one and connecting them to the existing graph. The algorithm involves sampling whether to add a new node or terminate, adding the node to the graph, checking for additional edges needed, and connecting the new node to an existing node. This process repeats until the model decides not to add another edge. The generation process can be customized by making edges directional or typed. The graph generation process involves making directional or typed edges and imposing constraints on structural aspects. It is a sequential decision-making process where nodes are added with probabilities, edges are added or not, and connections are made to existing nodes. Different orderings of nodes and edges can lead to different decision sequences for the same graph. The graph generation process involves sequential decision-making to add nodes and edges, with different orderings leading to varied decision sequences. To model this process, graph nets are proposed over conventional LSTM language models, as they utilize the graph structure for better representation of nodes and edges. Node embeddings are associated with each node in a graph, computed from node inputs and propagated to aggregate information from the local neighborhood. The propagation process involves iterative computation of message vectors on edges, updating node representations based on incoming messages. This process utilizes neural networks for mappings and can include recurrent neural network cores like GRU. Neural nets like GRU or LSTM are used for node embeddings in graph propagation. Multiple rounds of propagation aggregate information from neighbors without changing the graph structure. Different rounds can have different parameters to increase model capacity. To compute a graph representation, node representations are mapped to a higher dimension. Neural nets like GRU or LSTM are used for node embeddings in graph propagation, with multiple rounds of propagation aggregating information from neighbors. To compute a graph representation, node representations are mapped to a higher dimension, and a gating network can be used for aggregation. The graph generative model defines a distribution over the sequence of graph generating decisions. The decision steps in the algorithm are modeled using three modules, with the first module taking an existing graph and node representations as input to determine whether to terminate the algorithm or add another node. This involves running rounds of propagation, computing a graph representation vector, and predicting an output using a standard MLP. The algorithm's decision steps involve three modules: one determines whether to terminate or add a node based on graph and node representations, another predicts the probability of adding an edge to a new node, and the last computes scores for nodes after propagation rounds. When a new node is added to the graph, its state vector is initialized using input features associated with the node and a graph representation. The node state for the new node v is initialized as x v plus R init (h V , G) computed by an MLP. If R init (h V , G) is not used, nodes with the same input features added at different stages of the generation. The graph generative model addresses the issue of nodes with the same input features having the same initialization. Conditional generation can be achieved by adding conditioning information in specific modules of the model architecture. Conditioning information is added in modules such as the propagation process and node state initialization. In experiments, conditioning information is used in specific modules like f n and f init. Techniques like attention can also be applied to improve conditioning using the graph representation. The graph generative model defines a joint distribution over graphs and node/edge ordering. Computing the marginal likelihood for large graphs is intractable, requiring sampling or approximation methods like importance sampling. The graph generative model defines a joint distribution over graphs and node/edge ordering. To train and evaluate the model, samples are taken from q(\u03c0 | G) and the joint distribution p(G, \u03c0) is learned by maximizing the expected joint log-likelihood. Training with a specific p data (\u03c0 | G) drives the model distribution p(\u03c0 | G) close to the proposal distribution q(\u03c0 | G). The study focuses on improving the estimate of marginal probability by aligning the model distribution with the proposal distribution. Different ordering strategies for graph models are explored, with potential for learning an optimal ordering in future work. Various graph generation models and ordering techniques are evaluated on three tasks, with detailed results in the appendix. In the first experiment, graph generative models are trained on synthetic undirected graphs including cycles, trees, and Barabasi-Albert model BID1. Data is generated during training with specific node connections. The model is compared against BID8 random graph model and a LSTM baseline. Edge probability parameter is estimated using maximum likelihood, and LSTM language models are trained on decision sequences. Uniform random permutations of graph orderings are applied during training. During training, graph models and LSTM models are compared using randomly permuted graphs. The graph model has 16 node state dimensions and 2 propagation steps, while the LSTM model has a hidden state size of 64. Both models have a similar number of parameters. Training curves show that graph models train faster and perform better. Degree histograms for Barabasi-Albert Graphs are compared between models. The data distribution estimated from 10,000 examples is shown, along with the evaluation of samples from different graph models. Results in Table 1 and Figure 3 demonstrate the graph model's ability to match training data metrics. The success of the graph model over the LSTM baseline is attributed to its capability to refer to specific nodes in a graph. The graph model is effective in handling varying sets of objects and pointers, which is challenging for LSTM. In the second experiment, graph generative models are trained for molecule generation, utilizing the graph structure of molecules from the ChEMBL database. The study utilized previous versions of ChEMBL for molecule generation, filtering molecules with up to 20 heavy atoms. The dataset was split into training, validation, and testing sets. RDKit was used to convert SMILES strings to graph representations of molecules. Model hyperparameters were tuned on the validation set, with the number of propagation steps chosen from {1, 2}. The graph model was compared with LSTM language models trained on SMILES strings. Training was done using canonical SMILES representations with associated edge ordering, and models were also trained with permuted ordering. The models were trained with permuted ordering, evaluating negative log-likelihood on small molecules with up to 6 nodes. Results show the number of valid molecule representations generated and whether they were seen in the training set. Models trained with random ordering are not tailored to the task. In Appendix C.2, the distribution of chemical metrics for generated samples is shown to assess quality. The LSTM model on SMILES strings performs slightly better in likelihood under canonical ordering, but the graph model generates more valid and novel samples. The LSTM model trained with random ordering shows improved performance on canonical ordering, likely due to overfitting. The Graph architecture outperforms LSTM in NLL using a generic graph generation decision sequence. It is challenging to estimate the marginal likelihood for large molecules, but feasible for small molecules with up to 6 nodes. The evaluation of 6 models on small molecules with no more than 6 nodes showed that the graph model trained with random ordering had better marginal likelihood. The models trained with fixed ordering did not always have the best ordering, indicating potential for learning an ordering. Visualization in Figure 5 displayed different approaches in molecule generation between models trained with canonical and random ordering. In the last experiment, a conditional graph generation task was conducted using the Wall Street Journal dataset. LSTM sequence to sequence models with attention were trained on sequentialized parse trees. The graph model was trained using top-down depth-first traversal ordering, with experiments also conducted using breadth-first traversal ordering. The graph model was modified to pick one node as the parent for generating edges. The experiment involved training LSTM sequence to sequence models with attention on sequentialized parse trees for a conditional graph generation task. The graph model, using top-down depth-first traversal ordering, was modified to select a parent node for edge generation. Results showed that the LSTM model performed better on both perplexity and sample quality metrics compared to the graph model. The LSTM on sequentialized trees outperforms the graph model in a conditional graph generation task. The graph model relies on limited propagation steps for communication, hindering awareness of structural changes. Increasing propagation steps can improve information flow but slows down training. A tree-structured model like R3NN BID25 may be more suitable for propagating information efficiently. The graph model faces challenges in modeling longer sequences and changing structures, making training harder than LSTMs. Despite being capable of generating arbitrary graphs, there are still challenges to address, such as ordering nodes and edges for effective learning and evaluation. Learning an ordering of nodes and edges as a latent variable could be a potential solution. The graph model faces challenges in modeling longer sequences and changing structures, making training harder than LSTMs. To address the issue of scalability, the graph generative model proposed in the paper struggles with large graphs leading to very long generating sequences. This poses a significant disadvantage as it not only makes likelihood estimation difficult but also complicates training. To alleviate this problem, adjustments can be made to tie the graph model more closely to the problem domain, reducing multiple decision steps and loops to single steps. The paper proposes a deep generative model for graph generation, addressing challenges in training due to long sequences and changing structures. Lowering the learning rate can help with instability, but tweaking the model architecture may provide more effective solutions. The model can generate arbitrary graphs through a sequential process, studied on various graph generation problems. The model proposed in the paper is a deep generative model for graph generation, which has unique advantages over standard LSTM models. The model can generate arbitrary graphs through a sequential process, as demonstrated in Algorithm 1. The hope is that these results will inspire further research in this direction to improve generative models of graphs. The paper introduces a deep generative model for graph generation, showcasing its advantages over standard LSTM models. The model can create graphs sequentially, as illustrated in Algorithm 1. An example graph with three nodes and three edges is presented, along with two possible decision sequences. The text also mentions the possibility of multiple orderings to generate a graph. Additional implementation details of the graph generative model are discussed in this section. The implementation details of the graph generative model include using a fully connected neural network for the message function, implementing a recurrent cell in RNNs for the node update function, and using linear layers in the message functions. Parameters for the functions vary based on the task. In the graph generative model, parameters vary for different tasks. GRU and LSTM cells perform equally well, with GRU cells being slightly faster. Different parameters on all layers perform better than tied parameters. Gated sum is better than a simple sum for aggregating graph representations. In the graph generative model, gated sum is preferred over a simple sum for aggregating graph representations. The module f addnode takes an existing graph as input and produces a binary or categorical output. The probability of adding one more node is determined based on the graph type. The module for adding a new node to a graph produces a probability of adding an edge. It treats the new node differently in terms of parameters and always produces Bernoulli probabilities. Another module selects a node to connect to the new node based on computed scores. When adding a new node to a graph, a vector is produced for each node by adjusting the output size of the MLP. The probability of selecting a node and edge type is determined by a softmax function. The node vector is initialized using node type embedding, graph representation, and conditioning information. In graph generation tasks, an attention mechanism is used to compute a query vector for each input word. An LSTM is utilized to obtain word representations, and a score is computed using the query vector. The model is trained to maximize the expected joint likelihood of training graphs and specified orderings of nodes and edges. The log-likelihood log p(G, \u03c0) can be computed for a sequence in graph generation tasks. Two possibilities for p data (\u03c0|G) are explored: canonical ordering and uniform random ordering. Canonical ordering is a fixed ordering of graph nodes and edges, while uniform random ordering involves generating a random permutation of node indices. In this section, the experiment setup for a graph model with a hidden size of 64 and node states size of 16 is described. The LSTM model has a hidden size of 512, and both models use learning rates from {0.001, 0.0005, 0.0002}. The graph model utilizes GRU cores, while LSTM models work slightly better for the baselines. The LSTM models outperform the graph model. Parameters like node state dimensionality and learning rate are chosen through grid search. The best learning rate for the graph model is 0.0001, while for LSTM models it is 0.0002 or 0.0005. The LSTM model uses a dropout rate of 0.5, while the graph model uses 0.2. Increasing the number of propagation steps T is beneficial for graph representations, but a smaller T already shows good performance. The graph model is 2-3x slower than the LSTM model with similar parameters. Chemical metrics distribution for valid samples generated from trained models is examined. Histograms show distribution across samples and KL divergence is computed compared to training set metrics. All models match training distribution well. The graph and LSTM models trained on permuted nodes and edge sequences have a bias towards generating molecules with higher SA scores, indicating ease of synthesis. Training with RL can help overcome this bias. These models also show a tendency to generate larger molecules with more atoms and bonds. Further improvements are needed as they are not overfitting even after 1 million training steps. The graph and LSTM models trained on permuted nodes and edge sequences generate molecules with higher SA scores. By adjusting bias parameters for adding nodes and edges, the model can control graph size and edge density. Fine-tuning these biases results in shifts in the distribution of atoms and bonds in generated samples. This level of control allows for adjustments in edge density within the samples. The fine-tuning of bias parameters for adding nodes and edges allows for control over graph size and edge density in molecule generation. Adjusting these biases can lead to shifts in atom and bond distribution, impacting the size and complexity of the generated molecules. This level of control enables adjustments in edge density within the samples. Training a graph model with canonical ordering can lead to overfitting, as shown in Figure 9. In contrast, training with uniform random ordering can help reduce overfitting by exposing the model to various possible orderings for the same graph. Training with canonical ordering can lead to overfitting, but using uniform random ordering can help reduce overfitting. Models trained with canonical ordering may not assign the highest probabilities to the canonical ordering after training. The model can learn to concentrate probabilities to orderings close to the canonical ordering, but it still \"leaks\". In this experiment, a graph model with node state dimensionality of 64 and an LSTM encoder with hidden size 256 was used. Attention over input is implemented using a graph aggregation operation. The baseline LSTM models have hidden size 512 for both encoder and decoder, with a dropout of 0.5 applied. The graph model has fewer parameters (24M vs 52M) due to a smaller encoder size. The node state dimensionality and encoder LSTM hidden size were chosen from a grid search. For the LSTM seq2seq model, encoder and decoder sizes are tied and selected from {128, 256, 512}. Learning rate is chosen from {0.001, 0.0005, 0.0002}. Input text for LSTM encoder is always reversed. Graph model experimented with T \u2208 {1, 2, 3, 4, 5}, with T = 2 providing a balance. Visualization shows step-by-step generation process for a graph model trained with permuted random ordering."
}