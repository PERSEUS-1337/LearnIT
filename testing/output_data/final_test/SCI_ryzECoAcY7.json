{
    "title": "ryzECoAcY7",
    "content": "Hierarchical agents can solve sequential decision making tasks efficiently by breaking them down into subtasks. To achieve faster learning, they need to learn multiple levels of policies in parallel. However, this is challenging due to instability issues. A new framework called Hierarchical Actor-Critic (HAC) has been introduced to overcome these challenges in Hierarchical Reinforcement Learning (HRL). The HAC framework aims to address instability issues in learning multiple levels of policies in parallel. It trains each level independently as if lower levels are already optimal, accelerating learning in tasks with continuous state and action spaces. This approach successfully learns 3-level hierarchies and decomposes problems into smaller subproblems for faster sequential decision making. The HRL algorithm must learn multiple levels within the hierarchy in parallel, but existing algorithms struggle to do so efficiently. Learning multiple levels in parallel is challenging due to non-stationary state transition functions in nested hierarchies. The HRL algorithm learns multiple levels within the hierarchy in parallel. A high-level policy outputs a subgoal state for the low level to achieve, and the state it leads to depends on the current low-level policy. When all policies are trained simultaneously, transition functions above ground level will continue to change as long as lower-level policies are updated. This non-stationary setting challenges efficient learning of multiple levels in nested hierarchies. In a non-stationary setting, RL struggles to learn above ground level policies in nested hierarchies. Learning multiple policies in parallel is possible if each level can simulate a transition function using optimal lower level policies. The HAC framework can accelerate this learning process. The HAC framework introduces a new HRL framework that accelerates learning by enabling hierarchical agents to learn a hierarchy of policies. It consists of a hierarchical architecture and a method for learning multiple levels of policies in parallel with sparse rewards. The hierarchies produced by HAC have nested, goal-conditioned policies that break down tasks into subtasks using the state space. The highest level policy outputs a subgoal state based on the current and goal states, which is used as the goal state for the next level down. The HAC framework introduces a new HRL framework that accelerates learning by enabling hierarchical agents to learn a hierarchy of policies. It consists of a hierarchical architecture and a method for learning multiple levels of policies in parallel with sparse rewards. The hierarchies produced by HAC have nested, goal-conditioned policies that break down tasks into subtasks using the state space. The highest level policy outputs a subgoal state based on the current and goal states, which is used as the goal state for the next level down. The ant agent trained with HAC uses a 3-level policy hierarchy to move through rooms to reach its goal, with each level having a certain number of attempts to achieve its goal state. The HAC framework enables hierarchical agents to learn multiple levels of policies in parallel with sparse rewards. It involves a hierarchical architecture where each level outputs a subgoal state for the next level down to achieve. Agents have a fixed number of attempts to reach their goal state at each level before moving to the next subgoal state. Training subgoal policies involves using a transition function that simulates the optimal lower level policy hierarchy. Hindsight action transitions are implemented by using the achieved subgoal state instead of the original one. This allows for learning multiple policies in parallel independently. Training subgoal policies involves learning sequences of subgoal states that can reach a goal state independently of lower level policies. Hindsight goal transitions help in sparse reward tasks by using achieved states as new goal states. Evaluation on grid world tasks and simulated robotics environments showed agents with multiple levels of hierarchy outperforming others. Our framework demonstrates the effectiveness of using multiple levels of hierarchy in learning hierarchical policies, outperforming single-policy agents. Specifically, agents with 3 levels of hierarchy performed better than those with 2 levels. This approach surpassed a leading HRL algorithm on simulated robotics tasks and is the first to learn 3-level hierarchical policies in continuous state and action spaces. Our method, compared to existing approaches like Skill Chaining and HIRO, can learn options in parallel to move the agent from start to goal state efficiently in continuous domains. Our method can efficiently learn options in parallel to move the agent from start to goal state in continuous domains, unlike approaches that do not use hindsight and are less efficient in learning multiple levels of policies in sparse reward tasks. We aim to solve a Universal MDP (UMDP) by augmenting a Markov Decision Process with a set of goals G, where a control policy \u03c0 maximizes the value function v \u03c0 (s, g) for an initial state s and goal g. To implement hierarchical agents in tasks with continuous state and action spaces, two techniques from RL literature will be used: Universal Value Function Approximator (UVFA) and Hindsight Experience Replay. UVFA estimates the action-value function of a goal-conditioned policy using feedforward neural networks. UVFAs can generalize Q-values across different regions of the state, goal, action space, accelerating learning, but are less effective in tasks with sparse rewards. The UVFA struggles to generalize in sparse reward tasks, so Hindsight Experience Replay (HER) is used to accelerate learning by creating copies of transitions with achieved goals. This helps the UVFA learn about regions of the state, goal, action space with high Q-values, aiding in achieving current goals. The HRL framework, Hierarchical Actor-Critic (HAC), efficiently learns levels in a multi-level hierarchy in parallel. It consists of a hierarchical architecture and a method for learning levels simultaneously. The framework aims to learn a k-level hierarchy \u03a0 k\u22121 with k individual policies \u03c0 0 , . . . , \u03c0 k\u22121 in parallel by transforming the original UMDP. Each level of the hierarchy learns its own deterministic actions. In the HAC framework, each level of the hierarchy learns its own deterministic policy. The state, goal, and action spaces are set up in a specific way for each level to enable learning tasks with nested policies. HAC nests policies by embedding the policy at level i \u2212 1 into the transition function at level i. The transition function at each subgoal level works by selecting a subgoal action and assigning it as the goal of the previous level. The state transition function at level i depends on the full policy hierarchy below it. The state transition function at each level in HAC depends on the full policy hierarchy below it. Non-stationary transition functions can arise from updates to lower level policies and exploring lower level policies with a different behavior policy. In HAC, the transition function at each level depends on the full policy hierarchy below it. To effectively learn subgoal policies, each subgoal policy is trained assuming a transition function that uses the optimal lower level policy hierarchy. This approach overcomes non-stationary issues that hinder joint policy learning in RL methods. The subgoal policy in HAC is stationary and independent of lower level policies, allowing simultaneous learning at different levels. Hindsight action transitions simulate optimal policy hierarchy below the current level. After executing 5 primitive actions, the first action by \u03c0 1 is complete, creating a hindsight action transition. Hindsight action transitions use the subgoal state achieved in hindsight as the action component, incentivizing short paths to the goal and being independent of lower level paths. Hindsight action transitions simulate transitions using optimal lower level policy hierarchy \u03a0 * i\u22121. Rewards are based on state reached and goal state. Rewards are -1 if goal not achieved, 0 otherwise. Example: robot receives transition [initial state = s 0 , action = s 1 , reward = -1, next state = s 1 , goal = yellow flag]. The hindsight action transitions created by \u03c0 1 help the high level agent discover subgoals and are robust to changes in the lower level policy. Additional hindsight goal transitions enable each level to learn more efficiently. In sparse reward tasks, hindsight goal transitions are used to help levels learn more effectively. These transitions guarantee that each level receives a transition containing the sparse reward after every sequence of actions. For example, in a toy robot scenario, the low level creates two transitions after each primitive action executed. After each primitive action, the low level creates two transitions. The first transition evaluates the action taken given the goal state, while the second transition temporarily erases the goal state and reward components. Hindsight goal transitions are then created by filling in the missing components after a sequence of primitive actions. After each primitive action, the low level creates transitions evaluating the action taken and erasing the goal state and reward components. Hindsight goal transitions are created by filling in missing components after a sequence of primitive actions, with the \"next state\" elements in transitions selected as new goal states. Rewards are updated to reflect the new goal state, creating transitions for the high level of the toy robot in a similar manner. The UVFA critic function assigns high Q-values to transitions with sparse rewards, incentivizing effective goal-conditioned policies. Hindsight transitions help levels learn multiple policies. The UVFA critic function assigns high Q-values to transitions with sparse rewards, incentivizing effective goal-conditioned policies. However, a key issue remains with hindsight goal transitions in that agents can only learn about a restricted set of subgoal states, limiting the learning potential of hierarchical agents. When the Q-function ignores parts of the action space, it may assign higher values to distant subgoals that the agent is not focusing on, leading to erratic behavior. Hindsight action and goal transitions incentivize proposing paths to the goal state that may not be achievable with the current policy hierarchy. Our framework introduces subgoal testing transitions to address issues with suboptimal path selection by lower level policies. These transitions help agents determine if a subgoal state can be achieved by the current set of lower level policies. Subgoal testing transitions require the current lower level policy hierarchy to be followed exactly. If a subgoal is not achieved in a certain number of actions, a penalty is applied. In experiments, penalty is set to the negative of the maximum horizon of a subgoal. A discount rate of 0 is used to avoid non-stationary transition function issues. The low level policy has a maximum of 5 primitive actions to achieve a subgoal. Missing the subgoal results in a penalty transition. Q-values are affected based on the (state, goal, subgoal action) tuple. If the optimal policy below cannot complete the subgoal in H actions, the critic function learns Q-values of -H. Subgoal testing transitions can address the limitation of training solely with hindsight actions and goals by incentivizing critic functions to learn Q-values close to the targets. For tuples where the subgoal action can be achieved by the lower level policy hierarchy, subgoal testing has minimal impact. However, for tuples where the subgoal action is not achievable by the optimal policy below within H actions, subgoal testing effects are more nuanced. Subgoal testing transitions incentivize critic functions to assign Q-values close to the targets, balancing between hindsight actions/goals and penalty values. The critic is likely to assign Q-values closer to the targets prescribed by hindsight actions and goals rather than the penalty value of -H from subgoal testing transitions. Subgoal testing transitions in Hierarchical Actor-Critic (HAC) help overcome issues from training with hindsight transitions. Critic functions no longer ignore infeasible subgoals, allowing all hierarchy policies to be learned simultaneously. HAC implementation details are provided in the Appendix, along with the discrete version Hierarchical Q-Learning (HierQ). Hierarchical Q-Learning (HierQ) algorithm was evaluated in various tasks including grid world and simulated robotics environments. The experiments can be viewed in a video at https://www.youtube.com/watch?v=DYcVTveeNK0. Performance comparisons were made using different environments such as inverted pendulum and four rooms tasks. Our approach using policy hierarchies with 1, 2, and 3 levels significantly outperformed flat agents in all tasks. The 3-level agent outperformed the 2-level agent, and the 2-level agent outperformed the flat agent. Additionally, our framework showed that it can benefit from additional levels of hierarchy by learning multiple levels of policies in parallel. Comparing our approach HAC to another HRL technique, HIRO, our approach outperformed the other leading HRL techniques. HIRO is a 2-level hierarchical policy that outperforms other leading HRL techniques in continuous state and action spaces. It does not use Hindsight Experience Replay and handles non-stationary transition functions differently by using subgoal actions from a set of candidates. The 2-level version of HAC significantly outperformed HIRO in experiments on inverted pendulum, UR5 reacher, and ant reacher tasks. Ablation studies showed that our method outperformed baselines with subgoal testing procedures. The results and analysis of the ablation studies in section 6 of the Appendix show that the new HRL framework, HAC, can efficiently learn multiple levels of policies simultaneously. HAC overcomes instability issues by training each level of the hierarchy as if the lower levels are already optimal. Results in various domains confirm that HAC significantly improves sample efficiency. HierQ is an algorithm designed for domains with discrete state and action spaces, utilizing trained actor and critic functions. It does not use subgoal testing but instead employs pessimistic Q-value initializations to enhance agent performance. HierQ is an algorithm for discrete state and action spaces, using pessimistic Q-value initializations to improve agent performance. It employs hierarchical Q-learning with multiple levels in the hierarchy and trained Q-tables. The algorithm HierQ utilizes hierarchical Q-learning with multiple levels and trained Q-tables for discrete state and action spaces. The goal space at level 0 is defined as the state space, with a reward function based on the shortest path. The objective is to find a policy that maximizes the value function for initial state and goal combinations required by the policy from the higher level UMDP. The algorithm HierQ uses hierarchical Q-learning with multiple levels and trained Q-tables for discrete state and action spaces. The state space and action space are the same, with goals dictated by the level above except for the top level. Two transition functions are used for generating hindsight transitions and subgoal testing. The reward function is R i. The reward function in each level follows a specific hierarchy and issues penalty rewards during subgoal testing. The objective is to learn a policy for executing actions based on the current state and level. The algorithm HierQ utilizes hierarchical Q-learning with multiple levels and trained Q-tables for discrete state and action spaces. The subgoal testing ablation studies showed that unrealistic subgoals led to long sequences of actions not trained for, resulting in high Q-values. Penalizing subgoals even with a noisy lower level policy hierarchy performed worse than the implementation. The comparison of subgoal testing procedures showed that always penalizing missed subgoals incentivizes overly conservative subgoal levels, leading to longer learning times. Subgoal levels prioritize setting nearby subgoals due to noise in actions. Key details of the HAC implementation, including DDPG parameters, can be found on the GitHub repository. Q-Values: Critics' output is bounded to [-H, 0] using a negative sigmoid function. The lower bound of -H helps in learning Q-values for relevant actions. DDPG Target Networks: Target networks were removed for 2- and 3-level agents as they performed well without them. Flat agents generally performed better with target networks. The flat agent generally performed better with target networks, but our results show the better result of the two. Exploration strategy involves sampling actions randomly and adding noise. Neural networks had 3 hidden layers with 64 nodes each. HAC parameters include a maximum horizon of a subgoal."
}