{
    "title": "SklzIjActX",
    "content": "This paper introduces a technique for 8-bit low precision inference of convolutional neural networks, improving throughput and latency for deep learning applications. The optimized model is generated from a FP32 model without retraining, showing effectiveness across various applications like image classification and object detection. The methodology improves throughput and latency by 1.6X and 1.5X respectively with minimal loss in accuracy from FP32 baseline. Low precision inference is a key technique to address the high computational complexity of convolutional neural networks (CNN) during industrial deployment. Hardware acceleration enables more operations per second, reduced memory access pressure, and higher throughput and lower latency. The code and models will be publicly available soon. Convolution in CNN models commonly uses 8-bit low precision (INT8) inference for better performance. While INT8 computation works well for standard convolution, it struggles with large groups, especially in depthwise convolution. Exploring INT8 Winograd convolution can improve performance, but current solutions are limited by the complexity introduced by sum and concatenation operations in certain blocks. In CNN models, INT8 is commonly used for better performance in convolution. Current solutions struggle with large groups, especially in depthwise convolution due to complexity in sum and concatenation operations. This paper presents a technique for efficient INT8 inference, focusing on channel-wise quantization of convolution to maintain accuracy. The paper introduces an approach to channel-wise quantization of convolution for maintaining accuracy in depthwise convolution. It improves INT8 inference accuracy on MobileNet-V1 and MobileNet-V2. Additionally, it explores INT8 Winograd convolution and its calibration details, showing minimal accuracy loss on VGG-16. The support for sum in residual block, concatenation in inception block, and convolution for classification is added, enhancing inference speed with topology-wise INT8 support. The paper presents a systematic study on improving speed through data conversion reduction and memory saving. A calibration tool is developed to automatically generate optimized INT8 models from FP32 models for easy deployment. The study demonstrates the effectiveness of INT8 inference across various CNN models, improving throughput and latency while minimizing accuracy loss. The methodology is believed to be applicable to other CNN models and frameworks, with code and models to be made publicly available soon. Section 2 discusses low-precision inference in deep learning, Section 3 describes INT8 inference quantization approach for CNN models, Section 4 includes experimental results, and Section 5 concludes with future directions. CNN models face challenges in industrial deployment due to high computational complexity, but INT8 inference has shown minimal accuracy loss on models like GoogleNet and AlexNet. Researchers have experimented with low-precision inference in addition to existing industry tools and frameworks. Researchers have experimented with low-precision inference using customized low-bit activation and weights in deep learning tasks. Various quantization techniques have been explored to reduce storage requirements of neural networks, including using 8-bits of fixed precision with different bit-widths for number representation. Deep compression methods such as pruning, quantization, and Huffman coding have been utilized to maintain network accuracy while reducing storage needs. Huffman coding reduces storage requirements for neural networks without affecting accuracy, enabling deployment on edge devices. Efficient inference on commodity servers is emphasized, avoiding the need for special hardware like FPGA. Quantization and de-quantization processes are mathematically formulated for INT8 inference, with a focus on converting rational tensors to integer tensors with specific scale factors and bit-precision. The de-quantization function D approximates rational tensors with quantized forms for efficient inference on commodity servers. Sampling is done to set scale factors based on maximum absolute values, with precision p = 8 for non-negative activation tensors. Negative input tensors fall back to FP32 for INT8 convolution support. The INT8 convolution supports non-negative activations with precision p = 7 for weight tensors. Round-half-to-even is used for best statistical accuracy. The general INT8 recipe includes depthwise convolution, Winograd convolution, and more INT8 support. It works well for standard convolution but not for large groups or depthwise convolution. Depthwise convolution applies a single filter per input channel, requiring channel-wise scale factors for weight tensor to represent dynamic data ranges effectively. This approach improves Top1 accuracy of INT8 inference on MobileNet-V1 and MobileNet-V2 significantly. The study explores improving INT8 inference accuracy on MobileNet-V1 and MobileNet-V2 using Winograd convolution. The focus is on INT8 Winograd convolution with the F(2, 3) algorithm, leveraging integer-based input transformation matrix for performance gains. The key component is determining the scale factor for activation and weight after transformation. Equation FORMULA5 outlines the computation of the scale factor after transformation using transformation matrices B and B T from BID14. The study focuses on improving INT8 inference accuracy on MobileNet-V1 and MobileNet-V2 using Winograd convolution with the F(2, 3) algorithm. It involves determining scale factors for activation and weight after transformation, with experiments on VGG-16 showing minimal accuracy loss with certain scale factors. The approach is general and can be applied to other algorithms besides F(2, 3), extending INT8 computation to various types beyond convolution. In this section, we discuss topology-wise INT8 opportunities for computations, including max pooling and average pooling with INT8. For average pooling, we use INT32 accumulators to prevent arithmetic overflow. The inception block concatenates convolution outputs per filter, with INT8 output tensor scale factor set to the smallest scale factor of input tensors. Batch normalization folding in recent CNN models poses challenges for computing INT8 batch normalization without accuracy loss. Batch normalization is typically added after convolution in recent CNN models. It can be folded into the convolution kernel during inference. The convolution weight and bias are affine transformations of the original weight and bias. Post-operations like ReLU, Sum, and BatchNorm are efficiently computed in registers for residual networks. The convolution output is accumulated in FP32 and fused with post-operations before quantization back to INT8. The text discusses the fusion of post-operations like ReLU and Sum in residual networks, along with the development of a calibration tool for INT8 inference. The tool automatically generates optimized INT8 models from FP32 models without the need for fine-tuning or retraining, using a calibration dataset and pre-trained FP32 weights. The calibration tool facilitates the calibration process by providing options for iteration number, scale factor mode, calibration strategy, and accuracy tuning on INT8 models. It allows users to define parameters for sampling on activation and compute scale factors using Direct or KL algorithms. The tool also includes 18 CNN models for experimentation, selected based on classical representation and diversity. The rules for model selection include using classical and representative models from various use cases that are publicly available with pre-trained weights or easy to train. The Topology column displays the selected CNN models, including ResNet-50 and its variant from FaceBook. The Use case column categorizes the models for image classification, object detection, image segmentation, and super resolution. The Weight column indicates the availability of pre-trained weights. Calibration is performed on datasets such as ImageNet-1k, PASCAL VOC, and internal gaming images, with sampling iterations, scale factor modes, and algorithms for accuracy tuning. The calibration process involves selecting a scale factor mode and algorithm for accuracy tuning. The calibration cost is affordable and accuracy is measured independently on validation datasets. The best accuracy of CNN models under INT8 inference is shown in TAB2. Standard metrics like Top1, Top5, mAP, mean accuracy, IoU, SSIM, and PSNR are used to measure accuracy across different use cases. Experimental results show minimal accuracy loss compared to FP32 baseline, within 0.6% for Top1 and 0.3% for Top5 on image classification, 0.5% for mAP on object detection, 0.2% for mean IoU on image segmentation, and 0.1% for PSNR on super resolution. INT8 inference recipe is effective across various use cases. The evaluation of errors in image classification using FP32 and INT8 inference shows no bias based on image class. However, INT8 models struggle with distinguishing objects with small differences, leading to potential misclassifications. The entropy of Softmax output is higher for INT8 models, indicating decreased classification capability. The evaluation of errors in image classification using INT8 models shows decreased classification capability compared to FP32. Performance metrics indicate improved throughput and latency, with INT8 inference showing a 1.6X and 1.5X improvement on average, and up to 2.0X and 2.1X maximum improvement. Additionally, experiments with INT8 convolution show reasonable accuracy within 1% loss compared to FP32. The experimental results demonstrate the impact of calibration process on accuracy with different sampling iteration, calibration algorithm, and scale factor mode. Channel-wise scaling factors deliver better accuracy than single scale factor, especially for depthwise convolution. Direct algorithm is more effective than KL in most cases, while KL can outperform FP32 in some cases. More sampling iterations lead to stable dynamic data range and better accuracy. Selecting the optimal calibration strategy is a future research direction. The paper proposes a general recipe for INT8 inference and develops an automatic calibration tool for optimized model generation. Our automatic calibration tool improves inference throughput and latency by 1.6X and 1.5X respectively for 18 CNN models. Accuracy loss is minimal, ranging from 0.6% to no loss from FP32 baseline. The methodology is applicable to various CNN models and can serve as a guide for other frameworks."
}