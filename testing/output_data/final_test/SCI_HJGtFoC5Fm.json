{
    "title": "HJGtFoC5Fm",
    "content": "Past works have shown that over-parametrization can improve generalization in neural networks. A margin-based perspective is adopted to explain this phenomenon, showing that increasing over-parametrization improves normalized margin and generalization error bounds for deep networks. Infinite-width neural networks in two-layer networks offer the best generalization guarantees compared to kernel methods. The gap between neural net margin and kernel methods' generalization guarantees is validated empirically. The infinite-neuron viewpoint is also useful for analyzing optimization, demonstrating that a perturbed approach is beneficial. In deep learning, over-parametrization is crucial for optimizing neural networks. Increasing model size without regularization improves generalization, even after perfect data interpolation. Algorithmic regularization plays a key role in this surprising phenomenon. Algorithmic regularization, without an explicit regularizer, is a significant factor in achieving good generalization in deep learning. Recent studies have demonstrated that gradient descent tends to find the minimum norm solution for various problems. However, proving similar results for two-layer relu networks remains an open question. One proposed approach is to make regularization explicit to better understand generalization. The motivation for making regularization explicit is to analyze generalization without fully understanding optimization and to determine if gradient descent provides additional implicit regularization. By adding a norm-based regularizer to a multi-layer neural network with relu activations, the global minimizer achieves the maximum normalized margin if the regularizer is weak enough. This approach is relevant as 2 regularization is commonly used in practice. The margin in models with norm 1 that perfectly classify data is the smallest difference between the classifier score for the true label and the next best score. Optimizing training loss can lead to parameters with a large margin and better generalization error. Increasing the width of the architecture can improve the generalization bound, even if the dataset is already separable. The regularizer serves as a tiebreaker to guide the model towards the largest normalized margin. Proofs are simple and apply to any norm-based regularizer. Approximating the minimum loss within a constant factor yields the max-margin within a constant factor. Comparing neural network max-margin to kernel methods reveals differences in margin size. Neural networks demonstrate better generalization compared to kernel methods, with a small kernel margin translating to a significant gap in generalization error bounds. In the context of two-layer networks, over-parametrization aids optimization, with gradient descent resembling Wasserstein gradient flow in the limit of infinite neurons. Perturbed Wasserstein gradient flow efficiently finds global optimizers in polynomial time, as empirically validated. Test error decreases and margin increases with the growth of the hidden layer in two-layer networks. The test error decreases and margin increases as the hidden layer grows in neural networks, as predicted by theory. Various studies have explored neural network generalization, inductive bias, training time, sharpness of local minima, and implicit regularization towards specific solutions. New algorithms and insights have been proposed to improve generalization and optimize gradient descent. Implicit regularization can help gradient descent avoid overshooting optima. Logistic regression with weak regularization converges to the max margin solution. Research has focused on deriving tighter norm-based Rademacher complexity bounds for deep neural networks and new compression-based generalization properties. Studies investigate the generalization of kernel methods and explain how over-parametrization can remove bad local minima in optimization. Safran & Shamir (2016) and other studies demonstrate that over-parametrization can enhance random initialization quality and ensure all local minima are global in sufficiently overparametrized networks. BID11 find that for two-layer networks with quadratic activations, all second-order stationary points are global minimizers. Over-parametrization is viewed as implicit acceleration during optimization. Mei et al. (2018) and other researchers take a distributional perspective on over-parametrized networks, showing that Wasserstein gradient flow converges to global optimizers under certain assumptions. In this paper, symbols like \u00b7, \u03b1, X, Y, d, and \u0398 are used to represent mathematical concepts related to functions, data points, labels, dimensions, and parameters in prediction functions. The notation , is used to indicate values being less than or greater than up to a universal constant. The symbols O(\u00b7) and \u2126(\u00b7) are used to denote certain properties unless stated otherwise. In this section, a weak regularizer is added to cross-entropy loss with a positive-homogeneous prediction function, leading to the convergence of the normalized margin of the optimum to a max-margin solution. Feedforward relu networks are given as an example of positive-homogeneous functions. The family F of prediction functions considered are a-positive-homogeneous in their parameters for some a > 0, and continuous in \u0398. The \u03bb-regularized cross-entropy loss L \u03bb is studied for a general norm. The normalized margin of the optimum converges to a max-margin solution with a weak regularizer added to cross-entropy loss for positive-homogeneous prediction functions. The result applies to non-linear functions as long as they are homogeneous. Theorem 2.1 states that the normalized margin of the global optimum converges to the maximum margin as the regularizer strength approaches zero. Theorem 2.1 applies to feedforward relu networks, showing that global minimizers of weakly regularized loss achieve maximum margin. This framework allows for analyzing generalization properties without focusing on optimization details. Over-parameterization can improve generalization, leveraging existing bounds. Theorem 2.1 shows that global minimizers of weakly regularized loss in feedforward relu networks achieve maximum margin, allowing for analysis of generalization properties without optimization details. Over-parameterization can enhance generalization, as demonstrated in the study of max-margin properties in binary classification settings. Theorem 2.2 provides a method to approximate the margin accurately by optimizing a constant factor multiplicative approximation of L \u03bb. In Section 2, it was shown that optimizing weakly-regularized logistic loss leads to maximum normalized margin. The generalization properties of the solution are studied by combining Rademacher complexity bounds with Theorem 2.1, resulting in a generalization bound that scales with the inverse of the maximum possible margin and depth of the network. The size of the network can increase the maximum possible margin, enhancing generalization. Increasing the size of the network improves generalization by increasing the maximum possible margin, as shown in Theorem 3.3. Consider depth-K neural networks with 1-Lipschitz, 1-positive-homogeneous activation \u03c6 for K \u2265 2. The network computes a real-valued score using matrices W1,...,WK and hidden layer sizes. The weakly-regularized logistic loss of the depth-K architecture with hidden layer sizes M is denoted as L\u03bb,M. The optimizer of L\u03bb,M is denoted by \u0398\u03bb,M. The optimizer of the weakly-regularized logistic loss for depth-K neural networks with hidden layer sizes M is denoted by \u0398 \u03bb,M. The margin of \u0398 \u03bb,M is denoted by \u03b3 \u03bb,M. When the data is not separable by the network, \u03b3 ,M is defined as zero. The generalization error depends on the parameters through the inverse of the margin on the training data. The generalization error depends on the parameters through the inverse of the margin on the training data. Proposition 3.1 is obtained by applying Theorem 1 of BID14 with margin loss. Different generalization bounds exist, but we choose the bounds of BID14 due to their compatibility with 2 normalization. The bound in the two-layer case also follows from Neyshabur et al. (2015b). The proof is stated in Section C.1, and optimizing weakly-regularized logistic loss gives generalization error bounds based on the maximum possible margin of a network with the given architecture. The generalization error is influenced by the parameters through the margin on the training data. Proposition 3.1, derived from BID14's Theorem 1 with margin loss, provides bounds compatible with normalization. The margin decreases with depth, impacting the constraint and generalization. Optimizing weakly-regularized logistic loss yields the best generalization bound among all options. The weakly-regularized logistic loss leads to the best generalization bound among models with the given architecture. The maximum normalized margin increases with the size of the network, and additional overparameterization can improve generalization. Empirical evidence shows that test error decreases with larger network size while the margin remains non-decreasing. Theorem 3.3 demonstrates that increasing the number of neurons in a max-margin neural network improves generalization, contrasting with standard linear prediction. Comparison with kernel methods shows two-layer networks achieve larger margins and better generalization guarantees. The curr_chunk discusses the benefits of optimizing all layers of a network for generalization, comparing 1-SVM and 2-SVM solutions in a fixed feature space of relu activations. Theorem 4.3 shows that generalization upper bounds for 1-SVM are smaller than those for 2-SVM by a \u2126( \u221a d) factor. This work provides evidence supporting the optimization of all layers in a network for improved generalization. The curr_chunk discusses the challenges of optimizing activations in a network, specifically focusing on the width of the hidden layer denoted by m. The work links the maximum normalized margin \u03b3,m of a two-layer network to the 1-SVM over a lifted space using a lifting function \u03d5. The margin of linear functionals corresponding to \u03b1 \u2208 L 1 (S d\u22121 ) is examined in the context of the 1-norm SVM. The text discusses optimizing activations in a network by examining the margin of linear functionals corresponding to \u03b1 \u2208 L 1 (S d\u22121 ). It relates the 1-norm SVM over a lifted feature to a hard-margin optimization on \"convex neural networks\". The equivalence of optimizing weakly-regularized logistic loss over two-layer networks and solving a specific equation is highlighted. Proposition 4.1 further confirms this equivalence, with additional details provided in Section B. The work compares the 1-max margin on the lifted feature space achieved by optimizing a finite neural network to the 2 margin attainable via kernel methods. The kernel problem over \u03b1 \u2208 L 2 (S d\u22121 ) is defined, with \u03b3 2 used to obtain an upper bound on the generalization error of the kernel SVM. The generalization error of the standard kernel SVM with relu feature is bounded by a specific equation, with the bound following from standard techniques. Theorem 4.3 compares upper bounds for generalization error between 1 SVM with relu features and 2 SVM. A data distribution is constructed where the 1 SVM has a good margin, while the 2 SVM has a small margin. The 2 bound is larger than the 1 bound by a factor of \u2126( DISPLAYFORM0. Our construction in Theorem 4.3 highlights properties of distributions leading to better neural network generalization than kernel methods. Empirical validation in Section 6 confirms this gap. The distribution D of examples is based on standard basis vectors, with samples showing no linear separator. A relu network with four neurons can fit this complex decision boundary, while kernel methods struggle due to symmetries. In Section D, it is shown that symmetries in the data lead to cancellation in feature space for kernel methods, resulting in less predictive features and small margins. A significant gap between neural networks and kernel methods is proven in regression settings, with neural networks equivalent to solving a minimum 1-norm regression problem and kernel methods to a minimum 2-norm problem. Different distributions are constructed to show the generalization error bounds, highlighting the superiority of the 1-norm solution. In Section D.2, the 1-norm and 2-norm regression problems are defined. The construction is formalized in Theorem D.10. Previous work has shown that as the hidden layer size grows to infinity, gradient descent approaches the Wasserstein gradient flow over distributions of hidden units. The convergence of the gradient flow is assumed, and it is proven that the Wasserstein gradient flow converges to a global optimizer without specifying a convergence rate. Global convergence is also shown for the infinite-neuron limit of stochastic Langevin dynamics. In Section D.2, the 1-norm and 2-norm regression problems are defined. Previous work has shown that as the hidden layer size grows to infinity, gradient descent approaches the Wasserstein gradient flow over distributions of hidden units. A perturbed version of Wasserstein gradient flow converges in polynomial time on infinite-size neural networks. The optimization involves specific parameters and regularity conditions on the functions involved. In Section E.1, specific parameters for boundedness and Lipschitzness are detailed. Relu networks satisfy all conditions except differentiability of \u03a6. A neural network can be fitted under this framework, with \u03c1 interpreted as a distribution over network parameters. The distributional neural network computes outputs for training examples and the regularized logistic loss can be computed accordingly. The evolution of \u03c1 t follows standard Wasserstein gradient flow dynamics. The dynamics of neural networks are defined by continuous-time gradient descent with modifications to the Wasserstein gradient flow dynamics. Small uniform noise is added to ensure descent direction at each step, resembling re-initialization of neurons. Convergence to a global optimizer is proven in polynomial time. The perturbed Wasserstein gradient flow converges to an approximate global minimum in polynomial time for neural networks with regularity parameters satisfied. The theorem statement and proof are provided in Sections E.1 and E.2, respectively. The dynamics existence assumption is removed by analyzing a discrete-time version of the equation and assuming Lipschitz gradients for \u03a6 and V. This results in a polynomial time convergence for infinite networks, as stated in Section E.3. Theorem 5.3 implies that for infinite networks, the weakly regularized logistic loss can be optimized in polynomial time. With \u03bb \u22121 = poly(n), the maximum margin can be approximated within a constant factor. A comparison between neural networks and kernel methods for classification and regression is made, showing generalization error and predicted upper bounds. The neural network's generalization bound for classification varied due to data from a synthetic distribution. The neural network generalization bound for classification was affected by the lack of tuning in learning rate and training time, leading to suboptimal margin. Two-layer networks outperform kernel methods in test error as n increases, but there are discrepancies in the bounds. Test error decreases while the kernel generalization bound remains constant with n. Test error decreases with hidden layer size, while margin increases as predicted by Theorem 3.3. Experimental setup details are provided in Section F.2, and convergence to the max-margin solution is verified as regularization decreases in Section F.3. In Section F.4, modified WideResNet architectures are trained on CIFAR10 and CIFAR100, showing improvements in generalization by annealing weight decay during training. Maximizing margin is highlighted as an inductive bias of relu networks optimized with weakly-regularized cross-entropy loss. The framework allows direct analysis of network generalization properties, offering a simple explanation for how over-parametrization can enhance generalization. Future work could explore other generalization properties of the max-margin solution. The text discusses analyzing infinite-size neural networks to understand over-parametrized gradient descent. Future work aims to optimize the margin of finite-sized neural networks by applying the theory developed. The proof involves showing that as the regularization parameter decreases, the norm of the solution grows towards infinity. The proof technique involves bounding the scaling of the cross entropy with a lower bound that vanishes as the gap between parameters decreases. This is inspired by previous work. The analysis shows that as the regularization parameter approaches zero, the norm of the solution tends towards infinity. The proof of Lemma A.2 involves showing that as the regularization parameter approaches zero, the norm of the solution tends towards infinity. This is done by bounding the scaling of the cross entropy with a lower bound that vanishes as the gap between parameters decreases. The proof shows that the equation is violated with sufficiently large \u0398 \u03bb, leading to a contradiction. The proof of Theorem A.3 involves showing the maximum normalized margin as \u03bb approaches 0 in binary classification with logistic loss. This is achieved by reducing the problem to the multi-class case with l = 2 and constructing new labels. The proof of Theorem 2.2 involves demonstrating the convergence of \u03b3 \u03bb to \u03b3 in binary classification by choosing appropriate parameters and applying mathematical inequalities. The proof involves working with a minimum norm formulation equivalent to equation 4.2, showing the optimal objective for equation B.1, and solving the dual form using computation. It is then shown that a general result about linearly dependent sets can be applied, leading to the proof of Tibshirani (2013). The proof involves applying a general result about linearly dependent sets to show the desired statement. It follows Lemma 14 in Tibshirani (2013) and concludes with Proposition 4.1, which parametrizes a two-layer network with m units using top and bottom layer weights. The network computes a real-valued function using parameters \u0398. Equation 4.2 has an n-sparse optimal solution \u03b1. We construct \u0398 for a two-layer network with m \u2265 n hidden units and normalized margin at least \u03b3 1/2. The weights are denoted as W and U. The network has normalized margin at least \u03b3 1/2. The text discusses generalization error bounds using Rademacher complexity and margin theory for a hypothesis class of real-valued functions. It focuses on proving Proposition 3.1 and Lemma 4.2, assuming data X, Y are drawn i.i.d. from a distribution supported on X \u00d7 Y. The text also mentions applying Rademacher complexity bounds and a lemma to prove Proposition 3.1. The text discusses generalization error bounds using Rademacher complexity and margin theory for a hypothesis class of real-valued functions. It focuses on proving Proposition 3.1 and Lemma 4.2, assuming data X, Y are drawn i.i.d. from a distribution supported on X \u00d7 Y. The Rademacher complexity bounds of BID14 are used with Theorem C.1 to show a lemma bounding the generalization of neural networks with bounded Frobenius norms. The hypothesis class F K over depth-K neural networks is defined, and a bound on the 0-1 population loss is derived for classifying training data correctly with unnormalized margin \u03b3. The text discusses generalization error bounds using Rademacher complexity and margin theory for a hypothesis class of real-valued functions. Proposition 3.1 and Lemma 4.2 are proven, assuming data X, Y are drawn i.i.d. from a distribution supported on X \u00d7 Y. The Rademacher complexity bounds of BID14 are used with Theorem C.1 to show a lemma bounding the generalization of neural networks with bounded Frobenius norms. The hypothesis class F K over depth-K neural networks is defined, and a bound on the 0-1 population loss is derived for classifying training data correctly with unnormalized margin \u03b3. Applying Theorem A.3, Corollary 3.2 is concluded by using the above on \u0398 \u03bb,M and defining F 2,\u03c6 B as the class of 2-bounded linear functionals in lifted feature space. The text discusses linear functionals in the lifted feature space, focusing on analyzing the Rademacher complexity. The proof includes derivations for completeness, applying bounds to relu features. Lemma 4.2 is proven, showing the optimization of the 0-1 population loss for classifying training data correctly with a bounded Frobenius norm. In this section, the proof of Theorem 4.3 is completed by constructing a distribution D and providing a classifier with small 1 norm. The margin attainable by the SVM is upper bounded using Lemma D.2, with derivations and bounds applied to the features. The proof also includes bounding quantities regarding W i's and conditioning on a high probability event E. The proof of Theorem 4.3 is completed by constructing a distribution D and providing a classifier with small 1 norm. The margin attainable by the SVM is upper bounded using Lemma D.2, with derivations and bounds applied to the features. The proof also includes bounding quantities regarding W i's and conditioning on a high probability event E. Event E occurs with high probability and the proof assumes certain inequalities to show the conclusion of the Lemma. By utilizing the triangle inequality and rewriting equations, the proof is completed by proving bullets 1, 2, and 3. The proof of Theorem 4.3 involves constructing a distribution D and providing a classifier with small 1 norm. By utilizing Lemma D.2, the margin attainable by the SVM is upper bounded. The proof includes bounding quantities regarding W i's and conditioning on a high probability event E. Event E occurs with high probability and certain inequalities are assumed to show the conclusion of the Lemma. By rewriting equations and utilizing the triangle inequality, bullets 1, 2, and 3 are proven. The proof involves constructing a distribution D for rejection sampling, ensuring a sample with x^2 \u2265 2d log n. Lemma D.2 bounds the SVM margin, while Lemma D.1 leads to \u03b3 \u2264 1. Proposition 3.1 shows that with probability 1-\u03b4, L 1-svm \u2264 d log n/n + log log(d log n)/n + log(1/\u03b4)n. The regression problems are defined, connecting the 1-norm solution to neural networks. Optimizing weakly regularized loss over neural networks is equivalent to solving the 1 SVM in classification. In regression, solving weakly regularized squared error loss is equivalent to finding the minimum 1-norm solution. Theorem D.5 discusses two-layer neural networks and \u03bb-regularized squared error loss. The proof shows that an n-sparse solution \u03b1 can be found. The function x \u2192 \u03b1 , \u03d5(x) can be implemented by a neural network. The paper provides generalization bounds for classification. The paper at ICLR 2019 presents generalization bounds for neural networks. The bounds depend on the norms of the solution rather than the margin. A Rademacher complexity bound is derived for hypothesis class F. For Lipschitz activations \u03c6 with \u03c6(0) = 0, a bound on Rademacher complexity is provided. The proof involves decompositions and bounding terms based on Lipschitz properties. The Rademacher complexity bound for hypothesis class F with Lipschitz activations \u03c6 is derived, utilizing decompositions and bounding terms based on Lipschitz properties. The proof involves bounding terms by the difference between the supremum and infimum of a quantity, leading to an empirical Rademacher complexity scaled by n. The Rademacher complexity bound for hypothesis class F with Lipschitz activations \u03c6 is derived using a union bound. The generalization error bound is provided for the 2-norm and relu features, with probability 1 \u2212 \u03b4. The proof involves defining B j and assigning \u03b1 to different j to obtain the desired statement. Theorem D.10 states that for any activation function \u03c6 with a full-rank kernel matrix K, there exists a distribution p data such that certain conditions hold with high probability. If these conditions are met, the upper bound on L2-regularization does not decrease with the number of data points. Theorem D.10 shows that for any activation function \u03c6 with a full-rank kernel matrix K, there exists a distribution p data satisfying certain conditions with high probability. The bound on L2-regularization from Lemma D.9 is \u2126(1) and does not decrease with n. Lemma D.11 establishes the order of quantifiers, with distribution A independent of dataset X. Claim D.12 provides a closed-form expression for \u03b122. Theorem D.10 states that for any activation function \u03c6 with a full-rank kernel matrix K, there exists a distribution p data satisfying certain conditions with high probability. The bound on L2-regularization from Lemma D.9 is \u2126(1) and does not decrease with n. Lemma D.11 establishes the order of quantifiers, with distribution A independent of dataset X. Claim D.12 provides a closed-form expression for \u03b122. Regularity conditions on \u03a6, R, and V are detailed in Assumption E.1. Theorem E.4 presents conditions for 2-homogeneous functions \u03a6 and V, with Lipschitz and bounded properties. Starting from distribution \u03c10, dynamics in equation 5.2 must have a solution. Lemma E.5 ensures boundedness of \u03c1t during the algorithm. Lemma E.5 ensures the boundedness of distribution \u03c1t during the algorithm. Lemma E.9 proves that \u03c1t will maintain this property. The absolute value of L[\u03c1t] is bounded over the sphere. The decrease in objective value is related to the average velocity of parameters under \u03c1t with additional noise. The dynamics are run for a short time to analyze the second moment of \u03c1t. After analyzing the dynamics for a short time, the second moment of \u03c1 t grows slowly at a rate similar to the noise \u03c3. Integrating equations and rearranging, we find that our dynamics will not significantly increase the objective. Lemmas bound the change in expectation of a 2-homogeneous function over \u03c1 t, showing a decrease in loss related to this change. Lemma E.5 states that h is Lipschitz on the sphere, and by Corollary E.8, we can plug this into equation E.3 to obtain a result. This result is then used to bound the change in L [\u03c1 t ] over time in terms of the objective value change. Lemma E.12 defines Q(t) and integrating it gives us equation E.4. Applying Lemma E.11 leads to another equation, which is then integrated to get equation E.6. Finally, using certain problem constants, the statement in the lemma is proven, along with showing that L is Lipschitz on the unit ball. The proof shows that if \u03c1 t is far from optimality, the expected velocity of \u03b8 under \u03c1 t will be large, leading to a decrease in loss. Additionally, if a descent direction exists, added noise will help find it and decrease the function value. Theorem E.21 confirms polynomial time convergence for discrete-time updates under certain assumptions. Theorem E.21 confirms polynomial time convergence for discrete-time updates under certain assumptions. The experimental results in FIG5 show the performance of synthetic data generated from a ground truth network with specific parameters. Training was done for 80000 steps with specific learning rate and lambda values, using two-layer networks with varying hidden units. The experiments in FIG5 demonstrate the performance on synthetic data and MNIST with hidden layer sizes ranging from 6 to 15. Training was conducted for 600 epochs with a learning rate of 0.01 and \u03bb = 10 \u22126. All trials fit the training data perfectly, showing decreasing test error and increasing margin. The normalized margin convergence was verified on two-layer networks with one-dimensional input. The text discusses solving an approximate version of the SVM problem using features relu(wx i + b) for w 2 + b 2 = 1. The margin of the neural network converges to the 1 SVM objective with decreasing regularization. Test error on CIFAR10 and CIFAR100 is shown for initial \u03bb = 0.0005 using a modified WideResNet architecture. Reducing weight decay can still improve performance. Reducing weight decay can improve generalization error in non-homogeneous architectures. Batchnorm layers are removed in a 16 layer deep WideResNet for comparison. Different weight decay values are used for CIFAR10 and CIFAR100 datasets, with adjustments made during training epochs. Initial weight decay parameters are chosen based on model performance without annealing. Models with small weight decays at initialization failed to generalize well. Annealing weight decay can improve model generalization by directing the optimization algorithm towards the global minima. Test error decreases with weight decay annealing for small \u03bb values."
}