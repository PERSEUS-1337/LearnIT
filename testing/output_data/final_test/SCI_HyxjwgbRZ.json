{
    "title": "HyxjwgbRZ",
    "content": "The sign stochastic gradient descent method (signSGD) utilizes only the sign of the stochastic gradient in its updates, making it practical for distributed optimization. Convergence rates for signSGD on general non-convex functions are established for the first time, showing comparable performance to SGD in reaching first-order critical points. Simple experiments demonstrate that sign gradient descent can help avoid saddle points without stochasticity or curvature information in high-dimensional, non-convex, and stochastic error landscapes of deep neural network training. In high-dimensional, non-convex, and stochastic error landscapes, optimization techniques like Adam, RMSprop, and Rprop are popular. These algorithms involve rescaling gradients and have similarities to signSGD. Recent optimization algorithms focus on reducing communication costs in distributed training environments while maintaining good performance. In a distributed training environment, techniques like signSGD involve quantizing stochastic gradients at low precision. Empirical evidence shows using one-bit per dimension is effective. Our contribution is providing the convergence rate of signSGD to first-order critical points in non-convex problems, showing comparable rates to SGD. The study introduces a biased quantisation procedure with non-convex convergence rates comparable to SGD. It eliminates the need for randomisation in gradient quantisation algorithms and addresses the challenge of incorporating stochasticity in the sign non-linearity of the algorithm. Experimental testing without stochasticity suggests signGD may have an advantage over GD in escaping saddle points efficiently. The study introduces a biased quantisation procedure with non-convex convergence rates comparable to SGD. It eliminates the need for randomisation in gradient quantisation algorithms and addresses the challenge of incorporating stochasticity in the sign non-linearity of the algorithm. Experimental testing without stochasticity suggests signGD may have an advantage over GD in escaping saddle points efficiently. The paper outlines a non-convex theory of signSGD in Sections 3, 4, and 5, and experimentally tests its ability to escape saddle points in Section 6. Additionally, Section 7 compares signSGD against SGD and Adam on CIFAR-10. In Rprop BID16, weight updates focus on the sign of the gradient rather than its magnitude, similar to signSGD. RMSprop BID19 adapts Rprop for minibatches by estimating rescaling factors as averages over recent iterates. Adam BID10 combines RMSprop with momentum, estimating both gradient and rescaling factors as bias-corrected averages. These algorithms have practical applications and address the issue of saddle points in neural network training. Recent work in optimization theory focuses on the efficient use of noise and curvature information to evade saddle points in neural network training. In non-convex problems, finding the global minimum is challenging, leading theorists to settle for measuring success rates based on convergence to stationary points or local minima. Recent work in optimization theory explores the use of noise and curvature information to escape saddle points and find local minima. Distributed machine learning research focuses on reducing gradient communication costs by quantizing gradients, with some schemes showing good performance while maintaining unbiasedness. Convergence properties of quantized stochastic gradient methods are still not well understood, but some schemes have been proven to converge asymptotically. The proposed ternary quantization scheme retains unbiasedness of stochastic gradient by directly employing the sign gradient, avoiding randomization. This work establishes a convergence rate for biased quantization, related to signSGD and other gradient descent methods. Non-convex theory of signSGD is explored, contrasting with BID3's analysis of Adam in the convex world. The objective function is assumed to be L-Lipschitz smooth, with a stochastic gradient oracle providing independent estimates for each dimension separately. This assumption allows for measuring the error in trusting the local linearization of the objective, useful for bounding the error in a single step of the algorithm. The variance upper bound is stated for each dimension separately in optimization. A realization of the oracle involves choosing a data point uniformly at random and evaluating its gradient at a given point. The algorithm works with a minibatch of size n k in the k th iteration, with the corresponding minibatch stochastic gradient being the average of n k calls of the stochastic gradient oracle at x k. The variance bound is squashed to 2 /n k in this case. The primary result suggests that running signSGD with the prescribed learning rate and mini-batch schedules should lead to a point along the optimization trajectory with a gradient 1-norm smaller than O (N 0.25) after N stochastic gradient evaluations. This matches the non-convex SGD rate, disregarding constants. Theorems on non-convex convergence rates of signSGD and stochastic gradient descent are discussed. Key constants and notation are defined, including gradient, lower bound on objective function, dimension, total iterations, cumulative stochastic gradient calls, intrinsic variance, and maximum curvature. Algorithm 2 for stochastic gradient descent is presented. Theorems 1 and 2 outline convergence rates under specific assumptions and learning rate schedules. Theorem 2 discusses the non-convex convergence rate of stochastic gradient descent using Algorithm 2 under specific assumptions. It highlights the importance of bounding the per-step improvement in terms of the norm of the gradient to ensure progress in optimization. The proofs are deferred to Appendix B. The non-stochastic case in optimization faces obstacles due to curvature, which can make the gradient unreliable if steps are too large. To control this, the learning rate must be annealed. Stochasticity in optimization, like in signSGD, can lead to noise flipping the gradient sign, requiring a larger batch size to mitigate this effect and ensure good convergence rates. Growing the batch size in signSGD can lead to a convergence rate similar to SGD, as the positive effect of taking large steps cancels out the need for a larger batch size. SGD does not require increasing batch size due to noise being second order in the learning rate, resulting in a slower progress when the gradient is small. After making a clean comparison between signSGD and SGD, it is noted that signSGD deals more naturally with the one norm of the gradient vector. However, this results in a squared bound compared to SGD, making the constant factor in signSGD roughly worse by a square. This defect in dimensionality is expected since signSGD rarely takes the direction of steepest descent. SignSGD diverges from the steepest descent direction as dimensionality increases. Despite its limitations, it offers a stronger bound on the 1-norm of the gradient compared to SGD. The analysis focuses on convergence to stationary points rather than behavior around saddle points. The study investigates how signSGD behaves around saddle points, focusing on gradient rescaling to avoid them. Optimizers tested include gradient descent, perturbed gradient descent, sign gradient descent, and rescaled gradient method. No learning rate tuning was done, so only qualitative behavior is considered. SignGD does not seem to detect saddle points in the original objective function, even after axis alignment is broken by rotation. Rotation operation helps GD escape saddle points. The study explores how signSGD behaves around saddle points, focusing on gradient rescaling to avoid them. The rotation operation helps gradient descent escape saddle points, highlighting the brittleness of Du et al. FORMULA0's construction. The authors use a specially designed 'tube' function with saddle points to demonstrate how stochasticity can aid in escaping them. Gradient descent takes longer to navigate the tube compared to perturbed gradient descent. The study aims to investigate if the sign non-linearity in signSGD can efficiently help escape saddle points. The study compares signGD against the tube function to see if signGD can help escape saddle points. SignGD can take large steps even with small gradients and move in orthogonal directions, aiding in exploration. Experiments show that while signGD's abilities sometimes hold true, there are cases where they do not. The comparison includes sign gradient method, gradient descent, perturbed gradient descent, and rescaled gradient descent. No learning rate tuning was done, so focus on qualitative behavior rather than convergence speed. SignGD shows different qualitative behavior compared to other algorithms when pitted against the tube function. It can get stuck at saddle points in certain random rotations of the objective, forming perfect periodic orbits around them due to the update mechanism. The text discusses the behavior of different optimization algorithms when faced with the tube function. It mentions the possibility of updates summing to zero, the introduction of momentum to break symmetry, and a comparison of SGD, signSGD, and Adam on training Resnet-20 on the CIFAR-10 dataset. The comparison between signSGD and Adam shows similar performance, with SGD outperforming them in stable hyperparameter configurations. However, Adam and signSGD are more robust with larger learning rates. Theoretical results suggest signSGD should be worse than SGD by a factor of dimension d, but empirical evidence contradicts this. The authors note that setting Adam hyperparameters to specific values makes it equivalent to signSGD. In deep neural network applications, the dimension d can exceed 10^6. The structure in error surfaces of neural networks, not captured by theoretical assumptions, may improve the dimension scaling of signSGD. Weak coupling across dimensions and the presence of saddle points play a role in this improvement. Recent work focuses on using noise or curvature information to escape saddle points in non-convex functions. Sign gradient descent without stochasticity (signGD) was found to make progress unhindered by saddles when the objective function was axis aligned, suggesting it has a greater ability to explore and take larger steps in regions of small gradient compared to SGD. Sign gradient descent without stochasticity (signGD) can take larger steps in regions of small gradient and explore almost orthogonal to the true gradient direction. This exploration ability may help it break out of subspaces convergent on saddle points without sacrificing convergence rate, potentially leading to more robust practical performance compared to algorithms like Rprop and Adam. In non-axis-aligned objectives, signGD may get stuck in perfect periodic orbits around saddle points, but this behavior is less likely for higher dimensional objectives. The implications of these results for gradient quantization schemes and distributed optimization are also discussed. Our study investigates the theoretical properties of the sign stochastic gradient method (signSGD) for non-convex optimization. Results show promise for biased gradient quantization schemes, suggesting potential for new practical algorithms. The heatmap comparison between Adam and signSGD indicates algorithmic similarity, with signSGD and Adam showing stability for large learning rates. The study explores signSGD as an algorithm for non-convex optimization, showing convergence rates to first order critical points. SignSGD compares to SGD in terms of gradient evaluations but is worse in dimension. It outperforms existing quantization schemes by not needing randomization to remove bias. Future work includes analyzing convergence to higher order critical points and exploring success and failure modes around saddle points. The study presents preliminary experiments on the algorithm's success and failure modes around saddle points. Future work aims to investigate the efficiency of signSGD in escaping saddle points and its connection to Adam-like algorithms and gradient quantisation schemes. Experimental setup for CIFAR-10 experiments using Resnet-20 architecture BID8 is described. Tuning was done for optimisers in SGD, signSGD, and Adam using {weight decay, momentum, initial learning rate}. The study describes the experimental setup for CIFAR-10 experiments using Resnet-20 architecture. Optimisers in SGD, signSGD, and Adam were tuned with weight decay, momentum, and initial learning rate. Adam was implemented with specific parameters, while SGD and signSGD used a momentum sequence. Weight decay was applied traditionally. Non-mentioned details follow the setup in BID8. Code will be released upon paper acceptance. Theorem 1 discusses the convergence rate of signSGD under certain assumptions. The study discusses the experimental setup for CIFAR-10 experiments using Resnet-20 architecture and the tuning of optimizers. Theorem 1 then delves into the convergence rate of signSGD under specific assumptions, focusing on the expected objective improvement and stochasticity-induced error in the algorithm. The text discusses the convergence rate of stochastic gradient descent, emphasizing the importance of the noise scale relative to the gradient magnitude. Theorem 2 outlines the non-convex convergence rate under specific assumptions and scheduling of learning rate and mini-batch size. The text discusses the convergence rate of stochastic gradient descent, emphasizing the importance of noise scale relative to gradient magnitude. The proof involves decomposing the mean squared stochastic gradient into mean and variance, with a variance bound of d^2 for the full vector. By plugging in the learning rate schedule, the expectation over x_k is taken, yielding the final result."
}