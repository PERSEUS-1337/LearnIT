{
    "title": "SyjsLqxR-",
    "content": "Classifiers like deep neural networks are vulnerable to adversarial perturbations in high-dimensional input spaces. Adversarial training improves robustness against shared perturbations but not consistently against singular perturbations. Adversarial training reduces robustness against image transformations, making successful attacks less likely in the physical world. Singular perturbations can be easily detected and must exhibit generalizable patterns, even though they are specific for certain inputs. Deep learning is robust to random noise but vulnerable to adversarial perturbations that are misclassified by classifiers and imperceptible to humans. Various approaches have been proposed to explain the existence of adversarial examples. Adversarial examples exist due to perturbations that transfer between network architectures and are robust against image transformations. Universal perturbations mislead classifiers on most inputs. Methods to increase network robustness include adversarial training, virtual adversarial training, ensemble adversarial training, defensive distillation, stability training, robust optimization, and Parseval networks. An alternative approach to defending against adversarial examples is to detect and reject them as malicious. Adversarial training is a popular method for increasing network robustness, but its effect on classifiers and adversarial perturbations is not well understood. Recent research has shown that adversarial training displaces the classifier's decision boundary slightly, but not enough to prevent black-box attacks. In this work, the focus is on properties of adversarial perturbations under adversarial training in a white-box scenario. The study explores sharedness of perturbations, effectiveness of adversarial training in removing shared perturbations, and its impact on universal perturbations, robustness against image transformations, and detectability of adversarial perturbations. Adversarial training is found to be effective in removing universal perturbations. Adversarial training effectively removes universal perturbations and makes remaining adversarial perturbations less robust against transformations. It is promising for preventing certain attacks in the physical world and leaves singular perturbations easily detectable, suggesting detectable patterns that could improve adversarial training. The objective of an adversary is to find a perturbation that changes the model's output in a desired way. An untargeted attack aims to make the true or predicted class less likely, while a targeted attack tries to make a designated target class more likely. Adversarial perturbations are kept quasi-imperceptible by bounding their norm. Different methods have been proposed for generating adversarial examples, with some being computationally expensive while others are faster, like the fast gradient-sign method (FGSM). This method defines an adversarial perturbation as the direction in image space that yields the highest increase of a linearization of the cost function J under the norm. Adversarial training is a modification of the training procedure that increases classifier robustness against perturbations. It replaces the objective function with a modified version during training to act as a regularizer. Adversarial training is a method to increase classifier robustness by using adversarial perturbations. Label leaking can occur when using adversarial training, but it can be avoided by using the predicted class instead of the true class during training. Experiments were conducted on the CIFAR10 dataset using a pre-activation Residual Network with 18 layers. The basic iterative adversary with specific parameters was used for the experiments. Adversarial training was conducted on a CIFAR10 dataset using a pre-activation Residual Network with 18 layers. The classifier reached 94.1% accuracy on test inputs but only 0.5% on adversarial examples without adversarial training. After 250 epochs of adversarial training, the accuracy on unchanged test inputs was roughly 90% and 62% on adversarial inputs. Overfitting to perturbations of the training data was identified as the main issue hindering higher performance on adversarial test examples. Black-box attacks were not found to be more successful than white-box attacks. During adversarial training, the model may fail to increase robustness against certain adversarial perturbations. This can result in stable perturbations or vulnerability to different perturbations, leading to oscillation-like patterns in model performance. During adversarial training, the model may fail to increase robustness against perturbations. To determine prevalent failure cases, a temporal stability profile is computed by evaluating predicted probabilities of true class for perturbed inputs under different model parameters after epochs of adversarial training. The temporal stability profile is computed during adversarial training to assess model robustness against perturbations. It evaluates predicted probabilities for perturbed inputs under different model parameters. MaxMin and MinMax values indicate model vulnerability and oscillation between perturbations. Figure 1 illustrates stability profiles for different test inputs. The training makes the model robust against perturbations, with stable perturbations being more frequent and dominant. Singular perturbations are good predictors of stability, fooling the classifier on specific inputs while leaving predictions unchanged for most others. The cost change of a perturbation on an input x and parameters \u03b8 is defined as J \u2206 (\u03be, \u03b8, x, y) = J(\u03b8, x + \u03be, y) \u2212 J(\u03b8, x, y). The cost change of a perturbation on an input x and parameters \u03b8 is computed as J \u2206 (\u03be, \u03b8, x, y) = J(\u03b8, x + \u03be, y) \u2212 J(\u03b8, x, y). Profiles of test inputs under adversarial training show high-confidence correct classifications and incorrect classifications with high confidence. The cost change when transferring a perturbation to other inputs from the training set (\"sharedness\") versus stability for future classifier parameters is illustrated. The cost change of a perturbation on an input x and parameters \u03b8 is computed as J \u2206 (\u03be, \u03b8, x, y) = J(\u03b8, x + \u03be, y) \u2212 J(\u03b8, x, y). Profiles of test inputs under adversarial training show high-confidence correct classifications and incorrect classifications with high confidence. The perturbation's sharedness and stability when transferring to other inputs and model parameters are analyzed through random sampling and computation. The cost change of perturbations on inputs and parameters is analyzed, showing an anti-correlation between sharedness and stability. Adversarial training makes classifiers robust against shared perturbations but less effective against singular ones. Fast Feature Fool is a data-independent method for generating universal perturbations. The effectiveness of universal perturbations to fool classifiers trained with adversarial training has not been investigated. Adversarial training reduces the sharedness of adversarial perturbations, making it effective against universal perturbations. Models trained with adversarial training for 3 or more epochs classify at least 80% of the inputs with universal perturbations correctly. The universal perturbations generated for models in different epochs of adversarial training change considerably, with patterns evolving from checker-board to maze-like structures before settling into large, constant-color areas. This evolution suggests that perturbations on different training inputs become so different that they effectively cancel out, indicating singularity. Recent work has shown that universal perturbations generated on test data do not effectively fool the classifier, indicating singularity. Adversarial attacks on machine learning models can still be successful even when the adversary cannot manipulate the digital representation of a scene, but only the actual scene in the physical world. Recent work has demonstrated various adversarial attacks on machine learning models, including impersonation attacks on face recognition systems and fooling road sign classifiers with imperceptible artifacts. Additionally, a method has been proposed to make adversarial perturbations more robust to different transformations. The effect of adversarial training on perturbation robustness is evaluated by quantifying the destruction rate of adversarial examples against transformations. Adversarial training increases the destruction rate of adversarial examples under various transformations, such as changes in brightness, contrast, and Gaussian blurring. Models trained for more epochs show higher destruction rates, even when there are no improvements on test data. The impact of additive Gaussian noise on destruction rate is more complex. Adversarial training reduces the vulnerability of classifiers to being fooled by inputs but may make them less robust to strong Gaussian noise. Further research is needed to understand this phenomenon. Further experiments are needed to determine the effectiveness of defending against adversarial attacks by identifying and rejecting manipulated inputs. Various approaches have been proposed for detecting adversarial perturbations, but it remains uncertain if adversarial trained models exhibit the same detectable properties. The approach for detecting adversarial perturbations proposed by BID15 involves training a separate convolutional neural network as a detector to distinguish unchanged from adversarial inputs. The detectability of adversarial perturbations during adversarial training remains high, indicating that they still exhibit detectable patterns despite becoming more singular. The detectability of adversarial perturbations remains high during adversarial training, with detectors trained on examples from different epochs achieving over 90% accuracy. Patterns making adversarial examples detectable are relatively robust over time, unlike universal perturbations. Detectors trained on non-adversarially trained models do not reliably detect adversarial perturbations in later epochs, indicating a principled change in the first epoch of adversarial training. The method for generating universal perturbations was adapted to maximize the detector's predicted probability of inputs being adversarial. The perturbations show a checker-board pattern for models without adversarial training in epoch 0, but resemble universal perturbations in later epochs. However, these perturbations have a low mean probability of being detected as adversarial when added to inputs. The detector can identify adversarial perturbations with high accuracy. The detector accurately identifies adversarial perturbations, showing input-dependent patterns that cancel out. Results generalize to different classifiers, datasets, and adversaries, increasing perturbation destruction rate under various image transformations. Perturbations generated by FGSM and DeepFool are also affected. Adversarial training increases classifier robustness against universal perturbations and shared perturbations, making them more resistant to adversarial attacks. The destruction rate of perturbations is significantly increased under various image transformations, and perturbations remain detectable. Results are consistent across different datasets and classifiers, showing the effectiveness of adversarial training. Adversarial training enhances classifier resilience against universal perturbations and shared perturbations, reducing vulnerability to attacks. It also weakens singular perturbations, making them less robust against image transformations like brightness changes or blurring. This approach shows promise in preventing physical-world attacks on systems like face recognition or road sign classification. Future research should explore attacks on defended classifiers post adversarial training. Our results suggest that adversarial training may impact the feasibility of attacks on classifiers. While singular perturbations are detectable, adversarial training does not make classifiers robust against these patterns. Understanding stable patterns in adversarial examples could improve adversarial training and increase robustness against physical-world attacks. Further investigation is needed in this area. The learning curves of adversarial training show that accuracy on unchanged test inputs reaches around 90%, while accuracy on adversarial inputs is approximately 68% for FGSM and 62% for BI. The classifier starts to overfit after about 25 epochs, becoming robust against shared perturbations initially but then memorizing singular perturbations. This memorization does not generalize to test data. The black-box attacks in adversarial training are more effective than white-box attacks, attributed to gradient masking. Source and target models differ, making the attack rely on transferability between models. Adversarial training with stronger adversaries does not make classifiers more robust, only making adversarial examples harder to find. Adversarial examples generated early in training remain effective against later classifiers, indicating the presence of stable perturbations. Temporal stability profiles can be summarized using MaxMin and MinMax quantities, as shown in a scatter plot for test inputs. The scatter plot shows the impact of adversarial training on classifier robustness. Points in different corners represent various failure cases, with most points close to the diagonal indicating minimal oscillations between perturbations. The lower left corner represents the dominant failure case of stable perturbations fooling the classifier consistently. Fewer points are near the upper left corner, indicating oscillations between perturbations where the classifier is robust at different times. During adversarial training, classifiers show varying levels of robustness against perturbations. Shared adversarial perturbations on test data at different epochs were analyzed. Results showed that perturbations from the classifier without training were effective for 6.3% of other inputs on average. After adversarial training, sharedness of perturbations is significantly reduced. The most shared perturbation after 251 epochs is effective for less than 1% of other inputs. Adversarial training results in singular perturbations that work for specific inputs or a small fraction of similar inputs. The approach for generating universal perturbations is adapted for image classification. The procedure involves generating universal perturbations for image classification using a specific method. The experiments use certain parameters and iterations. Results show that universal perturbations generated on test data are not more effective than those on training data, indicating the classifier's robustness. The experiments involve generating universal perturbations for image classification. Results show that perturbations on test data are noisier than on training data, possibly due to remaining singular perturbations. Adversarial training makes perturbations less robust. Adversarial training reduces the robustness of perturbations to brightness, contrast, and blurring, making them more transferable to the physical world. FGSM and DeepFool adversaries show similar trends. FGSM is easier to detect than DeepFool. The experiments on the German Traffic Sign Recognition Benchmark (GTSRB) show that robustness against adversarial perturbations is higher compared to CIFAR10. Adversarial training with \u03b5 = 8 resulted in 95.2% accuracy on test data and 100% on training data. The training set was split into 35000 samples for training and 4209 samples for evaluation. Nonadversarial training was done for the first 25 epochs followed by 175 epochs of adversarial training. Adversarial perturbations using BI reach 95.2% accuracy on test data and 100% on training data in GTSRB, showing higher robustness compared to CIFAR10. Adversarial training with \u03b5 = 20 increases classifier accuracy against universal perturbations from 40% to over 98%. High-frequency noise in adversarial perturbations is effectively removed through adversarial training, enhancing robustness. Destruction rate of adversarial perturbations under different image transformations is shown in Figure 20 on GTSRB. Adversarial training on GTSRB increases destruction rate under various image transformations like brightness, contrast reduction, and Gaussian noise. Detectability of perturbations also increases significantly with higher \u03b5 values. Adversarial perturbations show little qualitative change during training, with detectors maintaining close to 100% detectability across different epochs. During adversarial training on GTSRB, the model becomes more robust against perturbations, with accuracy reaching 90% on test data and 97% on training data. The non-residual convolutional net initially shows more robustness than the residual network, but after sufficient adversarial training, the residual network becomes more robust. The accuracy of the convolutional classifier for universal perturbations of \u03b5 = 20 is shown in FIG1. Adversarial training increases classifier robustness against universal perturbations on GTSRB, with accuracy rising from 60% to over 95%. Universal perturbations consist of black or white curvy segments with no high-frequency noise. Adversarial training reduces their impact on accuracy, especially under brightness, contrast, and Gaussian noise changes. Adversarial training initially reduces the destruction rate of perturbations on GTSRB, but in the long run, it reaches a similar level as before. The detectability of adversarial perturbations on GTSRB increases considerably when \u03b5 is increased from 4 to 8. There is little qualitative change in adversarial perturbations during training, with many perturbations no longer fooling the model."
}