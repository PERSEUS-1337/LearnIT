{
    "title": "B1e9csRcFm",
    "content": "Learning distributed representations for nodes in graphs is essential for network analysis with various applications. Linear graph embedding methods optimize the likelihood of edges while constraining the norm of embedding vectors, which impacts generalization performance. The small norm of vectors, rather than the dimensionality constraint, is shown to be crucial for generalization error. Experimental evidence supports this argument, indicating that the norm of embedding vectors is influenced by early stopping of SGD and vanishing gradients. Graph embedding is crucial for network analysis, with various applications like link prediction and text classification. Linear graph embedding methods preserve graph structures by converting node embeddings into probability distributions. Several techniques have been proposed, showing impressive results in applications. Graph embedding methods convert node embeddings into probability distributions using techniques like negative sampling to optimize the softmax objective function. These methods essentially compute a factorization of the adjacency matrix of a graph, with the key to their generalization performance believed to be the dimensionality constraint. However, this paper argues that the small norm of embedding vectors is actually more crucial for good generalization. The paper argues that the small norm of embedding vectors is crucial for good generalization, supported by theoretical and empirical evidence. The success of linear graph embedding methods is attributed to early stopping of SGD, which implicitly restricts the norm of embedding vectors. Without proper norm regularization, overfitting can occur. The paper discusses the importance of the norm of embedding vectors for generalization in linear graph embedding. Experimental results for a hinge-loss variant further support this argument. The appendix contains details on experiment settings, algorithms, theorems, and related work. The embedding aims to learn vector representations for nodes in a graph to preserve its structure. These vectors can be used for tasks like node classification or link prediction. Linear graph embedding methods use inner products of vectors to capture edge likelihood. The objective is to minimize KL-divergence between embedding-based and actual neighborhood distributions. However, direct optimization of this objective is challenging due to softmax issues. The negative sampling technique is used to optimize the objective function for graph embedding by avoiding gradients over the full softmax function. Weighted cases can be handled by multiplying weights to the loss function of each edge. Deep neural network structures can also be utilized for graph embedding. The negative sampling technique is used for graph embedding optimization by avoiding gradients over the full softmax function. It involves computing embedding vectors using deep neural network structures. N+(u) can be direct neighbors in the original graph or an expanded neighborhood based on measures like random walk. Theoretical analysis of negative sampling is limited, with BID7 suggesting that embedding vectors approximate a low-rank factorization of the PMI matrix. Based on the analysis of negative sampling for graph embedding optimization, it is believed that optimizing under a dimensionality constraint is equivalent to computing a low-rank factorization of the PMI matrix. This concept is widely accepted in the context of word and graph embedding methods. Linear graph embedding techniques are thought to approximate low-rank factorizations of PMI matrices, with the dimensionality constraint of embedding vectors considered crucial for good generalization. In the context of graph embedding, the explanation of Levy & Goldberg regarding the sparsity of real-world networks is counter-intuitive. The total number of free parameters in graph embedding is usually larger than the total number of training data points, making it unlikely for the negative sampling model to guarantee generalization of embedding vectors. The good empirical performance of linear graph embedding methods is attributed to their small norm. The small norm of linear graph embedding vectors is attributed to vanishing gradients during optimization. Evidence shows that vector norm, not embedding dimension, determines generalization. The average norm of embedding vectors is small due to early stopping of SGD and vanishing gradients. Generalization performance drops when the norm of embedding vectors is large. Dimensionality constraint is helpful for small embedding dimensions with no norm regularization. Generalization error analysis of linear graph embedding is based on the uniform convergence framework BID0. The goal is to learn a model that generalizes well to the underlying distribution Q using training edges from Q and negative edges from a uniform distribution. The training error and generalization error of the embedding x are defined, and the uniform convergence framework aims to prove a statement over all possible embeddings in the hypothesis space H. The uniform convergence framework aims to prove that minimizing training error in linear graph embedding methods with norm constraints leads to small generalization error with high probability. The first technical result bounds the generalization error for nodes in the graph using a bounded 1-Lipschitz loss function and norm regularization. The importance of proper regularization in linear graph embedding is highlighted in Theorem 1, showing that choosing the right norm regularization is crucial for optimal generalization performance. Larger C values lead to smaller training error but a higher risk of overfitting, while smaller C values result in more restrictive models with larger training error. Optimal generalization performance hinges on selecting the most suitable norm regularization. Without norm regularization, embedding vectors can overfit training data on d-regular graphs, achieving zero training error even with randomly placed edge labels. The number of training samples needed for learning D-dimensional embeddings is at least \u2126(nD), considering many large-scale sparse graphs with average degree < 20 and embedding dimensions ranging from 100 to 400. In this section, experimental results suggest that generalization in linear graph embedding is determined by vector norm rather than embedding dimension. Stochastic gradient descent is used with a standard learning rate. Three datasets (Tweet, BlogCatalog, YouTube) are used with a default embedding dimension of 100. The average norm of embedding vectors increases consistently during the first 50 SGD epochs, but the rate slows down over time. Stochastic gradients also decrease in magnitude as SGD progresses, indicating vanishing gradients. This phenomenon is observed after a few epochs, especially when stopping early, resulting in small vector norms. After a few SGD epochs, most training data points are well fitted by embedding vectors, leading to small stochastic gradients in subsequent epochs. Generalization performance of embedding vectors drops after 5-20 epochs when \u03bb r is small, indicating overfitting. Performance is worst at the end of SGD when \u03bb r = 0, coinciding with largest norm of embedding vectors. FIG4 and FIG2 collectively suggest issues with generalization. The generalization of linear graph embedding is influenced by vector norm, as shown in FIG4 and FIG2. In the experiment on the Tweet dataset in FIG6, it is observed that without norm regularization, embedding vectors overfit the training data for dimensions greater than 10. The impact of embedding dimension choice is less significant with larger norm regularization, highlighting the importance of vector norm for generalization. Average Precision (AP) evaluates performance on ranking problems by computing precision and recall values at each position in the ranked sequence. The experimental results present a non-standard linear graph embedding formulation optimizing an objective with hinge-loss. The method uses dual coordinate descent (DCD) to avoid vanishing gradients in SGD and observe the impact of norm regularization. The optimization procedure using DCD can be found in the appendix. The pseudo-code for the optimization procedure using DCD can be found in the appendix. Impact of Regularization Coefficient: FIG8 shows the generalization performance of embedding vectors obtained from DCD procedure. Proper norm regularization is necessary for generalization. The choice of embedding dimension D is not very impactful as long as D is reasonably large (\u2265 30). These results are consistent with the theory that generalization of linear graph embedding is primarily determined by norm constraints. The generalization of linear graph embedding is mainly influenced by norm constraints. Evidence supports the argument that embedding vectors are determined by vector norm, aiming to embed vertices on a small sphere around the origin. The sphere's radius controls model capacity, with the embedding dimension balancing model expressiveness and computational efficiency. Norm regularization plays a key role in generalization performance, intuitively linked to the semantic meaning of embedding vectors. The probability value is determined by three factors: cosine similarity between vectors, norm of embedding vectors, and confidence level. Restricting the norm of embedding vectors helps prevent overfitting. Results clarify that linear graph embedding methods approximate factorization of matrices, but the norm of embedding vectors is constrained instead of the embedding dimension. This implies that the resulting factorization is not a standard low-rank one. The low-norm factorization is an alternative to standard low-rank factorization in linear graph embedding methods. Generalization is influenced by the norm of embedding vectors rather than the dimensionality constraint. Limiting the norm of embedding vectors leads to good generalization, with existing methods benefiting from early stopping of SGD and vanishing gradients. Our experiments show that the choice of embedding dimension only matters when there is no norm regularization. The best generalization performance is achieved by selecting the optimal norm regularization coefficient. Linear graph embedding methods may be computing a low-norm factorization of the PMI matrix, which is an interesting alternative to standard low-rank factorization. We used three datasets in our experiments, including a Tweet graph that encodes keyword co-occurrence relationships from Twitter data. During August 2014, 10,000 keywords were extracted as graph nodes with their co-occurrences as edges. Nodes with more than 2,000 neighbors were removed as stop words. Three datasets were used in experiments, including BlogCatalog, YouTube, and a Tweet graph encoding keyword co-occurrence relationships from Twitter data. Positive edges in training and testing datasets were paired with 4 negative edges for learning embedding vectors and evaluating average precision.\u03bb + = 1, \u03bb \u2212 = 0.03 were used for optimal generalization performance. In graph embedding research, initial coordinates of embedding vectors are uniformly sampled. Affinity graphs are constructed from data points' features, and low-dimensional embeddings are computed using eigenvectors. Recent techniques include using deep neural networks to compute embedding vectors, such as SDNE using adjacency lists to predict Laplacian. Prior research in graph embedding utilized deep neural networks like SDNE, GCN, and GraphSage with 2-3 layers, but evidence suggests that more layers may lead to worse generalization. It remains unclear if deep neural network structures are beneficial for graph embedding tasks. Previous studies have shown that norm constrained graph embedding could generalize effectively. The study explores norm constrained matrix factorization and its superior performance compared to standard low-rank matrix factorization on various tasks. The results are connected to linear graph embedding, showing the empirical Rademacher Complexity and contraction lemma in the hypothesis set. The text discusses matrix factorization with norm constraints, connecting it to linear graph embedding and the Rademacher Complexity. It also explores the Erdos-Renyi model for graph generation and the Central Limit Theorem for expected number of edges in a graph. The text discusses the unit sphere in n-dimensional Euclidean space and the probability of pairs of points satisfying a certain condition. It also estimates the order of ||A\u03c3||2 and presents the pseudo-code for the DCD method in linear graph embedding learning."
}