{
    "title": "BkeDGJBKvB",
    "content": "Multitask Soft Option Learning (MSOL) is a hierarchical multi-task framework that extends the concept of Options. It uses separate variational posteriors for each task, allowing a higher-level master policy to train faster on new tasks. MSOL outperforms hierarchical and flat transfer-learning baselines in challenging multi-task environments, addressing key challenges in Reinforcement Learning (RL) by scaling current approaches to higher complexity tasks efficiently. One way to improve exploration in Reinforcement Learning is by using efficient exploration priors such as reward shaping, curriculum learning, meta-learning algorithms, and transfer learning. Prior knowledge can be captured by decomposing policies into a hierarchy of subpolicies that can be reused to solve new tasks, known as Hierarchical RL (HRL). Hierarchical RL (HRL) is supported by the idea that humans use a hierarchical mental structure when solving tasks. In this approach, lower-level skills lead to directed behavior over multiple time steps, allowing for efficient exploration and reducing variance in future rewards. However, while HRL can speed up exploration and training, it may limit the expressiveness of the final policy and lead to suboptimal performance if the skills cannot express the required policy. Training on multiple tasks simultaneously is a promising approach to learn skills that generalize across tasks. Learning hierarchical policies end-to-end in a multitask setting poses challenges, as updating skills relies on already converged master policies. In this paper, Multitask Soft Option Learning (MSOL) is proposed as a novel approach to learning hierarchical policies in a multi-task setting. MSOL stabilizes end-to-end multitask training, promotes coordination between master policies, and allows for fine-tuning of skills. MSOL introduces a soft option framework that allows for fine-tuning of options, preventing unlearning of useful behavior. It differentiates between a prior policy for each option, shared across tasks, and a task-specific posterior policy. This approach enables accelerated training and adaptability to new tasks. MSOL introduces a soft option framework that allows for fine-tuning of options, preventing unlearning of useful behavior. It outperforms previous hierarchical and transfer learning algorithms during transfer tasks in a multitask setting. The training is stabilized by sharing priors across tasks, without the need for complex training schedules or specialized architectures. Planning as inference (PAI) frames RL as a probabilistic-inference problem where the agent learns a policy distribution over actions given states, inducing a distribution over trajectories. This structured variational approximation incorporates prior knowledge and true initial state and transition probabilities. Incorporating prior policy distribution and reward information, PAI aims to find a policy that approximates desirable trajectories by minimizing KL divergence in a multi-task learning setting. In a multi-task learning setting, the goal is to learn multiple tasks concurrently by distilling common information to learn faster on new tasks. The prior policy can be learned jointly with task-specific posterior policies, and minimizing the loss is equivalent to maximizing the regularized reward. Additionally, a discount factor is used in practice. In a multi-task learning setting, the goal is to learn multiple tasks concurrently by distilling common information to learn faster on new tasks. We make use of a discount factor \u03b3 \u2208 [0, 1] in the PAI framework. Options are skills that generalize primitive actions and consist of intra-option and termination policies. Our method aims to learn a reusable set of options for faster training on new tasks, preventing multiple options from learning the same behavior. Our novel approach, soft-options, consists of a shared option prior and task-specific option posterior to speed up training on new tasks while remaining flexible. The priors capture typical option behavior and are used to initialize the posterior, which is then regularized during training to prevent unlearning. If the reward cannot be maximized with the prior policy, the posterior can deviate. The soft-options framework introduces option selections and termination decisions to allow for flexible task-solving. It utilizes a hierarchical prior-posterior architecture and intra-option posterior policies to guide the agent's behavior. The soft-options framework introduces option selections and termination decisions to allow for flexible task-solving. It utilizes a hierarchical prior-posterior architecture and intra-option posterior policies to guide the agent's behavior. The joint posterior policy q T \u03c6i (b t |s t , z t\u22121 ) continues with the previous z t\u22121 or draws a new option, with z t \u2208 {1 . . . m}. The induced distribution q \u03c6i (\u03c4 ) over trajectories of task i transfers knowledge between tasks by a shared prior p \u03b8 (a t , z t , b t |s t , z t\u22121 ) over all joint policies. The parameterized priors p T \u03b8 (b t |s t , z t\u22121 ) and p L \u03b8 (a t |s t , z t ) can be used as initialization for posterior policies q T \u03c6i and q L \u03c6i. The soft-options framework utilizes a hierarchical prior-posterior architecture and intra-option posterior policies to guide the agent's behavior. The objective is to optimize the regularized return w.r.t. \u03b8 to distill common behavior into the prior policy and enforce similarity across posterior distributions of each option amongst all tasks. The multitask objective is extended by substituting induced policies from the hierarchical posterior policy and corresponding prior. The resulting objective maximizes a new regularized reward, encouraging exploration in the on-policy trajectories drawn from q \u03c6i (\u03c4). The regularization terms in the soft-options framework encourage exploration in the space of options and enforce similarity between option posteriors across tasks. This helps the master policy select the most specialized option that still achieves high rewards. During option learning, fixing the termination prior can encourage temporal abstraction of options, allowing for prolonged execution of one option while still allowing switching when necessary. This approach is similar to deliberation costs but with a more flexible cost model. Additionally, distilling a termination prior can lead to more decisive terminations for future tasks. The text discusses how hierarchical approaches in task learning rely on proxy rewards and subgoals to train lower level components. Some methods require knowledge of transition models, but unsupervised training objectives have been proposed for learning diverse skills. Hierarchical approaches in task learning use proxy rewards and subgoals to train lower level components. Some methods incorporate reward information but do not learn termination policies, limiting their ability to decompose tasks for new solutions. Training lower level policies to move towards goals in a learned embedding space can be challenging for high dimensional state spaces like images. Training hierarchical agents in reinforcement learning can be challenging, as the temporal extension of learned skills needs to be set manually. Some approaches focus on learning hierarchical agents to find decision states based on encoded information in the latent layer. Other methods use inference-motivated approaches to learn options and hierarchical policies. Previous works aim to overcome the limitations of options by allowing higher-level actions to modulate behavior. The curr_chunk discusses the use of multitask and transfer learning inspired by previous works to extract options without human supervision. The focus is on finding good termination states for the higher-level policy. The curr_chunk discusses different approaches to learning transferrable option priors, including Meta Learning of Shared Hierarchies (MLSH) and methods that differentiate between prior and posterior policies on multiple tasks. These approaches aim to improve diversity in learned options and decision regions linear in the state. The curr_chunk introduces MSOL, a method closely related to DISTRAL but with a hierarchical structure and the ability to condition on the last primitive action taken. MSOL overcomes local minimum issues, learns termination policies, and is applicable to discrete tasks. MSOL is a method that can learn termination policies and is applicable to both discrete and continuous domains. It utilizes soft options for fast transfer learning and optimal performance on new tasks. Architectural details and hyper-parameters are provided in the appendix. DISTRAL is a non-hierarchical transfer learning algorithm that utilizes prior and posterior distributions. DISTRAL(+action) uses the last action as an option-heuristic, MLSH is a multitask option learning algorithm that relies on 'hard' options and fixed option durations. Option Critic (OC) takes the task-id as additional input for multitask settings. The study starts with the 2D Moving Bandits environment. In each episode, the agent receives a reward for being close to marked positions in the environment. The agent can move in four directions, and the positions are not signaled in the observation. MLSH and DISTRAL are compared in multitask training, with MLSH not achieving diverse options like MSOL. DISTRAL struggles to utilize prior knowledge effectively. In a comparison between DISTRAL and MLSH in multitask training, DISTRAL struggles to effectively utilize prior knowledge. DISTRAL agents need to infer intentions based on the last action and goal locations, but even with a larger network, they struggle to do so. Longer training allows DISTRAL to perform as well as MSOL, highlighting the importance of adaptable options. Additionally, a modified version of the Taxi domain is used to demonstrate learning termination functions and transfer capabilities. The domain involves a taxi agent picking up and dropping off passengers at different locations on a grid with walls limiting movement. The observation is a one-hot encoding of the state, excluding passenger and goal locations, creating information asymmetry. Two versions of the Taxi domain are investigated, with different action spaces. In the Directional Taxi environment, the agent can move forward or rotate in cardinal directions. Tasks involve 12 pickup/drop-off location combinations with a reward for successful delivery and a penalty for each time step. During training, the agent starts from any valid state, while during testing, it starts without a passenger. MLSH struggles with option diversity and termination learning. MSOL performs well in both taxi environments, showing the utility of adaptable soft options during transfer. It successfully learns useful movement primitives and termination functions, with the same soft option representing different behavior depending on whether the passenger is picked up. MSOL performs well in both taxi environments, demonstrating the effectiveness of adaptable soft options during transfer. Learning information-asymmetric soft options aids in generalizing to unseen tasks, showing the need for soft options for training options that generalize well. MSOL demonstrates superior performance in adapting to new tasks in taxi environments and continuous multitask domains, outperforming hard options and flat policies. It shows the effectiveness of adaptable soft options for generalizing to unseen tasks and excels in learning information-asymmetric soft options. MSOL proposes reformulating options using prior and posterior distributions, offering advantages in transfer learning and multitask setups. It increases optimization stability and removes the need for complex training schedules between policies. Hierarchical deep reinforcement learning framework allows master policies to coordinate across tasks, learn option-termination policies autonomously, and incorporate prior information without imposing rigid structures. This formulation explicitly includes the bias that good options should be temporally extended. Future research can explore other types of information, such as tasks benefiting from a learned master prior. Hierarchical deep reinforcement learning integrates temporal abstraction and intrinsic motivation. The regularization of rewards involves minimizing KL-divergences between priors and posteriors along trajectories. Prior knowledge in the form of skills can be used in the soft option framework, with a temperature parameter determining the level of restriction. In the soft option framework, the temperature parameter \u03b2 determines the level of restriction in following prior skills. For \u03b2 \u2192 \u221e, policies are restricted to the prior, while for \u03b2 = 0, priors only initialize policies. The regularized reward can be treated as a classical RL reward, allowing the use of any RL algorithm to find optimal hierarchical policy parameters. A2C can be adapted to soft options, with a straightforward extension to PPO. The joint posterior policy depends on the current state. The joint posterior policy in (4) depends on the current state and the previously selected option. The value function V i must condition on this pair and is approximated with a parametrized model V \u03c6i. The k-step advantage estimation at time t of trajectory \u03c4 is given by minimizing L V (\u03c6 i , \u03c4 1. The policy gradient loss is optimized towards its bootstrapped k-step target. To encourage exploration, an entropy maximization loss is included. In addition to the entropy maximization loss for exploration, a linear schedule is used to anneal the exploration bonus during training to prevent the master policies from converging too quickly. This allows for better-defined options and more stable optimization. During training, a high value of \u03b2 improves options but delays finding the extrinsic reward. \u03b2 is increased linearly over training. Policies and value functions share the same encoder network with varying hidden layer sizes. Distral was tested with different model sizes to ensure capacity is not an issue. The encoder is shared across tasks, with master-policies and other functions having one layer taking the encoder's latent embedding as input. Our implementation of the Distral architecture utilizes a shared encoder across tasks, allowing for faster training. Options are specified as additional inputs to the network and concatenated to the state embedding. We use a single-column hierarchical policy with a modified loss function. The implementation is based on A2C/PPO by Kostrikov (2018) and MLSH by OpenAI. The implementation of Distral architecture involves task distribution and resetting individual tasks. Optimization for MSOL and Distral is done over specific values. Different gamma values are used for different environments. MLSH uses original hyper-parameters. Training is done with 30 parallel environments split between 10 tasks. Training duration and number of parallel environments are the main differences from the original paper. Training is done over 6 million frames per task for MSOL and Distral. Learning rate, beta, alpha, and lambda values are set for the training process. Training for different architectures involves setting specific values for learning rate, beta, alpha, and lambda. Distral uses beta = 0.04 and 0.6 million frames per task, while MSOL anneals beta from 0.02 to 0.1 and lambda from 0.1 to 0.05. MLSH was trained on 0.6 million frames for Taxi due to long runtime. Soft options remain useful even with changing task distributions, while hard options perform poorly compared to learning from scratch. Training with soft options is beneficial for transfer to out-of-distribution tasks, especially in environments with complex exploration problems. Using PPO instead of A2C for training Distral and MSOL leads to better performance on continuous tasks. We use PPO for better performance on continuous tasks. \u03bb H = 0.0004 for primitive actions and 0.1 for master-and termination policies. Learning rate is 0.0002, GAE with \u03c4 = 0.98. 2000 steps collected in parallel on 6 processes per task. Training over 6 million frames with linearly scheduled increase of \u03b2. MSOL and Distral have different \u03b2 values. MSOL and MSOL(frozen) share the same training. Soft options can accelerate training for larger grid sizes. Soft options can accelerate training for larger grid sizes, even if misspecified from being trained on different tasks."
}