{
    "title": "SyeyPEH23N",
    "content": "We investigate training difficulties of sparse neural networks and optimization dynamics. Sparse ResNet-50 architectures on ImageNet-2012 converge to worse solutions than pruning. Despite optimizer failures, a linear path with decreasing objective exists from initialization to a \"good\" solution. Finding a path from \"bad\" to \"good\" solutions in sparse subspace fails, but traversing dense subspace consistently finds a path. Extra dimensions may be needed to escape stationary points in sparse subspace. Sparse networks are a leading approach for reducing model parameter count in machine learning models. Deep neural networks can achieve state-of-the-art results under high levels of sparsity, significantly reducing parameter footprint and inference complexity. Training densely connected networks and modifying their topology during training has yielded state-of-the-art results in sparse networks. Training sparse neural networks involves modifying network topology through pruning techniques. Two approaches are commonly used: training a sparse network from scratch or reusing the sparsity pattern from pruning. Previous studies have shown that both methods achieve similar final accuracies, but lower than pruning. Despite being in the same energy landscape, training from scratch or using the lottery initialization fail to match the performance of pruning solutions. Understanding the limitations of current techniques in training sparse networks is crucial for further advancements in this field. Training sparse neural networks involves modifying network topology through pruning techniques. Previous studies have shown that training sparse networks from scratch or reusing the sparsity pattern from pruning achieve similar final accuracies, but lower than pruning. The role of changing connections during optimization is not clear. A series of experiments were conducted to improve understanding of training sparse neural networks and identify future directions. The experiments conducted aimed to understand the training of sparse neural networks and the impact of changing connections during optimization. Results showed that a linear path between scratch and pruned solutions had a high energy barrier, while removing sparsity constraints allowed for finding decreasing objective paths between sparse solutions. Dense connectivity may be necessary to escape stationary points in optimization. In the paper, dense connectivity is highlighted as necessary to escape stationary points in optimization. The experimental setup, results, and training methods are detailed in subsequent sections. Different training strategies are compared, including training a densely connected model and applying model pruning to find the Pruned Solution (P-S). Other strategies involve starting with a sparse connectivity pattern obtained from the pruned solution, resulting in Lottery Solution (L-S) and Scratch Solution (S-S). In experiments based on Resnet-50 architecture and Imagenet-2012 dataset, model pruning using magnitude-based approach is compared to variational dropout and L1 regularization. Top performing pruning schedules are chosen for each sparsity level, with hyper-parameters determined through grid search. The 80% sparse model maintains accuracy, while the 98% sparse model drops to 69% top-1 accuracy. Interpolation in parameter space was explored by generating networks along a segment and evaluating them on the training set of 500k images. This was done to understand the energy landscape of neural network training, especially in the context of sparse networks like Resnet-50 on Imagenet-2012 dataset. Interpolation in parameter space was explored by generating networks along a segment and evaluating them on the training set of 500k images. The objective is identical to the objective used during training, which includes a weight decay term scaled by 10^-4. Non-linear paths between the initial point and solution were found using parametric B\u00e9zier curves of order n = 2 and 3. Optimization was done using a stochastic method with a batch size of 2048, and a hyper-parameter search was performed over base learning rates. Our experiments focused on the energy landscape of sparse deep networks, revealing a gap in understanding compared to over-parameterized dense networks. Pruning prevents the optimizer from getting stuck in \"bad\" local minima, leading to surprising failures in optimization. The study explores the energy landscape of sparse deep networks, highlighting the challenges in optimization. The Hessian of a convolutional network trained on MNIST is found to be degenerate, indicating a flat landscape at the solution. Linear interpolation experiments show a path from sparse initial points to pruning solutions with monotonically decreasing training loss. The presence of saddle points in critical points far from global minima is discussed. The study investigates the energy landscape of sparse deep networks, focusing on optimization challenges. Linear interpolation experiments demonstrate a decreasing path from high loss to low loss solutions, indicating a saddle point. Model pruning enables the optimizer to descend steeply while still finding a good solution. The linear interpolation experiments show a decreasing path from high loss to low loss solutions, indicating a saddle point. Model pruning allows the optimizer to descend steeply while still finding a good solution. The training loss along the linear segment and the parametric B\u00e9zier curve connecting the scratch and the pruned solutions are shown in FIG3. Linear interpolation depicts a barrier between solutions, similar to randomly initialized networks. The sparse parametric curve found through optimization also fails to connect the two solutions with a monotonically decreasing path. The third order B\u00e9zier curve fails to decrease the maximum loss value compared to the second order curve. Removing the sparsity constraint and optimizing over the full parameter space results in consistently finding paths with significantly smaller objective values. The algorithm minimizes the integral of the objective over the interpolation segment, leading to paths that are not strictly monotonically decreasing. Our algorithm minimizes the integral of the objective over the interpolation segment, not enforcing monotonicity. Insights into optimization dynamics in the sparse regime may improve regularization techniques and optimization algorithms for training sparse networks. Despite the existence of monotonically decreasing paths, optimizers in the sparse regime converge to stationary points with sub-optimal generalization accuracy. Sparse networks reaching pruned accuracy levels remain a challenge. Our experiments use a pruning algorithm to reach a predefined sparsity goal during training. We use a batch size of 4096 and train the network for 48000 steps. The learning rate starts at 0 and increases linearly to 0.1 over 5 epochs, then remains constant until the 50th epoch. After that, the learning rate is decreased by a factor of 10 at the 50th, 120th, and 140th epochs. The first convolutional layer is not pruned due to sensitivity, and the maximum sparsity for the final fully connected layer is capped at 80%. Top 3 pruning schedules for each sparsity level are shared in a table. The average 1 norm of the gradient for the first setting in the table is 4e-6. Solutions in iterative methods are not stationary points as the gradient is never exactly zero. A stationary point is considered when the gradient is below 10^-5. Randomly setting a fraction of weights to zero at the beginning of training shows a sudden drop only if more than 99% of the parameters are zero. Initialization methods controlling variance of activations are crucial for training deep neural networks. In sparse-init experiments, a fraction of initial weights are set to zero based on pruning. Training succeeds with final accuracy around 76%, matching the original training performance. Randomly setting weights to zero shows no significant difference until 99.5%, after which there is a sharp drop in performance. The initialization requires a small number of non-zero weights to succeed. Regularization terms are consistently lower in weight magnitude than the initialization. Cross entropy loss shows a decreasing pattern in sparse to sparse interpolations but increases in dense to sparse interpolations. Time to drop is shorter with original initializations. The cross entropy is increasing with different types of initializations. Experiments fail to find paths where the loss decreases between \"pruned\" and \"scratch\" solutions. The training loss along the third order B\u00e9zier curve shows a small barrier between solutions."
}