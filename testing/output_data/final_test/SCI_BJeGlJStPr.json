{
    "title": "BJeGlJStPr",
    "content": "The practical usage of reinforcement learning agents is often hindered by long training times. To speed up training, distributed reinforcement learning architectures are used to parallelize the process. However, current scalable RL methods sacrifice sample efficiency for increased sample throughput. A new algorithm called IMPACT addresses this issue by incorporating changes to PPO, resulting in higher rewards in discrete action-space environments. IMPACT achieves higher rewards and up to 30% faster training than IMPALA in discrete action-space environments. In continuous control environments, IMPACT trains faster while maintaining sample efficiency similar to synchronous PPO. It addresses the sample efficiency issue of IMPALA by incorporating changes to PPO, resulting in improved performance. IMPACT, a novel algorithm, achieves faster training and higher rewards than IMPALA in discrete action-space environments. It maintains sample efficiency similar to synchronous PPO in continuous control environments. The algorithm introduces a target network to stabilize agents in distributed setups and demonstrates improved performance through ablation studies. IMPACT introduces a circular buffer for storing experiences, improving real-time performance and sample efficiency. It outperforms synchronous PPO and IMPALA in both real-time and timestep efficiency. The algorithm utilizes a batch buffer for worker experience and learner sampling, enhancing training efficiency in reinforcement learning setups. The transition dynamics and reward function model the environment in reinforcement learning. A stochastic policy maps actions to states, and rolling out the policy involves sampling trajectories. The goal is to maximize an objective by updating the policy using the Policy Gradient Theorem. The advantage estimator is typically the 1-step TD error, which is used to update the policy. Proximal Policy Optimization (PPO) optimizes policy \u03c0 \u03b8 from target \u03c0 \u03b8old with a clipping hyperparameter to reduce variance. PPO uses GAE-\u03bb as an advantage estimator and a surrogate loss with a clipping function to ensure reasonable steps. PPO can be viewed as an adaptive trust region introduced in TRPO. In TRPO, distributed PPO agents implement synchronous data-gathering. IMPALA decouples acting and learning, maximizing GPU utilization and sample throughput. V-trace is used to correct stale policy gradients as worker policies diverge from the learner policy. The IMPALA paper utilizes V-trace to correct stale policy gradients by adjusting the distributional shift. It involves initializing network weights, creating workers, updating policy and value networks, and broadcasting weights to workers. IMPACT algorithm, similar to IMPALA, separates sampling workers from learner workers. Each worker copies weights from the master network and uses their own policy to collect trajectories. Trust region conflicts are avoided by updating the learner policy after each batch. IMPACT allows for multiple SGD steps per async batch and maintains a stable trust region. The IMPACT algorithm utilizes a worker process to produce experiences, a \u03c0 learner for the current policy, and a \u03c0 target for the target network. The target network ensures a stable trust region and allows for multiple SGD steps per batch of experiences collected asynchronously from workers, improving sample efficiency. The target network periodically syncs with the master learner, and the master learner draws experiences from a circular buffer weighted by the importance ratio of \u03c0 target. This setup enables real-time efficiency and multiple steps per batch. The network provides a stable trust region for multiple steps per batch in the asynchronous setting. The improved objective involves gathering experience from previous policies and training the current policy through importance sampling. Each worker has its own policy generating experience for the policy network, with batch distribution parameterized as a categorical distribution. A target network is introduced for stability, similar to off-policy agents like DDPG and DQN. In IMPACT, the target network is updated periodically with the master network to prevent numerical instability. Clipping the importance sampling ratio from worker policy to target policy helps maintain stability. The target network update frequency is shown to be robust to different choices. The target network in IMPACT is updated periodically to prevent instability. Clipping the importance sampling ratio helps maintain stability and encourages the agent to learn faster at the cost of instability. GAE-\u03bb with V-trace is used to modify the advantage function. IMPACT introduces a circular buffer to emulate mini-batch SGD in standard PPO. The buffer stores N batches that can be traversed at max K times before being replaced by a new worker batch. This approach differs from standard replay buffers used in ACER and APE-X, where transitions are uniformly sampled or based on priority. The performance of the clipped-target objective is investigated relative to prior work like PPO and IS-PG. The study compares the performance of the clipped-target objective with PPO and IS-PG objectives. Two key findings are highlighted: R1 experiences performance drops due to network mismatch, while R2 struggles with conflicting worker suggestions. The learner moves forward with recent worker suggestions without a trust region, causing conflicts. Clipping the target-worker ratio prevents destabilization and conflicting suggestions. A target network provides singular guidance to prevent mutual destruction. An analogy is made between PPO's mini-batching mechanism and a circular buffer. The primary benchmark for target update frequency is n = N \u00b7 K, where N is circular buffer size and K is maximum replay coefficient. Testing update frequency in Figure 4 (b) shows agent performance is robust to varying frequencies. Based on empirical results, the agent's performance is robust to different frequencies, except when n = 1 \u223c 4. The agent's ability to train depends on forming a stable trust region. A low update frequency can strand the agent in the same trust region, slowing down learning. IMPACT shows better performance in both time and sample efficiency when K = 2, optimizing wall-clock time and sample efficiency by tuning values of K in the circular buffer. IMPACT demonstrated superior performance in wall clock-time and sample efficiency compared to PPO and IMPALA across various continuous and discrete tasks. The agent was tested on HalfCheetah, Hopper, Humanoid, Pong, SpaceInvaders, and Breakout environments using specific network architectures. Additional experiments were conducted on discrete environments with consistent network structures. The critic network in the study shares weights with the policy network and uses a stack of four down-sampled images as input. IMPACT outperformed PPO and IMPALA in training speed and sample efficiency for both continuous and discrete tasks. The optimal tuple (N, K) for continuous control tasks was found to be N = 16 and K = 20. IMPACT's performance in both continuous and discrete control tasks is influenced by the number of workers. More workers lead to increased sample throughput, resulting in better performance in less time. However, as the number of workers increases, the performance gains begin to decline. Agents learn faster from new experiences than replaying old ones, emphasizing the importance of exploration for achieving high performance in discrete environments. Distributed RL architectures like Gorila, A3C, A2C, IMPALA, ACER, and Ape-X are used to accelerate training by sending gradients or experience tuples to the learner. IMPACT incorporates the sample-efficiency benefits of PPO in an asynchronous setting, achieving better results compared to Surreal PPO. IMPACT extends PPO with a stabilized surrogate objective for asynchronous optimization, improving real-time performance without sacrificing efficiency. It utilizes a target network to define a stable trust region for the PPO surrogate objective, differentiating it from off-policy methods like DDPG and QProp. IMPACT outperforms tuned PPO and IMPALA baselines in both real-time and timestep metrics. Starting from IMPALA, the agent gradually incorporates PPO's objective function and circular replay buffer, forming an asynchronous variant of PPO (APPO). However, APPO does not perform as well as synchronous distributed PPO due to the high variance and large update-step sizes of the Vanilla Policy Gradient (VPG) objective used in IMPALA. Tuning the learning rate for VPG shows that higher rates lead to faster initial learning but result in negative returns later on. IMPALA struggles in continuous environments, likely due to its importance-sampled VPG objective. Our results with IMPALA show similar performance to other VPG-based algorithms. A3C, a close neighbor to IMPALA, performs well in InvertedPendulum but struggles in continuous environments. The IS ratio is large when worker assigns low probability, and IMPACT target-clip is a lower bound of the PPO-clip in distributed asynchronous settings. Trust region suffers from larger variance in this setting. In a distributed asynchronous setting, the trust region faces increased variance due to off-policy data. IMPACT target-clip ratio helps by promoting conservative policy-gradient steps."
}