{
    "title": "HJlmhs05tm",
    "content": "Unsupervised learning involves capturing dependencies between variables using generative models or energy functions. The energy function separates probable configurations from improbable ones, and can be used for anomaly detection. In contrast, generative adversarial networks use a critic to distinguish between real data and generated samples. By adding an entropy maximization regularizer, the critic can be interpreted as an energy function for separating the training distribution from other data. This paper explores running a Markov chain in latent space to map samples to data space, improving sample quality and gradient estimation. It utilizes neural estimators of mutual information to maximize entropy in the generator output, resulting in sharp samples and high evaluation scores. The early work on deep learning focused on training energy-based models like RBMs using unsupervised learning methods. However, the challenge of estimating the gradient of the partition function made training these models difficult. Various algorithms were proposed, but the introduction of variational auto-encoders and generative adversarial networks made training RBMs less competitive. In this paper, a novel approach called EnGAN is proposed for training energy-based models, inspired by the efficiency of sampling in latent space observed in auto-encoders. The distribution in latent space is simpler and flatter compared to the complex data manifold, leading to faster mixing and more efficient sampling. EnGAN is a novel approach for training energy-based models, emphasizing the need to regularize the generator to increase entropy and produce negative examples to eliminate spurious minima. This is crucial for matching the density associated with an energy function and balancing low energy configurations with a high-entropy distribution. EnGAN is a novel architecture for energy functions, utilizing GAN-based approaches to maximize mutual information and entropy. It proposes a general framework for training energy functions, incorporating an estimator of mutual information and MCMC sampling in latent space. EnGAN is effective for anomaly detection and generates sharp images with competitive scores. EnGAN is a novel architecture for energy functions that produces sharp images with competitive scores and better mode coverage than standard GANs. It avoids the blurriness issue of many generative models by optimizing the energy function parameters through maximum likelihood training. The positive phase distribution is matched with the negative phase distribution during training in order to shape two distributions. This concept has led to the idea that model samples are negative examples, and a classifier can learn an energy function by separating the data distribution from the model's samples. Goodfellow also made a similar connection related to noise-contrastive estimation. The objective function for Wasserstein GANs is similar to Eq. 2, which presents a way to train a form of WGAN where the discriminator computes an energy function. The main challenge is to obtain samples. The main challenge in obtaining samples from the distribution associated with the energy function is addressed through Monte-Carlo Markov chains. RBMs use Contrastive Divergence and Stochastic Maximum Likelihood algorithms for Gibbs sampling, but have not been competitive compared to autoregressive models in recent years. Energy-based models like RBMs have struggled to compete with autoregressive models, variational auto-encoders, and GANs in recent years. The difficulty lies in running a Markov chain in data space when the distribution is concentrated with many modes separated by low probability areas, making mixing between modes challenging in high-dimensional spaces. The authors propose a heuristic method for jumping between modes in high-dimensional spaces with concentrated probability mass. They suggest using an auto-encoder's latent space for a random walk to obtain data samples, arguing that auto-encoders flatten the data distribution. The EnGAN sampling method is similar but learns energy functions in both data and latent space, leading to better samples. This allows for efficient sampling from the energy function and opens up possibilities for Metropolis-Hastings rejection. In order to estimate the log-likelihood gradient with respect to the energy function, a GAN discriminator can be turned into an energy function by maximizing entropy at the output of the generator. This involves replacing the difficult to sample p \u03b8 with another generative process, such as the generative distribution associated with a GAN generator. A regularizer is used to avoid numerical problems in the scale of the energy, similar to the training objective of a WGAN. Optimizing the generator in a WGAN involves minimizing the KL divergence between the generated distribution and the target distribution. This can be achieved by maximizing entropy at the output of the generator and estimating mutual information between input and output. The entropy at the output of a deterministic function (the generator) can be computed using an estimator of mutual information between the input and output. Neural mutual information maximization methods can be applied to estimate and maximize the entropy of the generator by training a discriminator to separate the joint distribution from the product of the marginals. The training objective for the discriminator involves creating negative examples by shuffling columns of a matrix holding a minibatch. The Deep INFOMAX (DIM) estimator is used to maximize the Jensen-Shannon divergence between the joint and the marginal distributions. The discriminator is trained to increase entropy at the output of the generator by maximizing the mutual information IJSD(X, Z). The overall training objective for the generator involves minimizing the term IJSD(G(Z), Z) to maximize the generator's output entropy H(G(Z)). The training objective for the generator is to maximize the generator's output entropy H(G(Z)) by considering E \u03b8 \u2022 G as an energy function in latent space and running an adjusted Langevin in that space. The training objective for the generator is to maximize output entropy by considering an energy function in latent space and running an adjusted Langevin. The Metropolis-adjusted Langevin algorithm is used for MCMC sampling, with a proposal distribution in the latent space. The overall training procedure for EnGAN is detailed in Algorithm 1, with MALA referring to the procedure for sampling by MCMC. Generative models trained with maximum likelihood often suffer from spurious modes and excessive entropy, leading to incorrect probability assignments. Energy-based models like RBMs also struggle with poor approximation issues. The training process involves updating parameters using Adam, sampling real and latent data, and shuffling latent variables. EnGAN, unlike RBMs, does not suffer from spurious modes. Energy model trained on synthetic datasets shows sharp energy distribution. Similar to GANs, EnGAN is prone to mode dropping issues. The study evaluates the performance of generative models by training on the StackedMNIST dataset and comparing the number of captured modes and KL divergence. The reverse KL penalty and entropy maximization terms are used to address issues in the energy model's distribution. Experimental verification is crucial to assess the effectiveness of these methods. Our model, PacGAN, outperforms WGAN-GP in capturing all modes of the data distribution with low KL divergence scores. Empirical findings show that modeling 10^3 modes was trivial for WGAN-GP, prompting evaluation on a new dataset with 10^4 modes. The 4-StackedMNIST dataset was created for this purpose, with generative models trained using maximum likelihood to avoid producing blurry samples. Our EnGAN model is trained to match the data distribution with a reverse KL penalty. We evaluate its performance on the CIFAR10 dataset and CelebA dataset, comparing results with WGAN-GP using Inception Score and Frchet Inception Distance. EnGAN produces samples comparable to WGAN-GP in terms of perceptual quality. The EnGAN model produces high-quality samples from the CelebA and 3-StackedMNIST datasets. The visual quality of samples can be enhanced using the proposed MCMC sampler. Energy estimates are useful for density estimation and unsupervised anomaly detection in various fields like cybersecurity and medical care. Estimation is crucial for anomaly detection as anomalies are found in low probability density areas. The efficacy of an energy-based density model for anomaly detection is tested on KDDCUP and MNIST datasets. The model is trained on KDD99 data using the score norm ||\u2207 x E \u03b8 (x)|| 2 2 as the decision function. Our model's decision function is derived from 5 runs, with metrics averaged over the last 10 epochs. It shows high performance in anomaly detection on high dimensional image data, outperforming VAEs and comparable to DAGMM. The evaluation is done on MNIST dataset, treating each digit class as an anomaly. Our energy model excels in outlier detection and matches the state-of-the-art model. Our energy model outperforms VAEs for outlier detection and is comparable to BiGAN-based anomaly detection methods. The Metropolis Adjusted Langevin Algorithm in latent space produces good samples in observed space, showing a smooth walk on the image manifold. EnGAN is an energy-based generative model that utilizes a GAN-like technique to maximize entropy at the output of the generator. It produces energy estimates using an energy model and a generator that generates fast approximate samples. The model demonstrates good performance in unsupervised anomaly detection and produces semantically meaningful images by smoothly walking on the image manifold. EnGAN is robust in unsupervised anomaly detection and produces high-quality samples. MCMC in latent space outperforms MCMC in data space, which suffers from poor mixing and spurious modes. For all experiments, Adam optimizer with specific parameters is used. Toy Data experiments involve simple MLPs with 3 hidden layers. StackedMNIST experiments follow previous architectural choices. CIFAR10 experiments adopt the 'Standard CNN' architecture from SpectralNorm. For CIFAR10 experiments, the 'Standard CNN' architecture is used. The Statistics Network architecture is adapted similar to StackedMNIST experiments. A large energy norm coefficient is used for anomaly detection on the KDD99 dataset. The same architecture as StackedMNIST experiments is used for MNIST anomaly detection."
}