{
    "title": "r1Zi2Mb0-",
    "content": "Neural architecture search (NAS) has emerged as a promising approach for improving models automatically. This paper explores NAS for text sequences, focusing on language translation and reading comprehension tasks. Extensive searches were conducted on recurrent cells and attention functions, leading to translation improvements and transferable results to reading comprehension on the SQuAD dataset. The text discusses how results from attention searches can be applied to reading comprehension tasks on the SQuAD dataset. Various methods for finding neural architectures automatically have been explored, with a focus on vision tasks. The paper aims to bridge the gap by applying these methods to language tasks. The study explores neural architecture search for language tasks, focusing on improving neural machine translation (NMT) by finding new recurrent cells and similarity functions for the attention mechanism. Challenges in cell searches for NMT are reported, along with initial success in attention searches leading to translation improvements over strong baselines. The attention similarity functions found for NMT are shown to be transferable. The study explores neural architecture search for language tasks, focusing on improving neural machine translation (NMT) by finding new recurrent cells and similarity functions for the attention mechanism. The attention similarity functions found for NMT are transferable to the reading comprehension task on the Stanford Question Answering Dataset (SQuAD) BID14, yielding improvements over the standard dot-product function. NAS attention search on SQuAD further boosts performance. In neural architecture search (NAS), a controller iteratively samples a model architecture for a task to obtain a reward signal. The study focuses on neural architecture search for language tasks, specifically improving neural machine translation (NMT) by finding new recurrent cells and attention similarity functions. The reward signal is used to update the controller for better architectures over time. The search spaces for recurrent cells and attention similarity functions are described. The study focuses on neural architecture search for language tasks, specifically improving neural machine translation (NMT) by finding new recurrent cells and attention similarity functions. The search space is designed to search for LSTM-inspired cells using a balanced binary tree computation approach controlled by an RNN controller. The controller decides operations and nonlinearity for each node in the tree, incorporating previous cell information and producing output for new cells. The study explores neural architecture search for NMT, focusing on LSTM-inspired cells in a binary tree structure. A stack-based programming language is proposed as an alternative to fixed structures, reminiscent of the Push language used in genetic programming. The study proposes a stack-based programming language for neural architecture search in NMT, focusing on LSTM-inspired cells in a binary tree structure. The language allows unary or binary operations with the ability to copy outputs from previous steps. Inputs are pushed onto the stack at the start, and the output is either the sum of remaining values or the top value of the stack. The stack-based programming language for neural architecture search in NMT uses a sequence of operations to manipulate a stack. The attention mechanism in NMT is crucial for successful translation, with attention weights computed to determine useful memory slots for translation. The attention mechanism in NMT is computed using scoring functions to determine memory slots for translation. A stack-based search space is used to find the scoring functions, with a key vector followed by a query vector in the stack. The RNN controller predicts a list of operations, including a reduce operation to turn vectors into scalars. In the stack-based search space for attention, the search space complexity is set to 8. The set of binary and unary ops include linear, mul, add, sigmoid, tanh, relu, and identity. The controller predicts ops like (linear, mul) and all scoring functions end with a reduce op. Attention mechanisms are crucial in question answering systems, where the task is to output a span containing the answer in a given context paragraph and query sentence. Top models on datasets like SQuAD use some form of attention mechanism. The top models in question answering systems, such as those on the SQuAD dataset, utilize attention mechanisms like Bidirectional Attentive Flow (BiDAF). These models encode query and context separately, then perform bidirectional attention between them. To test generalization, attention mechanisms from Neural Machine Translation (NMT) are applied. Different effects of Neural Architecture Search (NAS) are tested on small and large-scale translation setups. In Neural Architecture Search (NAS), two phases are involved: searching for architectures by training child models and running convergence by training top architectures until convergence. Evaluation metrics include perplexity and BLEU BID11 scores, with reward signals being perplexity or BLEU. Data used is a small parallel corpus of English-Vietnamese TED talks, tokenized with a default Moses tokenizer. The final data has a vocabulary size of 17K. The final data has vocabulary sizes of 17K for English and 7.7K for Vietnamese. The full model is an attention-based sequence-to-sequence model with 2 LSTM layers of 512 units each. Training includes 12K steps with a dropout rate of 0.2 and batch size 128. The optimizer is SGD with learning rate 1, halving every 1K steps. Parameters are initialized within [\u22120.1, 0.1]. Child models for NAS have identical hyperparameters due to the small dataset. The dataset for the WMT German-English translation task consists of 4.5M training sentence pairs split into subword units using the BPE scheme BID20. The model architecture is based on Google's Neural Machine Translation systems with 4 LSTM layers of 1024 units. Training is done with SGD for 340K steps with hyperparameters similar to the English-Vietnamese setup. The training process involves dropout of 0.2 and a gradient norm of 5.0. Training is done with SGD for 340K steps, with a learning rate of 1.0 that halves every 17K steps. The child model used has 2 layers and 256 units, trained for 10K steps. The controller RNN is trained using Proximal Policy Optimization with a learning rate of 0.0005 and an entropy penalty of 0.0001. The controller weights are initialized within [-0.1, 0.1]. A pool of 100-200 GPUs is used for processing child networks proposed by the RNN controller. The experiments involve a pool of 100-200 GPUs for neural architecture search. The top 10 architectures are chosen based on dev performance saturation. Results show that architecture searches on translation tasks outperform NASCell, with cells achieving 26.2 BLEU on the IWSLT benchmark. Beating the strong baseline on the WMT task remains a challenge. Neural architectural searches for translation show improved performance compared to NASCell, achieving 28.4 BLEU on WMT and 29.1 BLEU with attention searches. The attention function is transferable to the IWSLT benchmark, reaching 26.0 BLEU. In this section, the results of NAS search for translation are discussed, highlighting the impact of optimizers and reward functions on architecture search. The use of Adam optimizer outperforms SGD, achieving higher BLEU scores on both dev and test sets. However, recurrent cells found by Adam are unstable, leading to worse performance. The recurrent cells found by Adam are unstable, yielding worse performance compared to those found by SGD. Glorot initialization scheme BID5 did not alleviate the problem of large gradients with Adam-found cells. Further hyperparameter tuning for final-model training may help. A small experiment comparing reward functions showed that the BLEU-based function led to higher dev and test BLEU scores. Attention mechanisms based on perplexity performed poorly, while those based on BLEU scores showed better results. Top-performing attention similarity functions were identified as reduce(sigmoid(relu(tanh(W (k q))))) and reduce(sigmoid(relu(W (k q)))). The equations in the curr_chunk show a pattern of interaction between keys and queries followed by linear transformation and nonlinearity. The search space could be improved by predicting when a program ends and when to perform reduce operations. Two attention searches with different reward functions (BLEU and perplexity) are compared in Figure 6. In the context of attention searches, two reward functions are compared: one based on BLEU and the other on perplexity. The model details involve embedding queries and context with pretrained GLoVE BID12 embeddings, encoding them independently, and combining them with context-to-query attention. The output from each position in the context is the result of attending over the query with that position's encoding. The attention mechanism involves searching over the equation form to compute similarity, followed by encoding through a stack of three model encoders. The span probabilities are calculated for start and end positions, with the highest scoring span selected. Results show that NASAttention in NMT improves over dot-product attention, achieving an F1 score of 80.5. Direct attention search on SQuAD further boosts performance to 80.1 F1. The best attention function for context-to-query is identified through these evaluations. In this paper, the authors introduce a novel attention function for context-to-query attention, achieving an F1 score of 80.1. They extend neural architecture search (NAS) to language tasks, such as machine translation and reading comprehension, outperforming previous methods. Their newly-found recurrent cells show better performance in translation tasks, and they propose a stack-based search space for more flexibility in attention function discovery. The authors introduce a new attention function for context-to-query attention, achieving high performance. They extend neural architecture search to language tasks, showing better results in translation and reading comprehension. Their experiments pave the way for future research in NAS for languages."
}