{
    "title": "B1gOe6NKPB",
    "content": "Knowledge graphs have gained popularity in capturing structured domain knowledge over the past decade. Relational learning models, such as Translating embedding models like TransE, enable the prediction of missing links within knowledge graphs. However, these models are inefficient in capturing symmetric relations and reflexive patterns. To address these limitations, the Multiple Distance Embedding model (MDE) is proposed, along with a framework for collaborative combinations of latent distance-based terms. The Multiple Distance Embedding (MDE) model is based on using limit-based loss and learning independent embedding vectors for each term to predict using contradicting distance terms. It allows modeling relations with (anti)symmetry, inversion, and composition patterns in a neural network model for mapping non-linear relations between embedding vectors and the expected output of the score function. The MDE model outperforms state-of-the-art embedding models on benchmark datasets by modeling entities and relations in multi-relational Knowledge Graphs. Predicting missing links in KGs is a key challenge for Statistical Relational Learning approaches. Distance-based KG embeddings are popular among SRL models. Distance-based KG embeddings are popular among SRL models due to their simplicity, low number of parameters, and efficiency on large datasets. These embeddings have been integrated with logical rule embeddings, used to encode temporal information, and applied to find equivalent entities in multi-language datasets. However, the TransE model was found to be inefficient in learning symmetric relations in KGs, leading to the development of various variations to address this limitation. In response to the inefficiency of TransE in learning symmetric relations in KGs, various models like TransH, TransR, TransD, and STransE were developed. However, these models faced challenges in handling inversion and composition relation patterns efficiently. To address these limitations, a translation-based model with independent vector representations was proposed in this study, allowing for the learning of all three relation patterns and optimizing the limit-based loss function. The study proposes a translation-based model with independent vector representations to address limitations in handling inversion and composition relation patterns efficiently. The model introduces an updating limit loss function to complement the current limit-based loss function, improving generalization power in link prediction tasks. It is structured as a neural network, named multipledistance embeddings (MDE), and outperforms state-of-the-art results in link prediction benchmarks by modeling relations between entities through geometric distance of vectors. A knowledge graph KG is a subset of true facts represented by triples. Embeddings map entities and relations to latent representations like vectors or matrices. A relational learning model consists of embedding and prediction functions to determine if a triple is true. Relations can be symmetric or antisymmetric, showing different patterns in the data. Tensor Factorization and Multiplicative Models define the score of triples via pairwise multiplication of embeddings. DistMult simply multiplies the embedding vectors of a triple element by element, but it cannot model anti-symmetric relations. ComplEx solves this issue by using complex-valued embeddings instead of real-valued ones. In ComplEx, complex-valued embeddings are used to calculate the score of a triple as Re(h diag(r)t), addressing the limitation of DistMult in encoding composition rules. RESCAL represents relations with matrices and uses outer products of entity vectors to calculate scores. HolE is a simplified version of RESCAL that performs circular correlation of entity vectors. Canonical Polyadic (CP) decomposition represents entities with two vectors and relations with a single embedding vector. MDE is based on independent vector embeddings. SimplE, similar to CP, uses independent vectors for training and avoids the symmetric issue of DistMult by combining two sets of entity vectors. It allows learning of symmetry, anti-symmetry, and inversion patterns but struggles with encoding composition rules efficiently. Latent Distance Approaches use distance between embedding vectors for scoring in relation modeling. Various models like SE, TransE, TransH, TransR, and RotatE have been developed to handle uni-relational and multi-relational data efficiently. TransH and TransR introduce relation-specific spaces, while RotatE combines translation and rotation for distance calculation. Neural Network Methods like ER-MLP, NTN, and SME are used for relation modeling in knowledge graphs. They involve training neural networks to learn interactions between head, tail, and relation vectors. NTN uses a neural tensor network that concatenates head and tail vectors and defines relation embeddings. SME combines relation with head and tail vectors to define a score function based on the dot product of these combinations. The linear SME defines a score function using the dot product of two functions in the hidden layer, while the MDE score function involves multiple terms. Inverse relation learning is discussed as a strong indicator in knowledge graphs, where learning the inverse of relations can be beneficial even if the actual inverse relation does not exist in the knowledge graph. Inverse relation learning is crucial in knowledge graphs. A score function is defined for symmetric relations learning, with a focus on modeling symmetry patterns efficiently. Previous studies have highlighted limitations in learning reflexive relations with certain models. The study focuses on learning reflexive relations with translation-based models like TransE, FTransE, STransE, TransH, and TransR. A new score function S4 is introduced to address limitations in symmetry and transitivity. The model incorporates different views of relations between entities using limit-based loss and aggregated terms in the score. The study introduces a new score function, S4, to improve symmetry and transitivity in learning reflexive relations with translation-based models. The model utilizes aggregated terms in the score to incorporate various views of relations between entities. The study introduces a new score function, S4, for reflexive relations in translation-based models. It incorporates multiple views of relations using weighted sum of score functions. Limit-based loss is used to ensure correct triplets have lower scores than incorrect ones. The study introduces a new score function, S4, for reflexive relations in translation-based models. It incorporates multiple views of relations using weighted sum of score functions. Sun et al. (2018) extends the limit-based loss to ensure correct triplets have lower scores than incorrect ones. The loss function minimizes the score for positive samples below a fixed limit and for negative samples above a fixed limit. The optimal limits for the loss are suggested to be updated during training. The introduction of a new score function, S4, for reflexive relations in translation-based models is discussed. The study extends the limit-based loss to ensure correct triplets have lower scores than incorrect ones. The model MDE allows learning of different relation patterns and combines terms efficiently. Unlike SimplE, MDE does not directly relate relation vectors, allowing for the independent combination of opposite terms. Previous studies have looked at symmetric relations and training over the inverse of relations, but MDE combines these benefits in one model. The MDE model introduces a novel way to combine different vector-based views of a knowledge graph. By incorporating a neural network layer, MDE N N reduces hyperparameters and allows for non-linear mappings through a logistic sigmoid activation function. This extension offers advantages over current distance-based models. The logistic sigmoid activation function in models like TransE and MDE allows for non-linear mappings between embedding vectors and expected outputs. TransE is popular for its scalability on large datasets with minimal time complexity. Experimental datasets include WN18 and FB15k extracted from Wordnet and Freebase. The study evaluated link prediction performance using various models like TransR, NTN, and ER-MLP. Evaluation metrics included Hit@N, mean rank (MR), and mean reciprocal rank (MRR). The implementation was done in PyTorch with one negative example generated per positive example for all datasets. Adadelta was used as the optimizer. The study used Adadelta as the optimizer and fine-tuned hyperparameters on the validation dataset. Hyperparameter ranges included embedding dimensions of 25, 50, 100, 200, batch sizes of 100, 150, and iterations of 50, 100, 1000, 1500, 2500, 3600. Best embedding sizes and values for different datasets were determined through grid search. The study fine-tuned hyperparameters using Adadelta as the optimizer. Best values for different datasets were determined through grid search. The experiments showed significant improvements in ranking performance, with MDE reaching 99% performance in 100 iterations and MDE NN reaching its best performance in just 50 iterations. Table 2 displays the results on FB15k-237 and WN18RR, showing a more significant improvement. Hard limits in the loss function resulted in mean rank improvements for both MDE and MDE NN. The study fine-tuned hyperparameters using Adadelta as the optimizer and showed significant improvements in ranking performance for MDE and MDE NN. MDE outperformed other state-of-the-art models, challenging RotatE with its use of smaller vector sizes and fewer negative samples. The application of sigmoid in MDE NN improved its performance significantly, especially in more challenging tests like WN18RR and FB15k-237. Notably, the Hit@10 result on FB15k-237 increased from 0.484 to 0.999. The MDE N N model outperforms current embedding models in MR and Hit@10 measures, especially on WN18RR and FB15k-237 benchmarks. Ablation experiments were conducted to understand the role of each term in the score function of MDE, showing that S 4 performs the best. The study conducted ablation experiments to analyze the impact of different terms in the score function of MDE. Results showed that S 4 performed the best, while S 2 performed the worst due to an antisymmetric pattern in the test datasets. Removing S 4 had the most negative effect on MDE performance, while removing S 1 had the least effect. Overall, the study demonstrated how MDE overcomes expressiveness restrictions of distance-based embedding models. The study demonstrated how MDE, a KG embedding approach, overcomes limitations of older models by framing it into a Neural Network structure. By utilizing multiple views for translation embeddings and independent vectors, MDE outperforms existing state-of-the-art models for link prediction. Experimental results confirm the competitive performance of MDE, particularly MDE NN, achieving state-of-the-art results on benchmark datasets. The text chunk discusses encoding relations with different patterns using entity representations and relations in a Knowledge Graph. It covers antisymmetric, symmetric, inversion, and composition patterns for encoding relations. The text chunk discusses encoding relations with composition patterns using entity representations and relations in a Knowledge Graph. It also proves Lemmas 2 and 3, showing boundaries for learning facts correctly. The text chunk discusses integrating new distance functions into MDE by setting boundaries for positive and negative samples. Lemmas 2 and 3 prove how to learn facts correctly in a Knowledge Graph. The text chunk introduces a method to update limits in the limit-based loss function during training iterations to optimize margin ranking loss with distance-based embeddings. The moving-limit loss, denoted as loss guide, gradually adjusts limits towards optimal values for positive samples. Limits are tested on the validation set every 50 epochs to determine the best values. The process of searching for optimal limits is detailed in Algorithm 1. The text introduces a method to update limits in the limit-based loss function during training iterations to optimize margin ranking loss with distance-based embeddings. The approach is based on adaptive learning rate, where the Adadelta optimizer adjusts the limits without stopping the training iterations. In experiments, the variables in Algorithm 1 are threshold = 0.05 and \u03be = 0.1."
}