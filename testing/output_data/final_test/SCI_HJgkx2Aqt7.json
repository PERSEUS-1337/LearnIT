{
    "title": "HJgkx2Aqt7",
    "content": "Simulation is a valuable tool for training machine learning models when annotated data is expensive or difficult to obtain. A reinforcement learning-based method is proposed to adjust simulator parameters automatically, optimizing the synthesized data distribution to improve model accuracy. This approach differs from previous methods by fully controlling the simulator to maximize accuracy, rather than mimicking real data distribution or generating random data. The method quickly converges to optimal parameters in experiments and successfully identifies good parameter sets for image rendering simulators in computer vision applications. This paper proposes a method to automatically adjust simulation parameters for training deep neural networks, optimizing data distribution for improved model accuracy. Unlike previous approaches, this method fully controls the simulator to maximize accuracy rather than mimicking real data distribution or generating random data. The approach quickly converges to optimal parameters and successfully identifies good parameter sets for image rendering simulators in computer vision applications. The paper proposes a method to adjust simulation parameters for training deep neural networks, optimizing data distribution for improved model accuracy. This approach departs from traditional methods by eliminating the need for human expertise in creating diverse training datasets and questioning the necessity of mimicking real data distribution. It aims to find the best simulation parameters for training models efficiently and effectively. The paper introduces a method to optimize simulation parameters for training deep neural networks without relying on human expertise or mimicking real data distribution. It aims to automatically learn simulator parameters to minimize the loss of a machine learning model over a validation dataset. The paper presents a bi-level optimization problem where the upper-level problem acts as a meta-learner to adjust data generation parameters, while the lower-level problem is the main task model. An approximate algorithm based on policy gradients is described to optimize the objective. The paper introduces an interface between the model's output \u03c8 and the simulator input for interacting with a black-box simulator. Section 4 analyzes different variants of the approach in experiments on toy data and real computer vision problems, exploring questions like training models with targeted data and choosing simulation parameters for optimal model training. Results show the approach quickly identifies good scene parameters \u03c8 that outperform validation set parameters on object counting and semantic segmentation tasks. The goal is to adjust \u03c8 such that the model trained on simulated data minimizes risk on real data. The paper introduces an interface between the model's output \u03c8 and the simulator input for interacting with a black-box simulator. It aims to adjust \u03c8 so that the model trained on simulated data minimizes risk on real data. The overall objective function is formulated as a bi-level optimization problem, with constraints on the lower-level problem like smoothness and invertible Hessian. Despite fulfilling these constraints, the objective remains non-differentiable due to sampling from a parameterized distribution. The paper introduces policy gradients BID24 to optimize \u03c8 for generating a synthetic dataset to maximize the main task model's accuracy on the test set. A policy \u03c0 \u03c9 is defined to sample parameters \u03c8 for the simulator, a generative model G(x, y| \u03c8) that generates data samples (x, y) conditioned on \u03c8. The paper introduces a policy \u03c0 \u03c9 to optimize parameters \u03c8 for a generative model G(x, y| \u03c8) that generates data samples. The policy receives a reward based on the accuracy of the main task model on the validation set, aiming to maximize the objective DISPLAYFORM0 with respect to \u03c9. The reward R is computed as the negative loss L or accuracy metric on the validation set. Gradients for updating \u03c9 are obtained following the REINFORCE rule. An unbiased estimate of the quantity is calculated, with the advantage estimate and baseline included. Different variants of the algorithm for learning to simulate data are introduced by adjusting training epochs of the MTM in each policy iteration. In each policy iteration, the size of the data set and the training duration of the main task models (MTM) are adjusted. The approach involves training or fine-tuning MTMs for a certain number of epochs on provided data, obtaining rewards based on accuracy, and computing advantage estimates. The impact of retaining MTM parameters or estimating them from scratch is explored in experiments. Our approach involves training MTMs on a validation set to compute advantage estimates and updating policy parameters. A black-box simulator is defined as a distribution over data samples parameterized by \u03c8, composed of a rendering process and sampling steps. \u03c8 defines the parameters of distributions in a Bayesian network for efficient sampling. The black-box simulator is a directed acyclic graph where parameters \u03c8 describe internal probability distributions. These parameters can be modeled as unconstrained continuous vectors to represent various distributions. The policy is modeled as a multivariate Gaussian with mean optimized and variance set to 0.05. The proposed approach involves optimizing for the mean and setting the variance to 0.05 in all cases. It acts as a meta-learner that modifies the training data to improve accuracy on a validation set by generating new, randomly sampled data in each iteration. This differs from previous works by creating new data instead of selecting subsets, allowing for the generation of unusual situations not present in the original training data. In this work, policy gradients are favored for their sample efficiency and success in prior art, unlike evolutionary algorithms or sampling-based methods. The approach involves training a policy to generate a program that creates a copy of an input image, using policy gradients to train the policy. The goal is to learn parameters of a simulator that maximize performance of a main task, similar to tuning parameters of a simulator to match the distribution of observed data in other works. In contrast to prior works, this study focuses on learning simulator parameters to improve main task model performance without matching observed data distribution. Addressing the domain gap between real and synthetic data is crucial in training machine learning models. The experimental evaluation aims to showcase the approach in a controlled toy experiment, analyze algorithm properties on a computer vision task, and demonstrate ideas on real data for semantic image segmentation. The study focuses on learning simulator parameters to enhance main task model performance without aligning with observed data distribution. It demonstrates the approach in a controlled toy experiment and analyzes algorithm properties on a computer vision task. The practical application is shown on real data for semantic image segmentation using a non-linear SVM with RBF-kernels. The study demonstrates adjusting simulator parameters to improve model performance without aligning with observed data distribution. It shows the approach in a toy experiment and analyzes algorithm properties on a computer vision task. The practical application is on real data for semantic image segmentation using a non-linear SVM with RBF-kernels. The policy gradually adjusts the data generating distribution to maximize accuracy on the validation set, leading to well-separated test data. Samples from the simulator are clearly different from the test data. The simulator's sampled data is different from the test data due to fewer components per class in the GMM. Decision boundaries are still learned well. Computer vision applications require a generative scene model and image rendering engine, focusing on traffic scenes using simulators like CARLA BID8 with Unreal engine. Extensions were needed to generate different scenes, not just different viewpoints of a static map. Other simulators lack parameterized scenes that can be changed on the fly. Our traffic scene model S(\u03c1 | \u03c8) allows sampling instances of scenes \u03c1 based on parameters \u03c8. The rendering model P (\u03c6 | \u03c8) displays rewards on the unseen test set. Despite using adversarial initialization, our approach converges to the same reward R, albeit at a slower rate. The model can adjust variables like car probability and weather conditions. The traffic scene model allows sampling scenes based on parameters. A convolutional neural network is trained to count cars in rendered images. The evaluation metric is the 1 distance between predicted and ground truth count. The reward is the negative 1 loss. The proposed policy is evaluated for different initializations, including a deliberately suboptimal one. The proposed policy is evaluated for different initializations, including a deliberately suboptimal one. The policy quickly reaches high rewards on the validation set and also performs well on the unseen test set. The model shows stable behavior with different random initializations. The difference between training the MTM from scratch or fine-tuning its parameters is explored. The experiment introduces the \"accumulated main task model (AMTM)\" which accumulates information over policy iterations. By comparing rewards, it is observed that the AMTM outperforms training MTM from scratch. The study evaluates the AMTM reward for evaluation purposes and compares it with two baselines. Training an accumulated main task network using a learning policy at each step is shown to be superior compared to training with a dataset generated using final or random parameters. Learning-to-simulate can converge even with an adversarial parameter initialization. The accumulated main task model's reward is evaluated with different training epochs, showing close performance to the ground truth. Our approach, \"LTS,\" outperforms the random parameter baseline and even the \"final policy params\" baseline, attributed to increased data distribution variety. It converges to a high reward even with an adversarial initialization. The impact of the number of epochs on training the main task model in learning to simulate is analyzed, showing robustness to lower training epochs for potential wall-time speed up. For the next set of experiments, semantic segmentation is used as the test bed in our modified CARLA simulator. The focus is on the segmentation accuracy of cars, measured as intersection-over-union (IoU), with the policy adjusting scene and rendering parameters to maximize car IoU. The study focuses on semantic segmentation accuracy in a modified CARLA simulator, optimizing car IoU. A CNN model predicts per-pixel classification from RGB images. Validation set parameters are generated for crowded traffic scenes. Results show the learned policy outperforms the validation set parameters, indicating they may not always be optimal for training segmentation models. The learning-to-simulate approach has a practical impact on semantic segmentation. The learning-to-simulate approach has a practical impact on semantic segmentation by training a main task model with a reward signal from real data. Results show improved segmentation accuracy on the KITTI test set using synthetic data generated by random parameters or learned parameters. The study focuses on training a network with synthetic data to improve semantic segmentation accuracy on the KITTI test set. They use a CNN with a ResNet-50 backbone to predict per-pixel classification output. The impact of targeted generation of simulated data on real data is investigated, with a focus on segmentation of cars by measuring IoU. The study trained a network with synthetic data to enhance semantic segmentation accuracy on the KITTI test set. They used a CNN with a ResNet-50 backbone to predict per-pixel classification output. The validation Car IoU metric was monitored for 600 separate models generated by the simulator with random parameters. The model with the highest validation reward was selected and tested on the KITTI test set, confirming superior results. The approach of learning-to-simulate showed promising results on the unseen real KITTI test set. The study utilized synthetic data to train a network for semantic segmentation on the KITTI test set. They found that parameter optimization using policy gradients outperformed random search. Learning to simulate adjusts simulator parameters to generate synthetic data for high accuracy on validation and test sets. This approach offers a practical solution for leveraging synthetic data in deep learning models. The model demonstrates learning to simulate on synthetic and real data, with plans to expand label space and explore dynamic memory for improvement. The scene model includes elements like roads, intersections, cars, houses, and weather, all tied to parameters for generation. The model simulates roads with sidewalks, generating buildings and cars. Car presence is determined by a probability parameter, and the type of car spawned is selected from 5 options. House presence is also determined by a probability parameter. The length to intersection is sampled from a distribution with 10 parameters, and weather is randomly sampled from a distribution with 4 parameters. The text discusses the use of a Categorical distribution with 4 parameters to determine weather identities. It presents classification accuracy results for a toy problem using different numbers of gaussians. Learning to simulate yields better results than using a dataset with test set parameters. The section visualizes the learning of parameters in a car counting problem, showing the evolution of weather and car types over time. The parameter M controls the dataset size generated in each policy iteration. The text discusses using a Categorical distribution with 4 parameters for weather identities. It compares classification accuracy results for a toy problem with different numbers of gaussians. The parameter M controls dataset size generated per policy iteration, with a dataset size of 20 being sufficient for model convergence. The text discusses training the MTM h \u03b8 using stochastic methods and verifying convergence in the car counting task with different random seeds. Figures 9a and 9b show that the reward and accumulated main task network test reward converge with different random seeds."
}