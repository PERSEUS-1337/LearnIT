{
    "title": "B1gJ1L2aW",
    "content": "Deep Neural Networks (DNNs) are vulnerable to adversarial examples, which can mislead them. To understand these attacks, we use Local Intrinsic Dimensionality (LID) to characterize the properties of adversarial regions. LID assesses the space-filling capability of these regions based on distance distribution. By analyzing LID characteristics, we can distinguish adversarial examples generated by different attacks. Deep Neural Networks (DNNs) are vulnerable to adversarial examples, which can mislead them. Local Intrinsic Dimensionality (LID) is used to characterize adversarial regions and distinguish attacks. LID shows potential in outperforming state-of-the-art detection measures for various attack strategies on benchmark datasets. This analysis motivates new directions for adversarial defense and challenges for developing better attacks to understand DNN vulnerabilities. Adversarial attacks on Deep Neural Networks (DNNs) can lead to incorrect predictions with high confidence, posing security concerns in real-world applications like self-driving cars and identity recognition. This paper aims to understand adversarial attacks by characterizing the regions where adversarial examples exist, known as 'adversarial regions' or 'adversarial subspaces'. These regions subvert the classifier in a similar way and can be defined in both input and activation spaces of DNN layers. Adversarial regions in Deep Neural Networks (DNNs) are areas where adversarial examples exist, subverting the classifier. Different works have attempted to characterize these regions, with debates on whether they are low probability regions scattered in high dimensional space or span a contiguous multidimensional space. No definitive method exists to reliably discriminate adversarial regions from normal data. Adversarial subspaces, as discussed by Tanay & Griffin (2016) and Tram\u00e8r et al. (2017), are low probability regions close to the data submanifold with boundaries near legitimate data points. These subspaces have different class distributions from their closest data submanifold and can be detected using Kernel Density (KD) estimation. In this paper, it is shown that kernel density is not effective for detecting certain types of attacks. Other density-based measures like the number of nearest neighbors and mean distance to the k nearest neighbors also have limitations in characterizing local adversarial regions. Expansion-based measures are considered as an alternative to density measures. Expansion-based measures, such as Local Intrinsic Dimensionality (LID), offer a more effective method for characterizing adversarial examples compared to density measures. LID generalizes the concept of intrinsic dimensionality by assessing the local dimensional structure of data sets, revealing the dimensionality of the local data submanifold near a reference point through the growth characteristics of the cumulative distribution function. In this study, LID is used to characterize the intrinsic dimensionality of adversarial regions and distinguish adversarial examples. The estimated LID efficiently captures the properties of adversarial regions, showing higher values for adversarial examples compared to normal data samples. The paper focuses on studying the LID properties of adversarial examples generated using advanced attack methods. The contributions include proposing LID for characterizing adversarial regions of deep networks and discussing how adversarial perturbations impact LID characteristics. The study focuses on how adversarial perturbations affect the Local Intrinsic Dimensionality (LID) characteristics of adversarial regions. It empirically shows that the LID of adversarial examples is significantly higher than normal data examples, especially in deeper layers of Deep Neural Networks (DNNs). The LID characteristics of adversarial examples generated using five attack methods can be easily distinguished from normal examples, and a baseline classifier using LID estimates outperforms existing detection measures on various attacks and datasets. The study demonstrates the effectiveness of Local Intrinsic Dimensionality (LID) measurement in detecting adversarial attacks on Deep Neural Networks (DNNs). Various attack methods generate adversarial examples with higher LID compared to normal data. Different attacks share similar LID properties, making it possible to detect complex attacks using simple ones. The Fast Gradient Method (FGM) and Basic Iterative Method (BIM) are mentioned as common adversarial crafting approaches. The BIM variant stops once misclassification is achieved, while JSMA iteratively perturbs pixels based on saliency maps. The Opt attack is considered the most effective, and defense techniques like adversarial training and distillation have limitations against it. Recent focus has shifted towards detecting adversarial attacks due to the challenges in defense. Recent works have shifted focus to detecting adversarial examples rather than defending against them. Various detection methods have been developed, but a recent study has shown that these methods can also be vulnerable to attacks. Classical expansion models measure the growth rate of data objects in the theory of intrinsic dimensionality. Expansion dimension models like BID16 and BID15 measure the growth rate of data objects as distance increases from a reference sample. This concept can be applied to statistical settings with the formal definition of LID BID13, which focuses on the local intrinsic dimensionality of data samples. The LID of a data sample x is defined by the cumulative distribution function F (r) of the distance from x to other samples. It describes the rate at which F (r) increases as distance r increases, and can be estimated using the distances to k nearest neighbors. The local intrinsic dimension at x is the limit as radius r tends to zero. The LID model estimates the dimension of a submanifold containing data points, using extreme value theory to approximate the underlying distance distribution. The LID model uses the Maximum Likelihood Estimator to approximate the distance distribution of data points. The estimator calculates the distance between a point x and its nearest neighbors within a sample drawn from the data distribution. The aim is to better understand the theoretical quantity of LID. The LID model uses Equation (4) to calculate estimates for adversarial regions. Motivation is provided regarding how perturbations affect LID characteristics. A detector can be designed using LID estimates to distinguish between normal and adversarial examples. Adversarial perturbations result in small coordinate differences, with the LID value representing the dimension of the data submanifold. Theoretical LID values associated with x are determined by the dimension of the adversarial subspace. Perturbation schemes exploit the full degrees of freedom in high-dimensional data, where x likely lies outside the data submanifold. The representational dimension in high-dimensional data is typically larger than the intrinsic dimension, leading to a greater theoretical LID for x. In practice, the values of LID must be estimated from local data samples using an appropriate estimator applied to a k-nearest neighborhood of the test samples. The estimation of the LID of x is reasonably accurate when the dimension of S is low. The neighborhood of x in the adversarial subspace is likely to contain neighbors from more than one manifold, but if mostly composed of samples from S, x would not likely be an adversarial example. When X is large, estimating the Local Intrinsic Dimensionality (LID) of an adversarial example x can be done by computing neighborhoods within a randomly-selected sample of the dataset. This approach reduces computational costs while still revealing the high intrinsic dimensionality of the data. The LID estimation model considers distances from x to dataset members as determined by independently-drawn samples, allowing it to be applied to any random minibatch. The estimator can be applied to distances induced by any random minibatch, ensuring k-nearest neighbor sets remain near x. As minibatch size decreases, variance of estimates increases. Even with small minibatch and neighborhood sizes, differences in LID values can reveal adversarial examples. Discrimination is possible with minibatch sizes as small as 100 and neighborhood sizes as small as 20. The LID estimator can detect adversarial examples with small minibatch and neighborhood sizes, as small as 100 and 20 respectively. LID estimates can be used as features to train a baseline classifier to distinguish adversarial examples, requiring training sets with adversarial, normal, and noisy examples. The methodology replicates BID7 and BID2, emphasizing the importance of robustness to random input noise. The LID estimator can detect adversarial examples with small minibatch and neighborhood sizes. Algorithm 1 outlines how LID features are extracted for training a classifier. Adversarial and noisy examples are generated from normal examples in each minibatch. The LID estimator can detect adversarial examples with small minibatch and neighborhood sizes. Adversarial and noisy examples are generated from normal examples in each minibatch. Adversarial examples are generated using an adversarial attack on normal examples, while noisy examples are generated by adding random noise to normal examples. The LID associated with each example is estimated from its k nearest neighbors in the normal minibatch. An LID estimate is calculated for each example and each transformation layer in the DNN. In the DNN, LID estimates are calculated using activation values of neurons in each layer. All transformation layers are used, including conv2d, max-pooling, dropout, ReLU, and softmax. LID estimates are used as feature values for training a classifier like logistic regression. The classifier can then classify test examples as adversarial or non-adversarial based on their LID-based feature values. The discrimination power of LID-based characterization is evaluated against various adversarial attack strategies. The study compares LID detector with KD and BU detection measures using various attack strategies (FGM, BIM-a, BIM-b, JSMA, Opt) on MNIST, CIFAR-10, and SVHN datasets. DNN classifiers are pretrained on designated training sets and tested on corresponding test sets. Non-correctly classified test images are discarded before analysis. The study compared LID detector with KD and BU detection measures on MNIST, CIFAR-10, and SVHN datasets. Pre-test images were classified, then subdivided into train and test sets. Detectors were trained separately and evaluated on normal, noisy, and adversarial images from the test set. No images from the test set were used during training to prevent cross-contamination. The study compared LID detector with KD and BU detection measures on MNIST, CIFAR-10, and SVHN datasets. Adversarial examples were generated using five selected attacks. Noisy examples were crafted using different methods for each attack. Logistic regression classifier was used as a detector with AUC score as the performance metric. A 5-layer ConvNet achieved 99.29% accuracy on MNIST pre-test images. The study compared LID detector with KD and BU detection measures on MNIST, CIFAR-10, and SVHN datasets. A 12-layer ConvNet achieved 84.56% accuracy on CIFAR-10 pre-test images, while a 6-layer ConvNet achieved 92.18% accuracy on SVHN pre-test images. Parameter tuning was done for KD and LID using nested cross validation within the training set. The study compared LID detector with KD and BU detection measures on MNIST, CIFAR-10, and SVHN datasets. Parameter settings were optimized for AUC, with specific bandwidths and k values chosen. Implementation details for various attack strategies were specified, and code availability was provided at a GitHub link. The LID characteristics of adversarial examples generated by Opt are shown in empirical results. Adversarial examples have higher LID scores at the softmax layer compared to normal or noisy examples, indicating higher intrinsic dimensionality. The transition from normal to adversarial examples increases complexity in the local data submanifold, leading to higher LID values. The LID characteristics of adversarial examples are more distinguishable at deeper layers of the network, particularly at the dense and softmax layers. Adversarial perturbations affect fully-connected and softmax transformations more than convolutional ones. LID shows more stability in performance compared to KD with parameter variation. Plots of LID scores for MNIST and SVHN datasets are available in Appendix A.2. LID outperforms KD and BU measures significantly on all attack strategies and datasets tested, achieving high AUC scores for the Opt attack strategy. The study found that LID outperformed KD and BU measures in detecting various attack strategies on different datasets, with high AUC scores for the Opt attack strategy. Additionally, the generalizability of KD, BU, and LID detectors was tested on previously unseen attack strategies on the CIFAR-10 dataset. The study compared the performance of LID, KD, and BU detectors in detecting different attack strategies on CIFAR-10 dataset. Results show that LID outperformed KD and BU, with high AUC scores for Opt attack. LID trained on FGM accurately detected complex attacks, while BU detector struggled with BIM-b examples. The study found that BIM-b adversarial examples have lower Bayesian uncertainties compared to other attack strategies, with only 4% exhibiting higher uncertainties. Detectors trained on FGM attack strategy showed potential in identifying other attacks. Future work could explore detection generalizability across all attack strategies. The study explores the impact of larger minibatch sizes on LID estimation for improving detection performance. Experimental analysis suggests that increasing the batch size can enhance detection performance without significantly increasing computational cost. The study investigates the tradeoffs among minibatch size, LID estimation accuracy, and detection performance. An adaptive attack against LID measurement is applied to evaluate the robustness of the detector. Two scenarios are tested for detection using LID features and scores at the pre-softmax layer. The Opt attack allows for a fair comparison with previous methods. The optimal constant \u03b1 is determined via an internal binary search for \u03b1 \u2208 [10 \u22123 , 10 6 ]. Adversarial examples have higher LID characteristics than normal examples, as demonstrated in Section 5.2. The adaptive attack in Scenario 2 fails to find valid adversarial examples 100%, 95.7%, and 97.2% of the time on MNIST, CIFAR-10, and SVHN respectively. Integrating Local Intrinsic Dimensionality (LID) into the adversarial objective does not hinder detection, as shown by the 100% detection rate in Scenario 1. This contrasts with BID2's findings on kernel density. The study focuses on understanding adversarial regions and using LID as a feature for detection, with promising results. The study highlights the effectiveness of Local Intrinsic Dimensionality (LID) in characterizing adversarial examples, showing its potential for state-of-the-art discrimination performance. LID-based detectors transform input into a space where LID values are altered, offering insights for new techniques in adversarial attack and defense. Future research on LID characteristics should consider the impact of DNN transformations and learning processes. Modeling the dimensional characteristics of DNNs and investigating the effect of LID estimation quality on adversarial detection performance are key areas for exploration. Improvements in estimator quality and sampling strategies could enhance practical benefits. Additionally, the LID scores of Opt attack strategy on MNIST and SVHN datasets are illustrated in FIG3. The LID scores of adversarial examples are higher than normal or noisy examples on MNIST and SVHN datasets. Detection AUC of LID characteristics is shown with different minibatch sizes and neighborhood sizes. Results are for detecting Opt attacks on MNIST, CIFAR-10, and SVHN datasets."
}