{
    "title": "BylE1205Fm",
    "content": "We study unsupervised learning to map between domains $A$ and $B$, where samples in $B$ contain all information in samples from $A$ and additional information. Missing information in mapping from $A$ to $B$ is replicated from an independent reference sample in $B$. Our solution uses a single two-pathway encoder and decoder for both domains, with separate and common parts encoded as vectors. The architecture ensures disentanglement between domains with minimal loss terms and a domain confusion term. Results show success in visual domain translation tasks like no-glasses to glasses and adding facial hair. The algorithm learns to map between samples in two unsupervised domains $A$ and $B. The paper discusses unsupervised domain mapping between two sets, A and B, using a guided image translation method. The method involves a shared latent space and a specific part for added content in B, resulting in disentanglement. Compared to other methods, this approach is simpler and more effective for image translation tasks. The comparison of unsupervised guided image translation methods shows that the proposed method is simpler and more effective due to the emergence of disentanglement. This allows for training with fewer parameters and without excessive tuning, making it a more efficient approach. The MUNIT architecture employs a shared latent space with domain-specific latent space, while EG-UNIT uses an adaptive method of masking features. DRIT focuses on guiding for style rather than content. The recent DRIT work by BID27 focuses on mapping between domains using a disentangled representation, emphasizing style over content. Their solution differs from ours in several ways: they use two-way mapping, shared weights for a common representation, and a VAE-like statistical characterization of the latent space. This approach is more complex than ours, employing separate encoders for style and content vectors with different representations. The method discussed focuses on spatial pooling and creating a smaller representation to ensure that different aspects of the image are encoded. Unlike other methods like DRIT or MUNIT, this method does not separate style and content, and aims to capture additional content. The PairedCycleGAN work is similar in goal but not in method, as it applies makeup from a reference face to a source face image. However, direct comparison is not possible due to limited dataset availability and code sharing. The method discussed involves using a generator with two image inputs to transfer makeup between input images and a second generator to remove makeup. It operates on pre-segmented facial regions and does not use an encoder-decoder architecture. Other methods in the field include guided methods that produce multiple outputs based on a reference image in the target domain. Work InfoGAN BID9 learns a representation where specific classes are encoded as a one-hot encoding of part of the latent vector. BID16 disentangles the representation by reducing class-based information. BID6, an extension of BID16, performs guided image to image translation but requires class-based information, unlike our approach. The algorithm deals with two domains A and B, each with their respective distributions and datasets. The generative model assumes a sample a and a specification c from an unknown domain C. An invertible function u(b) maps a sample b to its content and specification. The goal is to learn a target function y that transforms samples a and b to the analog of a in domain B with the specification of b. For example, transforming an image of a person to an image of that person with sunglasses. The target function is extended to inputs b1 and b2, mapped to a third b with the content of b1 and the specification of b2. An assumption is made on the distributions DA and DB. The framework assumes distributions of images with and without sunglasses are the same, except for the sunglasses. Generalization risk between two functions f1 and f2 is defined for a distribution D over X. The goal is to return a hypothesis h. The algorithm aims to learn a mapping y(a, b) without paired examples by using an encoder-decoder architecture. The encoder f and decoder g work together to approximate y(a, b) as g \u2022 f (a, b) \u2248 y(a, b), where f is decomposable into two parts. The algorithm utilizes encoders F and a decoder g to learn a mapping y(a, b) without paired examples. The encoder f is decomposable into two parts, e1 and e2, which encode shared and specific content respectively. The decoder g is a member of a set of decoders M. To learn the functions f and g, min-max optimization is applied with training losses involving a discriminator network d. The discriminator d is trained to minimize a domain confusion term. Theoretical analysis for the success of the proposed method involves technical notations like expectation, probability operators, Shannon entropy, and total correlation. The C-discrepancy measures adversarial distance between two distributions, aiding in minimizing generalization risk. Chazelle FORMULA1 Thm. 1 upper bounds the generalization risk for autoencoders, decomposed into reconstruction error, approximation error, and discrepancy term. The loss function must be symmetric and obey the triangle inequality. The upper bound is based on terms that can be minimized during training. The discrepancy term in the context of autoencoders can be simplified by replacing it with a new discrepancy disc M (e 1 \u2022 D A , e 1 \u2022 D B ) to measure the closeness between distributions of e 1 (a) and e 1 (b) for a \u223c D A and b \u223c D B. This new approach is theoretically and empirically shown to lead to a disentangled representation where e 1 (b) and e 2 (b) are independent, making the training process more stable. Lemma 1 discusses the independence between two distributions e1(b) and e2(b) in neural networks with a non-linear activation function. The results extend previous work on disentangled representations in autoencoders, focusing on conditions for minimal and disentangled mid-level representations. In Proposition 5.2, a tight upper bound is introduced on the sum T C(W b) + I(W b; b) for a linear representation. The goal is to show that for a neural network h = c\u2022f, both T C(f (b)) and I(f (b); b) are small when f is a high-level representation of the input. However, it was not possible to show that both terms are small simultaneously. In Corollary 5.3, the bound is extended to show that only the mutual information I(f (b); b) is small, assuming uncorrelated components in the mid-level representations of b in the layers of f. In Lemma 2, an upper bound for T C(f (b)) is provided, similar to previous bounds. The main difference in this case is the use of an autoencoder h = g \u2022 f, with a monotonically decreasing function q(\u03b1) for \u03b1 > 0. The total correlation of f (b) is bounded by Eq. 13, which includes three terms. The mutual information I(h(b); b) measures the dependence between input b and output h(b), while the ratio O(d1/d2) measures the dimension of the output of f compared to the previous layer. The ratio between the output dimension of f and the previous layer is crucial for regularization in the weights of the autoencoder. A tradeoff exists between regularization and mutual information I(h(b); b). Small regularization allows for better reconstruction, leading to higher mutual information. The bound on mutual information is essential for a small expected reconstruction error in the autoencoder. The lemma states that a well-separated dataset in an autoencoder with low reconstruction error maximizes mutual information. This implies that the representation layer coordinates are nearly independent. Training with a GAN using the shared representation can capture weakly dependent features. The method involves training a GAN with only the shared representation to bound generalization error. It is evaluated on facial attributes using celebA dataset. Different domains are used for experiments, adapting network architecture from previous work. Instance Normalization is used instead of Batch Normalization. The method involves training a GAN with only the shared representation to bound generalization error. Evaluated on facial attributes using celebA dataset. Network architecture adapted from previous work, using Instance Normalization instead of Batch Normalization. Normalization BID35 used, without dropout. Encoders e1 and e2 have specific layers and activations. Input images are 128x128, encoding size is 512x2x2. Results compared with MUNIT and DRIT BID27. Our method for image translation outperforms MUNIT and DRIT BID27 baselines in content addition tasks. It is more efficient in training with a single fixed hyperparameter, compared to the multiple hyperparameters of the baselines. Our method also has a lower memory footprint and shorter iteration duration, similar to the Fader network. Our method outperforms MUNIT and DRIT in content addition tasks, with a more efficient training process and lower memory footprint. A user study comparing different algorithms was not conducted, instead, the output of our method was directly compared with real images in two experiments. Users were able to detect real images over generated ones in most cases. The success ratio varies between image translation tasks and comparisons. Users were most confused in facial hair (\"beard\") category, but could tell real images from generated ones 74% of the time when replacing glasses. Mix-and-match combinations looked natural, but unwanted variation occurred in fixed attributes due to fitting content to new faces. The method of fitting content to new faces faces challenges with low quality inputs, resulting in lower quality outputs. Interpolation experiments show gradual changes in latent representation. Additional translation examples are provided in the supplementary appendix. User study results in Table 3 show confusion in facial hair category but better recognition when replacing glasses. Unwanted variations occur in fixed attributes when fitting content to new faces. Our method allows for the removal of specific content in images by decoding a representation. Comparing with the Fader network method, our removal process results in fewer residuals. We conducted an automatic classifier and user study to verify the quality of our method compared to Fader networks. The classifier trained on domain A and B shows low probability for class B before transformation. A user study with 20 participants favored our method over Fader network for glasses, facial hair, and smile removal. When converting between domains, ambiguity arises from domain-specific information. Guided translation uses a reference image to provide missing information. Previous work focused on texture-related missing information, but our work focuses on well-structured transformations that replicate all domain-specific content from the reference. Our work focuses on structured transformations that replicate domain-specific information from a reference image using a small number of networks and simple loss terms. The section introduces notations and terminology necessary for proofs, including the concept of a Markov chain and the Data Processing Inequality. In this section, useful lemmas are provided to aid in the proofs of main results. Lemma 4 discusses random vectors and invertible functions, showing the invariance of KL-divergence. The curr_chunk discusses KL-divergence invariance under continuous invertible transformations and presents Lemmas related to random variables and entropy. It also introduces an example of uncorrelated variables with a dimensionality reducing linear transformation. The curr_chunk discusses a linear transformation preserving information between independent uniform distributions. It introduces a homeomorphic transformation and states that mutual information is invariant to such transformations. Theorem 1 is presented for symmetric loss functions in autoencoders. The curr_chunk discusses the emergence of disentangled representations in autoencoders with log-normal regularization on the encoder. It introduces a non-linear activation function and the composition of encoder and decoder layers. The analysis follows the theory of BID0 to show the likelihood of learning disentangled representations. The curr_chunk discusses the composition of the encoder and decoder layers in the context of autoencoders with log-normal regularization. It presents a Gaussian dropout assumption for the posterior distribution and introduces a learned mean for the encoder layers. Lem. 9 provides an upper bound on the relationship between different variables in the model. The curr_chunk provides an upper bound on the total correlation of the k'th layer of the autoencoder h v (b), assuming Gaussian marginal distributions for y k and y k |z k\u22121. The assumption of no pair-wise linear correlations between components of z k\u22121 is made, suggesting a minimal representation."
}