{
    "title": "Bkg9ZeBB37",
    "content": "To train multi-speaker text-to-speech models that can generate clean speech, disentangled representations are crucial for controlling speaker identity and background noise independently. This is challenging due to the lack of labels for recording conditions and the correlation between speakers and recording equipment. The proposed method includes a conditional generative model, data augmentation for uncorrelated noise, and adversarial factorization to improve disentanglement. Experimental results show the effectiveness of this approach in separating speaker identities. The proposed method aims to disentangle speaker and noise attributes in training data to consistently synthesize clean speech for all speakers. Recent advancements in neural end-to-end TTS models allow control of speech attributes by conditioning synthesis on text and learned representations, enabling the training of high-quality multi-speaker TTS models using crowd-sourced speech data recorded under various acoustic conditions. Learning disentangled representations that control speaker and acoustic conditions independently is essential but challenging due to underlying acoustic conditions of an utterance. The text discusses the challenges of annotating acoustic conditions in speech data and the correlation between speaker identity and recording conditions. It introduces a conditional generative model with factorized latent variables and data augmentation to address these issues. The study utilizes the VCTK speech synthesis dataset and background noise signals from the CHiME-4 challenge to create a dataset with correlated speaker and background noise conditions. The proposed model aims to disentangle noise from speakers and generate clean speech using adversarial training. The TTS model is based on Tacotron 2, which generates mel spectrogram frames from text input. The TTS model utilizes latent variables z s and z r to control speech attributes other than text, with isotropic Gaussian prior distributions. Variational distributions q(z s | speech) and q(z r | speech) approximate the intractable posteriors of the latent variables. The model is a conditional generative model with two latent variables: p(speech | z s , z r , text). The TTS model uses latent variables z s and z r to control speech attributes other than text. An auxiliary speaker classifier is trained with the TTS model to encode discriminative speaker information in z s and residual information in z r. Decorrelating factors like background noise level from speaker identity is proposed to improve training. The proposal suggests augmenting the training set with noisy copies to flatten the SNR distribution of each speaker, making SNRs less discriminative. Adversarial training can be used to discourage encoding acoustic condition information in latent variables. If ground truth labels are unavailable, augmentation labels can be used for adversarial training. To replace acoustic condition labels in training, augmentation labels can be used as a noisy alternative. Invariance to augmentation is measured using empirical H-divergence, approximated with Proxy A-distance. This helps in making latent variables insensitive to acoustic conditions. The model illustrated in FIG0 consists of three modules: a synthesizer for speech generation, an inference network with two encoders, and an adversarial factorization module with speaker and augmentation classifiers. The parameters of the model include \u03b8, \u03c6 s , \u03c6 r , \u03c8 s , and \u03c8 a. Training aims to maximize conditional likelihood and speaker information in z s while minimizing generalization error. The model includes a synthesizer for speech generation, an inference network with encoders, and an adversarial factorization module with speaker and augmentation classifiers. The objective function combines an evidence lower bound (ELBO) with a domain adversarial training objective. The augmentation classifier is optimized differently from the rest of the model, and a gradient reversal layer is used to train the entire model jointly. The proposed augmentation-adversarial training method combines data augmentation for speech with domain adversarial neural networks (DANNs) to disentangle correlated attributes in generative models. This approach allows adversarial training without requiring domain labels and offers advantages over traditional DANNs. Our proposed method combines data augmentation with domain adversarial training to disentangle correlated attributes in generative models, addressing conflicts between domain and target attribute information. This approach alleviates issues by using H-divergence based adversarial training on latent variables. The dataset used for the study includes clean English speech from the VCTK corpus and background noise from the CHiME-4 challenge. The signals were downsampled to 16 kHz and split into training and testing sets. Speaker-correlated noise was simulated by mixing noisy speakers' utterances with noise at varying SNRs. The study involved generating an augmented set by mixing training utterances with noise signals at different SNRs. The synthesizer network used the Tacotron 2 architecture with additional input z s and z r. The speech was represented as mel-scale filterbank frames, and input text was represented as phonemes. The speaker and residual encoders followed a similar architecture. The architecture for the synthesizer, encoders, and classifiers closely follows the attribute encoder in BID9. Each encoder maps a mel spectrogram to vectors for mean and log variance. The classifiers are fully-connected networks with a hidden layer and softmax layer. The model is trained jointly with specific parameters and optimization techniques. Disentanglement is quantified by training separate classifiers for speaker and noise variables. The classification accuracy on a held-out set measures the information content of a latent variable in predicting targets. A linear discriminative analysis classifier is used for all tasks. Results comparing the full proposed model with two alternatives are shown in TAB1. Removing adversarial training and data augmentation leads to failure in disentangling speaker from noise. The proposed model successfully disentangles speaker from noise by encoding acoustic condition into zr with 96.5% accuracy on noise prediction. Adversarial training slightly degrades speaker information allocation, increasing speaker prediction accuracy using zr from 1.4% to 2.3%. Visualizing the learned representations using t-SNE BID16 shows the best allocation of acoustic information in the model. The proposed model successfully disentangles speaker from noise by encoding acoustic condition into zr with 96.5% accuracy on noise prediction. Results in Figure 2 show projected zs forming small clusters, separated by speaker and gender. Clean and noisy utterances have similar distributions in zs. The study involved ten utterances from five clean speakers and five noisy speakers, with latent variables inferred using encoders. A set of 100 phrases was synthesized based on different combinations of latent variables. Waveform amplitude distribution analysis was used to estimate SNR without a clean reference signal. Comparison was made with a baseline multi-speaker Tacotron model. The proposed model in the study can synthesize clean speech for noisy speakers by conditioning on inferred latent variables from clean utterances. Results show that the proposed model achieves the smallest discrepancy in SNR between different sets of latent variables compared to alternative models. The \"-adv\" variant shows worse disentanglement, while the \"-adv-aug\" variant fails to control noise through latent variables. The proposed model can generate clean speech for noisy speakers by using latent variables. Results show that the model can control noise using one specific latent variable, while a variant fails to do so. The model's ability to control noise is illustrated in synthesized samples, where it produces clean speech while the baseline always contains background noise. The model's effectiveness in controlling speaker identity is also examined using a speaker verification system. The study utilizes a nearest-neighbor classifier to assign speaker labels based on d-vector similarity. Synthesized samples closely mirror speaker characteristics of reference samples, regardless of conditioning variables. Speaker identity is controlled by one variable (z s) and remains consistent despite changes in another variable (z r). Fidelity is assessed through mean opinion scores (MOS) from native speakers, rating the naturalness of synthesized samples. The study compares the naturalness of synthesized speech using crowd-sourced mean opinion scores (MOS) based on conditioning variables. Results show that disentangling speaker and noise improves naturalness, with the proposed model outperforming the baseline when conditioning on clean signals. Sensitivity to speaker encoding dimensions is also studied for disentanglement performance. In the previous sections, good latent space disentanglement leads to better control of speaker identity and acoustic conditions for synthesis. This section evaluates latent space disentanglement by changing the dimension of z s. The proposed model's performance is compared for z s dimensions of 32, 64, 128, and 256. Variants without data augmentation or adversarial training struggle to disentangle in all configurations. Increasing the dimension of z s results in worse separation of information for both the proposed model and \"-adv\". The \"-adv\" model fails to encode noise information in z r when z s has 128 dimensions, possibly due to poor model parameter initialization. This suggests that disentanglement performance without adversarial training may heavily depend on model initialization. The proposed model shows high sensitivity to model initialization for disentanglement performance. It achieves the best noise prediction accuracy with z r and the lowest with z s. A neural network TTS model incorporates generative modeling, data augmentation, and adversarial training to learn disentangled representations for synthesized speech control. Extensive studies on a synthetic dataset validate the effectiveness of the proposed solution and its robustness to hyperparameter choices. The method for disentangling correlated attributes is general and can be applied to other pairs of factors. The proposed method aims to disentangle pairs of attributes in an unsupervised manner, such as reverberation and speaker, or text-to-image generation. Future work will explore the capability of the method further."
}