{
    "title": "H1x22Xn5Ur",
    "content": "Recent advances have enabled the creation of deep complex-valued neural networks, but many learning tasks still do not fully utilize complex representations. A new deep complex-valued method is proposed for signal retrieval and extraction in the frequency domain, focusing on audio source separation in the Fourier domain. The method leverages the convolution theorem and a complex-valued version of Feature-Wise Linear Modulation (FiLM). An explicit amplitude and phase-aware loss function is introduced, considering the complex-valued components of the spectrogram. Our proposed signal extraction method, based on Feature-wise Linear Modulation (FiLM), creates multiple separated candidates for each signal in a mixture of inputs. A signal averaging operation increases robustness to noise and interference, with dropout implemented to reduce interference and noise correlation. This method acts as local ensembling for audio source separation, aiming to retrieve distinct audio signals associated with each speaker. The proposed signal extraction method, based on Feature-wise Linear Modulation (FiLM), creates multiple separated candidates for each signal in a mixture of inputs. A signal averaging operation increases robustness to noise and interference. The experiments demonstrate the efficacy of the proposed masking method and the advantage of the new frequency-domain loss that considers magnitude and phase of signals. The Fourier transform allows for spectral information retrieval of clean signals using scaling and shifting representations. FiLM can infer these representations for speech separation. In the context of speech separation, convolving an impulse response with clean speech allows for mix reconstruction. Additionally, a stochastic process y can be represented as x plus noise, where x is the clean signal to estimate. The signal-to-noise ratio (SNR) is defined as the ratio of the power of a clean signal to the power of noise. Signal averaging and uncorrelated noises can increase the SNR by a factor of N. A weighted version of cosine similarity is proposed to maximize the signal-to-distortion ratio (SDR). The cosine similarity loss is defined in the real-valued domain with an equation involving element-wise multiplication. It considers the target signal y and estimated signal x after inverse STFT, implicitly accounting for phase. In speech enhancement, a weighted cos time loss is used to differentiate between speech and noise signals based on their energies. The complex inner product between speech signals is computed in the frequency domain, taking into account both magnitude and phase. It is shift invariant and can differentiate between signals with high or low energy. The inner product equation is used to compare signals and determine their similarity. The complex inner product between speech signals in the frequency domain considers both magnitude and phase. It is shift invariant and can distinguish signals based on energy levels. The similarity loss function, CSimLoss, maximizes the real part and minimizes the square of the imaginary part of the normalized inner product. Penalty constants \u03bb real and \u03bb imag are used, with \u03bb real set to 1 in all experiments and different values tested for \u03bb imag. In experiments, different values of \u03bb imag were tested, with \u03bb imag = 10 4 yielding the best results. Results are reported in Table 1 and Table 2 for CSimLoss with \u03bb imag = 10 4. FiLM techniques have shown impressive results in VQA by applying affine transformations to feature maps based on question embeddings. Multiple transformations of complex input spectrograms are created using FiLM in our approach, with parameters determined from the U-Net output. Complex masks are generated for the original input spectrogram and each FiLM-transformed spectrogram. The approach involves using a ResNet conditioned on the U-Net output to generate complex masks for original and FiLM-transformed spectrograms. These masks are multiplied with the spectrograms to produce multiple speech separation candidates, which are then averaged to estimate clean speech. The process can be seen as a local ensembling technique to estimate clean speech from different speakers. Scaling and shift matrices are generated based on the U-Net output to operate on the input mix spectrogram. The approach involves using a ResNet conditioned on the U-Net output to generate complex masks for original and FiLM-transformed spectrograms. These masks are multiplied with the spectrograms to produce multiple speech separation candidates, which are then averaged to estimate clean speech. The process aims to increase separation capability and reduce interference between speakers by generating complex masks through a sequence of convolution layers and residual blocks. The text discusses a new complex-valued framework for signal retrieval and separation in the Fourier domain, specifically focusing on audio source separation. A masking method based on a complex-valued version of the FiLM model is proposed to enhance separation capability and reduce interference between speakers. The complex masking procedure is detailed in Algorithm 1 in the appendix. In the study, a new phase-aware loss is introduced to improve the quality of signal separation in terms of SDR. The deep separator utilizes complex-valued neural networks and a proposed masking method, highlighting the effectiveness of complex representations in solving tasks in the frequency domain. This research opens up avenues for further exploration into the success of complex-valued representations. The study introduces a new phase-aware loss to enhance signal separation quality in terms of SDR using complex-valued neural networks and a novel masking method. The proposed deep complex architecture incorporates residual connections and complex layer normalization for improved learning stability. The complex masking method is detailed in the appendix. Our novel complex masking method, based on a complex-valued version of FiLM, incorporates identity connections and U-Nets for image segmentation. It utilizes basic complex residual blocks and feature map adjustments in the encoding and decoding paths. The blocks apply complex layer normalization, CReLU, and complex convolutions, with specific kernel sizes and strides for different operations. The complex masking method for image segmentation incorporates identity connections and U-Nets. It uses complex residual blocks with specific kernel sizes and strides. Bilinear interpolation is preferred over transposed convolution for upsampling. Residual U-Net block architecture is chosen for memory constraints and iterative refinement of representations. Trigonometric properties are utilized to cancel differences in amplitude and phase between signals x and y. The complex masking method for image segmentation incorporates identity connections and U-Nets with specific kernel sizes and strides. Trigonometric properties are used to cancel differences in amplitude and phase between signals x and y. A cosine similarity-based function is chosen to learn from noisy-only data, making it more convenient to work with the normalized inner product loss. Complex layer normalization computes mean and covariance statistics over layer features instead of batch instances, avoiding the need to estimate batch statistics during training. Batch normalization may not be suitable for speech processing due to the sparsity of speech signals in both time and frequency domains. Statistics computed across multiple utterance mixtures in a batch are considered meaningless as speakers are not controlled for volume or pauses. Batch statistics in speech processing can be misleading due to the variability in simultaneous speakers, volume, and pauses. Intra-sample normalization techniques like Layer Normalization are more suitable for speech data compared to batch normalization, which is better suited for natural images. Complex Layer Normalization computes statistics over layer features instead of batch instances, ensuring a more robust normalization of data with a large number of feature maps. The speech mixtures for training and validation sets were generated using sentences from the Wall Street Journal WSJ0 training set. The test set was created similarly using utterances from the WSJ0 development set. The data sampling rate is 8KHz, and STFT parameters include a Hann window of size 256. The data sampling rate is 8KHz. STFT parameters include a Hann window of size 256 and a hop length of 128. Tables 1 and 2 show results from experiments on the Wall Street Journal dataset. Models were trained using backpropagation with Stochastic Gradient Descent and Nesterov momentum set at 0.9. Learning rate schedule involved a warm-up phase with a constant rate of 0.01 for the first 10 epochs, then increased to 0.1 until epoch 100, followed by annealing at epochs 120 and 150. Training ended at epoch 200 with batch sizes of 40 for Table 1 models. All models in Table 2 were trained with a batch size of 24 to fit in GPU memory using 8 V100 GPUs. The Permutation Invariant Training criterion (PIT) was used for all tested losses to consider all possible assignments between target signals and estimated clean speeches. This criterion helps address the label permutation problem caused by randomly chosen target speaker orders. The PIT criterion addresses label permutation problem by considering output-target assignment with minimal training loss. Output-to-speaker assignment changes can decrease SNR and SDR. Holographic Reduced Representations (HRRs) enable key-value data storage and retrieval through convolution or inner product. By convolving data with keys or using inner product, FFT can be applied for elementwise multiplication between their Fourier transforms. This method is less expensive than time domain convolution. Danihelka et al. used associative memories to enhance LSTM capacity and robustness by applying independent permutations on memory to create multiple copies, followed by complex multiplication with the key. Signal averaging eliminates decorrelated noise and strengthens Signal-To-Noise ratio. During this decade, interest in Fourier domain representations has grown in the machine learning community. Bruna et al. introduced convolutions to graphs using the Graph Fourier Transform, which involves multiplying a graph signal by the eigenvector matrix of the graph Laplacian. However, computing the eigenvector matrix is expensive. More computationally efficient methods have been introduced recently. Recently, more computationally efficient methods have been introduced in the machine learning community to avoid explicit use of the Graph Fourier basis. Spectral pooling in Convolutional Neural Networks enables pooling in the frequency domain, maintaining spatial dimensionality and retaining more information. Parametrization of convolution filters in the Fourier domain leads to faster convergence during training. Arjovsky et al. designed a recurrent neural network with a unitary hidden transition matrix constructed using specific unitary transformations. The Discrete Fourier Transform and its inverse are used in RNNs to avoid vanishing and exploding gradients. Different approaches like STFT and FRU have been proposed to convert input to the frequency domain and back to the time domain. Speech separation is a well-studied topic in audio processing literature. Speech separation in audio processing literature has been studied extensively. Early attempts involved pure audio inputs or special microphone configurations for supervision. However, gathering clean recordings for strong supervision can be challenging. Subsequent work assumed monophonic audio signals and utilized matrix decomposition approaches, but these have limitations. Many speech separation approaches in audio processing have limitations, such as operating on the frequency domain without considering the phase component and being computationally prohibitive for large quantities of recordings. Recent interest has shifted towards using deep learning techniques to address the speech separation problem, with methods categorized as audio-only or audio-visual. This work focuses on the audio-only category, while a separate section discusses audio-visual methods. In section 6.8.2, neural speech separation methods using audio information are discussed. Huang et al. [2014] were the first to apply deep learning to monaural speech separation, using a combination of feed-forward and recurrent networks with soft masking. Du et al. [2014] also worked on estimating the log power spectrum of target speakers using neural networks. Hershey et al. [2015] proposed deep clustering for speech separation, learning embeddings of mixture signals for clustering. The deep attractor network by Chen et al. extends deep clustering by creating \"attractors\" to cluster time-frequency points dominated by different speakers. Other recent papers have integrated phase-information in speech separation systems, such as Erdogan et al. [2015] and Wang et al. [2018]. The network estimates the spectrum magnitude, while speech signals are retrieved using Griffin-Lim reconstruction. Recent work focuses on speech separation in the time domain directly. TasNet architectures perform speech separation using mixed time signals as input. Previous studies address speech separation considering phase information but without leveraging complex-valued deep learning advancements. Lee et al. utilize a fully complex-valued deep neural network for audio-source separation. The curr_chunk discusses a complex-valued deep neural network used for audio source separation, leveraging visual information from videos. Recent works by Gao et al. and Ephrat et al. utilize unannotated videos to train neural networks for isolating object sounds and enhancing desired speakers' speech. Gao et al. specifically use non-negative matrix factorization and a multi-instance multi-label neural network for this task. The curr_chunk discusses a multi-instance multi-label neural network for audio source separation, focusing solely on audio signals. Unlike previous works that incorporate visual information, this approach aims to address label permutation issues without losing key information. The curr_chunk discusses the configuration of feature maps in a neural network for audio source separation, using complex standard initialization for all layers except for specific convolutional layers. The curr_chunk explores the use of unitary initialization in convolutional layers preceding residual blocks for generating mask functions. Different architectures with varying numbers of mixture transformations are experimented with, showing minimal increase in parameters. Baselines in Table 1 do not include the proposed masking method and loss. The baselines in Table 1 do not include the proposed masking method and loss. Complex models outperform real counterparts in generating masks for clean speech, even with variations in parameters and depth. The experiments in Table 2 focus on two speaker speech separation using different input mixture transformations and dropout rates. The models contain 44 feature maps in the first downsampling layer and use k = 2 residual blocks. The SDR scores are shown in the last column, and the study emphasizes transformations and losses suitable for complex-valued models. The experiments in Table 2 show that wider and deeper models improve separation quality, additional input transformations lead to higher SDR scores, and losses computed in the spectral domain yield the best results. Local ensembling is beneficial for speech separation, confirmed in all experiments. Masks in speaker embedding contribute to the overall embedding, and performing dropout on masks may enhance performance. Performing dropout on masks can enhance performance by regularizing the retrieval and separation mechanism. Spectral loss functions yielded higher SDRs than time-domain counterparts, leading to better results with multiple input transformations. Wider and deeper models with additional input transformations outperformed those without, achieving higher SDR scores on average. From the results in Table 2, wider models may not be more beneficial for the separation task. Narrower models showed higher SDR scores with input transformations. Local ensembling with mixtures of transformations acted as a regularizer, leading to lower SDR performances. The local ensembling procedure acts as a regularizer, but increasing input transformations can lead to overfitting. Using a small dropout rate further regularizes the model. Experimenting with dropout rates showed that a rate of 0.1 yielded the best result, improving the SDR score. Finding a compromise in the number of transformations is crucial, with 10 mixture transformations yielding the highest SDRs in most cases. The CSimLoss consistently outperformed the L2 freq in all experiments. The CSimLoss outperformed the L2 freq in all experiments, yielding higher SDR scores. Regardless of dropout rate and input transformations, wider models using L2 freq did not surpass a 10.91 dB threshold. The highest SDR score with L2 freq was 10.93 for a narrower model with 15 input transformations. Validation curves for models with highest SDRs using L2 spectral loss or CSimLoss are shown in Figures 4 and 5."
}