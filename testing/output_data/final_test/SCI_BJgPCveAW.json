{
    "title": "BJgPCveAW",
    "content": "We propose reducing parameters in fully connected layers of neural networks using pre-defined sparsity. Results show no loss of accuracy at less than 0.5% classification layer connection density. The 'scatter' metric characterizes connection patterns. Tests on CIFAR, MNIST, and Morse code dataset demonstrate trends and limits of sparse connections in NNs. Learning systems drive new technologies like image processing and speech recognition. Modern neural networks, like Alexnet, Overfeat, and ResNet, have millions of parameters, requiring significant memory and processing power. Optimizing networks involves making them deeper and adding parameters, increasing storage complexity. Fully connected layers (FCLs) in these networks, especially in Alexnet, contain the majority of parameters. The majority of parameters in modern neural networks are concentrated in Fully Connected Layers (FCLs), such as in Alexnet where they account for 95.7% of the network parameters. The concept of sparsity is formalized in the context of sparse networks with reduced connection density. The question of how to best distribute weights in sparse architectures for optimized network performance is explored. The present work formalizes sparsity in neural networks and its impact on performance. It shows that pre-defined sparse networks do not suffer performance degradation and can significantly reduce parameters. Techniques for distributing connections in sparse networks are discussed, along with the use of scatter metric to evaluate network quality. For example, AlexNet has a weight size of 234 MB and requires 635 million arithmetic operations. Previous works have attempted to reduce parameters in neural networks by techniques such as dropout, pruning, quantization, and using low rank matrices to impose structure on network parameters. These methods aim to address the issue of overfitting and improve performance on test data. Our approach to simplifying neural networks involves pre-defining the level of sparsity or connection density in the network before training. This results in a network with fewer connections than its fully connected counterpart, reducing the parameter explosion problem. The network is structured with J junctions, each with a specific number of neurons in each layer. This design ensures that weights for absent connections are never utilized during training or inference. Our approach involves pre-defining sparsity in neural networks to reduce the parameter explosion problem. The network is structured with junctions, each with fixed fan-out and fan-in values. This ensures all neurons contribute equally and none get disconnected. The connection density in each junction is calculated, and overall connection density is defined. Previous works have proposed hardware architectures leveraging pre-defined sparsity for faster training. Our method introduces pre-defined sparsity in neural networks to address the parameter explosion issue. We experimented with different datasets and network structures, utilizing batch normalization, ReLU activation, and max-pooling layers. The Adam optimizer was used for training. In our study, we utilized the Adam optimizer, ReLU-activated hidden layers, and softmax output layer consistently across all networks. Results suggest that denser CL junctions closer to the outputs are more effective. A grid search determined CL junction densities, with configurations like 'conv+2CLs' and 'conv+3CLs' for different datasets. 'MNIST CL' and 'Morse CL' networks were also explored. The study explored CL only networks with varying junction densities, showing that sparse CLs outperformed dense FCLs with significantly fewer weights. Results indicate the promise of sparsity in improving performance. The study compared sparse CL networks with dense FCL networks on MNIST without data augmentation. Results showed that CL networks with low densities outperformed FCL networks with fewer weights. The network with CL density of 2.35% matched FCL performance. The study compared sparse CL networks with dense FCL networks on MNIST without data augmentation. Results showed that CL networks with low densities outperformed FCL networks with fewer weights. The second family of networks with only CLs showed performance drops at higher densities compared to conv layers. Large SCLs performed better than small FCLs with similar parameters, indicating the desirability of increasing hidden neurons with diminishing returns. The Morse code dataset presented a harder challenge for sparsity. The Morse code dataset poses a challenge for sparsity due to its 64-valued inputs and 64 classes. Performance degrades quickly as connections are removed in the network with 64 input and output neurons, and 1024 hidden layer neurons. Results show that deep networks with conv layers have severe redundancy and can be made extremely sparse without significant accuracy degradation. The conv layers in deep networks can be made sparse without affecting performance, leading to significant memory savings. The importance of conv layers is less in networks that already discriminate between classes well. Sparsifying conv layers may not result in large computational savings due to the dominance of conv layers in computational complexity. Other types of neural networks, like restricted Boltzmann machines, with a higher prominence of conv layers would benefit more from this approach. The study shows that sparse conv layers can lead to memory and computational gains, especially in networks with a higher prominence of conv layers like restricted Boltzmann machines. By pre-defining sparse conv layers, significant savings can be achieved without affecting performance. The importance of later junctions needing more connections than earlier ones is highlighted, with peak performance achieved at specific density levels. Adjacency matrices are introduced to describe junction connectivity. The study introduces adjacency matrices to describe junction connection patterns in neural networks. These matrices indicate the presence of connections between neurons in different layers. By multiplying these matrices, the effective connection pattern between any two junctions can be determined. Special cases, such as input-output adjacency matrices, are also discussed. The study discusses adjacency matrices to describe junction connection patterns in neural networks. It explores the concept of sparsity in connection patterns and the potential redundancy between neurons. The idea of grouping neurons into windows based on output dimensionality is also considered for optimizing performance. In neural networks, grouping neurons into windows based on output dimensionality helps optimize performance. The goal is to maximize connections between left and right neurons to capture a global view. Sparse connection patterns are crucial to avoid misinterpretations, such as mistaking the top half of a digit 2 for a digit 3. The pattern aims to spread connections across input windows to avoid misinterpretations in neural networks. By grouping neurons into windows, local information is spread to different parts of the right layer. The window size is chosen to optimize connections between left and right neurons, ensuring an ideal number of connections. In neural networks, the window size is optimized to spread connections across input windows, ensuring an ideal number of connections between left and right neurons. Scatter is used as a proxy for network performance, computed quickly to predict the effectiveness of a sparse network. The scatter metric is used to optimize connections in neural networks, with entries greater than 1 treated as 1. Scatter values are computed for forward and backward data flow directions, with the final metric being the minimum value. Low scatter values indicate poor network performance. Our experiments evaluated scatter in neural networks using different configurations, showing that high scatter indicates good performance, especially in CL only networks. Performance vs. scatter plots in FIG7 demonstrate this correlation, with planned connections leading to higher scatter values. When planning connections in neural networks, high scatter values indicate good performance, particularly in CL only networks. The correlation between performance and scatter is shown in FIG7, where high scatter values are achieved with planned connections. In analyzing the results, it was found that certain junctions with high values in S led to low values in another junction, resulting in low overall S. Random patterns tend to perform well due to this phenomenon. Differentiation between networks with equal S values is based on the next minimum value in S, as observed in the Morse and MNIST results. Insights drawn include the importance of considering non-minimum elements in S and the varying occurrences of specific values in S affecting network accuracy. The concept of windows and scatter is important for all CLs in neural networks, not just the first one. Scatter is a sufficient metric for performance, but not necessary. Carefully choosing which neurons to group in a window can increase predictive power. Pre-defining sparsity in CLs leads to a significant reduction in parameters without performance loss. The smaller the fraction of CLs in a network, the more efficient it is. Pre-defining sparsity in CLs leads to a significant reduction in parameters without performance loss. Achieving 0.2% density on Alexnet could result in a 95% reduction in overall parameters. Hardware acceleration for sparse networks can lead to more aggressive exploration of network structure. Network connectivity can be guided by the scatter metric and optimal distribution of connections. Future work may involve extending to conv layers and making junction 2 denser than junction 1 when overall CL density is fixed. When junction 1 density is minimized and junction 2 is maximized while maintaining overall density, accuracy is 36% for both subfigures (b) and (c). Flipping the densities results in accuracies of 67% for subfigure (b) and 75% for (c) in FIG3. Window output matrices for multiple junctions can be created by multiplying individual matrices. In the Morse network, f o 1:2 = 1024 and f i 1:2 = 1024 for equivalent junction 1:2 with N 1 = 64 left neurons and N 3 = 64 right neurons. The number of neurons in each window is rounded up to 1 in this case. Training a neural network involves finding the minimum of the cost function, which is affected by non-idealities like saddle points and poor conditioning. Overcoming these effects can make optimization more time-consuming. Training a neural network can be time-consuming due to non-idealities like saddle points and poor conditioning. Networks with fewer parameters converge faster, suggesting that networks with Structurally Constrained Layers (SCLs) train faster than Fully Connected Layers (FCLs) because of their reduced parameter count."
}