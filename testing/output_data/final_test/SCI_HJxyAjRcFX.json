{
    "title": "HJxyAjRcFX",
    "content": "Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are attributed to the success of conditional GAN models. The joint use of GAN loss with reconstruction loss in training methods has led to a lack of diversity in output samples. To address this issue, novel training schemes with moment reconstruction losses have been proposed to achieve both training stability and multimodal output generation. Experimental results on various tasks like image-to-image translation, super-resolution, and image inpainting demonstrate the effectiveness of this approach in generating diverse outputs while maintaining visual fidelity. Recent research has made significant progress in conditional image generation tasks like image-to-image translation, image inpainting, super-resolution, and video prediction using conditional GANs. However, training GANs, including conditional GANs, is unstable and prone to collapse. To address this issue, previous models in conditional image generation have utilized both reconstruction loss and GAN loss to improve output quality. The reconstruction loss in conditional image generation tasks helps with training stability but can worsen mode collapse in GANs. Many models do not include random noise input, leading to a lack of output diversity. The paper proposes moment reconstruction losses to address the issue of GANs ignoring noise input and lack of output diversity. By using maximum likelihood estimation loss to predict conditional statistics, the model aims to match generated distribution statistics with real data distribution, improving training stability and output diversity. The paper introduces novel loss functions to address the mismatch between GAN loss and reconstruction loss, improving training stability and output diversity in conditional generation tasks like image translation, super-resolution, and image inpainting. The curr_chunk discusses the limitations of existing models in generating diverse outputs for tasks like super-resolution, image inpainting, and video prediction. It highlights the need for stochasticity in models to generate diverse samples from a single conditional input. Various attempts have been made to incorporate stochasticity in conditional generation, such as Conditional VAE-GAN. The VAE-GAN model combines the decoder from VAE with the generator from GAN to produce a wide range of realistic images. It focuses on output fidelity by GANs and diversity by VAEs. Additionally, models like CVAE-GAN and Bicycle-GAN aim to learn disentangled representation for unsupervised image-to-image translation, splitting the embedding space into domain-invariant and domain-specific spaces. Conditional VAE-GANs and disentanglement-based methods utilize latent variables to maintain output diversity. A simpler approach introduces novel loss functions to achieve multimodal conditional generation by replacing the reconstruction loss. Conditional GANs aim to generate realistic samples indistinguishable from real data, with objectives typically including GAN loss and reconstruction loss terms. Conditional GANs utilize GAN loss and reconstruction loss terms. Another important loss term is the perceptual loss, which measures the distance between features encoded by neural networks. The training scheme involves a minimax game between the discriminator and generator, where the generator produces outputs based on input and random noise. The most common reconstruction losses in conditional GAN literature are the 1 and 2 losses. The 1 and 2 losses in conditional GAN literature stem from maximum likelihood estimations of Laplace and Gaussian distributions, respectively. These losses lead to estimating the conditional average or median of y given x, not generating samples from the distribution. The joint use of the reconstruction loss with the GAN loss in conditional GANs may worsen mode collapse by forcing the model to predict only the mean of p(y|x) and pushing the conditional variance to zero. The 2 loss decomposes the loss into irreducible and reducible terms, where minimizing the total loss reduces both systematic and variance effects to zero. The 2 loss reduces bias and variance to zero, minimizing the conditional variance of output. The Pix2Pix model is modified to include noise input, with different loss term combinations tested. The model trained with only the GAN loss fails to generate. The combination of GAN and 1 loss helps train the model to generate visually appealing images, but lacks variation. Our new objective enables the model to produce diverse output samples, proposing novel alternatives for reconstruction loss in conditional generation tasks. Our new loss terms enable the model to achieve stability in training and multimodal generation. The architectural comparison shows how our models differ from conventional conditional GANs by using MLE losses for generating output samples matched to their ground-truth. The second model, MR-GAN, is a more stable variant trained with MLE losses to estimate conditional mean and variances, ensuring sample moments match predictions. The key idea is to apply MLE losses to ensure similarity in statistics between the generator's sample distribution and actual data distribution. Novel losses for conditional GANs are proposed in sections 4.2-4.3, while section 4.4 shows that our losses do not conflict with the reconstruction loss. The 2 loss focuses on MLE of the conditional mean of y given x, with an option to estimate the conditional variance as well. Estimating conditional variance in BID15 involves interpreting heteroscedastic aleatoric uncertainty, with variance representing aleatoric uncertainty. The Laplace distribution yields a similar MLE loss as DISPLAYFORM1, where m is the predicted median and b is the predicted MAD BID3. Predicting the logarithm of variance or MAD BID15 is more numerically stable. Our methods are mainly described with the 2 loss under a Gaussian assumption, easily adaptable to the Laplace version for the 1 loss. The Moment Reconstruction (MR) loss function is used in a conditional GAN called MR-GAN, with updates in the architecture to produce K different samples. The MR-GAN introduces updates in the architecture to produce K different samples, with the generator approximating the real distribution by estimating moments. The final loss of MR-GAN is a weighted sum of the GAN loss and the MR loss, which can be simplified by using only the mean (MR 1) or both mean and variance (MR 2). The Proxy Moment Reconstruction (proxy MR) loss is proposed as a more stable training alternative to the MR loss in MR-GAN. It involves a predictor P trained with ground-truth data to predict conditional mean and variance before training the generator. The predictor does not have a noise source as input and predicts both mean and variance as output. MR-GAN utilizes predicted statistics of real distribution to guide the generator's outputs by matching mean and variance. The proxy MR loss is defined as the sum of squared errors between predicted and sample statistics. Different variants match different moments, with MR-GAN allowing direct access to real data for the generator. The proxy MR-GAN method provides more stable training with less variance in target values compared to using the predictor directly. It also helps avoid overfitting by selecting the predictor with the smallest validation loss. This approach results in a generator that suffers less from overfitting compared to using the MR loss directly. The two methods have their own advantages and disadvantages, which will be empirically compared in section 5.2. The approach discusses the optimal generators for GAN and 2 loss, highlighting the challenge of finding a solution that satisfies both simultaneously. The conditional generation tasks require diverse output, leading to the conclusion that a generator cannot be optimal for both losses at the same time. This complexity makes it difficult to predict the model's behavior when combining the two losses. The reconstruction loss alone can lead to mode collapse, but the approach discussed avoids loss of diversity in output samples. The optimal generators for GAN and 2 loss cannot be the same, as they have conflicting objectives. The 2 loss can reduce multimodality by pushing conditional variance to zero, while the optimal generator for GAN is also optimal for the discussed loss terms. The proof shows that the optimal generator for GAN loss is also optimal for the discussed loss terms. This approach avoids mode collapse and maintains diversity in output samples. The proof is specific to conditional GAN models without latent variables, and may not apply to models with latent variables that encode target diversity. The methods are applied to image-to-image translation, super-resolution, and image inpainting tasks using Pix2Pix, SRGAN, and GLCIC as base models. In image-to-image translation, super-resolution, and image inpainting tasks, Pix2Pix, SRGAN, and GLCIC are used as base models. The models are trained with MR and proxy MR objectives, generating diverse images successfully. Proxy MR loss shows stable training compared to MR loss in all tasks. Additional details and results are provided in the appendix. Our proxy MR 2-GAN model outperforms state-of-the-art methods in image-to-image translation, super-resolution, and image inpainting tasks by generating high-quality diverse images. The MR loss is more sensitive to the number of samples generated, with the proxy MR loss showing more stable training. However, the MR loss has limited applicability compared to the proxy MR loss. Evaluation on Pix2Pix-Cityscapes, SRGAN-CelebA, and GLCIC-CelebA tasks shows the diversity and realism of the generated images. The study evaluates the diversity and realism of generated images for different tasks using a trained model. The test sample set consists of 6,000 images, with LPIPS scores measured for diversity. Human evaluators determine the realism score by identifying fake images, with a scale from 0 to 1. The study includes eight configurations of methods for image generation tasks. The study evaluates the diversity and realism of generated images for different tasks using a trained model. There are eight configurations of methods for image generation tasks, with results showing competitive performance compared to base models, sometimes even better. For example, in Pix2Pix-Cityscapes, Gaussian MR 1, Gaussian proxy MR 1, and Gaussian proxy MR 2 outperform BicycleGAN and Pix2Pix+noise in both realism and diversity. Our methods significantly outperform BicycleGAN and Pix2Pix+noise in realism and diversity measures. The results show that our methods generate high-quality images with greater diversity while maintaining competitive realism scores. Matching means could be sufficient for guiding GAN training, suggesting that adding more statistics may not always improve performance and could lead to additional errors. In this work, a set of novel loss functions named MR loss and proxy MR loss were proposed to improve conditional GAN models for stability and multimodal generation. Empirical results showed successful integration with state-of-the-art models for image translation, super-resolution, and image inpainting tasks, producing realistic and diverse image samples on Cityscapes and CelebA datasets. There are potential future directions for research beyond this work. In this work, novel loss functions called MR loss and proxy MR loss were introduced to enhance conditional GAN models for stability and multimodal generation. The methods can be extended to other conditional generation tasks like text-to-image synthesis, text-to-speech synthesis, and video prediction. Additionally, exploring higher order statistics or covariance and utilizing high-level features' statistics for correlation capture are potential future research directions. The algorithms for the methods are detailed from Algorithm 5 to Algorithm 4, assuming single input per update and using non-saturating GAN loss. For Laplace MLEs, median and MAD statistics are computed. The text introduces MR loss and proxy MR loss for improving conditional GAN models. It explains a special trick to distribute gradients to all samples, not just the median sample, in order to enhance training effectiveness. The approach involves calculating differences between predicted and sample medians, setting target values, and calculating loss between targets and samples. This technique is also applied to MR loss in Gaussian MR 1-GAN. The text discusses various algorithms for updating generators in different types of MR-GAN models, including Gaussian and Laplace MR-GANs, as well as Gaussian proxy MR-GAN. The algorithms involve input data, ground truth, and the number of samples, with a focus on MR loss coefficients. The text discusses algorithms for updating generators in MR-GAN models, including Gaussian and Laplace MR-GANs, as well as Gaussian proxy MR-GAN. The algorithms involve input data, ground truth, and the number of samples, with a focus on MR loss coefficients. PyTorch is used for implementation with AMSGrad optimizer. In experiments, AMSGrad optimizer BID32 with LR = 10 \u22124, \u03b2 1 = 0.5, \u03b2 2 = 0.999 is used with weight decay and gradient clipping. Proxy MR-GAN training continues until overfitting, selecting the checkpoint with the lowest validation loss. GAN loss weight is fixed at 1. Our methods require more training steps for high-quality images due to a wider output range. Training stability is maintained with a large hyperparameter range for the proxy MR loss, unlike the unstable MR loss. Our Pix2Pix variant utilizes noise input, input normalization, specific batch sizes, loss weights, and update ratio. The generator is updated once per every discriminator update. Our SRGAN variant, based on PyTorch implementation, includes noise input with Gaussian noise tensors, input normalization for pixel values, specific batch sizes, loss weights, and update ratio. The generator is updated five times per every discriminator update. Our PyTorch implementation of the GLCIC model includes noise input with Gaussian noise tensors, input resizing and masking, input normalization, and specific batch sizes for the discriminator, predictor, and generator. The generator is updated five times per every discriminator update. The GLCIC model's training involves generating 12 samples per input, with a total batch size of 96. Loss weights include Gaussian proxy MR 2 loss with \u03bb pMR = 1000. Update ratio is three times per discriminator update to prevent mode collapse. Toy experiments show prevention of mode collapse in unconditional generation on synthetic 2D data. Our methods prevent mode collapse by penalizing deviations in generated sample statistics, reducing mode collapse significantly. These techniques show potential to stabilize training for unconditional generation tasks as well. The generated samples include input, ground-truth, predicted mean, sample mean, predicted variance, and sample variance. The 1 loss is decomposed to minimize it effectively. To prevent mode collapse, techniques penalize deviations in generated sample statistics, reducing mode collapse significantly. The 1 loss is decomposed to minimize it effectively by ensuring the gradient w.r.t. G(x, z) is zero for x and z where p(x) > 0 and p(z) > 0. The generator can be optimal for both GAN loss and 1 loss if every real data belongs to the interval of conditional median. If the generated distribution is identical to the real distribution, the generator is optimal w.r.t. both losses, although such cases are rare. The set of optimal generators for 1 loss is likely disjoint from the optimal set for GAN loss due to the conditional median condition. Experiments with different loss function combinations show that training the Pix2Pix model with only the proposed loss term results in samples similar to those with reconstruction loss only. MR 2 and proxy MR 2 losses generate diverse samples with varying styles, incurring high-frequency variation. The experiment explores the impact of different loss functions on sample variation. Proxy MR 2 loss shows low-frequency variation, while MR 2 loss exhibits high-frequency variation patterns. Increasing \u03bb Rec reduces sample variance, indicating reconstruction as a major factor. Interestingly, sample quality deteriorates initially but improves with higher \u03bb Rec values. The joint use of GAN, proxy MR 2, and reconstruction losses is not desirable, as each can find high-quality local optima individually. Testing different coefficients for the reconstruction loss while fixing GAN and proxy MR 2 losses coefficients."
}