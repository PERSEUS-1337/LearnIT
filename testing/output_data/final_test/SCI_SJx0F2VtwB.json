{
    "title": "SJx0F2VtwB",
    "content": "Conversational question answering (CQA) is a novel QA task that involves understanding dialogue context. It combines passage reading, coreference resolution, and contextual understanding. The SDNet model, a contextualized attention-based deep neural network, integrates BERT as a sub-module to improve performance. Empirical results show SDNet outperforms previous models on the CoQA leaderboard. Conversational question answering (CQA) is a task that requires deep understanding of dialogue context, passage reading, coreference resolution, and contextual understanding. Recent studies have focused on innovations in text encoding, attention mechanisms, and answer verification for this task. Traditional machine reading comprehension (MRC) tasks often oversimplify the conversational manner humans take when probing a passage. Conversational question answering (CQA) involves understanding dialogue context, coreference resolution, and passage reading. Public datasets like CoQA, QuAC, and QBLink evaluate models' effectiveness in this field. Traditional MRC models are not directly applicable, leading to the proposal of models like DrQA+PGNet for conversational QA tasks. Traditional MRC models are not directly applicable to conversational QA tasks. Models like DrQA+PGNet, BiDAF++, and FlowQA have been proposed to tackle conversational QA tasks effectively. SDNet is a contextual attention-based deep neural network for conversational question answering. It incorporates interattention and self-attention, includes previous rounds of questions and answers, and utilizes BERT contextual embeddings. SDNet is a neural model for conversational question answering that utilizes BERT as a contextualized embedder. It aligns traditional tokenizer with BPE tokenizer in BERT, employs a weighted sum of BERT layer outputs, and locks internal parameters of BERT during training to save computational cost. Evaluated on CoQA dataset, SDNet improves the F1 score by 1.6% to 76.6% and further increases to 79.3% with an ensemble model. These techniques are also applicable to other NLP tasks. The SDNet model for conversational question answering uses BERT as a contextualized embedder, improving F1 score on the CoQA dataset. It incorporates conversation history by prepending previous question-answer pairs to the current question, converting the task into a single-turn machine reading comprehension task. The Encoding layer encodes tokens in the passage and question using word embeddings and contextualized embeddings from BERT. The model utilizes the BERT language understanding model with fixed parameters and linear combination of embeddings from different layers. It employs multi-layer RNNs for contextual information, word-level attention, history-of-word concept, self-attention, and an output layer for final answer span computation using attention and bilinear projection. The model SDNet utilizes GloVe embeddings for words in the context and question, along with feature vectors for each context word. BERT is used as a contextual embedder in the model, with methods designed to leverage its capabilities. To incorporate BERT into the network, a conventional tokenizer is used to get word sequences, followed by the BPE tokenizer to partition each word into subwords. The contextual embedding of a word is defined as the averaged BERT embedding of its subwords. Additionally, thin task-specific linear layers are appended to BERT, utilizing a weighted sum of layer outputs to boost performance. Due to BERT's large number of parameters, computing and storing gradients requires significant time and space. To optimize efficiency during training with BERT, internal weights are locked, updating only linear combination weights. This approach reduces computational resources needed. Contextual embedding for a word is computed using trainable parameters. Word-level Inter-Attention is conducted based on GloVe word embeddings, simplifying notation for attention function. The attention function Attn(A, B, C) combines vector set C using scores from A and B, similar to transformer attention. Bidirectional LSTMs are used for contextual understanding in question and context. Variational dropout is applied to input vectors in RNN layers. Additional RNN layer is used for question understanding. Model integrates previous utterances for self-attention on the question. The model integrates previous utterances for self-attention on the question, employing multilevel inter-attention from question to context to extract different levels of semantic abstraction efficiently. Leveraging the history-of-word idea from FusionNet, the attention combines one RNN layer output to compute scores, reducing computational inefficiency. The history-of-word vector in SDNet includes an additional RNN layer for self-attention on the context. It uses the history-of-word concept to reduce output dimensionality and generate the final representation of context words. The output layer condenses the question into a single representation vector and generates answer spans in interval forms. Special answer types like \"yes\", \"no\", or \"unknown\" can also be output by SDNet. The model generates probabilities for \"yes\", \"no\", or \"unknown\" answers based on parametrized matrix and vector. During training, the goal is to maximize the probability of the ground-truth answer, including span start/end position and different answer situations. Inference involves selecting the highest probability for span/yes/no/unknown, with a maximum span length of 15. The model was evaluated on CoQA dataset for conversational question answering. The CoQA dataset contains passages from various domains with an average of over 15 question answering turns per passage. Word tokenization is done using spaCy and the BERT-large model is used for contextual embedding. Training includes a dropout rate of 0.4 for BERT layer outputs and 0.3 for other layers, with Adamax as the optimizer. The model is trained for 30 epochs with a clipped gradient at 10. The model is trained with \u03b2 = (0.9, 0.999) and a gradient clip at 10 for 30 epochs. Various hidden sizes are used for attention and RNN layers. SDNet outperforms baseline models in F1 evaluation metric. SDNet outperforms baseline models in F1 evaluation metric, with a single model improving overall F1 by 1.6% compared to FlowQA. An ensemble of 12 SDNet models further boosts F1 score by 2.7%. Ablation studies confirm the effectiveness of the proposed weighted sum of per-layer output from BERT. SDNet utilizes conversation history to improve performance in conversational QA tasks. The model's performance peaks when previous N rounds of questions and answers are included, with a significant drop in F1 score when dialogue history is excluded. Using BERT-base instead of BERT-large hurts the F1 score, while variational dropout and self-attention can enhance performance. The SDNet model leverages inter-attention and self-attention on passage and conversation history to improve conversational question answering. By incorporating BERT as a contextual embedder and using tokenizers alignment and weight-locking techniques, SDNet outperforms previous models on the CoQA dataset. Future work includes applying the model to open-domain multiturn QA tasks with large knowledge bases."
}