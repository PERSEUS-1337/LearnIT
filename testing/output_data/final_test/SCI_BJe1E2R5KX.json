{
    "title": "BJe1E2R5KX",
    "content": "Model-based reinforcement learning (RL) aims to reduce sample complexity in comparison to model-free RL. This paper introduces a new algorithmic framework for designing and analyzing model-based RL algorithms with theoretical guarantees. The framework includes a meta-algorithm that iteratively builds a lower bound of the expected reward based on estimated dynamical models and sample trajectories, maximizing this lower bound over policy and model. This approach extends the optimism-in-face-of-uncertainty principle to non-linear dynamical models without explicit uncertainty quantification, resulting in a variant of model-based RL algorithms called Stochastic Lower Bounds Optimization (SLBO). The paper introduces a new algorithmic framework for model-based RL called Stochastic Lower Bounds Optimization (SLBO). SLBO achieves state-of-the-art performance with limited samples on continuous control tasks, contrasting with model-free RL that requires a large number of samples for success. Model-based RL explicitly uses state observations to plan with estimated dynamical models, showing promise in reducing sample complexity. Model-based deep reinforcement learning has shown strong improvements in sample efficiency, but many theoretical properties are not well-understood, such as the impact of model estimation error on value function estimation and planning. Challenges arise in addressing these questions theoretically due to the high-dimensionality of deep RL with continuous state and action spaces. The uncertainty quantification of non-linear parameterized dynamical models in model-based RL is challenging. Prior work mostly focuses on finite-state MDPs or linear parametrization, with limited research on non-linear models. There is a lack of theoretical understanding on the impact of model estimation error on value function estimation and planning in deep RL with continuous state and action spaces. The paper proposes a novel algorithmic framework for model-based deep RL with convergence guarantees. The algorithm extends the optimism-in-face-of-uncertainty principle to non-linear dynamical models without explicit uncertainty quantification. It provides provable upper bounds on the error between estimated and real value functions in a neighborhood of a reference policy. The algorithm iteratively collects samples, builds lower bounds, and maximizes them over the dynamical model and policy. This approach guarantees monotonic improvement in policy performance in model-based deep RL. The theoretical guarantee of monotone improvement for model-based deep RL involves optimizing a robust lower bound through iterative maximization over the model and policy. Various discrepancy bounds are designed to encourage exploration in uncharted spaces. Norm-based model loss is recovered with the assumption of a Lipschitz value function, favoring a norm over the square of the norm. Experimental results show that learning with a 2 loss outperforms mean-squared error loss. Learning with a 2 loss outperforms mean-squared error loss in model-based deep RL. A discrepancy bound is designed to measure the loss of the model by comparing predicted and true next states, invariant to state space representation. The analysis compares model-based RL with on-policy model-free RL algorithms, utilizing \u03c7 2 -divergence for refining bounds. Equation (1.1) is a good approximator of V \u03c0 in a larger neighborhood compared to linear approximation. A variant of model-based RL algorithm, Stochastic Lower Bounds Optimization (SLBO), is designed based on the framework and analysis. SLBO shows state-of-the-art performance with limited samples on continuous control tasks. State space is denoted by S, action space by A, and policy \u03c0(\u00b7|s) specifies the conditional distribution over actions given a state. Dynamical model M (\u00b7|s, a) specifies the distribution of the next state given the current state and action. The target applications involve continuous state and action spaces, with results applicable to discrete spaces as well. In model-based RL, M is a dirac measure when deterministic. M is a function from S \u00d7 A to S. S 0 is the initial state variable. S \u03c0,M t is the state variable at step t under policy \u03c0 and model M. A t is the action at step t. R(s, a) is the reward function. The text discusses the reward function R(s, a) and the value function V \u03c0,M in model-based RL. It aims to maximize the reward-to-go on the true dynamical model over a policy \u03c0. The discounted distribution of states visited by policy \u03c0 is defined as \u03c1 \u03c0,M. The goal is to build a lower bound for V \u03c0,M. Our goal is to establish an optimizable discrepancy bound for the value function V \u03c0,M over the neighborhood of a reference policy \u03c0 ref. This bound should vanish when M is an accurate model and be estimable and optimizable using a known differentiable function f. Our goal is to establish an optimizable discrepancy bound for the value function V \u03c0,M over the neighborhood of a reference policy \u03c0 ref. This bound should vanish when M is an accurate model and be estimable and optimizable using a known differentiable function f. We can estimate discrepancy bounds for \u03c0 in the neighborhood of \u03c0 ref by sampling empirical trajectories from executing policy \u03c0 ref on the real environment M. One valid discrepancy bound we will prove in Section 4 is a multiple of the error of the prediction of M on the trajectories from \u03c0 ref. If we establish such a discrepancy bound with properties (R1), (R2), and (R3), we can devise a meta-algorithm in Section 4. In Section 4, a meta-algorithm (Algorithm 1) is proposed to optimize the lower bound over policy \u03c0 and model M iteratively. The discrepancy bound D \u03c0 k (M, \u03c0) ensures the model fits the sampled trajectories, aiding in model learning. The algorithm emphasizes the importance of the discrepancy bound for better performance. The algorithm optimizes the lower bound over policy \u03c0 and model M iteratively, emphasizing the importance of the discrepancy bound for better performance. It extends the optimism-in-face-of-uncertainty principle to non-linear settings, optimizing the lower bound directly without building confidence intervals. The uncertainty is measured by how errors affect the estimation of the value function. The maximization of V \u03c0,M can be solved by model-free RL algorithms with M as the environment. The algorithm optimizes the lower bound over policy \u03c0 and model M iteratively, extending the optimism-in-face-of-uncertainty principle to non-linear settings. It emphasizes the importance of the discrepancy bound for better performance. The uncertainty is measured by how errors affect the estimation of the value function. Optimizing V \u03c0,M can be solved by model-free RL algorithms with M as the environment without querying real samples. The main theorem shows that policy performance in the real environment is nondecreasing under certain assumptions. The value V \u03c0 k ,M converges to some V\u03c0 ,M as k \u2192 \u221e, with a finite sample complexity result possible. The proof of Theorem 3.1 shows that under certain conditions, the algorithm can achieve an approximate local maximum in a polynomial number of trajectories, with sample complexity that is logarithmic in smoothness parameters. The proof involves equations and convergence arguments to establish the optimality of the policy. In this section, we design discrepancy bounds to satisfy requirements (R1), (R2), and (R3) for a deterministic dynamical model M. A discrepancy bound D is derived to compare model predictions, showing the norm as a better metric than mean-squared error for learning the model. The telescoping lemma is introduced in the derivation process. The derivation introduces a telescoping lemma and assumes the value function is L-Lipschitz. The discrepancy between real and estimated values can be penalized during training. An upper bound for the prediction error is shown, but it cannot serve as a discrepancy bound due to optimization requirements. The main proposition in this subsection discusses replacing the distribution \u03c1 \u03c0 with a fixed distribution \u03c1 \u03c0ref for policies close to a reference policy \u03c0 ref. The expected KL divergence between two policies is used to define the neighborhood, with the second term in the equation being dominated by the first when the neighborhood size is small. The text discusses the discrepancy between two models, using a telescoping lemma to decompose the difference. The proof is similar to previous expansions, but applied to model discrepancies. The detail is deferred to Section B. The discrepancy between models is discussed, with Propositions 4.1 and 4.2 following from Lipschitzness. Limitations of norm-based bounds are highlighted, showing dependence on state representation and the non-identifiability of transformations from rewards alone. The discrepancy between models is addressed by proposing a discrepancy bound based on the difference between imaginary rewards. This bound aims to overcome limitations related to state representation and the non-identifiability of transformations from rewards alone. The proposed approach involves defining \u03b5 1 and \u03b5 max as averages of certain values and constructing a discrepancy bound using the absolute value of G. The proof is derived from the telescoping lemma. The discrepancy bound D is derived from the telescoping lemma and can be optimized using back-propagation through time. The second term in the equation is challenging to optimize but can be considered a second-order term. Proposition 4.4 provides insight into the technical reasons behind the model-based approach. Model-based reinforcement learning is more sample-efficient than policy gradient algorithms like TRPO or PPO due to decreasing approximation error with model accuracy or neighborhood size, allowing for faster convergence. The model can be accurate in a descent neighborhood of the current policy, eliminating the need for certain constraints. Model-based reinforcement learning is more sample-efficient than model-free algorithms and has been successfully applied to robotics using various dynamical models such as Gaussian processes, time-varying linear models, mixture of Gaussians, and neural networks. Kurutach et al. (2018) used an ensemble of neural networks to learn the dynamical model, reducing sample complexity compared to model-free approaches. Further improvements have been made by using a probabilistic model ensemble. In contrast to previous work on model-based reinforcement learning using probabilistic model ensembles, our approach focuses on theoretical understanding and algorithm design. We use a single neural network to estimate the dynamical model and introduce a discrepancy bound related to the value-aware model loss. Our method differs in using the absolute value of the value difference, the imaginary value function, and an iterative algorithm that converges to a local maximum. Our method focuses on combining model-free and model-based ideas to optimize performance in non-linear dynamical systems. It estimates models iteratively based on trajectory samples, aiming to maximize a lower bound of the reward. Our work focuses on model-based reinforcement learning in continuous and high-dimensional state spaces. We introduce a variant of model-based RL algorithms called Stochastic Lower Bound Optimization (SLBO), which removes constraints to optimize performance. In the context of model-based reinforcement learning in continuous and high-dimensional state spaces, a variant of RL algorithms called Stochastic Lower Bound Optimization (SLBO) is introduced. Constraints are removed to optimize performance, and a multi-step prediction loss using 2-norm is utilized for learning models. The loss function optimized includes a tunable parameter \u03bb and a stop gradient operation. The algorithm alternates between maximizing V \u03c0 \u03b8 ,sg( M \u03c6 ) and minimizing L (H) \u03c6 for optimization. The main advantage over standard model-based RL is the alternating updates of the model and policy within an outer iteration, introducing stochasticity and improving performance. The algorithm introduces stochasticity through alternating updates of the model and policy, reducing overfitting similar to SGD regularization. An ensemble of models is obtained at different inner iterations. Entropy regularization is applied to boost performance, with an additional entropy term added to the objective function in TRPO. The entropy bonus in TRPO aids exploration and prevents overfitting by diversifying data. Balancing nmodel and npolicy is crucial, with larger values often optimal due to complex interactions. Stochasticity can be achieved through extreme hyperparameter choices, enhancing the lower bound optimization naturally. Our algorithm SLBO is evaluated on five continuous control tasks from rllab, including Swimmer, Half Cheetah, Humanoid, Ant, Walker. Environments with longer horizons are harder to train. Comparison with 3 other algorithms: Soft Actor-Critic (SAC), Trust-Region Policy Optimization (TRPO), and Model-Based TRPO. Model-Based TRPO algorithm outperforms baseline algorithms in convergence rate and final performance with 1M samples. SLBO-MSE performs worse than SLBO on four environments. Ablation study of multi-step model training is also conducted. The novel algorithmic framework for designing and analyzing model-based RL algorithms guarantees convergence to a local maximum of the reward. Experimental results show that the proposed algorithm SLBO achieves state-of-the-art performance on mujoco benchmark tasks with limited samples. The trade-off between optimism and robustness is crucial for designing sample-efficient algorithms. The optimism-driven part of the meta-algorithm may lead to optimization instability and not necessarily improve performance. The text discusses the challenges of optimism-driven approaches in optimization and the need for practical implementation. It also mentions the theoretical limitations of discrepancy bounds and proposes using \u03c72-divergence as a closeness measure in the policy space to address these limitations. The text introduces a re-weighted distribution \u03b2 \u03c0 to optimize locally around the reference policy, using \u03c72-divergence as a measure of closeness in the policy space. It defines discrepancy bounds and mentions the relevance of these tools in analyzing model-free reinforcement learning algorithms like TRPO. The text discusses model-free reinforcement learning algorithms such as TRPO, PPO, and CPO. It presents a proof of Lemma 4.3 involving cumulative rewards and telescoping sums. Additionally, it introduces Lemma C.1 regarding deterministic models and policies. Lemma C.1 discusses deterministic models and policies, presenting transformed model, reward, and policy sets. It also introduces the value function and state-to-state transition kernels under different policies. The \u03c72-divergence between states and actions is analyzed, providing bounds for the difference between policy distributions. In this subsection, Proposition 4.1 is proven by controlling the difference between policy distributions using various inequalities and definitions. The proof involves setting expectations over randomness to complete the argument. The proof in this subsection involves bounding the difference of distributions induced by two Markov processes starting from the same initial distribution \u00b5. By defining transition kernels P and P, we can analyze the discounted distribution of states visited by the Markov process. The goal is to bound the difference between expectations of functions under these distributions using divergence measures. The proof in this subsection involves bounding the difference of distributions induced by two Markov processes starting from the same initial distribution \u00b5. Transition kernels P and P are defined to analyze the discounted distribution of states visited by the Markov process, aiming to bound the difference between expectations of functions under these distributions using divergence measures. The lemma characterizes the \u03c72 divergence between distributions \u1e20\u00b5 and \u1e20\u00b5, providing a refined analysis with special structured distributions. Lemma D.4 states the main result of the subsection, providing a proof for equation (D.2) by bounding the difference between distributions induced by two Markov processes. Lemma D.5 extends this result further, offering a stronger analysis that can potentially improve Proposition A.2. The text discusses the proof of the Kullback-Leibler (KL) divergence between two distributions being bounded by the \u03c72 distance. The \u03c72 distance between transitions is defined for two transition kernels P, P, and for any distribution \u00b5. The text discusses the proof of the Kullback-Leibler (KL) divergence between two distributions being bounded by the \u03c72 distance. It includes algebraic manipulation to derive equations and defines policies and transition matrices under different policies. The algorithm is benchmarked on six tasks using a physics simulator Mujoco. Contact information is removed from observations, and the reward function is adjusted. In our case, the reward function coefficients are set to 0. Actions are clipped and normalized. The policy produces actions in a specific range and the dynamical model uses a neural network with specific parameters for optimization. The policy network optimizes the loss function with learning rate 10^-3 and L2 regularization 10^-5. It predicts the normalized difference of states instead of the next state directly. The network has two hidden layers with 32 units each, using tanh activation function. The algorithm runs for 100 iterations, collecting 10,000 real samples at the start of each iteration. The algorithm collects 10,000 real samples at the start of each iteration using current policy with Ornstein-Uhlunbeck noise. It optimizes dynamics model and policy alternatively for 20 times, with hyperparameters fixed as in OpenAI Baselines' implementation. The value function uses a neural network with 2 hidden layers of 64 units and tanh activation functions. Generalized Advantage Estimator (GAE) is used to estimate advantages. The algorithm uses Generalized Advantage Estimator (GAE) BID18 to estimate advantages and shares hyperparameters with TRPO. It collects samples from the environment, optimizes \u03c6 with Adam, and runs TRPO on collected samples. Hyperparameters like multi-step model training parameter H, entropy regularization coefficient \u03bb, and policy iterations are tuned on Ant environment. Best parameters found are H = 2, \u03bb = 0.005, n policy = 40. Other hyperparameters are not tuned. Overfitting to the learned model is observed initially, so a reduction in policy iterations is made. The policy initially overfits to the learned model, so reducing the number of policy iterations at the beginning can speed up convergence. The key hyperparameters found are the number of policy iterations (n policy) and the entropy regularizer coefficient (\u03bb). Normalizing the state variables is crucial to prevent domination by large entries in the loss function. Multi-step model training is compared to single-step training, showing that smaller values for the multi-step parameter (H) can be beneficial. The text discusses the benefits of using smaller values for the multi-step parameter (H) in model training. It compares single-step and multi-step training, highlighting that smaller H values can help the model learn uncertainty and address error propagation. The study includes an ablation study on multi-step model training, showing the impact of real samples on the return from the learned policy. Entropy regularization can improve sample efficiency and final performance in training. SLBO outperforms SAC and MF-TRPO in various environments with limited samples. However, SAC surpasses SLBO after 2 million training steps. In optimizing the environment, SLBO may not surpass TRPO's performance at 8M steps. SLBO shows good asymptotic convergence and suggests a better policy optimizer than TRPO may be needed for further improvement. Theorem 3.1 is extended to a final sample complexity result, replacing expectations with empirical samples for optimization. The discrepancy bound D \u03c0ref (\u03c0, M) is assumed to be bounded and Lipschitz in parameters. The text discusses optimizing parameters \u03c0 and M, focusing on approximate local maximums and sample complexity bounds. The bounds scale linearly in p and logarithmically in L f, B, and B f. The text also introduces a definition of (\u03b4, \u03b5)-local maximum and presents Theorem G.2 with a sample complexity result. Theorem 3.1 states that using n = O(B f p log(BL f /\u03b5)/\u03b5^2) trajectories to estimate the discrepancy bound in Algorithm 1 will lead to an increase in total reward if \u03c0 t is not a (\u03b4, \u03b5)-local maximum. With high probability, the total reward will increase in the next step. Additionally, if the maximum possible total reward is B R and the initial total reward is 0, then for some T = O(B R /\u03b5), \u03c0 T will be a (\u03b4, \u03b5)-local maximum of the V \u03c0,M. This is proven using Hoeffding's inequality and a standard \u03b5-cover + union bound argument. The proof shows that if policy \u03c0 t is not a (\u03b4, \u03b5)-local maximum of V \u03c0,M, then after O(B R /\u03b5) iterations, a solution that is a (\u03b4, \u03b5)-local maximum will be found."
}