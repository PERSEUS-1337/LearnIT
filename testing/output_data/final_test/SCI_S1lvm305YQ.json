{
    "title": "S1lvm305YQ",
    "content": "In this work, TimbreTron is introduced as a method for musical timbre transfer, utilizing image-based style transfer techniques on a time-frequency representation of an audio signal. The Constant Q Transform (CQT) representation is highlighted for its suitability to convolutional architectures. TimbreTron is shown to produce high-quality waveforms using a conditional WaveNet synthesizer, confirmed through human perceptual evaluations. TimbreTron is a method for musical timbre transfer using image-based style transfer techniques on audio signals. Timbre is a perceptual characteristic that distinguishes musical instruments, with a nonlinear dependence on volume, time, and playing technique. Modeling timbre is challenging due to its multidimensional nature. In this paper, the focus is on high-quality timbre transfer between audio clips from different instruments, inspired by style transfer techniques for images using neural networks. The goal is to change the timbre of a musical recording to match reference recordings while preserving pitch and loudness. Recent advancements in audio generation methods have led to the development of techniques like WaveNet, SampleRNN, and Tacotron2, which can directly generate high-quality waveforms. These methods are particularly useful for manipulating high-level auditory representations and have shown promise in creating high-quality audio without the need for converting spectrograms into waveforms. TimbreTron is a pipeline for CQT-based timbre transfer in music, using high-quality waveform output. It is trained on unrelated samples of two instruments, utilizing the constant Q transform (CQT) for time-frequency representation. This representation is well-suited for musical timbre transfer due to its pitch equivariance and high frequency and temporal resolution properties. TimbreTron performs timbre transfer using a CQT spectrogram, CycleGAN, and WaveNet synthesizer. It successfully transfers timbre between instruments while maintaining musical content. The use of CQT representation is crucial for better timbre transfer compared to STFT. The TimbreTron pipeline performs timbre transfer from Violin to Flute using Constant Q Transform (CQT) for better results than STFT. Time-frequency analysis measures frequency domain changes over time, with STFT being commonly used. The STFT operation computes the Fourier transform of a masked signal. Constant Q Transform (CQT) is another time-frequency analysis technique. The Constant Q Transform (CQT) is a time-frequency analysis technique with geometrically spaced frequency values. It outperformed Mel-frequency cepstral coefficients (MFCCs) in sound classification tasks. Rainbowgram is a visualization of CQT that highlights subtle timbral features using color to encode time derivatives of phase. The Griffin-Lim algorithm is a common method for generating phase from STFT magnitude in time-frequency analysis. It iteratively refines phase values by performing STFT and inverse STFT operations until convergence, minimizing the mean squared error between target and predicted spectrograms. WaveNet is an auto-regressive generative model for raw audio waveform generation with high quality, using dilated causal convolution layers. It can be modified for conditional waveform generation, such as synthesizing human speech in TTS systems. One limitation is the expensive generation of waveforms, which is not ideal for certain training procedures. Generative Adversarial Networks (GANs) are implicit generative models introduced by BID15, consisting of a discriminator and a generator trained adversarially. GANs have advanced the quality of generated samples and CycleGAN is an architecture for unsupervised domain transfer between two domains without paired data. The CycleGAN architecture, proposed by Yi et al. (2017), consists of two generators and two discriminators. It learns mappings between two domains and enforces cycle consistency. In music processing, the focus is on transforming raw waveforms to Constant-Q-Transform representations using a conditional WaveNet synthesizer. The CQT representation is chosen for its suitability in processing musical audio signals. The Constant-Q-Transform (CQT) is used for processing musical audio signals, providing high frequency resolution for lower frequencies and high time resolution for higher frequencies. It allows for simultaneous recovery of pitch and timing information for individual notes, making it advantageous for instruments like cello or trombone. The CQT representation in TimbreTron also exhibits pitch equivariance. The CQT representation in TimbreTron shows pitch equivariance due to the geometric spacing of frequencies, allowing convolutional architectures to share structure between different pitches. Pitch shifts correspond to vertical translations of the spectral signature of musical instruments, demonstrating approximate equivariance. Imperfect multiples in real audio samples cause this equivariance to be only approximate. Instruments produce sound with harmonics that are not exact multiples of the fundamental frequency. Each instrument has a unique spectral signature for each pitch, which can vary based on how it's played. Phase information is discarded in image-based processing on a log-amplitude CQT representation, requiring inference of missing phase information to generate a consistent waveform. To convert log magnitude CQT spectrograms back to waveforms, a 40-layer conditional WaveNet with a dilation rate of 2k is used. The model is trained using pairs of CQT and waveform data, allowing reconstruction of audio samples. However, the WaveNet sometimes produces low-probability outputs and struggles with modeling local loudness variations. The WaveNet architecture aims to match the target CQT waveform by using a beam search to improve output quality. Percussive attacks are challenging to model due to difficulties in determining note onsets from blurred frequency information in CQT spectrograms. The WaveNet architecture addresses challenges in modeling percussive attacks by generating waveform samples in reverse order. The TimbreTron pipeline focuses on timbre transfer using unsupervised image-to-image translation with CycleGAN architecture adapted for audio representations. The convnet-resnet-deconvnet generators in CycleGAN caused checkerboard artifacts in the generated CQT, leading to noise in the waveform. To address this, deconvolution was replaced with nearest neighbor interpolation and regular convolution. The full-spectrogram discriminator was used to ensure consistency in different partials of the same pitch, as a local discriminator in frequency could not enforce this. Replacing the patch discriminator with the full-spectrogram one resulted in unstable training dynamics due to Gradient Penalty (GP). The addition of Gradient Penalty (GP) to the full-spectrogram discriminator helped stabilize GAN training dynamics. Identity loss was also incorporated to preserve color composition in the original CycleGAN, improving generator performance. The identity loss component in CycleGAN helps generators preserve music content for better audio quality. The weighting of identity loss follows a linear decay schedule, encouraging the generator to learn pitch preservation initially and then allowing for more expressive mappings as training progresses. Various details of the CycleGAN architecture, training, and generation methods are provided in Appendices C.2, C.3, and C.6. Previous works have used clever representations to manipulate images or audio signals, such as separating style and content in images and applying optimization techniques to the audio domain. BID43 applied optimization techniques from BID13 to the audio domain using image-based architectures on spectrogram representations. BID17 used hand-crafted features on spectrograms, but the disentanglement of timbre and performance control information remains unsolved. Zhu et al. introduced Cycle GAN for unsupervised image-to-image mapping, which BID23 used for human voice translation and BID5 for musical style transfer with MIDI. The commonality among these audio style transfer approaches is the focus on reconstruction quality. The aforementioned audio style transfer approaches focus on reconstruction quality. Strategies include using non-parametric algorithms like GriffinLim and WORLD vocoder, operating directly on waveforms with models like WaveNet and GANs for synthesizing waveforms with improved quality. Other approaches involve encoder-decoder models for Timbre Transfer. In a parallel work, BID2 used their MoVE model for many-to-many timbre transfer with a focus on Maximum Mean Discrepancy (MMD) as their objective. Our TimbreTron model, on the other hand, utilizes a GAN-based training objective for higher perceptual quality outputs. Two sets of experiments were conducted to validate our CQT representation choice and test the full TimbreTron pipeline, along with ablation experiments to confirm our architectural choices. See Appendix C for experimental setup details. Training TimbreTron involves using collections of unrelated recordings of source and target instruments, such as Piano, Flute, Violin, and Harpsichord. The datasets were divided into training and test sets, ensuring they were disjoint in terms of musical content. The training dataset was split into 4-second chunks for processing by CycleGAN and WaveNet. Links to source audio and dataset details can be found in Appendix C. In the context of the TimbreTron pipeline, pitch and tempo can be disentangled using the CQT representation. Pitch shifting is achieved by translating the CQT representation on the log-frequency axis, while audio time stretching can be done using either CQT or STFT representations with the WaveNet synthesizer. The WaveNet synthesizer can produce correct pitch based on local frequency content, allowing for independent pitch and tempo variation while preserving timbre and musical structure. Experiments on timbre transfer use real-world music recordings and synthetic MIDI data. Comparisons between CQT and STFT representations were made in the TimbreTron pipeline design. The STFT-based pipeline in TimbreTron had issues with transferring low pitches accurately and sometimes produced random pitch permutations. These problems were resolved by using the CQT representation, which provided better pitch equivariance and frequency resolution at low frequencies. Both WaveNet and Griffin-Lim algorithms were affected by these artifacts. The CQT representation in TimbreTron effectively addressed issues with low pitch accuracy and pitch permutations in the STFT-based pipeline. Both WaveNet and Griffin-Lim reconstruction methods were affected by these artifacts, indicating that they likely originated from the CycleGAN stage. Generalization from MIDI to real-world audio was successful, showcasing the ability to preserve pitch and transfer timbre. The study investigated if TimbreTron could transfer timbre while preserving musical content. Results are shown in TAB2 and 4, with detailed discussion. Comparison experiments were conducted for instrument and musical piece similarity. The study conducted comparison experiments on instrument and musical piece similarity. Results are shown in TAB0 for instrument similarity and TAB2 for music piece similarity. Respondents also provided subjective judgments on the instrument used for the samples. The study found that a different instrument playing the same notes may not always sound like the same piece due to adaptations made in musical contexts. The study evaluated instrument and musical piece similarity. Results in TAB0 showed that even with a target domain recording, notes and timings were not always identical. For instrument comparisons, 67.5% found them nearly identical, 22.5% related, and 10% different. The study also assessed timbre transfer challenges due to imperfect transfers. Participants were able to identify instruments in audio excerpts, often confusing similar-sounding instruments. For example, a harpsichord was mistaken for a banjo due to their similar timbre. Similar confusions were also observed with identifying ground truth instruments. Participants were able to identify instruments in audio excerpts, often confusing similar-sounding instruments. One participant described a real harpsichord as a sitar. TimbreTron can transfer timbre while preserving musical content. AMT results compared TimbreTron with different methods. A human study compared TimbreTron with CQT representation to STFT-Wavenet and STFT-GriffinLim. The study compared TimbreTron with different methods for generating audio samples. Results showed that the CQT TimbreTron was preferred over STFT-Wavenet and STFT-GriffinLim, indicating an improvement in timbre quality with training on STFT. The study compared TimbreTron with different methods for generating audio samples, showing that training on STFT improved timbre quality marginally. TimbreTron trained on CQT demonstrated significantly better timbre quality. An ablation study was conducted to justify modifications made to the original CycleGAN for MIDI CQT experiment. TimbreTron is a pipeline for high-quality timbre transfer on musical waveforms using CQT-domain style transfer, reconstructing inputs with WaveNet. The CQT is well suited for convolutional architectures due to its pitch equivariance. The MIDI-trained CycleGAN showed generalization to real-world music. TimbreTron transfers timbre while preserving musical content. Pitch is the \"height\" of a musical tone tied to the fundamental frequency. Loudness is linked to sound pressure perception. The perception of sound pressure is linked to loudness, described as the \"intensity\" of a tone. Timbre distinguishes between instruments with the same pitch and loudness based on energy spectrum and envelope. Instruments produce harmonics, observed in spectrogram visualizations. The timbre of an instrument is closely related to the harmonics' strengths, which can be observed in FIG1. The spectral signature of an instrument is influenced by the pitch and changes over time. The envelope of a tone is affected by the attack time, decay/sustain, and release, adding complexity to the sound and making it challenging to model. The complexity and richness of an instrument's sound make it difficult to model explicitly. CQT spectrogram can be computed from time-domain waveforms with specific parameters. STFT spectrograms are generated using a Hann Window with a frame hop of 16 ms. The MIDI dataset includes MIDI-BACH and MIDI-Chopin with 6 instruments, while the Real World Dataset consists of recordings from YouTube videos of solo performances on various instruments like piano, harpsichord, violin, and flute. The Real World Dataset includes recordings from YouTube videos of solo performances on instruments like piano, harpsichord, violin, and flute. The conditional wavenet used a kernel size of 3 for dilated convolution layers, with residual and skip connections having a width of 256. Training parameters included a learning rate of 0.0001, batch size of 4, and sample length of 8196. A moving average of network weights was maintained to improve generation quality. During autoregressive generation, the model uses averaged weights with a decaying factor of 0.999. The training dataset is augmented by randomly rescaling the waveform and adding a constant shift to the spectrogram. A modified beam search is performed to minimize the discrepancy between target and synthesized spectrograms. The algorithm extends candidate waveforms by pruning those with large errors using a beam search heuristic. A constant number of candidates is maintained by replicating remaining waveforms. Extra prediction steps are taken to align the local beam search with the global objective. In earlier attempts, generating 4-second segments resulted in volume inconsistencies. During test time, the CycleGAN generator can now generate music based on input of arbitrary length, resolving volume inconsistencies. The dataset is no longer in 4-second chunks, preserving the original length of the musical piece. The entire piece is fed into the generator at once, with spectrograms centered at -2. Global normalization is based on the distribution of spectrograms for each domain. \"We globally normalized input data based on spectrogram distribution for each instrument domain.\""
}