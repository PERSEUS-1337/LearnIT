{
    "title": "rJlpUiAcYX",
    "content": "We introduce an analytic distance function for point sets of known cardinality with desirable properties for machine learning applications. Comparing it to other distance functions, we conduct proof of concept experiments for training neural networks on point set prediction tasks like object detection. The text discusses the importance of defining a meaningful loss function for machine learning systems that predict sets of points, emphasizing the need for permutation invariance. It introduces an analytic distance function for point sets with desirable properties for object detection tasks. The text emphasizes the need for a loss function that is permutation-invariant when defining a meaningful distance between sets of points in machine learning. It introduces the concept of representing unordered sets as ordered tuples and defines the properties required for the loss function to be invariant to arbitrary ordering. Permutation-invariant loss functions are crucial for defining distance measures between sets of points in machine learning. The Hungarian loss, proposed for object detection, utilizes the Hungarian algorithm to find the best matching between target and prediction points. This approach represents a significant advancement in developing permutation-invariant loss functions for training machine learning models. The Hungarian loss function is a permutation-invariant loss function used for training machine learning models. While intuitive and well-conditioned, it has drawbacks such as undefined gradients at transition points with multiple optimal permutations. The computational complexity of solving the assignment problem for each evaluation is problematic in modern machine learning contexts. BID2 proposes using a neural network to jointly model set elements and their permutations, leading to an alternating optimization procedure. In this work, BID4 discusses neural network architectures that operate on sets in a permutation-invariant fashion. They propose a general characterization of permutation-invariant functions and use this insight to design neural network architectures for sets. The paper focuses on investigating properties of point set distance functions, assuming the cardinality is known. Future work could integrate these loss functions with cardinality learning procedures to replace the inner-loop solution of an optimal assignment problem. In this work, alternative permutation-invariant point set distance functions with O(N^2) running time are considered. These functions do not rely on solving an assignment problem and are suitable for gradient-based numerical optimization. A novel distance function called \"holographic loss\" is proposed, which encodes the structure of point sets in a metric distance on fingerprints. It has favorable properties and is analytic everywhere. The loss function discussed in the current text chunk has favorable properties, including a diagonal Hessian at the minima and global minima that correspond to exactly matched point sets. It also has a natural generalization to multi-sets. Proof of concept experiments on object detection using MNIST digits show that end-to-end training with this loss function is effective for point set prediction tasks. The one-sided Hausdorff distance is highlighted as a second-order loss function for finite point sets. The symmetric Hausdorff distance, denoted by L H,2, is easily computed with O(N^2) effort and is almost everywhere differentiable. It serves as a machine learning loss function but has properties that may not be ideal in this context. The Hausdorff distance is determined by a single pair of target and prediction points, leading to issues with gradient descent steps adjusting only one point at a time. This can result in subsequent steps jumping around and keeping almost-tied point pairs almost-tied. This behavior may be undesirable compared to loss functions that collectively move all points. A possible remedy to this problem is needed. The symmetric Hausdorff distance, denoted by L H,2, is easily computed with O(N^2) effort and is almost everywhere differentiable. It serves as a machine learning loss function but has properties that may not be ideal in this context. Loss functions that collectively move all points are preferred over those that adjust only one point at a time. A possible remedy is to sum the minimal distances-squared instead of taking their maximum. One-sided variants of distance functions are not considered in the comparison due to limitations. The focus is on symmetric loss functions L H and L SMD. In the next section, a construction for permutation-invariant loss functions is described. A fingerprint vector is mapped from tuples representing a point set, ensuring invariance under permutations. The fingerprint function based on moments of points is proposed as a universal permutation-invariant fingerprint. The squared Euclidean distance of moments vectors can be used as a loss function. In this section, the focus is on two-dimensional point sets, with a special case for one-dimensional sets. Points are identified as complex numbers, and tuples representing sets are mapped to complex polynomials. Monic polynomials of degree N are uniquely determined by their values at N distinct points. A polynomial of degree N is uniquely determined by its values at N distinct points. Choosing a set of evaluation points and defining a fingerprint using the L2 distance gives a \"holographic\" loss function. The distance-squared function is not holomorphic due to the anti-linearity of the complex scalar product. The term 'holographic' refers to the function depending collectively on all points. Matching candidate and target points involves matching complex amplitude and phase at evaluation points. The holographic loss function involves matching complex amplitude and phase at evaluation points, with properties such as symmetry and quadratic behavior. In practical applications, the squared Euclidean distance between points is used as a simpler loss function. The target-centered version of the loss function involves summing squared distances to prediction points for each target point. It is not symmetric like Eq. FORMULA9 but inherits properties of Proposition 1. There is no higher-dimensional equivalent of the polynomial construction in Eq. (6) due to the lack of a generalization of the Fundamental theorem of Algebra. However, Eq. (10) can be heuristically extended to point sets of arbitrary dimension. The gradient of the Holographic loss with an arbitrary set of evaluation points can be computed using complex backpropagation of errors. It is a weighted sum of vectors that pulls the prediction towards the target point, with the weight proportional to the distance between the target point and other prediction points. The Holographic loss gradient adjusts all points simultaneously, inducing dynamics. It has non-convex global minima and additional stationary points. For D=2, non-optimal stationary points are not local minima. The proof relies on complex polynomials and is specific to two-dimensional point sets. The Holographic loss near optima is characterized up to second order in Proposition 3, showing that ill-conditioning can occur if pairwise distances between points have different scales. Minimizing the loss is not a problem even for sets with around 40 randomly sampled points, as demonstrated in Appendix B. The Holographic loss is not a problem for sets with around 40 randomly sampled points due to the diagonal property of the Hessian at minima. The eigendirections of the Hessian align with coordinate directions, allowing for high precision representation of gradients. Different loss functions are applied to object detection tasks to demonstrate end-to-end training viability without explicitly modeling permutations or solving optimal assignment problems. Object detection is a key task in computer vision, predicting object locations in an image. Current approaches like R-CNN and YOLO use engineered systems to detect objects, rather than treating it as a point set prediction task. In object detection, systems like R-CNN and YOLO use pre-specified object bounding boxes with confidence scores. Post-processing steps like non-maximum suppression are involved. This study aims to show that end-to-end learning of object locations is possible using point set loss functions. A simple object detection dataset, MNIST-DETECT, is created with fixed number of MNIST digits in each image. Images are generated by sampling digits, cropping them, and placing them in random locations with added noise. The study uses a simple convolutional neural network to predict the location of digits in images from the MNIST-DETECT dataset. The network has three convolutional layers and two fully-connected layers with ReLU activation. Separate experiments are conducted for predicting center locations and bounding boxes. The network is trained using various loss functions and the Adam optimizer. The study uses a convolutional neural network with ReLU activation to predict digit locations in images from the MNIST-DETECT dataset. The network is trained with different loss functions and the Adam optimizer, with a fixed number of epochs and mini-batch size. Evaluation is done on a validation set, and the best weights are retained. The step size is tuned via grid search. Results are evaluated using Hungarian loss for center point prediction and detection rate for bounding box prediction. The neural network successfully learns the task with all three loss functions. The study successfully uses a simplistic CNN architecture to predict digit locations in images from the MNIST-DETECT dataset. Novel point set loss functions are discussed as alternatives for point set prediction tasks, showing that end-to-end training with simple loss functions is viable for tasks like object detection. The \"holographic\" point set distance introduced may have broader applications beyond point set predictions. The Lemma states that roots of a complex polynomial depend continuously on its coefficients. The Proposition is proven by showing that a certain condition implies a specific relationship between two sets. The text discusses the construction of a polynomial Q \u03bb that interpolates between two polynomials, and shows that certain conditions imply a relationship between sets of roots. The text discusses the construction of a polynomial Q \u03bb that interpolates between two polynomials and the simplification of the Hessian matrix at an optimum. The text explains how to compute gradients using Wirtinger calculus for a complex-differentiable loss function. It discusses sensitivities and the simplification of computations using complex differentiability. The text discusses how to link real sensitivities with complex sensitivities using Wirtinger calculus for a complex-differentiable loss function. It also mentions using moments of point sets as a permutation-invariant fingerprint. The fundamental permutation-invariant set fingerprint can be written as \u03c1(m Z) for symmetric functions on sets of N real numbers. However, constructing a practical permutation-invariant loss function faces challenges due to numerical conditioning issues. Simply using L2-distance between moments fingerprints has problems matching moderately-sized real point sets. Ways to compensate for distortion effects on low-order moments beyond \u223c 7 points exist, while maintaining high-order moments accuracy. The \"holographic\" loss function, when expressed in terms of polynomial coordinates, simplifies numerical problems by being a quadratic function for each input-coordinate separately. This approach reliably matches up point sets with more than 40 randomly sampled points, avoiding issues with ill-conditioned higher-degree polynomials. The \"holographic\" loss function can be generalized by allowing weights for contributions from different evaluation points to minimize discrepancies in the Hessian. Extending moments-based loss to higher dimensions is not straightforward. Using N \u00b7 D moments for every coordinate may not provide a proper set fingerprint. Moments for j k j \u2264 N are a more suitable choice, but scaling unfavorably with N may lead to numerical problems persisting in higher dimensions. In investigations, a fingerprint using moments up to total scaling dimension N in D = 2 is confirmed. Experiments show the holographic loss can match sets with more than 40 randomly sampled points using 64-bit floating point numerics. Figure 5 displays instances of matching sets of five two-dimensional points, with markers indicating time steps according to the ODE solution. The holographic loss function can match sets with more than 40 points using 64-bit floating point numerics. Gradient flows show how points are identified and moved in a meaningful way, allowing for information to be passed to earlier layers in ML training. In complex cases, multiple points may travel together until the degeneracy is resolved."
}