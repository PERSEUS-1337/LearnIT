{
    "title": "SJg1lxrYwS",
    "content": "Learning rich representations from predictive learning without labels has been a challenge in machine learning. The PatchFormer neural architecture is proposed for self-supervised representation learning on raw images, modeling spatial dependencies across patches. The method learns the conditional probability distribution of missing patches given surrounding patches. The utility of the learned representations is evaluated by fine-tuning on low data-regime classification tasks, achieving 30.3% and 65.5% top-1 accuracies on semi-supervised ImageNet classification. Deep neural networks can learn rich abstract representations from raw data without relying on abundant labeled datasets. Self-supervised and unsupervised learning approaches aim to utilize large unlabeled datasets to improve data-efficiency in supervised learning systems. Successful examples include word2vec, which demonstrates learning distributed vector representations of words through unsupervised learning. Authors demonstrated that learning distributed vector representations of words can be achieved by predicting neighboring words from surrounding words. The transition from word embeddings to sequence embeddings was initiated by Dai & Le (2015) with pre-trained sequence to sequence autoencoders, proving beneficial for tasks like text classification and sentiment analysis. Peters et al. (2018) further emphasized the utility of language modeling for generating deep contextual sentence embeddings that can be fine-tuned for various natural language understanding tasks. The success of Howard & Ruder (2018) also exemplifies this trend. Recently, the transformer architecture (Vaswani et al., 2017) has emerged as a powerful tool for modeling complex dependencies in long sequences using global self-attention. OpenAI's Generative Pre-Training (GPT) (Radford et al., 2018) showcased the effectiveness of training large Transformer models. The success of pre-training large Transformer models on BooksCorpus for language understanding tasks was demonstrated by GPT (Radford et al., 2018). BERT (Devlin et al., 2018) further advanced unsupervised pre-training by utilizing bi-directional transformers for masked language modeling and next sentence prediction. The question arises on how to apply these successes to images, considering language as a means of abstraction for understanding and communication. Images are raw sensory observations, making it challenging to model the relationship across pixels due to high dimensionality. While generative approaches are believed to be more suited for abstract inputs like language, exploring their potential for pre-training on images, such as with BiGAN, shows promise. Recent advancements in generative modeling have shown success in learning high-level representations of raw images using adversarial methods like BiGAN. These techniques involve modeling the conditional distribution of pixels to generate diverse images and audio waveforms. WaveNet and other models have demonstrated the effectiveness of this approach in generating future frames of videos and audio waveforms conditioned on text. Generative models have shown success in learning high-level representations of raw images using adversarial methods like BiGAN. Recent advancements include using strided self-attention for high-quality unconditional samples of ImageNet. The question arises about leveraging these successes for representation learning and designing a representation learning approach for images inspired by natural language processing and generative modeling successes. State-of-the-art systems for representation learning on images currently use contrastive methods. Contrastive methods, such as Contrastive Predictive Coding (CPC), have been shown to be powerful for representation learning across multiple modalities. Impressive results have been achieved in linear classifier probe metrics and semi-supervised image classification using these methods. However, high-quality generative approaches are still in development. In this paper, the focus is on exploring architectures that incorporate self-attention for generative pre-training on images. Stand-Alone Self-Attention has shown promising results in matching convolutional architectures for image classification and object detection. The goal is to draw inspiration from successful engineering decisions to design a new generative pre-training method for images. The design of a generative pre-training method for images involves predicting subscales and low-bit depth for pixels, focusing on the most significant bits for representation learning. Self-attention is used for aggregating global context in images, inspired by successful engineering decisions in incorporating self-attention for generative pre-training. (Wang et al., 2018) utilized non-local layers for activity recognition, while (Zhang et al., 2018) and (Brock et al., 2018) applied non-local layers for high-fidelity image generation. Self-attention has been effective for modeling distribution of latents in likelihood-based image generation. Examples of self-attentive density models include (Parmar et al., 2018; Menick & Kalchbrenner, 2018; Child et al., 2019). Learning spatial dependencies across patches is demonstrated by CPC, Image Transformers, and PixelCNNs. Modifications to CPC for image representation learning involve patch-based data extraction and modeling. PatchFormer is a new architecture that modifies CPC for image representation learning by utilizing patch-based data extraction and modeling dependencies in a BERT-like fashion using self-attention. It can decode missing patches in an image by extracting representations of given patches, using attention-pooling to aggregate context, and decoding low-bit grayscale sub-sampled versions of the missing patches. The model can be pre-trained on unsupervised decoding of missing patches and fine-tuned on low-data regime classification tasks, achieving competitive ImageNet classification results. The PatchFormer architecture modifies CPC for image representation learning by utilizing patch-based data extraction and self-attention. It extracts 16x16 patches from input images, masks 60% of them, and uses a Transformer to associate features from non-masked patches. Position embeddings are added for regularization. The PatchFormer architecture utilizes position embeddings for regularization and employs masked-self-attention with 10 layers and 16 attention heads. A pointwise MLP with GeLU nonlinearity is used, followed by a residual upsampling decoder to decode missing patches. The model is trained with AdamWeightDecay Optimizer on a v3-32 cloud TPU over 300,000 update steps. The PatchFormer architecture utilizes position embeddings for regularization and employs masked-self-attention with 10 layers and 16 attention heads. Random data-augmentations are applied to patches, including Inception Noise and random grayscale blending. A ResNet-41 convolutional neural network extracts embeddings for the patches, followed by a deep self-attentive transformer for global association across space. The PatchFormer architecture utilizes position embeddings for regularization and employs masked-self-attention with 10 layers and 16 attention heads. The self-attentive transformer globally associates patches across space with added position encodings before decoding missing patches. Subsampled and low-bit grayscale versions of missing patches are decoded for simpler prediction tasks. The decoder is removed, and attention-pooled patch embeddings are used for classification with a residual layer. Random masking of 10% of patches during fine-tuning helps regularize the model. Fine-tuning is done with the same optimizer as pre-training, using batch sizes and learning rates in specified ranges. Soft-labels are used during fine-tuning for additional regularization. The PatchFormer architecture utilizes position embeddings for regularization and employs masked-self-attention with 10 layers and 16 attention heads. It pre-trains using the unlabeled ImageNet dataset and fine-tunes for image classification on subsets of the dataset. The model incorporates dropout noise and a residual block for preventing overfitting. Soft-labels are used for regularization during fine-tuning. The proposed architecture competes with state-of-the-art contrastive pre-training methods in the low data-regime. The PatchFormer architecture competes with state-of-the-art contrastive pre-training methods like CPC on the low data-regime ImageNet classification benchmark."
}