{
    "title": "ByME42AqK7",
    "content": "Architecture search aims to find neural architectures automatically, competing with human-designed ones. Recent approaches excel in image recognition but struggle with resource constraints due to architectures optimized solely for performance and requiring vast computational resources. LEMONADE, an evolutionary algorithm, addresses these issues by approximating the Pareto-front of architectures under multiple objectives in a single run and implementing a Lamarckian inheritance mechanism for warmstarting children networks. LEMONADE generates children networks warmstarted with their parents' predictive performance using network morphism operators. It outperforms NASNets, MobileNets, and Wide Residual Networks on CIFAR-10 and ImageNet64x64 with significantly less compute power. Deep learning advancements in image recognition, speech recognition, and machine translation rely on novel neural architectures, traditionally developed manually. There is a growing interest in automatic architecture search methods due to the time-consuming and error-prone nature of manual development. Some automated architectures have surpassed manually-designed ones, but the algorithms for finding them require significant computational resources. Prior work focused on single-objective optimization, but the need for high predictive performance and low resource consumption has led to a trade-off between the two. Recently, architectures have been designed to reduce resource consumption. In this work, the focus is on automatic architecture search methods that aim to reduce resource consumption while maintaining high predictive performance. The use of network morphisms helps in preserving the function a network represents, reducing the required training time per network significantly. This mechanism can be likened to Lamarckian inheritance in evolutionary algorithms. In this work, a Lamarckian Evolutionary algorithm for Multi-Objective Neural Architecture Design, named LEMONADE, is proposed for joint optimization of various objectives like predictive performance, inference time, and number of parameters. The algorithm allows passing acquired skills to offspring and introduces approximate network morphisms to enable shrinking networks, crucial for multi-objective search. LEMONADE is a Lamarckian Evolutionary algorithm for Multi-Objective Neural Architecture Design that maintains a population of networks on the Pareto front of multiple objectives. It prioritizes evaluating cheap objectives first, then selects a subset of architectures to train and evaluate, reducing computational resource requirements during architecture search. Unlike other methods, LEMONADE does not require defining a trade-off between performance and other factors. LEMONADE is a Lamarckian Evolutionary algorithm for Multi-Objective Neural Architecture Design that does not require a trade-off between performance and other objectives upfront. It can handle various search spaces, including complex topologies, and returns a population of CNNs covering a wide range of architectures. LEMONADE is an evolutionary algorithm for multi-objective neural architecture design that can discover competitive architectures with 10,000 to 10,000,000 parameters in just 5 days on 16 GPUs. It outperforms hand-designed networks like MobileNet V2 BID22 and architectures designed with 40x greater resources. Multi-objective optimization deals with finding solutions that minimize multiple objective functions simultaneously. The Pareto-optimal solutions frame neural architecture search as a reinforcement learning problem, using a recurrent neural network to generate architecture strings. An alternative approach is neuro-evolutionary methods using genetic algorithms. This method is applicable for multi-objective optimization. Our proposed method for multi-objective optimization utilizes Lamarckian inheritance, passing learned parameters to offspring. Unlike population-based training BID9, which focuses on hyperparameter optimization, our approach could also include hyperparameter evolution. Most existing methods require significant computational resources to train and validate thousands of neural architectures. Speeding up evaluation can be achieved by predicting performance of partially trained models. One-Shot Architecture Search is a promising approach for speeding up performance estimation by treating architectures as subgraphs of a supergraph. However, a limitation is that the search space is restricted to the supergraph's subgraphs, and it may be limited to small supergraphs due to GPU memory constraints. It is unclear how one-shot models can be used for multi-objective optimization. LEMONADE can handle large, unconstrained search spaces efficiently, unlike one-shot models. It introduces approximate network morphisms for multi-objective optimization, extending the concept proposed by Elsken et al. and Cai et al. Multi-objective Neural Architecture Search has also been explored recently. Multi-objective Neural Architecture Search aims to optimize accuracy while considering resource consumption. Different approaches have been taken, such as parameterizing architectures with fixed-length vectors and using weighted product methods for scalarized objectives. LEMONADE is a novel approach for Neural Architecture Search that does not require complex macro architectures, can handle arbitrary search spaces, and does not need hard constraints or weights on objectives a-priori. It introduces network operators like network morphisms and approximate network morphisms for mutations in evolutionary algorithms. BID27 expanded on the concept of network morphisms in neural networks, defining them as function-preserving operators. These operators can be achieved by proper initialization and include inserting Conv-BatchNorm-ReLU blocks with specific initializations. The details of these operators used in LEMONADE are described in the appendix. Adding operations to modify neural networks can increase their capacity. These operations include adding blocks on top of ReLU layers, increasing convolution filters, and incorporating skip connections through concatenation or addition with a learnable parameter. These changes aim to enhance the network's capabilities. Incorporating operations to modify neural networks can increase their capacity, such as adding blocks, increasing filters, and using skip connections. An approximate network morphism (ANM) can reduce a neural architecture's resources while maintaining performance. ANM operators include removing layers or skip connections, and pruning convolutional layers. LEMONADE maintains a population of trained networks that constitute a Pareto front in the multi-objective space. Parents are selected inversely proportional to their density, and children are generated by mutation operators with Lamarckian inheritance through network morphisms. Children may incur a small increase in error compared to their parent but typically have very small initial error. Only a subset of generated children is accepted for training. LEMONADE is a Lamarckian Evolutionary algorithm for MultiObjective Neural Architecture DEsign. It aims to minimize multiple objectives, including expensive-to-evaluate ones. The algorithm maintains a population of trained networks forming a Pareto front, with parents selected based on density and children generated through mutation operators with Lamarckian inheritance. Only a subset of generated children is accepted for training. LEMONADE maintains a population of parent networks representing the Pareto front. In each iteration, parent networks are sampled based on cheap objectives to generate child networks. A subset of children is then evaluated on expensive objectives, utilizing the cheap evaluations to bias sampling towards sparsely populated areas of the objective space. This process results in a diverse set of children. LEMONADE generates a diverse set of children by sampling parent networks based on cheap objectives. A density estimator is computed on the cheap objective values of the current population to bias sampling towards sparsely populated regions of the objective space. More children are generated with similar objective values as their parents through network operators. LEMONADE uses a two-staged sampling strategy to generate and evaluate children that can fill gaps in the objective function. The Pareto front is computed from the current generation and the generated children to form the next generation. This process is repeated for a specified number of generations in experiments on searching neural architectures for CIFAR-10. LEMONADE was used to search for neural architectures for CIFAR-10 with different settings, optimizing 5 objectives for entire architectures, 2 objectives for entire architectures, and 2 objectives for cells. The discovered cells were transferred to ImageNet and ImageNet64x64. Experimental details can be found in the appendix. The progress of setting (ii) is visualized in FIG2, showing improvement in the Pareto front over time. The aim is to minimize five objectives for performance on CIFAR-10. In this experiment, LEMONADE searches for entire neural network architectures with five objectives: performance on CIFAR-10, performance on CIFAR-100, number of parameters, number of multiply-add operations, and inference time. The approach handles unconstrained search spaces, unlike other methods limited to small spaces. LEMONADE starts with basic architectures and aims to find state-of-the-art networks. The experiment ran for approximately 5 days on 16 GPUs in parallel, generating a Pareto front of around 300 neural network architectures. The operators used to generate child networks include network morphism operators and approximate network morphism operators. The resulting architectures were compared against NASNets and MobileNets V2 BID22 to ensure differences in test error were due to architectural differences. All architectures were retrained from scratch under the same conditions. The experiment retrained all architectures from scratch under the same conditions without using stochastic regularization techniques. Results show that LEMONADE performs on par with NASNets and MobileNets V2 for resource-intensive models but outperforms them in very efficient models. In terms of inference time, LEMONADE finds superior models compared to the baselines. LEMONADE outperforms NASNets and MobileNets V2 in efficient models and inference time, using only 80 GPU days compared to 2000. The study includes additional baselines and an ablation study, comparing different models with the same data augmentation and training pipeline. LEMONADE utilizes discovered cells within the ShakeShake macro architecture and ShakeShake regularization, outperforming DPP-Net across all parameter regimes. It competes with methods requiring more computational resources and shows transferability of cells to different datasets without re-running architecture search. In LEMONADE, architectures suited for ImageNet64x64 are built based on five cells discovered on CIFAR-10. Variations in the number of cells per block and filters in the last block result in different architectures. Comparisons are made against MobileNets V2, NASNets, and Wide Residual Networks (WRNs) BID29. The Pareto Front from all cells combined and from a single cell, Cell 2, outperform NASNets, WRNs, and MobileNets V2, showing the benefits of a multi-objective search. Cell 2 is also evaluated on the regular ImageNet benchmark for the \"mobile setting.\" LEMONADE, a multi-objective evolutionary algorithm for architecture search, achieved competitive results in the \"mobile setting\" with a top-1 error of 28.3% and a top-5 error of 9.6%. The algorithm utilizes network morphism operators to speed up training and could potentially perform better with optimized hyperparameters and resources. LEMONADE is a multi-objective evolutionary algorithm that speeds up training by utilizing network morphism operators. It has shown competitive results in architecture search, achieving a top-1 error of 28.3% and a top-5 error of 9.6% in the \"mobile setting\". Further improvements could be made by incorporating advanced concepts from the evolutionary algorithms literature and utilizing other network operators. The morphism equation allows for adding fully-connected or convolutional layers by linear mappings. It can widen a layer by increasing units or channels, known as the Net2WiderNet transformation. The morphism equation allows for widening a layer by increasing units or channels through linear mappings. It also enables skip-connections and the replacement of idempotent functions with arbitrary functions, incorporating any non-linearity. LEMONADE consists of three components: using approximate network morphism operators for shrinking architectures, Lamarckism to avoid training from scratch, and a two-staged sampling strategy. Deactivating each component individually showed improved performance. Experimental details for Search Space I are listed. In Search Space I, LEMONADE's Pareto front was initialized with four simple convolutional networks. An ablation study on CIFAR-10 was conducted to investigate the impact of deactivating different components of LEMONADE, including approximate network morphisms operators, Lamarckism, and the proposed sampling strategy. The study introduced convolutional networks with varying parameters and depthwise-separable convolutions. LEMONADE efficiently explores a large search space for generating children architectures. The network operators define the search space, ensuring a minimum number of convolutions and filters. In the experiments, the search space for architectures is defined by network operators. Different convolutional cells are used within macro architectures to build neural networks. The Pareto Front is initialized with four trivial cells, each consisting of different convolutional layers. The study explores depthwise separable convolution in MobileNets V1 and V2 for classifying CIFAR-10. Modifications were made to adapt the networks to lower spatial resolution. Various sizes of MobileNets V1 and V2 were tested by adjusting the width multiplier \u03b1. NASNets were also tested by varying the number of cells per block and number of filters in the last block. Standard data augmentation methods were applied, and the training set was split into training and validation sets. Weight decay was used for all models. The study explores depthwise separable convolution in MobileNets V1 and V2 for classifying CIFAR-10. Weight decay (5 \u00b7 10 \u22124) is used for all models, with a batch size of 64. Models are trained for 20 epochs using SGD with cosine annealing BID16. Test performance is evaluated by training models from scratch for 600 epochs with an initial learning rate of 0.025. LEMONADE ran for approximately 56 GPU days during the search for convolutional cells on CIFAR-10. No significant changes in the Pareto front were observed after 24 GPU days. The training setup remains the same throughout, and the setup on ImageNet64x64 is identical to previous work."
}