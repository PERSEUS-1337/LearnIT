{
    "title": "S1xFcknVFS",
    "content": "We elaborate on using importance sampling for causal reasoning, specifically for counterfactual inference, implemented in probabilistic programming. Optimizing the inference process by considering the counterfactual query structure and design choices. Introducing MultiVerse, a probabilistic programming engine for approximate causal reasoning, compared with Pyro. Counterfactuals are complex causal questions involving posterior inference and interventional reasoning, challenging to infer but valuable for explanation. Counterfactuals are powerful for explanation and have been applied in various fields such as medicine, advertisement, translation, and reinforcement learning. Existing frameworks are not fully equipped to handle the complexity of counterfactual inference, requiring enhanced tools and inference procedures. Counterfactual queries involve changing inputs to see the outcome in a posterior world. This differs from observational inference as it intervenes on variables in the posterior world, causing them to take new values. Counterfactual inference queries involve predicting variables after intervening in a \"posterior world\". This is done by evaluating the query in three steps: abduction (observational inference), where the joint posterior over latent variables is determined given the evidence, creating a new model with the same structure. In counterfactual inference, prediction of quantities of interest is done by updating variables and using approximate inference methods. Various approaches like twin network, single-world intervention graphs, and matching are used for counterfactual inference. Probabilistic programming systems allow users to easily write and iterate over generative probabilistic models as programs, set evidence for observed variables, and use efficient inference methods for queries. Most frameworks support only observational inference, with importance sampling as an approximate technique. Importance sampling is used for approximate inference by drawing samples from a proposal distribution to calculate posterior probabilities. Probabilistic programming engines do not natively support counterfactual inference, but there are related directions for performing probabilistic causal inference. Counterfactual inference methods, including the use of probabilistic programming languages like Omega C, have been developed for causal reasoning. These methods can be applied in various probabilistic programming systems, offering a language-independent approach. The MultiVerse engine prototype utilizes a single, immutable model for causal inference, making counterfactual inference more efficient. It employs importance sampling to represent the posterior distribution and modifies abduction, intervention, and prediction steps for counterfactual queries. The MultiVerse engine prototype uses importance sampling to efficiently perform counterfactual inference by representing the posterior distribution with a set of tuples. It is designed as a native probabilistic programming system for causal reasoning, allowing for observational and interventional queries on the same model without double-sampling. The MultiVerse engine prototype efficiently performs counterfactual inference using importance sampling without double-sampling. It intervenes on each sample to predict values of interest, saving on memory and time by not requiring new model or inference objects. Resampling from an already approximated representation in Pyro 3 introduces additional approximation errors. In MultiVerse, optimizations for counterfactual queries involve evaluating necessary parts of the probabilistic program execution trace per step. For counterfactual inference, noise variables must be represented as explicit random variables in the trace. \"Observable\" Random Procedures are introduced for this purpose. In MultiVerse, \"Observable\" Random Procedures are used for counterfactual queries, with explicit noise variables in the program trace. Performance evaluation compared different versions of MultiVerse and Pyro in terms of sampling speed and convergence to true values. Four inference systems were compared: \"MultiVerse\", \"MultiVerse Optimised\", Pyro without a smart proposal, and Pyro with a smart proposal. The smart proposal in MultiVerse forces inference to set noise variables to specific values to match observed data, leading to faster and more efficient convergence compared to Pyro. MultiVerse also outperforms Pyro in terms of speed and efficiency without needing new model objects. Additionally, optimized MultiVerse performs faster than the regular version. See Section A for more experiment details and results. In this paper, we introduce MultiVerse, a probabilistic programming system for causal reasoning that optimizes approximate counterfactual inference. We aim to develop an approximate causal inference engine for any counterfactual query expressed in a probabilistic program. We generated 1,000 Structural Causal Models and corresponding Bayesian networks for experimentation. MultiVerse is a probabilistic programming system for causal reasoning that optimizes approximate counterfactual inference. Experimental results show that \"MultiVerse\" and \"MultiVerse Optimised\" outperform Pyro in terms of speed, producing the same number of samples faster. For example, \"MultiVerse\" is 92.8% faster than Pyro in generating 5,000 samples. Additionally, \"MultiVerse Optimised\" is 26.1% faster than regular \"MultiVerse\" in generating 1,000,000 samples per run. In terms of statistical inference convergence quality, \"MultiVerse\" experiments show better performance compared to Pyro. For instance, with 5,000 samples, \"MultiVerse Optimised\" has a mean absolute error of 0.00539, while Pyro has 0.00723, making \"MultiVerse Optimised\" 25.4% more efficient. 1,000 Structural Causal Models were generated with Bayesian networks using probabilistic programs. Each network consists of 15 blocks with exogenous Bernoulli variables. The curr_chunk discusses a Bernoulli variable with a constant hyperparameter, a dependent endogenous Delta-distribution variable, and an exogenous noise variable. The probability parameters are randomly chosen during network generation. The vector elements are sampled randomly and normalized. Each block type contains a probabilistic procedure. An example network structure is provided in Figure 1. The curr_chunk discusses a network structure with exogenous Bernoulli variables, dependent endogenous Delta-distribution variables, and counterfactual queries involving evidence sets, interventions, and nodes of interest. The curr_chunk discusses experiments with networks using binary nodes for counterfactual query computation. Pyro and MultiVerse support both discrete and continuous variables. Different versions of experiments are run, including \"MultiVerse\" and \"Optimised MultiVerse\" for efficient variable evaluation in Python. The curr_chunk discusses lazy evaluations in a probabilistic model using Python, focusing on predicting, observing, and intervening variables. MultiVerse is used to optimize variable evaluation, with examples provided in different scenarios. The use of Pyro without a guide leads to rejected samples, while Pyro with a guide helps in defining noise variables. The curr_chunk discusses using Pyro for counterfactual inference by manipulating noise variables to match observations. Pyro traces can be reused to avoid additional sampling steps, making the process more computationally efficient. The curr_chunk discusses the implementation of counterfactual importance sampling inference in Pyro, highlighting the use of one processor due to limited support for parallelization. The posterior presentation of the abducted model for intervention is passed as an EmpiricalMarginal object, with potential computational costs for large models. The process involves repeating steps until non-zero weights are obtained, and both MultiVerse (MV) and Pyro implementations converge to the ground truth. The experimental results confirm that \"Pyro with guide\" converges slightly slower than \"Pyro without guide\" and both MultiVerse implementations are more efficient in terms of speed per sample than Pyro 9. Potential gains in computational efficiency may be explored in the future by using vectorised sampling in Pyro. Both Pyro and MultiVerse are used similarly by writing models as Python programs without vectorization. MultiVerse is faster than Pyro with guide, especially when running parallel importance sampling on a machine with 16 cores. Pyro lacks a simple way to run importance sampling in parallel. MultiVerse is faster than Pyro with guide for running samples, with times of 0.07431 milliseconds and 0.05692 milliseconds respectively. For 1,000,000 samples, MultiVerse averages 0.06616 milliseconds and MultiVerse Optimised averages 0.04890 milliseconds. Code snippets for a Gaussian model and a counterfactual query are provided in this section. The MultiVerse model example demonstrates running parallel chains using MCMC for counterfactual queries. Additional wrappers/helpers may be needed for running importance sampling samplers in parallel. The code snippet shows the process of running inference and calculating expectations in MultiVerse. MultiVerse allows users to define models, provide observations, and perform inference queries without the need to manually define a new model or store posterior probabilities. Counterfactual inference in Pyro can be implemented using the interventional do operator. To implement counterfactual inference in Pyro, a combination of different tools is needed instead of relying solely on the engine. Users must implement the inference query themselves and modify the model to include posterior values in the prediction step. This approach enforces modularity but limits communication between abduction, intervention, and prediction steps. Two sampling steps are required, one for abduction and one for prediction. Additionally, operating in a discretised continuous space for emission variables is necessary. To ensure accurate counterfactual inference in Pyro, users need to implement the inference query and adjust the model to incorporate posterior values during prediction. Discretizing the continuous space for emission variables is crucial to enable the sampled emission variable to match its observation. An alternative approach involves adjusting the noise variable \u03b5 to align the emission variable with its observation, similar to ObserverableERPs in MultiVerse. This can be achieved by defining a function observed noise invert function f\u03b5 such that f\u03b5(Y, X, Z) = \u03b5. To enable accurate counterfactual inference in Pyro, users must implement the inference query and adjust the model to incorporate posterior values during prediction. This involves discretizing the continuous space for emission variables and adjusting the noise variable \u03b5 to align the emission variable with its observation. A function f\u03b5 is defined to handle this adjustment, returning appropriate \u03b5 values. Configuration variables need to be set accordingly. Importance sampling is used to approximate observational posterior queries, with results compared across different options. Importance sampling is utilized to approximate observational posterior queries by generating samples from a proposal distribution and calculating weights based on prior, proposal, and likelihood probabilities. This method is also applied in probabilistic programming settings, where variables are represented as random procedures in a generative model. In counterfactual inference, variables are sampled from proposals and likelihoods are incorporated into weights. The abduction step is the most complex, involving joint posterior inference. Various inference techniques like importance sampling can aid in posterior inference. MultiVerse assumes default settings for computations. In MultiVerse, the program structure and random choices are fixed by default. Users can specify their own addressing scheme for complex program flows. Importance sampling involves sampling from a proposal and incorporating prior and likelihoods into weights. No need to update weights as they already represent the posterior space. The \"counterfactual's intervention\" operates in the posterior space defined by samples and weights, predicting the variable(s) of interest by taking weighted averages. The algorithm requires 2N + 1 evaluations, with the same complexity as observational queries but double the constant factor. The algorithm for \"counterfactual's intervention\" operates in the posterior space using weighted averages to predict variables of interest. It requires 2N + 1 evaluations, with the same complexity as observational queries but double the constant factor. Memory optimizations include discarding unnecessary samples and only keeping predicted values or requested statistics like mean or variance. Lazy variable evaluation was used in the \"MultiVerse Optimised\" experiments for efficiency. Similar to optimizations for Church and Venture, further enhancements can be made by tracking the dependency of a complex model graph to ensure that only necessary sub-parts are evaluated. An intelligent probabilistic programming engine can analyze the computation graph dynamically, potentially implementing \"just-in-time\" compilation for inference. Memory-efficient strategies like \"copy-on-write\" can be applied when intervening or predicting on sample parts. After intervention, samples in the probability space may become identical, requiring their weights to be summed for representation. Importance sampling focuses on calculating statistics over all samples, regardless of uniqueness. Counterfactual conditioning involves considering additional observations in the counterfactual world before prediction. Importance sampling is a key concept in designing a probabilistic programming system for counterfactual and causal reasoning. It allows for optimizations that existing general-purpose systems may not achieve. Optimizations for counterfactual inference in probabilistic programming systems are crucial for achieving efficient results. Basic variational inference approaches may not be sufficient due to the need to operate on the joint posterior distribution. More sophisticated optimization-based approaches are required to preserve the relation between variables in the joint distribution. In counterfactual inference, variables D \u2286 Y or K \u2286 Y in model M when P (X) is replaced by P (X | Y = e). This highlights the limitations of standard probabilistic expressions and the need for a three-part inference procedure. Different notational resolutions have been proposed to address this issue. Richardson and Robins (2013) propose Single World Intervention Graphs as a resolution for counterfactual inference. Using intervention mechanisms like Pyro or Edward, one can perform abduction, intervention, and prediction steps. However, current methods introduce redundancy, require model modifications, and don't optimize inference for counterfactuals. Omega C probabilistic programming language is also discussed in a paper by Tavares et al. (2018). Omega C is a new causal probabilistic programming language proposed by Tavares et al. (2018) for inference in counterfactual generative models. The language has its own syntax and semantics tailored for counterfactual inference. Future work includes optimizing counterfactual inference techniques in Omega C, comparing its syntax and semantics with other languages like Pyro and MultiVerse, and extending insights from Omega C to enhance other probabilistic languages for counterfactuals. In the field of probabilistic logic programming, P-log and CP-logic are used for causal and counterfactual reasoning. The methodology in Ness (2019b) for Pyro involves resampling from the posterior for counterfactual queries. Omega C is a new causal probabilistic programming language for inference in counterfactual generative models. Future work includes optimizing counterfactual inference techniques and comparing its syntax and semantics with other languages like Pyro and MultiVerse. In counterfactual settings, all latent variables, including noise variables, should be explicitly represented in probabilistic programs. Structural causal models require all exogenous variables to be explicitly represented, while noise variables in probabilistic programming are often implicit. In counterfactual settings, all latent variables, including noise variables, should be explicitly represented in probabilistic programs. Implementations of probabilistic programming systems often use implicit representations, but for proper counterfactual inference, defining \"observable probabilistic procedures\" (OPP) is suggested. OPP allows for explicit representation of noise variables while preserving the benefits of implicit representation in existing frameworks. When implementing an Observable Probabilistic Procedure (OPP), it must sample noise explicitly and have a method for observing and calculating the inverse transformation. Consider making all Probabilistic Procedures (PPs) OPPs by default. When writing an OPP implementation, ensure no parts of the posterior are skipped. When implementing an Observable Probabilistic Procedure (OPP), noise variables must be sampled explicitly to match observations. Using Pyro \"guide\" can help force noise variables to their inverse values. In Figure 4, different models show latent variables X and Z with Gaussian noise. In probabilistic programming, Figure 4a and 4b represent a model with emission and exogenous noise variables. The joint posterior is the same for both, but counterfactual queries differ due to the recording of randomness in the noise variable. In probabilistic programming, Figure 4a and 4b represent a model with emission and exogenous noise variables. The counterfactual prediction involves resampling variable Y from its prior given the posterior and intervention. \"Guide\" in Pyro is a model defining a proposal for efficient inference. The choice of representation and query should be based on informed decisions to avoid running unintended counterfactual queries. When working with structural causal models in probabilistic programming, it is important to consider intervening on endogenous variables. Descendants of intervened variables should also be endogenous variables, as per the convention suggested by Pearl et al. This requirement ensures proper handling of random variables with hyperparameters in the model. When working with structural causal models in probabilistic programming, it is crucial to handle intervening on endogenous variables carefully. An endogenous variable is defined by its own randomness and depends on other variables in the model. Management strategies include formulating queries with strict structural causal models and ensuring appropriate queries. Intervening on variables involves defining noise by its prior distribution, but this method is considered a \"hack\" due to implementation limitations. Proper language syntax constructions should be introduced for modeling such scenarios. In the context of structural causal models in probabilistic programming, handling interventions on endogenous variables is crucial. This involves adjusting models or conducting partial counterfactual queries. Observable Normal ERP and Observable Bernoulli ERP are illustrated, showing how noise variables affect the output of functions in different scenarios. The inverse function determines the noise variable value based on the observed output, ensuring the observation is satisfied. The curr_chunk discusses the concept of a noisy OR procedure in stochastic modeling, where a noisy-OR variable Y is influenced by N parent variables with associated noise variables. The noisy-OR variable Y becomes true only if both the parent variable and its associated noise variable are true. The noisy-OR variable Y is influenced by a \"leak\" cause and noise variables. The Observable Noisy-OR procedure involves setting noise variables based on observed values. The Observable Noisy-OR procedure involves setting noise variables based on observed values, where different proposals can be used to sample variables to enable Y to be True. When a control flow change occurs in a program, the new control flow sub-trace part is resampled from its prior. For example, if a value in a probabilistic program is intervened, the alternative branch expression will be resampled. This resampling is similar to noise resampling and has implications for counterfactual prediction. The code defines latent and emission variables with Gaussian noise explicitly represented in the trace for further prediction. The text discusses the process of providing proposal parameters to probabilistic procedure object calls, making observations and interventions in a probabilistic program, and predicting values. It also mentions the need to explicitly specify dependencies in probabilistic procedures and provides instructions for observations, interventions, and predictions in Python. Performing counterfactual and observational predictions in a probabilistic program involves interventions and predictions. By calling specific instructions, such as do(erp, value, do_type=DOTYPE_IV) for interventions and predict(expr, predict_counterfactual=False) for predictions, one can modify the model accordingly. Combining these instructions in a single model/query is possible, with counterfactual interventions yielding equivalent results to observational predictions. Inference is conducted by providing evidence, interventions, and running the inference method with the desired number of samples. The output includes predictions for each sample and their respective weights. The run_inference method can compute statistics using predictions and sample weights. It can run inference in parallel and assign trace addresses to ERP objects. MultiVerse currently allows observations and interventions on statically defined random procedures. Future work includes exploring interventions based on stochastic trace execution. The engine uses a simple incremental addressing scheme by default. For probabilistic programs with changing control flow, users must implement their own addressing scheme. Python was chosen for the prototype implementation to allow anyone familiar with Python to run counterfactual queries on probabilistic programs. Implementing optimizations mentioned in Section 2.2 and fully automating them in Python may be challenging. Similar engines may exist in a restricted subset of Python or in languages like Clojure. For \"MultiVerse Optimised\" experiments, the model was redefined to compute variables in a lazy way, using methods of the MultiVerse engine to skip unnecessary computations. Instead of computing all variables in the model in every step, variables are only computed if required. The method compute var helper can compute any variable by recursively computing its parents. Calls to compute var helper are wrapped in the method compute var, which uses MultiVerse's compute procedure to determine if a variable needs to be computed or if it has been intervened. If a variable has been intervened, MultiVerse returns a Delta-distribution with the intervened variable's value; otherwise, it computes the variable using a provided function. This logic also applies to observed variables. When computing variables in MultiVerse, observations are wrapped in an IF OBSERVE BLOCK and interventions are wrapped in an IF DO BLOCK. This ensures that computations are only executed when necessary, such as recording interventions during the initial run of the program."
}