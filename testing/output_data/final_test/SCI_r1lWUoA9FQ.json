{
    "title": "r1lWUoA9FQ",
    "content": "A wide range of defenses have been proposed to harden neural networks against adversarial attacks, but most are quickly broken. This paper analyzes adversarial examples theoretically and shows that for certain problems, such attacks are inevitable. Experiments explore the implications for real-world problems, considering factors like dimensionality and image complexity. In response to the prevalence of adversarial attacks on neural networks, various defenses have been proposed but are often easily overcome by new attacks. This paper delves into the inevitability of adversarial attacks for certain problems and establishes limits on a classifier's vulnerability based on data distribution and dataset dimensionality. Adversarial examples involve small perturbations to images that alter their classification, with different norms used to measure perturbation size. The choice of norm in crafting adversarial examples can significantly impact the theoretical guarantees for their existence. The analysis extends to the 0-norm, producing sparse adversarial examples that only perturb a small subset of image pixels. The paper explores the vulnerability of classifiers to adversarial perturbations based on data distribution and dataset dimensionality. In Section 8, the causes of adversarial susceptibility in real datasets and the effect of dimensionality are explored. Adversarial examples change image labels with imperceptible perturbations. Various defenses have been proposed but have been easily broken. Adversarial training initially thwarted attacks but was vulnerable to more sophisticated methods. Recent work has highlighted the ongoing vulnerability of classifiers to sophisticated multi-stage attacks, toppling defenses that rely on network distillation and specialized activation functions. Various defenses presented in ICLR 2018, including thermometer encoding, input transformations, and using generative models, have been broken. Some authors suggest sanitizing datasets to remove adversarial perturbations before classification, but approaches based on auto-encoders and GANs have also been compromised. Approaches based on auto-encoders and GANs have been broken using optimization-based attacks. Various defense mechanisms have been developed for classifiers, such as hardening a two-layer classifier using semidefinite programming and using a convex duality-based approach to adversarial training. These methods focus on training robust classifiers on low-dimensional datasets like MNIST, but their robustness often fails to generalize reliably to test examples. Sparse adversarial examples perturb a small subset of pixels, hiding \"fuzz\" in high-frequency image regions. Using 0-norm perturbations, distortions are hidden in the grass, challenging adversarial defense in higher dimensions. This paper utilizes high-dimensional geometry, specifically isoperimetric inequalities, to bound classifier robustness. Other authors have also explored adversarial susceptibility through a geometric lens. BID11 study adversarial susceptibility of datasets assuming they are generated by a model mapping Gaussian vectors to images. BID12 analyze classifiers on a synthetic dataset on concentric spheres. BID31 show high Lipschitz constant in untrained networks with random weights in high dimensions. BID20 also studied adversarial susceptibility with evasion and poisoning attacks. This work focuses on adversarial robustness for arbitrary data distributions and the impact of dimensionality on robustness limits. The text discusses quantifying the size of a subset A on a unit sphere in R^n using normalized measures. It also mentions using geodesic distance on spheres and p-norms on cubes for measuring distances between points. The text discusses classifying data points in a space into different object classes defined by probability density functions. It introduces the concept of a classifier function that partitions the space into subsets for each class label, without providing a confidence level. It then defines an adversarial example within this framework. An adversarial example is defined as a point that can be moved within a certain distance in a metric to change its class label. Different p-norm metrics are used to create adversarial examples, with p=\u221e being a common choice. Sparse adversarial examples manipulate only a small subset of pixels. In the context of adversarial examples and different p-norm metrics, the study focuses on classifiers for data on the sphere. The goal is to show that most points in a class lie close to the class boundary by defining the -expansion of a subset A with respect to a distance metric d. This provides bounds on the probability of points being close to the boundary. The result provides bounds on the probability of adversarial examples independent of the class boundary shape, based on an isoperimetric inequality. This inequality, a simple geometric statement, is a key factor in the study of classifiers for data on the sphere. The isoperimetric inequality provides bounds on the probability of adversarial examples, independent of class boundary shape, based on geometric principles. This has implications for mapping one class into another using small perturbations. The isoperimetric inequality bounds the probability of adversarial examples, regardless of class boundary shape, based on geometric principles. It has implications for mapping one class to another with small perturbations. Theorem 1 discusses the existence of adversarial examples in a classification problem with m object classes distributed over the unit sphere. The region R on the sphere labeled as class c by C and its complement R( ) are discussed in relation to the epsilon expansion. Safe points from class c that are correctly classified and immune to adversarial perturbations are considered. The set of safe points is the complement of R( ) with a normalized measure. The probability of a random point lying in the set of safe points is bounded above by a certain value. The probability of a point lying outside the safe region on a sphere is calculated using geodesic distance. The study shows that Theorem 1 is not strongly dependent on the distance metric used, as any -adversarial example in the geodesic metric would also be adversarial in the Euclidean and max norms. The proof of Theorem 1 relies on properties exclusive to the sphere, making it inapplicable to real-world images that lie in a hypercube. The question of whether adversarial examples are inevitable on the cube remains complex due to the absence of geometric isoperimetric inequalities. Researchers have derived algebraic isoperimetric inequalities for sets in a cube without identifying the shape that achieves the minimum expansion. A lemma on a measurable subset of the cube is provided, showing that most data samples in a cube admit adversarial examples. Researchers have derived algebraic isoperimetric inequalities for sets in a cube without identifying the shape that achieves the minimum expansion. A lemma on a measurable subset of the cube is provided, showing that most data samples in a cube admit adversarial examples. The text discusses partitioning the hypercube into measurable subsets and defining scalar constants, selecting a class with specific criteria, and sampling random data points to guarantee adversarial examples under certain conditions. In n dimensions, the diameter of the cube is \u221a n, and it is reasonable to choose = O( \u221a n) in equation 5. A strong bound of DISPLAYFORM5 DISPLAYFORM6 2 /2 (for z > 0), and \u03b1 = \u03a6 \u22121 (1 \u2212 f c ). To be meaningful with < 1, f c needs to be relatively small, roughly f c or smaller. This is realistic for some problems; ImageNet has 1000 classes, so f c < 10 \u22123 for at least one class. Guarantees of adversarial examples are stronger on the sphere than on the cube under \u221e -norm attacks. Theorems in the \u221e case are tight, making it more difficult to prove the existence of adversarial examples in the cube using the \u221e -norm. The existence of adversarial examples in the cube using the \u221e -norm is difficult to prove. Sparse adversarial examples involve changing a small number of image pixels to manipulate the class label. Investigating adversarial examples under the 0 metric involves perturbing a point x into a different class by modifying a small number of pixels. The bound for p = 0 is considerably tighter for small p but looser for large p. Previous studies have looked at the case p = 0. The proof of Theorem 3 for sparse adversarial examples in the cube involves using a p-norm distance metric. The theorem states that a random data point x from a class distribution can either be misclassified or adversarially perturbed with a certain probability. Theorem 4 provides conditions for the existence of adversarial examples in the setup of Theorem 2. It involves choosing a class occupying a fraction of the cube, setting a p norm, and determining the support of the class distribution. The bound for the case p = 0 is valid under certain conditions, and non-vacuous bounds are produced when using the 2-norm. The diameter of the cube and the bound become active under specific circumstances. The bound becomes active when the support of the class is larger than a hypercube of side length at least e \u2212\u03c0 \u2248 0.043. Unbounded density functions and low-dimensional data manifolds can lead to infinite density functions. Expanding the data manifold by adding random noise can help in crafting adversarial examples. Adding a small perturbation before crafting adversarial examples is a common strategy. Introducing a \"don't know\" class can reduce the region where adversarial examples could be found. In some cases, it may be easy for adversaries to degrade classifier performance by perturbing images into the \"don't know\" class. Feature squeezing, decreasing data dimensionality, and computational hardness can increase robustness against adversarial attacks. In this section, the relationship between dimensionality and adversarial robustness is discussed. High-dimensional classifiers are often thought to be more vulnerable to adversarial examples than low-dimensional ones. However, high-dimensional distributions may actually be more concentrated than low-dimensional ones, challenging this assumption. The study explores the effects of dimensionality on adversarial robustness using a \"big MNIST\" image distribution. By expanding the original MNIST images into larger dimensions, it is shown that classifiers on big MNIST are more susceptible to attacks. Theorems 1-4 establish fundamental limits of susceptibility for all classifiers, independent of image dimensionality. The relationship between dimensionality and susceptibility in big MNIST images is influenced by the weakness of the training process. Theorem 5 predicts that the perturbation needed to fool all 56 \u00d7 56 classifiers is twice that needed for 28 \u00d7 28 classifiers, as the 2-norm of a 56 \u00d7 56 image is twice that of its 28 \u00d7 28 counterpart. The susceptibility of big MNIST images is not significantly affected by resolution, as shown by the relationship between dimensionality and susceptibility. The expansion factor and concentration bound grow at the same rate, with no net effect on susceptibility. The fundamental limits of MNIST can be better understood by considering classifiers hardened by adversarial training. The curves of these classifiers demonstrate properties predicted by theorems, such as wider curves for larger image sizes. The classifier behavior near the fundamental limit predicted by Theorem 2 is observed in adversarially trained classifiers in FIG3. The concentration bound Uc for object classes plays a role in the susceptibility of high-dimensional classifiers to adversarial attacks. Images in MNIST are highly correlated and concentrated near low-dimensional manifolds, making them easier to fool despite Theorem 5 showing that increased dimensionality does not fundamentally increase susceptibility. The concentration bound Uc for object classes influences the susceptibility of high-dimensional classifiers to adversarial attacks. MNIST images are highly concentrated near low-dimensional manifolds, making them easier to fool. Choosing a more spread out dataset like CIFAR-10 increases susceptibility to attacks, despite structural similarities between the two datasets. The concentration limit Uc is a measure of image complexity, with smaller Uc indicating higher complexity and susceptibility to adversarial attacks. The effectiveness of adversarial training varies based on image complexity rather than dimensionality, suggesting that high complexity classes are more vulnerable. The question of whether adversarial examples are inevitable remains unclear. The paper discusses the fundamental limits of robustness to adversarial attacks in classification problems, highlighting the dependence on dataset properties, adversary strength, and perturbation metrics. It suggests that for complex image classes in high-dimensional spaces, these limits may be more severe than intuition suggests. The proof of Lemma 3 is provided, starting with a classical isoperimetric inequality for random Gaussian variables. The paper discusses the limits of robustness to adversarial attacks in classification problems, emphasizing the impact of dataset properties, adversary strength, and perturbation metrics. It introduces a classical isoperimetric inequality for random Gaussian variables and proves Lemma 3, which involves a Gaussian Isoperimetric Inequality for sets with the same Gaussian measure. Lemma 3 provides a proof that the function \u03a6 maps a random Gaussian vector onto a random uniform vector in the unit cube. The proof involves Gaussian and uniform measures on measurable subsets, leading to a simplified formula in the theorem. Lemma 3 proves that the function \u03a6 maps a random Gaussian vector to a random uniform vector in the unit cube. The proof involves Gaussian and uniform measures on subsets, leading to a simplified formula in the theorem. The result follows a three-step process by Talagrand, optimizing constants for tightness of the bound. The proof involves induction on dimensions, with a base case for n=1 and extending to n dimensions using the inductive hypothesis. The integral is upper bounded by integrating over \"slices\" along one dimension. The proof of Theorems 2 and 3 involves applying lemma 6 to produce a Markov inequality and optimizing the bound by choosing constants. The optimal value of \u03b1 is determined to optimize the bound, which is stronger than needed to prove Lemma 3 but useful for proving Theorem 4. The proof of Theorems 2 and 3 involves combining their proofs as they are similar. By choosing a class c with f c \u2264 1/2, we can define subsets R and R( ; d p ). The region R( ; h) contains correctly classified points safe from perturbations, with volume at most \u03b4. The mass of the class distribution lying in the \"unsafe\" region R c is obtained by subtracting U c \u03b4 from 1. For large enough , the expansion of the support of class c is shown to overlap with other classes, leading to -adversarial examples. The proof involves bounding A( , d p ) using equations from Lemma 3, with a focus on approximating \u03a6 \u22121 (\u03b7) and utilizing equation 17 for the case p = 0."
}