{
    "title": "r1esys0Nd4",
    "content": "Using higher order knowledge to reduce training data has become a popular research topic. However, the ability for available methods to draw effective decision boundaries is still limited. A novel algorithm is designed to optimize output probability distribution on a clustered embedding space, making neural networks draw effective decision boundaries. Weak supervisions are required to mark batches with output distribution differing from the target probability distribution. Empirical experiments show that this model can achieve higher accuracy with fewer high-quality labeled training examples compared to other semi-supervised learning models. Neural networks do not behave like doctors who make predictions based on prior knowledge of rare diseases. When trained with limited labeled examples, the output distribution can be biased towards certain labels, leading to overfitting. This can be seen in Figure 1a where the model overfits to training examples and leaves out labels between data points. Neural networks overfit to training examples and fail to learn the decision boundary correctly from a limited number of examples. To address this, a novel embedding space probabilistic constraint is proposed to introduce output label probability distribution as higher order knowledge when training neural networks. This constraint is applied on the neural network's embedding space, constructed unsupervisedly through an autoencoder in a high dimensional feature space with only a few labeled examples. The embedding space in an autoencoder preserves information of label clusters. Training with limited data leads to imbalanced label distribution and chaotic embedding space. By using few high-quality labeled examples, the model can converge faster than semi-supervised methods. Weak supervision in representation learning has been successful in computer vision and natural language tasks. In computer vision and natural language tasks, weak supervision has shown success in tasks like BID4, BID8, BID19, and BID1. Research is focusing on transferring knowledge from different datasets or introducing higher-level knowledge to address the need for large quantities of high-quality labels for training. One approach is tackling incomplete weak supervision problems, where labeled and unlabeled datasets are used together, with methods like AtlasRBF, Neural Rendering Model, LadderNet, or logical constraints. Learning with constraints incorporates higher order domain knowledge into neural network optimization. Different constraints are effective for different tasks, such as linear constraints for dense pixelwise semantic segmentation. Various frameworks like semantic loss and logical loss specify rules for training neural networks. A novel framework by BID18 enables learning physical or causal relationships without labels. This work addresses biased output distribution due to limited labeled examples, distinct from arithmetic or logical constraints. Incorporating higher order domain knowledge into neural network optimization involves placing an output probability constraint. The problem formulation and algorithm architecture are described, along with the introduction of higher order knowledge based on incomplete weak supervision. Domain knowledge distribution Q is specified, with a training algorithm A({X 1 , y 1 }, X 2 , Q}) for training a multi-layer perceptron DISPLAYFORM0. The algorithm minimizes loss function using weights and nonlinear functions like ReLU and softmax. It defines a loss term to regulate output distribution, focusing on the activation of the final softmax layer to reflect confidence in labels. Mean pooling of activation outputs is considered for improved accuracy in detecting low confidence examples. Our mechanism uses KL divergence to improve accuracy in detecting low confidence examples. Weak supervision is assumed for some batches, which can be easily done through supervision methods or auto-regressive algorithms. The algorithm and convergence analysis are detailed in Appendix A, and decision boundaries are optimized using existing unlabeled data. To optimize decision boundaries with existing unlabeled data, a probability constrained classifier is jointly optimized with an embedding space regularizer. The embedding space, represented by the ith hidden layer of a perceptron E(x), preserves data in a lower dimensional form. Using unsupervised loss helps maintain information on separations between label clusters, with a decoder D(\u00b7) reconstructing the original input in a separation-preserving embedding space. The proposed method constructs an unsupervised loss r to create a low-dimensional embedding space. Our proposed method utilizes unsupervised loss to create a low-dimensional embedding space, identifies cluster locations with limited labeled data, and determines decision boundaries based on output probability distribution. The updating loss function is defined as DISPLAYFORM2 with hyperparameter constants \u03bb1, \u03bb2, and \u03bb3. Experiments are conducted in a semi-supervised learning setting using a multilayer perceptron model. The model is trained with batch size 128 to focus on output probability distribution rather than individual labels. Our model is trained with batch size 128 and tested under various constraints, including datasetwise and batchwise probability constraints. We compare our model with other semi-supervised learning models and demonstrate that it can achieve high accuracy with fewer labeled examples. Our architecture shows the benefit of semi-supervised learning on the CIFAR 10 dataset, requiring fewer high-quality training examples to achieve high accuracy. By jointly optimizing the output constraint with hypothesis, a decision boundary can be drawn with smaller labeled training data compared to other methods. The focus is on demonstrating the power of weak labeling methods without high-quality techniques, leaving room for future research on designing algorithms with less supervision. Additionally, the formulation sums all activation functions for confidence measurement, offering potential for improved classification confidence in semi-supervised learning settings. In a semi-supervised learning setting, extra supervision is needed during the Bootstrap phase to mark entries with unusually high probabilities. This additional weak supervision helps the model judge if the current probability distribution contains unusual batches. The probability of this mark occurring can be bounded by concentration inequalities, as shown in Theorem 1. In a semi-supervised learning setting, extra supervision is needed during the Bootstrap phase to mark entries with unusually high probabilities. The random variable P(|Y = k) ranges from [0, 1], with the target distribution E[P(|Y = k)] = Q(k). The bootstrap algorithm redistributes the target distribution based on marked batches with higher probabilities. The output probability is rescaled to adapt to the higher-than-usual probability. The error threshold, domain knowledge output distribution, and neural network training process are discussed. The boundary between bootstrap and auto-regressive phases is determined by a hyper-parameter. Validation set accuracy above 70% triggers the auto-regressive phase. Training the network with loss function from Equation 3 is illustrated.lemma 1 states that a feed forward neural network with softmax activation yields a probability distribution. The text discusses the application of KL divergence to a probability distribution \u2126 from a target distribution Q, with specified constraints set K. It also mentions the independence of training examples within the same batch for statistical bounds application. Proof of Theorem 1.1 shows the safe margin d for controlling the tradeoff between human supervision and training batch accuracy. Empirical results support the upper bound application in experiments. Implementation details for datasets like MNIST, FASHION, and CIFAR-10 are provided. The dataset consists of 32x32 images belonging to ten classes like dog, cat, car. The training set has 50,000 images and the testing set has 10,000 images. To ensure equal representation, the minimum number of examples (5421) is chosen for each class. Training examples are randomly selected and normalized. Additionally, 50% of the time, Gaussian noise is added to prevent overfitting. In order to prevent overfitting during training, Gaussian noise and cropping techniques are applied. Gaussian noise involves adding random noise from a distribution with mean 0 and standard deviation 0.3 to the image. The image is also cropped by three pixels. A multi-layer perceptron model is used with an additional embedding layer of width 40. Batch normalization and dropout with a rate of 50% are implemented for model robustness. The architecture is compared with other state-of-the-art models using a 10-layer structure. The model architecture includes a 10-layer structure with convolution and max-pool layers, dropout, and batch normalization. The decoder uses a symmetric design with upsampling layers. Hyperparameters include SGD with a learning rate of 10e-4 and \u03bb values set to 1. Convergence is illustrated in FIG1. In the cold start session, our algorithm converges quickly with a batch size of 128. We set batch output probabilities empirically, covering 98% of data with a probability of 0.1. For the remaining data, we manually set probabilities at 0.02 and 0.28."
}