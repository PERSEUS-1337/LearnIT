{
    "title": "rk9kKMZ0-",
    "content": "In this paper, a Self-Paced Learning (SPL)-fused Deep Metric Learning (DML) framework called Learning Embeddings for Adaptive Pace (LEAP) is proposed. The method parameterizes mini-batches dynamically based on the easiness and true diverseness of the sample within a salient feature representation space. LEAP trains an embedding Convolutional Neural Network (CNN) using the Magnet Loss to learn an expressive representation space. The student CNN classifier selects samples for mini-batches based on easiness and true diverseness from the representation space sculpted by the embedding CNN. LEAP framework trains an embedding CNN using Magnet Loss to learn a representation space. It converges faster with fewer mini-batch updates for image classification tasks on various datasets. Data scheduling is crucial for training DNNs to optimize the order of training examples presented to the model. Previous studies on Curriculum Learning (CL) show that organizing training samples based on the ascending order of difficulty can favor model training. Self-Paced Learning presents a method for dynamically generating a curriculum by biasing samples based on their easiness under the current model parameters, leading to imbalanced sample selection. BID19 propose a batch selection strategy based on loss values of training data to speed up neural network training, achieving high performance on MNIST but failing on CIFAR-10. BID12 propose a Self-Paced Learning with Diversity (SPLD) framework that partitions data into groups based on diversity and easiness, aiming to improve training by selecting diverse samples of similar easiness. They suggest spreading sample selection widely instead of limiting it to specific groups. However, their use of K-Means and Spectral Clustering for grouping data may lead to sub-optimal results in learning non-linear features. Therefore, finding an appropriate metric to capture similarity among different data groups is crucial in practice. Deep Metric Learning (DML) approaches have gained attention for data grouping. The Magnet Loss shows superior performance in classification, clustering, and retrieval tasks. A new sample selection framework called LEAP is introduced for scheduling data examples in mini-batch training. Metric learning is leveraged to enhance self-paced learning, introducing a new knowledge aspect. The curr_chunk discusses the concept of self-paced learning and how it can be improved by utilizing a new type of knowledge through an embedding network trained by DML. It compares this approach to Curriculum Learning and explains how SPL quantifies the easiness of training instances. The training instances with loss values larger than a threshold are neglected during training, with the threshold dynamically increasing to include more complex samples. The self-paced sample selection framework aims to balance easy and complex training instances by pre-clustering data. Unlike SPLD, this method adapts to deep-learning scenarios with a DML approach to sculpt a representation space autonomously. Deep metric learning (DML) aims to learn a distance metric that minimizes distances within the same class and maximizes distances between different classes. DML approaches have shown promising results in various tasks such as semantic segmentation, visual product search, face recognition, and fine-grained image classification. It can also be used for extreme classification settings with a large number of classes and scarce examples per class. Our work utilizes the Magnet loss in deep metric learning to create a representation space without the need for preprocessing training data in paired format. This approach considers the global structure of the embedding space, unlike other methods that may result in reduced clustering. Additionally, we explore the concept of using DML for SPL-based DNN training, which has not been previously investigated. This allows for the construction of an end-to-end DML as a feature extractor using a deep CNN. The LEAP framework utilizes deep metric learning to construct a feature extractor using a deep CNN. It combines adaptive sampling, mini-batch online learning, and adaptive representation learning to create a self-paced strategy for DNN training. The framework consists of a dual DNN setup where an embedding DNN learns a representation space and transfers knowledge to train the student DNN. The focus is on training deep CNNs for various tasks. The LEAP framework utilizes deep metric learning with a dual DNN setup to create a self-paced strategy for training deep CNNs. An embedding CNN is trained alongside the student CNN, forming mini-batches based on sample importance priors. The framework focuses on supervised image classification tasks, using the Magnet loss for representation learning. The Magnet loss aims to improve classification accuracy by learning distance distributions for each example from assigned clusters. The approach involves repositioning cluster assignments using K-Means++ clustering and training the embedding CNN with mini-batches constructed through neighbourhood sampling. Seed clusters are sampled, and losses for each example are stored and averaged per cluster. LEAP aims to improve classification accuracy by computing average losses for each cluster during training. The objective involves assigning non-zero weights to \"easy\" samples and distributing non-zero elements across clusters to increase diversity, similar to SPLD. LEAP aims to improve classification accuracy by assigning non-zero weights to \"easy\" samples and distributing elements across clusters for diversity. The algorithm trains the student CNN with LEAP using parameters \u03bb, \u03b3 to extract the global optimum of the optimization problem. Easy samples are selected for training based on a specific criterion in Algorithm 2. The LEAP framework aims to enhance classification accuracy by assigning weights to \"easy\" samples and distributing elements across clusters for diversity. Samples are selected for training based on their loss values compared to a threshold \u03bb + \u03b3. Experiments were conducted using PyTorch and multi-GPU training on NVIDIA P100 Pascal GPUs. LEAP was compared against the SPLD algorithm and Random sampling on MNIST, FashionMNIST, and CIFAR-10 datasets. The embedding CNN is trained asynchronously in parallel with the student CNN to optimize computational requirements. The student CNN leverages multiprocessing for parallel computing to reduce computational requirements during training. It adaptively selects \"easy\" samples from cluster representations generated by the embedding CNN. Experimental setups involve comparing convergence in terms of mini-batches needed for state-of-the-art test performance and visualizing high-dimensional representations using t-SNE. The feature embeddings are extracted from a LeNet as the embedding CNN in experiments for MNIST, with a focus on learning a representation space using the Magnet Loss. The Magnet Loss is used to learn a representation space in the embedding CNN, replacing the fully connected layer in LeNet. Training is done with mini-batches of size 64 using Adam optimizer. Results show comparable test performance to Random sampling and better convergence than SPLD. Experiments on MNIST dataset demonstrate the effectiveness of the LEAP framework for end-to-end DNN training. FashionMNIST experiments use ResNet-18 BID7 with a similar setup. The FashionMNIST experiments utilized a ResNet-18 BID7 for the classifier, with the same LeNet for feature extraction. Training included data augmentation and achieved higher test accuracy with fewer mini-batch updates compared to SPLD and Random sampling. The dataset consists of 60,000 training examples and 10,000 test examples, each being a 28x28 grayscale image with 10 classes. The CIFAR-10 experiments used VGG-16 as the embedding CNN and ResNet-18 as the classifier. VGG-16 BID24 showed strong feature representations with the best convergence on Magnet Loss. The classifier was trained with SGD, momentum of 0.9, weight decay of 0.0005, and a fixed learning rate of 0.001. In experiments with VGG-16 and ResNet-18 on CIFAR-10, different training methods were used. VGG-16 was trained with SGD using a momentum of 0.9, weight decay of 0.0005, and a fixed learning rate of 0.001. ResNet-18 was trained with the LEAP framework at a fixed learning rate of 0.001, showing faster convergence and higher test accuracy compared to Random sampling or SPLD methods. LEAP framework ensures optimal clustering using Magnet Loss, adapting representation space as training progresses. Early stages feed diverse, easy samples to student CNN. Maintains diversity with mix of easy and hard samples. LEAP converges faster on CIFAR-10 compared to SPLD and Random sampling. The LEAP training protocol achieves higher test accuracy and lower test loss compared to SPLD and Random sampling. LEAP is designed to sample for heterogeneity, maximizing diversity and preventing overfitting. The framework was evaluated on CIFAR-100 dataset with a WideResNet student CNN and VGG-16 setup for the embedding CNN. On CIFAR-100, LEAP shows improvement over Random and SPLD due to its dynamic representation space and self-paced strategy, benefiting fine-grained recognition tasks. This is supported by the success of Magnet loss in classification for fine-grained visual recognition. Additionally, LEAP's effectiveness extends to more complex datasets like Street View House Numbers (SVHN). The SVHN dataset contains images with multiple digits, and the task is to classify the digit at the center. The dataset is split into training, testing, and extra sets with a total of 604,388 images. A WideResNet CNN was trained on the combined training and extra sets for 65 epochs without data augmentation. The model was optimized using SGD with specific parameters. LEAP outperformed Random and SPLD in terms of convergence speed and test accuracy. The LEAP framework improves DNN convergence towards optimal solutions by fusing a salient non-linear representation space with a dynamic learning strategy. Biasing samples based on easiness and diverseness enhances classification performance, outperforming Random and SPLD. Student CNN models show increased accuracy on various datasets with the LEAP sampling method. The LEAP framework enhances DNN performance by biasing samples for improved classification accuracy on datasets like MNIST, Fashion-MNIST, CIFAR-10, and SVHN. Despite challenges on CIFAR-100 due to limited training instances and low-quality images, LEAP achieves significant accuracy gains of 4.50 and 3.72 percentage points over SPLD and Random baselines. The LEAP framework improves DNN performance by biasing samples for better classification accuracy on various datasets like MNIST, Fashion-MNIST, CIFAR-10, and SVHN. It outperforms SPLD and Random baselines by 4.50 and 3.72 percentage points, especially for fine-grained classification tasks. Experimental results show increased accuracy across different datasets and sampling methods. LEAP is an end-to-end representation learning strategy for adaptive mini-batch formation, using a DML technique called the Magnet Loss. It involves training an embedding CNN and a student CNN to improve classification accuracy on diverse datasets. LEAP is a framework for adaptive mini-batch selection using a Magnet Loss technique. It trains an embedding CNN and student CNN simultaneously to improve classification accuracy on various datasets like MNIST, FashionMNIST, CIFAR-10, CIFAR-100, and SVHN. The framework, implemented in PyTorch, aims to advance end-to-end SPL fused DML strategies for DNN training and will be open-sourced on GitHub."
}