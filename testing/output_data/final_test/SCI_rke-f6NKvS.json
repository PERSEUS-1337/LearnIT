{
    "title": "rke-f6NKvS",
    "content": "Imitation learning, followed by reinforcement learning algorithms, is a promising paradigm to solve complex control tasks efficiently. However, learning from demonstrations often faces the covariate shift problem. Value Iteration with Negative Sampling (VINS) corrects errors in behavioral cloning policies on robotics tasks. It improves sample efficiency by using VINS to initialize reinforcement learning algorithms. The key technical challenge of learning from demonstrations is the covariate shift, where knowledge learned from the distribution of states visited by demonstrations may not transfer to other distributions. Errors in policy and value function learning can compound when executing the policy for multiple steps. An algorithm is developed to learn a value function that extrapolates to unseen states more conservatively to address the optimistic extrapolation problem. The algorithm addresses the optimistic extrapolation problem by learning a conservatively-extrapolated value function that stays close to demonstration states. This is achieved by ensuring that nearby states have lower values than demonstration states, guiding the policy to correct errors by returning to demonstration states. The algorithm learns a conservatively-extrapolated value function to address the optimistic extrapolation problem, guiding the policy to correct errors by returning to demonstration states. The algorithm uses a learned value function to guide the policy towards demonstration trajectories, correcting errors by returning to demonstration states. This approach improves efficiency compared to prior work that required significant time and samples to warm up randomly-initialized Q functions. The proposed algorithm, Value Iteration with Negative Sampling (VINS), outperforms prior work in sample efficiency on robotics benchmark tasks with sparse rewards. It formalizes the notion of value functions with conservative extrapolation, inducing policies that stay close to demonstration states and achieve near-optimal performances. Imiation learning is commonly used in robotics, and initializing an RL algorithm from VINS improves efficiency on the same benchmark tasks. Behavioral cloning is a standard approach in robotics, with algorithms like DAgger and AggreVaTe being stronger options when expert policies are available. The focus is on learning self-correctable policies from clean demonstration trajectories and sparse rewards. The focus is on learning self-correctable policies from clean demonstration trajectories and sparse rewards. Previous work has successfully combined generative models in environments with a large amount of interaction without rewards. Various methods have improved sample efficiency, including minimizing maximum mean discrepancy, a Bayesian formulation of GAIL, using off-policy RL algorithms, solving reward bias problems, and bypassing the learning of reward functions. Another approach aims to learn policies that stay close to demonstration sets by estimating the true MAP estimate of the policy. The algorithm aims to improve upon behavioral cloning without environment interactions by learning the value function instead of the reward function. This approach contrasts with inverse reinforcement learning, which focuses on learning a reward function optimized by the expert. Some related works use techniques like negative sampling or contrastive learning with negative samples from environments, while this method leverages demonstrations for sample-efficient reinforcement learning. In contrast to inverse reinforcement learning, which focuses on learning a reward function optimized by the expert, this method leverages demonstrations for sample-efficient reinforcement learning in continuous state and action spaces. The algorithm experiments with a soft version of actor-critic to address extrapolation issues in Q functions when the action space is discrete. Model-based reinforcement learning is utilized without generating fictitious samples. The algorithm does not use a dynamical model to generate fictitious samples for planning but instead combines learned dynamics with the value function to obtain a Q function. It is not considered a model-based technique and references recent work on model-based RL. Off-policy reinforcement learning has a significant body of prior works, including extensions of policy gradient and Q-learning. Fujimoto et al. propose solutions for off-policy reinforcement learning by constraining the action space and using double Q-learning. Our method adjusts the erroneously extrapolated value function by penalizing unseen states in off-policy learning from demonstration. The setting involves a deterministic MDP with continuous state and action spaces, sparse rewards, and a random initial state generation from a low-dimensional bounded support distribution. The goal is to find a policy \u03c0 that leads from initial state s0 to a set of terminal goal states G. The success rate of the policy is measured over T steps, with a sparse reward of -1 for each state-action pair. An expert policy \u03c0e provides demonstrations for off-policy learning. In imitation learning, a policy is learned from expert demonstrations without interacting with the environment. In reinforcement learning with demonstrations, interactions with the environment are allowed to minimize the need for additional interactions. The goal is to efficiently leverage demonstrations to reach the desired states. In imitation learning, the expert policy visits a small subset of states efficiently. This can lead to cascading errors in behavioral cloning, compounding mistakes quadratically. The distribution of states encountered by a learned policy differs from the demonstration state distribution, leading to errors compounding as the policy moves away from the initial states. Designing policies to correct themselves and stay close to the initial set of states is crucial. Off-policy evaluation of value or Q functions becomes problematic when the expert policy is not uniquely defined outside the initial state set. The expert policy \u03c0 e is not uniquely defined outside the initial state set, leading to challenges in defining the value function and Q function. A conservative extrapolation method is proposed to encourage the policy to stay close to the initial set of states. Initializing RL with imitation learning, such as behavioral cloning, has shown success in sample-efficient RL but can still be improved. The authors propose improving the method by learning a value function from demonstrations to initialize the RL algorithm faster. They suggest that the ideal extrapolation of the value function should decrease as we move further from the demonstrations. This correction effect is illustrated in Figure 2. The authors propose learning a value function from demonstrations to speed up the RL algorithm. The value function decreases further from the demonstrations, as shown in Figure 2. The gradients of V point towards U, with acx driving towards states closer to U. This conservative extrapolation matches V \u03c0e on demonstration states U and has smaller values outside U. The authors propose a self-correctable policy that relies on a conservatively-extrapolated value function, a learned dynamical model, and a behavioral cloning policy. The policy adjusts the behavioral cloning policy locally by maximizing the value of the predicted next state. The proposed policy adjusts the behavioral cloning policy locally by maximizing the value of the next state. Assumptions are made regarding the correctness of the behavioral cloning policy and the learned dynamics model in a local context. The assumption states that if a state is near the demonstration set, there exists an action that can bring it closer to the set, ruling out dynamics that do not allow corrections. The dynamics are locally-correctable within certain parameters, ensuring that small errors can be corrected to stay close to the set U. The main theorem claims that under certain assumptions, the policy, value function, dynamics, and projection operator are all Lipschitz, allowing for local extrapolation and corrections to keep the state close to the set U. The main theorem states that the induced policy in Alg. 1 remains close to the demonstration set and performs similarly to the expert policy. It also guarantees that following the induced policy will lead to a state with near-optimal value, given certain assumptions and small errors. The policy \u03c0 will achieve a state s T close to a states T with value at least V \u03c0e (s T ) \u2212(\u03b5 + \u03b4 \u03c0 ) in a certain number of steps. The proof is deferred to Section B. The key idea is demonstrated in Figure 2. Lemma 4.5 states that if the current state is \u03b5-close to U, then the next state will also be \u03b5-close to U. The Q function is effectively represented by V (M (s, a)) in Alg. 1 to address the degeneracy issue. The paper discusses learning a conservatively-extrapolated Q-function. The discussion focuses on learning a conservatively-extrapolated Q-function to induce self-correctable policies. It emphasizes penalizing unseen states rather than unseen actions for effective learning. Additionally, it mentions addressing Lipschitz issues in the value function with a modified reward function. The paper introduces a method for learning a value function with conservative extrapolation using negative sampling from demonstration trajectories. It focuses on enforcing the value function to satisfy a conservative extrapolation requirement through a novel technique. The method enforces a conservative extrapolation requirement by drawing random \"negative samples\" from the neighborhood of U, inspired by negative sampling in NLP. This approach helps in constructing a loss function that approximates the projection of points back to U in a high-dimensional state space. The method enforces a conservative extrapolation requirement by drawing random \"negative samples\" from the neighborhood of U. This approach helps in constructing a loss function that approximates the projection of points back to U in a high-dimensional state space. The learning algorithm for V \u03c6 is defined with a loss function L(\u03c6) = L td (\u03c6) + \u00b5L ns (\u03c6), where perturbing s with Gaussian noise is done empirically. Learning the dynamical model involves using standard supervised learning with a 2 norm loss for model parameters \u03b8 instead of MSE loss. The method uses a 2 norm loss for model parameters \u03b8 instead of MSE loss, following previous successful optimization techniques. The induced policy from V \u03c6 and M \u03b8 is optimized using projected gradient ascent. The use of a target value function improves training stability in reinforcement learning. The distribution of expert trajectories is denoted by \u03c1 \u03c0e. Random shooting is sufficient for optimization due to the low-dimensional action space. Randomness helps reduce overfitting of the model and value function. The method aims to reduce overfitting by sampling actions and choosing the one with maximum value. Value iteration is used with environment interaction, alternating between updating the value function and models. The algorithms are evaluated in three simulated robotics environments. In simulated robotics environments, Gym and MuJoCo are used for tasks like Reach, Pick-And-Place, and Push. Hindsight Experience Replay (HER) is employed for training policies, with successful trajectories collected as demonstrations. Two settings are considered: imitation learning from demonstrations only, and leveraging demonstrations in RL with limited interactions. The algorithm is compared with Behavioral Cloning and previous state-of-the-art methods in both settings. Ermon (2016) compares with Gao et al. (2018) for continuous actions. Behavioral Cloning (BC) learns a mapping from state to action using supervised learning with MSE loss. Nair et al. (2018) combines HER with BC, using additional replay buffer, behavioral cloning loss, Q-filter, and resets. Our algorithm does not require resetting to a demonstration state like GAIL (Ho & Ermon, 2016). Generative Adversarial Imitation Learning (GAIL) imitates the expert by matching the state-action distribution with a GAN-like framework. Hindsight Experience Replay (HER) deals with sparse-reward environments by changing goals in the replay buffer. Discriminator-Actor-Critic (DAC) is a sample-efficient imitation learning algorithm that addresses reward bias by adapting the reward function and introducing an absorbing state. The text chunk discusses the use of TD3 in place of the RL algorithm in GAIL to improve sample efficiency. It also mentions the use of neural networks to parameterize the value function and dynamics model, as well as the refinement of the HER demonstration policy with linear interpolation. The results are presented in Table 1 and Figure 3 for different settings. The algorithm VINS outperforms BC on harder environments Pick-And-Place and Push due to its conservatively-extrapolated value function correcting policy mistakes. Results show VINS initialized RL algorithm surpasses prior state-of-the-art in sample efficiency by requiring fewer samples for warming up the value function. In RL, a slightly worse variant of the policy induced by VINS was implemented, leading to a lower initial success rate. Ablation studies were conducted to understand the importance of negative sampling, searching in the neighborhood of Behavioral Cloned actions, and a good dynamics model. The results are shown in Table 2. The study conducted ablation experiments to analyze the impact of negative sampling, Behavioral Cloning actions, and dynamics models on the performance of VINS in RL. A new algorithm, VINS, was introduced to learn self-correctable value functions and dynamical models from demonstrations, showing promising results. The study analyzed the impact of negative sampling, Behavioral Cloning actions, and dynamics models on VINS performance in RL. Results showed promising performance of VINS with various configurations. In an ablation study of VINS components in a setting without environment interactions, it was found that VINS without negative sampling (NS) performed consistently worse than VINS, highlighting the importance of NS in addressing false extrapolation. Improving the learning of dynamics, potentially through collecting data with random actions, could significantly enhance the performance of VINS or VINS without Behavioral Cloning (BC). Searching over the neighborhood of BC actions may be necessary due to inaccuracies in dynamics at state-action pairs far from the demonstration set. The dynamics learned on the demonstration set may not be accurate for state-action pairs far from it. Negative sampling by Gaussian perturbation may not be suitable for high-dimensional pixel observations. Fitting Q \u03c0e from only demonstrations is problematic as it may not be accurate for all possible actions. When learning the dynamics model from demonstrations, accuracy may be an issue for state-action pairs far from the demonstration set. Negative sampling with Gaussian perturbation may not work well for high-dimensional pixel observations. Fitting Q \u03c0e solely from demonstrations poses accuracy challenges for all possible actions. However, learning the dynamics model does not suffer from degeneracy issues like learning the Q function does when reaching a random goal state. The Q function struggles with degeneracy as it matches the expert policy on demonstrations but fails to utilize action information. Learning dynamics model from demonstrations avoids degeneracy issues faced by fitting Q function solely from demonstrations. Parameterizing Q by Q(s, a, g) = V(M(s, a), g) also avoids degeneracy. Proof of Lemma 4.5 shows the closeness of actions and the assumption errors in the demonstration state set. The Lipschitzness of \u03c0 BC ensures that a cx belongs to the constraint set of the optimization in equation (4.3), with a maximum value bigger than a cx. By the Lipschitzness of the dynamical model and the value function V \u03c0e, it is shown that s cx is \u03b3\u03b5-close to the set U. The proof of Theorem 4.4 involves applying Lemma 4.5 inductively for T steps and proving the closeness of s i to U. The proof involves applying Lemma 4.5 inductively for T steps. To prove bullet 2, it is shown that as long as si is \u03b5-close to U, the value function can be improved by at least \u03c1\u2212? in one step. The algorithm VINS+RL is outlined in pseudo-code in Algorithm 3, with parameters initialized from the result of VINS (Algorithm 2). In three environments, a 7-DoF robotics arm is manipulated for different goals. In three environments, a 7-DoF robotics arm is manipulated for different goals. The task involves reaching a target location, grasping a box and reaching a target, or pushing a box to a target. The reward function is 0 for success and -1 for failure. An optimal agent should complete the task in the shortest path possible using a neural network with 3 hidden layers and ReLU activation functions. Training continues until the test success rate plateaus. We use default hyperparameters with 17 CPUs for GAIL, HER, and Discriminator-Actor-Critic implementations. Architecture includes feed-forward neural networks for values and dynamical models with specific hidden layer configurations. The algorithm uses reduced states and actions to predict the next states. Data augmentation is done through linear interpolation. The Adam optimizer is used with a specific learning rate. Some less relevant coordinates are removed from the state space. The states representation and perturbation are focused on the reduced states space. The algorithm focuses on training V on reduced states and designing separate perturb functions for each task. Gaussian noise is added to the state s for perturbation. Ablation study of VINS components without environment interactions is reported in Table 3. The study evaluates VINS in a setting without environment interactions. The success rate of BC with data augmentation is consistently lower than BC without data augmentation. Additional experiments include BC with data augmentation and VINS with oracle without negative sampling. The data augmentation process involves sampling and constructing new state-action pairs for behavior comparison. The study found that data augmentation for behavior cloning hurts performance, as it may provide incorrect information due to non-linear actions between states. Negative sampling is essential for the success of VINS, as shown by comparing VINS with and without negative sampling. VINS with fewer demonstrations was also evaluated. VINS was evaluated with only 40 expert trajectories, showing better performance than BC with 100 trajectories. VINS corrected actions using a learned value function and model, achieving a higher success rate. The study also explored VINS with a stochastic dynamics model, which can be easily generalized from the deterministic model. Proof-of-concept experiments in a stochastic environment based on Push showed that adding noise to actions improved performance. The study compared VINS and BC in achieving goals without environment interactions. VINS outperformed BC in all tested environments, even with a deterministic dynamics model."
}