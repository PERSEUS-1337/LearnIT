{
    "title": "HkePOCNtPH",
    "content": "In this paper, a method for algorithmic melody generation using a generative adversarial network without recurrent components is presented. The approach utilizes DCGAN architecture with dilated convolutions and towers to capture sequential information as spatial image data, enabling the learning of long-range dependencies in fixed-length melody forms like Irish traditional reels. Automated music composition has a long history dating back to the 1957 \"Illiac suite,\" evolving from rule-and-randomness based methods to modern machine learning techniques. One of the first machine learning approaches to music generation was by Conklin & Witten (1995), using entropy as a measurement in a multiple viewpoint system. Standard feedforward neural networks struggle with sequence-based information like music, leading to the use of recurrent neural networks for melody generation. Todd (1989) and Eck & Schmidhuber (2002) utilized sequential models and LSTM structures for music composition. In 1989, Todd introduced a model for music composition. Eck & Schmidhuber (2002) successfully used an adapted LSTM structure to generate music with short-term structure and higher-level context. Various RNN-based melody generators have been developed by different researchers. RNN architecture offers flexibility in generating music of varying lengths, but it may not be suitable for traditional Irish music like jigs and reels, which have a more rigid format. Several RNN-based generators were trained on Irish traditional melodies from TheSession, presenting music as two-dimensional to capture long-term dependencies and align related parts for better representation. In this project, deep convolutional generative adversarial networks (DCGANs) are used for melody generation without recurrent components. The focus is on capturing higher-level structures in music pieces and mimicking the interplay between musical motifs using dilated convolutions of semantically meaningful lengths. DCGANs have been used in music generation to capture long-range dependencies, often combined with recurrent components. However, pooling is ineffective for music due to pitch averaging creating notes outside the 12-semitone scale. Irish traditional music has influenced various music genres despite emigration and connections to neighboring cultures. The influences of traditional Irish music can be found in folk music of Newfoundland and Quebec, as well as Bluegrass from the Appalachian region. The core of Irish instrumental dance music is usually monophonic, with a distinct 8-bar structure that repeats with variations. Harmonies and embellishments are considered extra and not usually transcribed. The core of Irish instrumental dance music is usually monophonic, with a distinct 8-bar structure that repeats with variations. The second repeat of a part is transcribed separately, making the transcription 16 to 32 bars long. Irish tunes commonly use D major key, Dorian, and Mixolydian modes, with a time signature of 4/4 for reels and 6/8 for jigs. Musicians traditionally learn tunes by ear. The ABC notation, developed by Chris Walshaw in the 1970s, represents Irish tunes with letters from A-G and modifiers for octaves, sharps, and flats. Duration modifiers are used to indicate note lengths, with the default note length specified in the header. This text-based representation simplifies the learning process for musicians. ABC notation, developed by Chris Walshaw in the 1970s, simplifies the representation of Irish tunes using letters from A-G and modifiers for octaves, sharps, and flats. Note lengths are indicated with duration modifiers, with the default note length specified in the header. Bar separators and spacing are used for readability. ABC notation is widely used for Irish folk music, with features for ornaments and repetitions. TheSession dataset contains over 31,000 tunes, including 10,000 reels in a major key. After filtering out improperly formatted tunes and those with unwanted features, 820 reels were used for training. After filtering out tunes with more than 16 bars, 820 reels were used as training data. ABC notation samples were converted into numerical vectors with 16 numbers per bar (256 total), representing 16 possible notes. Simple postprocessing helped recover tunes with minimal variation. The encoding represented the vectors as a 64x4 image, with midi values as pixel values, allowing for long-range analysis of notes and bars alignment. The vertical alignment of notes and bars in tunes allows for exploiting long-range dependencies among phrases. Sequential data is traditionally learned using RNNs, but simple implementations struggle with capturing long-distance dependencies. Bi-directional RNNs with an activation mechanism have been used to address this issue, allowing for capturing information from multiple sections of the sequence. DCGANs with dilations enable capturing long-distance dependencies in sequential data, offering benefits over RNNs. Dilated convolutions allow for domain-specific heuristics to be incorporated into the neural network. Incorporating dilations in neural networks allows for domain-specific heuristics to be passed to the network, providing flexibility and fine-tuning not available in RNNs. GANs do not require a specific seed for generating outputs, making it easier to produce novel results. GANs have been successful with images, but music generation with GANs often relies on RNNs in the discriminator or generates music sequentially. Incorporating dilations in neural networks allows for domain-specific heuristics to be passed to the network, providing flexibility and fine-tuning. A convolution network with 6 convolutions run side-by-side is used, with dilated filters on 5 towers to capture global context and a 2x9 convolution for local context. Multiple towers learn different aspects to avoid \"tunnel vision\". The towers use 32 convolution filters with zero-padding for edge pixels. Outputs are stacked horizontally and passed to a second layer of convolution with 64 filters. The discriminator uses a 3x3 kernel with a 2x2 stride, no batch normalization, and a dense layer with 1024 neurons followed by a sigmoid neuron for prediction. The generator starts with a dense layer of 32x2x256, reshaped into 256 filters of size 32x2, and goes through two deconvolution layers halving the filters, with a final deconvolution layer to produce one image at 64x4 dimension. The generator in the GAN model uses a stride of 2x2 to return to the original image dimension of 64x4 and employs batch normalization. The generated music in a 64x4 matrix is mapped to notes in ABC notation, with notes in the D major key. The ABC notated music can be converted into sheet music or played with midi generators. This approach is similar to the model by Sturm et al. (2016) and Magenta's models are considered state of the art in music generation. The models in Magenta are state of the art in music generation. Samples were compared with Folk-RNN and Magenta models, specifically the MelodyRNN model. The comparison used the normalized mean Fr\u00e9chet distance to highlight similarity in structure. Results are shown in Figure 2. The comparison between generated samples from the MelodyRNN model and Folk-RNN model shows favorable results in terms of similarity in structure. The t-SNE algorithm was used to visualize the similarity between the tunes generated by both models and the training data. The tunable parameter perplexity was set to 50 for training data. Visualizations from t-SNE algorithm show that tunes generated by FolkRNN lie within the training data distribution, with FolkRNN producing the most diverse samples. Magenta's tunes are a subset of FolkRNN's. Frequency distribution of notes in samples is shown in Figure 4. The frequency distribution of notes in the samples, as shown in Figure 4, indicates that the tonic of the key (corresponding to D) is the most common, with the 3rd and 5th notes being more frequent. Our DCGAN model produces music comparable to RNN-generated melodies, with a focus on global tune structure using dilations. Converting sequential data into spatial information effectively generates samples of whole pieces, as seen in our exploration of melody generation for fixed-length music forms like an Irish reel. The model uses non-recurrent architecture for generating fixed-length music forms like an Irish reel. It learns global and contextual information simultaneously, even with a small model. Future work includes introducing boosting to capture distribution structure more faithfully and expanding the range of generated pieces. Other potential enhancements involve adding multiple channels for better duration capture and experimenting with higher-dimensional sequence data representation. The model uses non-recurrent architecture for generating fixed-length music forms like an Irish reel. Future work includes introducing boosting to capture distribution structure more faithfully and experimenting with higher-dimensional sequence data representation."
}