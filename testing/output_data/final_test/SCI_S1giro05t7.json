{
    "title": "S1giro05t7",
    "content": "In reality, current algorithms often make highly confident yet wrong predictions when faced with unexpected test samples from an unknown distribution different from training. Unlike domain adaptation methods, we cannot gather an \"unexpected dataset\" prior to test, and unlike novelty detection methods, a best-effort original task prediction is still expected. Two proposed methods aim to reduce overconfident errors of samples from an unknown novel distribution without drastically increasing evaluation time: G-distillation, training an ensemble of classifiers and then distilling into a single model, or NCR, reducing prediction confidence based on its novelty detection score. In experiments, calibrating using temperature scaling on familiar data is the best method for improving novel confidence. It can reduce confident errors, such as in gender recognition, by 95% on demographic groups different from the training data. The i.i.d. assumption in machine learning and computer vision is prevalent, where training and test sets are sampled from the same distribution. In real-life applications, test samples are often different from training samples, leading to unreliable predictions from deep networks. This issue extends to semantically related input, such as gender classifiers applied to faces of different age groups. Deep networks are prone to making confident wrong predictions on novel samples, leading to catastrophic consequences. Trust in AI systems can be eroded when errors occur, highlighting the risks of differently distributed samples. Deep networks can make confident errors on novel samples from different distributions, posing risks to AI systems and society. Trust in AI requires avoiding such mistakes, emphasizing the importance of training on relevant datasets. The paper aims to reduce confident errors in predictions on samples from unknown distributions, focusing on prediction confidence rather than just accuracy. Novel samples can lead to errors in AI systems, highlighting the need to train on relevant datasets. The paper focuses on improving prediction confidence by considering ensemble methods and distilling models to reduce errors on novel data. By training on both labeled and unlabeled examples, a single model can outperform ensembles. The study explores improving prediction confidence by training on labeled and unlabeled examples, outperforming ensembles with a single model. It suggests lowering confidence for unseen data using a novelty detector to reduce errors, potentially impacting familiar sample performance. The study demonstrates deep networks' generalization behavior by creating \"familiar\" and \"novel\" test splits on a 2-dimensional toy dataset. Training data is provided on the \"familiar\" distribution, while testing data is reserved for the \"novel\" distribution. Multiple network runs show similar performance on familiar regions but vary in novel regions due to optimization randomness. An ensemble of 10 networks can smooth predictions and reduce errors in novel regions. Column 5: An ensemble of 10 networks can smooth predictions and reduce errors in novel regions. Column 6: Distilling the ensemble using training samples results in the same irregularities as single models. Column 7: Proposes distilling the ensemble into a single model using labeled training data and unsupervised data from a \"general\" distribution. Evaluation focuses on negative log likelihood (NLL) and the fraction of highly confident misclassifications (\"E99\"). Contributions include highlighting the issue of highly confident wrong predictions. The curr_chunk discusses the problem of highly confident wrong predictions when samples are drawn from an unknown distribution different from training. It evaluates methods to reduce these errors and proposes an experimental methodology to study the issue. The paper highlights the importance of epistemic uncertainty and compares its approach to related works. The curr_chunk discusses evaluating models that agree with training data, focusing on Bayesian approaches and MC-dropout for uncertainty estimation. It proposes improving confidence estimates for novel samples and differs from related works in this aspect. The curr_chunk discusses efficient confidence estimation for novel samples compared to MC-dropout and ensembles. It introduces the BID13 method for reducing confidence towards unseen data, similar to the proposed NCR method. Domain adaptation BID36 aims to improve performance on a different target domain, which is challenging in image classification for unexpected samples. Sampling an \"unexpected dataset\" prior to test time is considered unviable. Domain generalization, similar to our work, focuses on building models that generalize well on unspecified domains with different distributions from training domains. These models aim to create domain-invariant feature spaces or models, or separate domain-invariant and domain-specific parts. Attribute-based approaches also aim to generalize to novel object categories by selecting discriminative features within classes to build invariance across object classes. Our method focuses on novel data without requiring multiple training domains, unlike other methods that estimate system failure and request external intervention. Unlike previous works, we analyze confidence estimates and rejection ability for samples from novel distributions. The text discusses outlier detection, novelty detection, and one-class classification methods for determining if a test sample comes from the same distribution as the training data. It also mentions techniques in deep learning literature to minimize the generalization gap, such as data augmentation, dropout, batch normalization, and weight decay. Additionally, better hyperparameter selection strategies for generalization are proposed. Bagging and model averaging techniques are also used prior to deep learning. Calibration methods aim to improve confidence estimates but do not reduce risk for novel samples. Experimental findings show that these methods perform well on unseen novel data. Distilling can be used on unsupervised samples to obtain soft labels on transformed unlabeled data. The focus of this work is to improve confidence estimates of deep models on unseen data by producing accurate confidences for samples from novel distributions. Unlike existing methods, which aim to build new classes or reduce sensitivity to adversarial examples, this approach concentrates on generalizing existing classes to novel samples within. The study aims to enhance confidence estimates of deep models on unfamiliar data by providing accurate confidences for samples from novel distributions. The framework focuses on classification tasks but can be extended to tasks like VQA and semantic segmentation. The goal is to generalize existing classes to novel samples within unfamiliar distributions. The study aims to improve confidence estimates of deep models on unfamiliar data by enhancing the estimation of P N (y|x). Training is done on F tr and unsupervised G tr, while testing involves F ts and N ts. Distillation of an ensemble involves training f Ens on F tr, obtaining soft labels y F, and training a new single-model network f \u03b8. Distillation involves training a new single-model network f \u03b8 using probability predictions from an ensemble, applying distillation loss L dis. A classification loss L cls is added for further improvement. G-distillation aims to mimic the ensemble on novel data x G \u223c G to enhance distillation performance on unfamiliar data. To improve distillation performance on unfamiliar data x G \u223c G, a diverse and complex dataset G tr needs to be selected. This dataset should be relevant to the task and can be obtained through online searches or mining examples from datasets with different annotations. For example, to make a gender classifier robust to additional age groups, G tr can be sampled from the CelebA BID45 dataset, even though CelebA does not have age information. To improve distillation performance on unfamiliar data x G \u223c G, a diverse dataset G tr is selected from ImageNet. An ensemble f Ens is trained to obtain soft labels y F and y G. Samples from G tr are added to the training set, and the model is trained using the combined data. At test time, probability estimation y F is evaluated against ground truth in F ts, and y N against those in N ts. Method 2 involves using a novelty detector ODIN for Novelty Confidence Reduction (NCR). The ODIN procedure is used for novelty detection on input x, providing probability estimate \u0177 and detection score s 0. The detection scores come from various datasets like ImageNet and COCO, but may need normalization. At test time, different methods for novelty detection have varying efficiency. G-distillation and NCR are faster as they require fewer forward passes compared to ensemble methods like BID24 and MC-dropout BID9. However, G-distillation involves training both an ensemble and a distilled model. For implementation details, refer to Appendix B. The effectiveness of handling unexpected novel data is demonstrated using various datasets like ImageNet and COCO. To analyze unexpected novel data and reduce failures, extensive analysis is conducted on four classification datasets. Different scenarios are mimicked, such as gender recognition with selective bias and ImageNet animal superclass recognition with unseen species. Cat-dog recognition and VOC-COCO recognition are also performed to ensure non-overlapping familiar and novel classes. The study focuses on VOC-COCO recognition, where familiar and novel object categories are distinguished. Pre-training on ImageNet may not be suitable as it contains classes that overlap with novel choices. Instead, the study uses Places365-standard BID54 for pre-training to ensure the novelty of the unseen classes. Our method generalizes to ImageNet pre-trained models with disjoint classes. Performance is reported on validation sets for various datasets, with training splits for hyperparameter tuning. Comparison is made to single models, standard distillation, and ensemble methods. In experiments comparing different methods for model calibration and uncertainty modeling, including distillation, ensemble methods, and BID12 and BID22 approaches, it was found that some methods perform better on novel samples while others perform better on familiar samples. This suggests a trade-off between smoothing probabilities regardless of sample familiarity or prediction accuracy. The performance of different methods for model calibration and uncertainty modeling varies based on sample familiarity and prediction accuracy. By adjusting the temperature of prediction logits, the trace of how familiar and novel negative log likelihood (NLL) changes can be analyzed. The NLL is used as a measure of prediction confidence quality, with a preference for confident correct predictions. The NLL is used as a measure of prediction confidence quality, with a preference for confident correct predictions. During evaluation, softmax probability estimates are clipped to prevent extremely confident wrong predictions from dominating the NLL. This approach aims to ensure the model learns from familiar labeled data and produces the correct confidence for novel data. The study evaluates the performance of binary classifiers and multiclass scenarios, including a confident error rate (\"E99\") for samples with probability estimates above 0.99. Accuracy, mean Average Precision (mAP), and comparison with baselines are also reported. The experiments are run 10 times for statistical significance, with the ensemble method run only once. G-distill is compared to baselines on four scenarios, showing that calibrated methods perform best in terms of novel NLL. Our methods outperform single-model methods on novel data NLL, while calibration methods perform the best overall. Trade-offs between familiar and novel NLL are shown in graphs, with our methods not outperforming single models in this aspect. Performance on VOC-COCO recognition is strong, with a focus on the overlap between familiar and novel data. In experiments with novel distribution N, G-distillation performs similarly to distillation, while NCR underperforms. Single models struggle with novel NLL but ensembles show improved performance on both familiar and novel data. Properly calibrated T-scaling with single models can match or exceed the performance of all methods using a single model at test time. Proper calibration is crucial for reducing confident errors on novel data, as shown in experiments. Single models can outperform all methods except ensembles when calibrated using familiar data. Our proposed methods, G-distill and NCR, perform well in balancing familiar-novel data. Distillation techniques show improvements in novel NLL, with G-distill outperforming in some cases. Smoothing trade-off affects uncertainty levels in predictions. Smoothing with different levels of \u03c4 can improve accuracy and mAP, but ultimately, single models are superior in confidence quality. Even if methods like uncertainty, distillation, G-distill, and NCR are calibrated, they may not outperform a regularly trained single model in confidence. The error rate among novel confident predictions is lower with calibrated models compared to non-calibrated methods, indicating better calibration. The effectiveness of the novelty detector in distinguishing familiar and novel samples is not very high, but it is effective in separating wrongly classified samples. The E99 for NCR and calibrated methods is lower than 0.01, indicating under-confident predictions. Other metrics show competitive performance for accuracy and mAP compared to other methods. The novelty detector is effective in separating wrongly classified samples, with competitive performance in various metrics. Factors like ensemble type and base network size impact the experiments, and training an ensemble of G-distillation models can improve performance at the cost of test time performance. Combining G-distillation and NCR did not significantly improve results. In this paper, the importance of minimizing harm from confident errors in unexpected novel samples is highlighted. Two simple methods using ensemble and distillation are proposed to regulate network behavior and reduce confident errors. Future work could explore handling adversarial examples and improving calibration with unexpected novel samples. The paper emphasizes minimizing harm from confident errors in unexpected novel samples by proposing methods using ensemble and distillation to regulate network behavior. Toy datasets are used to demonstrate generalization behavior of networks on 2-dimensional feature spaces, showing that networks may struggle to generalize to novel regions. The paper focuses on reducing errors in novel samples by utilizing ensemble methods. It shows that ensembles provide more reliable confidence estimates on novel data compared to individual networks. Additionally, the negative log likelihood performance is significantly improved for novel samples when using an ensemble. In experiments, the method performs closer to the ensemble than standard distillation, showing improved performance on novel data. Toy experiments with simple and complex datasets confirm these findings. The method emulates the ensemble in terms of robustness in novel regions. For toy experiments, 1200 samples are taken for both train and validation, with a test set divided into familiar and novel sets. A 3-hidden layer network with 1024 hidden units and Glorot initialization is used, along with batchnorm and dropout after ReLU. The same hyperparameter tuning, initialization, and training procedures are applied as described in the implementation details. For image experiments, \u03bb dis = 2/3 and \u03bb cls = 1/3 are set as experimental settings. In the experimental settings, \u03bb dis = 2/3 and \u03bb cls = 1/3 are set. Temperature T = 2 is used for sampling G tr. ResNet18 BID16 is primarily used for the network, with an experiment on DenseNet161 BID20 to ensure architecture independence. Pre-trained networks on large datasets are used for training on small datasets. A simple ensemble of 10 members is utilized, compared to a bagging ensemble scheme which undermines log likelihood. Glorot initialization is used for the final layer, and optimization is done with stochastic gradient descent. For data augmentation, random crop and mirroring similar to Inception are used. Learning rate is lowered to 10% when validation performance plateaus, and hyper-parameter tuning is done for parameters like learning rate and number of epochs. N is unknown during training, so F ts performance is used as the tuning criteria. Hyper-parameter tuning for NCR method is challenging due to the unavailability of a novelty dataset. The Pets dataset is split into training and validation sets. Hyper-parameter tuning is challenging, and the Pets dataset is split into familiar and novel sets for validation. A grid search is used to tune the hyperparameter, resulting in \u03bb s = 0.15 for all experiments. Simple ensembles are used over bagging ensembles for better performance on familiar data. Bagging shows better performance on novel data without smoothing, but there is a trade-off with a loss in both novel and familiar NLL. Network structure evaluation is also discussed. In experiments using a DenseNet161 base model, our methods outperform single models but are surpassed by calibrated models. G-distill underperforms T-scaling without smoothing. A bagging ensemble of G-distilled networks is trained, sacrificing test efficiency. Soft labels are obtained and used to train 10 G-distilled networks, whose outputs are averaged at test time for probability estimation. The \"G-distill \u00d710\" scheme involves training 10 G-distilled networks whose outputs are averaged for probability estimation. Results show improved foreign NLL compared to the original ensemble, but familiar NLL lags behind. Despite falling behind in test efficiency, G-distill \u00d710 performs similarly to the ensemble in terms of error rates and accuracy on both familiar and novel data. Using an ensemble of G-distilled models improves animal recognition with ImageNet, but lags behind in familiar data confidence compared to the original ensembles."
}