{
    "title": "HJerDj05tQ",
    "content": "Optimization on manifold is commonly used in machine learning to handle constraint optimization problems. Previous works have focused on single manifolds, but in practice, problems often involve multiple constraints represented by different manifolds. We propose a unified algorithm framework for optimizing on multiple manifolds by integrating information and moving along an ensemble direction. The convergence properties of the algorithms are proven, and they are applied to training neural networks with batch normalization layers, showing favorable empirical results. Training neural networks with batch normalization layers can lead to preferable empirical results. Optimization problems in machine learning often come with multiple constraints, which can be handled by adding regularization terms or optimizing on manifolds determined by the constraints. Optimizing on manifolds can make the optimization problem unconstrained on the manifold, leading to easier technical solutions and potential performance gains when training neural networks. In this paper, the focus is on optimization on multiple manifolds, where each constraint corresponds to a different manifold. Traditional optimization methods on manifolds may not work in this case, so a new method is proposed using drifts derived from descent information on other manifolds to solve the optimization problem. The paper introduces an algorithm for optimization on multiple manifolds, incorporating drifts from other manifolds to the moving direction. Various optimization methods on manifolds are discussed, including popular first-order algorithms and Riemann approaches applied to deep neural networks. The algorithm incorporates drifts from multiple manifolds during optimization, with a proof of convergence applicable to various algorithms. Manifolds are defined as subspaces of R^n with tangent spaces, and gradient descent iterates may not stay on the manifold due to it not being a linear space. The algorithm deals with optimization on manifolds, introducing a retraction function to ensure points stay on the manifold. Gradient descent on the manifold is defined using the Riemannian gradient, which projects the gradient onto the tangent space. A lemma describes properties of local optima for the optimization problem. The text discusses the concept of local optima for optimization problems on manifolds, introducing the Lipschitz gradient condition and descent condition for iterative algorithms. The convergence theorem is presented for object functions with lower finite values. Theorem 2.1 states that if f is lower finite and the iteration sequence satisfies the descent condition, then the limit of a_k times the gradient of f(x_k) as k approaches infinity is 0. The algorithm proposed integrates information from two manifolds to find the minimum point on their intersection. The algorithm presented integrates information from two manifolds to find the minimum point on their intersection. It involves gradient descent on manifold with drifting, where the updating rules reduce to normal gradient descent under certain conditions. The convergence theorem explains how to set parameters in the algorithm for convergence to a local minimizer. The construction of the descent condition and the drift in the algorithm involve integrating information from two manifolds to find the minimum point on their intersection. The drift gives a force moving towards the minimizer on the other manifold, with the distance between points x and y becoming small as k approaches infinity. The scale of the drift is the same as the original Riemannian gradient, making information from another manifold have minimal effect. The algorithm integrates information from multiple manifolds to find the minimum point on their intersection. The contribution from another manifold can be controlled by adjusting a parameter. The convergence rate of the algorithm can be proven, and the number of iterations needed for a specific accuracy can be determined. The algorithm is described for the case with more than two manifolds, with the updating rule defined based on Lipschitz gradient. The updating rule for the algorithm based on Lipschitz gradient is defined as DISPLAYFORM2. By choosing a specific value for a(1), the input value to a neuron is transformed. Batch normalization can have adverse effects on optimization due to different scaling leading to convergence to different local optima. To avoid this, the effect of scale can be eliminated. To address the adverse effects of different scaling in optimization, weight w on Grassmann or Oblique manifold can be considered to ignore scale. BID3 and BID6 discuss BN(w) having the same image space on G(1, n) and St(n, 1), allowing optimization on the intersection of the two manifolds. Optimization on a manifold relies on Riemannian gradient gradf(x) and Retr x, leading to a unit critical point on both Grassmann and Oblique manifolds. The discussion focuses on Grassmann and Oblique manifolds, where elements with the same direction are considered the same. Training neural networks with batch normalized weights involves finding a local minimizer on the intersection of St(n, 1) and G(1, n). Riemannian gradient is used for optimization on G(1, n), leading to a unit critical point on both manifolds. In the context of Grassmann and Oblique manifolds, the discussion delves into the application of Riemannian gradient for optimization on G(1, n) and St(n, 1). The process involves treating gradient and weight matrices as vectors to simplify calculations, updating parameters layer by layer using Algorithm 1. The data sets CIFAR-10 and CIFAR-100 are utilized in this section. In this section, the algorithm is tested using the CIFAR-10 and CIFAR-100 datasets, which are color images with 10 and 100 classes respectively. The neural network used is WideResNet BID15, with batch normalization applied to weight parameters in every hidden layer. The minimizers of the neural network with batch normalized weights lie on the intersection of Grassmann and Oblique manifolds. Biases in hidden layers are updated separately by SGD, while the algorithm calculates mean loss for each training step. The algorithm tested on CIFAR-10 and CIFAR-100 datasets uses WideResNet BID15 with batch normalization. It operates on two manifolds, G(1, n) and St(n, 1), with updating rules including norm-clipping and learning rate adjustment. Three methods are compared: Drift-SGDM, SGDM, and SGD. Output class prediction is made by combining two model outputs. In the experiment, the algorithm uses WideResNet BID15 with batch normalization on CIFAR-10 and CIFAR-100 datasets. It operates on two manifolds, G(1, n) and St(n, 1), with updating rules including norm-clipping and learning rate adjustment. Three methods are compared: Drift-SGDM, SGDM, and SGD, with output class prediction made by combining two model outputs.\u03b4 = 0.9 and initial learning rate \u03b7 m = 0.4 for weights parameters, \u03b7 for biases is 0.01, norm clip is 0.1, training batch size is 128, and the number of training epochs is 200. In the experiment, the algorithm operates on Grassmann manifold and Oblique manifold, comparing Drift-SGDM and SGDM. Accuracy curves and test set results validate the algorithm's performance, especially on larger neural networks. The algorithm lacks a regularization term affecting generalization, which can be improved by adding a term like in BID3. Setting a smaller \u03b4 can enhance the influence of the drift term. The method presented in the paper addresses optimization problems with multiple constraints on the intersection of multiple manifolds. The method integrates information among multiple manifolds to find minimum points on each manifold without adding extra conditions to optimization problem constraints. The algorithm's flexibility allows for the derivation of various other algorithms. The use of different retractions, such as Exp x, may lead to more elegant results. In this paper, the focus is on optimization on manifolds, specifically embedded sub-manifolds and quotient manifolds. The challenge lies in calculating the tangent space and Riemannian gradient. The framework of gradient descent with drift is studied, with R^n being considered as a manifold in a special case. The framework of gradient descent with drift is explored, treating R^n as a manifold. The algorithm involves gradient descent with momentum and convergence is proven. By incorporating a regularization term, the iteration point is achieved. The non-stochastic drift can be modified to a stochastic term for a non-descent algorithm, allowing for jumping from local minimizers. The update rule includes a random vector with mean vector \u00b5 and covariance matrix \u03a3. The algorithm involves a non-descent method with stochastic noise, using a random vector with mean vector \u00b5 and covariance matrix \u03a3. The convergence theorem of the algorithm is provided, showing that it is non-descent and how the parameters a k and b k are set. The theorem states that for a function f (x) \u2265 f * > \u2212\u221e with Lipschitz gradient, the sequence converges when 0 < \u03b4 < 2. The noise \u03be k has a small effect on the iteration process when k is large, allowing for jumping from local minimizers. The proof of Theorem 2.1 and Theorem 2.2 is provided, along with the necessary lemmas. The algorithm involves a non-descent method with stochastic noise, showing convergence with Lipschitz gradient conditions. The projection matrix P x k is discussed, and the impact of random variables on algorithm (2) is highlighted. The algorithm (2) is not necessarily a descent method due to two random variables. \u03a3 is a symmetric positive definite matrix. Schwarz equality and definition of a_i lead to DISPLAYFORM3. Fatou's lemma results in DISPLAYFORM4."
}