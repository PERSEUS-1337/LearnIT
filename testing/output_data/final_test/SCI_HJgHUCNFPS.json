{
    "title": "HJgHUCNFPS",
    "content": "Multi-view video summarization (MVS) faces challenges with inter-view correlations and overlapping cameras. Existing offline methods lack focus on uncertain environments, requiring extra bandwidth and time. A new edge intelligence based MVS approach is proposed, utilizing spatio-temporal features for activity recognition in IoT environments. Multi-view videos are segmented on slave devices using a light-weight CNN model, with mutual information used to generate summaries. The system encodes and transmits data to a master device with neural computing stick (NCS) for efficient activity recognition and inter-view correlation computation, saving resources and time. Experiments show a 0.4 increase in performance. The proposed edge intelligence based MVS approach utilizes spatio-temporal features for activity recognition in IoT environments. Experiments show an increase in F-measure score on MVS Office dataset and activity recognition accuracy on UCF-50 and YouTube 11 datasets. Time complexity is reduced for faster MVS processing. A new dataset with synthetic fog demonstrates system adaptability for surveillance in uncertain environments. Surveillance cameras generate large video data, leading to storage and computational complexity issues addressed by video summarization. Video summarization aims to condense data size by extracting key information from lengthy videos and suppressing redundant frames. Single-view video summarization (SVS) is generated from a single camera, while multi-view video summarization (MVS) is from a camera network. SVS is extensively researched for applications in surveillance, sports, and news videos. MVS faces challenges like computing correlations among views and varying light conditions. The basic flow of MVS includes input acquisition, preprocessing, feature extraction, post-processing, and summary generation. The mainstream MVS methods focus on clustering and low-level features for summary generation, but overlook specific targets like persons and vehicles. This leads to important frames being missed in the final summary. Existing techniques lack analysis of the generated summary for tasks like indexing and activity recognition. These methods are limited to certain environments and do not address uncertain scenarios, making them inadequate for real-world use. Activity recognition at the edge is crucial in today's technological era, as existing methods are not suitable for real-world environments. Most techniques process data on powerful servers, leading to delays and lack of responsiveness in abnormal situations. A novel framework is proposed for activity recognition over the edge, addressing challenges in both certain and uncertain scenarios. This framework is essential for surveillance in smart cities. The proposed framework integrates MVS and activity recognition in IoT environments using interconnected resource-constrained devices. It aims to address challenges in both certain and uncertain scenarios, enabling object detection, summary generation, and activity prediction. The framework integrates object detection, summary generation, and activity recognition using a wireless sensor network. Slave devices capture video data, generate summaries, and transmit keyframes to a master device equipped with an INTEL Movidius NCS for activity classification. The NCS, a deep learning accelerator, enables low-power activity recognition using temporal point processing for effective classification. The framework integrates object detection, summary generation, and activity recognition using a wireless sensor network. An algorithm for MVS on resource-constrained devices reduces time complexity with higher accuracy. Uncertainties like fog are added to an outdoor MVS benchmark dataset to showcase the framework's adaptability in various scenarios. The framework also considers the capacity and traffic of WSN for efficiency. The paper discusses the adaptability of a framework for wireless sensor networks (WSN) in handling capacity and traffic. It explores tradeoffs in transmission time, keyframe quality, and activity recognition accuracy using different classifiers. The literature review covers MVS and activity recognition, highlighting the complexity of MVS methods and the diversity of techniques in activity recognition. Activity recognition is a diverse research field with various techniques for different applications. MVS methods typically use handcrafted features and traditional machine learning for summary generation. Initial MVS schemes utilized features like SIFT descriptors and motion features with statistical learning approaches. Later trends added preprocessing steps and used supervised or unsupervised learning for summary generation. The latest trend in MVS utilizes mid-level features and motion-based shot boundary detection. The recent advancements in MVS involve utilizing mid-level features, motion-based shot boundary detection, and deep learning techniques for video summary generation. Techniques such as sparse coding, clustering, template matching, and sparse representative selection are used with learned features like BVLC CaffeNet and Spatiotemporal C3D for improved video representation and summary generation. The recent success of deep learning methods in activity recognition involves extracting features from deep learning models and applying sequential learning. Various approaches, such as using pre-trained AlexNet with bi-directional LSTM or optical flow convolutional features with multi-layer LSTM, have shown promising results. However, some methods have limitations in real-world surveillance networks due to their long running time. The text discusses optimizing the VGG-19 model for resource-constrained devices in activity recognition systems. The framework includes modules for video acquisition, shot segmentation, and deep feature extraction for activity prediction. Multiple resource-constrained devices are connected to a WSN in IoT for efficient processing. The proposed system involves slave devices with cameras capturing multi-view video data, applying shot segmentation, extracting keyframes, encoding, and transmitting them to a master device for computing inter-view correlations and activity recognition. A tiny version of the YOLO CNN model is used for detecting humans and vehicles, with a focus on overcoming limitations in foggy scenarios by retraining the model with foggy data. The YOLO tiny model was retrained on foggy data for better learning in real-life surveillance scenarios. Fog was applied globally to images from the COCO dataset using different levels of fog. An optimal fog parameter of 220 was selected for object detection in foggy environments, providing reliability and overcoming limitations in existing datasets. The trained model is used for object detection on multi-view outdoor datasets. Frames with persons or vehicles above a confidence score of 0.9 are stored. Segmented frames with objects are used to identify events like sitting on a chair or entering a room in MVS datasets. Shots with humans detected are segmented for further analysis. The shots with desired objects are numerous but contain redundant frames, so mutual information between consecutive frames is computed to provide a compact representation. This process is carried out on slave devices with limited resources, using low-level features intelligently. Instead of using distance measuring methods, mutual information between feature vectors is compared for better results. Keyframes are selected from frames with persons to avoid redundancy, and the formula for mutual information is provided. In surveillance video analysis, mutual information between consecutive frames is computed to create a compact representation on a constrained device. Human activity recognition involves extracting spatiotemporal features, with a pre-trained VGG-19 CNN model for spatial features and an auto-encoder for temporal features. VGG-19 is preferred over AlexNet and GoogleNet for capturing tiny patterns in visual data. The VGG-19 model is used for activity recognition, extracting visual features with 33 filters in convolutional layers and a fixed 1-pixel stride. It has sixteen convolutional and three fully connected layers, with deep features extracted from the final fully connected layer (FC8) of 11000 dimensions. The FC7 layer outputs 4096 dimensions, resulting in a large feature vector for a single sequence. The VGG-19 model used for activity recognition generates a large feature vector for a single sequence, making it unsuitable for real-time processing on resource-constrained devices. Spatial frame-level features are extracted at 15 fps, resulting in 15000 features per second. These features are compared to compute inter-view correlations, with a compressed model size of 287.3 MB for processing on Movidius VPU stick. The auto-encoder is used to learn temporal changes and sequential patterns in 15000 features. Experiments were conducted to reduce the dimensionality of the features, with one setting encoding to 8000 and then to 1000 features. This setting showed low mean square error but had high time complexity. Another setting directly encoded the 15000 features. In one setting, 15000 features were encoded to 8000 features, and in another setting directly to 1000 features. Despite a slight accuracy compromise with linear SVM, computational requirements for embedded devices were met. Auto-encoders were trained for 40 epochs with sparsity and L2 regularization to prevent over-fitting. Mean square error decreased to 0.012 and 0.044 in the two settings. Encoded features represent spatiotemporal activity, passed to SVM for recognition using one versus all multi-class SVM. The framework designed for activity recognition problem in WSNs and IoT devices includes various SVM classifiers such as linear, quadratic, and cubic SVM. The aim is to provide an independent activity analysis system adaptable to different environments. Extensive experiments were conducted to capture multi-view video data, analyze it, and provide a compact summary of recognized activities in surveillance videos. Experiments were conducted with different configurations and parameters for activity recognition in WSNs. Slave nodes can encode summaries with PNG compression before transmission to master nodes. Various classifiers were used to balance execution time and accuracy. Evaluation details of these configurations are discussed in Section 4. The experimental evaluation of the proposed framework is divided into three subsections: MVS, activity recognition, and statistical analysis of data transmission. MVS performance is compared with state-of-the-art techniques on resource-constrained devices. Activity recognition is carried out on the master device using a Movidius setup. The MVS module utilizes Raspberry Pi for experiments but can be applied to other embedded vision devices. MATLAB is initially used for simulation, then transformed to Keras deep learning framework in Python for training the model. The model, trained with Keras and Tensorflow, can be converted to Movidius compatible graph format using NCSDK. The model was tested on Raspberry Pi and can be used on other Movidius-supported devices. Results from UCF-50 dataset show activity recognition using VGG-19 features. Five datasets were used for experiments, three for object detection in foggy scenarios and two for activity recognition. The Bl-7f dataset is challenging in the MVS literature with 19 cameras at Taiwan University's 7th floor. Road multi-view dataset has shuddering effects and variable light conditions captured with handheld cameras. Both datasets have publicly available ground truth for research purposes. The dataset contains persons and vehicles recorded in outdoor daylight for testing object detection in foggy conditions. Object detection results over the Road dataset in uncertain environments are shown. The activity recognition framework is evaluated using UCF50 and YouTube 11 action datasets, with UCF50 having high-level shuddering and scalability challenges, while YouTube 11 is less challenging with motion, cluttered background, and light variations. The dataset includes 11 action videos for testing object detection in foggy conditions. Results of the implemented framework are compared to other techniques using benchmark datasets. The framework shows better performance with higher F-measure scores and lower computational complexity compared to existing methods. Our method outperforms previous techniques in terms of speed and functionality on resource-constrained devices. It is evaluated using benchmark action datasets UCF-50 and YouTube 11, showing competitive accuracy compared to state-of-the-art methods. Experiments were conducted using different feature encoding techniques and SVM variants. Our method outperforms previous techniques in terms of speed and functionality on resource-constrained devices. Different settings of feature encoding to a 1000 feature vector can be used for various scenarios, considering accuracy and complexity. On UCF50 dataset, trajectories analysis, hierarchical clustering, ML-LSTM, and deep auto-encoder with CNN achieved high accuracy. On YouTube 11 action dataset, our S1 and S2 with cubic SVM achieved competitive overall accuracy compared to state-of-the-art methods. Our system achieves high accuracy with quadric SVM, surpassing 90%, making it suitable for processing on resource-constrained devices like low-cost VPU sticks. However, experiments with MobileNetV2 show lower accuracy, indicating it is not ideal for real-time surveillance. Transmitting large surveillance videos over wireless networks is not feasible due to limited bandwidth and storage capacity. The curr_chunk discusses the challenges of transmitting surveillance videos due to limited bandwidth and storage capacity. It presents statistical details on saving storage capacity, bandwidth, and transmission time. A specific dataset to confirm the efficiency of the framework is not available, so an example video is used to compare parameters. The total number of pixels in the example video is calculated, and the frames are considered non-compressed for ideal transmission. The size of a single frame in an office video is 0.29 MB. The office video contains 26,940 frames, resulting in a total size of 7892.57 MB. This significant difference in storage capacity highlights the effectiveness of the system in saving storage. This saving directly impacts communication bandwidth, as the bandwidth consumed is proportional to the data size. Transmission time is optimized by encoding keyframes with PNG compression and transmitting only important frames over IoT networks. The transmission time for overall frames, keyframes, and encoded keyframes is 4137.98, considering the distance, speed, and data rate in a network scenario. The transmission time for overall frames, keyframes, and encoded keyframes is 4137.98, 4.45, 1.13 seconds, respectively. The setup includes slave devices with cameras and wireless sensors, a master device with Intel Movidius NCS for running deep learning models on the edge. The paper integrates MVS and activity recognition in an IoT framework, showing significant savings in transmission time and storage capacity. The slave devices process multi-view video data, detect objects, extract features, and generate a summary for activity recognition. The MVS algorithm and activity recognition models in this paper outperform state-of-the-art methods. Future work will focus on exploring different parameters for multi-view action recognition algorithms in resource-constrained environments and utilizing spiking neural networks for advanced spatio-temporal feature extraction in activity recognition."
}