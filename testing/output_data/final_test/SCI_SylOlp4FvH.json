{
    "title": "SylOlp4FvH",
    "content": "V-MPO is introduced as an on-policy adaptation of MPO for deep reinforcement learning. It outperforms previous scores on Atari-57 and DMLab-30 benchmarks without the need for entropy regularization or hyperparameter tuning. V-MPO achieves high scores in problems with high-dimensional action spaces, outperforming previous results in controlling simulated humanoids and OpenAI Gym tasks. Deep RL with neural networks has shown superhuman performance in challenging domains like Dota 2 and Starcraft II using policy gradient-based methods like PPO. In this work, a new algorithm called V-MPO is introduced as an alternative to policy gradient-based methods like PPO and IMPALA. V-MPO adapts MPO to the on-policy setting by using a learned state-value function instead of a state-action value function. This modification aims to reduce variance and prevent policy collapse, achieving high scores in tasks with high-dimensional action spaces. V-MPO updates policy parameters using policy gradient in the direction of a target distribution with sample-based KL constraints. It outperforms previous algorithms in multi-task settings for discrete actions in Atari-57 and DMLab-30 benchmarks without hyperparameter tuning. Achieves high scores in individual levels surpassing single-task performance. V-MPO demonstrates high performance in multi-task settings, particularly excelling in challenging games like Ms. Pacman. It is effective in high-dimensional, continuous action spaces and outperforms previous algorithms in various OpenAI Gym tasks. The algorithm optimizes policy \u03c0 in discounted RL settings for Markov Decision Processes. In deep reinforcement learning, the policy \u03c0 \u03b8 specifies the agent's action probabilities using a neural network. Two key functions in RL are the state-value function V \u03c0 and the state-action value function Q \u03c0. The goal is to find a policy that maximizes the expected return. Policy gradient algorithms directly optimize this objective. In reinforcement learning, the policy is optimized by estimating the gradient of the expected return. Maximum a Posteriori Policy Optimization (MPO) focuses on finding a policy that achieves a specific goal. The policy improvement theorem states that policy updates can improve if there is a positive advantage in at least one state-action pair. In this work, an exponential function of advantages is chosen for policy optimization. V-MPO algorithm shares similarities with MPO, TRPO, and PPO in using KL constraints for policy updates. The V-MPO algorithm utilizes an E-step constraint and differs from MPO in its design for sample-efficient off-policy learning. The actor-learner architecture includes a target network for generating agent experience and updating every T target learning steps. The policy and value networks share parameters through a shared input encoder and LSTM, with an additional LSTM used for processing language instructions in DMLab. The E-step constructs a conditional target distribution q(a|s) based on a state-action value function Q(s, a). V-MPO is designed as an on-policy algorithm with a joint distribution \u03c8(s, a) in the E-step, using only one action per state without a learned Q-function. It differs from Fitted Q-iteration by Advantage Weighted Regression and Relative Entropy Policy Search by introducing the M-step KL constraint and top-k advantages. The value function in V-MPO is nonlinear, unlike REPS which uses a linear function of a learned feature representation. In V-MPO, the value function is learned directly from n-step returns using a neural network. Unlike previous attempts with REPS, V-MPO can reliably train powerful neural networks for RL tasks by combining EM-style policy optimization principles with appropriate constraints. V-MPO is closer to a policy gradient algorithm like PPO than Supervised Policy Update (SPU), as it uses the nonparametric distribution differently. V-MPO is an approximate policy iteration algorithm that uses a target network for learning steps. It generates trajectories from an old policy, evaluates the policy, and estimates an improved policy based on advantages. The V-MPO algorithm utilizes a nonparametric target distribution for policy updates and adjusts the parametric policy towards this distribution while maintaining a KL constraint. The approach involves policy evaluation and policy improvement steps, with optimization through gradient descent using a simple loss function. The algorithm involves learning an approximate state-value function for a fixed policy over multiple target learning steps. In the V-MPO algorithm, the value function for the target policy is updated using the online network, while the value function update is performed on the current parameters. A parametric value function is fitted by minimizing the squared loss with the n-step target. Advantages for policy improvement are calculated for each state-action pair in the batch of trajectories. PopArt normalization was used for the value function in the multi-task setting, even when training on a single task. Benefits were observed in using PopArt due to not tuning the relative weighting of policy evaluation and policy improvement losses. The value function outputs a separate value for each task in normalized space, converted to actual returns through a shift and scaling operation. Specific statistics were learned during training with scale bounds and a learning rate to guard against numerical issues with sparse rewards. In the context of using PopArt normalization for the value function in a multi-task setting, the lower bound is set to guard against numerical issues with sparse rewards. Importance-weighting can be applied to correct for off-policy data, but was not used in the experiments presented. The section discusses estimating an improved policy based on the advantage function and the state-action distribution induced by the old policy. The goal is to find the mode of the posterior distribution over parameters conditioned on the event of successfully improving the policy. The text discusses finding the maximum a posteriori estimate of parameters \u03b8 in the context of policy improvement using the V-MPO algorithm. It involves two steps similar to the expectation maximization algorithm: choosing a variational distribution to minimize the KL term in the E-step, and maximizing the lower bound in the M-step. In the E-step of the V-MPO algorithm, the goal is to choose a variational distribution \u03c8(s, a) to tighten the lower bound on log p \u03b8 (I = 1). This distribution is determined by the old parameters \u03b8 old and the relative improvement probability p \u03b8old (I = 1|s, a). The probability of actions leading to higher advantage is controlled by a temperature parameter \u03b7. The optimization process involves tuning the temperature parameter \u03b7 to improve learning by focusing on samples with top advantages. This technique is similar to the Cross Entropy Method and Covariance Matrix Adaptation. The optimization process involves tuning the temperature parameter \u03b7 to improve learning by focusing on samples with top advantages, similar to techniques used in the Cross Entropy Method and Covariance Matrix Adaptation. Importance weighting for off-policy corrections can also be utilized in the policy improvement step. In the E-step, the nonparametric variational state-action distribution \u03c8(s, a) is found to provide a tight lower bound to p \u03b8 (I = 1). The M-step involves maximizing this lower bound along with the prior term log p(\u03b8) to find new parameters \u03b8 that minimize the weighted maximum likelihood policy loss. The introduction of the nonparametric distribution separates the RL procedure from neural network fitting, optimizing for the policy \u03c0 \u03b8 (a|s). The sample-based computation of the maximum likelihood policy loss assumes zero weight for state-action pairs not in the batch of trajectories. The new policy is kept close to the old policy to minimize the KL divergence. The constrained optimization problem involves specifying a bound on the KL divergence instead of directly tuning the parameter \u03b1. The constraint in Eq. 8 limits the change in the parametric policy, particularly for states and actions not in the batch of samples. The constrained optimization problem involves using Lagrangian relaxation to optimize the unconstrained objective by alternating between optimization over \u03b8 and \u03b1. Lagrange multipliers \u03b7 and \u03b1 are projected to small positive values after each gradient update. KL constraints are generally well satisfied, especially in the E-step. The goal is for the bounds to be saturated, not just satisfied. The full loss function for V-MPO is provided, consisting of a policy evaluation loss and a policy improvement loss. Parameters \u03c6 and \u03b8 are used for the value and policy networks, respectively, with Lagrange multipliers \u03b7 and \u03b1. The networks share parameters and are optimized together, with the value network parameters considered fixed. The policy improvement loss for V-MPO involves weighted maximum likelihood loss with advantages estimated using a standard method. Alternating optimization over \u03b8 and \u03b1 is performed while applying a \"stop-gradient\" to the Lagrange multiplier and KL term. The KL constraint serves as a trust-region loss. The KL constraint, a form of trust-region loss, is used in V-MPO with decoupled KL constraints for continuous action spaces. The Adam optimizer is used with fixed learning rate for optimization. Network parameters are initialized and updated using Adam to optimize the total loss. In V-MPO, trajectories are collected and \u03b8, \u03c6, \u03b7, \u03b1 are updated using Adam to minimize total loss. Hyperparameters for DMLab tasks are detailed in Appendix F. DMLab-30 is a 3D environment with pixel control for representation learning. Hyperparameters were fixed for V-MPO, achieving higher scores than IMPALA. The V-MPO agent achieved higher scores than the best IMPALA run in DMLab-30 and Atari-57. Unlike previous studies, population-based training was not used. Results for multi-task DMLab-30 show V-MPO matching the performance of the Recurrent Replay Distributed DQN agent. The V-MPO agent outperformed IMPALA in DMLab-30 and Atari-57 without using population-based training. Results for multi-task DMLab-30 show V-MPO matching the performance of the R2D2 agent. The R2D2 agent achieved high scores in individual levels for 10B environment steps per level, surpassing previous results for both R2D2 and IMPALA. The Atari Learning Environment (ALE) was used with standard preprocessing and a maximum episode length of 30 minutes. In the single-task setting, no discount was set on loss of life to allow the agent to achieve the highest score possible. The maximum reward was not clipped in the single-task setting to preserve the original reward structure. Results for multi-task Atari-57 showed a single agent achieving \"superhuman\" median performance in approximately 4 billion environment frames. Population-based tuning of hyperparameters can lead to even higher performance. In the single-task setting, V-MPO achieves high scores on individual Atari levels, matching or exceeding previous results with fewer interactions. Using Transformer-XL instead of LSTM core, no reward clipping or value function rescaling is employed. In Ms. Pacman, performance nears 300,000 with a 30-minute timeout. V-MPO demonstrates effectiveness in high-dimensional, continuous action spaces by controlling simulated humanoids with 22 and 56 degrees of freedom. It outperforms other algorithms in terms of asymptotic returns and sample efficiency. In the \"gaps\" task, the 56-dimensional humanoid learns to run forward at 4 m/s and jump over gaps between platforms. V-MPO shows superior performance in high-dimensional, continuous action spaces by controlling simulated humanoids with 22 and 56 degrees of freedom. It achieves high asymptotic performance and sample efficiency, outperforming other algorithms. The algorithm can learn challenging visuomotor tasks from scratch and achieve higher returns than previously reported. In this work, a scalable on-policy deep reinforcement learning algorithm, V-MPO, is introduced for both discrete and continuous control domains. The algorithm does not require importance weighting or entropy regularization, and uses the same learning rate for all experiments. V-MPO is derived from similar principles as the original MPO algorithm for off-policy learning, showing the benefits of this approach as an alternative to policy gradient-based methods. The E-step temperature loss in Eq. 22 is derived as an improvement criterion in Eq. 5, aiming to minimize \u03c8(s, a) with a bound \u03b7 on the KL term. This optimization can be seen as a trust region for the variational distribution, transformed into an unconstrained objective using Lagrangian relaxation. The optimization problem aims to maximize the unconstrained objective with a bound \u03b7 on the KL term. By differentiating J and setting it to zero, we obtain the general solution for policy improvement. The value of \u03b7 is found by optimizing the dual function, leading to the loss in the unconstrained objective. A normal prior is used with a scaling parameter \u03b1 and Fisher information for the policy evaluated at the old parameters. The optimization problem involves maximizing the unconstrained objective with a bound on the KL term. The KL divergence is approximated heuristically as a state-averaged expression, and decoupled KL constraints are used for continuous action spaces parametrized by Gaussian distributions. The KL divergence between two multivariate normal distributions is utilized in the optimization process. The overall KL divergence can be separated into a mean and covariance component, with a smaller \u03a3 compared to \u00b5 for quicker learning in action space. The Supervised Policy Update (SPU) strategy involves solving an optimization problem and fitting a neural network via a supervised loss function. The Supervised Policy Update (SPU) strategy involves solving an optimization problem by fitting a neural network with a KL loss, leading to a parametric optimization problem equivalent to the nonparametric one. This compensates for network lag in distributed implementations by using importance weights. Advantages of importance weights \u03c1(s, a) are discussed in the context of behavior policy parameters \u03b8 D and \u03b8 target. Visual observations in DMLab are 72\u00d796 RGB images, while Atari uses 4 stacked frames of 84\u00d784 grayscale images. The ResNet architecture for processing visual data is similar to Hessel et al. (2018) but with increased channels. The curr_chunk discusses the architecture used for humanoid control tasks in DMLab, including convolution, max-pooling, and residual blocks. It also mentions the addition of an LSTM for language processing tasks. For humanoid control tasks in DMLab, different architectures were used, including LSTM for language processing tasks. The models had varying layer configurations and batch sizes, with specific details on the core components and parameters used for policy logits and value functions. The implementation of V-MPO in an actor-learner framework utilized TF-Replicator for distributed training on TPU 8-core and 16-core configurations. Data was split into minibatches for processing, with computations averaged across cores/replicas. Cross-replica communication was considered for full-batch computation but not implemented. The implementation of V-MPO in an actor-learner framework utilized TF-Replicator for distributed training on TPU configurations. Cross-replica communication for full-batch computation was found unnecessary. DMLab action set consists of 5 integers with reduced action set used. Settings for continuous control included physics and control time steps. Multi-task Atari-57 with population-based training was conducted with evolved hyperparameters. In the original experiment, hyperparameters were evolved via copy and mutation operators roughly once every 4 \u00d7 10 8 environment frames. The agent in the humanoid gaps task received proprioceptive information in addition to the primary pixel observation. This information included joint angles and velocities, root-to-end-effector vectors, root-frame velocity, rotational velocity, root-frame acceleration, and the 3D orientation relative to the z-axis."
}