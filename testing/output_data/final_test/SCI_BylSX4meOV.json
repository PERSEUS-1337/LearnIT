{
    "title": "BylSX4meOV",
    "content": "Data augmentation is essential to prevent overfitting in large convolutional neural networks with limited training data. Instead of using predefined transformations, a new approach learns data augmentation directly from training data using an encoder-decoder architecture and spatial transformer network. The method generates new, more complex samples for the classifier within the same class. This approach outperforms previous generative data augmentation methods and is comparable to predefined transformation methods when training image classifiers. Data augmentation is crucial for improving network performance with limited training data. Predefined transformations like flip, rotations, and color changes are commonly used. Automatic DA learning methods aim to avoid manual selection of transformations, but may limit the model from finding other useful transformations. Our work combines generative models and transformation learning approaches in an end-to-end network architecture. Our model, based on a conditional GAN, learns to generate transformations of a given image. Our approach combines a global transformation defined by an affine matrix with a more localized transformation to generate samples from the conditional distribution p(X|X), using a conditional GAN architecture. The generator first applies a global transformation using a spatial transformer network and then more localized transformations using a convolutional encoder-decoder network. The proposed model combines a spatial transformer network (STN) with a conditional GAN architecture to generate augmented samples in an adversarial way. It is important to jointly train the generator with the classifier in an end-to-end fashion to achieve optimal performance. The model is fully differentiable and adds an adversarial loss between the generator and classifier to generate difficult samples for the classifier. The proposed GAN-based architecture combines a spatial transformer network with a conditional GAN to augment training data for image classification. The model is fully differentiable, trainable end-to-end, and outperforms models trained with predefined data augmentation in low-data regimes. Training the model jointly with the image classifier is crucial for optimal performance. The proposed model combines a spatial transformer network with a conditional GAN to generate image transformations for improved classification. It includes two discriminators - one ensuring generated samples belong to the same class as input samples, and another forcing transformed samples to be dissimilar to input samples but similar to samples from the same class. The proposed model combines a spatial transformer network with a conditional GAN to generate image transformations for improved classification. It includes two discriminators - one ensuring generated samples belong to the same class as input samples, and another forcing transformed samples to be dissimilar to input samples but similar to samples from the same class. The experiments conducted test the model on various datasets like MNIST, Fashion-MNIST, SVHN, and CIFAR-10, comparing the efficiency of the learned data augmentation (DA) with heuristically chosen DA methods. Light DA involves random padding and cropping, while Strong DA includes rotation, scaling, and flipping for CIFAR-10. The proposed model combines a spatial transformer network with a conditional GAN to generate image transformations for improved classification. It includes two discriminators - one ensuring generated samples belong to the same class as input samples, and another forcing transformed samples to be dissimilar to input samples but similar to samples from the same class. Experiments compare the accuracy of baseline classifier, baseline with data augmentation (DA), and the DA model on increasing training samples. The DA model, trained jointly with the generator, outperforms predefined DA methods as the number of samples increases, achieving 80.5% accuracy with 4000 training samples. The gap between learned DA and strong DA diminishes with more examples. The proposed model combines a spatial transformer network with a conditional GAN to generate image transformations for improved classification. Experiments show that the model outperforms predefined data augmentation methods, with the best model consistently performing better than light and strong data augmentation. The performance of data augmentation is dataset dependent, with transformations that are useful in some domains not being usable in others. Joint training, where the generator of augmented images and the classifier are trained simultaneously, shows promising results compared to separate training. In separate training, the generator is first trained to create augmented images, which are then used to improve the classifier. The performance of separate training decreases over time, while joint training and baseline training accuracies increase. Generating the right samples at the right moment is crucial for data augmentation success. The method presented in this work focuses on improving classifier learning through automatic generation of augmented samples. It combines global and local transformations to enhance performance, showing superiority over other methods based on GAN models. The approach is fully differentiable and can be learned end-to-end, with joint training of the generator and classifier being crucial for improved classification performance. The generator and classifier are trained jointly, using global transformations with STN and local transformations with U-Net for high accuracy. Future work includes adding more differentiable transformations like deformations and color transformations to evaluate their impact on accuracy. The generator learns which image transformations are most useful for training the classifier, making it easier in low data scenarios. The transformation in training the classifier involves balancing loss terms with hyper-parameters to minimize fake classifications, maximize dissimilarity, and make the classifier robust against adversarial samples. The class discriminator, dissimilarity discriminator, and classifier are key components addressed in the process. During training, the generator is supported by two discriminators: the class discriminator (D_C) ensures generated images belong to the same class as the original, while the dissimilarity discriminator (D_D) ensures generated samples are different from the original. The dissimilarity discriminator D_D takes a pair of samples as input and outputs a score between 0 and 1. Its loss function maximizes dissimilarity between original and true samples, while minimizing dissimilarity between true and transformed samples. The image classifier C is trained with real and augmented samples, with a loss function that minimizes cross entropy between predicted labels of true samples and true label distribution. The global loss in the optimization process aims to find the optimal parameters for the generator, discriminators, and classifier by minimizing different loss functions for each component. The adversarial nature of the defined loss leads to the generator and discriminator/classifier pushing losses in opposite directions, generating augmented samples beneficial for training the classifier near decision boundaries. The generator network in the experiments applies basic pre-processing to images by subtracting mean pixel value and dividing by standard deviation. It consists of a STN module followed by a U-Net network, taking an image and Gaussian noise vector as input. Parameters for the generator loss are estimated on a validation set. The class discriminator and dissimilarity discriminator use the same architecture, adapted to take image and label inputs, and pair of images inputs respectively. The architecture for the classifier in the model is adapted from BID4 and uses Adam as the optimization method. Training parameters include using Adam optimizer with specific initial learning rates, \u03b2 values, and balance factors for different datasets. The model's architecture includes specific initial learning rates, \u03b2 values, and balance factors for different datasets. The classifier, discriminators, and generator details are provided in different tables, showcasing dataset-specific transformations such as zoom, flip, and color changes."
}