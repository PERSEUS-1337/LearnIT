{
    "title": "SJeQEp4YDH",
    "content": "The paper addresses the vulnerabilities of deep neural networks to adversarial examples and proposes a defense mechanism using asymmetrical adversarial training (AAT) to train robust subspace detectors. This approach integrates classifiers and detectors to provide a performance guarantee against attacks, promoting the learning of class-conditional distributions for more robust and interpretable detection and classification methods. Deep neural networks are vulnerable to adversarial attacks, including evasion and poisoning attacks. Various methods are evaluated for robust and interpretable detection and classification, showcasing competitive performances on adversarial detection and robust classification problems. Deep neural networks are vulnerable to adversarial attacks, hindering their deployment in sensitive domains. There is a surge in designing defense mechanisms against these attacks, but stronger attacks are being developed to defeat these defenses. Proposed defenses have shown limitations and are often ineffective. A large body of work has focused on detecting adversarial examples, with most mechanisms focusing on non-adaptive threats. These detection methods aim to identify attacks that bypass the defense mechanisms, even when the attacker is unaware of the detection process. The current research focuses on detecting adversarial examples under adaptive attacks, unlike previous heuristic approaches. It proposes partitioning the input space into subspaces for classification, allowing for more robust detection mechanisms. The research proposes partitioning the input space into subspaces for robust detection mechanisms. The novel asymmetrical adversarial training (AAT) objective is used to train robust binary classifiers in these subspaces. AAT supports detectors to learn class-conditional distributions, leading to generative detection/classification solutions. The contributions include developing adversarial example detection techniques with performance guarantees and demonstrating competitive performances on MNIST and CIFAR10 datasets. The detection framework utilizes asymmetrical adversarial training (AAT) to train robust classifiers in subspaces, promoting learning of class-conditional distributions. This approach allows for precise control over data generation, leading to competitive performances on benchmarking datasets like CIFAR10 and ImageNet. Adversarial attacks on neural networks have been a focus of research, with iterative projected gradient descent (PGD) identified as an effective approach. Adversarial detection techniques are based on detecting attacks on trained classifiers. Adversarial detection techniques involve generating adversarially attacked samples to discriminate between natural and perturbed sets. Different approaches include learning binary classifiers, adding an \"attacked\" class, and using intermediate layer features for detection. Metzen et al. (2017) proposed hardening detectors against adaptive adversaries using adversarial training. Detection methods are less effective against strong attacks. Classifiers are used for classification on datasets, with binary classifiers trained to discriminate natural and adversarial samples. Adversarial examples are determined using a procedure involving L p norm bounded samples. To determine if a sample is an adversarial example, obtain the estimated class label and use the corresponding detector. The algorithm's detection accuracy is crucial for minimizing classification errors. Training detectors involves minimizing the distance between the detector's output and the label. To be robust against adaptive attacks, incorporate the attack into the training objective inspired by robust optimization techniques. Incorporating the attack into the training objective, a robust optimization technique is used to derive an upper bound for the loss term. The proposed asymmetrical adversarial training (AAT) objective involves training detectors with in-class natural samples and detector-adversarial examples from out-of-class samples. The iterative PGD attack is used for the inner maximization. The AAT objective involves training detectors with in-class natural samples and detector-adversarial examples from out-of-class samples using a minimax problem approach. Instead of a generator, AAT relies on gradient descent (PGD attack) to generate data samples, allowing fine-grained control over the generation process. The detector in the AAT framework retains density information and avoids converging to a uniform solution. It uses the Gibbs distribution to calculate joint probabilities and applies the Bayes classification rule to obtain a generative classifier. Rejected samples are based on low probability inputs. In adversarial example detection, rejected samples are seen as adversarial examples. Testing the robustness of individual detectors shows that training a detector with a PGD attack prevents significant performance reduction by a stronger adversary. The optimization landscape for asymmetrical objectives may differ from symmetrical ones. Imbalanced positive and negative samples when training with objective 5 are addressed by using re-sampling to balance. Our solution addresses the highly imbalanced positive and negative samples by using re-sampling to balance classes. Additionally, we employ adversarial finetuning on CIFAR10 and ImageNet to accelerate detector training. Robust optimization introduces robustness within this new training paradigm, measured using AUC as a stable and reliable metric for detection performance evaluation. The overall performance of the integrated detection system is evaluated by validating the robustness of individual detectors. A universal threshold is used for all detectors, with the optimal value of each detector's threshold determined by optimizing a utility function. True positive rate (TPR) is computed on the test set containing natural and perturbed samples at different thresholds. The true positive rate (TPR) and false positive rate (FPR) are computed on the test set using a given threshold T. The FPR definition includes a constraint to only count true adversarial examples as false positives. Three attacking scenarios are considered to craft the perturbed dataset D, including a classifier attack where the perturbed sample x is computed by minimizing the loss based on the classifier's logit outputs. The integrated detection system is used to perform untargeted attacks by constructing a single detection function H. Perturbed samples are produced by attacking the integrated detection system to fool both the classifier and detectors. The attack based on the loss in 9 is found to be significantly more effective. Perturbed samples are created by attacking the integrated detection system using two loss functions. A single detection function H is constructed by aggregating logit outputs of individual detectors. The largest logit output of the aggregated detector and classifier logit outputs are used to create a surrogate classifier g. The perturbed example x is computed by minimizing the loss function. The text discusses creating perturbed samples to attack an integrated detection system using different loss functions. It introduces a new classification scheme that includes a reject option based on an integration of a naive classifier and detectors. The goal is to optimize examples to be adversarial or to fool the aggregated detector. Performance analysis is done by computing True Positive Rate on D and False Positive Rate on D. Attacks are performed using loss 9 against the generative detection method, as well as cross-entropy loss and CW loss 8. The scheme integrates a naive classifier with detectors to attack the classifiers. Performance is measured using standard accuracy and robust accuracy on natural and perturbed datasets. The accuracy is computed based on correctly classified samples, while the error is calculated for misclassified samples on the perturbed dataset. The perturbed dataset's error is a more proper measure of a classification system's performance than accuracy. Different detection systems were trained with PGD attacks, optimizing adversarial examples. Robustness test results confirm the effectiveness of the approach. The robustness test results in Table 1 show that detectors trained with objective 5 can withstand stronger PGD attacks. Results for L \u221e = 0.3 and L \u221e = 0.5 trained detectors, cross-norm and cross-perturbation test results, and random restart test results are included in Appendix D.1. AUC scores of the first two detectors tested with different PGD attacks using Adam optimizer are shown in Table 1. Generative detection outperforms integrated detection, especially when attacked using loss 9. Generative detection (attacked with loss 9) is more effective than integrated detection, especially with a low detection threshold. It outperforms CW loss and cross-entropy loss for attacking generative detection. Our method using L \u221e = 0.5 trained base detectors surpasses the state-of-the-art method by Carlini & Wagner (2017a). The mean L 2 distortion computation procedure is described in Appendix C. Our robust classification performance is compared with a state-of-the-art classifier in Figure 2b. Our classification methods offer a balance between standard accuracy and robust error by adjusting rejection thresholds. The generative classifier shows superior performance compared to the integrated classifier at low rejection thresholds. Even under a strong attack, the generative classifier remains robust while the robust classifier is compromised. Perturbed samples from targeted attacks reveal distinguishable features of the target class in the generative classifier. The base detectors have learned class-conditional distributions for the generative classifier, while perturbations for attacking the robust classifier are not interpretable but can cause high logit outputs. By using larger perturbations, unrecognizable images can be generated to confidently predict the robust classifier. Targeted attacks reveal distinguishable features of the target class in the generative classifier. The targeted attack maximizes the logit output of the targeted class using L \u221e = 0.4 constrained PGD attack. Both classifiers use L \u221e = 0.3 for training. Base detectors on CIFAR10 are trained with L \u221e = 8 constrained PGD attack. The robust classifier is trained with the same L \u221e = 8 constraint but with a different step-size. Results show base detector models can withstand stronger attacks than the training attack. Random restart test results and robustness tests are presented in Appendix D.2.1. In Figure 4a, combined attack is most effective against integrated detection, while generative detection outperforms integrated detection at low detection thresholds. Attack using loss 9 is more effective than cross-entropy and CW loss attacks. Our method outperforms state-of-the-art adversarial detection. Integrated classification can reach standard accuracy but increases error on perturbed sets. The generative classifier and robust classifier show similar errors on perturbed sets, but attacking the generative classifier requires more changes to cause the same error. Hard to recognize images can still cause high logit outputs in the robust classifier, highlighting a weakness in defense mechanisms based on ordinary adversary training. Samples that cause high logit outputs in the generative classifier have clear semantic meaning. The AAT improves robust and interpretable feature learning in classifiers trained with L \u221e = 8 constrain. Visual similarity between generated and real samples suggests successful learning of conditional data distributions. Asymmetrical adversarial training on ImageNet induces detection robustness and supports learning of class-conditional distributions, demonstrated on a dog class detector trained on Restricted ImageNet. The study focuses on using a pre-trained ResNet50 model for detecting dogs in ImageNet classes. The detector was trained with a L \u221e = 0.02 constraint using a PGD attack. Results were presented for attacking generative and robust classifiers using targeted attacks. The study also utilized a L 2 = 30 \u00d7 255 constrained attack for generating images from class-conditional Gaussian noise. In this paper, a novel adversarial detection scheme based on input space partitioning is proposed under the robust optimization framework. The scheme, called asymmetrical adversarial training (AAT), introduces a new generative modeling technique that enhances class-conditional distribution learning. This leads to improved generative detection and classification methods with better resistance to \"rubbish examples\". However, a major drawback is the high computational cost, and future work will explore shared computation between detectors. Adversarial training (AAT) aims to learn an energy function that distinguishes target class data points from others. The energy function is defined using the logit output of the target detector. The Gibbs distribution helps obtain a density function representing the joint distribution of data points and class categories. The partition function is a normalizing constant that can be computed for generative classification. AAT goes beyond discriminative training by learning the underlying structure. Adversarial training (AAT) goes beyond discriminative training by learning the underlying density function. Results on 1D and 2D datasets show detectors covering all models of distributions. Unlike GANs, AAT can retain density information by properly constraining the adversary. This allows for reliable density estimation, justifying the energy-based generative classification formulation. Our energy-based generative classification formulation extends to high dimensional data, with CIFAR10 and ImageNet image generation results presented. Our models capture target object structures without irrelevant elements like backgrounds. Additionally, our approach provides classification/detection performance guarantee against norm constrained adversarial examples. The energy function, after training, can capture the internal structure of the data, with red points filling the gap between two sets of blue points. The positive class data is sampled from a mixture of Gaussians, with both blue and red data having 500 samples. The estimated density function is computed using Gibbs distribution and network logit outputs. PGD attack steps 20, step size 0.05, and perturbation limit = 0.3. The architecture of the MLP model for solving these tasks is 2-500-500-500-500-500-1. PGD attack steps 10, step-size 0.05, and perturbation limit L \u221e = 0.5. All base detectors are trained using a network consisting of two max-pooled convolutional layers each with 32 and 64 filters, and a fully connected layer of size 1024. The base detectors are trained using a network with two max-pooled convolutional layers, each with 32 and 64 filters, and a fully connected layer of size 1024. Positive and negative samples are balanced by resampling the out-of-class set. The CIFAR10 base detectors are trained using the ResNet50 model. The detector is trained by finetuning a subnetwork using objective 5, with a pretrained classifier achieving 95.01% test accuracy. In each training iteration, a batch of 300 samples is used, with in-class samples as positives and out-of-class samples for adversarial examples. Adversarial examples are optimized using PGD attack for L2 and L\u221e models. The detection threshold is set to achieve 0.95 TPR, and a new loss function is constructed by adding a weighted loss term to measure perturbation size. The detector is trained using a weighted loss term to measure perturbation size. An unconstrained PGD attack is used to optimize the detector, with binary search to find the optimal parameter c. Different configurations are detailed for the binary search and PGD attack. The detector achieved high FPR on MNIST and CIFAR10 datasets. Finding the optimal c using binary search is challenging, affecting the performance based on mean L2 distortion. The performance of the detector is not precise due to challenges in finding the optimal parameter c using binary search. Future work should focus on measuring detection performances based on norm constrained attacks. AUC scores of base detectors under different attacks are detailed in tables, including PGD attacks using normalized steepest descent and cross-norm attacks. MNIST base detector performance is also evaluated under fixed start and multiple random restarts attacks. The performance of base detectors is evaluated under various attacks using different constraints. AUC scores are provided for detectors trained with different L\u221e values and tested with corresponding PGD attacks. Additionally, the performance of CIFAR10 base detectors under fixed start and random restarts attacks is discussed. Training base detectors with adversarial examples optimized with a small step-size is crucial for detection robustness. A model trained with a step-size of 1.0 showed poor robustness when tested with a smaller step-size, indicating a difference between naturally occurring data samples and adversarial examples. This highlights the challenge of generalizing performance to real attacks. In studying the effects of perturbation limits on asymmetrical adversarial training, a comparison was made between a base detector trained with L \u221e = 2.0 and L \u221e = 8.0. The model trained with L \u221e = 2.0 reached robustness quickly through adversarial finetuning, maintaining high performance on natural samples. However, the model trained with L \u221e = 8.0 did not converge even after 20K iterations, showing a decrease in performance on natural samples. Training with larger perturbation limits is more time and resource consuming, leading to a decrease in performance on natural samples. However, it allows the model to learn more interpretable features. Perturbations generated by attacking the naturally trained classifier do not have clear semantics, while those from the L \u221e = 8 model are easily recognizable. Large perturbation models are still under training and have not yet reached robustness. The Gaussian noise attack experiment is used to spark a discussion on this topic. Our approach in this section discusses the interpretability of our generative classification model compared to the discriminative robust classification model. The generative classifier provides a probabilistic view of the decision-making process, supported by experimental results. Posterior class probabilities are determined differently for each approach: the discriminative classifier computes them from logit outputs using the softmax function, while the generative classifier involves training base detectors to solve the inference problem. In our approach, we use Bayes rule to compute posterior probabilities, with the exponential of the logit output having a probabilistic interpretation. Gaussian noise attack is employed to demonstrate this interpretation, showing that logit output increase direction corresponds to semantic changing direction. Targeted PGD attack is used against logit outputs of classification models to perturb images. The generative classifier incurs higher computational cost compared to the softmax classifier, with the inference speed being roughly ten times slower on a specific GPU. The generative classifier has slower inference speed on a specific GPU compared to the softmax classifier. Training involves K logit outputs for the generative classifier and shared parameters for the softmax classifier. Adversarial training methods differ between the two classifiers. Speed testing involves computing gradients for N samples and M \u00d7 K adversarial examples for each classifier. In a speed comparison between a generative classifier and a softmax classifier, it was found that the generative classifier is slower on a GPU. The generative classifier requires computing gradients for N samples and M \u00d7 K adversarial examples, while the softmax classifier uses shared parameters and different adversarial training methods. In experiments, scenario 1 took 683 ms \u00b1 6.76 ms per loop, while scenario 2 took 1.85 s \u00b1 42.7 ms per loop, making asymmetrical adversarial training about 2.7 times slower than ordinary adversarial training."
}