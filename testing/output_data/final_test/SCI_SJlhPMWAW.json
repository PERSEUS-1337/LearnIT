{
    "title": "SJlhPMWAW",
    "content": "Deep learning on graphs has recently gained popularity with various applications in chemistry, medicine, and computer vision. Past work focused on learning graph embedding tasks, but there is potential to transfer progress to generative models for graphs. A proposed method sidesteps linearization hurdles by using a variational autoencoder to output a probabilistic fully-connected graph directly. This approach is evaluated on the task of conditional molecule generation. In contrast to advances in generative models for images and text, progress in encoding graphs into vector representations has been slower. Generating graphs is challenging due to their discrete nature, but a new method proposes using a probabilistic fully-connected graph output by a decoder to overcome this hurdle. This approach is evaluated in the context of conditional molecule generation. The method GraphVAE utilizes a probabilistic fully-connected graph output by a decoder to generate molecules in cheminformatics. This approach, within the framework of variational autoencoders, shows promise despite limitations in generating smaller graphs. This work represents a significant step towards efficient graph decoders in deep learning. The model outputs a probabilistic graph without a prescribed order of construction transformations, formulated as an autoencoder. It utilizes a stochastic graph encoder to embed the graph into a continuous representation and a novel graph decoder to output a probabilistic fully-connected graph. Our novel graph decoder outputs a probabilistic fully-connected graph on predefined nodes, conditioned on label y for controlled sampling. The autoencoder's reconstruction ability is facilitated by approximate graph matching for consistent predictions. Our method is a generative model that produces a probabilistic graph from a single opaque vector, without specifying the number of nodes or structure explicitly. Related work includes random graphs, stochastic blockmodels, and state transition matrix learning. Generative models for discrete data, like text, are usually trained using teacher forcing to avoid backpropagation through output discretization. Our work proposes a novel approach to overcome the non-differentiability problem in generative models for molecule design. By formulating the loss on a probabilistic graph, we can generate molecules with specific criteria in a continuous embedding space. This conditional model allows for intuitive graph representation of molecules, moving away from fixed syntax like SMILES strings. Our method aims to generate graphs from vectors in a continuous code space, aligning them with ground truth using a graph matching algorithm. Formulated within the framework of variational autoencoders (VAE), our approach addresses the non-differentiability issue in molecule design models. The method involves learning an encoder and decoder to map between graphs and their continuous embeddings in a VAE framework. The encoder defines a variational posterior and the decoder a generative distribution, with a prior distribution imposed on the latent code representation. The model is trained by minimizing the reconstruction loss and KL-divergence to allow for sampling of latent codes. The second term, KL-divergence, regularizes the code space for sampling of z directly from p(z) in the VAE framework. The dimensionality of z is kept small to encourage high-level compression. Graph generation remains a challenge due to the discrete nature of graphs, unlike text sequence generation. In the VAE framework, graph generation faces challenges due to the discrete nature of graphs. To simplify the task, the decoder can output a probabilistic fully-connected graph on a small number of nodes at once, sidestepping issues with non-differentiable decisions and complex connectivity. The decoder in the VAE framework models node and edge attributes as multinomial variables, with a probabilistic interpretation for each tensor of the graph representation. The architecture uses a multi-layer perceptron with sigmoid activation for node probabilities and softmax for edge and node class probabilities. At test time, a discrete point estimate of the graph is often of interest. At test time, obtaining a discrete point estimate of the graph involves taking edge and node-wise argmax in A, E, and F. Evaluation of Equation 1 requires computing the likelihood p \u03b8 (G|z) = P (G| G). Approximate graph matching can produce a binary assignment matrix X \u2208 {0, 1} k\u00d7n, allowing mapping of information between input and predicted graphs. The input adjacency matrix is mapped to the predicted graph as A = XAX T. The goal of (second-order) graph matching is to find correspondences between nodes of graphs based on similarities, expressed as an integer quadratic programming problem. The formulation considers matched and unmatched nodes and edges, with a reconstruction loss as a weighed sum of terms. A binary assignment matrix allows mapping information between input and predicted graphs. The goal of graph matching is to find correspondences between nodes based on similarities, approximated by relaxation into a continuous domain. The similarity function evaluates edge and node pairs, considering feature and existential compatibility. The method aims to improve graph matching through gradient descent on the loss, solving the matching step approximately due to the stochastic training of deep networks. Graph matching aims to find correspondences between nodes based on similarities, using a robust algorithm like Max-pooling matching (MPM) by BID2. The algorithm outputs a continuous assignment matrix X*, which is discretized to X using the Hungarian algorithm for a strict one-on-one mapping. The encoder in the graph matching process utilizes a feed forward network with edge-conditioned graph convolutions (ECC). The encoder does not use pooling except for global pooling with soft attention pooling. The approach of using a non-differentiable operation like the Hungarian algorithm for matching is common in object detection tasks. The encoder in the graph matching process uses edge-conditioned graph convolutions and global pooling with soft attention pooling. The VAE model enforces a Gaussian distribution for the encoder and decoder, with the decoder conditioned on a label vector for more control over generated graphs. The model is limited to generating small graphs due to GPU memory constraints. The method is limited to generating small graphs due to GPU memory constraints and matching complexity. Results are demonstrated for up to k = 38, focusing on molecule generation tasks using QM9 and ZINC datasets. Evaluating generative models for graphs can be challenging, as qualitative evaluation is often subjective and unintuitive for humans. The graph representation of molecules is used as a testbed for generative models, allowing for easy visualization and chemical validity checks. The complexity of valid graphs makes molecule generation challenging, with even small changes potentially rendering a molecule chemically invalid. To address this, three remedies are introduced to assist the network in generating valid molecules. In this application, three remedies are introduced to assist in generating valid molecules. The decoder output is made symmetric by predicting only the upper triangular parts of A and E. Prior knowledge is used to construct a maximum spanning tree at test time, and Hydrogen is not generated explicitly. The QM9 dataset BID22 contains organic molecules with specific atomic numbers and bond types. Testing and validation samples are set aside, and the model is compared to other generators. The encoder and decoder architecture for the generative models BID7 (CVAE) and BID15 (GVAE) are adapted for an artificial task of molecule generation. The encoder includes graph convolutional layers and soft attention pooling, while the decoder consists of fully connected layers. The model is trained with specific parameters and visualized for quality assessment. The encoder and decoder architecture for generative models BID7 (CVAE) and BID15 (GVAE) are adapted for molecule generation. The learned embedding z is visually evaluated by traversing it in two ways: along a slice and along a line. The quality and smoothness of the embedding are assessed by plotting two planes for frequent and less frequent labels in QM9, showing a varied and smooth mix of molecules. The quality of a conditional decoder is evaluated based on the validity and variety of generated graphs. Samples are drawn and decoded to determine chemically valid molecules, with a focus on ratios of valid and novel compounds. In Table 1, around 50% of generated molecules are chemically valid, with conditional models achieving about 40% accuracy in labeling. Larger embedding sizes result in more unique samples and lower conditional model accuracy. The ratio of valid samples shows less consistent behavior. The QM9 models are evaluated based on various metrics such as mean test-time reconstruction log-likelihood, mean test-time evidence lower bound, and decoding quality. Baselines CVAE and GVAE are compared, with GVAE generating the highest number of valid samples but with low variance. The importance of graph matching is also investigated by using identity assignment X. Our model, NoGM, outperforms GVAE in generating valid samples with lower variety. We achieve good performance in both metrics simultaneously. The evidence lower bound (ELBO) is reported, showing a decrease in reconstruction loss and KL-divergence with larger c. Model selection is challenging due to the lack of correlation between ELBO and Valid. The ZINC dataset BID9 contains drug-like molecules with specific atomic and bond characteristics. The setup for investigating scalability of an unconditional generative model with distinct atomic numbers and bond types. A wider encoder is used with different channels. The best model achieved Valid = 0.135, lower than QM9. CVAE failed to generate valid samples, while GVAE achieved Valid = 0.357. The performance is attributed to a higher chance of producing chemically relevant inconsistencies. Keeping graphs under 20 nodes resulted in Valid = 0.341. The study focuses on the robustness of graph matching using a similarity function S for good performance of GraphVAE. Gaussian noise is added to input graphs to create noisy versions for evaluation. The quality of matching is assessed using a noisy assignment matrix X. The advantage of this approach is invariance to permutation of equivalent nodes. Varying the maximum graph size k is done to analyze scalability. In this work, the quality of the matching process is not a major hurdle to scalability when generating graphs from a continuous embedding in the context of variational autoencoders. The method was evaluated on two molecular datasets, showing reasonable quality on small molecules but struggling with complex chemical structures. Our decoder struggled with capturing complex chemical interactions for larger molecules, but we see it as an important first step towards more powerful decoders. Future work includes improving the method, extending it to real chemistry problems, and utilizing the graph-based decoder for predicting detailed attributes of atoms and bonds. Additionally, the autoencoder can be used to pre-train graph encoders for fine-tuning on small datasets. The relaxed graph matching algorithm involves determining a continuous correspondence matrix between nodes of graphs based on similarities of node pairs. The optimization strategy is equivalent to the power method with iterative updates until convergence. The matrix-vector product can be interpreted as sum-pooling over match candidates. The authors propose a more robust max-pooling version for the matrix-vector product Sx, considering only the best pairwise similarity from each neighbor. They investigate the performance on conditional and unconditional QM9 models with implicit node probabilities, emphasizing improvements. The decoder assumes independence of node and edge probabilities, allowing for isolated nodes or edges. They also explore making node probabilities a function of edge probabilities in connected graphs. The evaluation on QM9 shows improved metrics in both conditional and unconditional settings, but with lower variability and higher reconstruction loss. The regularization in VAE hinders perfect reconstruction, especially for small embedding sizes. Unconditional models for QM9 achieve a mean test log-likelihood of roughly -0.37. Our architecture can achieve a mean test log-likelihood of roughly -0.37 for all c values, but struggles to achieve perfect reconstruction of inputs due to overfitting on small training sets. The network faces challenges in finding generally valid rules for assembly of output tensors."
}