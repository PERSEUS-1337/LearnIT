{
    "title": "rkl42iA5t7",
    "content": "Principal Filter Analysis (PFA) is a method for neural network compression that exploits filter responses within network layers to recommend a smaller network footprint. Two compression algorithms are proposed, one allowing users to specify the preserved spectral energy proportion in each layer, and the other being a parameter-free heuristic. Evaluation on various architectures and datasets shows significant compression rates with minimal accuracy loss. For example, PFA achieves compression rates of 8x, 3x, and 1.4x for VGG-16 on CIFAR-10, CIFAR-100, and ImageNet, respectively, with accuracy gains of 0.4%, 1.4%, and 2.4%. Principal Filter Analysis (PFA) is a method for compressing neural networks by preserving spectral energy proportion in each layer. It achieves significant compression rates with minimal accuracy loss, making it effective for domain adaptation. Despite the empirical nature of neural network design, recent advances show that depth accelerates learning and wider layers aid optimization. However, deploying large networks on storage-constrained devices remains a challenge. Neural networks often face limitations in storage, memory, and computation resources. Decorrelated filters have shown to improve accuracy by reducing correlation in layer responses. Principal Filter Analysis (PFA) compresses networks by leveraging intra-layer correlation, independent of training methods or loss functions. This method allows for efficient network compression while maintaining accuracy. The Principal Filter Analysis (PFA) method compresses neural networks by leveraging correlation within layer responses. Two closed-form algorithms, PFA-En and PFA-KL, suggest the number of filters to remove in a layer based on spectral energy analysis. The suggested filters are then removed and the network is fine-tuned through retraining. This approach allows for efficient network compression while maintaining accuracy. The PFA algorithms achieve better compression and accuracy than the state of the art on various datasets and architectures. Network compression techniques include quantization, knowledge distillation, tensor factorization, and network pruning. Tensor factorization exploits redundancy in convolution layers to replace tensors with a sequence. Network pruning is a technique that compresses networks by removing connections based on weight salience. It includes sparse pruning, where individual neurons are removed, and structured pruning, where entire filters are removed. PFA is a structured network pruning technique that achieves better compression and accuracy than the state of the art. Some pruning techniques require user-defined parameters that are difficult to choose and predict their effect on the network's footprint. PFA is a structured pruning technique that estimates filter saliency after training, making it applicable to any trained network without needing knowledge of the loss function. It differs from SVD-based approaches by performing SVD on layer responses rather than filter weights, without any projection. This is particularly relevant for domain adaptation. PFA is a structured pruning technique that estimates filter saliency after training, applicable to any trained network without knowledge of the loss function. It differs from SVD-based approaches by performing SVD on layer responses rather than filter weights, without any projection. This is relevant for domain adaptation. PFA derives different architectures from the same initial model when analyzed for different tasks, using the spectral energy of filters' responses to decide how many filters to preserve. PFA is a compression technique that utilizes correlation within responses to compress all layers at once, unlike other methods. It is orthogonal to quantization, tensor factorization, and distillation, making it a complementary strategy for network compression. PFA-En and PFA-KL algorithms exploit correlations between responses in convolutional and fully connected layers for network compression. The dataset used to train the network consists of samples with input dimensionality. The input data are images with height, width, and channels. The output tensor from a network layer is analyzed, focusing on convolutional and fully connected layers. PFA can be applied to any layer in the network, excluding recurrent neural networks in this study. For a convolutional layer, the output is defined as the convolution operation without the bias term. The response vector of a layer is defined as the spatially max-pooled and flattened tensor. The distribution of eigenvalues of the covariance matrix of the response vector provides insight into the correlation within the layer. Two algorithms are presented to guide network compression based on this distribution. Our hypothesis is that layers with high correlation in filter responses can learn well with fewer filters. Two strategies are presented for maximizing compression while reducing correlation. A compression recipe, denoted as \u03b3[3] = 0.6 for example, determines the percentage of filters to keep in each layer. Filter selection is discussed in Sec. 3.2.3, where PCA can be used for dimensionality reduction to maximize variance in a lower dimensional space. The proposed method involves extracting eigenvectors and eigenvalues of the covariance matrix to maximize data variance. By keeping a minimum set of filters based on a user-defined energy factor \u03c4, the network can be re-architected for compression. The parameter \u03c4 allows users to control the compression ratio, with the method closely linked to PCA for dimensionality reduction. The proposed method introduces an alternative formulation for optimal compression without the need for a parameter. It focuses on achieving an optimal recipe based on KL divergence, allowing for compression based on the distribution of filters in a layer. The method aims to maximize data variance by extracting eigenvectors and eigenvalues of the covariance matrix. The proposed method introduces a KL divergence-based recipe for optimal compression of filters in a layer. The recipe aims to preserve all filters when the actual distribution is identical to the ideal distribution, and only one filter when it is identical to the worst case distribution. The compression strength is determined by the divergence value, with a strong compression for values close to the upper bound and a milder compression for values close to the lower bound. In this work, a linear mapping \u03c8(x, u KL ) = 1\u2212 x /u KL is used for compression. PFA-En and PFA-KL provide filter numbers for each layer but not which filters to keep. Options include retraining from scratch or selecting filters based on correlation. The correlation matrix from matrix A is used to select filters for removal based on their 1-norm. If multiple filters need to be chosen, the correlation matrix is updated iteratively until the desired number of filters is removed. PFA is compared against other pruning approaches in terms of compression ratio and accuracy change. After comparing PFA with other pruning approaches in terms of compression ratio and accuracy change, three networks (VGG-16 BID41, ResNet-56 BID17, and SimpleCNN) are evaluated on CIFAR-10 and CIFAR-100 datasets. VGG-16 BID30 is also tested on ImageNet. Baseline performance for each architecture is established using 10 random initializations. The study evaluates PFA for compression on three networks (VGG-16 BID41, ResNet-56 BID17, SimpleCNN) on CIFAR-10 and CIFAR-100 datasets. The compressed architectures are created following PFA recipes, and two types of training are performed for each new architecture. The accuracy reported is the mean of 10 retrainings without hyper-parameter tuning. In experiments, SGD optimizer with specific parameters is used for SimpleCNN, VGG-16, and ResNet-56. PFA-En is computed for various energy values, while PFA-KL is parameter-free. An empirical upper bound on accuracy at different compression ratios is determined by randomly removing filters. The result averaged across trials shows the ease or difficulty of compressing a network without affecting accuracy. Results from experiments show that PFA strategies trained from scratch on CIFAR-10 produce better results than randomly compressed architectures. PFA-En and PFA-KL achieve a 60% footprint with only a 0.5 pp accuracy drop, outperforming filter selection methods. Filter selection strategies like PFA-En and PFA-KL improve accuracy by about 1 pp above the Scratch version for footprints above 40%. On CIFAR-100, filter selection increases its gain up to 2 pp compared to the Scratch version. The filter selection strategy converges faster during training and performs consistently better across different architectures and datasets. At the 10% footprint mark, random initialization appears to be better than filter selection. Filter selection strategies like PFA-En and PFA-KL outperform other methods in terms of compression ratio on CIFAR-10 and CIFAR-100. PFA-En achieves models significantly smaller than NS, FP, and VIB, with minimal accuracy change. Filter selection strategies like PFA-En and PFA-KL achieve smaller models with minimal accuracy change compared to other techniques on CIFAR-10 and CIFAR-100. PFA-En outperforms FGA at a compression ratio of 40% and achieves better accuracy. PFA-KL also achieves a smaller footprint with a minimal accuracy drop. PFA consistently provides better compression or accuracy than the state of the art, with complexity dominated by PCA analysis. For example, for ImageNet BID11, the time to compute PFA per layer ranges from 1.24s to 127.5s for different layer sizes in a VGG-16 architecture. The time to compute PFA per layer ranges from 1.24s to 127.5s for different layer sizes in a VGG-16 architecture. The complexity of filter selection depends on layer size, with a worst-case complexity of O(\u00f1n^2). PFA's time consumption is negligible compared to training time, offering long-term benefits of smaller footprint and faster inference. Other techniques achieve good results but rely on user-defined parameters, unlike PFA. NS, FP, VIB, and FGA all require user-tuned parameters for compression, with no intuitive way to set these parameters other than through manual tuning based on architecture and dataset. PFA achieves comparable accuracy and FLOPs with a higher compression ratio compared to other techniques. It requires a single intuitive parameter or can be parameter-free. PFA can derive different architectures specialized for different domains, utilizing training data from a different domain. PFA can be used for domain adaptation by randomly sampling classes from CIFAR-100. The PFA fine strategy performs similarly to the full fine-tuned model but produces models more than 4 times smaller. PFA-KL recipes show similar performance when adapting from one domain, while varying when adapting from another domain. Target domain being a subset of a specific domain explains this effect. In the target domain subset of D C100 A, PFA shows stronger and selective responses due to using images for training, leading to stronger correlations. PFA adapts recipes to task complexity, with more filters for 10 class subsets. In contrast, models trained on D A, which has not seen the target domain, generate less correlation and more uniform recipes. PFA transfers knowledge effectively when the original network has exposure to the target domain, creating adapted recipes. Two techniques, PFA-En and PFA-KL, use Principal Filter Analysis to compress neural networks without compromising accuracy. These methods exploit filter response correlations within layers and can be applied without knowledge of training procedures or loss functions. PFA-KL is parameter-free, while PFA-En requires only one parameter to preserve energy in each layer. By analyzing responses instead of weights, PFA allows for effective transfer learning. PFA enables users to utilize transfer learning for domain adaptation and create specialized networks for specific tasks from a base model. Compression algorithms like PFA-KL do not converge when repeatedly applied due to the retraining step altering weights. This theoretical limitation could inspire future work, suggesting expanding layers for optimal size at convergence."
}