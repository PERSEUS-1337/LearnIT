{
    "title": "Hyl_XXYLIB",
    "content": "Continual Learning via Neural Pruning (CLNP) is a method for lifelong learning in fixed capacity models by pruning low-activity neurons. An L1 regulator promotes the presence of these neurons, which are disconnected from active neurons after training. This biologically inspired approach achieves state-of-the-art results with lower computational complexity, allowing models to learn new tasks without performance deterioration. Continual learning faces challenges like catastrophic forgetting, where machines trained on new tasks forget previous ones. Various approaches exist, such as using networks with submodules or penalizing weight changes for previous tasks. Current fixed capacity methods require non-local computations, but a new algorithm inspired by biological neurons allows training with standard methods and zero forgetting. The new algorithm for continual learning prevents catastrophic forgetting by using sparsifying L1 regularization and neural pruning, leading to state-of-the-art performance on forgetting benchmarks. This approach differs from existing methods that use submodules or add new modules for each task. The approach for continual learning prevents catastrophic forgetting by using sparsifying L1 regularization and neural pruning, leading to state-of-the-art performance on forgetting benchmarks. The network structure and training scheme are standard, with weight elasticity methods addressing forgetting by penalizing important weight changes. Sparsification is used as a crucial tool, but not the focus of this work. The core idea of the method is to leverage the over-parametrization of neural networks through sparsification, allowing for compression with minimal loss of performance. The approach involves developing a continual learning scheme that prevents catastrophic forgetting by utilizing the unused capacity of the model. Graceful forgetting is discussed to balance sparsification and model performance in lifelong learning. The focus is on sparsity in fully connected layers and individual neurons or channels. The method leverages neural network over-parametrization through sparsification to compress without significant performance loss. It involves a continual learning scheme to prevent catastrophic forgetting and balance sparsity with model performance. The focus is on sparsity in fully connected layers and individual neurons or channels. The network is divided into active weights connecting active nodes, free weights connecting any node to inactive nodes, and interference weights connecting inactive nodes to active nodes. By setting interference weights to zero, free weights can be adjusted without affecting the network output, allowing for training new tasks without forgetting previous ones. The network is divided into active weights connecting active nodes, free weights connecting any node to inactive nodes, and interference weights connecting inactive nodes to active nodes. The free weights can be adjusted without affecting the network output, allowing for training new tasks without forgetting previous ones. Transfer learning is measured by the number of new active neurons at each layer after training subsequent tasks, indicating the sufficiency of already learned features for new tasks. To fully flesh out a continual learning scheme, the connectivity structure of the output nodes needs to be specified. One option is to use a new output layer for each new task while saving the previous output layer, known as the multi-head approach. This method allows for training new tasks without the need to train new neurons at previous layers. Using a single-head approach with a dynamic partitioning of the final hidden layer into multiple unequal sized parts for different tasks, achieved through a multiplicative masking operation with task-dependent masks. The single-head continual learning algorithm CLNP is the first of its kind, utilizing a fixed structure with task-dependent masks. The methodology, tested with ReLU networks, allows training on new tasks without catastrophic forgetting. Sparsification is achieved through a specific scheme similar to network trimming, involving L1 weight addition during task training. The sparsification method involves adding an L1 weight regulator during task training to promote sparsity in the network. Neuron pruning based on average activity is then used to remove connections, with surviving weights adjusted for efficiency. This approach mimics biological processes like synaptic communication and neuron lifecycle. The sparsification method involves adjusting surviving weights after pruning through fine-tuning, which helps the model regain lost performance. To achieve higher sparsity levels, one can iterate pruning and fine-tuning steps. The network is partitioned into active and inactive parts based on average activity over the dataset. In a network with ReLU activations, active neurons are identified as a compression of the network into a sub-network of smaller width. The weights of each layer are divided into active, free, and interference parts. Model sparsity is crucial for training later tasks but should not overly reduce performance. As sparsity increases, the model's generalization performance initially improves, but pushing sparsity too high can lead to a decrease in performance. In lifelong learning, balancing model performance with network capacity for future tasks is crucial. Graceful forgetting, sacrificing some accuracy to prevent catastrophic forgetting, is essential. Successful continual learning algorithms need to incorporate a graceful forgetting scheme, such as sparsity vs. performance compromise. In lifelong learning, balancing model performance with network capacity for future tasks is crucial. Graceful forgetting involves sparsifying the model after each task to prevent further deterioration during subsequent training. This approach contrasts with weight elasticity methods and ensures past task performance. Sparsity hyperparameters are chosen through grid search to maintain validation accuracy within a margin of the best accuracy achieved. The margin parameter controls the compromise on performance. The algorithm efficiently finds optimal hyperparameters for sparsity in models, maintaining validation accuracy within a margin of the best accuracy achieved. This approach incurs no additional computational burden during training and grid search. The algorithm efficiently finds optimal hyperparameters for sparsity in models without adding computational burden. The experiment evaluates the approach on tasks derived from the MNIST dataset with variations in structure. The experiment evaluates the approach on tasks derived from the MNIST dataset with variations in structure. For the first variation, a single-head approach is employed, while the second variation uses layers of width 100 instead of 2000. Hyperparameters are searched over for the first task, settling on specific values for learning rate and L1 weight regularization for different layers. The approach virtually eliminates catastrophic forgetting and achieves an average accuracy just shy of single task performance. In the experiment, a narrow multi-head network structure is used for sparsification with 2 iterations of fine-tuning. A graceful forgetting margin is chosen to saturate the network after training on 10 tasks, achieving an average accuracy of 95.8%. The results show state-of-the-art performance on networks of comparable size, surpassing prior methods with higher complexity. The experiment involves training an image classifier sequentially on CIFAR-10 and CIFAR-100 split into 10 tasks, each with 10 classes. The same multi-head network from a previous study is used, with two different training schemes. The experiment used a narrow multi-head network structure for sparsification with two iterations of fine-tuning. Two training schemes were employed, one with a graceful forgetting margin of 1% and the other with 2%. The validation accuracy of the tasks showed that the 1% scheme ran out of capacity after the fourth task, while the 2% scheme maintained high performance throughout all tasks. The experiment utilized a narrow multi-head network structure for sparsification with two fine-tuning iterations. Results show minimal catastrophic forgetting on the first task with improved sparsification. A wider single-head network was also used for comparison. The number of new channels learned at each layer for consecutive tasks is illustrated in Fig. 3b. The first layer stops growing new channels after the second task, indicating that features learned in lower layers are more transferable. Models for continual learning should be wider in higher layers to accommodate this lack of transferability. Our methodology leverages over-parametrization of neural networks to train new tasks in inactive neurons/filters without catastrophic forgetting. Sacrificing some accuracy at the end of each task allows for regaining network capacity. Empirical results show exceeding or matching state-of-the-art performance with less computational intensity. Model capacity graphs indicate transferability and sufficiency of features in different layers. By using graphs, it is confirmed that features learned in earlier layers are more transferable. Our method identifies layers with capacity limitations and addresses them by increasing neuron numbers, allowing for accommodating more tasks and compensating for network width choices."
}