{
    "title": "H1kMMmb0-",
    "content": "Achieving machine intelligence requires integrating perception and reasoning. Models often specialize in one area, making manipulation of symbols from perceptual spaces challenging. A proposed two-tiered architecture involves information processing modules and a controller trained with reinforcement learning to solve tasks like visual arithmetic. The proposed two-tiered architecture involves information processing modules and a controller trained with reinforcement learning to solve tasks like visual arithmetic efficiently by combining perception with symbol manipulation. In this work, the focus is on training machine learning systems to solve perceptuo-symbolic problems with a small number of examples. Specifically, the visual arithmetic task involves performing basic arithmetic algorithms on hand-written digits in an image based on a specified symbol. One approach is to treat this task as a classification problem for a convolutional neural network to learn the correct answer. The focus is on training machine learning systems to solve perceptuo-symbolic problems with a small number of examples, specifically the visual arithmetic task involving basic arithmetic algorithms on hand-written digits in an image. A convolutional neural network could be trained to map input images to correct answers, but there is untapped structure in the problem that could improve sample efficiency. Imposing model structure becomes important for capturing human-like abilities of strong generalization and learning from small datasets. Providing the learner with access to modules implementing information processing functions relevant to the task is crucial. In this work, an approach is proposed to solve perceptuo-symbolic problems with a small number of examples by identifying and assembling modules into an information processing machine called an interface BID30. This machine is designed to be coupled to the external environment, allowing for the application of classifiers and symbolic transformations to the correct portions of the image and representations at the appropriate time. The approach proposed in this work involves using an interface BID30 to process information from the external environment. Reinforcement learning is utilized to train a controller to use the interface for visual arithmetic tasks, combining various pre-trained neural networks and modules for arithmetic operations. This method allows for the sequential combination of processing modules to solve complex tasks without the need for differentiability. The system designer selects modules and trains a controller using RL for visual arithmetic tasks. The model requires fewer training examples than feedforward networks. The article is structured with a general approach in Section 2, details of the visual arithmetic task in Section 3, empirical results in Section 4, and a review of related work in Section 5. Our approach for visual arithmetic tasks utilizes reinforcement learning formalisms, modeling the external world as a Partially Observable Markov Decision Process (POMDP). The agent's actions are based on observations and result in rewards, with the state being updated stochastically. The agent's policy is parameterized to map observation-action histories to action distributions. The concept of an interface is extensively used in our approach. In this work, an interface is formalized as a POMDP distinct from the external world, mediating the agent's interaction. The agent interacts with a combined POMDP comprised of the external world and the interface. The current work introduces a POMDP C combining E and I, with observation and action spaces from I. Controllers learn to manage interfaces for algorithmic tasks, extending to tasks with perceptual domains using pre-trained deep networks. The controllers are trained using the actor-critic algorithm to maximize expected rewards. The REINFORCE algorithm involves maximizing an objective using gradient ascent, with an unbiased estimate of the gradient obtained from sample trajectories. The algorithm samples trajectories, estimates the objective function, computes the gradient, and updates the parameter vector using gradient ascent. The REINFORCE algorithm uses unbiased gradient estimates from sample trajectories to maximize an objective function. To reduce variance, a baseline function can be introduced without biasing the gradient estimate. Training a value function to approximate the current policy can help lower variance. Sharing parameters between the value function and policy can speed up learning. The REINFORCE algorithm uses unbiased gradient estimates from sample trajectories to maximize an objective function. To reduce variance, a baseline function can be introduced without biasing the gradient estimate. Training a value function to approximate the current policy can help lower variance. Sharing parameters between the value function and policy can speed up learning. The controller learns useful representations even without reward, speeding up learning when reward is sparse. The objective includes a term encouraging high entropy for exploration. Tasks in the Visual Arithmetic domain are image classification problems with (n, n) grid input images. In the Visual Arithmetic domain, tasks are treated as image classification problems with (n, n) grid input images. Each task involves applying a reduction operation to digits in the image to determine the correct label. There are 5 tasks, including Sum, Product, Maximum, and Minimum, where the agent must output an answer based on the task and digits displayed in the image. In the Visual Arithmetic domain, tasks involve applying reduction operations to digits in grid images to determine the correct label. Tasks include Sum, Product, Maximum, and Minimum. A combined task requires identifying the operation (addition, multiplication, maximum, minimum) based on a letter in the image. The agent must execute the arithmetic algorithms and determine the correct operation for each input instance. In the Visual Arithmetic domain, tasks involve applying reduction operations to digits in grid images to determine the correct label. Modules implementing functions like detecting salient locations, classifying digits, and symbol manipulation are useful for Visual Arithmetic tasks. The interface I is used to solve tasks by processing information with various actions like updating salience and classifying digits. The interface I is used for Visual Arithmetic tasks, with modules for detecting salient locations, classifying digits, and symbol manipulation. It includes pre-trained deep neural networks like LeNet for classifying letters and digits, and a salience detector with multiple hidden layers. The salience network is pre-trained to output a salience map when given scenes of randomly scattered EMNIST characters. In the Visual Arithmetic setting, E is a degenerate POMDP emitting the same observation every time step. E responds to actions from I with rewards based on the correctness of the arithmetic problem represented by the input image. Each episode runs for 30 time steps. The Visual Arithmetic domain involves using a Long Short-Term Memory (LSTM) BID8 with 128 hidden units for the controller, which accepts observations from the interface and outputs a softmax distribution for action selection. The LSTM weights are updated using the actor-critic algorithm, with experiments conducted on high-level tasks in this domain. Our reinforcement learning approach utilizes the interface described in Section 3.2 and experiments focus on the impact of external environment training samples on performance. Training the controller with reinforcement learning requires thousands of experiences on a small set of input-output training samples. The learner has access to a simulator for the interface but not for the external environment. Comparisons are made against convolutional networks trained with cross-entropy loss. In comparison to convolutional networks trained with cross-entropy loss, our approach focuses on sample efficiency in image classification tasks. We use LeNet architecture with varying units in the fully-connected layer and treat tasks as classification problems with 101 classes. Integers greater than 100 are grouped under the 101st class. Experiments demonstrate the sample efficiency of candidate models on Single Operation and Combined tasks, leveraging reinforcement learning to achieve good performance with smaller sample sizes compared to feedforward convolutional networks. Production systems, like GPS, remain relevant in cognitive science despite falling out of favor in mainstream AI. Recent advances in reinforcement learning have not been applied to systems like ACT-R, SOAR, and EPIC, which typically use hand-coded controllers. There is a lack of research in using reinforcement learning for difficult tasks in these systems. Some work has been done with recurrent neural networks for supervised learning, such as training a network to control attentional windows. This approach focuses on tasks with challenging algorithmic components and structured interfaces. Comparisons can be made to recent research on deep neural networks learning algorithms. Recent research focuses on deep neural networks learning algorithms on sequences of symbols. Different frameworks like BID25 and TerpreT generate code, while others like NTM, NRAM, NP, and NPI use a controller network with a differentiable interface. This approach provides flexibility by not requiring the external computational medium to be differentiable. Our work is similar to BID30, using reinforcement learning for algorithms and deep networks in interfaces for tasks with perceptual components like visual arithmetic. Previous studies like BID10 trained a multi-layer perceptron for arithmetic operations from visual input, but did not focus on sample efficiency or hand-written digits. The work by Gaunt et al. addresses a task domain similar to Visual Arithmetic using a differentiable code-generation method. Their approach involves learned perceptual modules and reliance on pre-trained modules. Other work focuses on using neural modules and adaptively choosing groups of modules based on input. BID11 utilizes reinforcement learning to train a recurrent neural network for laying out a feedforward neural network with learnable neural modules. Unlike Module Networks, the network's layout depends on past module applications within the same episode. Deep Sequential Neural Networks (DSNN; BID3) use trainable neural modules on a fixed directed acyclic graph (DAG) for processing inputs. PathNet is a recent advancement in modular neural networks for transfer learning in RL. Modules are recruited for the entire task duration, unlike the step-by-step basis in the approach discussed. Future directions include exploring the potential benefits of this approach. Future directions for the current work include exploring the benefits of conditional computation, adaptive computation time, simultaneous training of perceptual modules and controller, and the potential for large speedups in tasks with computationally intensive modules. The controller uses reinforcement learning to coordinate various competencies, including perceptual and algorithmic abilities, through an interface containing pre-trained neural networks and symbolic manipulation modules. This approach aims to solve tasks requiring sophisticated perception and symbolic computation. The controller uses an interface to solve tasks with fewer training examples compared to traditional approaches, demonstrating the interface's inductive bias in the Visual Arithmetic task domain."
}