{
    "title": "Bkxd9JBYPH",
    "content": "This paper introduces a novel approach for representing a system's belief using multi-variate normal distributions based on a deep neural network. The proposed method addresses the computational complexity of obtaining model uncertainty by expressing the parameter posterior in sparse information form. The inference algorithm utilizes a Laplace Approximation scheme with a diagonal correction of the Kronecker-factored eigenbasis. A low-rank approximation of the eigenbasis and a memory-efficient sampling scheme are devised to overcome the intractability of the information matrix inversion required for full Bayesian analysis. The approach is validated through theoretical analysis and empirical evaluation on benchmark data sets, demonstrating its superiority over existing methods in machine learning applications. The current learning approaches in machine learning focus on providing precise estimations of failure probabilities, especially in safety-critical applications like medical image analysis or autonomous driving. Existing methods, such as using the softmax function in DNNs for classification, tend to underestimate failure probabilities due to neglecting model uncertainty. Dropout at test time, as proposed by Gal (2016), can help mitigate this issue by incorporating Monte-Carlo dropout. Monte-Carlo dropout (MC-dropout) can mitigate underestimation of predictive uncertainty but has drawbacks like requiring specific stochastic regularization during training. Variational inference and expectation propagation are alternatives without these drawbacks, but they have limitations like using a diagonal covariance matrix. In response to the limitations of current methods for modeling uncertainty in deep neural networks, a new Laplace Approximation (LA) for DNNs is proposed to represent model uncertainty in sparse information form of Multivariate Normal Distribution (MND). This approach aims to improve upon existing Kronecker factored approximations of the Hessian. The Laplace Approximation for DNNs improves Kronecker factored approximations of the Hessian by correcting diagonal variance in parameter space. The information matrix of the resulting parameter posterior is more accurate, approximating model uncertainty in MND form. The approach involves spectral sparsification to make sampling more tractable while maintaining accuracy. The curr_chunk discusses a novel low-rank representation of Kronecker factorization for more accurate uncertainty estimates and calibration in large network structures. It introduces an algorithm for sparsification and sampling computations, showing effectiveness on benchmark datasets. The main contribution includes a Laplace Approximation scheme. The curr_chunk presents novel contributions including a Laplace Approximation scheme with diagonal correction, a low-rank representation of Kronecker factored eigendecomposition, an algorithm for low rank approximation, and experimental results showcasing state-of-the-art performance in Bayesian Neural Networks. The approach explores a sparse information form to represent model uncertainty in DNNs for the first time. The curr_chunk introduces a novel approach to model uncertainty in DNNs by representing a neural network as a concatenation of layers. The posterior is approximated using a Gaussian with mean \u03b8 MAP and covariance from the Hessian of the log-likelihood. Loss functions like MSE or cross entropy are used. The curr_chunk discusses the use of a low rank approximation on Kronecker factored eigendecomposition to preserve structure in eigenvectors, making computations memory-wise feasible. The Kronecker product of matrices U A and U G is defined, allowing for the computation of the diagonal matrix D recursively. The curr_chunk discusses the challenges of matrix inversion in sampling and the importance of sampling from the posterior for estimating prediction uncertainty. It also highlights the difficulty of directly sampling from certain equations, using an example from a specific architecture. The text discusses the challenges of matrix inversion in sampling and the importance of sampling from the posterior for estimating prediction uncertainty. It presents an example of a computationally infeasible operation due to high complexity, proposing a sparse formulation for tractability using a low rank approximation. This approximation preserves the top K and additional J eigenvalues, maintaining a Kronecker structure in eigenvectors. The eigendecomposition preserves Kronecker structure in eigenvectors by preserving top K eigenvalues, resulting in L = K + J eigenvalues. An example illustrates the preservation of top 3 eigenvalues and corresponding eigenvectors in a matrix E, leading to a sparse formulation for tractability. Preserving Kronecker structure in eigenvectors involves storing additional eigenvalues to maintain the definition of a Kronecker product. Algorithm 1 is proposed for computing low rank approximations that preserve Kronecker structures. The indexing rules for Kronecker factored diagonal matrices are defined for diagonal matrices S A and S G. The algorithm proposed in Algorithm 1 computes low rank approximations that preserve Kronecker structures in eigenvectors. The indexing rules for Kronecker factored diagonal matrices S A and S G are defined to maintain the Kronecker product. The process involves computing eigenvalues and eigenvectors based on specific indices and preserving them accordingly. The proposed algorithm in Algorithm 1 computes low rank approximations that preserve Kronecker structures in eigenvectors, allowing for efficient sampling from the given covariance matrix. This approach simplifies the sampling computations and bounds the complexity of intractable computations in deep neural networks. The proposed algorithm computes low rank approximations to simplify sampling computations and bound complexity in deep neural networks. It shows that the information matrix tends to be sparse, leading to a sparse parameter posterior representation. The algorithm weakens weak nodes and links in a preserving fashion, with diagonal corrections to maintain node information. Sampling computations can be done in a memory-wise feasible way, and the method can be applied to existing architectures. The proposed algorithm simplifies sampling computations in deep neural networks by computing low rank approximations. It results in a sparse parameter posterior representation with diagonal corrections to maintain node information. The approach yields a sparse information form of MND with a low rank eigendecomposition and diagonal structure, preserving Kronecker structure in eigenvectors. The formulation of model uncertainty is novel, and theoretical results are provided for further insights and justifications. This sparse information filter approach differs from traditional Kalman Filters by sparsifying the information matrix while keeping the diagonals accurate. The Hessian of DNNs is efficiently approximated using layer-wise Kronecker factorization, demonstrating notable scalability. Eigenvalues of the Kronecker factored matrices are re-scaled to ensure exact diagonal variance in its eigenbasis. The work focuses on achieving higher accuracy by correcting inaccurate estimates of eigenvectors in the parameter space. It builds upon Laplace Approximation as a practical inference framework, demonstrating competitive results in continual learning. In the context of approximate inference, a more expressive posterior distribution is proposed. SLANG is similar to the approach but does not explore Kronecker structures. Dimensionality reduction beyond principal component analysis and singular value decomposition is discussed, with a focus on Kronecker factored eigendecomposition. Algorithm 1 is proposed with theoretical properties outlined in section C. An empirical study is presented with regression and classification tasks across various datasets to demonstrate the quality of predictive uncertainty, effects of varying LRA, and gains in computational complexity reduction. Experiments are implemented using Tensorflow. The evaluation on a toy regression dataset involves comparing approximations to the Hessian using a single-layered fully connected network. The study compares different methods for predictive uncertainty using a regression dataset. HMC, BBB, Diag, KFAC, and EFB Laplace are evaluated, showing varying levels of uncertainty. Diag, KFAC, and EFB Laplace predict high uncertainty even within training data regions. DEF variants slightly underestimate uncertainty but fit well with FB Laplace and HMC. Keeping diagonals of IM exact results in accurate predictive uncertainty. The effects of Low Rank Approximation (LRA) on uncertainty estimation are quantitatively studied by evaluating approximations of IM. The results show that DEF provides accurate estimates regardless of dimensions, while EFB has more approximation errors due to inaccurate eigenvector estimates. KFAC produces errors on diagonal elements, indicating assumptions of Kronecker. In this experiment, KFAC produces errors on diagonal elements due to its assumption of Kronecker factorization, while EFB outperforms KFAC and Diag estimates for off-diagonal errors. The error profile of off-diagonal error I i j explains the principles of LRA, showing that decreasing ranks increases errors in a preserving manner. These results align with Lemma 1 and 4 of section C, reflecting the design principles of the method. The study also evaluates predictive uncertainty on classification tasks, focusing on known and unknown classes to assess the proposed low-rank representation's necessity. In experiments evaluating predictive uncertainty on classification tasks, ECE is used for known classes, while normalized entropy is used for unknown classes. Comparisons are made with MC-dropout, ensemble, Diag, and KFAC Laplace methods on MNIST-notMNIST experiments. These methods serve as state-of-the-art baselines without requiring changes in the training procedure. The experiments on predictive uncertainty in classification tasks compared various methods on MNIST-notMNIST data. The chosen architecture was LeNet with RELU and L2 coefficient of 1e-8. The results showed significant improvements over the deterministic method, with DEF Laplace achieving the lowest ECE and highest mean entropy on out-of-distribution samples. The method effectively separated between wrong and correct predictions due to domain change. Our method demonstrated better calibration performance and out-of-distribution detection on CIFAR10 and SVHN datasets using a 5-layer architecture. Hyperparameter tuning showed that increasing \u03c4I reduced ECE on CIFAR10 but underestimated uncertainty on SVHN, while DEF Laplace achieved a good balance with minimal regularization. Dropout was omitted as it would alter the network architecture for comparison purposes. The proposed LRA addresses the computational challenges of MND by reducing complexity through parameter and low rank dimensions. Certain layers, like FC-1 in MNIST and CIFAR experiments, are computationally intractable without LRA. Different success criteria show no uniform inference method superiority. Our method demonstrates effectiveness in representing layer-wise MND in a sparse form and compares well to state-of-the-art techniques. By focusing on accurate diagonals and sparsifying off-diagonals, we achieve outstanding predictive uncertainty performance across various data and models. Future work should explore better metrics for comparing approximations to the true posterior in DNNs. The curr_chunk discusses addressing a key limitation in their work related to the sparsity of information matrices and the importance of maintaining accurate diagonals while sparsifying off-diagonals. While empirical evidence supports their approach, there are no theoretical guarantees. The authors suggest future research connecting information geometry of DNNs and Bayesian Neural Networks. They believe their work can be a stepping stone similar to sparse Gaussian Processes. The curr_chunk discusses a novel approach to representing model uncertainty in deep neural networks using Multivariate Normal Distribution, aiming to improve accuracy compared to existing methods. The authors plan to demonstrate real-world applications of this approach in the future. The matrix normal distribution is described as a probability density function for random variables in matrix form. The curr_chunk introduces the Multivariate Normal Distribution (MND) as a representation of model uncertainty in deep neural networks. It discusses the dual representation of MND in canonical and information forms, with equations defining the canonical and information forms. The Information form is denoted by equation 13, with variables x, \u00b5, and \u03a3 representing random variable, mean, and covariance respectively. The Information form includes the calculation of information vector W IV MAP and matrix I. The curr_chunk discusses the analytical form of diagonal elements for U A \u2297 U G without fully evaluating it, using Kronecker product and diagonal matrix. It explains the computation of diagonal entries of (U A \u2297 U G )\u039b(U A \u2297 U G ) T. The curr_chunk explains the derivation of diagonal elements for (U A \u2297 U G )\u039b(U A \u2297 U G ) T using Kronecker product and diagonal matrix. It also discusses the importance of sampling in a full Bayesian analysis for computing predictive uncertainty. The curr_chunk discusses the challenges of computing a canonical form of MND in high dimensions and proposes a sampling computation method to address these issues. The curr_chunk discusses sampling operations from a Multivariate Normal Distribution and the computation of symmetrical factor Fc in a Kronecker structure. The Kronecker structure of Fc is exploited to compute Fc Xl efficiently, reducing complexity to O(L^3). Sampling from a multivariate Gaussian is computationally cheap. Symmetrical factor for covariance \u03a3 can be found using Cholesky decomposition. Sampling computations are optimized using Kronecker structure, bounding complexity to O(L^3). The symmetrical factor for covariance \u03a3 can be efficiently computed using Cholesky decomposition, with complexity bounded by O(L^3). Woodbury's Identity is utilized for inversion operations, resulting in smaller matrices U A 1:a and U G 1:g. The complexity of computing the symmetrical factor for covariance \u03a3 is reduced to O(L^3) by utilizing Cholesky decomposition. Smaller matrices U A 1:a and U G 1:g are derived, along with diagonal matrices D and I mn. The computations exploit rules of Kronecker products, leading to an analytical solution for sampling from a low-rank and information formulation of MND. The proposed LRA simplifies matrix operations by preserving Kronecker structure in eigenvectors. Theoretical properties include a diagonal correction term for Fisher information matrix estimates. Lemmas and corollaries provide insights into the accuracy of different estimation methods. The proposed LRA simplifies matrix operations by preserving Kronecker structure in eigenvectors. The analysis includes a comparison of different estimation methods for Fisher information matrix. Lemma 2 provides insights into the accuracy of the proposed method. Lemma 2 discusses the approximation error of low-rank estimates of the Fisher information matrix obtained by preserving top eigenvalues. It suggests that if preserving all eigenvalues results in a large covariance matrix, preserving a subset can be a memory-wise feasible option. The method ensures a nondegenerate covariance matrix by maintaining symmetry and positive definiteness in the diagonal correction matrix and T. The Lemma comments on the validity of the resulting parameter posterior and proves that sparsifying the matrix can lead to a valid non-degenerate covariance if two conditions are met. Searching the rank can be automated with off-line computations that do not involve any data, and if the matrix does not turn out to be valid, techniques like eigen-value clipping or finding nearest positive semi-definite matrices can be used. Adding a prior precision term and a scaling factor ensures that the inverse of the matrix does not become numerically unstable. The real Fisher information matrix I is estimated using low rank DEF, EFB, and KFAC methods. The approach is effective in capturing diagonal variance and estimating off-diagonal entries. The sparsification scheme reduces weak links while preserving diagonal information. The sparsification scheme reduces weak links in the Fisher information matrix while preserving diagonal variance through diagonal correction after LRA, leading to more accurate estimation. The DEF and EFB estimates of the real Fisher information matrix have errors that can be quantified using a squared Frobenius norm. The off-diagonal terms in the estimates contribute to the error, leading to a guaranteed bound on the difference between the estimates and the real matrix. This bound holds for both KFAC and EFB estimates compared to the real Fisher Information matrix. The proof of the approximation error for low-rank estimates of the Fisher information matrix is provided in George et al. (2018). The KFAC library from Tensorflow was used to implement the Fisher estimator. The empirical Fisher is typically biased and not a good estimate. The KFAC library offers various estimation modes for Fisher estimation in fully connected and convolutional layers. Gradients mode was used for KFAC Fisher estimation, while the exact mode was used for diagonal approximations. Exponential averaging and inversion scheme were not used in the experiments. NVIDIA Tesla was used for grid searching parameters, and 1080Ti for other experiments. Training details included a gradient descent optimizer with a learning rate of 0.001. The gradient descent optimizer from tensorflow was used with a learning rate of 0.001, and zero prior precision or L2 regularization coefficient. Mean squared error (MSE) was used as the loss function. The block-wise Hessian and their approximations contained zero values on its diagonals, indicating zero variance in the information matrix. This led to a degenerate information matrix for the likelihood term. To address this, variances were treated as deterministic to make the information matrix non-degenerate. Similar findings were reported by MacKay (1992). The study provides a detailed analysis to clarify the results of toy dataset experiments. The toy regression problem simplifies the understanding of the process. Qualitative evaluation is done to check predictive uncertainty, but no quantitative analysis has been reported. LA using KFAC and Diag is sensitive to hyperparameters in this dataset, making comparison difficult. In a toy regression problem, qualitative evaluation of predictive uncertainty was conducted. LA using KFAC and Diag is sensitive to hyperparameters, making comparison challenging. Random was introduced as a user-set \u03c4I for covariance estimation. Different methods showed high uncertainty estimates with no training data. Tuning hyperparameters resulted in visible over-prediction of uncertainty and inaccurate predictions. In experiments with higher precision, figure 8 illustrates how equation 3 affects the estimated covariance matrix. Adding a small \u03c4 can lead to over prediction of uncertainty and inaccurate predictions due to degenerate approximate Hessian. Increasing the dataset size from 20 to 100 data points resulted in more zero diagonal entries in the approximate Hessian, indicating over-parameterization of the model. This leads to an under-determined Hessian, suggesting the importance of accurately estimating it without considering zero eigenvalues. Changing the ratio of data points to parameters alters the structure of the approximate Hessian and can result in an under-determined approximation. The approximate Hessian changes structure with dataset size, leading to under-determined approximation and loss landscape alteration. Hyperparameters impact predictive uncertainty, especially in KFAC. Further analysis on hyperparameters' relation to loss landscape is suggested for future work. Numpyro was used for HMC implementation with 50000 MC samples for convergence. Bayes By Backprop was implemented using an open-source code with similar settings for Gaussian noise sampling in batches. For MNIST experiments, Gaussian noise is sampled in batches with 10000 iterations for network convergence. Architectures are taken from Tensorflow tutorials, consisting of 2 convolutional layers and 2 fully connected layers without down-scaling inputs. Pooling layers of size 2 by 2 with a stride of 2 are included. The model for MNIST experiments includes 2 convolutional layers and 2 fully connected layers. Pooling layers of size 2 by 2 with a stride of 2 are used. The first layer has 32 filters with a 5 by 5 kernel, the second layer has 64 filters with a 5 by 5 kernel. The first fully connected layer has 1024 units, and the last layer has 10 units. RELU activation is used for all layers except the last one, which computes softmax output. Dropout with a rate of 0.6 is applied to the fully connected layer. Cross entropy loss is used with ADAM optimizer and a learning rate of 0.001. The CI-FAR10 experiments used an architecture with 2 convolutional layers and 3 fully connected layers. Pooling layers of size 3 by 3 with strides 2 were applied. Batch normalization with specific parameters was used before pooling. A weight decay factor of 0.004 and stochastic gradient descent with a learning rate of 0.001 were employed. The CIFAR10 experiments utilized an architecture with 2 convolutional layers and 3 fully connected layers. The first layer had a 5x5 kernel with 64 filters, followed by a second layer with more parameters due to RGB input. Fully connected layers had 384, 192, and 10 units. Data augmentation included random cropping, flipping, brightness, and contrast adjustments. Low rank approximation was used with the maximum rank possible after removing zero eigenvalues. 1000 Monte-Carlo samples were used for MNIST, while 100 samples were used for CIFAR10 and toy regression experiments. The deep ensemble implementation used 15 networks trained with different initializations for MNIST and CIFAR10 experiments. Results were similar to MC-dropout for MNIST. Dropout probabilities of 0.5 and 0.8 were tested, and Laplace approximation hyperparameters were optimized. The grid-search for optimal values of \u03c4 was conducted using a validation set. Different methods required varying \u03c4 values for accuracy. Grid-search started based on mean prediction values. Minimum ece points were selected and reported for MNIST."
}