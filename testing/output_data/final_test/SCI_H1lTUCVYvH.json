{
    "title": "H1lTUCVYvH",
    "content": "In this work, a novel approach to curriculum learning called Learning with Incremental Labels and Adaptive Compensation (LILAC) is proposed. LILAC focuses on incrementally learning labels instead of difficult samples, improving network learning by introducing ground-truth labels in fixed increments and adapting target vectors for failed predictions. The effectiveness of LILAC is evaluated against similar methods. LILAC is a novel approach to curriculum learning that incrementally learns labels and adapts target vectors for failed predictions. It outperforms batch learning in recognition accuracy and performance consistency across CIFAR-10, CIFAR-100, and STL-10 benchmarks. Additionally, LILAC achieves state-of-the-art performance on CIFAR-10 with label order invariance and simple data augmentation. Stochastic Gradient Descent with mini-batches is commonly used in deep learning but often fails to find stable, generalizable, and scalable solutions. Curriculum learning and label smoothing are alternative strategies to improve learning in deep networks. Curriculum learning gradually increases the difficulty of samples to train deep networks, improving performance on corrupted and small datasets. Recent studies have focused on categorizing samples and analyzing the pace at which they are presented to deep networks. Previous works assumed samples cover a broad spectrum of difficulty, leading to computational overheads and reduced effective data for model learning in early epochs. Label smoothing is a complementary approach to curriculum learning, helping to prevent over-fitting and avoid local minima in deep networks. Various methods, such as replacing ground-truth labels with noise or using logits from previously trained models, have been proposed to achieve generalizable solutions. In this work, the LILAC method is proposed as a label-based curriculum approach with adaptive compensation to improve accuracy and stability in deep networks. It aims to enhance outcomes by penalizing highly confident outputs and offering an alternative target distribution for training samples. Unlike other methods, LILAC does not require re-training of the parent network or introduce additional computation. LILAC is a method for learning strong embeddings through incremental label introduction and adaptive compensation. It uses unlabelled/wrongly-labelled data as hard negative examples to train the network in two key phases. The model gradually introduces groups of labels and contrasts them against a diverse set of negative examples to develop a strong understanding of each class. Adaptive compensation is used once all ground-truth labels are revealed. The adaptive compensation phase of training in LILAC replaces incorrectly classified samples with softer distributions, improving model stability and performance. Soft distributions are generated on-the-fly from the model's outputs, not pre-computed by an alternative model. LILAC's performance is compared to strong baselines on image benchmarks, offering a novel approach compared to incremental and continual learning methods. In LILAC, negative mining and focused training are used to improve learning on a fully available dataset, avoiding data deficient learning. The importance of hard negative mining in improving learning is emphasized, with contributions including a new take on curriculum learning by incrementally introducing adaptive compensation. In LILAC, a new approach to curriculum learning is introduced where labels are incrementally learned instead of samples. The method compensates for incorrectly labeled samples by softening their target distribution, improving performance and reducing computational overhead. This approach improves recognition accuracy and decreases performance variation across image classification benchmarks compared to batch learning. The focus is on inducing better learning in deep networks by treating all samples equally beneficial and regularizing the network to prevent overfitting. In the incremental phase of the algorithm, ground-truth labels are gradually revealed for previously misclassified samples. Two sets of data, \"Seen\" and \"Unseen\", are kept constant during fixed intervals of training. Data from \"Unseen\" is sampled to match the number of samples from \"Seen\". In the incremental label introduction phase, data is partitioned into two sets: Seen and Unseen. Samples in Seen use ground-truth labels, while Unseen samples use a designated unseen label throughout training. LILAC assumes a random ordering of labels. LILAC assumes a random ordering of labels, with data initially placed in Seen (S) and Unseen (U) sets. Labels in U are revealed incrementally in intervals, with training happening at fixed intervals of epochs. Mini-batches are sampled uniformly from the dataset, ensuring no skew in predictions towards U. The curated mini-batches of data are used to train the neural network. Ground-truth labels are revealed in intervals, and samples are moved from U to S. Adaptive compensation is implemented if the network struggles to predict labels, adjusting the target vector for incorrectly classified samples on-the-fly. During the adaptive compensation phase, predictions from the previous epoch are used to adjust the target vector for misclassified samples. The final target vector for each instance is computed based on the model. Three datasets, CIFAR-10 and CIFAR-100, are used for training. The study uses three datasets, CIFAR-10, CIFAR-100, and STL-10, to evaluate their method. CIFAR-10 and CIFAR-100 have 50,000 training images and 10,000 testing images each, while STL-10 has 500 training samples and 800 testing samples per class. The evaluation metrics include average recognition accuracy and consistency across 5 trials. ResNet18 is used for CIFAR-10/100 experiments. The study uses ResNet18 for CIFAR-10/100 experiments and ResNet34 for STL-10. Training involves 10 epochs for CIFAR-10/100 and 5 epochs for STL-10 in each LILAC incremental phase. Different learning rates are used for each dataset. Epoch thresholds are set for adaptive compensation. Stochastic gradient descent with mini-batches is the baseline for comparison. Curriculum learning aims to accelerate model learning. The study uses ResNet18 for CIFAR-10/100 experiments and ResNet34 for STL-10, with different learning rates and epoch thresholds for adaptive compensation. Curriculum learning accelerates model learning by creating a \"Simple\" dataset and training on it before the entire dataset. Label Smoothing and Dynamic Batch Size are used as baselines for regularization and variable batch size importance in training neural networks. The custom baselines DBS and RA highlight the importance of variable batch size and virtual data partitions in training neural networks. DBS copies data within a mini-batch to mimic variable batch size, while RA uses data from a randomly chosen class within a mini-batch. Table 1 shows improvements in recognition accuracy and consistency compared to batch learning. Shake-Drop + LILAC achieves superior performance without unnecessary overheads compared to other methods. It shows a drop in standard deviation and higher average performance. Harvesting data from all classes is highlighted as important for learning from \"negative\" samples. LILAC, a method that utilizes data from all classes simultaneously, shows improved performance and lower standard deviation compared to other methods. It emphasizes the importance of variable batch size and data partitioning. LILAC can be extended to achieve the highest performance on CIFAR-10 with standard preprocessing. It consistently increases classification accuracy and decreases standard deviation across datasets compared to batch learning methods. The evolution of the embedding in the incremental phase is illustrated in Fig. 2, showing more separation compared to batch learning. Table 3 breaks down the contribution of each phase in LILAC, with comparisons between LILAC and batch learning in terms of representation spaces. The impact of incremental label introduction on the representation space is highlighted, showing improved spacing. The deep network in training space shows comparable points when all labels are available, indicating a better initialization than standard batch learning. LILAC starts from a qualitatively better solution, leading to improved performance. Adding adaptive compensation has a mixed impact on standard batch learning, with no clear improvement across benchmarks. In combination with the incremental label introduction phase of LILAC, adaptive compensation improves average performance and decreases standard deviation, showing improved stability and consistency. The embedding space learned by incrementally introducing labels is distinct from standard batch learning and more amenable to AC. Previous experiments have established the general applicability of LILAC, and further characteristics such as label ordering and target vector distribution smoothness are explored. The study explores the impact of label ordering and target vector distribution smoothness on the performance of LILAC. Results show that LILAC is relatively invariant to the order of label introduction and performs well with varying values of the alternate target vector. The peak average performance values typically range between 0.7 to 0.5. In this work, LILAC was designed to introduce one label per incremental step to avoid conflicting decision boundaries. Adding multiple labels at once decreases performance, supporting the hypothesis that fewer labels per step improve adaptive compensation in the embedding space. LILAC shows high performance on CIFAR-10 with simple data augmentations, outperforming batch learning and label smoothing. The next step is to include a confidence measure for dropout effects and explore handling partial inputs for incremental learning. In the incremental label introduction phase, the choice of E for fixed training intervals varies across datasets. CIFAR-10 shows a peak followed by a decrease, STL-10 exhibits a consistent increase, and CIFAR-100 lacks a clear pattern. The selection of E depends on the dataset, with no explicit pattern for choosing it without trial runs. Runtime constraints should also be considered when selecting E values, as both m and E impact it."
}