{
    "title": "BygqBiRcFQ",
    "content": "The natural notion of stability in data analysis is often geometric, as seen in computer vision. Scattering transforms create deep convolutional representations that are stable to input deformations, interpreting this stability as resistance to changes in the domain's metric structure. In this work, scattering transforms are generalized to non-Euclidean domains using diffusion wavelets, maintaining stability with respect to metric changes measured by diffusion maps. The resulting representation is stable to metric perturbations while capturing high-frequency information. Unlike CNNs, scattering transforms use pre-selected filters from a multi-resolution filter bank instead of trained filters. In this paper, scattering transforms are discussed in the context of graphs and signals supported on graphs, such as brain connectivity networks, social networks, and user similarity networks. The goal is to define a family of graph-scattering transforms for analysis and stability with respect to deformations in the underlying domain. The paper discusses graph-scattering transforms for analyzing signals on graphs. It defines diffusion wavelets for multi-resolution filtering and uses diffusion distances to measure deformation. The main finding is that diffusion graph scattering transforms are stable with respect to deformations measured by diffusion distances. The paper discusses the stability of diffusion graph scattering transforms with respect to deformations measured by diffusion distances. It shows that the operator norm distance between graphs is bounded by a constant dependent on the spectral gap, not the number of nodes. Stable representations are important for signal analysis, with linear filters shown to provide stable and rich descriptions in numerical analyses. Linear filters can provide stable or rich representations but not both simultaneously. Diffusion scattering transforms offer stable and rich representations for graph signal classification, especially beneficial for graphs without regular structures like CNNs. Graph scattering transforms offer stable and rich representations for graph signal classification, similar to CNNs but without the need for training. This approach sheds light on the appropriateness of graph convolutions for signal classification problems. Graph scattering transforms provide stable representations for graph signal classification, similar to CNNs but without the need for training. The value of GNNs lies in their stability relative to domain deformations close to permutations, as shown in this work. These scattering representations have been extended to handle transformations on complex groups and domains like audio processing and quantum chemistry. Extensions of scattering to general graphs have also been explored, focusing on Haar wavelets and multiresolution pairings. Our work focuses on building multiresolution pairings using diffusion wavelets to obtain stable graph representations. Unlike previous work, we establish stability results with respect to diffusion metric perturbations, which define a weaker topology. Our approach does not rely on extrinsic deformations and is closely related to the analysis of stability in Graph Neural Networks. Our framework aims to achieve stability in signal representations on non-Euclidean domains, focusing on deformation stability for images. We seek signal representations that are stable under small deformations, addressing the challenge of transforming high-frequency content while maintaining discriminative power. The text discusses the transformation of high-frequency content in signals to achieve stability using wavelet decompositions and pointwise modulus activation functions. This method results in a signal representation similar to a CNN, with feature maps that are not recombined. The signal representation utilizes wavelet decompositions and pointwise modulus activation functions to achieve stability, resembling a CNN structure. The scattering transform satisfies a deformation stability condition for certain signal classes and wavelet families, capturing high-frequency information with spatially localized wavelets. Generalizing deformations to non-Euclidean domains is challenging. In this work, intrinsic geometric stability is explored without relying on low-dimensional embeddings. Deformations are defined via perturbations of the Euclidean metric, extending stability notions to general domains. Representations on metric spaces and comparing metric changes are discussed using graphs as flexible data structures. The scattering transform can be generalized using computational harmonic analysis on graphs, focusing on deformations on the underlying graph domain. Diffusion wavelets BID8 offer a framework for multi-resolution analysis on a weighted, undirected graph with a diffusion process in its nodes. The adjacency matrix W defines a diffusion process A in its nodes, characterized by its eigenvalues and eigenvectors. The diffusion process is well-localized in space, self-adjoint, and satisfies A \u2264 1. The spectrum of A is assumed to be non-negative, and powers of A will be taken in the analysis. The diffusion operator T = 1/2 (I + A) is used to define a multiscale wavelet filter bank and a low-pass average pooling in Section 4. It also constructs a metric on G and measures distances between nodes x, x \u2208 V using diffusion scattering representation. The diffusion distance between two graphs G, G is defined by comparing the diffusion metric generated by G and G up to a node permutation. The diffusion distance is defined in the space of permutation matrices, comparing points at later stages of diffusion as time increases. The distance at s = 1/2 is used as the main deformation measure, yielding a stronger topology between graphs. The study of stability relative to other distances is left for future work. Our goal is to build a stable and rich representation \u03a6 G (x) for variable-sized graphs using soft correspondences. This representation can model signals and domains, considering a prespecified x = f (G) or marginalizing from an exchangeable distribution. The motivation is to address dynamic environments where the signal of interest may be measured across different individuals. In building generative models for graphs, efficient representations of graphs are needed to distinguish between different graphs. This involves solving a matching problem between kernel matrices, which is an NP-hard combinatorial problem. Multiscale filters are constructed using a lazy diffusion operator associated with a graph of size n. Graph diffusion wavelets are constructed using the diffusion operator T 2 j to create a family of multiscale filters. These wavelets are localized in both space and frequency, with optimal spatial localization. The filters can be obtained with only two coefficients, h 0 = 1 for diffusion T 2 j\u22121. The wavelets capture temporal differences at increasingly spaced diffusion times, with the finest scale corresponding to one half of the normalized Laplacian operator. This approach provides a graph wavelet filter bank analog in the Euclidean domain, offering a unique way to define graph wavelet decompositions. Graph diffusion wavelets are constructed using the diffusion operator to create multiscale filters that are localized in both space and frequency. The wavelets can be expressed with few diffusion terms for stability reasons. If the graph exhibits a spectral gap, the linear operator \u03a8 defines a stable frame, providing Littlewood-Paley bounds to control the filter bank's ability to capture and amplify the signal x. The diffusion wavelets constructed using the diffusion operator create multiscale filters localized in space and frequency. The frame bounds in Proposition 4.1 provide lower bounds on energy loss, with the spectral gap \u03b2 determining the appropriate diffusion scale J. The maximum scale can be adjusted based on the cutoff r aligning with \u03b2. The Euclidean Scattering transform is constructed using wavelet decomposition, pointwise modulus activation, and averaging operators. For graphs, the Diffusion Scattering transform is defined similarly with wavelet decomposition, activation function, and averaging operator. The average over a domain represents diffusion at infinite time. Multiple layers of transformation yield a final representation. The stability of graph diffusion scattering aims to bound signals on graphs of different sizes. The stability of the diffusion scattering on graphs is determined by the spectral gap of the graph. The diffusion distance can be directly computed by comparing nodes with a given order. The stability of the low-pass operator U with respect to graph perturbations is studied in Lemma 5.2. The spectral gap of graphs, denoted by \u03b2, plays a crucial role in the stability of diffusion scattering. The scattering transform coefficients are obtained using a low-pass operator U, and the stability of U with graph perturbations is analyzed in Lemma 5.2. The objective is to prove the stability of the scattering process. The objective is to prove stability of the scattering coefficients \u03a6 G (x) using Theorem 5.3, which involves comparing graphs G and G, their diffusion operators, and certain parameters. The stability bound on the scattering coefficients is computed in Corollary 5.4 for x \u2208 R n. Corollary 5.4 demonstrates that the scattering representations of graphs G and G are closer when their diffusion metrics are similar. The stability bound is influenced by topological properties, spectral gaps, and the number of layers. A smaller spectral gap and more layers lead to a higher stability bound. Graphs with beta values closer to 1 have weaker diffusion paths, making them more sensitive to perturbations. Conversely, graphs with beta values further from 1 are less affected by perturbations. The spectral gap of a graph affects its tolerance to edge perturbations. The size of the graph impacts stability results due to the distance measure. The focus is on localized graph wavelet banks for computational efficiency. The scattering transform in BID34 is stable. The scattering transform in BID34 is stable with respect to a graph measure dependent on the spectrum of the graph. The stability is computed using a permutation-invariant metric and is stable to small perturbations around permutations. The diffusion wavelets can be approximated as a cascade of low-pass diffusions followed by a high-pass filter at resolution 2j. The pyramidal structure of multi-resolution analysis wavelets shows that diffusion scattering is a specific case of GNNs. Each layer corresponds to a different scale, with feature representation at each layer updated using a pair of operators. The resulting GNN representation is stable with respect to graph measures, with parameter-dependent constants. This stability is proven through the diffusion distance definition. Future work could explore further relationships in this context. The stability of GNN representations is related to gradient descent optimization biases. Empirical results show dependence on spectral gap and discriminative power in classification tasks. Consider a small-world graph with random graph signals and scattering transforms. Comparing scattering representations on different graph supports allows for computation of differences. The stability of GNN representations is analyzed by computing differences in scattering representations on different graph supports. Varying the spectral gap by changing p SW from 0.1 to 0.9 affects the representation difference, with results showing a decrease in stability as the spectral gap approaches one. The stability of GNN representations decreases as the spectral gap approaches one. Comparing different scattering representations, it is noted that the representation difference increases as \u03b2 reaches one. Linear models are trained using features from various representations, including graph scattering with varying depth. The aim is to show that scattering representations are rich and stable to graph deformations, particularly in authorship attribution tasks. In authorship attribution tasks, author profiles are constructed using word adjacency networks (WAN) to determine if a text was written by a specific author. A WAN serves as the graph support for word count representation. Stability is illustrated by constructing a WAN with 188 nodes using texts for training and testing with excerpts from Jane Austen and other authors. Classification error is shown in FIG1 based on the number of texts used. The authors conducted experiments with 308 data points, showing that graph scattering transforms improve steadily with more training data, indicating stability and discriminative power. In a second task involving a 234-node graph modeling Facebook interactions, the objective was to localize the source node in a diffusion process, determining its community membership. These signals can be used to model rumors spreading through the social network. The study involved generating training samples of 2,000 nodes with random diffusion times. Graph Fourier Transform (GFT) was computed using the eigenbasis of the operator T. Classification was performed using linear SVMs on representation coefficients obtained from the training set. Test set results showed classification errors as a measure of representation usefulness. Perturbations were illustrated by dropping edges with probability p. In this work, the stability of graph representations was addressed by designing a scattering transform using diffusion wavelets. It was shown that this transform is stable under deformations of the underlying graph support, with deeper graph scattering leading to more accurate classification. Trainable GNN architectures are noted to outperform scattering-based representations in regimes with sufficient labeled examples. The scattering transform of a graph signal on different graphs is related to the diffusion distance between them. Stability bounds for Graph Neural Networks using diffusion operators were obtained. The descriptions can classify plays by author and identify signal origin. Future research should focus on developing stability bounds robust to vanishing spectral gaps. The research focuses on developing stability bounds for graph neural networks using diffusion operators, extending the analysis to broader families of wavelet decompositions on graphs, and achieving stability with respect to the GromovHausdorff metric. The operators diagonalize in the same basis, and frame bounds are obtained by evaluating specific values. The spectrum of the operators is verified, leading to the conclusion that stability bounds can be achieved. The spectrum of \u03c8 j is given by (p j (\u03bb 0 ) , . . . , p j (\u03bb n\u22121)), and it is verified that Q(x) is continuous in [0, 1). By plugging FORMULA5 into (22), we obtain DISPLAYFORM6 (1\u2212\u03b2 2) 3. The node assignment that minimizes T G \u2212 \u03a0T G \u03a0 T is assumed to be the identity. The leading eigenvectors of symmetric matrices T G and T G are bounded with a spectral gap. \u03c1 G = \u03c1 G = \u03c1 due to being a pointwise nonlinearity. For k = 0, U G x \u2212 U G x is bounded by Lemma 5.2. For k = 1, we have DISPLAYFORM10 using the triangular inequality and submultiplicativity of the operator norm. From Lemmas 5.1 and 5.2, we get \u03a8 G \u2212 \u03a8 G \u2264 \u03b5 \u03a8 and U G \u2212 U G \u2264 \u03b5 U. For k = 2, we observe DISPLAYFORM13 and focus on DISPLAYFORM15. After bounding the error induced in the first layer, we plug the results back into the equation, satisfying the condition for k = 2. For general k, the error induced by the mismatch on the low pass filter is \u03b5 U. The error induced by the mismatch on the low pass filter is \u03b5 U, along with a term accounting for propagation through (k \u2212 1) equal layers of initial error, yielding \u03b5 \u03a8, and a third term for the error induced by the previous layer, (k \u2212 1)\u03b5 \u03a8. The propagation of errors in the initial layer is computed, and further analysis is done using submultiplicativity. The proof is completed by induction. From Theorem 5.3, we have certain equations which lead to a final inequality. Applying the inequality, we derive a specific expression for each term."
}