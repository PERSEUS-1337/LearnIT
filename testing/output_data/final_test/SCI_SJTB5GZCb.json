{
    "title": "SJTB5GZCb",
    "content": "The backpropagation algorithm's biological plausibility has been questioned by neuroscientists due to the need for different signals in forward and backward phases, and symmetric bidirectional connections between neurons. A two-phase learning procedure for fixed point recurrent networks is proposed, where neurons integrate leakily and weights are updated locally. This method extends Equilibrium Propagation to general dynamics, approximating the true gradient of the objective function based on the symmetry of feedforward and feedback weights. The backpropagation algorithm's biological plausibility is questioned due to the need for different signals in forward and backward phases. Deep learning BID18 is widely used in computer vision, speech recognition, and machine translation. These applications share the common principle of optimizing objective functions using backpropagation. The question arises whether the brain's cortex implements a similar mechanism for optimizing objective functions. The Equilibrium Propagation framework, based on the work of Hinton & McClelland (1988), suggests error signals in biological networks can be encoded in neural activity derivatives without the need for a separate network. This approach combines inference and error back-propagation. The framework requires network dynamics derived from an energy function to compute gradients accurately. However, this study extends the framework to include general dynamics without energy functions or symmetric connections, aiming for biological realism. The proposed algorithm for supervised learning in neuroscience is based on models that do not assume symmetric connections between neurons. It includes a local update rule for synaptic changes, compatible with spike-timing-dependent plasticity. The algorithm optimizes an objective function and is experimentally validated. Historically, neural network research has focused on models based on energy functions and gradient dynamics. These models often have mathematical properties that simplify analysis, but they are restrictive in terms of the dynamics they can model. In this work, a simple learning algorithm is proposed that does not require computing the gradient of the objective function, aiming to make progress in biologically plausible machine learning. In biologically plausible machine learning, studying dynamics beyond gradient dynamics is essential. Analog hardware implementation of machine learning algorithms may not correspond to gradient dynamics due to the limitations of gradient fields. The leaky integrator neuron model's dynamics, as studied in this work, do not derive from a primitive function. The leaky integrator neuron model studied in this work is not a gradient dynamics unless extra assumptions like exact symmetry of synaptic weights are made. Most work in deep learning focuses on optimizing objective functions through gradient descent, even though following the gradient may not always be the best option. Many optimization methods rely on computing the gradient, with computing the gradient almost synonymous with optimizing in the field of deep learning. The BID19 algorithm challenges the traditional assumption that computing the gradient of an objective function is necessary for optimization in deep learning. Instead, it focuses on finding a direction in the parameter space that leads to optimization, without directly computing the gradient itself. This approach has shown success in optimizing objective functions without relying on gradient computation. The work challenges the need to compute the gradient of the objective function in deep learning, suggesting a shift towards finding directions in the parameter space for optimization. This approach aims to address the weight transport problem in backpropagation and the requirement of symmetric connections in the Hopfield model, proposing a focus on understanding dynamics in the weight space for optimizing objective functions in a biologically plausible manner. The dynamics of neuron models involve membrane voltage and firing rates, with synaptic strengths denoted by Wij. Unlike energy-based models, the connections between neurons in this model are not symmetric. The model studied involves neurons connected in a directed graph, unlike the Hopfield model which is represented by an undirected graph. Spike-Timing Dependent Plasticity (STDP) is a key mechanism for synaptic change in biological neurons, relating synaptic weight changes to timing differences between postsynaptic and presynaptic spikes. The paper discusses the importance of postsynaptic voltage in driving Long Term Potentiation (LTP) and Long Term Depression (LTD), as well as the simulation of Spike-Timing Dependent Plasticity (STDP) using a Hebbian update rule based on pre-and post-synaptic activity. The update rule is referred to as \"STDP-compatible weight change\" and a machine learning justification is proposed for it. The global state variable and connection weights matrix are defined, with a vector field indicating the direction of each neuron's activity changes. The paper discusses the importance of postsynaptic voltage in driving Long Term Potentiation (LTP) and Long Term Depression (LTD), as well as the simulation of Spike-Timing Dependent Plasticity (STDP) using a Hebbian update rule based on pre-and post-synaptic activity. The update rule is referred to as \"STDP-compatible weight change\" and a machine learning justification is proposed for it. The global state variable and connection weights matrix are defined, with a vector field indicating the direction of each neuron's activity changes. In the supervised setting, a two-phase learning procedure is described based on dynamics for state and parameter variables. The paper provides theoretical insights into a proposed algorithm for a neural network. The network consists of input units x and dynamically evolving units h, including hidden layers and an output layer. The vector field \u00b5 is defined by its components on different layers, and the neurons h follow specific dynamics. The output layer h0 is where predictions are made, and target outputs y are compared. The output layer h0 is where predictions are made and compared to target outputs y using a quadratic cost function. The dynamics of the neurons almost always converge to a fixed point h0, which is then compared to the actual target y to minimize the objective function. The objective function to minimize is the cost at the fixed point h0, similar to the method proposed by BID1 BID28. Instead of computing the true gradient of J, a simple algorithm based on leaky integrator dynamics and STDP-compatible weight change is used to compute a proxy for the gradient of J. Equilibrium Propagation BID29 introduces the concept of an \"extended vector field\" \u00b5 \u03b2, controlled by the scalar \u03b2 \u2265 0, to drive output units h 0 towards their target y. The dynamics of the state variable h are redefined with an \"external force\" induced by the external potential \u03b2C. This force, along with the vector field \u00b5, influences the temporal derivative of h. The external force induced by the external potential drives output units towards their target. When \u03b2 = 0, output units are not sensitive to targets, in the free phase. For \u03b2 > 0, output units are driven towards the target, in the weakly clamped phase. A two-phase learning procedure is proposed, with inputs clamped in the first phase and output units free to relax. In the free phase, units relax towards the fixed point without constraints. The second phase, weakly clamped, involves a small positive influence parameter driving output units towards their targets. In the weakly clamped phase, a small positive influence parameter drives output units towards their targets, causing a perturbation that propagates backward through the network, leading to weight changes according to the update rule \u2206W \u221d \u03bd(W). In the weakly clamped phase, weight changes are driven by a small positive influence parameter, leading to the update rule \u2206W \u221d \u03bd(W) where \u03bd(W) acts as a proxy to the gradient \u2202J \u2202W. The algorithm aims to optimize the objective function J by ensuring convergence to a fixed point and a negative scalar product \u2202J \u2202W \u00b7 \u03bd(W) at the point W. During training, the dynamics of h converge to a fixed point and J consistently decreases. The gradient of J can be expressed in terms of \u00b5 and C, and the vector field \u03bd is directly linked to the \"degree of symmetry\" of the Jacobian of \u00b5. In the setting of Equilibrium Propagation, the Jacobian of \u00b5 is symmetric. The Hessian of the objective function E is symmetric, leading to \u03bd being a gradient field of the objective function J. The dynamics converge to an energy minimum, with \"good parameters\" covering a large proportion of the weight space. A form of symmetry or alignment arises between feedforward and feedback weights during training. During training, a form of symmetry emerges between feedforward and feedback weights in a neural network with 3 hidden layers of dimension 512. This symmetry is a result of the learning process itself, although the exact reasons behind it are not fully understood. Previous studies have also noted similar observations in different contexts. Additionally, experiments have shown that using fixed random feedback weights during the backward pass does not hinder learning, contrary to the weight transport problem in backpropagation. During training, a form of symmetry emerges between feedforward and feedback weights in a neural network with 3 hidden layers of dimension 512. The learned feedforward weights tend to align with fixed random feedback weights, and denoising autoencoders show a similar symmetry in weights during learning. Theoretical results suggest that symmetric solutions minimize reconstruction errors in deep generative models. This approach allows for implementing machine learning models in continuous-time systems with minimal dynamic requirements. Our framework allows for implementing a learning system on a physical substrate without exact knowledge of the dynamics or mechanisms. This approach trains the actual analog circuit, accounting for device variations, to perform tasks directly. Our model demonstrates biologically plausible learning in neural networks without imposing symmetry constraints on neural connections. The algorithm assumes two phases, with synaptic changes occurring in one phase. Neurophysiological findings suggest phase-dependent mechanisms play a role in learning and memory consolidation. Theta waves can modulate the learning rule or computation in the network. The work presented in this paper extends the framework of BID29 to general dynamics by relaxing the requirement of an energy function. This leads to an approximation of the gradient in the weight space, with the precision depending on the alignment between feedforward and feedback weights. Experimental observations show that weights symmetrize during training, similar to findings in other settings. Our work extends the framework of BID29 to general dynamics by optimizing objective functions without computing the true gradient. The dynamics in the weight space can be characterized without relying on gradient-based methods. This framework allows for learning in physical substrates like analog electronic circuits, leading to faster and more efficient implementations. The algorithm and theoretical results are presented in this Appendix. The text discusses a physical system with state and parameter variables influenced by an external input. It focuses on optimizing a cost function at a stable fixed point, distinguishing between the cost function and the objective function. The training objective involves minimizing the cost at the fixed point through constrained optimization. The training objective is to optimize a cost function at a stable fixed point using constrained optimization. Traditional methods for computing the gradient of the cost function are considered biologically implausible. Instead, the approach is to let the parameter variable follow a vector field close to the gradient of the cost function. The concept of an extended vector field with an influence parameter is introduced, allowing for differentiability in finding the fixed point for any value of the parameter. The function (\u03b8, \u03b2) \u2192 s \u03b2 \u03b8,v is differentiable for a fixed data sample v. A two-phase training procedure allows for estimating the second term in a biologically realistic way. The gradient of the objective function and the vector field \u03bd can be computed at the fixed point s 0 \u03b8. Theorems and proofs are provided for the differentiation of the fixed point equation with respect to \u03b8 and \u03b2. In the weakly clamped phase, an \"external influence\" slightly attracts the output state to the target, improving prediction error. Proposition 5 generalizes this property to any vector field and cost function. Stable fixed points of the vector field are defined, with the vector field at points in the neighborhood of the fixed point. The Jacobian of \u00b5 at the fixed point s 0 is negative, indicating stability. A small perturbation nudges the network towards lower cost value. Previous methods have been proposed to compute the gradient. The adjoint method leads to Backpropagation Through Time (BPTT) and Recurrent Backpropagation in fixed point recurrent neural networks. BPTT is commonly used in deep learning but requires storing all past states, while Recurrent Backpropagation initializes at the fixed point and does not need to store past states. Our algorithm is considered more biologically plausible. Our algorithm proposes a more biologically plausible approach by computing a proxy to the gradient using STDP-compatible weight changes. We introduce a continuous-time version of Backpropagation Through Time and Recurrent Backpropagation, showing that our method is more biologically plausible. The cost function L(\u03b8, s0, T) is defined for initial state s0 and duration T, converging to J(\u03b8) as T approaches infinity. The gradient \u2202L/\u2202\u03b8 is computed as T approaches infinity by considering the partial derivative \u2202L/\u2202sT-t. The formulas in Theorem 6 provide a continuous-time version of BPTT for propagating partial derivatives, ensuring values of \u2202L/\u2202sT-t hi do not exceed 1. The gradient descent update rule for the state variable h ensures it stays within the range 0 \u2264 h \u2264 1. Different learning rates are used for each layer, improving performance possibly due to approaching fixed points with finite precision. We initialize weights using Glorot-Bengio initialization. The weights are initialized using Glorot-Bengio initialization. Hyperparameters are set for training on the MNIST dataset, with a training error decreasing to 0.00% and a generalization error between 2% and 3%. A Convolutional Neural Network (CNN) was also trained on MNIST with around 2% generalization error."
}