{
    "title": "rkeJzpNtPS",
    "content": "The proposed approaches of locally adaptive activation functions, layer-wise and neuron-wise, enhance the performance of deep neural networks. This adaptation is achieved by introducing scalable hyper-parameters at each layer or for each neuron individually and optimizing them using stochastic gradient descent. Neuron-wise activation functions act as vector activation functions, unlike traditional scalar activation functions. Additionally, an activation slope based slope recovery term is included in the loss function to speed up training and reduce costs. The proposed method utilizes deep neural networks with locally adaptive activation functions, including neuron-wise adaptations and slope recovery terms. It is shown to accelerate training in various deep learning benchmarks. The approach avoids sub-optimal critical points and local minima, enhancing gradient descent algorithms' performance. Neural networks have gained popularity worldwide for their successful applications in fields like speech recognition, computer vision, and natural language translation. Various datasets such as MNIST, Fashion-MNIST, KMNIST, Semeion, SVHN, and CI-FAR are used for training neural networks before applying them in real-world scenarios. These datasets include handwritten digits, images of clothing, accessories, Japanese letters, street view house numbers, and color images. The CIFAR data set contains color images used for training machine learning algorithms, with CIFAR-10 having 10 classes and CIFAR-100 having 100 classes. Efficient algorithms are crucial for designing effective neural networks, with various architectures like Dropout NN proposed to improve efficiency for specific applications. In this work, the focus is on adaptive activation functions in neural networks, which automatically adapt to train the network faster. Various methods for adaptive activation functions have been proposed in the literature, such as adaptive sigmoidal activation functions for feedforward NNs and learning activation functions in convolutional NNs. Different approaches include using multiple activation functions per neuron and tunable activation functions with a single hidden layer. Recently, researchers have proposed adaptive activation functions for neural networks to improve the learning process. These functions introduce scalable hyper-parameters that can be optimized to change the slope of the activation function, leading to faster learning, especially during initial training. This approach, known as globally adaptive activations, optimizes the slope for the entire network. Local adaptation by introducing scalable hyper-parameters at hidden layer-wise or neuron-wise levels can further enhance performance. The curr_chunk discusses a neuron-wise locally adaptive activation function based physics-informed neural network (LAAF-PINN), where activation slopes from every neuron contribute to the loss function. It also outlines the methodology of proposed layer-wise and neuron-wise locally adaptive activations, including the slope recovery term and its impact on training cost. The curr_chunk discusses the methodology of proposed layer-wise and neuron-wise locally adaptive activations in a neuron-wise locally adaptive activation function based physics-informed neural network (LAAF-PINN). It includes the impact on training cost, numerical experiments approximating nonlinear functions, solving the Burgers equation, and presenting results with various deep learning benchmarks. In supervised learning of solution of PDEs, the training data is crucial for training the neural network, obtained from exact solutions, high-resolution numerical solutions, or experiments. Optimal weights are sought for the neural network representation composed of layers with independent and identically distributed weights and bias terms. In PINN, the goal is to minimize a defined loss function by finding optimal weights through gradient descent. The loss function includes mean squared error terms for residual and boundary/initial training data. The neural network must satisfy governing equations and known conditions, leading to an optimization problem for weights and biases. The stochastic gradient descent (SGD) algorithm is commonly used in machine learning for weight updates. The ADAM optimizer, a variant of SGD, is utilized in this work. To optimize the adaptive activation function, two approaches are proposed, involving defining the hyper-parameter hidden layer-wise. The activation function in neural networks can be defined at the hidden layer or neuron level, introducing additional hyper-parameters to optimize. This approach aims to find the minimum of a loss function by optimizing these parameters along with weights and biases. Training can be accelerated by scaling the activation function. The introduction of a scalable hyper-parameter does not change the structure of the loss function. The adaptive activation function aims to increase the slope for faster training of the neural network. This is achieved by including a slope recovery term in the loss function. The adaptive activation function introduces a slope recovery term in the loss function to increase the training speed of the neural network. The inclusion of this term forces the network to quickly increase the activation slope, leading to faster training. The proposed methods ensure that a gradient descent algorithm does not converge to a sub-optimal critical point or local minimum, given appropriate initialization and learning rates. Theorem 2.1 discusses the conditions for a sequence generated by a gradient descent algorithm to converge to an optimal critical point. It outlines three scenarios involving Lipschitz continuity of the gradient, learning rate constraints, and adaptive learning rate selection. The limited minimization rule, Armijo rule, or Goldstein rule are discussed. For both L-LAAF and N-LAAF, no limit point is a sub-optimal critical point or local minimum. Initial condition J(\u0398 0 ) < Jc(0) + S(0) is required. The proof of Theorem 2.1 is in appendix A. Regression problem of nonlinear function approximation using deep neural network is solved. Burgers equation using physics-informed neural network is in appendix B. Standard neural network is used to approximate a discontinuous function with loss function consisting of data mismatch and slope recovery term. The deep neural network approximates a discontinuous function with a discontinuity at x = 0. The domain is [-3, 3] with 300 training points, using tanh activation function, learning rate of 2.0e-4, and four hidden layers with 50 neurons each. The solution is shown in three columns: solution, solution in frequency domain, and pointwise absolute error in log scale. Locally adaptive activation functions show increased training speed compared to fixed and globally adaptive activations. Locally adaptive activation functions, such as L-LAAF and N-LAAF with slope recovery term, accelerate training and reduce error compared to other methods. The activation slopes increase rapidly with the slope recovery term, as shown in the layer-wise variations. Loss function comparisons indicate the effectiveness of L-LAAF and N-LAAF with slope recovery. The Loss function for L-LAAF and N-LAAF decreases faster without the slope recovery term, especially during initial training, compared to fixed and global activation function algorithms. Adaptive activation functions show advantages in physics-related problems. The section explores if these advantages remain with standard deep neural networks for other deep learning applications, presenting numerical results with various benchmark problems. Figures 5 and 6 show mean values and uncertainty intervals for different activation functions. The text discusses the use of different activation functions and slope recovery terms in neural networks. Results show that the slope recovery term improves the methods, with L-LAAF outperforming GAAF. The study used pre-activation ResNet for CIFAR-10, CIFAR-100, and SVHN datasets, and a standard variant of LeNet for training. The study utilized a standard variant of LeNet with ReLU for various datasets, consisting of five layers including convolutional and fully connected layers. Hyper-parameters were consistent across all models, with a fixed mini-batch size, learning rate, and momentum coefficient. The learning rate was reduced at specific epochs for all experiments. In this paper, two versions of locally adaptive activation functions are introduced to improve neural network training speed. An activation slope based slope recovery term is added to the loss function to enhance performance. Various test cases and benchmark problems are solved to validate the approach, and it is theoretically proven that no sub-optimal critical point attracts gradient descent algorithms. The proposed methods (L-LAAF and N-LAAF) with the slope recovery term prove that no sub-optimal critical point attracts gradient descent algorithms. The proof is based on various conditions and rules of the learning rate, contradicting the assumption for both L-LAAF and N-LAAF. The Burgers equation is a fundamental partial differential equation found in various fields. The proof for N-LAAF involves contradiction, showing that no sub-optimal critical point attracts gradient descent algorithms. The Burgers equation, introduced by Bateman and studied by Burgers, is a differential equation used in nonlinear acoustics, gas dynamics, and fluid mechanics. The viscous Burgers equation with initial and boundary conditions is considered, leading to a steep solution due to small values. The analytical solution can be obtained using the Hopf-Cole transformation. The training points and parameters for solving the equation are specified. The Burgers equation is being analyzed with a focus on the evolution of frequency plots using different activation functions and hidden layers. The frequencies converge faster towards the exact solution with adaptive activations compared to fixed activation functions. The comparison of different activation functions shows that the adaptive activations, particularly GAAF, decrease faster. The comparison of activation functions in the Burgers equation shows that the presence of a slope recovery term increases the training speed. This is evident in the layer-wise variation of activation functions for both L-LAAF and N-LAAF with and without the slope recovery term. The results indicate that the slope recovery term enhances the activation function, leading to faster network training speeds."
}