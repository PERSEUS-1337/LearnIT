{
    "title": "HJM4SjR5KQ",
    "content": "We propose a framework based on a mixture of shared hidden Markov models to model sequential data from entities connected in a graph with a known topology. The method exploits the graph structure to create sparse mixtures. Experiments in various application domains have been conducted. Hidden Markov models (HMMs) are widely used for sequential data modeling, including in fields like speech recognition, computational biology, data compression, and computer vision. In cognitive radars, HMMs are applied to model channel occupancy by primary users. When HMM expressiveness is insufficient, mixtures of HMMs are used, combining independent standard HMMs through a memoryless transformation. This approach is effective for scenarios with multiple network-connected entities interacting in various real-life settings. In neuroimaging studies, the brain is seen as a network where nodes represent specialized regions and links represent communication pathways. Team sports involve complex interactions among players, requiring understanding of teammates and opponents. Extracting knowledge from these data streams for decision-making remains a challenge. The adaptation of Hidden Markov Models (HMM) for decision-making from data streams is challenging. BID9 proposed a hybrid approach combining Self-Organizing Map (SOM) and HMM for clustering and visualization. Different approaches in modeling wireless networks with HMMs have been explored, including outlier detection in wireless access points. BID4 developed a Sparse Coupled Hidden Markov Model (SCHMM) for analyzing functional MRI data. The proposed model suggests using a sparse mixture of Hidden Markov Models (HMMs) to capture intercorrelations between sensors in networked data streams, promoting sharing of components between similar nodes. This approach aims to improve the learning process by acting as a regularizer. The distributed sparse representation in networks with multiple nodes considers intra-and inter-correlations, leading to a joint sparse model. Multitask learning techniques suggest that related tasks should share some structure, such as parameters or hyperparameters. Based on multitask learning techniques, models for related tasks can share structure through parameter transfer approaches. The generative distribution of data from each node in a network can be modeled as a sparse mixture from a dictionary of distributions, with similarities between nodes captured in an affinity matrix. The learning process aims to reuse HMM atoms between similar nodes. The text discusses modeling the conditional distribution of sequences in a network using a mixture of Hidden Markov Models (HMMs), with latent variables and hidden states. The nodes in the network are connected by an undirected weighted graph, where the weights represent affinity between nodes. The goal is to capture similarities between nodes and reuse HMM atoms for related tasks. Equation 2 assumes conditional independence between sequence X and node y, given latent variable z. This promotes parameter sharing among the K mixtures. Inference involves finding the likelihood ppX|yq using equations 2 and 3. Marginals ppX|zq of each HMM can be computed efficiently using the Forward algorithm. Inference in the model is done in at most OpM S 2 T q time, with sparse mixtures leading to smaller time complexity. The model defined by equations 2 and 3 can be easily trained using the Expectation-Maximization (EM) algorithm, maximizing the log-likelihood objective with various model parameters such as mixture coefficients, initial state probabilities, state transition matrices, emission probability means, and emission probability covariance matrices. The EM algorithm can be used to train the model defined by equations 2 and 3. The emission probabilities are assumed to be gaussian with diagonal covariances. Maximizing the objective can be done using Algorithm 1 in section A.1, with update formulas obtained from the standard EM procedure. To exploit the known structure of G, a regularization term is introduced in the objective. The expectations E z\"ppz|y\"j,\u03b8q rppz|y \" k, \u03b8q have interesting properties proven in Proposition 1, which states that for all M-nomial probability distributions, the objective is to minimize the expectation of the dot product of two M-dimensional vectors \u03b1 p and \u03b1 q representing distributions p and q, subject to certain constraints. The equality E z\"p rqpzqs \" 1 holds if p and q are chosen from the defined set. The equality E z\"p rqpzqs \" 1 holds if p and q are chosen from the set defined in Proposition 1, where the distributions p and q are the same and non-zero for a single assignment of z. This proves statement 3. To prove statement 4, it is shown that there are no other maximizers by transforming inequalities into equalities based on specific conditions. Specifically, for distinct nodes j, k in set Y, the regularization term is maximized when the mixtures have one single active component. By setting \u03bb to zero, the initial objective is achieved where inter-node correlations are modeled only via parameter sharing. As \u03bb increases, two scenarios may occur: if G j,k is greater than zero, all nodes will share the same single mixture component, learning one HMM for the whole network. If G j,k is less than zero, each node will learn its own HMM independently. This promotes sparsity in mixtures, allowing coefficients to capture sparse graph structures and leading to faster inference with less overfitting. The introduction of a regularization term in the objective function makes it necessary to use gradient ascent in the M-step to update mixture coefficients. Reparameterization is adopted to ensure admissible solutions during iterative steps. The study evaluates four models: a single HMM trained on all graph nodes, K independently trained HMMs, and others. Source code, pre-trained models, and datasets are publicly available for reproducibility. The study evaluates different models for graph nodes, including K independently trained HMMs, a mixture of HMMs (MHMM), and a regularized mixture of HMMs (SpaMHMM). Models are compared based on performance and sparsity of mixtures. Training is done with a similar number of state transitions for fair comparison. The Wi-Fi network infrastructure consists of K access points (APs) where users may switch between seamlessly. Various anomalies can occur during operation. Models like 1-HMM, K-HMM, MHMM, and SpaMHMM are trained using different algorithms for performance evaluation and sparsity of mixtures. Anomalies in wireless networks, such as overloaded APs, RF interference, and authentication failures, are crucial to detect for mitigation plans. Ground truth annotation of these anomalies is costly, so simulations using tools like OMNeT++ and INET are common for evaluation. In a typical Wi-Fi network setup, simulations using OMNeT++ and INET were conducted to generate traffic. The network consists of 10 APs and 100 users, with detailed information on traffic patterns collected in 15-minute time slots over 10 consecutive hours. Each sample includes data on unique users, sessions, association time, octets transmitted and received, and packets transmitted and received in the AP. In an experiment with Wi-Fi network simulations, anomalies occur for a limited time within sequences. Sequences are labeled as \"anomalous\" if they have at least one anomaly period. Normal data was used for training, while testing included normal and anomalous sequences. Traffic in adjacent APs is expected to be similar as users move between them. Weight G j,k in the graph G represents the connection between nodes j and k. The weight G j,k in the graph G represents the connection between nodes j and k, set to the inverse distance between APs j and k. Sequences were preprocessed and reduced to 3 features using PCA. MHMM used 10 mixture components and 10 hidden states. SpaMHMM used the same values and cross-validated for regularization hyperparameter \u03bb. 1-HMM and K-HMM were also cross-validated for the number of hidden states. Models were trained for 100 EM iterations or until the loss plateaus. SpaMHMM had 100 iterations of the inner loop on each M-step. The models MHMM and SpaMHMM outperformed 1-HMM and K-HMM in terms of average log-likelihood and ROC curves. K-HMM had the worst performance due to being trained with small amounts of data from each graph node. The models MHMM and SpaMHMM outperformed 1-HMM and K-HMM in terms of average log-likelihood and ROC curves. K-HMM had the worst performance due to being trained with small amounts of data from each graph node. In contrast, 1-HMM uses a single HMM trained with the entire dataset, but struggles to capture the diverse data distributions from different access points (APs). MHMM and SpaMHMM combine the strengths of both models by training each HMM with sequences from all graph nodes, allowing them to capture a wider range of behaviors. The models MHMM and SpaMHMM outperformed 1-HMM and K-HMM in terms of average log-likelihood and ROC curves. SpaMHMM was expected to be sparser and have better performance than MHMM, but only the former supposition was true by a small difference. The lack of performance gains in SpaMHMM may be due to the dataset consisting of static users, where the assumption that closer access points have similar distributions does not provide an advantage. Our model predicts human joint positions from motion capture data using deep recurrent neural networks. Experiments on the Human3.6M dataset show promising results for short-term prediction of actions like \"walking\", \"eating\", \"smoking\", and \"discussion\". The dataset records 32 joints at 50 Hz, represented in a 32x32-dimensional symmetric matrix G. The study utilized a dataset with 32 joints represented in a symmetric matrix G. Data was down-sampled and transformed for analysis, with training conducted on 6 subjects and testing on one. Cross-validation was performed to determine optimal parameters for the baseline mixture MHMM model. The study used cross-validation to optimize parameters for the baseline mixture MHMM model, resulting in M=18 and S=12. SpaMHMM also used M=18 and S=12, with \u03bb=0.05. 1-HMM had 51 hidden states, K-HMM had 11 hidden states per HMM. Models were trained for 100 iterations or until loss plateaus. SpaMHMM used a learning rate of \u03c1=10\u00b42 for 100 iterations on each M-step. The study evaluated different models (1-HMM, K-HMM, MHMM, SpaMHMM) for generating predictions for joint movements. SpaMHMM outperformed the other models in all actions except \"eating\", where MHMM performed slightly better due to asymmetry in body sides. Results are presented in table 2. The study compared different models for joint movement predictions, with SpaMHMM performing best overall. The skeleton structure information in the graph G proved useful as a prior for SpaMHMM, leading to better solutions than MHMM. Results for 1-HMM and K-HMM were worse due to limitations in training data. The mean angle error for short-term motion prediction on Human3.6M for various actions and time horizons is shown in Table 2. The study compared different models for joint movement predictions, with SpaMHMM performing best overall. ERD, LSTM-3LR, SRNN, GRU supervised, and QuaterNet were compared, with the latter two outperforming all models. RNN-based architectures like ERD and LSTM-3LR were designed specifically for this task, showing better results than SpaMHMM. RNNs can model more complex dynamics than HMMs, allowing for long-term motion generation. SpaMHMM models the probability distribution of data directly, making it suitable for novelty detection. Experiments show that SpaMHMM mixture coefficients are sparser than MHMM. The human body can be divided into four parts, with joints in the same part having coherent motion. SpaMHMM, trained to exploit skeleton structure, shows this effect more than MHMM. Training MHMM and SpaMHMM for \"walking\" with four mixture components, the most likely component for each action was determined. MHMM successfully divides the body into two main parts, assigning torso and upper body joints to the red cluster and hips, legs, and feet joints to the green cluster. Symmetric joints are mostly assigned to the same cluster. Some assignments are unnatural. SpaMHMM divides the body into four regions: upper body and upper spine in the green cluster, arms in the blue cluster, lower spine and hips in the orange cluster, and legs and feet in the red cluster. The model uses joint connections and symmetry to create a natural division, unlike some assignments that are unnatural. The distribution of joints per cluster is uneven, with the green cluster being the most represented. The proposed method divides the skeleton into intuitive clusters, ensuring even distribution of joints per cluster. This approach helps SpaMHMM outperform MHMM by preventing any HMM component from learning too many motion patterns. The method is based on a mixture of HMMs with regularized coefficients, allowing for the modeling of sequential data from connected nodes in a graph with a fixed topology. The proposed method involves regularizing coefficients during the learning process to promote similarity in affine nodes and sparsity in mixtures. It has been evaluated in anomaly detection and human motion forecasting tasks, demonstrating effectiveness and versatility. Future work includes extending SpaMHMM for sequence clustering and exploring mixtures of more powerful generative distributions. The proposed method involves extending ideas from HMMs to develop a more flexible architecture based on deep recurrent models for sequence modeling. The objective is to maximize a well-known function with respect to model parameters, constrained to represent valid probabilities. The Lagrangian is used to minimize the function with respect to a specific value, yielding the usual solution. The proposed method extends ideas from HMMs to develop a flexible architecture based on deep recurrent models for sequence modeling. In the M-step, the goal is to find the maximum value for a function with respect to model parameters, using the Lagrangian to minimize the function for a specific value. The update equations for parameters are unchanged, except for \u03b1, which requires a reparameterization and updating of unconstrained parameters \u03b2 via gradient ascent. In the context of deep recurrent models for sequence modeling, the update equations for parameters involve reparameterizing and updating unconstrained parameters \u03b2 via gradient ascent. The resulting gradient is scaled by the joint data likelihood, affecting the learning rate. Equation 8 provides the derivatives for the parameters \u03b1 and \u03b2."
}