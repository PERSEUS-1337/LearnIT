{
    "title": "r1lGO0EKDH",
    "content": "GraphZoom is a multi-level framework designed to enhance the accuracy and scalability of unsupervised graph embedding algorithms. It incorporates node attribute information and addresses issues like node attribute noise. The framework involves graph fusion to create a new graph encoding both topology and node attributes, followed by coarsening the graph for efficiency. Existing embedding methods can be applied to the coarsened graph, with progressive refinement of the embeddings. GraphZoom enhances unsupervised graph embedding algorithms by progressively refining embeddings on coarsened graphs, improving classification accuracy and reducing runtime. Graph embedding encodes graph elements into low-dimensional vectors for tasks like vertex classification and link prediction. Existing methods have drawbacks, such as random-walk based algorithms like DeepWalk. Several drawbacks exist in current graph embedding algorithms. Random-walk based methods like DeepWalk and node2vec do not incorporate node attribute information, limiting their power. Graph convolutional networks (GCN) aim to smooth node embeddings over the graph but may suffer from noise in initial node features. Additionally, few algorithms can scale well to large graphs due to high computation and storage costs. When stacking multiple GNN layers, the final embedding vector of a node involves computing many intermediate embeddings from its neighbors, leading to increased computations and high memory usage. Research efforts focus on improving accuracy and scalability in graph embedding methods, with most addressing only one of these problems. For example, Fu et al. (2019) proposed multi-level methods for high-quality embeddings but with limited scalability. Liang et al. (2018) developed a heuristic algorithm to coarsen the graph for improved efficiency. GraphZoom is a multi-level spectral approach to enhancing unsupervised graph embedding methods. It includes four kernels: graph fusion, spectral graph coarsening, graph embedding, and embedding refinement. The approach aims to improve the quality and scalability of graph embeddings by fusing node feature matrices with the original topology graph. The GraphZoom framework enhances unsupervised graph embedding methods by fusing node feature matrices with the original topology graph. It utilizes spectral graph coarsening to retain key graph structures and applies embedding refinement to refine embeddings back to the original graph. The framework is validated on three transductive benchmarks: Cora, Citeseer, and Pubmed citation networks. GraphZoom framework improves classification accuracy on various datasets including Cora, Citeseer, and Pubmed. It generates high-quality embeddings by encoding graph structures and node attributes, reducing noise for up to 19.4% accuracy increase. The approach also enhances scalability by reducing embedding run time through effective graph coarsening. GraphZoom accelerates the embedding process by up to 40.8x without sacrificing accuracy. It is compatible with various unsupervised embedding techniques and draws inspiration from multi-level graph embedding and graph filtering to enhance performance and speed. Fu et al. (2019) and Zhang et al. (2018b); Akbas & Aktas (2019) proposed hierarchical sampling and embedding on multi-level graphs to improve embedding quality but lacked scalability. Liang et al. (2018) introduced MILE, which trains the coarsest graph and refines embeddings with GCN, but it is time-consuming for large graphs and not inductive. GraphZoom is a spectral approach to improving unsupervised graph embedding models by utilizing graph filters, which are analogs of classical filters in signal processing. It aims to enhance embedding quality and scalability compared to traditional methods like GCN. Maehara (2019) demonstrated the connection between graph embedding and filtering, showing that GCN implicitly uses graph filters to remove noise from node features. This insight led to the proposal of a filter neural network (gfNN) to derive stronger graph filters for improved embedding results. The proposed GraphZoom framework consists of four key phases: graph fusion, spectral graph coarsening, applying graph embedding methods, and mapping embedding vectors onto a finer graph for refinement. This framework aims to improve embedding quality for various classification tasks. The GraphZoom framework consists of four phases: graph fusion, spectral graph coarsening, graph embedding methods, and mapping embedding vectors for refinement. Graph fusion constructs a weighted graph with the same nodes but potentially different edges to encapsulate the original graph's topology and node attributes. Graph fusion in the GraphZoom framework involves converting the initial attribute matrix into a weighted node attribute graph using a k-nearest-neighbor (kNN) graph based on the l2-norm distance between attribute vectors. To construct the attribute graph in linear time, a spectral graph coarsening scheme is utilized to reduce the graph to fewer nodes. This approach is similar to spectral graph clustering and allows for efficient graph construction. Graph fusion in the GraphZoom framework involves converting the initial attribute matrix into a weighted node attribute graph using a k-nearest-neighbor (kNN) graph based on the l2-norm distance between attribute vectors. A spectral graph coarsening scheme is used to reduce the graph to fewer nodes, similar to spectral graph clustering. The approximate kNN graph construction has a total run time complexity of O(MN) when a proper coarsening ratio is chosen. The weight of each edge in the attribute graph is assigned based on the cosine similarity of the nodes' attribute vectors. Finally, the fused graph is constructed by combining the topological graph and the attribute graph. The fusion process in the GraphZoom framework combines the topological graph and attribute graph using a fusion equation. This fused graph allows graph embedding models to utilize both graph topological and node attribute information. Graph coarsening is achieved through global spectral embedding, reducing the size of the original graph while preserving important spectral properties. In this work, an efficient local spectral embedding scheme is used for graph coarsening to identify node clusters based on graph signal processing techniques. The approach leverages similarities between traditional signal processing and graph signal processing, where signals at different nodes in a graph correspond to signals at different time points in classical Fourier analysis. The text discusses using low-pass graph filtering to obtain smoothed vectors for k-dimensional graph embedding, which can be achieved in linear time. By applying the smoothing function on a random vector x, a smoothed vector x can be obtained as a linear combination of the first few eigenvectors of the graph Laplacian. The text discusses using Gauss-Seidel iterations to solve linear systems of equations for graph embedding. Nodes are embedded into a low-dimensional space based on spectral similarity, measured by spectral node affinity. Aggregation schemes are determined based on these calculations, allowing for spectrally-reduced graph mapping operators. The text discusses using spectral sparsification to control graph densities and achieve desired sparsity levels. A tool called \"GRASS\" is used for this purpose. Node embeddings are obtained for the coarsest graph using unsupervised methods. Projection operators are then used to project node embeddings from coarse to fine-grained graphs. The operator H i i+1 copies node embeddings from coarse to fine-grained graphs. A local refinement process smooths embeddings by minimizing an objective function involving Laplacian matrices. The refined embedding matrix is obtained by solving an equation enforcing agreement with mapped embeddings and using Laplacian smoothing. The Laplacian smoothing technique is used to smooth embeddings over a graph efficiently. By utilizing a graph filter in the spectral domain, the process avoids time-consuming matrix inversions. The graph filter is approximated using a first-order Taylor expansion to generalize the smoothing process. Adding self-loops to nodes in the graph enhances the filtering effectiveness in removing high-frequency noise components. We modify the adjacency matrix to include self-loops for each node and use a low-pass graph filter to smooth the embedding matrix. The refinement stage of our approach involves sparse matrix multiplications without the need for training. GraphZoom framework is evaluated against existing techniques on various datasets, including a scalability analysis on the Friendster dataset with millions of nodes and edges. Friendster dataset with 8 million nodes and 400 million edges is analyzed. GraphZoom kernels are evaluated separately. Various datasets like Cora, Citeseer, Pubmed, Friendster, PPI, and Reddit are used for transductive and inductive tasks. Comparison is made with transductive models DeepWalk, node2vec, and DGI, as well as inductive models like MILE. GraphZoom is compared against GraphSAGE using different aggregation functions to show its enhancement in inductive learning. Hyperparameters of DeepWalk, node2vec, DGI, and GraphSAGE are optimized on original datasets, and then applied to embed coarsened graphs in HARP, MILE, and GraphZoom. Experiments are conducted on a machine with specific specifications, and results are reported in Tables 2 and 3 for transductive and inductive tasks. The CPU time for graph embedding is measured for DeepWalk, node2vec, DGI, and GraphSAGE. HARP, MILE, and GraphZoom total run time includes graph coarsening, embedding, and refinement. Hyperparameters are fine-tuned for DeepWalk, node2vec, DGI, and GraphSAGE. GraphZoom is compared with baseline embedding methods, showing improved accuracy and speed in unsupervised embedding tasks. It outperforms DeepWalk and node2vec by 8.3%, 19.4%, and 10.4% on Cora, Citeseer, and Pubmed datasets for transductive learning, with up to 40.8x runtime reduction. Compared to DGI, GraphZoom achieves comparable or better accuracy with a speedup of up to 11.2x. Reducing graph size while retaining key spectral properties boosts embedding speed and quality. GraphZoom outperforms multi-level frameworks like HARP and MILE in accuracy and speed, with up to 7.6x speedup. GraphZoom achieves better accuracy and speed compared to MILE across all datasets, even with increasing coarsening levels. It can retain key graph structure information for high-quality node embeddings. Results on non-attributed graphs and large graph embedding, such as the Friendster dataset, show significant performance improvements. GraphZoom significantly boosts the Micro-F1 score compared to MILE and DeepWalk, with a speedup of up to 119.8\u00d7. It effectively coarsens large graphs by merging redundant nodes while preserving important structural properties for high-quality node embeddings. In contrast, MILE's heuristic coarsening algorithm struggles to maintain meaningful graph structures, especially with a large reduction ratio. Comparisons of kernel combinations in GraphZoom and MILE show that GraphZoom's refinement kernel can improve embedding results, especially with a large coarsening level. Our proposed graph filter in refinement kernel successfully filters out high frequency noise from the graph to improve embedding quality, especially with a large coarsening level. Comparing coarsening kernels in GraphZoom and MILE, GraphZoom's coarsening kernel improves embedding quality more than MILE's. Combining GraphZoom's coarsening kernel and refinement kernel achieves better classification accuracy compared to MILE's kernels, showing that they play different roles in boosting embedding performance. GraphZoom is a multi-level framework that improves embedding quality and scalability of unsupervised graph embedding techniques by encoding graph structure and node attributes. It utilizes spectral coarsening and refinement methods to remove high frequency noise from the graph, leading to improved classification accuracy. The combination of graph fusion kernel with GraphZoom framework significantly enhances embedding performance, incorporating both graph topology and node attribute information. Experiments show that GraphZoom improves classification accuracy and embedding speed on popular datasets like Cora, Citeseer, and Pubmed. Future work could focus on propagating node labels to the coarsest graph to support supervised graph embedding models. Additionally, evaluations were done on the Friendster dataset, which contains millions of nodes and edges, with a small percentage used for training and testing. In Friendster dataset, nodes represent users linked as friends. Experiments on protein-protein interaction (PPI) and Reddit datasets were conducted using DeepWalk and node2vec for node embeddings. PPI dataset consists of human tissue graphs, while Reddit dataset contains user post nodes connected by comments. Training and testing splits were 60/40 for PPI and 65/35 for Reddit. Node2vec, Deep Graph Infomax (DGI), GraphSAGE, and HARP are different approaches for generating node embeddings. Node2vec introduces parameters for neighborhood exploration, DGI maximizes mutual information between local and global graph information, GraphSAGE learns aggregation functions for node embeddings, and HARP coarsens the graph into levels for training embeddings. GraphZoom is compared with MILE, a state-of-the-art unsupervised graph embedding framework, on various datasets including Friendster. MILE uses a heuristic-based coarsening kernel and Graph Convolutional Network (GCN) for embedding refinement. Graph size at different coarsening levels on six datasets is detailed in Table 4. The mapping operator H i+1 i is a matrix with 0s and 1s, representing node aggregation in graph coarsening. It is locality-preserving and ensures connectivity in the coarsened graph. The spectral coarsening algorithm reduces the graph size and produces an adjacency matrix for the reduced graph. The GraphZoom framework utilizes a graph reduction ratio to efficiently reduce total CPU time for graph embedding. The proposed graph filter acts as a band-stop filter, attenuating frequencies within a specific range to refine the embedding process. The band-stop filter may not effectively remove high-frequency noises from graph signals. Adding self-loops to each node in the graph can squeeze Laplacian eigenvalues towards zero, allowing the filter to effectively filter out high-frequency components while retaining low-frequency components. Choosing an appropriate value for \u03c3 is crucial, with \u03c3 = 2.0 being chosen for all experiments in this work. GraphZoom with a fusion kernel shows a tradeoff between embedding quality and runtime efficiency. It achieves the highest quality when the fusion kernel is included. Evaluation on non-attributed datasets like PPI(Homo Sapiens) and Wiki shows improvement without the fusion kernel."
}